"

As we celebrate our 40th year, I’d like to recap some of Cato’s accomplishments and tell you where we’re heading.



Cato has been a vigorous proponent of entitlement restraints, the right to bear arms, marriage equality, fundamental tax reform, downsizing government, property rights, drug legalization, school choice, free trade, immigration liberalization, criminal justice reform, and term limits. We have energetically resisted corporate welfare, campaign finance restrictions, government constraints on the Internet, global warming excesses, overarching executive power, infringements on civil liberties, the administrative state, imperial wars, dubious foreign entanglements, and unnecessary government secrecy.



Cato was the first to address Social Security’s financial problems and offer a private‐​investment alternative. José Piñera, co‐​chairman of Cato’s Project on Social Security Choice, was the architect of privatization in Chile. We’re still fighting for entitlement reform here, where the fiscal implications for Social Security, Medicare, and Medicaid are dismal.



On the health care front, Cato’s efforts yielded Health Savings Accounts — a significant step toward free market health care. And we led the battle against Obamacare. The 2015 Supreme Court challenge was crafted by Cato’s director of health policy studies, Michael Cannon, who demonstrated Obamacare’s flawed structure and legal infirmities.



Our Center for Constitutional Studies, under Roger Pilon’s innovative leadership, has been a forceful advocate for a textual interpretation of the Constitution and a principled judicial engagement to bind the legislative and executive branches with the chains of the Constitution. Pilon and Ilya Shapiro, editor of the peerless _Cato Supreme Court Review_ , compiled an amazing record of amicus briefs, with the Court favoring the party we supported in an overwhelming percentage of cases.



In 2014, we expanded our Center for the Study of Science, which challenges the pseudoscientific claims of climate‐​change alarmists. That same year, Cato’s Center for Monetary and Financial Alternatives got under way — aggressively responding to the threat of an undisciplined central bank and fiat money.



We recognize, of course, that reaching a young audience is essential. Our Libertarian Leadership Project will enable us to dramatically expand our online contact with young, tech‐​savvy friends of liberty — complementing our intern program and Cato University. We’ve also fostered high‐​quality education through the work of the late Andrew Coulson, who directed Cato’s Center for Educational Freedom. Coulson produced a three‐​part documentary that embraces freemarket reforms to make schools more innovative and responsive. _School Inc. — A Personal Journey with Andrew Coulson_ is now available on PBS stations across the country.



Persuasion is key to our mission; and David Boaz’s latest book, _The Libertarian Mind_ , is the perfect messenger — a scholarly but readable work that popularizes and legitimizes libertarianism. Indeed, Cato books are in every major college library and have sold tens of thousands of copies. Cato scholars also deliver hundreds of college lectures annually, presenting the libertarian alternative to the next generation.



In the economic arena, our experts analyze tax reform and budget proposals — unraveling them so they’re digestible. Cato’s “Fiscal Report Card on the Nation’s Governors” is widely quoted, and our Herbert A. Stiefel Center for Trade Policy Studies shaped the debates over Trade Promotion Authority and the Trans‐​Pacific Partnership.



Cato has also been a prominent defender of civil liberties — especially those related to government surveillance and privacy. Meanwhile, our Project on Criminal Justice has steered public opinion against police militarization and the drug war.



In addition, we’re emphasizing the moral and philosophical arguments for liberty. Online courses are available from our Lib​er​tar​i​an​ism​.org website. They’re aimed at young people but accessible to a large and growing audience, as is our CatoAudio app for iOS devices, which contains our daily podcast, archived policy forums, _Classics of Liberty_ , the monthly CatoAudio magazine, and lots more.



A big focus of our 40th year has been finding ways to keep getting better and even more impactful. There is scope to be more connected and effective on Capitol Hill and in states, and in the academy, too, by hosting more visiting scholars. We also have great opportunities to expand the audience of our existing scholarship and research through the expanded use of technology and a broader array of content. We plan to strengthen our efforts to nurture young talent and the development of more organizations and initiatives across the liberty movement.



In short, Cato is an independent, nonpartisan source of intellectual ammunition to the public, government, educators, and the media. Ideas do matter. That’s the reason Cato is indispensable. As our 40th year draws to a close, we reaffirm our enduring commitment to the cause of human freedom.
"
"As British high streets and farm fields lie under water this week, Boris Johnson has repeatedly been urged to put on his wellies, go out and listen to flood victims. So far though, his response has been more about tin ears than rubber boots: during Storm Dennis the prime minister was reportedly holed up in a 17th-century mansion in the Kent countryside. As even Nigel Farage and the Sun have pointed out, this is a pathetic failure of leadership. On a more sinister level, it is entirely consistent with a darkly elitist view of how to deal with the climate crisis. The question of climate priorities will grow increasingly important. Which communities will the government defend and which will they abandon? This threat to human civilisation ought to merit collaborative action at the highest global level. This seemed to be the case in the 1980s and 90s – those halcyon days of the “international community” – when world leaders stepped up to reverse the depletion of the ozone layer and put in place new United Nations structures to address the then nascent threats of global warming and biodiversity loss. Since then, decades of fossil fuel-funded denial and a shift in the political and economic landscape have taken their toll. Today, a growing number of governments prefer to stress how impotent they are in the face of market and natural forces, while multibillionaires – who are more powerful than ever – have started building apocalypse sanctuaries, applying for New Zealand citizenship or backing politicians who promise to erect higher physical and legal walls at their borders. Anything to keep out the weather and the climate-affected masses. Outright denial of climate science is now almost impossible. In the UK, the lengthening summer heatwaves and more intense winter deluges have seen to that. They have also demonstrated that walls and money are not enough. The environment secretary, George Eustice, admitted as much this week, when he said: “We’ll never be able to protect every single household just because of the nature of climate change and the fact that these weather events are becoming more extreme.” This raises the question of climate priorities, which will grow increasingly important as floods and heatwaves affect more people and property. Which communities will the government defend and which will they abandon? More importantly, how will it balance the resources for adaptation infrastructure (such as sea walls, flood barriers, drainage channels ) with those for mitigation (cutting emissions through forests, wetlands, regulation of petrochemical firms and a transition to renewable energy)? This is a choice between tackling the short-term local symptoms or the long-term global causes. Traditionally that has been the political dividing line between the right and the left. Today, it is the difference between climate apartheid – effectively excluding those affected on economic or racial grounds – and climate solidarity. The UK has steered a pragmatic course between these two extremes until now. Most politicians in this country recognise the most cost-efficient way of dealing with global warming is to cut emissions now to avoid far more expensive damages in the future. On the left and right, there has been a sense of social responsibility and national pride that Britain initiated the Industrial Revolution and should take the lead in clearing up the mess it left behind. But will that consensus hold as the domestic costs of climate action ramp up? Or will the free-market extremists in the government focus on protecting wealthy and economically productive regions rather than sharing the burdens and thinking about the future? This sodden week alone does not answer those questions, but it is worrying that we have an environment minister apparently ready to abandon some areas, and a prime minister who has cut himself off from those suffering the consequences. The country and the world needs a leader who steps out and steps up on climate. So far, Johnson has done neither. • Jonathan Watts is the Guardian’s global environment editor"
"
Share this...FacebookTwitterAnd that includes those of you who do not agree with me. Now here’s a humorous little clip that I think is a little relevant.

It’s always a good idea to have quality data and analyses before deciding on and implementing precautionary measures.
Have a good one, folks!
Share this...FacebookTwitter "
"

What a wonderful year, 1998! Global temperatures reached their highest value recorded in all three available records — surface, satellite, and weather balloon. Sayers of doom had pronounced dire and immediate consequences — so for once it was possible to check their models of misery against what actually happened when it really got warm. 



**El Niño vs. Greenhouse Warming**



Judging from the conflation of El Niño and human‐​induced global warming, you might think the two were one and the same, or maybe even, as Vice President Al Gore intimated, that one caused the other. 



Like many of his reaches, there was a bit of truth in the stretch, but only a bit. Global warming didn’t cause El Niño in any appreciable sense, but the two were related: It was a very good El Niño year, and it was a very, very warm year. 



El Niño is natural. Just because scientists discover something, or because we, as taxpayers, shell out tens of millions of dollars to research something, does not mean that something new has happened. Chemicals existed before chemists, DNA existed before its discovery won a Nobel Prize, and El Niño ebbed and flowed long before the first climatologist was born. 





No one knows why the trades suddenly slow or even reverse, piling warm water up against South America. Reid Bryson, the modern founder of the very true notion that climate does change in ways that are important to people, believes this reversal is mainly an effect of some other large‐​scale physical fluctuation. During an El Niño, a large portion of the tropical Pacific is much warmer than average — as much as 8oC (14.4oF) — and this heat eventually disperses through the atmosphere. 



Heat is energy, and an El Niño shows up both as warming and as motion. Its reach extends into the tropical Atlantic, where it suppresses — yes, suppresses — hurricanes. Rain, often absent for years, falls in the ultradeserts of Peru and Argentina. And the global temperature warms. 



When the trade winds return, the cold upwelling reappears. This is La Niña. It stands to reason that the more the cold water is suppressed, the greater the amount that eventually bubbles up, so a big El Niño warm spike may mean a big temperature fall in the months thereafter. 





We’re totally confident that 1998’s big warming spike was a result of El Niño, and not dreaded “global warming” — that is, a human product. We know because the stratosphere tells us so. 



The human version of global warming is caused by increasing amounts of “greenhouse” gases in the lower atmosphere. These compounds absorb the heating radiation that results from the sun’s warming of the earth’s surface, and reemit that radiation either downward, resulting in additional atmospheric warming, or out to space. If these compounds weren’t there, the radiation would pass directly outward. 





So what we should see from the increasing greenhouse effect is a lowering trend for stratospheric temperature. And El Niño should temporarily stop that trend, at least for a year or so. Figure 3 shows that 1998 was indeed one of the warmest years in the stratosphere in the last two decades and is testimony to El Niño as the cause. 



**Greenorama**



We were besieged with news reports about how El Niño (and, by not‐​so‐​subtle extension, global warming) would cause terrible agricultural disasters. Who can forget the miles of CNN footage showing tractors mired in the Georgia mud, or network reels of browned corn in Texas? 



Well, some folks did poorly, and some folks did well. That happens every year. About the best way we know of to settle the overall score is in the wheat, corn, and soybean markets. When there’s a big supply, the price goes down. Demand fluctuates some too, but a perpetually increasing population has a way of ensuring more mouths to feed. 



By late 1998, the price of U.S. wheat stood, after adjusting for inflation, at its lowest level since reliable records began in 1915. Fluctuations in America’s massive supply of agricultural products, more than anything, dictate the global price. 





Many agricultural economists and a few climatologists have made careers of studying the influence of global weather patterns on crop yields. Moisture at planting time and in the winter before harvest is the major determinant — by far — of winter wheat yield. Winter wheat is planted in the early fall, requires moisture to germinate, and then, when spring springs, is really poised to take advantage of wet soil. In addition, yields are positively influenced by above‐​normal winter temperatures. 



Undoubtedly, the climate of 1998 led to the record yields. But there was another factor as well: Increased carbon dioxide in the air increases yields and makes crops much more efficient in their use of moisture. As Sylvan Wittwer, former head of the Board on Agriculture of the U.S. National Research Council wrote,“Overall, it has been conservatively estimated that global crop productivity has risen by approximately 2.5 to 10 percent, and possibly as high as 14 percent from the current increase in atmospheric CO2 over pre‐​industrial levels.” 



**We’ve Seen Fire and We’ve Seen Rain**



Two other prominent newsmakers this year included the spate of overland fires in Florida during the early summer, and Mitch, a real son‐​of‐​a‐​gun of a flood, but really not much of a hurricane by the time it hit land. 





That didn’t stop everyone from blaming all this on El Niño and global warming, but the fact is that, in general, there is no relationship between summer dryness in Florida and the existence of an El Niño during the previous winter. That’s because El Niño makes it rain during the winter greening season. In the summer, there’s precious little documentation that El Niño does anything at all to Florida weather. Of course, we could blame Florida’s high temperatures this year on global warming, but that would mean ignoring the fact that changing the greenhouse effect warms up the coldest air masses a lot more than it heats up the warmer tropical ones. 





That was inflammatory nonsense. A 1974 hurricane named Fifi, which was also a Category 2 at landfall, took much the same track and killed 7,500. Janet and Edith had much more powerful winds and wreaked tremendous havoc. Perhaps the most interesting comparative aspect with Mitch is that a tropical storm named Claudette, in 1979, also produced 50 inches of rain and resulted in nine deaths (that’s about 9,990 fewer than Mitch caused) when it hit Texas. Perhaps infrastructure and poverty, not global warming, created the tragedy named Mitch. Maybe, just maybe, allowing us to save our money for investment in developing nations like Honduras and El Salvador is a better idea than taking it away in an attempt to stop something that would happen anyway. 



El Niño and hurricanes do share at least one common trait. They have been around for a long time and the biota of the world, thanks to the nature of evolution, likely take advantage of them. In California, rains of the magnitude that associate with El Niño are required to make the desert bloom. Just any old storm isn’t enough, even though the ground gets wet. In that environment, many seeds have to be scarred by the motion of overland movement of water before they’ll even germinate. 





Long before banging the climate‐​disaster gong became the key to career advancement, federal climatologist George Cry calculated the percentage of normal rainfall that comes from all tropical cyclones, including tropical depressions, storms, and hurricanes in the eastern United States. Figure 7 shows the result for September. 



In most of the areas with high values, American agriculture has adopted a double‐​crop system that plants one early, fast‐​maturing crop, and then replaces it with an October harvest crop, mainly soybeans. Late August and September rain can be very important determinants of final yield. It’s pretty clear that years in which amounts are below normal because of lack of tropical cyclones are those in which yields are in jeopardy. 



People adapt to their climatic environment. The biota of the world take advantage of change, and so does our agriculture: One of the biggest El Niños in recent centuries produced a glut of food. That’s the lesson of 1998. 



But the climate hype of 1998 also has portents. If this past year is any guide, when global warming becomes a major focus of the Y2K presidential campaign, the amount of distortion, exaggeration, scare stories and fear‐​mongering we’re sure to witness will be a real climate disaster. 



**References:**



Cry, G.W., 1967. Effects of tropical cyclone rainfall on the distribution of precipitation over the eastern and southern United States. ESSA Professional Paper, 1, U.S. Dept. of Commerce. Washington, D.C. 



Michaels, P.J., 1979. Atmospheric anomalies and crop yields in North America. Ph.D. Dissertation. University of Wisconsin. 



Wittwer, S., 1995. Food Climate and Carbon Dioxide. CRC Press, Boca Raton, Fla., pp. 236.
"
"A graveyard of trees fixes into focus on the screen. The television footage zooms in on their burnt skeletons, thin and black. Dark ash is spread beneath their leafless arms. The camera then catches the peloton, a kaleidoscope of colour blurred against the barren landscape. The riders snake through the hillside; their movements are almost alien, otherworldly. It’s the Tour Down Under, at the opening of the 2020 UCI World Tour. Months earlier, across the Australian continent, wildfires raged. These fires spread their fury across the nation, reaching a devastating crescendo along the south-east coast. The red, black and grey ate up national parks, vineyards and farmland. It scorched the beautiful bush and damaged ancient Aboriginal cultural sites. With only a slight altering of circumstance, there might have been no Tour Down Under.  Cycling is a sport with no parallel. What other sport is played out across the world, traversing thousands of kilometres on countless roads, twisting and turning, up mountains, along rivers, next to the coastlines and through narrow towns? What other sport explores the edges of a nation, and races across its heart? It’s an ever-changing playing field. In pro cycling, only a thin line exists between the athlete and the natural world. It’s beautiful, it’s dramatic and it’s what makes cycling such a great sport. But it could also be its greatest threat. Heatwaves and unpredictable weather could easily stop a race in its tracks, changing outcomes, or even preventing any start. Though human-induced climate change may not be the sole cause of each bushfire, it certainly has increased their severity and destructiveness. While the weather has always been a key character in the world of cycling, what happens when that character becomes increasingly sinister? Professional cycling is a sport that has weathered many storms in its long history. The sport’s ongoing fight against the scourge of doping has been one of its greatest challenges. Yet, it is also arguably the case that not many other sports have had the same level of introspection, self-scrutiny and action on this front either. Cycling, in some ways, has faced up to the reality of doping and is actively working on solutions to it. The sport no longer views it as a scab that will simply heal itself; rather it is something that needs attention, action. The future and the integrity of the sport depends on it. A consequence of this has been the acknowledgment of doping’s existence and its impact in cycling. The same, however, cannot be said for the threat of climate change. It’s too easy to be passive in this regard. Admittedly sport does not have the power of a policy-making government. Professional cycling does not necessarily drive global economic investment. Rather than be a bystander, professional cycling and its constituents can be part of the solution. The future of the planet does not have to be marred in black ink; we do not need to be resigned to a desolate future. At its best, sport can be the expression of shared human experience, bringing us together. Professional sport – in its ability to display the possibility and artistry of the human body – is not an idle actor. It has currency and power. Policies and strategies, led by the governing body, need to be crafted in a manner that factors in the likelihood of worsening weather conditions due to climate change. The current extreme-weather protocols, enacted in 2016, are inadequate in the face of future climatic challenges. The protocols are not strategic, but rather reactionary, with no broader perspective outside of assessing individual weather events as they arise. Professional cycling could also take a more critical approach with sponsorship, considering what role a sponsoring entity may have in the context of climate change. These are not easy tasks, nor do they have straightforward or painless solutions. Professional cycling, almost exclusively, relies on sponsorship, and every year it seems as though teams are on the edge of collapse. Financial stability and sustainability are very real challenges for the sport. Unsurprisingly, money often speaks louder than the more complicated ethical considerations that may be attached to the cash. Yet with the predicted economic costs associated with climate change, perhaps a lack of cycling sponsorship could be another consequence of our warming planet. Plans for the sport’s future should be developed with this global reality factored in. Discussions on how pro cycling can be a part of the global movement to reduce emissions should take place. And now, as summer draws to an end and the peloton moves on from Australian leg of the World Tour, the fire and trauma for those on the front line remain. But perhaps, just maybe, a much-needed conversation will begin. Professional cycling’s future may depend on it."
"

In the grand scheme of things, the Keystone XL pipeline is of little significance to anything tangible — including gas prices, jobs, and, yes, the environment. The price of gasoline is for the most part determined by global forces in the oil market, of which the Keystone XL oil will be but a drop.



The pipeline’s job‐​creation potential is largely ephemeral; although the construction of the pipeline will create tens of thousands of jobs, the operation of it thereafter is expected to create fewer than 100 permanent jobs. And least significant of all is its impact on climate change. If it were to operate at full capacity for the next 85 years, the consumption of oil delivered by the pipeline would lead to global warming of less than a hundredth of a degree — an amount that is scientifically undetectable and environmentally inconsequential.





The fight over the Keystone XL pipeline is, and always has been, nothing but a symbol.



The fight over the Keystone XL pipeline is, and always has been, nothing but a symbol — of dedication to environmentalism, for the Left; of resistance to excessive government interference, for the Right. Huge amounts of time and money have been spent — or, more accurately, wasted — arguing fruitlessly that it is something more concrete. Practically speaking, its implications are tiny.



This is not true, however, of the litany of carbon‐​dioxide‐​limiting regulations that President Obama has imposed through the EPA. These onerous regulations try to force a reduction in demand through increasing the price of energy derived from fossil fuels (that is, coal, oil, natural gas). They will infiltrate the daily lives of each American, making everything more expensive and potentially threatening the reliability of America’s energy supply. And for what?



As far as the environment is concerned, the EPA’s meddling will have no demonstrable effect. Even a complete cessation of all greenhouse‐​gas emissions from the U.S., starting now and lasting forever, would avoid only a fraction of a degree of global temperature rise. At the local level, where people interact with the climate, natural variability would swamp any effect that U.S. emission reductions may have on the daily weather.



And for all the talk of an increasing intensity and frequency of extreme weather events — hurricanes, tornadoes, droughts, floods, heat waves, cold snaps, and any other manner of unpleasant weather — there is little actual scientific research that either identifies much of a trend, or unequivocally links such weather events to climate change, much less traces a direct link to carbon‐​dioxide emissions.



So, when it comes to challenging Obama on climate policy, congressional Republicans should set their sights on issues with tangible and wide‐​ranging impacts, and be willing to trade off largely symbolic projects like the Keystone XL pipeline.



Keystone XL has little to offer besides a moral victory. Limiting the economic damage that the EPA regulations may inflict, on the other hand, will be a benefit to all Americans, for years to come.
"
"A few weeks ago I was visiting a colleague in Brazil who told me he had a new post-doctoral researcher working for him from West Africa, but that he was in 21 days quarantine. I asked him if the newest member of his staff was in the university’s hospital; he replied “no”, he is wandering around the streets of the city until his 21 days are up – he is just not allowed on the campus.  As disease researchers know the great problem of the modern era is transport: since the times of our great-grandfathers human ability to traverse the planet has increased exponentially. And the risk of disease epidemics such as Ebola has followed. The present Ebola crisis appears, like HIV before it, to have started with the disease jumping from a wild species, in this case bats, to humans. Ebola and bats have been battling out an evolutionary war for thousands of years and have more or less come to a stalemate whereby bats are infected and the virus can reproduce itself, but bats are not killed.  A similar situation is seen in the case of simian retroviruses (SIV), the precursors of HIV.  Both come about from the arms race that occurs between a disease and its host: if lions start to run faster then so do their prey, otherwise the prey and ultimately the lion will go to extinction.  These wars between diseases and their hosts can be found everywhere on the planet. But what about the native people who live in forests? Surely they have been fighting this same evolutionary arms race with these same diseases. The answer is perhaps not. Studies of the Ache tribe of hunter gathers in Paraguay show that they do not hunt species indiscriminately. While their jungle home contains thousands of potential animal species to consume, they basically focus on eating only 12. The items on their menu are selected in terms of their energetic profitability; that is, the minimum amount of search time for the maximum amount of calories. In this case the favoured food item is the armadillo. Historically hunter gather tribes were small and widely dispersed. Thus, if they did get Ebola everyone might have died but there would have been little transmission to other groups of humans – and no epidemic. Agriculture changes everything, as large well-connected groups can easily transmit diseases. It is also worth remembering that diseases can jump the species barrier in both directions. About five years ago in the Brazilian city of Belo Horizonte all of the wild urban marmosets in one area died out due to cold sore infections. Cold sores are caused by a herpes virus. This outbreak probably started unintentionally when a person with a cold sore gave a fruit they were eating to some marmosets. Disease transmission is very much a two-way street and there is increasing evidence of human diseases passing on to wildlife, especially primates. Wild animals hunted and eaten in tropical forests are known as bushmeat – and bushmeat represents a crisis of its own, as hunting threatens to make many species of wildlife extinct. The crisis has its origins in poverty. People simply need to eat animals to survive, a situation that is made worse by deforestation and the fragmentation of natural habitats.   There is the often romanticised view of native peoples as conservationists since they are generally not thought to have made animal species go extinct. But this situation is more to do with their limited technology and small populations relative to their environment, rather than because native people live in an ecologically friendly manner. As their traditional forests are hit, and the easy pickings dry up, eventually the menu of such people will need to include new less energetically profitable food. And access to technology such as firearms can make previously unattainable prey available. Much of the modern bushmeat trade is no longer connected to native people needing to exploit wildlife as a food resource, but the descendants of these people who have developed a taste for the food. It is for this reason that several hundred tonnes of bushmeat enter Europe each year, where its illegality has made it a status symbol in some sections of society.  Part of the problem with this trade in bushmeat is that judges in the countries where the hunting takes place often, naïvely, believe the hunter’s pleas of poverty and just “smack them on the wrists”. Research in the north-east of Brazil has proven conclusively that hunting birds for food is much more expensive than buying chicken from the supermarket. Humans spent the past few thousand years breeding chickens, cows and pigs for a reason: they make a nicer, cheaper and less dangerous dinner than bats, gorillas or armadillos. Unfortunately, the threat of picking up a dreadful disease from bushmeat may not save these animals from extinction. A few years ago there was a yellow fever outbreak in Brazil and it was announced on the television that monkeys can be a host for this disease: this lead to the killing of wild urban primates in some cities. If humans continue to increase the items on their bushmeat menu then we can expect more diseases like Ebola and HIV to appear."
"
I’m pleased to announce that the www.surfacestations.org project has reached a major milestone, with 67% of the 1221 USHCN network now surveyed.
819 of 1221 stations have been examined in the USHCN network. Of the 819, 807 have been assigned a site quality rating. In some of those cases we’ve found the stations closed, or we are waiting for supplemental information to enable assigning a rating.
The  Google Earth map below shows current coverage. We are in sight of the goal. However there are still some holes, especially in south Texas, Alabama, Idaho, Arkansas, Missouri and Illinois.
See this Google Earth generated image. The circles with question marks are stations left to be surveyed.

Click for a larger image
A Google Earth USHCN Station Rating Map (KML file used to generate the above image) is available – download  here
You can download the Google Earth application for free from this link
Sincere thanks to Gary Boden for this  contribution! This is a very useful tool to help locate stations as hi resolution lat/lon values and descriptions are available from each map icon. Of course, Google Earth will also plot driving directions too.
I’m hoping to reach a minimum of 75% before I start doing data analysis. I want to find more rural stations, with the hope of finding more of the better sited stations since the lions share is comprised of CRN3-5 stations.  I’m hoping those of you that live near some of these “holes” can help. if you can, please leave a comment below and I’ll help you locate stations. You’ll also need to visit the website www.surfacestations.org and register as a volunteer. It’s free and easy.
Here is what the current rating breakdown looks like:

click for a larger image
For those unfamiliar with the rating system, it is identical to the one used by NOAA/NCDC to select sites for their new Climate Refernece Network (CRN) They drew this rating scheme from a paper published by Michel Leroy, of MeteoFrance, that he devised for their meteorological network. Here are the details:
Climate Reference Network Rating Guide – adopted  from NCDC Climate Reference Network Handbook, 2002, specifications  for siting (section 2.2.1) of NOAA’s new Climate Reference  Network:
Class 1 (CRN1)- Flat and horizontal ground surrounded by a  clear surface with a slope below 1/3 (<19deg). Grass/low vegetation ground  cover <10 centimeters high. Sensors located at least 100 meters from  artificial heating or reflecting surfaces, such as buildings, concrete surfaces,  and parking lots. Far from large bodies of water, except if it is representative  of the area, and then located at least 100 meters away. No shading when the sun  elevation >3 degrees.
Class 2 (CRN2) – Same as Class 1 with the  following differences. Surrounding Vegetation <25 centimeters. No artificial  heating sources within 30m. No shading for a sun elevation  >5deg.
Class 3 (CRN3) (error >=1C) – Same as Class 2, except no  artificial heating sources within 10 meters.
Class 4 (CRN4) (error >=  2C) – Artificial heating sources <10 meters.
Class 5 (CRN5) (error  >= 5C) – Temperature sensor located next to/above an artificial heating  source, such a building, roof top, parking lot, or concrete surface.”
Here is how the survey status breaks down by state. States highlighted have less than 50% coverage and are in the need of the most help from volunteers.




State
Number of Stations
Survey Report Done
Percent Reported


Alabama
15
8
53%


Arizona
26
21
81%


Arkansas
15
7
47%


California
54
54
100%


Colorado
25
17
68%


Connecticut
4
4
100%


Delaware
5
4
80%


Florida
22
21
95%


Georgia
23
20
87%


Idaho
26
17
65%


Illinois
36
13
36%


Indiana
36
33
92%


Iowa
23
13
57%


Kansas
32
27
84%


Kentucky
13
7
54%


Louisiana
18
17
94%


Maine
12
10
83%


Maryland
17
9
53%


Massachusetts
12
12
100%


Michigan
24
19
79%


Minnesota
33
30
91%


Mississippi
32
25
78%


Missouri
25
11
44%


Montana
44
27
61%


Nebraska
45
27
60%


Nevada
13
13
100%


New Hampshire
5
4
80%


New Jersey
12
8
67%


New Mexico
28
17
61%


New York
59
28
47%


North Carolina
29
26
90%


North Dakota
24
15
63%


Ohio
26
15
58%


Oklahoma
45
36
80%


Oregon
41
28
68%


Pennsylvania
24
11
46%


Rhode Island
3
3
100%


South Carolina
29
20
69%


South Dakota
24
11
46%


Tennessee
15
12
80%


Texas
48
24
50%


Utah
40
24
60%


Vermont
7
6
86%


Virginia
19
7
37%


Washington
44
35
80%


West Virginia
13
6
46%


Wisconsin
22
13
59%


Wyoming
33
26
79%



















For those that wish to help here is what you need to do:
1. Visit  www.surfacestations.org and register as a volunteer. It’s free and easy.
2. Look over the the How To Guide for surveying a station. All you need is a digital camera, and optionally a portable GPS, but it is not mandatory. A GPS that can get you to a lat/lon you enter is helpful though.
3. Find a station that is unsurveyed by using either the Google Earth KML file download above, or by looking for stations with no entries yet in the Surfacestation image gallery database 
When you decide on stations to survey, drop a comment here to make sure we don’t get duplication of effort.

4. Locate the details on station that you want to survey. The KML file has popup ballons for each station that gives details, and you can get lat/lon from doing a right click and “properties” for a station in Google Earth.Google Earth can give you driving directions. Note that lat/lon values are not alway accurate. I’ve seen them spot on, and sometimes they are as much as a 1/2 mile off., but they’ll generally get you close.
You can also visit the NCDC MMS database here:  http://mi3.ncdc.noaa.gov/mi3qry/login.cfm and use the “guest login” button. Then do a search for the station name and match up with the city and the USHCN station # ID  in the Google Earth KML file balloon. Getting that USHCN ID# right is crucial, as some towns may have 2 or three COOP stations which are not part of the USHCN network. Once you find the right station, click on the link. Be sure to note iuf it says “current” or not.
Another clue to make sure you have the right station in the NCDC database is the “station type” field which will say something like “COOP-A, COOP, LAND SURFACE, A, A” If there is no “A” in the description, then it is not a climate station.
Also check the “Location tab” in the NCDC database, which will say something like like “fire station” or “sewage treatment plant”…you maye have to look down a few entries from the top. Once you have that, some Google web searches will often help you narrow down a likely street address if the Google Earth imagery doesn’t help you visualize the location.
The “Equpiment tab” is also useful, since it will tell you what to look for. Here is a photo link that has most of the usual components of a climate station hat will help you get an idea.
5. If you determine that the station is located at a private residence, you’ll need help locating the observer. For that you need to find the observer name. Thankfully these exist on the NCDC database also, as a signature on many of the B91 forms the observers send in. To find B91 forms with observer names, go to this url:
http://www7.ncdc.noaa.gov/IPS/coop/coop.html
Then narrow down the state and station name in the web form, and click through to see what B91 forms are available, if you don’t see any within the last 6 -12 months, chances are the station is closed (a growing problem).
Download one and you may see an observer name at the lower right. A web lookup for the name and address may lead you there. Most private observers are interested and helpful. Just be sure that you advise them that you only want to get photos of their station and immediate surroundings (6 photos minimum: NSEW at about 20-30 feet, and two overall wide shots showing the station in relation to it’s surrounding) and that you are not going to reveal their names, addresses or phone numbers in any way, or any other private info.
6. Plan your trip. If you have trouble, or need help locating a station, drop a  comment here.
7. Set your camera for 3.1 megapixels (2048×1536) for best results. Or use a photo editor program later to shrink the images to that size if you use a higher resolution. High resolution is good for long distance shots, such as are sometimes required when the station is at a fenced public facility like a water plant. You can then later crop out areas of the hi-res image. It’s like having an extra zoom level. All images should be 2 megabytes or less in size for uploading.
8. Fill out the station survey form (available here ) as best you can, making notes about the station. be sure to save it as a Adobe Acrobat PDF file, which is what is need to upload into the database. A free print to PDF application is available here at www.primopdf.com should you need one.
9. Navigate to the empty folder for the station you surveyed at the Surfacestation image gallery database and click on “add a photo” or “add items” on the left menu. Don’t try to do them all at once, as you may get a time out if your connection speed is slow. Doing 4 at a time really works well.  Here at this link is what a completed survey looks like after uploading.
10. Drop us note at info { at } surfacestations dot org to let us know! Or if you need help.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e997cc081',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Boris Johnson’s ambition to have some big ego project to memorialise him in the manner of great politicians and engineering projects of Victorian Britain would be better served by not building HS2 or indeed the barmy bridge from Scotland to Northern Ireland. All those billions should be invested in major flood protection projects across the country (Flood alerts at new high as storm lashes UK, 17 February).  Storms Ciara and Dennis over the past two weekends have shown the need for much-improved flood defences and river management systems, a need which will become increasingly critical as the climate emergency worsens and such events regularly occur in ever greater measure. It is appalling to see the destruction of people’s homes, businesses and communities by flooding and storms. It is shocking to see the distress that people are suffering from the repetition of these inundations when they have barely recovered from earlier ones. Some are suffering serious and long-term mental health problems as a consequence. The government must get its priorities right. Otherwise, by the time HS2 is completed, the communities that it will connect to in the north will be semi-permanently underwater and uninhabitable.Paul FaupelSomersham, Cambridgeshire • As the UK is battered by yet another large and destructive storm and so many of us have to battle to save our homes from flooding (in many cases for the second time in a week) it is perplexing that Extinction Rebellion, the one organisation trying to raise awareness of the causes, is considered to be equivalent to a terrorist threat. In contrast, the recently elected government, which has little if any credibility in responding to the climate emergency, considers that repeating imaginary figures as to their plans for the NHS, transport infrastructure, etc is acceptable. Maybe following all the climate catastrophes over the past weeks, there will finally be an urgent political awakening and necessary action to curtail the burning of fossil fuels. An increasing number of ordinary people are now experiencing first-hand what the alternative is. And it is bleak.Prof Michael SymondsLoughborough, Leicestershire • Given that running cold water over a plate smeared with marmalade has little effect, what about using it as the filler/sealant between sandbags? May be more effective than some of the failed flood defence schemes. Pleased to report that the Bathampton Meadows floodplain on which the Tory Bath and North East Somerset council proposed to build a massive car park is doing its job – flooded.Tim DaviesBatheaston, Somerset • Join the debate – email guardian.letters@theguardian.com • Read more Guardian letters – click here to visit gu.com/letters • Do you have a photo you’d like to share with Guardian readers? Click here to upload it and we’ll publish the best submissions in the letters spread of our print edition"
"

WOW look at the SIZE of that seal! (photo added by Anthony, not NYT)
Bearing Up
By SARAH PALIN
Published: January 5, 2008,
Juneau, Alaska
ABOUT the closest most Americans will ever get to a polar bear are those cute, cuddly animated images that smiled at us while dancing around, pitching soft drinks on TV and movie screens this holiday season.
This is unfortunate, because polar bears are magnificent animals, not cartoon characters. They are worthy of our utmost efforts to protect them and their Arctic habitat. But adding polar bears to the nation’s list of endangered species, as some are now proposing, should not be part of those efforts.
To help ensure that polar bears are around for centuries to come, Alaska (about a fifth of the world’s 25,000 polar bears roam in and around the state) has conducted research and worked closely with the federal government to protect them. We have a ban on most hunting — only Alaska Native subsistence families can hunt polar bears — and measures to protect denning areas and prevent harassment of the bears. We are also participating in international efforts aimed at preserving polar bear populations worldwide.
This month, the secretary of the interior is expected to rule on whether polar bears should be listed under the Endangered Species Act. I strongly believe that adding them to the list is the wrong move at this time. My decision is based on a comprehensive review by state wildlife officials of scientific information from a broad range of climate, ice and polar bear experts.
The Center for Biological Diversity, an environmental group, has argued that global warming and the reduction of polar ice severely threatens the bears’ habitat and their existence. In fact, there is insufficient evidence that polar bears are in danger of becoming extinct within the foreseeable future — the trigger for protection under the Endangered Species Act. And there is no evidence that polar bears are being mismanaged through existing international agreements and the federal Marine Mammal Protection Act.
The state takes very seriously its job of protecting polar bears and their habitat and is well aware of the problems caused by climate change. But we know our efforts will take more than protecting what we have — we must also learn what we don’t know. That’s why state biologists are studying the health of polar bear populations and their habitat.
As a result of these efforts, polar bears are more numerous now than they were 40 years ago. The polar bear population in the southern Beaufort Sea off Alaska’s North Slope has been relatively stable for 20 years, according to a federal analysis.
We’re not against protecting plants and animals under the Endangered Species Act. Alaska has supported listings of other species, like the Aleutian Canada goose. The law worked as it should — under its protection the population of the geese rebounded so much that they were taken off the list of endangered and threatened species in 2001.
Listing the goose — then taking it off — was based on science. The possible listing of a healthy species like the polar bear would be based on uncertain modeling of possible effects. This is simply not justified.
What is justified is worldwide concern over the proven effects of climate change.
The Center for Biological Diversity, which petitioned for the polar bear to be protected, wants the listing to force the government to either stop or severely limit any public or private action that produces, or even allows, the production of greenhouse gases. But the Endangered Species Act is not the correct tool to address climate change — the act itself actually prohibits any consideration of broader issues.
Such limits should be adopted through an open process in which environmental issues are weighed against economic and social needs, and where scientists debate and present information that policy makers need to make the best decisions.
Americans should become involved in the issue of climate change by offering suggestions for constructive action to their state governments. But listing the polar bear as threatened is the wrong way to get to the right answer.

Sarah Palin, a Republican, is the governor of Alaska.
h/t to L Nettles


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d4159eb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The nearly decade‐​old Regional Greenhouse Gas Initiative (RGGI) was always meant to be a model for a national program to reduce power plant carbon dioxide (CO2) emissions. The Environmental Protection Agency (EPA) explicitly cited it in this fashion in its now‐​stayed Clean Power Plan. Although the RGGI is often called a “cap and trade” program, its effect is the same as a direct tax or fee on emissions because RGGI allowance costs are passed on from electric generators to distribution companies to consumers. More recently, an influential group of former cabinet officials, known as the “Climate Leadership Council,” has recommended a direct tax on CO2 emissions (Shultz and Summers 2017).



Positive RGGI program reviews have been from RGGI, Inc. (the program administrator) and the Acadia Center, which advocates for reduced emissions (see Stutt, Shattuck, and Kumar 2015). In this article, I investigate whether reported reductions in CO2 emissions from electric power plants, along with associated gains in health benefits and other claims, were actually achieved by the RGGI program. Based on my findings, any form of carbon tax is not the policy to accomplish emission reductions. The key results are:



• There were no added emissions reductions or associated health benefits from the RGGI program.



• Spending of RGGI revenue on energy efficiency, wind, solar power, and low-income fuel assistance had minimal impact.



• RGGI allowance costs added to already high regional electric bills. The combined pricing impact resulted in a 12 percent drop in goods production and a 34 percent drop in the production of energy-intensive goods. Comparison states increased goods production by 20 percent and lost only 5 percent of energy-intensive manufacturing. Power imports from other states increased from 8 percent to 17 percent.



The regional program shifted jobs to other states. A national carbon tax would shift jobs to other countries. A better policy to reduce CO2 emissions is to encourage innovation rather than rely on taxes and regulation. The United States has already reduced emissions 12 percent from 2005 to 2015, more than any other developed country with a large economy, mainly through innovations in natural gas drilling techniques. There are many other opportunities to invest in innovation, for example, improved solar photovoltaic cells, more efficient batteries, small modular nuclear reactors, and nascent technologies that use fossil fuels without emitting CO2.



Ten northeast states joined together to form the RGGI to require power plants with a capacity of more than 25 megawatts to buy emission allowances for each ton of CO2 emissions. The states included Connecticut, Delaware, Maine, Maryland, Massachusetts, New Hampshire, New Jersey, New York, Rhode Island, and Vermont. The allowances were sold in quarterly auctions beginning in 2008. The initial plan was to gradually reduce the number of allowances available to achieve a 10 percent emission reduction by 2018. New Jersey dropped out of the plan in 2011. In 2013, RGGI, Inc. announced plans for a 45 percent reduction in the number of allowances available in auctions beginning in 2014, with an additional 2.5 percent reduction each year until 2020 (Brown 2013: 1). Consequently, allowance prices began to rise, and RGGI states are now negotiating an extension to 2030, with an additional 30 percent reduction in allowable emissions.



The program is touted by RGGI, Inc. as a market-based system. However, the program applies a minimum reserve price and a Cost Cap Reserve that kicks in additional allowances if an annual price cap is exceeded (Figure 1). The proposed agreement for 2030 also includes an Emissions Containment Reserve whereby states can withhold allowances if auction prices fall below a set target price. A true market-based cap and trade program would allow the market to set the price. Allowance prices averaged about $3/ton from 2008 to 2013 ranging from about $2 to $4. In 2014, there was a dramatic cut in the number of available allowances that forced prices to a high of $7.50/ton in 2015, tracking the Cost Cap Reserve target. Prices began to fall after the Clean Power Plan implementation was stayed by the Supreme Court, and hit $2.53/ton in June, 2017, compared to a reserve price of $2.15. The extension targets a $13/ton price in 2021, and $24/ton in 2030. Speculators have made up roughly one-quarter of allowance purchases, trading with compliance entities in a secondary market.





According to Hibbard et al. (2011:15), in a report for the Analysis Group, “Within the electric system, the impacts of these initial (RGGI) auctions show up during the 2009–11 period, as power plant owners priced the value of CO2 allowances into prices they bid in regional wholesale prices.” A flow diagram in that report (p. 22) shows how the auction costs flow from the electric generators to the electric distributors, and on to consumers, the same as a direct tax or fee would do.



In order to claim success for RGGI, the first cap and trade program in the United States, we need to consider some related issues:



1\. Can the measured emission reductions be accounted for by non-RGGI causes?



2\. Can the impacts on the economy be clearly broken down into statistically confirmable independent (RGGI inputs) and dependent variables (real GDP, or electric price changes)?



3\. Can the RGGI revenue expenditures be shown to have been necessary and to have had significant impacts?



4\. Were energy efficiency project claimed savings rigorously tested by weather-adjusted “before and after” meter readings?



RGGI fails to answer these questions. Unfortunately, the data needed for a robust statistical analysis (question 2) are not readily available and obtaining them is beyond the scope of this article. The other three are noted in the text that follows.



The change in electricity demand, by necessity, must consider the interplay of real economic growth, the details of that growth, changes in population, the impact of pricing, and changes in energy efficiency. The RGGI program has an impact on these parameters.



It is difficult to compare electric prices from state to state because of significant regional differences in power cost. Also, at roughly the same time RGGI started, many states began requiring increased use of energy sources like wind and solar in their Renewable Portfolio Standard (RPS) laws and set energy efficiency requirements.



A further complication is that a number of states deregulated the supply portion of electric bills allowing market competition just prior to the start of the RGGI program. All the RGGI states deregulated. Fortunately, there is a comparison sample of five non-RGGI states (Illinois, Ohio, Oregon, Pennsylvania, and Texas) that deregulated electric supply in a manner similar to the RGGI states, and also had significant RPS requirements. Both RGGI and non-RGGI states have wide variation in their RPS programs, which adds uncertainty. Increasing wind and solar power raises electric rates because they are premium-priced power sources. For example, the increase in Delaware’s electric prices by 9 percent is directly related to the RPS, which shows up on consumers’ Delmarva Power electric bills. Likewise, Maryland electric bills have increased 14 percent for the same reason, according to a report from the Maryland Energy Administration (Tung 2017: 17).1



Non-RGGI comparison states actually added more wind and solar generation than RGGI states: adding 5.5 percentage points to generation compared to 2.3 percentage points in the RGGI states. Even removing the large wind farm construction effort in Texas from the calculation, the non-RGGI comparison states still outperformed the RGGI states: adding 3.4 percentage points compared to 2.3. The cost of wind and solar power has averaged two to three times the megawatt-hour rate compared to existing conventional fuel sources. The price impact should be greater in the non-RGGI states. Despite this disadvantage, the non-RGGI states still had lower overall price increases.



Several states that offered limited deregulation were not included in the comparison, and New Jersey is not included as an RGGI state because it dropped out of RGGI in 2011, and California is not included because it began a carbon tax just a few years ago. The results shown in Figure 2 cover the period from 2002 to 2015 to capture the impact of the four policies taken together (deregulation, RPS, energy efficiency, and RGGI).





To more accurately isolate the impact of RGGI between 2007 and 2015, the weighted average of total electric revenue for the multi-state groups is used in Table 1, and shows RGGI prices rose 64 percent more than comparison states. The increase was split between direct RGGI cost pass-though and indirect cost. Direct emission allowance cost was $436 million in 2015, about half the price differential between RGGI and comparison states. The rest of the difference may be due to indirect RGGI costs. For example, when power is imported to Delaware and Maryland from the PJM Regional Transmission Organization, there are premium charges for transmission distances, transmission congestion, and capacity. An earlier study, “Cost Impacts of 2013 RGGI Rule Changes in Delaware” (Stevenson and Stapleford 2016: 2), demonstrates that RGGI allowances directly added $11 million a year to Delaware electric bills, while the indirect costs added another $28.5 million.





Prices in RGGI states rose concurrent with more energy-intense manufacturing segments of the economy leaving the RGGI states with slower overall real economic growth based on Regional Real Chained GDP (Table 2). Linking real economic growth to RGGI alone is fraught with problems: real economic growth rates in RGGI states between 2007 and 2015 varied widely from a negative 7.1 percent for Connecticut to a plus 11.9 percent for Massachusetts. Can we realistically claim RGGI helped Massachusetts but hurt Connecticut at the same time?





The comparison states’ economies grew 2.4 times faster than the RGGI states. Data from the U.S. Bureau of Economic Analysis show that the RGGI states lost 34 percent of energy-intensive businesses (primary metals, food processing, paper products, petroleum refining, and chemicals); the comparison states lost only 5 percent. The RGGI states lost 12 percent of overall goods production, while the comparison states grew by over 20 percent. We see this impact show up in industrial electric demand with the RGGI states falling 18 percent, while non-RGGI comparison states fell only 4 percent (Table 3).





Consideration also needs to be given to energy efficiency improvements as shown by the improvement in energy intensity (Table 4). RGGI states improved by 9.6 percent, while non-RGGI comparison states improved 11.5 percent. (Energy intensity improves when it goes down.)





According to RGGI, Inc. (2016), RGGI states are investing the RGGI revenue in energy efficiency projects, suggesting RGGI states should be improving energy efficiency faster than other states. Based on gains in overall energy intensity, this claim appears to be false. An explanation for this disparity may be that the funds are not going to energy efficiency, or that the energy efficiency projects may not be working well. Both effects are seen in Delaware where 35 percent of allowance revenue is assigned to the Department of Natural Resources & Environmental Control (DNREC), and the rest flows through a private, nonprofit organization known as the Sustainable Energy Utility (SEU). Delaware has received $100 million in RGGI revenue: $55 million remains unspent and another $22 million has gone to administrative overhead and fuel assistance, with just $23 million (23 percent) going for energy efficiency projects.2



The Maryland Energy Administration (2016) reported that only 25 percent of RGGI revenue was allocated to grants for energy efficiency projects, and that doesn’t take into account any money from the grants used for administration by the grantees.



Could the energy efficiency and renewable energy projects have been completed without the RGGI grants? The Maryland 2016 report, Appendix B, lists hundreds of projects receiving grants. Most of the renewable energy grants went to individuals or companies to install solar photovoltaic cells. The grants were small, running from $700 to $1,000 for residential systems that typically cost about $20,000. Solar projects receive federal tax credits, and the owners can sell renewable energy production credits to utilities that are required to buy them by state law, and receive full credit for every kilowatt-hour of energy produced from the local utility. Using a proprietary spreadsheet program, I find that the internal rate of return of a residential system falls from 10.6 percent with the state grant to 9.2 percent without the grant.3 Most of the projects would move forward without the RGGI revenue grants.



In a report for the Delaware Department of Natural Resources and Environmental Control, Small (2012: 3) found that the federally financed “Weatherization Assistance Program,” which receives 10 percent of RGGI revenue, was shut down for two years while all existing projects were reviewed and redone as needed after a federal audit found various quality control issues. This shows how state evaluation, measurement, and verification measures are not working.



The most rigorous test for energy efficiency projects is to check weather-adjusted meter readings before and after the project is implemented. I have found only one large-scale study by Alberini, Gans, and Towe (2013) that did this. The authors found Maryland homeowners who replaced their heat pumps with no incentives saved an average of 16 percent on electric usage. Meanwhile, homeowners receiving cash incentives of $300, $450, and $1,000 or more had energy savings of 6.2, 5.5, and 0 percent, respectively. The authors concluded on page 7 that “the survey responses provide suggestive evidence the ‘rebaters’ were disproportionately replacing ‘inadequate’ units, leading us to conjecture that the rebates are being used to defray the cost of more powerful units, or of units that end up being used more.”



Table 5 shows predicted changes in electricity demand in the RGGI states based on the 2007 demand adjusted for economic growth (7.2 percent from Table 4), population change (1 percent from U.S. Census data), loss of goods production (−12 percent from Table 2), and efficiency improvements (−9.6 percent from Table 4). The actual demand fell 11 million megawatt-hours, close to the projected 14 million.





Emissions were reduced about 40 percent from 2007 to 2015 from electric generating units in the RGGI states (Table 6). That compares to only about a 20 percent reduction in emissions for the country as a whole and the comparison states, suggesting RGGI has been a suc⁠cess. As raw percentages, this would be true, but the base emissions of the RGGI states are much lower than the total for the country, so a relatively small change can appear as a relatively large percent.





Table 7 shows high CO2 emission coal-fired generation _drops_ 16 percentage points in both RGGI and non-RGGI comparison states, and natural gas _rises_ virtually the same amount (10 for RGGI states versus 9 for non-RGGI states).





The non-RGGI comparison states actually added more wind and solar generation than the RGGI states (5.5 percentage points versus 2.3), even after allowing for a very large wind farm proliferation in Texas. Some RGGI auction revenue was invested in solar energy projects, but the RGGI, Inc. (2016) report identifies less than 100 MW of added solar capacity, which would account for only about 1 percent of the total wind and solar capacity added in the RGGI states according to generation data in the U.S. EIA _Electric Power Monthly_ .



Another way to sort out the impacts of the RGGI program on emissions reductions is to review regulatory and market impacts to the generation mix and emissions in detail. The impacts of exporting emissions through the increased importing of power must also be considered. If a comparison is made of the estimate of emission reductions using just factors common in all states, the comparison should isolate the impact of the RGGI program. The result of this comparison is discussed below and shows RGGI had no impact on emissions.



Delaware provides an early example of exporting emissions that can be found in a number of articles published in the _Wilmington News Journal_ beginning in January 2008. On December 17, 2008, Delaware participated in its first regional cap and trade auction. Three weeks later the Valero-owned Delaware City Refinery announced the shut-down of its electric generation at the plant. According to RGGI, Inc. (2009), CO2 emissions from the plant’s electric generation facility accounted for 17 percent of Delaware’s initial emission allocation. Valero had been gasifying petroleum coke, a waste product from the refinery, to fuel the power plant. Petroleum coke has emission rates similar to coal, but by gasifying it Valero reduced emissions of other air pollutants. So, three weeks into the RGGI program Delaware met its total 10 percent RGGI reduction goal. That isn’t the end of the story. Valero sold the facility to PBF Energy. PBF restarted portions of the power plant fueled with conventional natural gas. The petroleum coke was loaded onto ships and sent to China to be burned directly for electric generation without pollution controls.



The RGGI states export CO2 when they increase the import of electricity from other states. Between 2007 and 2015, the RGGI states doubled their imports (Table 8). Much of the imported power comes from the PJM transmission region. Adjusting for this factor decreases the RGGI state emissions reductions about 11 million tons.





CO2 emissions are down across the country. A number of major EPA regulations have been implemented since 2009. Electric power plants have seen the most impact from regulation including the Mercury & Air Toxics Standard (MATS), the Cross State Air Pollution Rule (CSAPR), the Carbon Pollution Standard for New Power Plants that established New Source Performance Standards (NSPS), and the Clean Power Plan (CPP), all aimed at reducing the use of coal and forcing the closure of older, smaller power plants that were not worth upgrading with expensive new filtration equipment, given the low cost of natural gas.



The question is how much of the improvement in power plant emission reduction was caused by EPA regulations. As shown in Figure 3, nominal natural gas prices dropped significantly starting about 2009, driven by an increase in supply from the deployment of hydraulic fracturing and horizontal well drilling technology in shale formations. The types of coal used for electric generation have no other significant uses, and price tends to be stable because electric demand does not vary much from year to year. Natural gas has a number of high volume uses, such as for industrial feedstock and as a primary fuel for heating. Heating demand can vary significantly from year to year. For example, very cold temperatures in the winter of 2014 caused a spike in demand and price. Lower overall natural gas prices played a major role in the switch from coal to natural gas for electric generation starting in 2009, and regulations impacted generation capacity starting in 2012.





Total electric generation was relatively constant since 2003, but increased almost 3 percent from 2009 to 2016 as the economy recovered from the recession (Figure 4). That increase in demand was met with wind and solar power growth driven by state Renewable Portfolio Standards along with federal and state subsidies. Coal-fired generation was relatively constant until 2008, but began to fall in 2009. The fall paralleled declining natural gas prices. Natural gas generation has been increasing at a relatively constant rate.





EPA regulations did impact coal-fired generation capacity as shown in Figure 5. The downturn in coal capacity coincides with new regulation implementation beginning in 2012. Lower natural gas prices obviously influenced the decisions to close down the coal-fired generation.





However, more important to coal-fired generation was the change in the capacity factor, that is, how often power plants ran in comparison to natural gas-fired power plants (Figure 6). The decline tracks the falling natural gas price curve that began in 2009.





With some certainty nationally, coal plant capacity reductions were caused by EPA regulations, and output reductions were caused by falling nominal natural gas prices. The impact of the two trends can be parsed. The computational details are provided in Stevenson (2017: 12). The result, both nationally and for the RGGI states, is an identical 28 percent from lost generation capacity, and 72 percent from lower natural gas prices. If the RGGI allowance program had a significant impact, it would have offset some of the impact of lower natural gas prices, because the allowance cost acts as an additional variable production cost, and would have shifted the ratio, but it didn’t. This result is not unexpected as RGGI allowance revenue only averaged 0.6 percent of electric revenue between 2007 and 2015 ($0.3 billion/$51 billion).



To complete the estimate of emissions from common factors, the changes in natural gas-fired and petroleum-fired generation need to be added. Table 9 shows that the total net estimated reduction in emissions for RGGI states, due to factors common to all states, was 59.7 million metric tons. That figure is slightly higher than the actual reduction of 57.2 million metric tons, which suggests that the actual reduction is accounted for without any significant additional contribution from the RGGI program.





According to RGGI, Inc. (2016), in its report titled _The Investment of RGGI Proceeds through 2014_ , 15 percent of RGGI revenue ($178.2 million) went to direct low-income electric bill assistance to 2.6 million households from the beginning of the RGGI auctions in 2008 through 2014. The RGGI funds, about $30 million a year, were added to the federal Low Income Home Energy Assistance Program (LIHEAP). According to the U.S. Department of Health and Human Services (2014: 10–11), the federal government provided $795 million to RGGI states in 2014. Thus, RGGI added less than 4 percent to LIHEAP ($30 million annual RGGI contribution/$795 million federal contribution).



RGGI allowance revenue totaled $1.8 billion through 2014. The allowance program added $0.85/megawatt-hour to electric bills between 2008 and 2014 ($294 million a year/348 million megawatt-hours demand a year). RGGI state residential electric demand has been fairly flat, and averaged 130.9 million megawatt-hours/year. According to the U.S. Census Bureau (2010), there were 17.3 million households in the RGGI states. Thus, residential electric demand averaged 7.6 megawatt-hours per year (130.9/17.3). The total cost of RGGI equaled $6.50/household ($0.85 × 7.6). This reduces the net contribution to low-income households to $5/year ($11.50−$6.50). Therefore, the net RGGI contribution to the federal LIHEAP was only 1.6 percent, an insignificant amount.



In this article, I investigate claims by the Acadia Center (Stutt, Shattuck, and Kumar 2015: 6) and RGGI, Inc. (2016) that the RGGI program has generated significant benefits. Using data from five comparison states with similar overall electricity policies, except for RGGI, along with looking at national trends, I find the RGGI, Inc. and Acadia Center claims to be misleading.



The Acadia Center claims that compared to other states RGGI states increased electric prices by half as much, had 3.6 percent more economic growth, and reduced emissions 16 percent more leading to greater health benefits from pollution reduction. In reality, from 2007 to 2015, net weighted average nominal electricity prices rose 4.6 percent in RGGI states compared to 2.8 percent in comparison states. Linking real economic growth to RGGI alone is fraught with problems. Real economic growth rates in RGGI states between 2007 and 2015 varied widely from a negative 7.1 percent for Connecticut to a plus 11.9 percent for Massachusetts. Also average RGGI revenue amounted to only 0.01 percent of the combined average real GDP of the RGGI states, so one wouldn’t expect much impact. Ignoring those difficulties, real economic growth was 2.4 times faster in comparison states than in the RGGI states. High RGGI state electric rates led to a 34 percent reduction in energy-intensive industries and a 12 percent drop in the goods production sector, while comparison states saw only a 5 percent drop in energy-intensive industries and a 20 percent gain in goods production.



This article finds there were no added reductions in CO2 emissions, or associated health benefits, from the RGGI program. RGGI emission reductions are consistent with national trend changes caused by new EPA power plant regulations and lower natural gas prices. The comparison requires adjusting for increases in the amount of power imported by the RGGI states, reduced economic growth in RGGI states, and loss of energy-intensive industries in the RGGI states from high electric rates.



The RGGI, Inc. report focuses on the impacts of spending the allowance revenue and suggests significant gains in energy efficiency, wind and solar investments, and assistance with low-income energy bills. Noticeably, RGGI, Inc. does not make claims of superior emission reductions or lower power prices. In reality, the spending of the allowance revenue had marginal impacts. All states have shown energy efficiency gains. The RGGI states saw a lower improvement in energy intensity at 9.6 percent compared to 11.5 percent for comparison states, so there appears to be no RGGI-related gain in overall energy efficiency. Wind and solar energy installation was slower in RGGI states, increasing by only 2.3 percentage points, while comparison states grew by 5.5 percentage points, more than twice as fast. RGGI grants for wind and solar power accounted for only about 1 percent of all the wind and solar power added by the RGGI states. The net fuel assistance help for low-income households, 15 percent of all households, added only 1.6 percent to the federal Low Income Home Energy Assistance Program, or less than $5/year. RGGI had no meaningful impact on lower-income families. Meanwhile, the other 85 percent of households saw an increase in electricity cost of $6.50/year directly caused by the RGGI allowance cost.



Alberini, A.; Gans, W.; and Towe, C. A. (2013) “Freeriding, Upsizing, and Energy Efficiency Incentives in Maryland Homes.” Fondazione Eni Enrico Mattei Working Paper No. 82. Available at https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2336044.



Brown, J. (2013) “RGGI States Propose Lowering Regional CO2 Emissions Cap 45%, Implementing a More Flexible Cost-Control Mechanism.” RGGI, Inc. Available at www.rggi.org/docs/PressReleases/PR130207_ModelRule.pdf.



European Commission (2016) “CO2 Time Series 1990–2015 per Region/Country.” _Emission Database for Global Atmospheric Research (EDGAR)_ , release version 4.3.2. Joint Research Centre (JRC)/PBL Netherlands Environmental Assessment Agency.



Hibbard, P. J.; Tierney, S. F.; Okie, A. M.; and Darling, P. G. (2011) _The Economic Impacts of the Regional Greenhouse Gas Initiative on Ten Northeast and Mid-Atlantic States._ Boston: Analysis Group.



Maryland Energy Administration (2016) _Maryland Strategic Investment Fund Report on Fund Activities FY2015._ Available at http://energy.maryland.gov/Reports/FY15_SEIF_Annual_Report.pdf.



PJM/EIS (2016) “Generation Attributes Tracking System.” _System Mix 2015_ . Available at https://gats.pjm-eis.com/GATS2/PublicReports/PJMSystemMix/FilterPJM.



RGGI, Inc. (2009) “RGGI Historical Emissions 2000 to 2008 Spreadsheet.” Available at http://www.rggi.org/historical_emissions.



______ (2016) “The Investment of RGGI Proceeds through 2014.” Available at www.rggi.org/docs/ProceedsReport/RGGI_Proceeds_Report_2014.pdf.



______ (2017) “Auction Results for Auctions 1–36.” Available at www.rggi.org/market/co2_auctions/results.



Shultz, G. P., and Summers, L. (2017) “This Is One Climate Solution That’s Best for the Environment and for Business.” _Washington Post_ (June 19).



Small, D. (2012) _Annual Report on the Weatherization Assistance Program_ . Wilmington, Del.: Delaware Department of Natural Resources & Environmental Control.



Stevenson, D. T. (2017) “Sorting Root Causes of Air Quality Improvements 2009 to 2015.” Caesar Rodney Institute Policy Paper (January). Available at https://criblog.wordpress.com/2017/02/12/sorting-root-causes-of-air-quality-improvements-2009-to-2015.



Stevenson, D. T., and Stapleford, J. E. (2016) “Cost Impacts of 2013 RGGI Rule Changes in Delaware.” Caesar Rodney Institute Policy Paper (August). Available at www.caesarrodney.org/pdfs/Cost_Impacts_of_2013_RGGI_Rule_Changes_in_Delaware.pdf.



Stutt, J.; Shattuck, P.; and Kumar, V. (2015) _Regional Greenhouse Gas Initiative Status Report, Part 1: Measuring Success._ Boston: Acadia Center (July).



Tung, M. B. (2017) “RPS Requirement and Aggregated Cost of RPS.” Presentation to the House Economic Matters Committee, Maryland Energy Administration, Baltimore, Md. (January 15). Available at http://news.maryland.gov/mea/wp-content/uploads/sites/15/2017/01/EconomicMattersPresentation1.23.2017.pdf.



U.S. Bureau of Economic Analysis (n.d.) “Real Chained GDP by State.” Interactive Tables available at www.bea.gov.



U.S. Census Bureau (2010) “Population and Housing Unit Estimate Tables by Year.” Available at www.census.gov/programs-surveys/popest/data/tables.2010.html.



U.S. Department of Health and Human Services (2014) _LIHEAP Report to Congress for Fiscal Year 2014._ Division of Energy Services. Washington: DHHS.



U.S. Energy Information Agency (EIA) (2016a) Detailed State Electricity Data: “Average Price by State by Provider (EIA-861)”; “Revenue from Retail Sales of Electricity by State by Sector by Provider (EIA-861)”; “Retail Sales of Electricity by State by Sector by Provider (EIA-861)”; “Net Generation by State by Type of Producer by Energy Source (EIA-906, EIA-920, and EIA-923)”; “U.S. Electric Power Industry Estimated Emissions by State (EIA-767, EIA-906, EIA-920, and EIA-923).” Available at www.eia.gov/electricity/data/state.



______ (2016b) _Electric Power Monthly_ (2007 and 2015): “Table 1.4.B Coal by State by Sector, Year-to-Date”; “Table 15B Petroleum Liquids by State by Sector, Year-to-Date”; “Table 1.7.B Natural Gas by State by Sector, Year-to-Date”; “Table 1.9.B Nuclear Energy by State by Sector, Year-to-Date”; “Table 1.10B Hydroelectric (Conventional) Power by State by Sector, Year-to-Date”; “Table 1.14.B Wind by State by Sector, Year-to-Date”; “Table 1.17.B Solar Photovoltaic by State by Sector, Year-to-Date”; Table 4.2 Total Average Cost of Fossil Fuels for Electric Utilities”; “Table 6.7A Capacity Factors for Utility Scale Generators Using Fossil Fuels.” Available at www.eia.gov/electricity/monthly.



1I use 2007 as the base year through 2015, unless otherwise noted. The reason for using 2007 is that RGGI auctions began in 2008, which was also the first year of the Great Recession.



2Calculation is based on information provided in an unpublished e-mail to a state senator of how DNREC spent RGGI allowance funds from 2014 to 2016, and from SEU Annual Reports (available at www.energizedelaware.org/sustainable-energy).



3I assume a 7,500 watt system @$2.85/watt cost with 20 year life, 9,000 KWh first-year generation reduced 0.5 percent per year, $0.1425/KWh electric rate rising 2 percent per year, $6 SREC value, and 30 percent federal investment tax credit.
"
"

 _Global Science Report_ _is a weekly feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
Come the cold season, whenever there is some type of strong storm system near the U.S. Eastern Seaboard—be it a Nor’easter, a blizzard, or ex-hurricane Sandy—you don’t have to look very hard to find someone who will tell you that this weather is “consistent with” expectations of climate change resulting from human greenhouse gas emissions. The worse the storm, the more “consistent” it becomes.   
  
The complete collection of climate science describes just how complex the physical processes are governing such storm systems. Teasing out any anthropogenic influence, including even the direction of any influence, is darn near impossible. Claims to the contrary are usually based on a highly selective assessment of the science or the data.   
  
A case in point:   
  
The latest _en vogue_ explanation linking human greenhouse gas emissions to strong winter-season East Coast storms involves changes in the characteristics of the jet stream—a river of fast moving air in the atmosphere that influences both the strength and the forward speed of extratropical storm systems. A prominent (in the media, anyway) research study last year by Rutgers’s Jennifer Francis and University of Wisconsin’s Stephen Vavrus suggests that the declining temperature difference between the Arctic and the lower latitudes (adding greenhouse gases into the atmosphere warms colder, drier regions more so than warmer, wetter ones—with the notable exception of Antarctica) has led to changes in the jet stream which result in slower moving, and potentially stronger East Coast winter storm systems.   




Just Google “Jennifer Francis global warming” to see how this mechanism is supposedly tied to all sorts of extreme weather events.   
  
Even before the Francis and Vavrus study made it to print, we noted that their findings ran afoul of other existing literature which painted a far murkier picture of the influence (if any) that anthropogenic global warming was having in extratropical cold-season storm systems. After reviewing the literature, we cautioned:   




So where does this leave us? When the new paper by Francis and Vavrus comes to the attention of the mainstream press, it’ll play as if a warming Arctic and declining sea ice—an asserted consequence of human greenhouse gas emissions—has been definitively tied-in to all sorts of weather extremes across the U.S. No mention will be made to the fact that other research, which is many cases is more robust and detailed, has concluded nearly the opposite.



Conflicting research findings continue to be published.   
  
One was published several months ago in _Geophysical Research Letters_ by James Screen and Ian Simmonds, who looked for changes in jet stream characteristics using a different methodology than that of Francis and Vavrus. A robust signal should be apparent no matter how you look at it (within reason). But Screen and Simmonds found few statistically robust changes and what changes they did find were contrasting depending on the methodology they used. They noted that the changes they found were much smaller (and non-significant) than the large (and significant) changes reported by Francis and Vavrus. Screen and Simmonds concluded that their findings held “different and complex possible implications for midlatitude weather, and we encourage further work to better understand these.”   
  
In other words, the picture is far less clear than that described by Francis and Vavrus.   
  
This point is further driven home by a another paper just accepted in _Geophysical Research Letters_ by Colorado State University’s Elizabeth Barnes. Barnes, too, examined the relative warming in the Arctic and its possible link to extreme weather events in the Northern Hemisphere mid-latitudes. In a nutshell, she found little if any definitive relationship—again in contrast to the Francis and Vavrus results. Barnes discussed this discrepancy directly:   




We conclude that the mechanism put forth by previous studies (e.g. Francis and Vavrus [FV12]; Liu et al. [2012]), that amplified polar warming has led to the increased occurrence of slow-moving weather patterns and blocking episodes, is unsupported by the observations….A recent study by Screen and Simmonds [2013] also provides evidence that the trends in planetary waves suggested by FV12 may be an artifact of their methodology… The Arctic is changing rapidly, and these changes will likely have profound effects on the Northern Hemisphere. This study, however, highlights that the relationship between Arctic Amplification and midlatitude weather is complex.



The more folks look the less robust the popular global-warming-is-leading-to-more-extreme-winter-storms finding by Jennifer Francis and Stephen Vavrus seems to be.   
  
It’ll be interesting to see during this upcoming winter season how often the press—which seems intent on seeking to relate all bad weather events to anthropogenic global warming—turns to the Francis and Vavrus explanation of winter weather events, and whether or not the growing body of new and conflicting science is ever brought up.   
  
If you don’t see it in the morning paper, you will most certainly find it here!   
  
**References:**   
  
Barnes, E., 2013. Revisiting the evidence linking Arctic Amplification to extreme weather in the midlatitude. _Geophysical Research Letters_ , in press, doi: 10.1002/grl.50880.   
  
Francis, J. A. and S. J. Vavrus, 2012: Evidence linking Arctic amplification to extreme weather in mid-latitudes. _Geophysical Research Letters_ , **39 (6)** , L06 801, doi:10.1029/2012GL051000.   
  
Screen, J. and I. Simmonds, 2013: Exploring links between Arctic amplification and midlatitude weather. _Geophysical Research Letters_ , **40** , 1–6, doi:10.1002/GRL.50174.   
  
  
  
  
  



"
"Alok Sharma, the former international development secretary, is the surprise choice to take on the role of president of the crunch UN climate talks to be hosted by the UK this November. He has also been made business secretary as part of Boris Johnson’s cabinet reshuffle.  Sharma has garnered praise from campaigners for his role at the Department for International Development, but will face an uphill task after nearly two weeks of trouble surrounding the post of Cop26 president. The former energy minister Claire O’Neill was abruptly sacked from the role and unleashed a vitriolic attack on Johnson, while several other high-profile figures including David Cameron and William Hague turned down the role. Sharma has a mixed record on voting on green issues in parliament. The Guardian’s Polluters project scored MPs on how they swung on a range of key votes. Sharma scored only 15%, a poor showing, as he was present for 13 votes affecting climate and environmental issues, but voted positively on only two of them. Since 2010 he has been MP for Reading West, to the west of London and just over 20 miles from Heathrow, and he has both opposed and publicly favoured Heathrow expansion. TheyWorkForYou, which rates MPs on their voting records, found he “generally voted against measures to prevent climate change”. In the record of MPs’ interests, he has received a donation of £15,000 from Offshore Group Newcastle, which makes platforms for oil, gas and wind energy companies. However, he has used his role at DfID to promote action on the climate crisis, by assisting developing countries to improve their resilience to the impacts of extreme weather, and tackling issues such as deforestation and clean energy. Last October, he urged the World Bank to focus more of its funding on the climate crisis. Mohamed Adow, director of energy and climate at thinktank Power Shift Africa, said: “It’s a relief to finally have a Cop president in post. But now the hard work must start. For such a crucial summit it’s worrying that Alok Sharma takes up the reins with only nine months to go. He will need all the resources of government and the diplomatic service to ensure the UK Cop is not a failure. “This is the UK’s first real test post-Brexit and so far Britain has not looked like a serious player on the global stage. The eyes of the world are watching and the UK’s Commonwealth allies in Africa and around the world will be demanding an outcome that sees those of us on the front lines of the climate crisis protected.” Sharma will be expected to combine the Cop26 role with the job of business secretary, one of the key roles in cabinet, especially as the government wrestles with Brexit. Rachel Kennerley, climate campaigner at Friends of the Earth, asked whether they were compatible. “Can Alok Sharma serve as both business secretary and president of the UK’s biggest global climate summit? The presidency isn’t like a student picking up a few extra bar shifts, it’s about leading the world’s climate ambition during a crucial time for the environment,” she told the Guardian. “We cannot afford a part-time president.”  Kat Kramer, global climate lead at the charity Christian Aid, said Sharma faced a “grave and delicate” task but could take advantage of his business secretary role to ensure the UK presented itself as a credible leader in the climate fight, through a national plan of action for reaching net zero emissions. She said: “It would have been a big task had Alok Sharma been in post from the beginning, rather than coming in late in the process. It’s now vital [he] work very closely with the backing of the prime minister to both get other countries to commit to new pledges to tackle the climate crisis but also put the UK’s own house in order and enact policies to accelerate UK decarbonisation. As secretary of state for business, energy and industrial strategy, Sharma will be well placed to oversee this.” Several ministers had been mooted for the role, including Michael Gove, who told a conference on Tuesday there were “many, many, many, many” people who would do better as Cop26 president, such as Zac Goldsmith and Kwasi Kwarteng, the clean energy minister. Gove hesitated when asked what would make the summit a success, before answering that success would be for countries to “accept the need to change and that leads to irreversible, accelerating and inclusive action” on the climate crisis. That falls well short of requiring governments to come forward with concrete new plans on cutting carbon in line with the goals of the 2015 Paris agreement, which is what activists and supportive countries such as the EU are hoping for. The job was turned down by former prime minister Cameron, who is said to have been too busy, and former foreign secretary Hague, who is believed to have had concerns about the role. Their reluctance suggests a perceived low standing for the role, which will not help the new incumbent. Previous Cop summits have tended to be led by the host government’s most prominent minister with a relevant portfolio, usually the environment minister.  France signalled its determination to forge a new global agreement in Paris by appointing Laurent Fabius, then foreign secretary, who led a tireless round of visits to foreign capitals in his military jet, while also getting other major figures such as Ségolène Royal closely involved in the intensive diplomacy. The then French president, François Hollande, also played a leading role. If Cop26 is to be a success – and so far the government has promised success but set a low bar for what that might look like – then Sharma will need to be able to call on the international firepower of the whole government, and Johnson will have to shake up every major department to come forward with domestic plans to prove the UK is on the road to net zero."
"

What’s worse than a public policy debate that turns bitter and impolite? Well, for one, having the courts step into the marketplace of ideas to judge which side of a debate has the best “facts.”



Yet that’s what Michael Mann has invited the D.C. court system to do. In response to some scathing criticism of his methodologies and an allegation of scientific misconduct, the author of the infamous “hockey stick” models of global warming — because they resemble the shape of a hockey stick, with temperatures rising drastically beginning in the 1900s — has taken the global climate change debate to a record low by suing the Competitive Enterprise Institute, _National Review_ , and two individual commentators. The good Dr. Mann claims that some blogposts alleging his work to be “fraudulent” and “intellectually bogus” were libelous. (For more background on the matter, see this excellent summary by _NR_ ’s editor Rich Lowry; linking to that post is partly what led Mann to target CEI.)





Public figures must not be allowed to use the courts to muzzle their critics.



The D.C. trial court rejected the defendants’ motion to dismiss this lawsuit, holding that their criticism could be taken as a provably false assertion of fact because the EPA, among other bodies, have approved of Mann’s methodologies. In essence, the court seems to cite a consensus as a means of censoring a minority view. The defendants appealed to the D.C. Court of Appeals (the highest court in the District of Columbia).



Cato has now filed a brief, joined by three other think tanks, in which we urge the court to stay out of the business of refereeing scientific debates. (And if you liked our “truthiness” brief, you’ll enjoy this one.)



We argue that the First Amendment demands that failing to leave room for the marketplace of ideas to operate stifles academic and scientific progress, and that judges are ill‐​suited to officiate policy disputes — as history has shown time and again. The lower court clearly got it wrong here — and there are numerous cases where courts have more judiciously treated similarly harsh assertions for what they really are: expressions of disagreement on public policy that, even if hyperbolic, are among the forms of speech most deserving of constitutional protection.



The point in this appeal is that courts should not be coming up with new terms like “scientific fraud” to squeeze debate over issues impacting government policy into ordinary tort law. Dr. Mann is not like a corner butcher falsely accused of putting his thumb on the scale or mixing horsemeat into the ground beef. He is a vocal leader in a school of scientific thought that has had major impact on government policies.



Public figures must not be allowed to use the courts to muzzle their critics. Instead, as the U.S. Supreme Court has repeatedly taught, open public debate resolves these sorts of disputes. The court here should let that debate continue outside the judicial system.
"
"
Share this...FacebookTwitterThey all admit the IPCC is flawed and in need of an overhaul, yet they still insist the science is correct. Go figure. That’s pure rubbish, of course. Bad process = bad product. 
It sounds just like the National Academy of Sciences reaching the conclusion that Michael Mann’s hockey stick science had no value, but his answer was still correct.Here are the excerpts from some of the leading online papers in Germany, Austria and Switzerland. (I’ve also added some UK links below, h/t: http://thegwpf.org/
 SÜDDEUTSCHE  ZEITUNG in Germany (a favorite of Stefan Rahmstorf) writes:
Obligation to be more open
An examination of the IPCC reveals: The IPCC has to change the way it works. Yet, there’s no basis for the foaming attacks on its results.
DER STANDARD  in Austria writes:
In the expert team’s assessment, they recommend formulating stricter scientific guidelines for handling data on climate change. Forecasts and projections should be made only based on solid scientific evidence.
Sounds good. But if that were to be implemented, the entire IPCC 4AR would be reduced to only 2 pages:  a front and back cover.
DER SPIEGEL in Germany writes:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Consequence of the crisis: Experts urge an overhaul of the UN IPCC. They harshly criticised the work of the leadership. Not only the leadership, but also the scientific work of the climate panel is in need of reform.
 DIE ZEIT in Germany writes:

IPCC to be a reform project
Flawed data, hacked e-mails: The doubts about the IPCC were enormous. Now the UN draws a conclusion: Its work was correct, but its credibilty must be strengthened.
 DIE WELT  in Germany writes:
The IPCC which has been under fire because of series of follies has to be comprehensively reformed in order to remove doubts about its credibility. A report presented by the UN-appointed experts has reached the conclusion that a ‘fundamental reform’ of the IPCC is needed in order for it to strengthen its scientific standards and organisation.
NZZ in Zurich has a more lengthy piece, and writes:
The Interacademy Council (IAC) said critics were only partially right. In a report presented to the UN in NewYork on Monday, the IPCC was attested as having done, on the whole, good work. But it is criticised that its management structures and public relations work of the IPCC did not fulfill the requirements of today.
From the UK, h/t : http://thegwpf.org/
The Sun: UN ‘lacks Solid Evidence’ in Climate Warnings
Daily Express: Climate Change Lies Are Exposed
BBC News: Stricter controls urged for the UN’s climate body
The Times: Climate chief under pressure to quit after report on glacier blunder
The Guardian: Rajendra Pachauri, head of UN climate change body, under pressure to resign
The Independent: IPCC feels the heat as it is told to get its facts right about global warming
Share this...FacebookTwitter "
"

Regular WUWT readers know of the issues related to Arctic Sea Ice that we have routinely followed here. The Arctic sea ice trend is regularly used as tool to hammer public opinion, often recklessly and without any merit to the claims. The most egregious of these claims was the April of  2008 pronouncement by National Snow and Ice Data Center scientist Dr. Mark Serreze of an ice free north pole in 2008. It got very wide press. It also never came true.
To my knowledge, no retractions were printed by news outlets that carried his sensationally erroneous claim.
A few months later in August, when it was clear his first prediction would not come true, and apparently having learned nothing from his first incident (except maybe that the mainstream press is amazingly gullible when it comes to science)  Serreze made another outlandish statement of “Arctic ice is in its death spiral” and” The Arctic could be free of summer ice by 2030″. In my opinion, Serreze uttered perhaps the most irresponsible news statements about climate second only to Jim Hansen’s “death trains” fiasco. I hope somebody at NSIDC will have the good sense to reel in their loose cannon for the coming year.
Not to be outdone, in December Al Gore also got on the ice free bandwagon with his own zinger saying on video that the “entire north polar ice cap will be gone within 5 years“. There’s a countdown watch on that one.
So it was with a bit of surprise that we witnessed the wailing and gnashing of teeth from a number of bloggers and news outlets when in his February 15th column, George Will, citing a Daily Tech column by Mike Asher, repeated a comparison of 1979 sea ice levels to present day. He wrote:
As global levels of sea ice declined last year, many experts said this was evidence of man-made global warming. Since September, however, the increase in sea ice has been the fastest change, either up or down, since 1979, when satellite record-keeping began. According to the University of Illinois’ Arctic Climate Research Center, global sea ice levels now equal those of 1979. 
The outrage was immediate and widespread. Media Matters: George Will spreads falsehoods Discover Magazine: George Will: Liberated From the Burden of Fact-Checking Climate Progress: Is George Will the most ignorant national columnist? One Blue Marble Blog: Double Dumb Ass Award: George Will George Monbiot in the Guardian: George Will’s climate howlers and Huffington Post: Will-fully wrong
They rushed to stamp out the threat with an “anything goes” publishing mentality. There was lots of piling on by secondary bloggers and pundits.
Feb 15th NSDIC Arctic Sea Ice Graph - click for larger image
Meanwhile, back at the ranch, I got interested in what was going on with odd downward jumps in the NSIDC Arctic sea ice graph, posting on Monday February 16th NSIDC makes a big sea ice extent jump – but why? Then when I was told in comments by NSIDC’s Walt Meier that the issue was “not worth blogging about” I countered with Errors in publicly presented data – Worth blogging about? 
It soon became clear what had happened. There was a sensor failure, a big one, and both NSIDC and Cryosphere today missed it. The failure caused Arctic sea ice to be underestimated by 500,000 square kilometers by the time Will’s column was published. Ooops, that’s a Murphy Moment.
So it is with some pleasure that today I offer you George Will’s excellent rebuttal to the unapologetic trashing of his column . The question now is, will those same people take on Dr. Mark Serreze and Al Gore for their irresponsible proclamations this past year? Probably not. Will Serreze shoot his mouth off again this year when being asked by the press what the summer ice season will bring? Probably, but one can always hope he and others have learned something, anything, from this debacle.
Let us hope that cooler heads prevail.
Climate Science in A Tornado

By George F. Will, Washington Post
Friday, February 27, 2009; A17

Few phenomena generate as much heat as disputes about current orthodoxies concerning global warming. This column recently reported and commented on some developments pertinent to the debate about whether global warming is occurring and what can and should be done. That column, which expressed skepticism about some emphatic proclamations by the alarmed, took a stroll down memory lane, through the debris of 1970s predictions about the near certainty of calamitous global cooling.
Concerning those predictions, the New York Times was — as it is today in a contrary crusade — a megaphone for the alarmed, as when (May 21, 1975) it reported that “a major cooling of the climate” was “widely considered inevitable” because it was “well established” that the Northern Hemisphere’s climate “has been getting cooler since about 1950.” Now the Times, a trumpet that never sounds retreat in today’s war against warming, has afforded this column an opportunity to revisit another facet of this subject — meretricious journalism in the service of dubious certitudes.
On Wednesday, the Times carried a “news analysis” — a story in the paper’s news section, but one that was not just reporting news — accusing Al Gore and this columnist of inaccuracies. Gore can speak for himself. So can this columnist.
Reporter Andrew Revkin’s story was headlined: “In Debate on Climate Change, Exaggeration Is a Common Pitfall.” Regarding exaggeration, the Times knows whereof it speaks, especially when it revisits, if it ever does, its reporting on the global cooling scare of the 1970s, and its reporting and editorializing — sometimes a distinction without a difference — concerning today’s climate controversies.
Which returns us to Revkin. In a story ostensibly about journalism, he simply asserts — how does he know this? — that the last decade, which passed without warming, was just “a pause in warming.” His attempt to contact this writer was an e-mail sent at 5:47 p.m., a few hours before the Times began printing his story, which was not so time-sensitive — it concerned controversies already many days running — that it had to appear the next day. But Revkin reported that “experts said” this columnist’s intervention in the climate debate was “riddled with” inaccuracies. Revkin’s supposed experts might exist and might have expertise but they do not have names that Revkin wished to divulge.
As for the anonymous scientists’ unspecified claims about the column’s supposedly myriad inaccuracies: The column contained many factual assertions but only one has been challenged. The challenge is mistaken.
Citing data from the University of Illinois’ Arctic Climate Research Center, as interpreted on Jan. 1 by Daily Tech, a technology and science news blog, the column said that since September “the increase in sea ice has been the fastest change, either up or down, since 1979, when satellite record-keeping began.” According to the center, global sea ice levels at the end of 2008 were “near or slightly lower than” those of 1979. The center generally does not make its statistics available, but in a Jan. 12 statement the center confirmed that global sea ice levels were within a difference of less than 3 percent of the 1980 level.
So the column accurately reported what the center had reported. But on Feb. 15, the Sunday the column appeared, the center, then receiving many e-mail inquiries, issued a statement saying “we do not know where George Will is getting his information.” The answer was: From the center, via Daily Tech. Consult the center’s Web site where, on Jan. 12, the center posted the confirmation of the data that this column subsequently reported accurately.
The scientists at the Illinois center offer their statistics with responsible caveats germane to margins of error in measurements and precise seasonal comparisons of year-on-year estimates of global sea ice. Nowadays, however, scientists often find themselves enveloped in furies triggered by any expression of skepticism about the global warming consensus (which will prevail until a diametrically different consensus comes along; see the 1970s) in the media-environmental complex. Concerning which:
On Feb. 18 the U.S. National Snow and Ice Data Center reported that from early January until the middle of this month, a defective performance by satellite monitors that measure sea ice caused an underestimation of the extent of Arctic sea ice by 193,000 square miles, which is approximately the size of California. The Times (“All the news that’s fit to print”), which as of this writing had not printed that story, should unleash Revkin and his unnamed experts.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e97d2a231',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Click for full PDF
From Dr. Roger Pielke Sr. Climate Science Weblog
There is a letter to the President published by the Cato Institute that headlines [thanks to ICECAPand Dr. Patrick J. Michaels to alerting us to it];
“Few challenges facing America and the world are more urgent than combating climate change.The science is beyond dispute and the facts are clear.” — PRESIDENT-ELECT BARACK OBAMA, NOVEMBER 19 , 2008
With all due respect Mr. President, that is not true.
The letter is signed by over 100 scientists.
Climate Science wants to comment on the specific statements of science in the letter which is reproduced below:
“We, the undersigned scientists, maintain that the case for alarm regarding climate change is grossly overstated. Surface temperature changes over the past century have been episodic and modest and there has been no net global warming for over a decade now.1,2 After controlling for population growth and property values, there has been no increase in damages from severe weather-related events.3 The computer models forecasting rapid temperature change abjectly fail to explain recent climate behavior.4 Mr. President, your characterization of the scientific facts regarding climate change and the degree of certainty informing the scientific debate is simply incorrect.”
Comments by Climate Science

“Surface temperature changes over the past century have been episodic and modest and there has been no net global warming for over a decade now.”

This is correct using the global average surface temperature. An effective analysis of this issue has been presented at the weblog http://rankexploits.com/musings/category/climate-sensitivity/. However, using the global average upper ocean heat content changes, the warming in the 1990s and early 2000s ended in 2003, so the more rigourous metric for global warming indicated “no net global warming” for 6 years.

After controlling for population growth and property values, there has been no increase in damages from severe weather-related events.

This is a correct statement which has been extensively discussed and summarized at http://sciencepolicy.colorado.edu/prometheus/category/climate-change; see also Chapter 2 in  Pielke, R.A., Jr. and R.A. Pielke, Sr., 1997: Hurricanes: Their nature and impacts on society.

The computer models forecasting rapid temperature change abjectly fail to explain recent climate behavior.

This is a robust conclusion both on the global scale (e.g. see) and on the regional scale (e.g see and see). 
The dismissive response on Real Climate and on Grist to this letter do not provide the objective scientific rebuttal to these science claims. This is unfortunate and is misleading policymakers, but, as we have learned and reported many times on at Climate Science and elsewhere (e.g. see and see), this is the way the IPCC and CCSP community deals with solid science that disagrees with their perspective. 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e97247a60',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

A few weeks ago, the Clinton administration finally got around to signing the Kyoto Protocol, the global warming treaty that obligates the United States to reduce greenhouse gas emissions by 7 percent below 1990 emission levels by the year 2012. Both supporters and opponents of the treaty agree that meeting that goal will require between a 30 and a 40 percent reduction in the greenhouse gas emissions that would otherwise occur about a decade hence. Beyond that, sorting out the scientific and economic argument between the two is difficult for a nonspecialist. Here’s the scorecard to date. 



Treaty supporters say that just about all the scientists engaged in global warming research now accept that the problem is real and must be addressed. Well, yes and no. Most (but by no means all) scientists engaged in the field agree that industrial emissions are probably affecting the climate. But the evidence is circumstantial. As the United Nations’ International Panel on Climate Change (IPCC) noted in its most recent (1995) report, the evidence thus far “cannot be considered as compelling evidence of cause‐​and‐​effect link between anthropogenic forcing and changes in the Earth’s surface temperature.” 



The “balance of evidence suggests” (in the words of the IPCC) that industrial emissions are the culprit, but that’s hardly conclusive. And the consensus about the matter is not as nearly universal as suggested. Seventeen thousand scientists (half of whom are trained in physics, geophysics, climate science, meteorology, oceanography, chemistry, biology or biochemistry) recently signed a petition written by Frederick Seitz, a past president of the National Academy of Sciences, declaring that there is no compelling evidence to justify reducing greenhouse gas emissions at all. 



Nor do scientists agree on how hot it would get if we did nothing. The IPCC’s “best guess” back in 1990 was that industrial greenhouse gas emissions would increase average global temperatures by 5.8 degrees (all temperature figures are Fahrenheit) by the end of the next century. In 1995 the IPCC adjusted its “best guess” down to 3.6 degrees. Three studies published this year (by Hansen, Dlogokencky and Myhre) suggest that the present “best guess” stands at about 2.25 degrees of warming by the end of the next century. Most experts doubt that that amount of warming would be particularly worrisome (indeed, we’re already about half way there temperature‐​wise, and the effect of this “global warming” has thus far proven underwhelming, to say the least). 



Killing the coal industry to reduce temperatures 1/​7th of 1 degree 50 years hence is justified by treaty advocates as a necessary “first step” of about 30 that must necessarily come. Treaty opponents do a quick cost/​benefit analysis and conclude that treaty advocates have lost their grip on reality. 



While the Kyoto Protocol envisions significant cuts in greenhouse gas emissions, scientists on all sides of the debate agree that its impact will be virtually undetectable. Tom Wigley, a highly respected senior scientist at the U.S. National Center for Atmospheric Research (and a scientist, moreover, usually thought of as in the alarmist camp), recently calculated that the Kyoto Protocol would only reduce temperatures by 0.13 degrees by 2050 if we accept the IPCC’s 1995 estimate of warming under a business‐​as‐​usual scenario. The Kyoto Protocol would have no meaningful impact on future climate change because, as along as we use fossil fuels, the question of global warming is not “if” but “when.” 



Finally, advocates of the treaty argue that its costs will be negligible, while opponents warn of a replay of the 1970s energy crises. The best evidence for each argument comes from studies issued by the Clinton administration (the study most optimistic about costs comes from the president’s Council of Economic Advisers (CEA), the most pessimistic from the Energy Information Administration). Both studies make valid projections. The differences are found in the assumptions. But it’s worth noting that the economic models relied on by the CEA reveal that, absent a truly global emissions trading system (a system that was hotly opposed by most nations at the recently concluded talks in Buenos Aires), America would be forced to abandon coal‐​fired electricity within the next decade to keep compliance costs from skyrocketing. 



Killing the coal industry to reduce temperatures 1/​7th of 1 degree 50 years hence is justified by treaty advocates as a necessary “first step” of about 30 that must necessarily come. Treaty opponents do a quick cost/​benefit analysis and conclude that treaty advocates have lost their grip on reality. 



And that, dear readers, is where we stand today. Misleading public characterizations of the global warming debate notwithstanding, the case for the Kyoto Protocol is pretty threadbare. Of course, the Senate is unlikely to ratify the treaty, a fact conceded by the administration in its decision not to send it up for a vote until at least 2001. If the past is any prologue, the case for ratification will continue to weaken. The question is, Will anyone be able to see through the political hot air to notice?
"
"Now that Japanese giants Toshiba and Hitachi have walked away from UK nuclear power projects that had previously been abandoned by others, it has forced the government to reassess the pro-nuclear bias of its energy policy. Greg Clark, the UK business secretary, has recognised that nuclear power is no longer cost competitive with renewable energy, but don’t expect any extra push into the cheaper technology.  There is easily enough solar and wind energy available to make up for the cancellation of the nuclear projects and to produce the low-carbon electricity required to make the UK’s 2030 carbon emissions targets achievable. Instead, however, the country’s incentives and regulations favour developing more power plants driven by natural gas. Having hacked back emissions from power by over two-thirds since 1990, progress with decarbonising the grid risks coming to an end.  According to the UK parliament’s Committee on Climate Change, the UK needs to cut power emissions from about 265g of carbon dioxide per kilowatt hour in 2017 to under 100g by 2030. The government had been substantially relying on nuclear power to do this, having originally identified eight sites as viable for new plants. Six projects were taken forward, including Hitachi and Toshiba plants in Wales and Cumbria respectively.  Yet despite much larger government incentives than those available for renewables, most private nuclear builders are now steering clear, having seen the problems with new plants in the likes of the US and France. The only two projects still on the slate are a joint venture by EDF of France and CGN of China – both foreign state-owned companies. They are building the UK’s first new plant in over two decades, Hinkley C in south-west England; while also planning a second, Bradwell B, in the south east.  Even before the latest announcement that Hitachi’s Wylfa plant in Wales was being suspended, the Committee on Climate Change was already saying the UK needed to build more renewable capacity to reach its carbon reduction targets. Now the problem is even worse.  In 2018, 19% of the UK’s electricity was generated by nuclear plants. With most existing plants due to retire over the next few years, I calculate this may now fall to 10% by 2030 when you factor in the new-build cancellations. Solar and wind generation could easily more than make up for this. For years, renewables’ share of generation has been steadily rising. It reached 30% in 2018 and is due to reach 35% in 2020. But with no new incentives for onshore wind and solar and only limited incentives for offshore wind, it looks likely to fall far short of its potential.  The government incentivises renewable energy projects through so-called contracts for difference (CFD) auctions in which the most competitive bidders are granted contracts to supply electricity at fixed prices. This year, it is set to auction some new offshore wind farm contracts. Only a trickle of projects that don’t receive such incentives go ahead, so the number of contracts on offer effectively dictates how much offshore wind capacity will be built.  With offshore wind currently providing about 7% of generation, I have heard from informal soundings that the new contracts will add less than 10% more. In other words, it would at best offset the decline in nuclear. With no further contracts in the pipeline at present, it suggests low-carbon power in the UK is at a standstill.  The reason why more renewables are not on the cards is because the Treasury is keen to limit energy incentives. It worries that the electricity price has been increasing – and hence the Treasury wants to strictly limit new incentives, the costs of which are added to electricity bills. This, however, ignores the fact that CFD prices will benefit from the falling cost of building offshore wind farms – the price has more than halved in three years. Nevertheless, the amount of money available to pay for the contracts is being limited to around half that being made available to owners of gas-fired power plants to supply capacity when the wind isn’t blowing. If all 27GW of offshore wind power schemes in various stages of planning got contracts, I calculate it would supply around one-third of the total electricity requirement. Coupled with the remaining nuclear power and the renewables that are already onstream, that would reach the 75% of power that the Committee on Climate Change reckons needs to be coming from these low-carbon sources by 2030 to achieve the emissions targets. This is not counting potential offshore wind resources which are not even being mobilised, plus large possibilities for onshore wind and solar. Instead, gas-fired power looks set to supply around half of UK electricity by 2030, compared to 40% at present. One government justification for being less generous to renewables is that unlike gas or nuclear, they do not represent “firm” power – in other words, they only generate when the wind is blowing or the sun is shining. 
Proponents of renewable energy counter that you can reduce the generating capacity required by increasing the use of batteries to store power on the grid and by incentivising consumers to, say, use more power overnight when demand is lower.  Yet one other option that attracts less attention is that you also get spare “firm” capacity from small gas engines or open-cycle turbines. These can be built quickly and would only be sparingly needed in a system mostly supplied by renewables.  Based on my calculations using Hinkley C and Wylfa, they cost around one-twentieth of the projected cost of new nuclear power. They are also nearly half the price of large gas-fired “CCGT” plants. Instead, however, the government spends the lion’s share of its incentives pot on large conventional power plants, many of which would operate whether they were subsidised or not.  The underlying obstacle seems to be political opposition within the Conservative Party. My understanding from renewable energy lobbying sources is that the government’s Department for Business, Energy and Industrial Strategy would like to promote renewable energy more, but is held back by the Treasury, which wants to leave it to “the market”.  The upshot is that government policy is offering large incentives to new nuclear, gas-fired power and also shale gas extraction – but, paradoxically, not many are actually being developed. Meanwhile the cheapest options – onshore wind, solar and offshore wind – are being discriminated against. The collapse of the UK’s nuclear power plans should be an opportunity to think again. How frustrating that decarbonising power is instead falling off the agenda."
"Oil prices have fallen dramatically since August – and, rather counter-intuitively, this could be a bad thing. As of late October, the price of oil has fallen from US$110 per barrel (bbl) to below US$85. There are predictions it could further descend to US$60-$70. While a drop was expected, due to lower global demand and oversupply, such levels would be radical indeed. A major price fall is indeed a bad thing – and, if it continues, it could be a very bad thing. But why? Haven’t we been told for half a century that cheap oil is beneficial? Doesn’t it lower consumer fuel bills, control inflation, reduce transport expenses and thus the price of necessities like food – while stimulating industrial activity by slashing costs? And haven’t prices above US$100 been an anchor on economies recovering from the global financial crisis?  Let’s not forget that high oil prices also transfer huge amounts of cash and geopolitical influence to petrostates such as Russia or Iran – and also non-state actors that control oil fields, in particular, Islamic State. So low prices really are good, right? Not necessarily. Though all of the above remains largely true, there are now other considerations that outweigh those factors – and indeed should have outweighed them earlier. High prices have stimulated major innovations in energy, including efficiency. We are truly in the middle of a revolution in vehicle technology: hybrid cars are now a common sight in many of the world’s major cities, with every big car maker producing several models, including luxury brands like Mercedes, Lexus, BMW, and Cadillac.  Numbers in the US for plug-in and all-electric (EV) models have soared in the last three years, with as many as 20 new EV models released in 2014, eight of them priced below US$35K. Coming improvements in battery technology will continue to increase the driving range for electric vehicles and will continue to bring down prices, making them affordable for many millions.  There has also been growth in alternative fuels, now including natural gas for long-haul trucks and marine vessels, replacing far more polluting and carbon-emitting diesel and bunker fuels.  Taken together, this represents a substantial, but still early-stage, shift away from petroleum as the single source of global transport and towards a future most people would now call more sustainable.  But there is more. The past five years of consistently elevated prices have increased public support for renewables and have encouraged the idea of sustainability as a measure of the common good. This has aided local and national governments to choose more sustainable options in energy efficiency, carbon emissions, water use, and more.  High prices have also encouraged many businesses and industries to improve their image and lower costs by recycling more and reducing energy use and emissions. But what about geopolitical issues? Here, too, we find a different calculus in place. Over time, a shift away from petroleum would solve the problem of cash and power going to petrostates and might force them to diversify their economies. But don’t look for this to happen anytime soon.  In the meantime, however, high prices have brought the full-scale development of shale oil/gas in North America, making it a growing alternative to Russia and OPEC. While debate continues over the impacts of fracking, the US is on trend to match the country’s all-time production peak of 9.5m bbls/day. America has already replaced Russia as the world’s largest exporter of petroleum products and could begin exporting significant volumes of crude oil if its long-time ban on such exports were lifted.  Needless to say, reducing Europe’s dependence on Russian oil and natural gas would be beneficial, given that country’s aggressive foreign policy. Similarly, energy from North America (US and Canada) could reduce Japan and Korea’s reliance on OPEC, now at over 90%.  These considerations must be weighed against the negative effects of high prices, of course. In the oil cosmos, there are no easy alternatives. Yet prices matter a great deal; the lower they are, the fewer the alternatives. All of the positives noted above would end were oil to reach US$60-$70 and remain there. Does this seem exaggerated? We have historical precedents. The dramatic price collapse of 1986, after which oil stayed cheap for nearly two decades, effectively killed all sustained interest in energy improvements (although the 1990s did bring us a major vehicle innovation – the gas-guzzling SUV). Low oil prices are a narcotic and an enemy of a different future."
"
Minus 13 degrees – the coldest it’s been in April
 From Weatherzone – Brett Dutschke, 
Wednesday April 29, 2009 – 14:58 EST
Charlotte Pass, 1,837m, Snowy Mountains of New South Wales, Australia
 
A new Australian record was set early this morning, a temperature of minus 13 degrees, at Charlotte Pass on the Snowy Mountains.
This is the lowest temperature recorded anywhere in Australia in April and is 13 below the average. Nearby at Perisher it dipped to minus 11 degrees and at the top of Thredbo it dipped to minus 10.
Across the border, on the Victorian Alps April records were broken at Mt Hotham where it chilled to minus eight degrees and Mt Buller and Falls Creek where it got as low as minus seven.
A few other locations set April low temperature records also. In Tasmania Lake Leake was as cold as minus six, Sheffield and Dover both reached minus one and Flinders island got to zero. Hobart had its coldest April night in 46 years, recording a low of 1.7 degrees, seven below average.
While much of inland NSW and Victoria will be colder tomorrow morning than it was this morning under clearer skies, the Alps should be a little warmer due to a rise in humidity.

Note, all temperatures in the story above are in Centigrade. Photo and map added by Anthony.
Here are the all-time highs and lows for the continent of Australia (source Perth Weather Center)
HIGHEST RECORDED TEMPERATURE:

 Oodnadatta, South Australia  50.7 C  (123.3 F) on the 2nd January, 1960

LOWEST RECORDED TEMPERATURE:

 Charlotte Pass, New South Wales  -23.0 C  (-9.4 F) on the 29th June, 1994

While this is certainly a significant new cold record this early in Australia’s fall going on winter, one must always remember that weather is not climate. – Anthony
(h/t to WUWT reader “Chuck”)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96bc62ff',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Spurred by tax competition, the flat tax revolution continues to generate positive results. Albania will have a 10 percent flat tax beginning in January 2008. The corporate rate also will be 10 percent, as will the payroll tax. The latter reform is particularly interesting since many of the flat tax nations in Eastern Europe retain punnitive payroll tax rates — a policy that undermines the pro‐​growth and pro‐​employment effects of the flat tax. The _Southest European Times_ reports: 



In a bid to promote growth and improve the business climate, the administration of Albanian Prime Minister Sali Berisha plans a major overhaul of the tax system. The biggest change is a switch to a flat tax. “As of January 1st, 2008, Albania will have implemented the 10% flat tax system, one of the lowest in Europe,” Berisha told a business community meeting in late March. Corporate taxes, currently at 20%, are to be slashed in half. Social security contributions from businesses will likewise be capped at 10%. The government and other supporters of the reform say it will widen the taxable base and simplify tax administration, while also making Albania an easier place to invest. According to Finance Minister Ridvan Bode, the changes will lead to a more streamlined fiscal system. “The flat tax helps eliminate the potential arbitrage between corporate tax, dividend taxes and the income tax,” he says. VAT and other taxes will also be gradually reduced in order to woo investors, the minister added. …In the past, the IMF has been wary of plans to reduce taxes in Albania. This time, however, it seems more receptive — provided the overhaul is combined with more effective revenue collection. “We will negotiate with the Albanian government about the tax reduction, depending on the tax collection,” IMF representative Ann Margaret Westin told the press.
"
"

 **MEMORIES  
LIGHT THE CORNERS OF MY MIND  
MISTY WATERCOLOR MEMORIES  
OF THE WAY WE WERE …  
WHAT’S TOO PAINFUL TO REMEMBER  
WE SIMPLY CHOOSE TO FORGET**



Raisa Burykina, an 85‐​year‐​old pensioner whose father was killed in the battle, said she was pleased to have seen Mr. Putin at the concert.





“Prices went down under Stalin; now they are going up,” said Ms. Burykina, who had a clutch of Soviet orders pinned to her red sweater.  
— _Wall Street Journal_ , February 7, 2018



  
 **BOOTLEGGERS AND BAPTISTS**  
“The president’s budget proposes to replace in significant part the very successful current system of having SNAP recipients use EBT cards to purchase food,”… Jim Weill, president of the Food Research and Action Center, said in a statement.



The proposal is also likely to enrage food retailers — particularly Walmart, Target and Aldi — which stand to lose billions if food‐​stamp benefits are cut, analysts say. On Monday, the Food Marketing Institute, a trade association for grocery stores, condemned the Harvest Box proposal as expensive, inefficient and unlikely to generate any long‐​term savings.  
— _Washington Post_ , February 13, 2018



 **HOW TO GET A HOUSE UNDER SOCIALISM**  
Sexual assault and harassment are rife across all sectors of North Korea’s misogynistic society, according to a new report. …



Many reported that men in positions of authority used their power to take advantage of women.



One woman described going to the mayor’s office to be allocated a house. “I was 32 years old and I must have looked attractive in his eyes. I was raped in his office and received a house in return.”  
— _Washington Post_ , March 8, 2018



 **NO TIME TO MAKE THE DONUTS**  
A French boulanger has been ordered to pay a €3,000 fine for working too hard after he failed to close his shop for one day a week last summer. …



Under local employment law, two separate regulations from 1994 and 2000 require bakers’ shops to close once a week — though exceptions can be made in specific cases.  
— _The Guardian_ , March 14, 2018



 **YOU MIGHT BE, ACTUALLY**  
I can’t be the only one concerned that an increasing amount of the orange juice Americans consume might be sourced in Brazil and Mexico rather than Florida and California.  
— Andrew Furman in the _Wall Street Journal_ , March 23, 2018



 **WHEN YOU PUT GOVERNMENT IN CHARGE OF SOMETHING, YOU’RE PUTTING POLITICIANS IN CHARGE**  
D.C. Council member Trayon White Sr. (D‐​Ward 8) posted the video to his official Facebook page at 7:21 a.m. as snow flurries were hitting the nation’s capital. …



“Man, it just started snowing out of nowhere this morning, man. Y’all better pay attention to this climate control, man, this climate manipulation,” he says. “And D.C. keep talking about, ‘We a resilient city.’ And that’s a model based off the Rothschilds controlling the climate to create natural disasters they can pay for to own the cities, man. Be careful.”  
— _Washington Post_ , March 18, 2018



 **SOME SAY ANTI-SEMITISM IS THE SOCIALISM OF FOOLS, BUT SOMETIMES THE SOCIALISM OF FOOLS IS JUST SOCIALISM**  
President Nicolás Maduro late Thursday briefly outlined his monetary rescue plan. In a country where a dozen eggs can cost 250,000 bolivars ($5) amid worsening inflation, he would chop three zeros off the currency — arguably bringing the price for those eggs down to 250.  
— _Washington Post_ , March 23, 2018



 **AS USUAL**  
Regulators say they may need more power  
— Headline in the _Washington Post_ , February 7, 2018



 **CITIES PLAN FURTHER HOUSING SHORTAGES**  
Lawmakers and advocates in California, Illinois and Washington State are pushing to repeal state laws that forbid rent control or place limits on cities’ ability to regulate rent increases.  
— _Wall Street Journal_ , February 5, 2018
"
"
Share this...FacebookTwitterA new temperature reconstruction carried out by a team of German/Russian scientists has yielded interesting results. It finds no correlation over the last 400 years between atmospheric CO2 and the temperature in the Arctic regions studied.


Yuri Kononov of the Institute of Geography, Russian Academy of Sciences in Moscow and Michael Friedrich of the Institute of Botany, University of Hohenheim collect tree samples of Scots pine in the Khibiny Low Mountains of the Kola Peninsula in Arctic Russia

Recall that CO2 concentrations have been rising steadily since the start of the Industrial Revolution, 1870, yet the press release starts with:
Parts of the Arctic have cooled clearly over the past [20th]century, but temperatures have been rising steeply there since 1990.
Rising since 1990? That’s more than 100 years after the start of the Industrial Revolution. The press release continues:
The reconstructed summer temperature on Kola in the months of July and August has varied between 10.4°C (1709) and [peaking at] 14.7°C (1957), with a mean of 12.2°C.  Afterwards, after a cooling phase, an ongoing warming can be observed from 1990 onwards.
The temperature fluctuated between 10.4°C  and a peak of 14.7°C in 1957 , and then cooled until 1990. The scientists say it correlated very well with solar activity until 1990. Then beginning in 1990, the temperatures started to rise rapidly again. Does anyone see a CO2 correlation there? I don’t.
The only time we have a correlation between CO2 concentrations and temperature is from 1990 until…? Unfortunately the press release does not even mention that.  Until today? 2005? 2000? It really is annoying that they didn’t specify the end of the time scale. If the reconstruction was only up to 2000, then we are only speaking about a 10-year period – a completely meaningless time period. Even 20 years would be highly dubious.
I called UFZ early this afternoon here in Germany to try to find out, but the secretary said that all scientists had already left for the weekend.
*****************************************************
UPDATE! The German press release now has the following graphic. The dataset ended 2001! The press spokesman just told me on the phone. So there was warming from 1990 until 2001!  As you see, the graphic iteself is misleading. It almost looks as if the curve goes until 2010.
Press spokesman Tilo Arnhold informed by telephone that the dataset goes only up to 2001, yet the press release graphic clearly shows a curve beyond 2001. Graphic Source: Stephan Boehme/UFZ
Interestingly, also, the graphic shows warming since 1650.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




******************************************************
The press release also states:
What stands out in the data from the Kola Peninsula is that the highest temperatures were found in the period around 1935 and 1955, and that by 1990 the curve had fallen to the 1870 level, which corresponds to the start of the Industrial Age.
The temperature fell to 1870-levels by 1990? Wait a minute – the CO2 theory say it’s supposed to go up, and not down.
The team compared their Kola region data to Swedish Lapland and the Yamal and Tamimyr Penninsula temperature reconstructions: Here’s what they found:
The reconstructed summer temperatures of the last four centuries from Lapland and the Kola and Taimyr Peninsulas are similar in that all three data series display a temperature peak in the middle of the twentieth century, followed by a cooling of one or two degrees.
Cooling from the middle of the twentieth century until 1990. Cooling!
Concerning Yamal, it is no surprise that it’s out of the ballpark because that dataset was not handled scientifically, see Yamal-gate
(Keith Briffa cherrypicks tree rings to get the temperature reconstruction he wants to see).
So if it’s not CO2, then what could be driving temperature?  The press release goes on:
What is conspicuous about the new data is that the reconstructed minimum temperatures coincide exactly with times of low solar activity. The researchers therefore assume that in the past, solar activity was a significant factor contributing to summer temperature fluctuations in the Arctic.
The only mystery left is why was there was a sudden increase in warming from 1990 until 2001? The scientists believe it has to do with local factors. Clearly it isn’t CO2.
Share this...FacebookTwitter "
"

As billions of little light bulbs brighten America this holiday season, Al Gore is calling for thousands across the nation to interrupt their regularly scheduled activities and hold house parties showing his environmental _cri de coeur_.



Gore announced recently on the Oprah Winfrey Show that Americans should congregate this Saturday, December 16, to watch and discuss his DVD, _An Inconvenient Truth_ , advertised as “a true story about the hard science and real threats of global warming.”



The idea is to demonstrate that “action” is wanted on climate change.



If climate alarmists are to be believed, Americans must cut their electricity use substantially, and soon, to reduce greenhouse‐​gas emissions associated with fossil‐​fuel combustion. Celebratory holiday lighting — what doomsayer Paul Ehrlich once called “garish commercial Christmas displays” — would surely be the first to go, coming before indoor lighting, cooking, heating, and air conditioning.



But are these changes really necessary for the United States, the world’s most prolific user of energy? The good news — and a reason for holiday cheer — is that the science behind rapid, disruptive global warming scenarios is murky at best. Though the debate is highly politicized and emotionally charged, good science is beginning to drive out bad.



The Kyoto Protocol and other sledgehammer approaches to cutting greenhouse‐​gas emissions in the advanced countries are coming under intellectual, not just political, assault.



A sampling of recent issues of _Science_ , the journal of the American Association for the Advancement of Science, shows that peer‐​reviewed studies dispute virtually all the tenets behind climate alarmism. A November 17 feature, “False Alarm: Atlantic Conveyor Belt Hasn’t Slowed Down After All,” rebuts the hyped hypothesis that melting ice from global warming (read: man‐​made global warming) would disrupt ocean currents and plunge Europe into an Ice Age.



The same _Science_ report takes on the idea that warming causes drastic cooling, the complicated, and ironic scenario Al Gore said “some scientists are seriously worried about.”



 _Science_ comments that even if global warming were cooling specific regions (a big if), “it would be decades before the change would be noticeable above the noise.”



And here, in a nutshell, is what the climatology debate is about: if and how much the human influence on climate is detectable above natural variability.



For instance, rapid rises in sea level produced by global warming is another popular alarm, one very relevant for residents of the Texas Gulf Coast area. But as the November 24 issue of _Science_ says, “It remains unclear whether the recent rate increase [since 1993] reflects an acceleration in sea‐​level rise or a natural fluctuation.”



Indeed, sea level has been rising for well over a century for the same natural reasons that brought the end of a little ice age. What scientists are measuring and debating concerns not feet but inches, and fractions thereof, over many decades. This hardly seems the crisis scenario that Al Gore portrays.



Gore claims, “There is now a strong, new emerging consensus that global warming is indeed linked to a significant increase in both the duration and intensity of hurricanes.”



But hurricane specialists disagree. The November 10 _Science_ says, “The best theory and modeling still indicate the ocean temperature has only a minimal effect on storms.”



Exaggerated forecasts of disrupted ocean circulation, rapid sea‐​level rise, and more intense hurricanes make for splashy headlines, but sober science suggests that these scares _du jour_ may go the way of yesterday’s alarms over global cooling, the population bomb, and mineral‐​resource exhaustion.



Nonetheless, one part of these scare stories is genuinely frightening: the heavy‐​handed government intervention that advocates always look to as the source of salvation. Yesterday’s foes of the free market were socialists, communists, and Keynesians. Today’s are greens who want government engineering to “stabilize” the climate and ensure “sustainability.”



I will not be watching Al Gore’s quasi‐​sci‐​fi horror movie this Saturday night. I’ll probably be driving through neighborhoods of people I don’t even know, enjoying the gift of their holiday lights. And to them I say: Don’t fall for exaggerations. Enjoy your regularly scheduled activities, and keep the lights on.
"
"In a remote village in north Norfolk, nine-year-old Amelia Bradbury has been standing alone outside her school gates every Friday for months. Like hundreds of thousands of young people across the world, she is following Greta Thunberg’s lead and campaigning for action on the climate crisis – but, far from any of the big city demonstrations, she’s having to go it alone. “I was quite scared the first time because no one was doing it with me,” says Amelia. “But I’m doing this because I care about something. I really want people to listen to me and to make a difference.”  She holds a handmade sign reading: “I’m striking for our nature”, and it is her passion for wildlife and the outdoors that keeps her going each week. On the weekends she volunteers for Norfolk Wildlife Trust with her family and enjoys birdwatching. Nevertheless, there are times when striking alone can be difficult. “It is quite hard in the cold, especially when it’s freezing,” she says. A few of her friends at school are interested, but their parents are not so sure – with only one person, it is hard to get the ball rolling. Although there are young people from all walks of life striking alone, it’s often those in rural areas who struggle to make themselves, and the issues they most care about, heard. Holly Gillibrand, 14, in Fort William has been striking for more than a year: “The bigger towns and cities get all this media attention, obviously, because a lot of people turn up. “But I think the media tend to forget about the people in the rural places around Scotland and the rest of the UK. We have a different perspective on things and our voices deserve to be put out there just as much as anyone else’s.” But social media has provided a platform for rural voices to be amplified. In November Amelia’s father uploaded a video of her to Twitter after the prime minister, Boris Johnson, failed to show up to the climate leadership debate before the election. In it she said: “Tomorrow I’m going to be standing outside in the rain and you couldn’t be bothered to turn up in a warm studio to debate the other leaders. How pathetic are you?” It generated more than 1,000 retweets and praise from the wildlife presenter Chris Packham. “It was a bit crazy but I feel really proud because it shows that people notice and care,” Amelia says. It was the power of social media that inspired Anna Kernahan, 17, Grace Maddrell, 14, and Helen Jackson, 21, to set up Solo But Not Alone, a Twitter page dedicated to sharing the stories of solo climate strikers. “People will say: ‘Oh, you’re not alone,’ but it’s hard to see that when you are sitting there at the strike and there’s no one else around you, everyone’s walking past,” says Anna. She strikes alone in Belfast from 12pm to 3pm every Friday, often reading a book or catching up on homework. Although she struggles to get friends to join her, she has one powerful supporter to keep her going – Greta Thunberg. “My phone crashes whenever she retweets me because she gets so many likes,” says Anna. Within weeks of setting up Solo But Not Alone at the end of 2019, the trio had hundreds of followers, and have been able to profile solo strikers across the globe. It has helped them connect with people such as Mulindwa Moses, a 23-year-old climate activist from Uganda who strikes alone on the roadside. At one point he did it for 55 days consecutively, but now just strikes on Fridays and Saturdays, raising awareness for the Save Congo Rainforest and Two Trees a Week campaigns. Moses was inspired to take action after speaking to people who had lost family members in landslides and floods, which he later found were being caused by the climate crisis. “There are literally no reports about the climate and ecological crisis in the media, which has kept the population ignorant, and leaders are taking advantage of this to not take action,” Mulindwa says. Living in Kampala, Uganda’s capital, he strikes alone not because he lives in an isolated area, but because of his country’s lack of tolerance for climate activism. “Being a climate activist in Uganda is very hard,” Mulindwa says. “You cannot hold a strike with large numbers to create awareness because the government [does not] allow it, and I have lost friends, who say they can no longer associate with me because I stand on the side of roads holding signs and spend most of my time planting trees.” But like other solo climate strikers around the world, his loneliness is eased by the support he receives from fellow climate activists online. Anna says: “We really want to make sure that even if only one person is striking, their voice is heard and it is loud.”"
"To sort or not to sort, that is the question. Lots of people wonder whether it’s really worth their time and effort to separate, wash and store recyclable materials – especially if it takes more energy to recycle, or if the plastics sent for recycling end up in overseas landfill. The truth is, the issue is complex, and even experts can’t agree on the economic and environmental benefits of recycling.  There are four popular arguments, typically used by organisations and individuals to promote recycling: that it reduces landfill waste, that it saves public money, that it creates jobs and that it encourages consumers to reduce waste in the first place. Let’s consider each of these in turn.  Images of putrefying waste in landfill sites, generating greenhouse gas emissions and polluting the environment, are one of the most compelling reasons for recycling. The 1999 European Landfill Directive set targets to reduce biodegradable waste, and in response the UK government increased tax on landfill disposal, introducing an escalating duty,  which currently sits at £88.95 per tonne.  Then, in 2003, the Waste and Recycling Act established kerbside collection of recyclable materials. Rising levels of recycling and incineration, as well as the escalating landfill tax, have certainly reduced the proportion of waste dumped on landfill sites in the UK.  But the National Audit Office revealed that some of the plastics that residents separate for recycling are being exported overseas, to places such as Malaysia and Vietnam, where there are insufficient checks to ensure this material is actually recycled. The industry is also facing investigation for fraud and corruption, over these matters. So it could be that millions of tonnes of UK recycling is simply ending up in landfill in other parts of the world. 


      Read more:
      China bans foreign waste – but what will happen to the world's recycling?


 Part of the problem is that there are limited facilities to recycle mixed plastics in the UK. It costs a lot of money to separate and recycle different types of plastic, using specialist machinery. But there is infrastructure for plastic bottle recycling in the UK, which is why many council schemes historically only collected this type of plastic.  It costs a lot of money for local authorities to manage household waste. Disposal facilities owned by private companies, such as Veolia and SITA, charge local authorities gate fees per tonne of waste - around £107 per tonne for landfill and £86 per tonne for incineration. Local authorities in England produced 22.4m tonnes of waste in 2017, of which 45% was recycled – so that’s a lot of money saved.  Variations in collection systems across England and in material streams (such as paper, glass, cans) make it difficult to predict the cost per tonne of recycling, but it does cost significantly less than disposal, because the material can be sold. Aluminium cans are one of the more profitable materials in your waste and are sold by local authorities or waste contractors to be melted down and made into new cans.  In the UK, campaigns such as Recycle for London’s Nice Save use a moral message to emphasise the savings that local authorities can make when people recycle. But this is partly because laws such as the landfill tax have made recycling the cheaper option. The prices of different recyclable materials can fluctuate, which can limit the savings made. But this depends on the contracts local authorities strike with private waste management companies, and who takes on the risk.  In any case, UK citizens might wonder why taxpayers foot the bill for recycling, when in other parts of Europe producers are responsible. The government’s new waste strategy for England does include plans to extend responsibility for packaging to producers, by introducing a deposit scheme for bottles and asking producers to pay to cover the cost of recycling. But it’s not clear how this will work with existing private waste contracts. The charity Green Alliance claimed that recycling and reuse could create over 200,000 new jobs in the UK. Compared to disposal, recycling does create jobs, because waste sorted by consumers provides feed stock for an economy in global materials. How consumers sort their waste – whether in one box or separate boxes – leads to different supply chains and labour processes.  For example, if you’re putting all of your dry recyclables into one box, these materials will need to be taken to a special facility that employs people to sort them by hand, alongside machine processing. Many jobs in the recycling industry are low skilled and dirty work. They are often performed by migrant labour, or within the precarious informal economy overseas. Yet there are also mid-skilled, professional jobs – such as public and private waste service officers, who manage and supervise operations – and these opportunities will grow if the government creates incentives for producers to use recyclable materials, or invests in systems to promote reuse.  There are, of course, more effective ways of dealing with waste than recycling. Reusing, reducing and preventing waste – for example, by choosing products that are less packaged, refusing disposable coffee cups or buying secondhand – are all better options. Environmental organisations and influencers have targeted keen recyclers with this message, in the hope that they will take further steps to live more sustainably.  But there are only limited changes a person can make to their shopping habits, in a marketplace where packaging is embedded within infrastructures of provision. So responsible waste management is really a responsibility shared between governments, producers, local authorities, waste companies and citizens. In particular, the companies that create the materials that become household waste have huge power to reduce it. On the whole, it probably is worth the effort to sort your waste, despite some problematic practices, because recycling does drive down the amount of waste going to landfill and demand for recycling services will help drive improvements and oversight. There’s still a long way to go, before the UK can manage its waste sustainably as a society – and it’ll only get there if governments and citizens keep up their efforts to improve this process."
"
Another anemic solar cycle 23 sunspeck, could 19th century astronomers have seen it?
From Spaceweather.com

SUNSPOT 1016: A ring-shaped sunspot numbered 1016 has emerged near the sun’s equator. Its magnetic                polarity identifies it as a member of old Solar Cycle 23. Until                these old cycle sunspots go away, the next solar cycle will remain                in                abeyance.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96505207',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Mike Ronanye writes:
SWPC has just made a change in their solar cycle predictions in the middle of the month without any preannouncement. Both Sunspot and F10.7cm predictions were altered significantly. 



See the following links:
http://www.swpc.noaa.gov/SolarCycle/
The off-cycle update is in this week’s PDF report which contains the altered graphics:
http://www.swpc.noaa.gov/weekly/pdf/prf1747.pdf
You can see the last monthly summary here which I have been complaining reporting about, here:
http://www.swpc.noaa.gov/weekly/pdf/prf1745.pdf
This should have been the January 2009 summary but SWPC recycled the December 2008 summary.
I looked for but was unable to find any press releases. Please search for any additional information and post it here. If you downloaded any SWPC data or graphics hold on to it. I will be updating my SWPC Sunspot animation.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9859b2b9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The Ganges is a lifeline for millions of people who live within its catchment as a source of water, transport and food. During the Hindu pilgrimage known as Kumbh Mela the Ganges plays host to the largest human gathering on Earth as 120m people arrive to bathe in the river over 49 days. Despite its tremendous spiritual significance, the Ganges is also notorious for having some of the most polluted water in the world. For 79% of the population of the Ganges catchment, their nearest river fails sewage pollution standards for crop irrigation. Some 85% of the population live near water that isn’t safe for bathing and Allahabad – where Kumbh Mela takes place in 2019 – is one of those places.  Our own research suggests that as the number of people living in nearby cities increases, the problem with water quality in the Ganges worsens. Urban populations in the Ganges catchment contribute around 100 times more microbial pollution per head to the river than their rural counterparts. This means that untreated sewage discharged from a sewer appears worse for river water quality than sewage discharge where there are no sewers at all. When we examined 10 years of water quality data we found that the concentration of faecal coliforms – a common pollution indicator found in human faeces – increased when the density of people living upstream increased. This makes sense: more people means more poo.  But we also found that people living in cities in India contribute more pollution per person than those in rural areas – how much more depends on the population density. A person living in an area in India with 1,000 people per km², a density similar to central London, contributes on average 100 times more pollution to the nearest river than they would in an area with 100 people per km² – say, rural Devon in the UK. So why does it appear that a person living in an Indian city produces more sewage pollution than someone living in the countryside? Of course, people in the cities are unlikely to actually contribute significantly more faeces than those in rural communities. Instead, it’s probably sewers that are to blame. In cities, extensive sewage networks efficiently flush sewage to the river, whereas in rural areas more people defecate in the open or in pit latrines. This means faeces in rural areas are less likely to be washed into the river and the bacteria and viruses they carry are more likely to die in situ. As the population density of a place increases, sewers become more common. Sewage removal is essential for the protection of public health, but without effective treatment, as is typically the case in the Ganges catchment, it comes at the cost of increased river pollution and waterborne diseases for people living downstream. It’s therefore clear that water quality in the Ganges is a more complex and widespread problem than previously thought. We’d expected that cities, with their more advanced sewage management, would be better for the river. What we found was the opposite – more sewers without sewage treatment makes river pollution worse. The urgency to invest, not only in sewers, but in the treatment of sewage has never been greater – especially in the most densely populated areas. However, the Western approach of taking all waste to a central treatment plant is expensive and so may not be the best solution.  Onsite treatment technologies such as off-grid toilets or decentralised treatment plants are rapidly developing and may help improve river water quality sooner, enabling more and more people to celebrate Kumbh Mela safely."
"Conservation is often a conflict between the demands of development and a desire to do what is best for the environment. It’s rare that we get the chance to report a decision which was taken for the good of people that has also panned out well for nature’s ecosystems. However, that is just what our new research paper found. Saving energy from street lighting is not just a green option, it also makes good financial sense. Two solutions in particular include replacing old High-Pressure Sodium (HPS) lightbulbs with new and energy-efficient Light-Emitting Diodes (LEDs) and turning the lights out entirely during the latter part of the night when fewer people are around. In the UK, these changes in lighting technology have been gradually taking effect over the last decade or so. While these decisions were made for good reasons, we knew little about how they would influence nocturnal wildlife. Our team of experts at York and Newcastle universities were interested in finding out how moths might be affected by the switchoff and new LED lighting, as they play an important role as night-time pollinators of a wide range of flowers, and have declined in abundance by 40% in 40 years.  Light pollution, from street lights and other sources, has been suggested as a possible cause of this decline, though there are other factors such as climate change and habitat loss. Our previous research showed that under HPS street lighting left on all night, moths were distracted from visiting flowers and instead flew higher up, around street lights. This resulted in less pollen being carried by moths in lit areas, and a subsequent study by Swiss researchers demonstrated that this actually caused reduced fruit production. 


      Read more:
      Fatal attraction: how street lights prevent moths from pollinating


 In our study, we asked whether the disruption to nocturnal ecosystems from street lights might be eased or exacerbated by the introduction of new energy-efficient LED street lighting. Working on farmland in East Yorkshire in the UK, we set up a chain of mock street lights alongside hedgerows that would allow us to manipulate the type and duration of lighting. We compared older HPS lights to LEDS, and standard full-night lighting to part-night lighting, in which lights were turned off at midnight. All lighting was compared to an unlit control that replicated natural darkness. Under each lit and unlit treatment we placed several plants of White Campion (Silene latifolia), a common wildflower known to be pollinated by both bees and moths. We left each plant in the field for four days and nights before measuring what proportion of flowers had been pollinated, and the weight and number of seeds in every fruit. LEDs are rich in blue light which is attractive to moths but we found no difference in the rate of pollination between plants under LEDs and those under HPS lights. Our data did show that the differences between pollination under full-night lighting and in natural darkness were erased when lights were turned off at midnight.  Surprisingly, this wasn’t just a partial improvement. We found no significant difference between rates of pollination in part-night lighting treatments and in natural darkness, and this suggests that turning lights off at or after midnight may allow nocturnal ecosystems to function as normal in the second half of the night. These results are quite encouraging. Local authorities can save money and energy from street lighting and help nocturnal ecosystems recover from light pollution at the same time.  So there’s no evidence that the switch from HPS lights to LEDs increases the negative impacts of lighting on wildlife, and even better, switching to part-night lighting actually appears to reduce them. By switching lights off at midnight there is the potential to tackle two issues at once – reduce energy bills and the ecological impact of light pollution."
"

Some naive people might have been convinced that the U.S. House voted to wreck the American economy by endorsing cap and trade because it was the only way to save the world. But even many environmentalists had given up on the bill approved last Friday. It is truly a monstrosity: it would cost consumers plenty, while doing little to reduce global temperatures.   
  
  
But the legislation had something far more important for legislators and special interests alike. It was a pork‐​fest that wouldn’t quit.   
  
  
Reports the _New York Times_ :



As the most ambitious energy and climate‐​change legislation ever introduced in Congress made its way to a floor vote last Friday, it grew fat with compromises, carve‐​outs, concessions and out‐​and‐​out gifts intended to win the votes of wavering lawmakers and the support of powerful industries.   
  
  
The deal making continued right up until the final minutes, with the bill’s co‐​author Representative Henry A. Waxman, Democrat of California, doling out billions of dollars in promises on the House floor to secure the final votes needed for passage.   
  
  
The bill was freighted with hundreds of pages of special‐​interest favors, even as environmentalists lamented that its greenhouse‐​gas reduction targets had been whittled down.   
  
  
Some of the prizes were relatively small, like the $50 million hurricane research center for a freshman lawmaker from Florida.   
  
  
Others were huge and threatened to undermine the environmental goals of the bill, like a series of compromises reached with rural and farm‐​state members that would funnel billions of dollars in payments to agriculture and forestry interests.   
  
  
Automakers, steel companies, natural gas drillers, refiners, universities and real estate agents all got in on the fast‐​moving action.   
  
  
The biggest concessions went to utilities, which wanted assurances that they could continue to operate and build coal — burning power plants without shouldering new costs. The utilities received not only tens of billions of dollars worth of free pollution permits, but also billions for work on technology to capture carbon‐​dioxide emissions from coal combustion to help meet future pollution targets.   
  
  
That deal, negotiated by Representative Rick Boucher, a conservative Democrat from Virginia’s coal country, won the support of the Edison Electric Institute, the utility industry lobby, and lawmakers from regions dependent on coal for electricity.   
  
  
Liberal Democrats got a piece, too. Representative Bobby Rush, Democrat of Illinois, withheld his support for the bill until a last‐​minute accord was struck to provide nearly $1 billion for energy‐​related jobs and job training for low‐​income workers and new subsidies for making public housing more energy‐​efficient.   
  
  
Representative Joe Barton, a Texas Republican staunchly opposed to the bill, marveled at the deal‐​cutting on Friday.   
  
  
“It is unprecedented,” Mr. Barton said, “but at least it’s transparent.”



This shouldn’t surprise anyone who follows Washington. Still, the degree of special interest dealing was extraordinary. Anyone want to imagine what a health care “reform” bill is likely to look like when legislators finish with it?
"
"Flooding not only wrecks businesses, destroys homes and disrupts everyday life but also causes long-lasting and dangerous levels of stress, residents from flood-hit communities have said. Hundreds of homes have been flooded and six people are thought to have died across England and Wales after heavy downpours and successive storms further exposed the fragility of flood defences and the gravity of the climate emergency.  More flooding is expected as river levels continue to threaten to breach barriers. People in flood-hit areas are grappling with a seemingly relentless onslaught of extreme weather conditions, and are making preparations for the future while having to deal with the present consequences. “I am lucky, only one room of my house was flooded, but some people have had to leave everything,” Amanda Gillender, a 71-year-old painter from Eardisland on the River Arrow in Herefordshire, told the Guardian. The scale of the flooding in Gillender’s village has been unprecedented and there are record levels of water in rivers nearby. She fears life there may become untenable if the climate crisis is not taken seriously. But for now, her most immediate concern is dealing with her insurance claim as she assesses the damage to her 500-year old cottage. “The worst thing about flooding is the waiting and watching as the water rises foot after foot deep,” she said. “There’s nothing you can do. Once it has happened you’re completely isolated because you can’t go out and are left alone with the painful, disruptive aftermath.” Elsewhere, the cumulative stress is having a grave impact on communities. “There are thousands of small human stories of stress, worry and disappointment,” said Rachel Buchanan, from the Ludlow area in Shropshire. “We’re all looking at river level websites now before going out. It just makes everyone a little less willing to leave their home.” People are being prevented from getting to work and social events, caring for family members and attending to livestock, and are becoming ever more lonely, Buchanan believes. “You become slightly more isolated as the effects of the flooding encroach on everyday life,” she said. “A friend of mine who is a single mother simply won’t go out now. Small businesses are losing a day or a week of work and the roads are crumbling – it’s a mess.” All across the UK, flooding has become more frequent and the impact can be felt years later. While insurance agents visit to assess how much compensation is to be paid out, houses in areas not previously classified as being at risk of flooding are revalued. Greenhouse gas emissions from burning fossil fuels, forest destruction and other human activities are trapping heat and putting more energy into the climate system.  Hotter air means heatwaves are much more likely. For example, scientists now say the unprecedented heat and wildfires across the northern hemisphere in 2018 “could not have occurred without human-induced climate change”. In Australia, the scorching summer of 2016-17 in New South Wales was made at least 50 times more likely by global heating, linking it directly to climate change. Hotter air can also carry more water vapour, meaning more intense rain and more flooding.  Another important factor in the northern hemisphere is the impact of changes in the Arctic. The polar region is heating more rapidly, reducing the temperature difference with lower latitudes. There is strong evidence that this is weakening the planetary waves (including the jet stream) that normally meander over Europe, Asia and North America. When these waves stall, weather gets fixed over regions and becomes extreme. This has been linked to past floods in Pakistan, heatwaves in Russia and drought in California.  Most of the planet’s trapped heat goes into the oceans and rising sea temperatures mean more energy for hurricanes and typhoons. Record-breaking cyclones hit Mozambique last year. The deluge delivered in the US by Hurricane Harvey in 2017 was made three times more likely by climate change. Rising sea level also means storms cause more coastal damage. Natural variability would cause some extreme weather, even without global heating, but our impacts on the climate make such extremes more likely. Carbon Brief analysed more than 230 studies and found 95% of heatwaves were made more likely or worse by climate change. For droughts, 65% were definitely affected by our hotter world, while the figure for floods was 57%. With the ‘rapidly accelerating’ likelihood of 40C temperatures in UK, it is now undeniable that global heating is causing more extreme weather. “One of our neighbours lost £70,000 in value,” says Ian Young, a father of two who lives with his partner on the Isle of Man and who was affected by flooding last year. “We lost both our cars and did not receive market value for them. Our premiums went through the roof as they had to go down as at-fault claims even though the vehicles were parked on the driveway and we weren’t in them.” He and his family have been living in temporary accommodation since the beginning of October and work to repair his property, the ground floor of which was completely flooded, has only just begun. “It was absolutely horrific,” he said. “You spend years trying to build up your life centred around your home and garden and then you’re kicked out. We’ve lost a Christmas with our kids. They only believe in Father Christmas until they’re eight or nine. But it was a write-off not having our own home.” In Surrey, where the Thames flooded twice in 2014, residents are still dealing with the consequences. “I don’t think we ever fully recovered from the experience,” said the fluid dynamicist and racing boat maker Carl Douglas. “It knocked the momentum out of us, left us poorer and messed up our business.” Receiving an adequate payout from his insurers proved an “incredibly stressful” ordeal that he knows was not unique to himself, and in 2015 he developed circulatory problems that forced him to have a triple bypass operation. “The surgeon looked at me and said: ‘You look pretty fit for your age so I wouldn’t have expected you to need this procedure. But have you been under a lot of stress?’ ‘Oh yes,’ I replied,” Douglas said. In many cases, communities have pulled together to help those who are most affected. In Hebden Bridge in West Yorkshire, which has been hit by flooding over the last couple of years, neighbours have responded by helping older people move their possessions to safety, sharing route information on Facebook, offering spare bedrooms and helping to flood-proof homes. But people such as Rory Deighton, a 49-year-old NHS worker, are looking towards Westminster for real action. “If we get nothing again from a central government that ignores climate change, does not act on grouse shooting or discuss farming practices, and does not focus on how we can prevent flooding, then people will just ask: what’s the point?”"
"In these times of rising activism on climate change and other environmental threats, a new band of campaigners has joined the fight: street artists. And these artists are using the landscape, communities and social media to spread their message. Banksy, probably the most famous street artist in the world, recently made his views clear through a new piece in Wales featuring a boy under what looks like snow, but is actually pollution from an industrial bin.  Banksy has always been overtly political and controversial, but this was a clear environmental message in an area which is home to one of the largest steelworks in Europe. The image was displayed in the news and on social media across the globe. Through his use of a child, Banksy’s piece echoes the work of another street artist, Ernest Zacharevic, who reached international fame in 2012 with his two street murals little children on a bike and kid on a motorcycle located in the city of George Town, Malaysia. Like Banksy, Zacharevic’s more recent work was also inspired by environmental pollution. In 2015, a haze of pollution made the air in Malaysia almost unbreathable. Zacharevic’s inquiries led him to learn that that the smoke in the region came all the way from Indonesia, caused by unauthorised slash-and-burn techniques used by smallholder farmers to clear the forest to make room for palm oil plantations. The Lithuania-born artist began to take an interest in the palm oil industry and researched the issue for two years by engaging with local people in Indonesia. At the same time, NGO campaigns in the UK and elsewhere in Europe were trying to alert consumers to how the palm oil industry was destroying the environment and abusing human rights. They also began to collaborate with UK investors to engage with the urgent sustainability consequences of deforestation, land conflict and labour conditions, and to advocate for a sustainable palm oil industry. Zacharevic then partnered with the London-based Sumatra Orangutan Society and Indonesia-based Orangutan Information Centre to form the “Splash and Burn” campaign. Its aim was to make people aware of the social and environmental tensions caused by the current practices in the palm oil industry. In 2017, he discreetly invited a team of international street artists to join him in Indonesia to produce haunting public art pieces in remote villages, natural landscapes and towns. Famous figures in the street art scene were eager to participate in what they saw as a much needed demonstration of grassroots art activism. Here, for instance, is noted urban sculpturer Mark Jenkins: Through different creative and sometimes improvised techniques, the street art collective created awareness of the damage caused by unregulated deforestation. Their action was relayed through their Instagram accounts, personal websites, and online press.  Here’s a mural Zacharevic created of an orangutan being chased by fire: In 2018, Splash and Burn took a turn towards “land art” when Zacharevic and his team drew a giant SOS sign on a 124-acre former palm oil plantation in north Sumatra, Indonesia. The land had just been acquired by the environmentally conscious cosmetics company Lush, which raised funds to replant an indigenous forest. The artists also shot a short movie to raise global awareness and to connect artists with civil society organisations.  Street artists are becoming more and more internationally and officially recognised for their environmental work. In 2018, Hawaiian-born artist Sean Yoro, who goes by the artist name Hula, made the Forbes “30 under 30” list for his murals, mostly of female faces being submerged in water. His works raise the question of rising sea levels due to climate change. Street art has typically focused on megacities and urban festivals. But a generation of digitally ultra-connected artists has been encouraged to engage with grassroots campaigners and spread their brushes and spray cans elsewhere – to forests and seas – and to creatively question our relationships to the natural world. Street artists have recently been criticised for “selling out” to big companies for taking on commissioned work, without showing any critical awareness of the social impact of these big companies. Yet these examples of climate activist street art shows artists can actually bring an alternative and responsible message to the public through their work."
"
The idea with measuring climate accurately, is to get as far away as possible from human/urban influences so that those things don’t bias the readings of the thermometer. For example, on my way from Las Vegas to Reno this week, I passed through the near-ghost town of Mina, Nevada, which has a USHCN station. Mina is about as in the “middle of nowhere” as you can be. In fact, the view to the east of the Mina USHCN station is stunning for it’s remote beauty:

According to Wikipedia, Mina has quite a varied range of temperature:
Average July high temperatures range from 61° to 96 °F, with January averaging between 22° and 47 °F. The highest temperature ever recorded in Mina was 110 °F in 1933, with a low in 1990 of –23 °F. Mina receives very little rainfall, and in an average year gets about six inches, with no month getting more than one inch in a normal year. The Mina Airport is at the southeastern end of town.
The USHCN station is at the private residence of the airport operator, who also runs a KOA type trailer/RV park. The airport is a simple dirt strip, so no runway to generate extra heat. I’ve been all over the USA looking at the USHCN network. In almost every station I visit, there’s some sort of surprise. Mina was no exception, and I discovered what Stevenson Screens are really used for:
– as mounts for other weather stations.

In this case, an Oregon Scientifc WMR-968 wireless weather station, which is quite possibly the worst electronic weather station on the market. I once sold these at my online store weathershop.com, and stopped doing so when failure rates started approaching 30% out of the box.
Fortunately, the WMR-968 is not the “primary” instrument of the USHCN station, though it appears to be used as backup for the primary MMTS/Nimbus instrument. In this photo, you can see the wire from the small solar panel running inside to the temperature sensor.
What is most interesting about this station, is that while it truly is in the “middle of nowhere” and has that great “rural” view to the east, the primary MMTS sensor is just a few feet from where all the RV’s park while they register at the office:

Click for a large image
Unlike the Stevenson Screen, The MMTS is just a few feet from the office due to the famous cabling issue. It also has some nice sized rocks to act as heat sinks for those cold desert nights:

View looking North – click for a larger image
Besides the mixture of shade, rock heat sinks, road and building proximity, there’s also the requisite BBQ or two:

View looking south – click for a larger image
The rain gauge also has issues, due to the wind ducting that is likely created by these two trailers:


Click for a larger image
You can see the complete set of photos at the Mina gallery at surfacestations.org
As for the temperature trend:

Data from GISS – see original plot here
The trend is up, curiously, even though the town appears to be dying, so urbanization shouldn’t be the cause. According to NCDC’s MMS database, the station switched from using the Stevenson Screen to the MMTS electronic sensor in 1986. MMTS is well known for building proximity, That may account for some of the trend. There was also a station move to the present location in August 2007.
US 95 is about 100 yards away from the Mina USHCN temperature sensor, so perhaps there is an urbanization component in the form of more traffic.  I simply don’t know.  Interestingly, the station at Bishop, to the west, shows very little long term trend, while the station to the south, Tonopah, does show a trend. But Tonopah is growing, unlike Mina, which is dying and is now listed on a Ghost Town forum. Tonopah also had it’s weather station converted to ASOS, which when combined with other airport improvements, tends to add a positive trend.
So it’s a puzzle, and I welcome comments with ideas.
The thing about the Mina station though, is that without knowing a site history and history of the surrounding changes, we simply don’t know how much of the signal is real or from land use changes around the sensor. In it’s new position at an active RV park, it is now in a dynamic environment within feet of daily vehicular traffic. We simply should not have to figure such things out for a climatic reference station, even if it is in the “middle of nowhere”.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b9586de',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The hallmark of liberalism has long been its perceived commitment to individual rights. But not this administration. Bill Clinton is a new kind of Democrat–a jackboot liberal.



While the president has been out lobbying weather forecasters about the alleged threat of global warming, his interior secretary, Bruce Babbitt has been attacking energy companies for criticizing administration scare‐​mongering. Mr. Babbitt charged the firms with attempting “to distort the facts and to mislead,” adding: “I think that the energy companies need to be called to account, because what they are doing is un‐​American in the most basic sense.” He left unsaid how he would call such “un‐​American” businesses “to account,” but climate scientists report that the administration has long used its control of grants to punish researchers who question the climatic Chicken Littles.



Mr. Babbitt’s implicit threat unfortunately reflects the administration norm. In many cases, Clinton officials have directly targeted critics. More generally, warns Timothy Lynch, assistant director of the Cato Institute’s Center for Constitutional Studies: “Although President Clinton has expressed support for an ‘expansive’ view of the Constitution and the Bill of Rights, he has actually weakened a number of fundamental guarantees.”



The administration has politicized the FBI, using it to justify the White House Travel Office purge. Presidential aides snooped through FBI files on potential administration opponents. The IRS is auditing not only Paula Jones, who has accused Bill Clinton of sexual harassment, but a suspiciously large number of conservative foundations and groups. No liberal organizations are undergoing similar reviews. The White House pressured the Treasury Department over the latter’s probe of Madison Guaranty, which financed the Clintons’ Whitewater investment.



Early in the first Clinton term, the Department of Housing and Urban Development launched dozens of investigations of local activists who opposed federally subsidized housing projects. HUD subpoenaed copies of organization membership lists and financial information, people’s diaries, and other records, demanded cessation of public criticism, and threatened protestors with prosecution for speaking out.



Similarly, in 1995 the U.S. Commission on Civil Rights issued subpoenas to leaders of two anti‐​immigration groups. The commission, whose chairman and staff director were appointed by President Clinton, wanted computer printouts, internal documents, reports and other information. Both HUD and the commission retreated only under public pressure.



The Justice Department supported draconian restrictions on abortion protestors, including a prohibition on the display of any “images” that could be “observed” within abortion clinics. The Defense Department attempted to gag millitary chaplains, preventing them from discussing the Catholic Church’s Life Postcard Campaign regarding the president’s veto of legislation banning partial‐​birth abortion. More recently, the administration has threatened to prosecute any physician who provides a prescription for marijuana under state law.



Intimidation has been a persistent administration tactic elsewhere. In 1994, President Clinton expressed outrage that radio talk show host Rush Limbaugh could get on the air and “have three hours to say whatever he wants. And I won’t have an opportunity to respond.” White House Communications Director Mark Gearan called for radio talk shows to put on opposition–meaning administration guests. Senior adviser George Stephanopoulos suggested resurrecting the misnamed “Fairness Doctrine” to be enforced by the Federal Communications Commission, to regulate political broadcasts.



The Energy Department created a press rating system. Reporters and sources were judged based on their opinion of the department. Department press secretary Barbara Semedo explained that a low rating “meant we weren’t getting our message across, that we needed to work on this person a little.” Of course, getting the message meant spouting the department’s line.



Advertising, too, has been an administration target. The Food and Drug Administration even sought to prohibit the use of brand names on non‐​tobacco products (such as lighters and T‐​shirts) and the use of non‐​tobacco brand names on tobacco products. The administration supported labeling restrictions, deemed unconstitutional by the Supreme Court, on beer producers. The president backed FCC Chairman Reed Hundt’s campaign to bar the advertising of distilled spirits on television.



“The Clinton civil liberties record is breathtaking in both the breadth and the depth of its awfulness.”



The administration supported the Communications Decency Act, which would have attempted to ban the transmission of “indecent” materials over the Internet. Although well‐​intentioned, the law, voided by the Supreme Court, would have meant heavy‐​handed censorship of today’s least regulated communication medium.



Although President Clinton has spoken of reforming affirmative action, his administration promotes it with a mailed fist. Perhaps the ugliest episode was his Justice Department’s support (recently reversed) for the Piscataway, NJ., school district that fired a teacher because she was white. The Education Department responded to California’s passage of Proposition 209 by threatening to prosecute the university system for dismantling its racial spoils program.



Within the administration “diversity” has become a code word for preferential treatment. HUD requires that employees not only implement federal diversity policy, but demonstrate “interest” and “personal commitment” to diversity, be active in “minority, feminist or other cultural organizations ” and participate in “cultural diversity activities outside of HUD.” The Agriculture Department reassigned an employee for criticizing, on his own time, the department’s policy of offering spousal benefits to same‐​sex partners.



But the harshest examples of jackboot liberalism have come from the Justice Department and federal law enforcement agencies. The Branch Davidian and Randy Weaver cases continue to stand as examples of government run‐​amok, persecuting people who wanted little more than to be left alone. The administration’s response to the Oklahoma City bombing was to impose sweeping new powers, such as restricting the right of habeas corpus and expanding the use of wiretaps, for itself, even though the president was unable to point to a single example where civil liberties protections had hampered efforts to combat terrorism.



The administration, the most wiretap‐​friendly in U.S. history, has sought to eliminate Fourth Amendment protections against government searches. The president claims to possess “inherent authority to conduct warrantless searches for foreign intelligence purposes.” The administration requires public housing residents to sign away their constitutional rights. The Justice Department backed warrantless (indeed, suspicionless) drug tests for high school athletes. The administration has requested greater FBI authority to conduct “moving wiretaps” without a court order. President Clinton pushed the Communications Assistance Act, which requires telephone companies to retrofit their systems to ease police surveillance, supported restrictions on the sale of Internet encryption technology, and requested legislation forcing firms to give the government the “keys” to such technology.



No squishy, compassionate liberal he, the president has sought to thwart Arizona and California voters who approved measures to allow the desperately ill–victims of AIDS and cancer, in particular– from using marijuana to ease their nausea and pain. Mr. Clinton responded to criticism that sellers of crack were being punished far more severely than those who peddled cocaine by arguing that penalties against the latter–which already ensure that minor drug dealers spend more time in jail than do many armed robbers, rapists, and murderers–should be raised. (He recently suggested a mode move in the other direction, reducing the differential from a hundredfold to tenfold.)



The administration also throws people in jail for resisting federal designation of their (very dry) property as “wetlands,” and committing other environmental offenses. In 1994, the Justice Department relaxed its control of environmental prosecutions in order to allow individual U.S. Attorneys greater latitude in prosecuting business. Of course, the department still retains the right to proceed if a local U.S. attorney refuses to bring charges.



Any particular presidential decision can be defended on one ground or another, but as Wired magazine’s John Heilemann observes, the Clinton civil liberties record is “breathtaking in both the breadth and the depth of its awfulness.” And there’s more– proposed curfews for kids, support for random drug tests of welfare recipients and kids seeking drivers licenses, attacks on the requirement of a jury trial, ex post facto tax increases, attempts to gain court sanctions for uncompensated property takings, prosecution implicating the double‐​jeopardy clause, pretentious claims of federal criminal jurisdiction, and infringements of the Second Amendment right to possess a firearm. Mr. Lynch details these and more in his devastating study, “Dereliction of Duty: The Constitutional Record President Clinton.”



Administration spokesmen argue that the president is carefully balancing rights and liberties. It’s not balance President Clinton wants, however. It’s power. There was a time when Democrats were genuinely liberal. No longer. The American people are paying for Bill Clinton’s philosophy of jackboot liberalism with their freedom.
"
"
by John Goetz
In what seems to be a script straight from a Monty Python classic, the good folks of Santa Coloma de Gramenet in Spain seem to have found a rather novel use for the dead: as a tool in the fight against global warming.
From the TimesOnline
November 28, 2008
by Graham Keeley in Barcelona
Spanish graveyard new front in the fight against global  warming
Solar panels are  installed in cemetery

Solar panel in Santa Coloma
A graveyard in Spain has become an unlikely front in the fight against global  warming, with hundreds of black panels placed on top of mausoleums providing  year-round power for homes.
The 462 panels produce 124,374 kilowatts of electricity, enough to supply 60  homes for a year in Santa Coloma de Gramenet, near Barcelona. The exorbitant  price of land in the densely populated satellite city inspired a solar energy  company to propose using one of the last remaining available plots of land – the  cemetery.
Conste-Live Energy and the local council spent three years persuading  relatives of those interred and near-by residents that the unusual proposal  would benefit the living without demeaning the dead. “The best tribute we can  pay to our ancestors is to generate clean energy for new generations,” Esteve  Serret, a company director, said.
The panels cost €720,000 (£612,500) to install and each year will keep about  62 tonnes of carbon dioxide out of the atmosphere, Mr Serret said.
“This is not much, but it will do something to help combat global warming,”  said Bartomeu Muñoz, the Mayor of Santa Coloma. The glinting blue-grey panels  are fixed on top of mausoleums, which in Spain hold five layers of coffins.
The panels, which face south to soak up maximum sunshine, were turned on last  week after three years of planning. Santa Coloma is so densely populated that  all 124,000 inhabitants live within a 4sq km area. Putting solar panels on  coffins was a tough sell, said Antoni Fogué, a city councillor. “Let’s say we  heard things like, ‘They’re crazy. Who do they think they are? What a lack of  respect’,” he said.
City hall and cemetery officials waged a public awareness campaign to explain  the worthiness of the project and the painstaking care with which it would be  carried out. 
Eventually they won over doubters, Mr Fogué said. The panels were erected at  a low angle to be as unobtrusive as possible. “There has not been any problem  because people who go to the cemetery see nothing has changed,” Mr Fogué said.  “This installation is compatible with respect for the deceased and for the  families of the deceased.” 
The cemetery holds the remains of 57,000 people. The solar panels cover less  than 5 per cent of the total area. Community leaders hope to erect more panels  and triple output. Santa Coloma has four solar parks, but the cemetery is the  biggest and the first to attach panels to graves.
When I read this I suddenly recalled the infamous “Bring out yer dead” scene from Monty Python and the Holy Grail:
The Dead Collector: Bring out yer dead.
Large Man with Dead Body: Here’s one.
The Dead Collector: That’ll be ninepence.
The Dead Body That Claims It Isn’t: I’m not dead.
The Dead Collector: What?
Large Man with Dead Body: Nothing. There’s your ninepence.
The Dead Body That Claims It Isn’t: I’m not dead.
The Dead Collector: ‘Ere, he says he’s not dead.
Large Man with Dead Body: Yes he is.
The Dead Body That Claims It Isn’t: I’m not.
The Dead Collector: He isn’t.
Large Man with Dead Body: Well, he will be soon, he’s very ill.
The Dead Body That Claims It Isn’t: I’m getting better.
Large Man with Dead Body: No you’re not, you’ll be stone dead in a moment.
The Dead Collector: Well, I can’t take him like that. It’s against regulations.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a73ec94',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The life story of any animal involves daily struggles and triumphs, twists and turns – and each individual has its own unique narrative. The new Life Story series from the BBC Natural History Unit shows in intimate detail both familiar and rarely seen examples of such life-determining events. Breath-taking footage provides viewers with the highest quality images ever seen of animals experiencing the challenges involved in making their way through life towards the ultimate achievement, leaving offspring. That, as narrator David Attenborough puts it, is “the next best thing to immortality”. We were the Open University consultants to the series, which covers everything from bower birds and cheetahs to weaver ants and elephants. While it helped us to appreciate just how much television can now allow viewers to experience the daily lives and struggles of wild animals, we all still know very little about the basic biology and survival needs of many of the animals we share the planet with.  Often it is the most recognisable animals that people actually know the least about, which is where David and I come in. We’re educational experts, as well as biologists specialising in reproduction and how animals use sound.  African elephants feature in Life Story but there are now thought to be two, not one living species of these (the bush elephant and the forest elephant). The series follows a family group of bush elephants in south Kenya. It tells the story of a new mother who is supported by the older more experienced mothers, but has to keep up with the herd as they travel.  There is a fascinating moment where they pass elephant bones and take the time to pause, exploring them with their trunks. The fact that they stop despite their need to keep moving, probably for food or water, and that the bones are important enough to stop and explore provides an insight into the intimate behaviour of these creatures.  There is also a third species that does not appear in the series: the Asian elephant. Unlike African elephants, which are mostly wild, people have since the third millennium BCE attempted to “domesticate” these slightly smaller animals. But it has always been hard to get them to breed in captivity, which sets them precariously apart from truly domesticated species such as dogs, chickens, sheep and cows that generally reproduce so well that they outnumber their wild counterparts.  Elephants are large, long-lived, travel far – and females have strong social bonds. These complications help to explain why captive Asian elephants have never become truly domesticated and their populations are endangered. Research conducted back in 1998 confirmed that captive Asian elephants breed less successfully than those with access to wild populations in a comparison with semi-captive environments including a logging camp in Myanmar.  Since then, managers have been slightly more successful at breeding these elephants in captivity. But this may be down to developments in artificial insemination and sperm preservation rather than any increases in our knowledge about the complicated reproductive biology of elephants, their social behaviour or management needs. Elephants are known to communicate with each other through smell, sight and touch, but their ability to use sound in different ways is particularly interesting. They produce rumbling sounds through the folds in their larynx. These rumbles are structurally very varied, which means that all sorts of social information could be exchanged.  The sounds are very low-frequency and travel well through air. Elephants have been observed to respond to sounds from another elephant 2.5 km away. We know quite a lot about the characteristics of the sounds, but their precise role in the social structure of different elephant groups is still not well understood. This takes us back to Life Story and the elephants stopping to explore the remains of a fellow traveller: is this because of curiosity, or a deeper social meaning? The long gestation period of the programme (four years in the making) surpassed that of even the Asian elephant (up to 22 months). When you work with such animals, you get used to things taking a long time. And trainee researchers need to learn how to study for long periods to be able to extract new findings from observations. Students get a taste of this by studying elephant behaviours and interactions they are unlikely to have seen before, that take place during an intimate glimpse of how these magnificent animals go about their daily business within their own life stories for up to four hours on specially recorded footage from Woburn Safari Park in Bedfordshire.  Soon after the elephant study filming was completed, Damini, a 20-year-old female successfully produced a healthy female calf. This followed artificial insemination with sperm from Raja, the resident male, in another success at breeding Asian elephants in captivity. As in the new BBC series, another life story begins."
"

 **A Review of _Reality and Rhetoric: Studies in the Economics of Development_ , by P.T. Bauer, Cambridge, Mass.: Harvard University Press. 184 pp. $15.00**



Professor Peter T. Bauer, of the London School of Economics, is one of those intellectually heroic figures who has stood fast against the fads and hysteria of his time. While the vast currents of “development economics” inundated us with “overpopulation” theories and “vicious cycle of poverty” doctrines that depicted massive foreign aid as the only salvation of the Third World, Bauer said “No!” loud and clear—but virtually alone.



Despite his scholarly achievements and personal experience in the underdeveloped world, Bauer was long ignored or disparaged as he poked holes in the prevailing orthodoxy. Today, he can no longer be ignored‐​not even in academic and media circles where the prevailing orthodoxy was once treated as the one true faith. Bauer’s message has begun to be heard, not only because of his own perseverance and insights, but also because the repeated failures and massive disasters of “development planning” have finally broken through the smug unanimity that long substituted for evidence or critical analysis.



 _Reality and Rhetoric_ is a compilation of Bauer’s essays over the years on such topics as foreign aid, “planning” versus markets in the Third World, imperialism, and the moralistic pronouncements of the clergy on the international economic order. These essays are written and reasoned in a very straightforward way. It is enough to make you forget that he is an economist.



Bauer’s criticisms of current thinking about Third World nations are both wide and deep. He questions the very concept of “foreign aid” or “the third World.” Whether international transfers of money to the less‐​developed nations are an aid or a hindrance to their economic progress is for Bauer a question rather than a foregone conclusion. His own reading of the evidence is that it has hindered more than it has helped.



The tremendous range of extremely different nations lumped together as “the Third World” likewise makes no sense to him. All that these nations have in common is that they receive “foreign aid.” A few of these recipient nations have even had higher per capita income than some of the donor nations. Most—if not all—of the poorer nations have classes of people who are more affluent than the average Western taxpayer in the donor nations—and it is precisely those affluent people who have the inside track in getting their hands on the foreign aid.



Bauer is not a mere Scrooge who says “Bah! Humbug!” to the poor. On the contrary, his vision of the world accords far more respect to the less‐​developed regions and peoples than does the conventional viewpoint. Bauer denounces the “contempt for ordinary people” that underlies development planning. 



Drawing on his own many years of research and observation, he punctures the idea that Third World people can progress only under the tutelage of foreign experts or their own westernized elite. Evidence to the contrary, he notes, is found in “the large scale capital formation in agriculture by the local people” in West Africa; the fact that over half the acreage planted with rubber trees in southeast Asia was owned by Asians, even before World War II; and large‐​scale international migrations by poor and illiterate people who were nevertheless “well informed about economic conditions in distant and alien countries.” 



Dramatic economic changes over time likewise belie the stereotypical picture of hopelessly stagnant peasants needing foreign “experts” or … So much for the notion that Third World masses cannot think beyond today.



Bauer also recognizes “the reality and importance of group differences” within the population of a given nation, even though this subject “is virtually proscribed in the profession.” Particular segments of the population of very poor and backward nations often have people who are entrepreneurial, hard‐​working, thrifty, and with great initiative and imagination. Far from making use of such people for advancing the economic level of the country, many Third World governments devote great efforts to stifling or even expelling such groups, especially when they are racial or ethnic minorities whose prosperity is envied and resented by others. The Chinese in Southeast Asia, the Indians in East Africa, the Lebanese in West Africa, and the Jews historically in Eastern Europe are only some of the more‐​prominent examples of this very widespread phenomenon.



Early in his career, Bauer was struck by these inter‐​group differences, which were largely ignored by other development economics: “The differences in economics performance and hence in achievement among groups were immediately evident, indeed startling.” Unskilled plantation workers in Malaya, working with primitive implements, nevertheless differed in output by a ratio of two‐​to‐​one as between Chinese and Indian workers, though both were “undedicated coolies.” Differences in other occupations‐​especially entrepreneurial occupations‐​are even greater.



Contrary to the prevailing egalitarian ethic, Bauer declares that “differences in incomes and rates of progress and regions…are not reprehensible. They are inevitable.” Egalitarianism is to Bauer simply the “legitimization of envy.” He rejects “the notion that the well‐​off have prospered at the expense of the poor” and calls it “the most pernicious of all economic misconceptions.” Implicitly, it assumes a zero‐​sum world, in which A gains only at the expense of B, turns attention away from the central issue of how to increase total wealth. Throttling the production of wealth, in the name of equality, is not humanitarianism but moralistic self‐​indulgence. So is guilt. Bauer regards “guilt in the West toward the Third World” as “a feeling which does nothing to assist the ordinary people” of the poorer countries.



If your purpose is to understand economic development in the poorer nations, you cannot get a better brief introduction to the subject than in Reality and Rhetoric. If your purpose is to learn the latest fashions in theories and buzzwords, this is not the place. “Statement of the obvious,” Bauer says, “has become a major task,” in part because “prominent economists have perpetuated the grossest elementary transgressions of fat and logic.” Words like infrastructure and phrases like the vicious cycle of poverty have created reputations and programs, even as they have soared above reality and left disaster after disaster in their wake.



Bauer not only mentions some of these disasters but points out how “foreign aid” subsidizes them. The international aid organizations’ emphasis on “need” in general and short‐​run crisis management in particular means that poor nations that have behaved responsibly, and lived within their means, are far less likely to get money than governments that have spent lavishly, engaged in grandiose social and economic experiments, and run up huge foreign debt without any concern for how—or whether—they would pay it off. Bauer is not afraid to call this “preferential treatment of the incompetent, the improvident or dishonest.”



In its effects on national well‐​being, the difference between responsible and irresponsible government is seen by Bauer as far more important than the sums transferred by international aid organizations. Insofar as these transfers reward counterproductive government policies, the losses they engender may readily exceed any benefits they can purchase. The sums involved in these international transfers are often not very large relative to total national output but are very impressive as a percentage of government discretionary spending. Therefore their effect on government policy may be very large—and very counterproductive—while they directly add relatively little to the available resources of the economy.



In India, for example, foreign aid in 1980 amounted to less than 2 percent of gross national product (GNP), but it was 18 percent as large as the government’s total tax receipts. In Tanzania, foreign aid was 18 percent of GNP and slightly larger than all taxes received by the government.



In short, foreign aid greatly increases the recipient government’s economic leverage in the economy. In addition, international development agencies tend to be biased toward statist policies, both inherently and as a matter of choice. Inherently because it is, after all, governments that receive both financial resources and the advisory personnel provided by the international development agencies. Moreover, “many staff members of the international organizations favour dirigiste policies (state economic planning),” according to Bauer.



“The international aid organizations and their staffs are not disinterested,” Bauer points out, but instead have heavy personal and institutional stakes in a large and growing amount of foreign aid. These aid organizations are politically active and effective in the Western nations. Their version of the world economic picture is constantly fed through the media to the public as the only humane and decent way to see it. They have patronage to offer academics in the form of jobs and consulting arrangements. At the same time, these international bureaucratic empires are dependent on the Third World nations to accept their aid—and often express fears that the aid would be refused if various conditions were attached to ensure responsible behavior by the recipients.



The moral climate generated by Western intellectuals—including the media and the clergy‐​is one of the key ingredients in the political success of this process of draining money from Western taxpayers for the benefit of Third World ruling classes and international bureaucracies. Guilt is one of the factors in this moral climate.



The idea that the poverty of some nations is caused by the affluence of other nations is taken as axiomatic in many quarters. Bauer, however, treats this notion as a hypothesis instead of an axiom and looks at the evidence. He finds that in fact poverty and backwardness are greatest in those Third World nations that have been least touched by Western imperialism, trade, or multinational corporations‐​for example, Ethiopia and Liberia in Africa, and Bhutan, Sikkim, Tibet, and Nepal in Asia.



Far from deferring to the moral authority of politically active clergy, Bauer characterizes their arguments as “immoral because they are incompetent.” He says: “There is profound truth in Pascal’s maxim that working hard to think clearly is the beginning of moral conduct.” Bauer sees these activist clergy as “seeking a new role for themselves in the face of widespread erosion or even the collapse of traditional beliefs.” Their susceptibility to any idea that calls itself “social justice” he regards as symptomatic of a lost religious faith that finds a substitute in secular credulity.



Professor Bauer is no longer alone, though he is still vastly outnumbered by those with a vested interest in the foreign‐​aid status quo. This book will make it harder for them to continue to pull the wool over the eyes of the taxpaying public.
"
"Plans for a new £600m “floating” cycle route along the edge of the River Thames in London have been announced. It is expected to stretch for around seven miles from Battersea, west of the city centre, to Canary Wharf, the financial district to the east. The idea has been put forward by the River Cycleway consortium, a group of architects, engineers and artists. The construction is expected to rise and fall with the river tides and to have a number of access points along the route. The “motion” of the cycleway will be used to generate energy to power lighting. It is expected that at least some of the funding will come from private finance – a charge, expected to be £1.50 per trip, will be used to cover ongoing maintenance costs of the infrastructure. In a large, congested city, the Thames stands out as an under-used transport corridor and local authorities want to encourage cycling. Such an expensive and high-profile scheme would help give the idea that cycling is being treated seriously as a mode of travel. So, on the face of it, this sounds like a great idea.  The proposal, however, raises a number of issues and worries, which the plans as published do not seem to adequately address. There does not appear to have been any serious attempt to study demand – who would use it and what sorts of numbers might be expected on a daily basis? In practice it seems the main focus may be on leisure: tourists and Sunday trippers who wish to see the sights of London from an unusual perspective. Will it really appeal to the daily commuter cyclist, especially given the costs of use? There is a major question mark over the cost; £600m seems very expensive for just seven miles of cycleway. Road-based cycleways typically cost significantly less than this and arguably give far greater value for money.  London is already a growing cycling success story. The capital has more cyclists than ever and has received considerable investment in cycling infrastructure, a public bicycle scheme (the so-called “Boris Bikes”) and support for cycling goes right to the top of City Hall. Focusing investment once again on the capital isn’t really the best and most equitable use of such a considerable sum of money. Compare the £600m with the £77.2m invested by the government in eight cities outside of London (with a further local contribution of £45.4m) through the Cycle City Ambition grants.  The £600m might be better spent elsewhere in the country, or even perhaps in London’s suburbs, areas where fewer people cycle and where dedicated cycle infrastructure is poor or non-existent. In the UK, aside from a handful of urban areas which perform as well or better than London, much of the country has lower levels of cycling and could benefit from investment. In any case, is this the kind of infrastructure cyclists really want? Recent research has clearly shown that cyclists do not fit neatly into a single category and that their views and ideas for what works best for them cover a wide spectrum. A single, expensive and very geographically focused piece of infrastructure is unlikely to appeal to large numbers and, more importantly, is unlikely to be of practical use to many cyclists.  Perhaps the proposers should start to talk with cyclists and those who currently do not cycle (but might be persuaded to do so) to gain a better understanding of what they want. The answer is likely to be rather more mundane: better cycle paths, lighting, signposting and possibly further controls on aggressive driving.  This isn’t to argue against thinking big – major investments of this type could have a huge and positive impact for many people. But they would have to be focused on something that helps many people, not just the lucky few who fancy a scenic trip to Westminster."
"
Magicians and Illusionists Penn and Teller have a popular TV show on the Showtime channel called, ahem, “Bullshit”. In homage to their debunking mentor, James Randi, they take on a number of subjects they feel could use a little “clarity”.

Click image to watch the video
They recently (last Thursday night) took on Al Gore and carbon credits. The entire 30 minute show is available via the website VREEL (update You Tube has it now, VREEL started installing  Zango a couple of days ago – a spyware) 
See YouTube Part1 Part2 Part3

Warning: more than a few obscenities are uttered in the show, but mostly for comic effect.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9de644e4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterMilutin Milankovitch
Milankovic Cycles and Climate Change
Is it distance from the sun, or length of summer?
By Ed Caryl
A draft paper by Dr. James Hansen and Dr. Makiko Sato triggered a rebuttal by Dr. Martin Hertzberg on WUWT. The Hansen paper made a claim that weaker insolation in the Northern Hemisphere (NH) due to distance from the sun in NH winter should lead to cooling, but that this is offset by increasing CO2.
The Dr. Hertzeberg rebuttal claimed that the warming was due to the longer length of summer in the NH. Both are wrong!  Both are victims of Confirmation Bias, seeing only data that confirms their beliefs.
It seemed a little strange that the Hansen paper was written for a conference in Milankovic’s honor, yet the paper does not contain the famous diagram illustrating the Milankovic Cycle (or Milankovitch, both spellings are common in the literature), nor does the Hertzberg rebuttal. Here it is:

Figure 1. Source: Wikipedia Commons
The important part of this chart, and the reason both authors are wrong, is the black plot in the lower center of the chart, (that part of the plot is shown again below) the insolation per day at °65 N latitude. If the scale of this plot were anomaly, instead of absolute value, the anomaly would be slightly negative. The insolation change due to the length of summer and the distance from the sun are almost exactly canceling, and the insolation is not going to change very much for the next 20,000 years.

Figure 2. The blue dot is the current date. Source: Wikipedia Commons


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The position in our orbit where we are closest to the sun is called the parahelion point. The tilt of the earth’s axis is related to the parahelion by a relationship known as the “longitude of the parahelion”. The °360-°0 point is where the Vernal Equinox (1st day of Spring) and the parahelion coincide. If you divide the °360 by 4 (=°90) you see that the parahelion will be in spring from °0 to °90, summer from °90 to °180, autumn from °180 to °270, winter from °270 to °360. Right now we are at °283 and the parahelion happens on the 3rd of January.

Figure 3. Plot of the Longitude of the Parahelion point. Plotted from NASA data. The year 2000 is marked by the blue square. You can see that in the year 12,000, 10,000 years from now, we will be closest to the sun in NH mid-summer.
Will this lead to warming in mid-summer? No, because the earth reaches a point of minimum axial tilt at that time, and rapidly decreasing orbital eccentricity (as eccentricity reaches minimum, the orbit approaches a perfect circle) will make the reduced distance from the sun in summer much less important. See the next chart. Again, the current date is the blue squares.

Figure 4. Plot of Obliquity (Earth axial tilt) and orbital eccentricity (deviation from a circle). Plotted from NASA data obtained here.
When will the next ice age be?
One of the comments to the Hertzberg article in WUWT asked, “When will the next ice age will begin?” Answer, not for a while yet. From Wikipedia: “An often-cited 1980 study by Imbrie and Imbrie determined that, ‘Ignoring anthropogenic and other possible sources of variation acting at frequencies higher than one cycle per 19,000 years, this model predicts that the long-term cooling trend which began some 6,000 years ago will continue for the next 23,000 years.’[16]”
All these orbital cycles are much too long and gradual to account for late 20th Century warming. That warming is due to short-term solar output changes, solar magnetic field influence on galactic cosmic rays changing cloud cover, and ocean cycles.
You can see in Figure 2 that the next negative swing in NH insolation is over 50 thousand years in the future, and the next really negative swings are over 100 thousand years off. Even if we cool for the next 23,000 years, it should not get cool enough to trigger an ice age… unless the sun does something really strange.
Share this...FacebookTwitter "
"

_Washington Post_ political reporter Colby Itkowitz writes:



During floor debate ahead of a vote on the Green New Deal, Sen. Mike Lee (R‐​Utah) told his colleagues that if they really want to address environmental concerns they’ll encourage people to couple off and have more babies. … This recommendation, to add more people to the planet, doesn’t track with science or reason. A 2017 research article determined that one way an individual could contribute to eliminating greenhouse gases is to have one fewer child.



That’s the nut from her snide web article, “Sen. Mike Lee Says We Can Solve Climate Change with More Babies. Science Says Otherwise.” _Post_ national correspondent James Hohmann deemed the article noteworthy enough to be the “Hot [Read] on the Left” in his “Daily 202″ newsletter.



Problem is, Itkowitz seems to not understand the point Lee was trying to make. Instead, she “talks past” him — something all‐​too‐​common in politics, but not something reporters should do. Worse, if history is a guide, her view is more likely to prove a‐​scientific and unreasonable than his.



Here’s the story: Last Tuesday, the Senate held a floor debate and vote on the so‐​called “Green New Deal” — which is to say the Senate engaged in silly Republican grandstanding over a silly Democratic proposal. As part of the debate, Lee delivered a floor speech that featured everything from a picture of a machine‐​gun‐​firing, bazooka‐​toting Ronald Reagan riding a velociraptor waving an American flag (no, really), to references to _Star Wars_ tauntauns and the Hanna‐​Barbera Aquaman’s giant seahorse, to _Sharknado 4_ and Austin Powers’ Dr. Evil. That is, Lee met double‐​silliness with more silliness.



One can argue that this was inappropriate for a Senate debate, especially on a serious topic like climate change. But then, comedy can be an effective means to truth.



Toward that end, at the conclusion of his speech Lee offers a serious point. Itkowitz selectively quotes it; here’s the complete section:



The Green New Deal is not the solution to climate change. It’s not even part of the solution. It’s part of the problem. The solution to climate change won’t be found in political posturing or virtue signaling like this. It won’t be found in the federal government at all.  
  
You know where the solution can be found? In churches, wedding chapels, and maternity wards across the country and around the world. This, Mr. [Senate] President, is the real solution to climate change: babies. Climate change is an engineering problem — not social engineering, but the real kind. It’s a challenge of creativity, ingenuity, and technological invention. And problems of human imagination are not solved by more laws, but by more humans.  
  
More people mean bigger markets for innovation. More babies mean more forward‐​looking adults — the sort we need to tackle long‐​term, large‐​scale problems. American babies, in particular, are likely going to be wealthier, better educated, and more conservation‐​minded than children raised in still‐​industrializing regions. As economist Tyler Cowen recently wrote on this very point, “by having more children, you are making your nation more populous — thus boosting its capacity to solve [climate change].“  
  
Finally, Mr. President, children are a mark of the kind of personal, communal, and societal optimism that is the true prerequisite for meeting national and global challenges together. The courage needed to solve climate change is nothing compared with the courage needed to start a family. The true heroes of this story aren’t politicians or social media activists. They are moms and dads, and the little boys and girls they are — at this moment — putting down for naps, helping with their homework, building tree houses, and teaching how to tie their shoes.  
  
The planet does not need us to “think globally and act locally” so much as it needs us to think family and act personally. The solution to climate change is not this unserious resolution, but the serious business of human flourishing — the solution to so many of our problems, at all times and in all places: fall in love, get married, and have some kids.



No doubt, the Utah senator’s comments were, at least in part, his own virtue‐​signaling to his predominantly Mormon constituency. But he draws on an important economic idea: at the margin, human beings have a positive effect on the world. Human ingenuity, hard work, preferences, and values create goods, and among those goods can be improved environmental quality. Julian Simon popularized this idea in his 1981 book _The Ultimate Resource_ , and it has been restated in recent years by, among others, Cowen in his new book _Stubborn Attachments_ (reviewed by Pierre Lemieux here), Bryan Caplan in his 2011 book _Selfish Reasons to Have More Kids_ , and my Cato colleague Marian Tupy and BYU economist Gale Pooley in their 2018 paper on the “Simon Abundance Index.”



Itkowitz claims “science and reason” say different. To justify that, she links to a 2017 paper that actually _doesn’t_ determine “that one way an individual could contribute to eliminating greenhouse gases is to have one fewer child,” but rather examines Canadian high school science textbooks’ recommendations for reducing greenhouse gas emissions. A possible strategy — one that _none_ of the textbooks recommend, which the paper’s authors lament — is to “have one fewer child.” That paper, in turn, cites a 2009 paper that estimates the carbon emissions resulting from an offspring, including a share of the subsequent emissions of that offspring’s descendants. Not surprisingly, that’s a big number, dwarfing the carbon‐​reduction benefits of such common strategies as recycling, switching from gasoline‐​powered cars to hybrids and electric cars, and upgrading lightbulbs. (My takeaway from the 2017 paper is how pointless many of the commonly advocated carbon‐​reduction strategies are.)



The 2009 paper is a mathematical modeling exercise under various assumptions, resulting in different estimated marginal “carbon legacies.” But that doesn’t show Itkowitz is right and Lee’s being foolish because the paper ignores Lee’s point about the effects of population change on innovation and living standards.



From tin shortages in the ancient world, to William Strong Newberry’s 1875 (yes, **18** 75) warning that the world was running out of oil, to Paul Ehrlich’s _Population Bomb_, to Jimmy Carter’s moral equivalent of war, population growth has placed humanity on the Malthusian edge of poverty and privation — or so we keep being told. But we never fall off that edge. In fact, we keep moving away from it: we grow fatter (alas), longer‐​lived, and more comfortable. The reason is simple: more people means more innovation and resource availability, which means a higher standard of living rather than the opposite.



That doesn’t mean humanity is guaranteed to find some easy, innovative way to cut greenhouse gas emissions. As my Cato colleague Peter Van Doren noted, both Lee’s optimism and Itkowitz’s pessimism are attitudes (probably correlated to their politics) rather than testable, scientific hypotheses. That said, history suggests it’s more likely that humanity will find innovative ways to cut emissions, or geo‐​engineer around climate change, or accommodate change, than that reducing (or government constraining) population growth will save us from a much warmer world — or that there will be no future environmental quality innovations.



All that said, some legitimate criticisms can be made of Lee’s remarks. Government _can_ be a useful tool for addressing externalities, just as it can also be a terrible tool. And there are plenty more examples of people showing the “courage” to start families than there are of policymakers showing the courage to address difficult policy problems (e.g., entitlements insolvency, government debt, pork and rent‐​seeking, better childhood education…) Those criticisms would have made good reading, as would a broader discussion of Malthus and neo‐​Malthusianism, government intervention, and population change. Unfortunately, instead of that, readers got was the equivalent of the aforementioned Reagan–velociraptor picture.
"
"Ten years ago we witnessed one of the worst natural disasters in history, when a huge earthquake off the coast of Sumatra triggered a devastating tsunami which swept across the Indian Ocean.  An estimated 230,000 people lost their lives, and 1.6 million people lost their homes or livelihoods.  The impact was greatest in northern Sumatra because of its proximity to the earthquake. Catastrophic shaking was followed within minutes by the full force of the tsunami.  Thousands of people were also killed in distant countries, where the earthquake could not be felt. If they had received a warning of the approaching tsunami, they could have moved inland, uphill or out to sea, and survived. Tsunami take several hours to cross an ocean, becoming much larger and slower as they reach the coast. Back in 2004 there were long-established tsunami warning systems in the Pacific Ocean, which has many subduction zones – places where two tectonic plates collide – capable of generating huge earthquakes or volcanic eruptions.  Other regions, including the Indian Ocean, did not have a warning system. The probability of a major tsunami was judged to be too low to justify the cost, especially for poorer countries.  The Boxing Day 2004 disaster changed all that.   In early 2005, the UN agreed to develop an international warning system including regional systems in the Indian Ocean, North East Atlantic & Mediterranean, and Caribbean. The Indian Ocean tsunami warning system was developed between 2006 and 2013, at a total cost of at least $19 million.  In the three years prior to October 2014, bulletins were issued about 23 Indian Ocean earthquakes, resulting in a small number of potentially life-saving coastal evacuations. Most of these 23 earthquakes did not actually generate a threatening tsunami because they did not cause significant uplift or subsidence of the seafloor. But false alarms can provide reassurance that communications work well, or highlight weaknesses.  Communications and evacuation procedures are also regularly tested by international mock drills, often based on worst case scenarios.   All warning systems work in the same general way. First, a network of broadband seismometers detects the seismic waves generated by an earthquake, which travel at speeds of several kilometres per second. When several seismometers have detected the seismic waves, the location and approximate magnitude of the earthquake can be computed. If the epicentre is under water and the magnitude large (greater than 6.5 on the Richter, or moment magnitude, scale) a tsunami bulletin, watch or warning is issued to local communication centres, ideally within three minutes of the earthquake.  If the epicentre is nearby and the probability of a tsunami is high, evacuation procedures will be initiated immediately.  Otherwise, local centres will standby for confirmation of whether a tsunami has actually been generated. Confirmation comes within about 30-60 minutes, using a network of tsunami buoys and seafloor pressure recorders. These detect the series of waves (usually less than a couple of metres high and travelling at about 800 km/h) in the open ocean, and transmit the data by satellite to a regional control centre.  Tsunami warnings reach the public via TV, radio, email, text messages, sirens and loudspeakers. You can sign up to receive tsunami alerts anywhere in the world by SMS on your mobile phone, thanks to a not-for-profit humanitarian service called CWarn.org. Many high-risk areas also have signage to alert people to “natural” warnings (such as strong shaking or a sudden withdrawal of the sea), and direct them to higher ground. The Pacific and Japanese warning systems helped to ensure the major tsunami generated off the coast of Japan on 11 March 2011 caused far fewer deaths (15,000) than the 2004 disaster. However, it showed that even a wealthy and well-prepared nation such as Japan cannot fully protect people from extreme hazards, and that warning systems can sometimes lead to a false sense of security.  The slow rupture of the subduction zone near Japan meant the initial warnings underestimated the magnitude of the earthquake and resulting tsunami. Many people did not move to higher ground in the vital few minutes after receiving the warning, because they wrongly assumed the tsunami would be stopped by 5-10 m high sea walls.  Japan has learned from this tragedy and, among other things, made changes to tsunami warning messages, improved coastal defences, and installed more seismometers and tsunami buoys.  It is impossible to predict exactly when or where the next major tsunami will occur. They are very rare events in our limited historical record. But by dating prehistoric tsunami deposits, we can see that major tsunamis happen on average every few hundred years in many coastal regions.  Future tsunami disasters are inevitable, but with better technology, education and governance we can realistically hope that a loss of life on the scale of the 2004 tsunami disaster will not happen again."
"
Share this...FacebookTwitterAwhile back I did a story on Anders Levermann of the über-alarmist Potsdam Institute For Climate Impact Research (PIK), and reader Arnd B brought my attention to an article called: Our systems are especially vulnerable, which appeared in the online Frankfurter Allgemeine Zeitung (FAZ) in late December.

Hieronymous Bosch paints a scene of a Renaissance mountebank fleecing incredulous gamblers.
Levermann makes a number of interesting comments that provide insight on how the PIK views climate. Unfortunately, all his predictions are based on models, and ignore real-life observations.
Global warming could enhance cold weather
Levermann starts off saying the bitter cold and snow in Germany last month is a sure sign of “how out-of-whack the climate system is.” Levermann serves up the “science” that supports it:
The current cold weather in Europe is everything but evidence against climate change, rather it could even be enhanced by global warming. Colleagues have discovered the mechanism for this: Through the ice melt in the Kara Sea, high pressure zones can form, which then divert the Eurasian winds and lead to cold temperatures in Europe.”
It takes a real climate scientist to make such a profound discovery, and that with no data to back it up. Not only that, Levermann adds:
The more and faster we emit greenhouse gases, the more our climate gets knocked out of whack.”
At this point, I have to ask myself: “Just how gullible must the average FAZ reader be to take this seriously?
Extreme weather events prove manmade climate change
And as usual Levermann goes down the laundry list of last year’s weather events…floods in Pakistan, heat wave in Russia, mudslides in Brazil, etc., etc. and claims this is evidence supporting the man-made global warming link, and that it had all been predicted by models. Yet, Levermann forgets to mention that the accumulated cyclone energy (ACE) was near a record low last year, and that temperatures have not risen over the last decade – something his models have missed. Still he insists:
It’s now practically sure that in a rapidly warming world we have to expect more and stronger extreme events.”
PIK models can now see to the year 2200
Keep in mind that Anders Levermann is a lead author for the next IPCC Report on the subject of sea levels. The next report will deliver the latest “projections” based on various CO2 output scenarios. So where does Levermann say the globe is headed?
What we can already say, based on our latest studies, is: We currently find ourselves on the warmest possible future trajectory […] The temperature projection shows a warming of more than eight degrees Celsius in the year 2200.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Unfortunately for Levermann, there hasn’t been any warming in a decade, as the following HadCrut chart shows:
HadCrut shows cooling over the last decade. (WoodFor Trees)
Remember that he is a lead author on sea ice for the upcoming IPCC 5th assessment report. What kind of sea level projection do you think he’ll concoct with 8°C of warming? Expect the 5th assessment report to be worse in scientific quality than the 4th report of 2007. Sci-fi sequels tend to get worse and worse as they are taken over by B-rated directors.
Anders Levermann (Photo: PIK)
Ignoring real world observation and data, Levermann stares deeper into his crystal ball and sees only horrors. He wonders if man will be able to adapt to these rapidly changing climate conditions (the ones in his crystal ball). What are the limits of human adaptability, he asks? 4°C? 6°C?
Warming 50 times faster than the warming that ended the ice age
Claiming that his crystal ball sees 8°C of warming over the next 190 years, he says that this 4°C rise per century will be unprecedented. Levermann says the difference between an optimum and an ice age is about 5°C, and that it took 5000 years for the earth to make the 5°C climb out of the last ice age:
The transition from ice age to warm period lasted a good five thousand years. When man continues to emit greenhouse gases unabated, then we will reach the same warming 50 times faster than in the past.”
That’s assuming his models are correct. Looking at the above HadCrut chart, there has been no warming. And Levermann doesn’t mention that most of the temperature rise ending the last ice age took place in about 1000 years, and the temperature difference was more than 5°C. It was closer to 8°C. So he’s fudging there quite a bit.
He also ignores the huge temperature swings of up to 6°C which occurred in just a matter of a few decades during the Younger Dryas – all naturally.
Of course, Levermann doesn’t forget to play emotion-card Africa, and predicts dire scenarios for the poor continent.
It is probable that in such a situation, countries like Bangladesh and parts of Africa will have become uninhabitable. Whether the drinking water supply collapses because of drought, or sea water claiming the land, or because agriculture becoming impossible. Even without the extreme events, the United Nations estimates that the number of climate refugees will reach 90 million if the sea level rises 1 meter.
Here he ignores studies that show the African Sahara is shrinking and getting more desperately needed rainfall during this modern warm period. And he ignores that sea levels have decelerated over the last 5 years. And even if the sea level did rise 1 meter, something that only the most fanatic among us predict, it would not happen overnight. Most humans just don’t have the habit of standing still for 100 years and watching the water rise around them.
Finally, Levermann ends by saying:
The wall that we are racing towards is hidden in fog, but it is there!
At PIK it’s: If you can’t see it, then it’s proof it’s there. Just believe us.
Share this...FacebookTwitter "
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   




Using a simple, publically-available, climate model emulator called MAGICC that was in part developed through support of the EPA, we ran the numbers as to how much future temperature rise would be averted by a complete adoption and adherence to the EPA’s new carbon dioxide restrictions*.   
  
The answer? Less than two one-hundredths of a degree Celsius by the year 2100.   
  
0.018°C to be exact.   
  
We’re not even sure how to put such a small number into practical terms, because, basically, the number is so small as to be undetectable.   
  
Which, no doubt, is why it’s not included in the EPA Fact Sheet.   
  
It is not too small, however, that it shouldn’t play a huge role in every and all discussions of the new regulations.   
  
*********   
  
* Details and Additional Information about our Calculation   
  
We have used the Model for the Assessment of Greenhouse-gas Induced Climate Change (MAGICC)—a simple climate model emulator that was, in part, developed through support of the EPA—to examine the climate impact of proposed regulations.   
  
MAGICC version 6 is available as an on-line tool.   
  
We analyzed the climate impact of the new EPA regulations by modifying future emissions scenarios that have been established by the United Nation’s Intergovernmental Panel on Climate Change (IPCC), to reflect the new EPA proposed emissions targets.   
  
Specifically, the three IPCC scenarios we examined were the Representative Concentration Pathways (RCPs) named RCP4.5, RCP 6.0 and RCP8.5. RCP4.5 is a low-end emissions pathway, RCP6.0 is more middle of the road, and RCP8.5 is a high-end pathway.   
  
The emissions prescriptions in the RCPs are not broken down on a country by country basis, but rather are defined for country groupings. The U.S. is included in the OECD90 group.   
  
To establish the U.S. emissions pathway within each RPC, we made the following assumptions:   
  
1) U.S. carbon dioxide emissions make up 50 percent of the OECD90 carbon dioxide emissions.   
  
2) Carbon dioxide emissions from electrical power production make up 40 percent of the total U.S. carbon dioxide emissions.   
  
Figure 1 shows the carbon dioxide emissions pathways of the original RCPs along with our determination within each of the contribution from U.S. electricity production.   








_Figure 1. Carbon dioxide emissions pathways defined in, or derived from, the original set of Representative Concentration pathways (RCPs), for the global total carbon dioxide emissions as well as for the carbon dioxide emissions attributable to U.S. electricity production._



As you can pretty quickly tell, the projected contribution of U.S. carbon dioxide emissions from electricity production to the total global carbon dioxide emissions is vanishingly small.   
  
The new EPA regulations apply to the lower three lines in Figure 1.   
  
To examine the impact of the EPA proposal, we replace the emissions attributable to U.S. power plants in the original RCPs with targets defined in the new EPA regulations. We determined those targets to be (according to the EPA’s Regulatory Impacts Analysis accompanying the regulation), 0.4864 GtC in 2020 and 0.4653 GtC in 2030. Thereafter, the U.S. power plant emissions were held constant at the 2030 levels until they fell below those levels in the original RCP prescriptions (specifically, that occurred in 2060 in RPC4.5, 2100 in RCP6.0, and sometime after 2150 in RCP8.5).   
  
We then used MAGICC to calculate the rise in global temperature projected to occur between now and the year 2100 when with the original RCPs as well as with the RCPs modified to reflect the EPA proposed regulations (we used the MAGICC default value for the earth’s equilibrium climate sensitivity (3.0°C)).   
  
The output from the six MAGICC runs is depicted as Figure 2.   




_  


![Media Name: gsr_061114_fig2.jpg](/sites/cato.org/files/styles/pubs/public/wp-content/uploads/gsr_061114_fig2.jpg?itok=R2UJJXni)

_





_Figure 2. Global average surface temperature anomalies, 2000-2100, as projected by MAGICC run with the original RCPs as well as with the set of RCPs modified to reflect the EPA 30% emissions reductions from U.S power plants._



In case you can’t tell the impact by looking at Figure 2 (since the lines are basically on top of one another), we’ve summarized the numbers in Table 1.   






  
  
In Table 2, we quantify the amount of projected temperature rise that is averted by the new EPA regulations.   






  
  
The rise in projected future temperature rise that is averted by the proposed EPA restrictions of carbon dioxide emissions from existing power plants is less than 0.02°C between now and the end of the century assuming the IPCC’s middle-of-the-road future emissions scenario.   
  
While the proposed EPA plan seeks only to reduce carbon dioxide emissions, in practice, the goal is to reduce the burning of coal. Reducing the burning of coal will have co-impacts such as reducing other climatically active trace gases and particulate matter (or its precursors). We did not model the effects of changes in these co-species as sensitivity tests using MAGICC indicate the collective changes in these co-emissions are quite small and largely cancel each other out.


"
"
Guest Post by Steven Goddard

Over the last year or so I have been taking an informal survey of a key news metric – Google news searches for the term “global warming.”  A year ago, the ratio of alarmist/skeptical articles was close to 100/1.  About six months ago, the ratio was 90/10, Two months ago it was 80/20, and today it hit 50/50 for the first time – including the lead skeptical story “A Cooling Trend Toward Global Warming“.  One thing that has changed is the rise of blogs written by informed citizens, complemented by the demise of corporate newspapers which make money from keeping people continually alarmed about one thing or another.
Congratulations to Anthony and all the readers for being a big part of this.  Democracy in it’s purest form – hope and change we can all believe in.
The top two items from Google news “global warming” search today.  The distribution of all stories through the first few search pages was similar in makeup as seen below:


The Tech Herald
A Cooling Trend Toward Global Warming
The New American – ‎1 hour ago‎
With the election of a president who is solidly in the global–warming-alarmist camp – and with many high-level appointees who are bona fide climate-change …
Global warming and climate change: facts and hype Examiner.com
UN global warming stand criticized Delta Farm Press
UN Con on Global Warming Nearly Foiled NewsMax.com
Opposing Views – Atlanta Journal Constitution
all 36 news articles »

New York Times
House Democrats release draft energy, climate bill
New York Times – ‎8 hours ago‎
By DARREN SAMUELSOHN AND BEN GEMAN, Greenwire Democratic leaders of the House Energy and Commerce Committee today unveiled a 648-page draft global warming …
House Democrats unveil sweeping plan to reshape energy in America MiamiHerald.com
Waxman’s clean energy draft includes cap-and-trade proposals Oil & Gas Journal
US lawmakers present draft bill on ‘clean energy’ AFP
iBerkshires.com


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96f29872',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterBy guest writer
Ed Caryl
Arctic stations near heat sources show warming over the last century. Arctic stations that are isolated from manmade heat sources show no warming. The plots of “isolated stations” and “urban stations” below clearly illustrate the differences. 
Stevenson Screen, Verhojansk, Russia
All the GISS temperature anomaly maps show the Arctic warming faster than the rest of the globe, especially northern Alaska and Siberia, but the satellite data shows a different pattern. See the 2 charts for 2009 that follow. The GISS surface map:


Satellite chart:

The baseline period selected for the GISS surface temperature chart is the 1933 to 1963 Atlantic Multi-decadal Oscillation (AMO) warm period. This period more closely matches today’s temperatures than the default 1951 to 1980 cool period that GISS uses. The satellite data uses the average over the satellite period since 1979, the modern warm period.
The satellite data show cooling in central Siberia, similar to the surface anomaly map, and very little warming for most of Alaska. It also shows cooling for the Antarctic Peninsula, where the surface map shows warming. But there is a scattering of hot grid squares across the HISS surface station map for the Arctic. So what is going on?
I selected the stations that correspond to those warm grid squares, as well as other stations in the same latitudes. In this age of everyone carrying a camera posting all photos on the Internet, there is a lot of information available on these stations. For some I could locate the Stevenson screens, for most I’ve found pictures of the surroundings, while others have investigated many of these sites already, and so links to that research are included. I downloaded the raw temperature data from GISS for 24 stations closest to the North Pole, which are all classified as “rural”.
“Urban” Arctic Stations
Contrary to GISS claims, many of these stations are actually not “rural” with respect to their siting quality. Many are at airports associated with sizable towns or research stations with sizable staff and infrastructure. In the Arctic, any town of more than a few families can be a large heat source. In the case of many towns in Russian Siberia, “central heating” takes on a whole new meaning. These towns have a central power plant that provides electricity and steam heat to the whole town. Large pipes, both insulated and un-insulated, carry steam, water, and sewage, up and down the streets to and from each dwelling. These pipes cannot be buried because of the permafrost, so they are elevated, and at street crossings are elevated 4 or 5 meters. The temperature differential between these pipes and the surrounding air can be 140° C in winter, and even more for a pressurized system.
But GISS applies the same Urban Heat Island (UHI) criteria to all stations globally, regardless of the latitude or average temperature. They look at the satellite night brightness and population to judge whether urban or rural. By GISS criteria, all the stations in the high Arctic are rural; there are no corrections for UHI.
But let’s look at each of these “urban” locations. Each name is also a link to the GISS surface temperature raw data.
List of Urban Arctic Stations (see the annex at the end of this post for details on each station)
  1. Kotzebue, Ral (66.9 N,162.6 W), Alaska
  2. Barrow/W. Pos (71.3 N,156.8 W) Alaska
  3. Inuvik (68.3N, 133.5W) Inuvik, Canada
  4. Cambridge Bay (69.1 N,105.1 W) Nunavut, Canada
  5. Eureka, N.W.T. (80.0 N,85.9 W), Canada
  6. Nord Ads (81.6 N,16.7 W Northeast Greenland
  7. Svalbard Luft (78.2 N,15.5 E), Norway
  8. Isfjord Radio (78.1 N,13.6 E), Norway
  9. Gmo Im.E.T.(80.6 N,58.0 E), Russia
10. Olenek (68.5 N,112.4 E), Russia
11. Verhojansk (67.5 N,133.4 E), Russia
12. Cokurdah (70.6 N,147.9 E), Russia
13. Zyrjanka (65.7 N, 150.9 E), Russia
14. Mys Smidta (68.9 N,179.4 W), Russia
15. Mys Uelen (66.2 N,169.8 W), Russia
The following graphic is a temperature chart for 10 of the above stations (5 of the shorter ones were left out to avoid over-crowding). All are warming, some faster than others. Barrow, for which we have the UHI study, is not the fastest warming.
Temperature trends of the ""urban"" stations.
Isolated Stations
Now let us look at the isolated stations, which are located at similar latitudes like the above “urban” stations. One important thing to note about these isolated stations – there is limited electrical power, and so incandescent light bulbs in the Stevenson screens is unlikely. Detailed descriptions of these stations are listed in the annex at the end of this report.
16. Alert,N.W.T.(82.5 N,62.3 W), Canada
17. Resolute,N.W. (74.7 N,95.0 W), Canada
18. Jan Mayen (70.9 N,8.7 W), Norway
19. Gmo Im.E. K. F (77.7 N, 104.3 E), Tamyr Peninsula, Russia
20. Ostrov Dikson (73.5 N,80.4 E, Russia
21. Ostrov Kotel’ (76.0 N,137.9 E), Russia
22. Mys Salaurova (73.2 N,143.2 E), Russia
23. Ostrov Chetyr (70.6 N,162.5 E), Russia
24. Ostrov Vrange (71.0 N,178.5 W) , Russia
Now here is the chart of the temperatures of these isolated stations, not subjected to manmade structures or heat sources.
Isolated Stations
Note that most of the trends are flat or decreasing. Only Resolute and Ostrov Vrange are increasing slightly. Both of those might be slightly influenced by UHI. The longest records clearly show warming in the late 1930’s and 40’s, and cooling in the 1960’s, and none show a hockey stick. The GISS data for Alert ends in 1991, though the weather station is still there, and still reporting. Data for Mys Salaurova and Ostrov Chetyr also ends at about that time, probably due to the fall of the Soviet Union.
Here is an average of all the isolated stations:
Isolated Stations - Average
Note that the peak-to-peak trend is nearly zero. The linear trend is about 0.4°C/century, but the R2 value (the statistical significance for the trend) is very low, 0.023.
 Here is a plot of the AMO versus the average temperature of the isolated stations.

The temperature as measured at stations isolated from any UHI is simply tracking the AMO. 
Looks like an awfully good fit. There is very little, if any, global warming. We need to wait until the bottom of the next AMO cycle to get a decent reading of global temperature change. That will be in about 2050 if the AMO cycles as it has since 1850.
———————————————————————————————-
Annex – station descriptions
The “urban” stations, nos. 1-15
1. Kotzebue, Ral (66.9 N,162.6 W), 
2. Barrow/W. Pos (71.3 N,156.8 W)
These towns are of similar size, and are growing at the same rate. In 1940, both towns had a population of 400. In 1980 both had just over 2000 population, and now they both have over 3000 people. Both have airports of sufficient size to handle multi-engine turboprop and small jet aircraft, and both are served daily by regional airlines. Kotzebue is on a peninsula and the airport is across the middle of the peninsula, somewhat restricting the growth of the town. Barrow has somewhat the same problem due to a series of small ponds around the town and the airport. Barrow was studied for UHI effects in 2003. That paper was in the International Journal of Climatology here. That paper describes the UHI average temperature increase in winter as 2.2°C compared to the surrounding hinterland. GISS data indicates that Barrow average temperature has increased over the years as population has increased. (See below, or click on link above.)
Barrow, AK


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Source: http://en.wikipedia.org/wiki/File:BRW-g.jpg
Source: http://en.wikipedia.org/wiki/File:OTZ-g.jpg
The Barrow NWS station (Stevenson Screen) is here. On the airport picture, it is at the base of the rotating beacon tower. Kotzebue NWS station is not visible in published pictures.
3. Inuvik (68.3N, 133.5W)
Inuvik is a relatively new town, begun in 1954. The population as of 2006 has grown to about 3500 people. Because it is a “planned” community in the arctic, built on permafrost, the water and sewage infrastructure is above ground in heated and insulated “utilidors”, like the heating systems in Siberia. The weather station, from weather reports, Google Earth and Google Street View, appears to be at the airport, in a compound just north of the entrance.
4. Cambridge Bay (69.1 N,105.1 W), Cambridge Bay, Nunavut, Canada
There is a Wikipedia picture of Cambridge Bay here. The population has grown from just a few people in the 1940’s to about 1500 today. It also has an airport with daily regional airline service.
5. Eureka, N.W.T. (80.0 N,85.9 W), Eureka, N. W. T., Canada
There are the only four stations at or north of 80° latitude, Eureka, Alert, Nord and Krenkle (Gmo. I.M.ET). Only Eureka has an unbroken temperature record to the present date, and it begins in 1947. The population at Eureka has
Eureka station
never been high. In winter it has always been 4 or 5 men. In summer, the population increases to as high as 20. The station infrastructure though, has expanded through the years. Each year, some of those 20 workers add or expand buildings. In the beginning, it was one or two buildings, with water and sewage handled in tanks and barrels internal to the buildings. The Stevenson screen was originally placed where the blue New Main Complex building is now. When that was built, the Meteorological instruments were moved to the current location. Over time, the water supply, plumbing, and sewage treatment was upgraded and the outfall pipe installed. It, of course, must be heated to facilitate flow to the sewage lagoon. All the water pipes exposed to the outdoors must be heated to prevent freezing.
Image from a recent article by Anthony Watts on WUWT here.
6. Nord Ads (81.6 N,16.7 W, Northeast Greenland
Nord Station
Nord is the furthest north inhabited place on earth, on the Northeast coast of Greenland. It was built in the period from 1952 to 1956 as an emergency airfield for aircraft operating out of Thule. Access is impossible by sea because the sea ice never moves away from the coast there. Legend has it that “Blowtorch” Murphy, a mythic arctic construction worker, scraped the first runway, using a parachute dropped caterpillar tractor after he himself parachuted onto the site. His nickname came from his habit of wearing a lit blowtorch hanging from his waistband on a wire; a lit blowtorch being somewhat useful when working outside when it’s 40° below zero.
There are about 40 buildings at Nord. Not all of them are continuously heated, but those near the Stevenson Screen are. The winter population is 5 or 6 men. More pictures here.
7. Svalbard Luft (78.2 N,15.5 E)
8. Isfjord Radio (78.1 N,13.6 E)
These two stations are only 47 kilometers apart. But data for both is fragmentary for 1976 and 1977, and there is no overlap. Svalbard Luft (airport) has been discussed on WUWT here and here, so I won’t cover it in detail here. Warwick Hughes has an article on Isfjord Radio here that makes the case for warming of Isfjord Radio due to moving of sea ice away from the islands in summer since 1912. Neither station in Svalbard shows on the anomaly map because there was no common station in both the base period and the anomaly period. Here’s a map with 1998 to 2008 as the base period where Svalbard appears.
9. Gmo Im.E.T.(80.6 N,58.0 E)
This is the Krenkel meteorological station on Hayes Island, or Ostrov Kheysa in Russian, in the Franz Josef Land Archipelago, Russia. Link The station has been moved or re-built twice since it was established. It was moved from Hooker Island (article in German) in 1957/58. A fire destroyed the power station in 2000, and it was rebuilt in 2004 closer to the shoreline. The GISS record is from 1958 with a gap from 2001 to 2009. The population was as high as 200 during Soviet times, but is down to 4 or 5 now. The population and the temperature seem to track roughly during Soviet days, and the move in 2004 was to a warmer location. In the picture you can see the old buildings on the ridge in the distance. The red grid-square on the anomaly map above corresponds to this station.
Source: http://www.sevmeteo.ru/foto/15/88.shtml
10. Olenek (68.5 N,112.4 E)
This is the town of Ust’-Olenek, Russia.
Photo sources here and here.
The town doesn’t look like much, but notice the Tundra Buggies parked next to the Stevenson Screens. It is on the Laptev Sea, on the northern Siberia Coast, but on a peninsula on a south-facing beach. The buildings are right on the shore. The wide view above was taken from out on the ice. This is one of the few places in Russia that the Google Earth satellite view actually has enough resolution to see the Stevenson Screens. They are much too close to the heated building.
11. Verhojansk (67.5 N,133.4 E)
This is one of the “centrally heated” towns in Russian Siberia. The picture at the top of this article is of the Stevenson Screen. Verhojansk is called the “cold pole” of the earth, but the measurements are too warm by far. Look closely at the picture. Any photographer will note that the warm glow inside the Stevenson Screen is just the color temperature of an incandescent light bulb. If the steam heat in the town isn’t enough, or the cattle in the pole-barns in the distance, the heat from the light bulb will warm up the measurements. This site was covered on WUWT here and here. Anthony Watts notes that warm anomalies would appear and disappear in this part of Russia “as if a switch were thrown”. Could it be as simple as the switch on that light bulb?
12. Cokurdah (70.6 N,147.9 E)
Also spelled Chokurdakh. The population has been dropping in recent years, but was still over 2500 people in 2002. The town is sandwiched between the Indigirka River and the airport. There is no way to tell where the Stevenson Screen is located, but the infrastructure at the airport blends right into the town. See an aerial photo here.
13. Zyrjanka (65.7 N, 150.9 E) Also spelled Zyryanka, another steam-heated town in eastern Siberia, well inland. The airport is in this picture on the north edge of town, along the Kolyma riverbank. This airfield was built during WWII as a stop for aircraft being ferried to the Soviet Union from Alaska. A second airport 7 miles west of town was probably built during the cold war for the military. The town was established in 1931. The population is currently about 3500. During the Soviet Union it was up to 15,000.
14. Mys Smidta (68.9 N,179.4 W)
Or Cape Schmidt.  John Daly wrote a bit about this location in 2000 (scroll way down in the article). The population was nearly 5000 in 1989, but has dropped since the fall of the Soviet Union. The population now is probably less than 1000. It is on the north coast of eastern Siberia, nearly at 180° longitude. The airbase there was built in 1954 as a staging base for any bombers headed for the U. S. It is still used by a regional airline.
15. Mys Uelen (66.2 N,169.8 W)
Or Cape Uelen. This is on the easternmost tip of Siberia, across Bering Strait from Kotzebue, Alaska. The current population is about 700 people. It is also centrally steam heated. The town is restricted by the geography, on a narrow spit sticking out into the sea, backed by a cliff on the landward side. The airport is a helipad. Cargo and fuel arrives by barge in the summer.
Below is a temperature chart for many of the above stations. All are warming, some faster than others. Barrow, for which we have the UHI study, is not the fastest warming.
16. Alert,N.W.T.(82.5 N,62.3 W), Alert, Canada
Alert, Canada has had a weather station since 1951. The population has never been more than 4 or 5 in the winter, with a higher population in the summer. I could not definitively locate the Stevenson screen, but there are two possibilities in this photo, both well away from the buildings.
THE ISOLATED STATIONS, NOS. 16-24
17. Resolute,N.W. (74.7 N,95.0 W)
The population of this Canadian station rose from zero prior to 1947, to 229 in 2006. There is an airport here, and the Stevenson Screen can be seen across the aircraft parking area from the airport terminal at the left edge of the photo.
18. Jan Mayen (70.9 N,8.7 W)
Pictures of the station are here, and a web site is here. The 18 people on the island live at Olonkinbyen, or Olonkin “City”. The meteorological station is 2.6 km away. The 4 people that work there live in Olonkin City. The Stevenson Screen appears to be well away from the station building, and the surroundings have probably not changed since the station was built.
19. Gmo Im.E. K. F (77.7 N, 104.3 E)
This is a Russian station on the Tamyr Peninsula at Cape Chelyuskin (Mys Chelyuskin). Nothing is visible at that location on the Google satellite view, but the resolution is very low. I found an article by Warwick Hughes dated September 2000 that speaks of cooling of the Tamyr Peninsula here. He also talks about “non-climate” warming of Verhojansk and Olenek.
20. Ostrov Dikson (73.5 N,80.4 E
Dikson, Russia airfield
This is Dickson Island in English. There is a town of Dikson 10 kilometers away on the mainland. The airport is on Dikson Island at the point called Ostrov Dikson on the map below. Pictures of the airport can be seen here. The town is pictured on this 1965 stamp.
Wikipedia link
21. Ostrov Kotel’ (76.0 N,137.9 E)
The full name is Ostrov Kotel’nyy. In English this is Kettle Island. The first documented explorer found a copper kettle, so obviously he was not the first person to find the island. A single building is barely visible on Google 3D mapsat the “settlement” known as Kalinina.  This may be the meteorological station. No other signs of civilization can be seen on the whole island.
22. Mys Salaurova (73.2 N,143.2 E)
This also spelled Mys Shalaurova. The station is on the south-facing shore of an island and is visible on Google Earth here. There is a tide gauge, and the tide data is on that same page.
23. Ostrov Chetyr (70.6 N,162.5 E)
The full name is Ostrov Chetyrekhstolbovoy. This is a small island in the East Siberian Sea in the Medvezhy Island (Bear Island) group.
Map source here.
A description of the place is found: here. “A polar meteorological station and a radio station are situated on the shore of a small bay which indents the S side of the island.”
24. Ostrov Vrange (71.0 N,178.5 W) 
This is otherwise known as Wrangle Island. It is about 125 kilometers off the Siberian coast on the 180th meridian. The weather station is at Ushakovskiy on a spit at Rogers Bay, at the right in this picture, well separated from the village. One building in the village is visible at the left. Link
The population in the village grew to as many as 180 people in the 1980’s, but when the Soviet Union dissolved, subsidies declined and the population moved to the mainland. The last villager was killed by a polar bear in 2003. The population at the weather station, when occupied, has always been 4 or 5.
Share this...FacebookTwitter "
"Researchers have long known that man-made climate change will harm yields of important crops, possibly causing problems for the world’s food security. But new research shows air pollution doesn’t just harm crops indirectly through climate change; it seems to harm them directly. Pollution from soot and ozone has caused a major decrease of crop yields in India, with some densely populated states experiencing 50% relative yield losses. To ensure the world has enough food, we need to look directly at air pollution. Jennifer Burney and Veerabhadran Ramanathan from the University of California, San Diego systematically investigated the impact of air pollution and anthropogenic climate change on crops in India, where yields have levelled off or decreased in recent decades despite continued improvements in agricultural technology. Their study showed that overall air pollution has caused a third of loss in wheat yield and one fifth of loss in rice yield in India in 2010, using 1980 as a baseline. Their findings are published in the journal Proceedings of National Academy of Sciences. Many previous works have studied the impact of climate change on crop yield. However, this new study suggests that air pollution from ozone and soot caused far more loss of crop yield than climate change. From 1980 to 2010, the increase in temperature and change in precipitation as a result of anthropogenic climate change has caused a 3.5% decrease in wheat yield on a country level in India. However, air pollution has caused more than 32% decrease in wheat yield during the same period. Blame pollution not climate for declining wheat yields: Soot – or black carbon – is emitted mainly from burning plants and fossil fuels. It directly absorbs sunlight, reducing the amount of light available for crops to photosynthesise. Black carbon alone has caused more damage to Indian wheat yields than climate change. Ozone is a gas formed in the atmosphere through chemical reactions of precursors including nitrogen oxides (NOx) and volatile organic compounds (VOCs) in the presence of sunlight. NOx are mainly generated from fossil fuel combustion while VOCs are emitted from both natural sources and human activities.  Ozone damages crops by entering leaves during normal gas exchange. As a strong oxidant, ozone causes symptoms in crops such as yellowing, cell injury, tiny light-tan irregular spots, bronzing, and reddening. This directly affects the growth of crops and thus reduces their yield. Ozone is the key pollutant causing the yield loss of crops, for example wheat, which is very sensitive to ozone exposure. Ozone exposure could have an even bigger impact on yields of soybean, peanut and cotton. The picture is unlikely to improve any time soon. My colleague William Bloss, an expert in atmospheric chemistry, points out that background ozone levels have approximately doubled since the earliest measurements (performed near Paris in the 1870s). “Looking to the future”, he says, “models predict that ground level ozone will continue to rise in many areas of the world”. Ozone pollution will continue to be a major challenge for food security. It is important to note that there are significant regional variations in the crop yield loss in India, with some states seeing more than 50% losses in wheat yield, mainly due to air pollution. This has significant implications for other developing countries, in particular China.  Since the emission of soot and ozone precursors is significantly larger in China than in India, the impact of air pollution on Chinese agriculture is expected to be even larger than that in India. China is now the world’s largest food importer. Could air pollution in China have led to the thirst for food in the global market? Fortunately ozone and black carbon have short atmospheric lifetimes (unlike some greenhouse gases which can linger for decades or centuries). This means there is a strong, direct benefit to addressing such pollution, and it would be apparently relatively soon."
"
Share this...FacebookTwitterEd Caryl has become a regular contributor here, and today he presents insights on the causes of glacial melt. Here he discusses how absorption of solar energy by soot and Black Carbon contribute significantly to glacial melting and that CO2 is a minor factor.
 
Glaciers – The Dark Side. It’s Not the CO2 Carbon

by Ed Caryl
The global warming “hockey stick”, invented by Dr. Michael Mann, has been proven to be a distortion. [i]  But if carbon dioxide is not significantly warming the planet, then why are most northern glaciers shrinking?
Since the end of the last ice age 12,000 years ago, glaciers have been receding, dramatically in the first few thousand years of warming when the oceans rose by 120 meters. The remaining glaciers have been receding since the end of the “Little Ice Age” in the early 1800s. This is normal. Compared to an ice age, it is warm.
There is evidence that this retreat has stopped and even slightly reversed in the last ten years for some glaciers; those on Mount Shasta in California are examples. These have increased in mass because of greater snowfall. Glaciers in Alaska, California, Europe, and South Greenland are still receding. Some of the melt of South Greenland is because of the Atlantic Ocean.[ii]
The following temperature plots are of the sea off the west coast of Greenland. For a full resolution, better quality graphic go to the link. The years shown are 1992 to 1999.

and 2000 – 2007:As the above chart shows, the Atlantic Ocean off southern Greenland began warming in the early 1990’s and is only now beginning to cool. This local warming is due to the Atlantic Multi-Decadal Oscillation (AMO), a natural Atlantic cycle with a period of about 70 years described here. But not all of the Greenland melting is due to the warm Atlantic.
Soot and Black Carbon
Glaciers are melting in the Alps, Alaska, Canada, most of the Sierra Nevada in California, and the Himalayas because of the other carbon emission: soot. This is known in the literature as Black Carbon. You can even see the geographic source in the map below, south Asia, China, and Russia. The emission sources are coal burning, bio-fuel (including dung), diesel engines, fuel-wood smoke, forest fires, and other incomplete combustion processes that take place in highly populated areas.
The result is clearly visible in nearly any photo of a melting ice field or glacier. Soot is visible also on Greenland glaciers. See here for a photo from National Geographic (also shown to the left). Note the black stains in the ice field; that is Black Carbon. The soot is deposited on the snow in the winter and spring. As the snow melts, it gets concentrated on the surface as the melt water drains away between the ice crystals. When the melt gets down to smooth ice, the soot concentrates in cracks. The sunlight heats the cracks, widening them.
Clean snow melts slowly because 99 percent of the sunlight is reflected away. Dirty snow or ice melts quite quickly because much more of the sunlight is absorbed as heat by the soot. 10 parts per billion of soot in freshly fallen snow is enough to significantly enhance melting.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Soot on glacial tongues in Northern Bhutan. This NASA photo shown below is from here.
The problem is very acute in places like the Swiss Alps, which are surrounded by industrial nations burning a lot of diesel fuel. Those glaciers are disappearing rapidly. The Alaskan and Canadian glaciers are receding because of soot from China. The problem has been recognized in the Himalayas. There is substantial disagreement (a factor of 200 on how much melt forcing takes place), though, on the extent of the problem. A quote from this document:
Black carbon on snow during spring melt in the Tibetan Plateau, for example, creates forcing rates 200 times higher than was assumed for black carbon on snow in the IPCC Fourth Assessment Report.
Here is a map of worldwide black carbon optical thickness from above, and concentration on the surface:

Source: NASA, Dorothy Koch and James Hansen here.
Note how much carbon is in the high Arctic, compared to that at the equator and further south. NASA admits that soot is part of the melting glacier problem, but downplays its importance. They hedge their bets on the subject.[iii] But some of the analysis views soot as an aerosol, and some of it as soot in freshly fallen snow. Only passing mention is made of concentrated soot resulting from melting.
A recent article states that half of the Arctic warming since 1890 may be due to Black Carbon. If that is true, perhaps some of the world’s warming in the last century is due to black soot, and not CO2. Read here.
Black Carbon is much easier to curb than CO2. The European Union has already put severe restrictions on Black Carbon emissions from diesel engines. In the U.S., the EPA has done the same. The time frame of effectiveness is also much shorter for Black Carbon. Eliminate a source, and the Black Carbon from that source is washed out of the atmosphere in days or just a few weeks. On a glacier, the problem will be much reduced in one snow season.
The problem is that there are many sources, all over the populated world. In Asia, cooking fires are a major source, so supplying and improving cook-stoves should be a priority. Low quality cooking fuel, such as animal dung, should be discouraged. In China, coal-fired power plants produce most of their electricity, and are planned to produce more in the future. China must insure that these plants use the very latest in technology to prevent Black Carbon emissions. In Africa and South America, forest clearing fires are a major source, so preventing rain-forest destruction should be a double priority.
For even more information on Black Carbon, see here, and here.

[i] For more on the hockey stick: http://www.john-daly.com/hockey/hockey.htm, The Hockey Stick Illusion by A. W. Montford, here, and many others.
[ii] South West Greenland Ocean Temperature. (2009). In UNEP/GRID-Arendal Maps and Graphics Library. Retrieved 19:53, January 3, 2010 from http://maps.grida.no/go/graphic/south-west-greenland-ocean-temperature.
[iii] See: http://earthobservatory.nasa.gov/IOTD/view.php?id=4082
http://www.nasa.gov/centers/goddard/news/topstory/2003/1223blacksoot.html
http://www.nasa.gov/centers/goddard/news/topstory/2003/0509pollution.html
http://www.nasa.gov/vision/earth/environment/arctic_soot.html
Share this...FacebookTwitter "
"
Share this...FacebookTwitterIn my last post I wrote about China’s aggressive, yet legitimate, expansion of its energy supply. I wonder what the western climate hand-wringers think about the following graphic? Will they ask China about this in Cancun?
My bet is that they’re going to ignore it and focus instead on USA’s and Europe’s emissions.
What I find particularly entertaining is that even if the USA did cut its CO2 emissions by 2o% by 2030 (to about 4.7 million tons annually), Chinese growth would wipe out the savings in just 1o years (2020) or less.
Germany wants to put its entire country through economic hardship to reduce its CO2 emissions by 30% by 2020. That’s a drop of about 250 million tonnes. China’s growth would wipe out the alleged benefit of that in just a matter of months.
And if small countries like Canada, Australia,  Denmark or New Zealand all make their cuts, China’s emissions growth will offset their reductions in a matter of days or hours.
Share this...FacebookTwitter "
"In the UK, there are approximately nine million dogs and almost eight million cats – with around one in two households owning a companion animal. This large pet population is estimated to consume billions of tonnes of meat each year. So, given that more people are trying to do their bit to help save the plant and keep meat consumption to a minimum, it’s not surprising pet food has become the latest sector to think about its environmental credentials. Pet food trends tend to lag human dietary trends by around 12-18 months. And there are now many opportunities for dogs or cats to eat a vegetarian, vegan, gluten-free, low-allergenic or super-food diet. Then there is also a big market in “raw foods”, which have becoming increasingly popular. These are comprised solely of premium human-grade meats, raw fruits and vegetables – and are unrefined and minimally processed. The latest addition to the line-up is a high-protein pet food that meets the requirements for low environmental impact, as it is made from insects. Yora pet foods describes its “Green Insect Pet food” as: Made from 100% insect protein, grain-free, low allergenic potential, sustainable and an environmentally friendly delicacy for the modern canine household.  The domestication of canines has allowed their systems to adapt to be better at digesting starch –- found in grains, beans and potatoes –- than their wolf ancestors. This adaptation probably allowed the domestic dog to flourish on human grains and cereals. Their gut microbiome has also adapted to be better at breaking down carbohydrates and to some degree is able to produce amino acids normally sourced from meat. Dogs are true omnivores – they can survive on both plants and animals – unlike their carnivorous ancestors.  


      Read more:
      Vegan dogs: should canines go meat free?


 Our domesticated feline friends on the other hand, remain obligate carnivores – much like their larger and wilder ancestors. Cats still require many essential nutrients that can only be obtained from eating meat.  Responsible manufacturers of pet foods describe their products as “complete” if they meet all nutrient requirements for a dog or cat, according to established guidelines. Ideally, they register with the Pet Food Manufacturers Association to guarantee their product is in accord with certain standards.  Wet pet foods – such as tins, pouches, trays – are fed to approximately 41% of dogs and 77% of cats in the UK. Each is more often than not labelled as being a “meaty flavour” – such as beef, lamb, poultry, duck. The actual amount of meat in the feed varies according to the stated claim on the label – anything from 4% to 60% is common. These foods contrast steeply with the market-leading dry “kibbles” that are ultra-processed and ultra-refined – and account for 85% of all pet food sold.  The environmental impact of pet food in the US alone is estimated to be around 60m tonnes of CO₂-equivalent methane and nitrous oxide production per year – which is a huge amount. So could insect-based pet food be the answer? A first in the UK, Yora now offers a product with sufficient protein to satisfy our favourite companion animals and one that also has eco-credibility. Other manufacturers have also entered the fray with a few insect-based pet foods available online. On its website, Yora claims that the resources needed to produce just 10kg of protein from beef are 2,100 m² of land – which generates 1,500 kg of greenhouse emissions and uses 1,120,000 litres of water. Considering equivalent values to produce 60kg of insects used in its products are 45m² of land and 54,000 litres, then it’s clear that Yora could be on to something. But independent studies into such products are now needed to really conclude if the nutritional impact weighs up.  Of course, it isn’t just pet food that comes under question for it’s environmental credentials. With an expanding global population, nutritional scientists have, for many years, been considering how to produce sufficient quality protein from more efficient sources.  At the University of Nottingham, for example, academics are working on a range of projects evaluating the use of insects as both human food and animal feed. One of the major challenges though is what to feed the insects – plant and animal waste have been considered. It would defeat the object if they were fed foodstuffs more usually consumed by humans or farm animals – and kept in heated greenhouses.      Of course, cynics might say the answer is to reduce pet ownership altogether. But it’s important to not forget the positive impact that pets can have on people’s lives. Dog ownership increases activity levels and social interactions and lowers risk of premature death . Having a family pet also reduces the chances of a child in that house becoming asthmatic – by exposing their immature immune system to novel antigens at an early age. Research shows that owning a cat can also make you happier. Our feline friends can help to reduce stress levels – along with our blood pressure – and help to make us feel less lonely.   So despite the environmental impact of what they eat, the fact remains that pets are good for us. Perhaps now with increased choices on pet food, owners can make more informed, ethical decisions. And the industry could also help by labelling foods with an indication of how environmentally friendly a product is."
"
From NOAA News, Susan Solomon predicts the future with certainty. In other news, on the same day Caterpillar, Sprint, Texas Instruments, and Home Depot announce massive layoff plans to the tune of 50,000 people,  unemployed climate modelers get a government bailout today courtesy of our new president to the tune of 140 million dollars. That should be just enough to pay the electric power bill for the new supercomputer I’m sure NOAA will just “have to have” now to keep up with the new toy for the Brits at Hadley. (h/t to Ed Scott for the NOAA pr)
New Study Shows Climate Change Largely Irreversible
January 26, 2009
A new scientific study led by the National Oceanic and Atmospheric Administration reaches a powerful conclusion about the climate change caused by future increases of carbon dioxide:  to a large extent, there’s no going back.
The pioneering study, led by NOAA senior scientist Susan Solomon, shows how changes in surface temperature, rainfall, and sea level are largely irreversible for more than 1,000 years after carbon dioxide (CO2) emissions are completely stopped. The findings appear during the week of January 26 in the Proceedings of the National Academy of Sciences.
“Our study convinced us that current choices regarding carbon dioxide emissions will have legacies that will irreversibly change the planet,” said Solomon, who is based at NOAA’s  Earth System Research Laboratory in Boulder, Colo.
“It has long been known that some of the carbon dioxide emitted by human activities stays in the atmosphere for thousands of years,” Solomon said. “But the new study advances the understanding of how this affects the climate system.”
The study examines the consequences of  allowing CO2 to build up to several different peak levels beyond present-day concentrations of 385 parts per million and then completely halting the emissions after the peak. The authors found that the scientific evidence is strong enough to quantify some irreversible climate impacts, including rainfall changes in certain key regions, and global sea level rise. 
If CO2 is allowed to peak at 450-600 parts per million, the results would include persistent decreases in dry-season rainfall that are comparable to the 1930s North American Dust Bowl in zones including southern Europe, northern Africa, southwestern North America, southern Africa and western Australia.
The study notes that decreases in rainfall that last not just for a few decades but over centuries are expected to have a range of impacts that differ by region. Such regional impacts include decreasing human water supplies, increased fire frequency, ecosystem change and expanded deserts. Dry-season wheat and maize agriculture in regions of rain-fed farming, such as Africa, would also be affected.
Climate impacts were less severe at lower peak levels. But at all levels added carbon dioxide and its climate effects linger because of the ocean.
“In the long run, both carbon dioxide loss and heat transfer depend on the same physics of deep-ocean mixing. The two work against each other to keep temperatures almost constant for more than a thousand years, and that makes carbon dioxide unique among the major climate gases,” said Solomon.
The scientists emphasize that  increases in CO2 that occur in this century “lock in” sea level rise that would slowly follow in the next 1,000 years. Considering just the expansion of warming ocean waters—without melting glaciers and polar ice sheets—the authors find that the irreversible global average sea level rise by the year 3000 would be at least 1.3–3.2 feet (0.4–1.0 meter) if CO2 peaks at 600 parts per million, and double that amount  if CO2 peaks at 1,000 parts per million.
“Additional contributions to sea level rise from the melting of glaciers and polar ice sheets are too uncertain to quantify in the same way,” said Solomon. “They could be even larger but we just don’t have the same level of knowledge about those terms. We presented the minimum sea level rise that we can expect from well-understood physics, and we were surprised that it was so large.”
Rising sea levels would cause “…irreversible commitments to future changes in the geography of the Earth, since many coastal and island features would ultimately become submerged,” the authors write.
Geoengineering to remove carbon dioxide from the atmosphere was not considered in the study. “Ideas about taking the carbon dioxide away after the world puts it in have been proposed, but right now those are very speculative,” said Solomon.
The authors relied on measurements as well as many different models to support the understanding of their results. They focused on drying of particular regions and on thermal expansion of the ocean because observations suggest that humans are contributing to changes that have already been measured.
Besides Solomon, the study’s authors are Gian-Kasper Plattner and Reto Knutti of ETH Zurich, Switzerland, and Pierre Friedlingstein of Institut Pierre Simon Laplace, Gif-Sur-Yvette, France.
NOAA understands and predicts changes in the Earth’s environment, from the depths of the ocean to the surface of the sun, and conserves and manages our coastal and marine resources.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e994d102c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"At Christmas, size is everything: so says an online “oven selector guide”. And it is true, ovens are designed and optimised for roasting large birds. As a result, they are typically oversized for regular use – making their total energy consumption greater than necessary. It is not only ovens that are designed to cope with the special demands of the festive season. Christmas is a moment of peak load within the kitchen, for parcel delivery services, and for road, rail and communication networks. Looking at this period helps us understand the relation between capacity, flexibility and energy demand. Let us start with the oven. For many ovens, December 25 is the day of peak demand – in the UK, something like 10m turkeys are roasted at roughly the same time. The key features of this event – many people and a turkey large enough to feed them all – are inscribed in the details of oven design. There are similarities between a Christmas dinner and a traditional Sunday roast, and it is perhaps not surprising that the European standard test of oven efficiency, the “wet brick test”, is designed to simulate roasting a chicken.   No one knows much about how ovens are actually used, but we do know that the traditional Sunday dinner is in decline and that oven-baked pizzas are increasingly common. It would therefore be possible, and more efficient, to produce ovens optimised for normal use rather than special occasions.  The problem is oven sizes are fixed: having spare capacity through the year avoids the need for special measures, like renting an extra oven for the day – a strategy adopted by one of our great grandmothers in the 1930s.  While there are year round implications for energy demand, oven design is inseparable from the social conventions of Christmas. If turkey was not the classic meal and if roasting was not the norm – a large stew produced on the hob could feed as many people – ovens would not be sized and designed as they are today. This is not the only situation in which year round capacity reflects anticipated peak demand and therefore leaves a surplus for much of the time. Cars, for instance, are generally able to carry four or more passengers plus all their luggage, yet on average they are occupied by just 1.6 people.   However, capacity is not always fixed in quite this way.  Consider systems for delivering parcels ordered online. In this case the Christmas peak reflects the need to dispatch a very large number of items and ensure they arrive before the critical day – in short, it is the need for simultaneity, not size as such, that matters. While there are physical limits in terms of capacity – the volume of warehousing and lorry space is fixed, much like the size of the oven – the difference is that there is scope to work harder: to hire seasonal staff, and to push more items through the system.  Shipping and logistics businesses are simply not optimised for Christmas peaks, and no one would expect this either. In this example measures adopted to cope with extreme pressure are layered on top of a system that is in fact designed for regular loads, unlike the oven which is sized for peak demand. Christmas peaks provide other insights into the patterning of social life and the energy demands that follow. In the UK there used to be a detectable spike in electricity demand associated with the Queen’s speech. At 3pm, other activities paused and televisions were switched on, closely preceded and followed by millions of kettles.  This is no longer so. Instead, the anticipated afternoon surge, once the turkey has been eaten, is in the flow of data. An estimated 136,000 extra gigabytes of data will be used on Christmas Day alone.  These details provide a glimpse of wider, year round, trends in forms of information, communication and entertainment. As ways of spending time evolve – even on Christmas day – new peaks and surges occur. Ironically, the extreme circumstances and the unusual synchronisation of Christmas reveal things about “normal” energy demand. Appliances and systems are sized to cater for what are essentially social loads like those of eating Christmas dinner together.    As Christmas also demonstrates, there are different types of peak and different types of response, such as maintaining idle standby capacity at all times (exemplified by turkey-sized oven), or designing and optimising for base load and augmenting that at critical moments (hiring seasonal staff).    Perhaps most important of all, Christmas peaks in demand arise as a consequence of what people do: as people use the internet more and watch less TV, new peaks emerge. Normal and extreme forms of energy demand are alike in being embedded in the ongoing dynamics of daily life."
"
It has been an interesting couple of days. Today yet another scientist has come forward with a press release saying that not only did their audit of IPCC forecasting procedures and found that they “violated 72 scientific principles of forecasting”, but that “The models were not intended as forecasting models and they have not been validated for that purpose.” This organization should know, they certify forecasters for many disciplines and in conjunction with John Hopkins University if Washington, DC, offer a Certificate of Forecasting Practice. The story below originally appeared in the blog of Australian Dr. Jennifer Marohasy. It is reprinted below, with with some pictures and links added for WUWT readers. – Anthony
 
J. Scott Armstrong, founder of the International Journal of Forecasting
Guest post by Jennifer Marohasy
YESTERDAY, a former chief at NASA, Dr John S. Theon, slammed the computer models used to determine future climate claiming they are not scientific in part because the modellers have “resisted making their work transparent so that it can be replicated independently by other scientists”. [1]
Today, a founder of the International Journal of Forecasting, Journal of Forecasting, International Institute of Forecasters, and International Symposium on Forecasting, and the author of Long-range Forecasting (1978, 1985), the Principles of Forecasting Handbook, and over 70 papers on forecasting, Dr J. Scott Armstrong, tabled a statement declaring that the forecasting process used by the Intergovernmental Panel on Climate Change (IPCC) lacks a scientific basis. [2]
What these two authorities, Drs Theon and Armstrong, are independently and explicitly stating is that the computer models underpinning the work of many scientific institutions concerned with global warming, including Australia’s CSIRO, are fundamentally flawed.
In today’s statement, made with economist Kesten Green, Dr Armstrong provides the following eight reasons as to why the current IPCC computer models lack a scientific basis:
1. No scientific forecasts of the changes in the Earth’s climate. 
Currently, the only forecasts are those based on the opinions of some scientists. Computer modeling was used to create scenarios (i.e., stories) to represent the scientists’ opinions about what might happen. The models were not intended as forecasting models (Trenberth 2007) and they have not been validated for that purpose. Since the publication of our paper, no one has provided evidence to refute our claim that there are no scientific forecasts to support global warming.
We conducted an audit of the procedures described in the IPCC report and found that they clearly violated 72 scientific principles of forecasting (Green and Armstrong 2008). (No justification was provided for any of these violations.) For important forecasts, we can see no reason why any principle should be violated. We draw analogies to flying an aircraft or building a bridge or performing heart surgery—given the potential cost of errors, it is not permissible to violate principles.
2. Improper peer review process. 
To our knowledge, papers claiming to forecast global warming have not been subject to peer review by experts in scientific forecasting.
3. Complexity and uncertainty of climate render expert opinions invalid for forecasting. 
Expert opinions are an inappropriate forecasting method in situations that involve high complexity and high uncertainty. This conclusion is based on over eight decades of research. Armstrong (1978) provided a review of the evidence and this was supported by Tetlock’s (2005) study that involved 82,361 forecasts by 284 experts over two decades.
Long-term climate changes are highly complex due to the many factors that affect climate and to their interactions. Uncertainty about long-term climate changes is high due to a lack of good knowledge about such things as:
a) causes of climate change,
b) direction, lag time, and effect size of causal factors related to climate change,
c) effects of changing temperatures, and
d) costs and benefits of alternative actions to deal with climate changes (e.g., CO2 markets).
Given these conditions, expert opinions are not appropriate for long-term climate predictions.
4. Forecasts are needed for the effects of climate change. 
Even if it were possible to forecast climate changes, it would still be necessary to forecast the effects of climate changes. In other words, in what ways might the effects be beneficial or harmful? Here again, we have been unable to find any scientific forecasts—as opposed to speculation—despite our appeals for such studies.
We addressed this issue with respect to studies involving the possible classification of polar bears as threatened or endangered (Armstrong, Green, and Soon 2008). In our audits of two key papers to support the polar bear listing, 41 principles were clearly violated by the authors of one paper and 61 by the authors of the other. It is not proper from a scientific or from a practical viewpoint to violate any principles. Again, there was no sign that the forecasters realized that they were making mistakes.
5. Forecasts are needed of the costs and benefits of alternative actions that might be taken to combat climate change. 
Assuming that climate change could be accurately forecast, it would be necessary to forecast the costs and benefits of actions taken to reduce harmful effects, and to compare the net benefit with other feasible policies including taking no action. Here again we have been unable to find any scientific forecasts despite our appeals for such studies.
6.  To justify using a climate forecasting model, one would need to test it against a relevant naïve model. 
We used the Forecasting Method Selection Tree to help determine which method is most appropriate for forecasting long-term climate change. A copy of the Tree is attached as Appendix 1. It is drawn from comparative empirical studies from all areas of forecasting. It suggests that extrapolation is appropriate, and we chose a naïve (no change) model as an appropriate benchmark. A forecasting model should not be used unless it can be shown to provide forecasts that are more accurate than those from this naïve model, as it would otherwise increase error. In Green, Armstrong and Soon (2008), we show that the mean absolute error of 108 naïve forecasts for 50 years in the future was 0.24°C.
7. The climate system is stable. 
To assess stability, we examined the errors from naïve forecasts for up to 100 years into the future. Using the U.K. Met Office Hadley Centre’s data, we started with 1850 and used that year’s average temperature as our forecast for the next 100 years. We then calculated the errors for each forecast horizon from 1 to 100. We repeated the process using the average temperature in 1851 as our naïve forecast for the next 100 years, and so on. This “successive updating” continued until year 2006, when we forecasted a single year ahead. This provided 157 one-year-ahead forecasts, 156 two-year-ahead and so on to 58 100-year-ahead forecasts.
We then examined how many forecasts were further than 0.5°C from the observed value. Fewer than 13% of forecasts of up to 65-years-ahead had absolute errors larger than 0.5°C. For longer horizons, fewer than 33% had absolute errors larger than 0.5°C. Given the remarkable stability of global mean temperature, it is unlikely that there would be any practical benefits from a forecasting method that provided more accurate forecasts.
8.  Be conservative and avoid the precautionary principle. 
One of the primary scientific principles in forecasting is to be conservative in the darkness of uncertainty. This principle also argues for the use of the naive no-change extrapolation. Some have argued for the precautionary principle as a way to be conservative. It is a political, not a scientific principle. As we explain in our essay in Appendix 2, it is actually an anti-scientific principle in that it attempts to make decisions without using rational analyses. Instead, cost/benefit analyses are appropriate given the available evidence which suggests that temperature is just as likely to go up as down. However, these analyses should be supported by scientific forecasts.
The reach of these models is extraordinary, for example, the CSIRO models are currently being used in Australia to determine water allocations for farmers and to justify the need for an Emissions Trading Scheme (ETS) – the most far-reaching of possible economic interventions.   Yet, according to Dr Armstrong, these same models violate 72 scientific principles.
********************
1. Marc Morano, James Hansen’s Former NASA Supervisor Declares Himself a Skeptic, January 27,2009. http://epw.senate.gov/public/index.cfm?FuseAction=Minority.Blogs&ContentRecord_id=1a5e6e32-802a-23ad-40ed-ecd53cd3d320
2. “Analysis of the U.S. Environmental Protection Agency’s Advanced Notice of Proposed Rulemaking for Greenhouse Gases”, Drs. J. Scott Armstrong and Kesten C. Green a statement prepared for US Senator Inhofe for an analysis of the US EPA’s proposed policies for greenhouse gases.  http://theclimatebet.com

Sponsored IT training links:
Get guaranteed success in 312-50 exam in first try using incredible 642-374 dumps and other 310-200 training resources prepared by experts.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e98f2bcc7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
\---   
  
Although it’s a favorite headline as people shiver during the coldest parts of the winter, global warming is almost assuredly _not_ behind your suffering (the “warming” part of global warming should have clued you in on this).   
  
But, some folks steadfastly prefer the point of view that all bad weather is caused by climate change.   
  
Consider White House Office of Science and Technology Policy (OSTP) head John Holdren. During the depth of the January 2014 cold outbreak (and the height of the misery) that made “polar vortex” a household name, OSTP released a video featuring Holdren telling us that “the kind of extreme cold being experienced by much of the United States as we speak, is a pattern that we can expect to see with increasing frequency as global warming continues.”   
  
At the time we said “not so fast,” pointing out that there were as many (if not more) findings in the scientific literature that suggested that either a) no relationship exists between global warming and the weather patterns giving rise to mid-latitude cold outbreaks, or b) the opposite is the case (global warming should lead to fewer and milder cold air outbreaks).   
  
The Competitive Enterprise Institute even went as far as to request a formal correction from the White House. The White House responded by saying that the video represented only Holdren’s “personal opinion” and thus no correction was necessary. CEI filed a FOIA request, and after some hemming and hawing, the White House OSTP finally, after a half-hearted search, produced some documents. Unhappy with this outcome, CEI challenged the effort and just this past Monday, a federal court, questioning whether the OSTP acted in “good faith,” granted CEI’s request for discovery.   
  
In the meantime, the scientific literature on this issue continues to accumulate. When a study finds a link between human-caused global warming and winter misery, it makes headlines somewhere. When it doesn’t, that somewhere is usually reduced to here.   




Case in point: Last week, _Washington Post_ ’s Capital Weather Gang published a piece by Jason Samenow that highlighted a pair of new findings that suggested that global warming was leading to more blizzards along the East Coast. The mechanism, favored by the global-warming-is-making-cold/blizzards-worse crowd is that Arctic warming, enhanced by melting sea ice there, is causing the curves (i.e., ridges and troughs) in the jet stream to become bigger, and thus slower. This “locks in” a particular weather pattern and can allow cold air to drop further southward as well as set up condition necessary for big snow storms. To us, this seemed more a case of natural variability than global warming, but we suppose beauty is in the eye of the beholder.   
  
But what you haven’t read in the _Washington Post_ (or anywhere else for that matter), is that an even newer paper has just been published by scientists (including Martin Hoerling) at NOAA’s Earth System Research Laboratory that basically demonstrates that global warming and Arctic sea ice loss should, according to climate models, lead to warmer winter temperatures, less temperature variability, and milder cold air outbreaks. This is basically the opposite conclusion from the one preferred and disseminated by Holdren et al.   
  
From the paper’s abstract:   




The emergence of rapid Arctic warming in recent decades has coincided with unusually cold winters over Northern Hemisphere continents. It has been speculated that this “Warm Arctic, Cold Continents” trend pattern is due to sea ice loss. Here we use multiple models to examine whether such a pattern is indeed forced by sea ice loss specifically, and by anthropogenic forcing in general. While we show much of Arctic amplification in surface warming to result from sea ice loss, we find that neither sea ice loss nor anthropogenic forcing overall to yield trends toward colder continental temperatures. An alternate explanation of the cooling is that it represents a strong articulation of internal atmospheric variability, evidence for which is derived from model data, and physical considerations. Sea ice loss impact on weather variability over the high latitude continents is found, however, characterized by reduced daily temperature variability and fewer cold extremes.



They were even more direct in paper’s conclusion:   




We…showed that sea ice loss impact on daily weather variability over the high latitude continents consists of reduced daily temperature variability and fewer cold extremes indicating that the enhanced occurrences of cold spells during recent winters (e.g., Cohen et al. 2014) are not caused by sea ice loss.



This is pretty emphatic. Global warming results in warmer, less variable winters in North America (Figure 1).   






  
  
_Figure 1. Modeled change in winter mean temperature (left), daily temperature variability (middle), and temperature on the coldest 10 percent of the days (right) as a result of decline in Arctic sea ice. (source: Sun et al., 2016)._   
  
Now, if only our government’s “top scientist” were paying attention.   
  
**Reference:**   
  
Sun, L., J. Perlwitz, and M. Hoerling, 2016. What Caused the Recent “Warm Arctic, Cold Continents” Trend Pattern in Winter Temperatures? _Geophysical Research Letters_ , doi: 10.1002/2016GL069024.


"
"
One of the claims about “global climate change” is that it will affect the normal ranges of flora and fauna of our planet. Well, with a very cold northern hemisphere this winter, that seems to happening. A bird not seen (as a mature adult) in Massachusetts since the 1800’s , an Ivory Gull, normally an inhabitant of arctic areas, has been spotted. Here are the details from the Plymouth, MA Patriot-Ledger. – Anthony

GULL-LOVER’S TRAVELS: Birdwatchers flock to Plymouth to spot rare specimen

PLYMOUTH — Jan 28th, 2009
The temperatures were in the single digits, but not low enough to keep the gawkers away. A celebrity was in town, behind the East Bay Grille, a visitor not seen in these parts in decades, if not longer.
But these weren’t paparazzi, and this wasn’t a Hollywood star. Rather, they were avid birdwatchers – about 20 in all – braving the frigid air as they scanned the bay and the edges of the breakwater with binoculars and spotting scopes.
And they would be rewarded, catching a glimpse of a glimpse of a rare, fully mature ivory gull. A birdwatcher reported seeing one in Plymouth last week, and another was spotted at Eastern Point Lighthouse in Gloucester. From Sunday through Tuesday, the avian visitor was a regular in Plymouth, much to the delight of birdwatchers, who came from near and far in hopes of adding the extremely rare bird to their life list.
Ivory gulls normally stay well above Newfoundland, living on Arctic ice where they follow whales and polar bears to feed on the scraps and carcasses they leave behind after making a kill.

Until this year, the last report of a fully mature ivory gull in Massachusetts was in the 1800s. Three immature birds were seen in the 1940s. In 1976, another immature bird had been spotted in Rockport.
Russell Graham of Dallas is flying in Friday for a three-day visit. He’s hoping the gull will still be in town when he arrives.
“The ivory gull is one of a handful of birds that every birder dreams of seeing but almost no one has.,” he said. “This isn’t a dream that’s confined to North America. There is also an immature bird in France that is causing the same reaction there. There are a couple of places where you can go in the summer and expect to see one but they are distant and expensive – Svalbard on Spitsbergen, Norway and Pond Inlet on Baffin Island, Canada.
“I never thought I would have the chance to see one and I can’t pass up this once-in-a-lifetime opportunity.”
If the gull is gone, Graham will consider a side trip to Nova Scotia, where two adult ivory gulls have been seen recently. “I’ll be keeping my fingers crossed,” he said.
John Fox of Arlington, Va., and his friend Adam D’Onofrio of Petersburg drove more than eight hours on Sunday to see the gull.
“No bird this morning,” Fox said a day later, shaking his head. “We left Virginia at three in the morning yesterday and arrived here 20 minutes too late.”
On Sunday morning, hundreds of people got to observe and photograph the gull as it fed on a chicken carcass someone put out on one of the docks in the parking lot. The bird stayed until 11 a.m., then flew across the harbor. It was not seen again for the rest of the day.
“We arrived at 11:20 and spent the rest of the afternoon in the parking lot, hoping it would return,” Fox said.
They stayed at Pilgrim Sands Motel and arrived at the parking lot early Monday morning for one more chance to see the ivory gull before returning to Virginia. Fox said it was his first time in Massachusetts. If he didn’t see the bird, he said, at least he could see Plymouth Rock before they left for home.
“That’s how it goes sometimes,” he said. “We don’t always see what we come for, but it’s nice to see some of the sights when you travel to a new area in hopes of seeing a rare bird.”
As Fox was planning his exit, a commotion caught his attention. One of the birders pointed toward the sky and said with a shout, “There it is.”
The pure white gull was flying toward the parking lot, silhouetted against a bright blue sky. Someone in the crowd announced for the record the gull had arrived at 7:45 a.m.
The bird flew in circles overhead, then landed on a snow bank in the middle of the parking lot. Cameras clicked and the birders “oohed and ahhhed” each time the ivory gull switched positions.
“Look how white it is,” someone said. “It’s got black feet, black eyes and a grayish-black beak,” said another.
The gull eyeballed the chicken carcass, still there from the day before, but it didn’t eat. Instead, it flew to the railing along the edge of the boat ramp and perched with a group of sea gulls. The photographers followed, changing positions to get the best lighting.
Fox stood with the group, talking with other birdwatchers, as the gull sat peacefully on the railing, observing all the people gathered around it. Was it worth the long drive up from Virginia?
“It sure was,” Fox said with a smile.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e98ddacfb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterWhy do I get the feeling this Accenture/Barclay’s report is going to have a serious backlash? Good if it does! The warmists outlets are already out there doing pre-emptive damage control, going into denial and wishful thinking.
A study carried out by consultants Accenture and Barclays Bank confirms that “climate protection” is going to cost a bundle and will involve “gigantic investments” if Europe’s target of reducing CO2 emissions 20% by 2020 is to be reached. The price tag for Europeans: 2.9 TRILLION euros, i.e. €2,900,000,000,000.00! With 450 million Europeans, that means €6,444.00 for every man, woman and child. The warmist klimaretter writes:
But at the same time, the conclusion that climate protection is expensive cannot be drawn from the calculations. The study does not analyse the costs the of climate protection, but only the necessary investments. Among these there are some that are economically attractive and thus will save money over the years.”
The question ought to be: “What are all these costs going to lead to?” The answer is: nothing. How about taking a look around and opening your eyes? Look at all the poverty out there that is screaming for investment. Look at the sorry state of many schools and hospitals in Europe.
Yet, instead of investing in these important things, the EU wants to blow the money on an energy system that no one really needs – one that is mandated by a fraud.
But the warmists are doing their damnedest to put a positive spin on it. Remember that protecting the climate is an abstract concept that exists only in Fairyland. The concept that we can “protect the climate” is a complete myth. As best I can tell, climate protection for warmists means the production of good weather. Good luck! Klimaretter writes:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The study shows foremost that climate protection is a big business opportunity, also for banking services because it will provide a large share of the needed capital. Therefore one has to view the necessary investments as a chance for the economy.
That this is going to be a big bonanza for the banks is no exaggeration. They are going to make a killing, but at the expense of the consumer. And saying these investments are a “chance” for the economy is really incorrect labelling. It’s going to be a “gamble” for the economy – Russian Roulette style. Klimaretter writes:
Climate protection is ‘one of the megatrends of the economy’.
They’re right about that. But that does not make it a trend that will lead to success. Having everyone go off the edge of a cliff as a trend does not make it a reasonable endeavour. Klimaretter then reports on Environment Minister Norbert Röttgen meeting with industry leaders of Allianz, Metro, Siemens and Viessmann, saying that the energetic renovation of buildings will have top priority and “the politicians must see to it that the renovation is equally attractive for property owners and users.”
More on Allianz in a day or two. In the meantime just keep in mind that this is all based on “purchased science”.  If they looked at the real science, all that capital could be directed to real problems.
======================================================
Sorry folks for the disappearing articles. It’s been a long day and I’ve hit some wrong buttons and so a really rough draft that was not meant to appear showed up as a new post. I’ll most likely post it tomorrow. – PG
Share this...FacebookTwitter "
"
This NOAA press release just showed up in my inbox, it seems to be a completely different take on the Hurricane season than that of Florida State’s COAPS and Ryan Maue who says:
Record inactivity continues:  Past 24-months of Northern Hemisphere TC activity (ACE) lowest in 30-years.
 
Global and Northern Hemisphere Accumulated Cyclone Energy: 24 month running sum through October 31, 2008. Note that the year indicated represents the value of ACE through the previous 24-months.
This was discussed at length at Climate Audit here


FOR  IMMEDIATE RELEASE
Contact: Carmeyia  Gillis
Nov. 26,  2008
301-763-8000,  ext. 7163 (office)
  240-882-9047  (cellular)
Dennis  Feltgen
305-229-4404  (office)
305-433-1933  (cellular)
 
Atlantic  Hurricane Season Sets Records 
The 2008 Atlantic Hurricane Season  officially comes to a close on Sunday, marking the end of a season that produced  a record number of consecutive storms to strike the United States and ranks as  one of the more active seasons in the 64 years since comprehensive records  began.
A total of 16 named storms formed this  season, based on an operational estimate by NOAA’s National Hurricane Center.  The storms included eight hurricanes, five of which were major hurricanes at  Category 3 strength or higher. These numbers fall within the ranges predicted in  NOAA’s pre- and mid-season outlooks issued in May and August. The August outlook  called for 14 to 18 named storms, seven to 10 hurricanes and three to six major  hurricanes. An average season has 11 named storms, six hurricanes and two major  hurricanes.
“This year’s hurricane season  continues the current active hurricane era and is the tenth season to produce  above-normal activity in the past 14 years,” said Gerry Bell, Ph.D., lead  seasonal hurricane forecaster at NOAA’s Climate Prediction  Center.
Overall, the season is tied as the  fourth most active in terms of named storms (16) and major hurricanes (five),  and is tied as the fifth most active in terms of hurricanes (eight) since 1944,  which was the first year aircraft missions flew into tropical storms and  hurricanes.
For the first time on record, six  consecutive tropical cyclones (Dolly, Edouard, Fay, Gustav, Hanna and Ike) made  landfall on the U.S. mainland and a record three major hurricanes (Gustav, Ike  and Paloma) struck Cuba. This is also the first Atlantic season to have a major  hurricane (Category 3) form in five consecutive months (July: Bertha, August:  Gustav, September: Ike, October: Omar, November: Paloma).
Bell attributes this year’s above-normal  season to conditions that include:

An ongoing multi-decadal signal. This  combination of ocean and atmospheric conditions has  spawned increased hurricane activity since 1995.
Lingering La Niña effects. Although  the La Niña that began in the Fall of 2007 ended in June, its influence of light  wind shear lingered.
Warmer tropical  Atlantic  Ocean temperatures. On average, the tropical  Atlantic was about 1.0 degree Fahrenheit  above normal during the peak of the season.

NOAA’s National Hurricane Center is  conducting comprehensive post-event assessments of each named storm of the  season. Some of the early noteworthy findings include:

Bertha was a tropical cyclone for 17  days (July 3-20), making it the longest-lived July storm on record in the  Atlantic  Basin.
Fay is the only storm on record to  make landfall four times in the state of Florida, and to prompt tropical storm  and hurricane watches and warnings for the state’s entire coastline (at various  times during its August lifespan).
Paloma, reaching Category 4 status  with top winds of 145 mph, is the second strongest November hurricane on record  (behind Lenny in 1999 with top winds of 155 mph).

Much of the storm-specific information  is based on operational estimates and some changes could be made during the  review process that is underway.
“The information we’ll gain by  assessing the events from the 2008 hurricane season will help us do an even  better job in the future,” said Bill Read, director of NOAA’s National Hurricane  Center. “With this season behind us, it’s time to prepare for the one that lies  ahead.”
NOAA will issue its initial 2009  Atlantic Hurricane Outlook in May, prior to the official start of the season on  June 1.
NOAA understands and predicts changes  in the Earth’s environment, from the depths of the ocean to the surface of the  sun, and conserves and manages our coastal and marine  resources.
A graphic track map of this season’s  storms and satellite visualization of the entire season is available at  http://www.noaa.gov.
On the Web:
NOAA’s Climate Prediction Center:  http://www.cpc.ncep.noaa.gov
NOAA’s National Hurricane Center:  http://www.hurricanes.gov
NHC 2008 Tropical Cyclone Reports:  http://www.nhc.noaa.gov/2008atlan.shtml
###


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9af631e5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Activists from Extinction Rebellion (XR) blocked traffic outside a London fashion week venue on Saturday and also staged a protest at Gatwick airport. Dozens of demonstrators prevented traffic from passing through a busy intersection leading to the Strand in Westminster, where the fashion trade show was being held.  Protesters carried placards reading: “No more false fashion” and “No fashion on a dead planet,” while others wore dresses made from chains. Last week, XR members delivered a letter to the British Fashion Council, calling for it to cancel the next London fashion week, due to be held in September. Sara Arnold, who helped coordinate the protest, said: “London is home to the cutting edge of sustainable and ethical design and yet London fashion week lags behind. “And despite having an active political engagement programme, you have done almost nothing to lobby for environmental policies, without which a transition with the urgency we need is simply impossible. “We have all failed, but now radical leadership is required. We need you, the British Fashion Council, as appointed industry administrators, to find the power and courage to centre a visionary process and protocol, without delay.” A handful of activists held a peaceful demo at Gatwick in Sussex, including one man dressed in a tiger onesie, to raise awareness of aircraft pollution. XR activists in disguise gathered in the airport’s south terminal at about 9.30am before revealing themselves. Protesters were instructed to arrive incognito and pretend to be waiting to meet someone amid fears they would not be allowed in the airport. The group of about 10 activists unveiled their full complement of XR banners, shirts and badges, and began mingling with the public. Passengers landing on flights from Salzburg, Madrid and Kingston were greeted by the protesters. Dan Burke, 16, a youth activist, said: ”We are already in climate crisis. We need to act now and, as we have seen in history, one of the best ways to bring forward actual legislation is to be in nonviolent disobedience.” Leaflets handed out apologised for the disruption but said: “We need your help.” A post on Facebook for the Gatwick Action event said: “Let’s get the message out – change can happen – and those who fly have the opportunity to make a big contribution by cutting their flights.”"
"To exaggerate is human, and scientists are human. Exaggeration and the complementary art of simplification are the basic rhetorical tools of human intercourse. So  yes, scientists do exaggerate.  So do politicians, perhaps even when, as the UK’s former environment secretary Owen Paterson did, they claim that climate change forecasts are “widely exaggerated”.  A more pertinent question is: does the way in which scientists and politicians speak publicly lead to wild exaggeration? When both are engaged in advocacy, there is little difference; both politicians and scientists will use whatever rhetorical devices they have to win an argument.  But this is not the case when scientists speak publicly through their own very special form of mass media, the peer-reviewed literature. In peer-review, statements that do not follow deductively from the data are subject to forensic examination and often expunged, or at least subjected to the “death by caveat” that makes so much academic writing almost indigestible.  Peer-review is by no means flawless, as it is a human procedure and so is subject to the same blind spots and groupthink of every human endeavour. Nevertheless, it does tend to make scientists cautious in their statements and wary of adversarial debate. The peer-review process is becoming more transparent, which may not remove its biases completely, but will allow them to be discovered more easily. Owen Paterson would have done better to focus on exaggeration’s rhetorical twin: simplification. Simplification is not expunged by the peer-review process. Far from it; simplification is at the very heart of how scientists, including climate scientists, do science.  The world is a complicated place, and understanding – let alone predicting – the climate requires consideration of each compartment of the environment: the atmosphere, ocean, land, and ice. In each compartment, physical, chemical, and biological processes interact strongly.  Faced with such complexity, scientists have adopted a hierarchical approach to knowledge generation. This means using highly simplified conceptual models to sketch out the limits of the system; analyses of intermediate complexity to scope out the broad-brush behaviour; and models of often mind-numbing intricacy to attempt to simulate the “real thing”.  These models of reality can be physical or computational: in the Birmingham Institute of Forest Research we are building a physical model of future ecosystem behaviour which bears comparison with the Large Hadron Collider in its ambition and complexity. Even so, we will be pressed on, and perhaps even criticised for, the simplifications we have had to make in order to set up a manageable and affordable experiment. Such is the life scientific.  Our results may lie at the extreme end of our current best guesses (forcing us to make further corroborative experiments), or may lie comfortably in the middle (hence improving our confidence) – in neither case will they be adequately described as exaggeration. In general, limiting or extreme results come about because a simplified analysis is missing an important feedback or because an intricate model is being “exercised” by simulating an extreme scenario.  Yet such simplifications are the stuff of science. Galileo, for instance, was only able to develop his ideas on gravity by ignoring air resistance. He simplified things to get at a fundamental property of the world about us. This process represents exaggeration only when the results are taken out of their experimental context. Scientists, wandering unwarily into the realm of advocacy, may be guilty of taking the results out of context, as may be activists and politicians, but it is not the science itself that is “widely exaggerated”. Is UK energy policy informed solely by the exaggerations of advocacy – political or scientific – or, at least in part, by the exaggeration-phobic scientific literature? As a taxpayer I would like to believe that physical and computer models provide evidence to politicians who use it to assess the strength of the arguments of the various advocacy groups. I am not so politically naïve as to believe that all policy is, or even should be, based solely on objective evidence.  I do hope, though, that claims of scientific exaggeration are seen for what they are: advocacy targeted not just at winning the rhetorical argument but also aimed, rather cynically, at undermining the evidence."
"

One of the most interesting legal issues percolating in the American legal system is jury nullification. A poll by Decision Quest and the National Law Journal in 1998 found that three out of four citizens believed jurors should do what they think is right regardless of what the judge says the law is. In February 1999 the Washington Post found “a significant pattern of juror defiance” that has resulted in a sharp increase in the number of hung juries. Alarmed by these developments, trial judges are aggressively attempting to thwart jury nullification. The question now before the courts is, How far can a trial judge go before overstepping his or her authority and coercing a verdict? 



Over the past year a string of rulings have illustrated the problem. In April the Colorado Court of Appeals overturned a contempt of court conviction against a juror for her failure to reveal during voir dire that she did not intend to follow the trial judge’s instructions on the law. _People v Kriho_ , No. 97CAD700, 1999 WL 249143 (unpublished decision). In May a U.S. district court judge dismissed a deliberating juror for her unwillingness to follow the law and allowed the remaining jurors to convict the defendants on trial. _U. v Abbell_ (SD Fla) No. 93–0470-CR. In June the U.S. Court of Appeals for the Ninth Circuit reversed the convictions ofJohn Fife Symington III, the former governor of Arizona, because his Sixth Amendment right to an impartial jury was violated when the trial judge dismissed a holdout juror after eight days of deliberations. _U.S. v Symington_ , 1999 _Daily Journal_ DAR 6295, 1999 WL 415345-



The California Supreme Court will soon decide whether a trial court violated a defendant’s right to trial by jury by dismissing a deliberating juror because he did not intend to follow the law on a particular charge. People v Williams, No. S066106 (rev gr Feb. 18, 1998). With all of this appellate court activity, the issue seems destined to go to the U.S. Supreme Court for resolution 



A surprising number of lawyers hold ill‐​considered opinions about jury nullification. The conventional wisdom holds that jury nullification is absolutely improper. Thus, a trial judge should dismiss any juror who refuses to follow the court’s legal instructions. On first blush, that line of reasoning appears sound, but a closer examination will show that it is mistaken 



A leading case in this area of law, _U. v Thomas_ (1997) 116 FM 606, embodies the conventional wisdom on jury nullification, so it is useful to scrutinize the rationale of its holding Judge Jose A. eabranes opined that because “no juror has a right to engage in nullification,” trial courts have a duty to thwart nullification by dismissing any juror who refuses to follow the letter of the law. Although the judge acknowledged that nullification sometimes occurs, he said that it happens because the specter of nullification does not come “to the attention of a presiding judge before the completion of [the] jury’s work” 116 F3d at 616. Thus, he concluded, a trial judge has a responsibility to thwart such “misconduct” whenever the opportunity arises. 



There are several problems with Judge Cabranes’s analysis. First, his claim that jurors do not have the right to engage in jury nullification would have startled the framers of the Constitution. John Adams, who signed the Declaration of Independence and served as our second president, observed that “It is not only [the juror’s] right, but his duty … to find the verdict according to his own best understanding, judgment, and conscience, though in direct opposition to the direction of the court.” It is important to note that Adams was not just speaking for himself. Similar statements by Hamilton, Jefferson, and others show that Judge Cabranes’s view of jury service is at odds with the original understanding of that duty.



Second, the judge’s claim that “trial courts have [a] duty to forestall” jury nullification is weak. If such a duty truly existed, the law would give trial judges the discretionary power to direct verdicts for the prosecution. The same principle that denies judges the discretion to direct guilty verdicts should also operate to deny judges the discretion to dismiss deliberating jurors.



Third, Judge Cabranes seems to think that nullification sometimes occurs because “jurors are not answerable for nullification after the verdict has been reached.” That claim may be true, but it does not lend much support to his argument. After‐​all, the law could empower trial judges to enter a judgment of conviction notwithstanding the verdict. As is the case in civil trials, a JNOV would “cure” a nullification verdict and leave the jurors themselves “unanswerable” regarding the votes they cast in the jury room‐ The same principle that denies judges the power to grant JNOVs in criminal cases should also operate to deny judges the power to dismiss jurors who refuse to condemn the defendant in the circumstances of the case before them. 



Fourth, Judge Cabranes’s rule, which would permit trial judges to dismiss deliberating jurors, would yield highly unsatisfactory results. In the Thomas case, several jurors complained to the court that a verdict could not be reached because a lone holdout refined to follow the law. That juror was subsequently dismissed. The propriety of that dismissal became the central issue on appeal. Judge Cabranes’s opinion said that in such situations the trial judge has to proceed cautiously to determine if such complaints have merit. If the allegations are found to be true, the trial judge should dismiss the holdout juror for “misconduct.” ” Such an approach sounds reasonable to many lawyers, but consider the implications of such a procedure under some alternative fact patterns.



What if, after a full week of deliberations, a trial judge learns that two or three jurors have decided that they cannot in good conscience enforce the law against the defendant? Are we going to allow several alternates to take their places so that a conviction can be obtained? Take a defendant on trial for multiple charges. Wha~if two jurors believe that the defendant is indeed technically guilty on every count, but they disagree with the other jurors with respect to whether the defendant deserves to have the proverbial book thrown at hirn. If the “hard‐​line” jurors complain to the judge, should the trial court throw the “lenient” jurors off the case because they are unwilling to convict on every single count? According to Judge Cabranes’s reasoning the answer would be yes. Such meddling with the give‐​and‐​take of jury deliberations cannot possibly be reconciled with the defendant’s right to a trial by jury and to a unanimous verdict. Make no mistake, if trial courts begin to exercise power in this way, trial by jury will be so hamstrung as to be unrecognizable.



There is a better way to resolve such controversies. The key lies in the duty of jurors to deliberate. The duty to deliberate means that each juror must be willing to listen to the views of others with a disposition toward reexamining his or her own views.That is the duty that ought to concern the trial court. Should a judge receive a note that complains about a juror who refines to follow the letter of the law, the trial judge should resist the impulse to conduct an inquisition. Instead, the judge should calmly and respectfully ascertain whether the so‐​called holdout is willing to deliberate. Of course, a court’s instruction about the duty to deliberate should always be followed by the caveat that no juror should surrender a conscientious opinion solely because of the opinion of other jurors or for the mere purpose of returning a unanimous verdict. 



**Any juror who is unable or unwilling to deliberate** should be excused and replaced with an alternate. But a deliberating juror’s conscientious refusal to cast a vote for conviction should not be considered misconduct and thus a sufficient basis for removal. When a jury is deadlocked because one or more jurors cannot in good conscience vote to convict, the trial judge has only two options: send the jury back to continue deliberating or declare a mistrial. Unlike the holding in the Thomas case, this approach is attentive to the twin imperatives. of safeguarding the province of the jury from incursions fi‐​om the bench and protecting the defendant’s right to a unanimous verdict.



Jury nullification seems to be one of those topics where strong passions overcome reasoned discussion. Some lawyers focus obsessively on instances in which a jury or juror abused the system. Clearly, some cases of abuse exist. To maintain perspective, however, lawyers ought to recall some words of wisdom from judge David L. Bazelon, concurring in US. v Dougherty (1972) 473 F2d 1113, 1142: “Trust in the jury is … one of the cornerstones of our entire criminal jurisprudence, and if that trust is without foundation we must re‐​examine a great deal more than just the nullification doc‐ trine.” And since even conservatives rec‐ ognize that the Constitution placed its trust in juries, not judges, to determine criminal guilt, there’s a good chance that the hostile climate surrounding jury nullification may eventually subside. 
"
"The Earth’s climate has always changed. All species eventually become extinct. But a new study has brought into sharp relief the fact that humans have, in the context of geological timescales, produced near instantaneous planetary-scale disruption. We are sowing the seeds of havoc on the Earth, it suggests, and the time is fast approaching when we will reap this harvest.  This in the year that the UN climate change circus will pitch its tents in Paris. December’s Conference of the Parties will be the first time individual nations submit their proposals for their carbon emission reduction targets. Sparks are sure to fly. The research, published in the journal Science, should focus the minds of delegates and their nations as it lays out in authoritative fashion how far we are driving the climate and other vital Earth systems beyond any safe operating space. The paper, headed by Will Steffen of the Australian National University and Stockholm Resilience Centre, concludes that our industrialised civilisation is driving a number of key planetary processes into areas of high risk. It argues climate change along with “biodiversity integrity” should be recognised as core elements of the Earth system. These are two of nine planetary boundaries that we must remain within if we are to avoid undermining the biophysical systems our species depends upon.  The original planetary boundaries were conceived in 2009 by a team lead by Johan Rockstrom, also of the Stockholm Resilience Centre. Together with his co-authors, Rockstrom produced a list of nine human-driven changes to the Earth’s system: climate change, ocean acidification, stratospheric ozone depletion, alteration of nitrogen and phosphorus cycling, freshwater consumption, land use change, biodiversity loss, aerosol and chemical pollution. Each of these nine, if driven hard enough, could alter the planet to the point where it becomes a much less hospitable place on which to live. The past 11,000 years have seen a remarkably stable climate. The name given to this most recent geological epoch is the Holocene. It is perhaps no coincidence that human civilisation emerged during this period of stability. What is certain is that our civilisation is in very important ways dependent on the Earth system remaining within or at least approximately near Holocene conditions. This is why Rockstrom and co looked at human impacts in these nine different areas. They wanted to consider the risk of humans bringing about the end of the Holocene. Some would argue that we have already entered a new geological epoch – the Anthropocene – which recognises that Homo sapiens have become a planet-altering species. But the planetary boundaries concepts doesn’t just attempt to quantify human impacts. It seeks to understand how they may affect human welfare now, and in the future.   The 2009 paper proved to be very influential, but it also attracted a fair amount of criticism. For example, it has been argued that some of the boundaries are not in fact global in scale. There are very large regional variations in consumption of freshwater and phosphorus fertiliser pollution, for instance. 


Phosphorous pollution in croplands.
Steffen et al

 That means that while globally we may be in the green, there could be an increasing number of regions that are deep in the red. The latest research develops the methodology so that it now includes regional evaluations. For example it assesses basin-level freshwater use and biome-level species extinction rates. It also includes a new boundary of “novel entities” – new forms of life and novel compounds the likes of which the Earth system has not experienced and so impact of which is extremely challenging to assess. Ozone-depleting CFCs are perhaps the best example of how a seemingly inert substance can produce planetary damage. The paper also gives an update on where we stand on some of the planetary boundaries. At first sight, it looks as though there may be some good news in that climate change is no longer in the red. But then closer inspection reveals that a new yellow “zone of uncertainty with increasing risk” has been added to the previous green and red classification.  Climate change impacts are firmly within this new yellow zone. Our atmosphere currently has about 400 parts per million (ppm) of carbon dioxide. To recover back to the green zone we still need to get back to 350ppm – the same precautionary boundary as before. Perhaps most importantly the research produces a two-tier hierarchy in which climate change and biosphere integrity are recognised as the core planetary boundaries through which the others operate. This makes sense: life and climate are the main columns buttressing our continual existence within the Holocene. Weakening them risks amplifying other stresses on other boundaries. And so to the very bad news. Given the importance of biodiversity to the functioning of the Earth’s climate and the other planetary boundaries, it is with real dismay that this study adds yet more evidence to the already burgeoning pile that concludes we appear to be doing our best to destroy it as fast as we possibly can.  Extinction rates are very hard to measure but the background rate – the rate at which species would be lost in the absence of human impacts – is something like ten a year per million species. Current extinction rates are anywhere between 100 to 1000 times higher than that. We are possibly in the middle of one of the great mass extinctions in the history of life on Earth.  James Dyke is answering your questions about planetary boundaries in a Reddit AMA."
nan
"It is 2050. Beyond the emissions reductions registered in 2015, no further efforts were made to control emissions. We are heading for a world that will be more than 3C warmer by 2100 The first thing that hits you is the air. In many places around the world, the air is hot, heavy and, depending on the day, clogged with particulate pollution. Your eyes often water. Your cough never seems to disappear. You think about some countries in Asia, where, out of consideration, sick people used to wear white masks to protect others from airborne infection. Now you often wear a mask to protect yourself from air pollution. You can no longer simply walk out your front door and breathe fresh air: there might not be any. Instead, before opening doors or windows in the morning, you check your phone to see what the air quality will be.  Fewer people work outdoors and even indoors the air can taste slightly acidic, sometimes making you feel nauseated. The last coal furnaces closed 10 years ago, but that hasn’t made much difference in air quality around the world because you are still breathing dangerous exhaust fumes from millions of cars and buses everywhere. Our world is getting hotter. Over the next two decades, projections tell us that temperatures in some areas of the globe will rise even higher, an irreversible development now utterly beyond our control. Oceans, forests, plants, trees and soil had for many years absorbed half the carbon dioxide we spewed out. Now there are few forests left, most of them either logged or consumed by wildfire, and the permafrost is belching greenhouse gases into an already overburdened atmosphere. The increasing heat of the Earth is suffocating us and in five to 10 years, vast swaths of the planet will be increasingly inhospitable to humans. We don’t know how hospitable the arid regions of Australia, South Africa and the western United States will be by 2100. No one knows what the future holds for their children and grandchildren: tipping point after tipping point is being reached, casting doubt on the form of future civilisation. Some say that humans will be cast to the winds again, gathering in small tribes, hunkered down and living on whatever patch of land might sustain them. More moisture in the air and higher sea surface temperatures have caused a surge in extreme hurricanes and tropical storms. Recently, coastal cities in Bangladesh, Mexico, the United States and elsewhere have suffered brutal infrastructure destruction and extreme flooding, killing many thousands and displacing millions. This happens with increasing frequency now. Every day, because of rising water levels, some part of the world must evacuate to higher ground. Every day, the news shows images of mothers with babies strapped to their backs, wading through floodwaters and homes ripped apart by vicious currents that resemble mountain rivers. News stories tell of people living in houses with water up to their ankles because they have nowhere else to go, their children coughing and wheezing because of the mould growing in their beds, insurance companies declaring bankruptcy, leaving survivors without resources to rebuild their lives. Contaminated water supplies, sea salt intrusions and agricultural runoff are the order of the day. Because multiple disasters are often happening simultaneously, it can take weeks or even months for basic food and water relief to reach areas pummelled by extreme floods. Diseases such as malaria, dengue, cholera, respiratory illnesses and malnutrition are rampant. You try not to think about the 2 billion people who live in the hottest parts of the world, where, for upwards of 45 days per year, temperatures skyrocket to 60C (140F), a point at which the human body cannot be outside for longer than about six hours because it loses the ability to cool itself down. Places such as central India are becoming increasingly challenging to inhabit. Mass migrations to less hot rural areas are beset by a host of refugee problems, civil unrest and bloodshed over diminished water availability. Food production swings wildly from month to month, season to season, depending on where you live. More people are starving than ever before. Climate zones have shifted, so some new areas have become available for agriculture (Alaska, the Arctic), while others have dried up (Mexico, California). Still others are unstable because of the extreme heat, never mind flooding, wildfire and tornadoes. This makes the food supply in general highly unpredictable. Global trade has slowed as countries seek to hold on to their own resources. Countries with enough food are resolute about holding on to it. As a result, food riots, coups and civil wars are throwing the world’s most vulnerable from the frying pan into the fire. As developed countries seek to seal their borders from mass migration, they too feel the consequences. Most countries’ armies are now just highly militarised border patrols. Some countries are letting people in, but only under conditions approaching indentured servitude.  Those living within stable countries may be physically safe, yes, but the psychological toll is mounting. With each new tipping point passed, they feel hope slipping away. There is no chance of stopping the runaway warming of our planet and no doubt we are slowly but surely heading towards some kind of collapse. And not just because it’s too hot. Melting permafrost is also releasing ancient microbes that today’s humans have never been exposed to and, as a result, have no resistance to. Diseases spread by mosquitoes and ticks are rampant as these species flourish in the changed climate, spreading to previously safe parts of the planet, increasingly overwhelming us. Worse still, the public health crisis of antibiotic resistance has only intensified as the population has grown denser in inhabitable areas and temperatures continue to rise. The demise of the human species is being discussed more and more. For many, the only uncertainty is how long we’ll last, how many more generations will see the light of day. Suicides are the most obvious manifestation of the prevailing despair, but there are other indications: a sense of bottomless loss, unbearable guilt and fierce resentment at previous generations who didn’t do what was necessary to ward off this unstoppable calamity. • This is an edited extract from The Future We Choose: Surviving the Climate Crisis by Christiana Figueres and Tom Rivett-Carnac, published by Manilla Press (£12.99). To order a copy go to guardianbookshop.com. Free UK p&p over £15 • Christiana Figueres and Tom Rivett-Carnac will be in conversation at a Guardian Live event at the Royal Geographical Society, London SW7, on Tuesday 3 March, 7pm"
"
During our last check in, we had a look at northern Canada from the Arctic Circle to the North pole, and found we had quite a ways to go before we see an “ice free arctic” this year as some have speculated.
Today I did a check of the NASA rapidfire site for TERRA/MODIS satellite images and grabbed a view showing northern Greenland all the way to the North Pole.
There’s some bergy bits on the northeastern shore of Greenland, but in the cloud free area extending all the way to the pole, it appears to still be solid ice.

Click for a larger image – Note: image has been rotated 90° clockwise and sat view sector icon and time stamp added, along with “N” for north pole marker.
Link to original source image is here:
http://rapidfire.sci.gsfc.nasa.gov/realtime/single.php?T082121805
With more than half of the summer melt season gone, it looks like an uphill battle for an ice-free arctic this year.
Here is another view from today from the Aqua satellite:

Click for a larger image – Note: image has been rotated 90° counter- clockwise and sat view sector icon and time stamp added, along with “N” for north pole marker.
Source image is here:
http://rapidfire.sci.gsfc.nasa.gov/realtime/single.php?A082121655
This dovetails with a press release and news story about more ice than normal in the Barents Sea
From the Barents Observer:
http://www.barentsobserver.com/?cat=16149&id=4498513
New data from  the Norwegian Meteorological Institute shows that there is more ice than normal  in the Arctic waters north of the Svalbard archipelago.
In most years, there are open waters in the area north of the archipelago in  July month. Studies from this year however show that the area is covered by ice,  the Meteorological Institute writes in a press release.
In mid-July, the research vessel Lance and the Swedish ship MV Stockholm got  stuck in ice in the area and needed help from the Norwegian Coast Guard to get  loose.
The ice findings from the area spurred surprise among the researchers, many  of whom expect the very North Pole to be ice-free by September this year.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d8b805c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Germany is well on its way towards having a predominantly green electricity supply. The transition from nuclear and fossil fuel electricity to using renewables is happening faster than anyone had anticipated. This is a success, but there is a downside: it is hugely expensive.  The energy transition is an explicit policy goal in Germany, having been made a priority project by the German chancellor, Angela Merkel. It has four strands: reducing CO2 emissions, improving energy efficiency, promoting renewable energy and the gradual phase-out of nuclear power. Nuclear phase-out is actually an old story that started in 2000 when the Schroeder administration first announced a 20-year timetable. It was a bit of a “yes-no” rollercoaster until the Fukushima incident, after which the decision in favour was final. This is widely supported by the German public, meaning that nuclear power is politically not an option at the moment. Yet without a doubt, the most significant development within the energy transition project has been the growth of Germany’s renewable energy sources (RES). The chart below shows how it has developed in the past few years and where the government expects it to be by 2050.  Germany’s renewable energy supply The horizontal black line depicts the approximate maximum demand at any time, which is about 85GW (this will not change much in the future). This shows that installed renewable capacity is now already more or less equal to maximum demand. On a very sunny and windy day, renewables are now capable of meeting the demands of the entire country.   But as we all know, the weather is notoriously unreliable and variable. So a secure system needs more renewable capacity and also more reserve capacity from conventional power plants (mainly fuelled by natural gas) to make sure it can always meet demand. As the chart indicates, installed renewable capacity in 2050 is expected to be 180GW, which is roughly twice maximum demand. By that time, the target is that 80% of electricity supply will be from renewables (basically this is how much renewable power you need to meet this level of supply on a regular basis).  In common with other countries moving in the same direction, the government has various motives for this big shift. Renewables are carbon-free and rely on no fossil fuels, so they are an essential component of meeting European emissions targets. The government hopes for positive spin-off effects on exports, innovation and new jobs. And once the investment cost of the transition has been incurred, we would hope that electricity supply is actually quite cheap. After all, sun and wind are free. Germany sees the energy transition as an investment in the future: we pay for the next generation. The move to renewables has been a success. It has happened at high speed since the late 1990s. The debate is no longer whether it will succeed, but rather what do we do with “too much” renewable power. But behind this positive story, the dark side is the huge expense.  Early in 2013, the then-minister of environment Peter Altmaier mentioned the staggering amount of €1 trillion (£790bn) as the potential cost of the overall transition. This relied on a quick-and-dirty back-of-the-envelope calculation, which raises many questions and was never confirmed, but it does give a feel for the order of magnitude. The end-users – and thus the voters in Germany – are starting to feel the pain. Since the installation costs mean that renewables currently cost more per unit of power than conventional power, they are subsidised by a surcharge on the electricity price. In other words, electricity end-users directly pay for it. As you can see from the chart below, the surcharge for small end-users has soared since 2009 to cope with the rapid growth of installed capacity (the step-change that year reflected a sudden big rise in solar power, which is particularly expensive). The total subsidy is currently about €20bn/year, which amounts to €218/year per household on top of the normal electricity bill. Whether this is still affordable is a key question in the country right now. Germany’s rising renewable surcharge The energy transition has meanwhile changed the face of the electricity market, with severe consequences for traditional firms like E.ON and RWE. They are suffering badly at the moment and are having to rethink their business models completely. In short, they face three challenges. The nuclear phase-out means they have to make very significant write-downs on their nuclear plants, at a loss to the shareholders. They are still fighting the government for compensation payments.  Second, renewable power is suppressing electricity wholesale prices – essentially because they are cheaper to run per unit of power, which under the rules for calculating the wholesale price tends to bring it down across the board. This means that the revenues for conventional power plants are low and no longer cover the investment costs.  Third, conventional power from gas and coal is being pushed out of the market. This means that a lot of conventional power plants are largely standing idle and not making any money. Since the future business model for such plants is looking bleak, the power companies are sitting on investments, which are not going to be profitable. Of course, RWE and E.ON are adjusting their long-term strategies to survive. While this has been going on, the rising costs for residential end-users have become a political problem. In 2014 the government responded with a reform package, which slows down the energy transition in an attempt to control the costs. Basically the annual growth of new renewables has been capped to a pre-determined level.  This seems to be working. The surcharge for 2015 has been calculated at 6.17ct/kWh, which is a small decline compared to 2014. Politically, this may well have been a wise policy, as public support for the energy transition was dwindling. It means that green energy development will happen more slowly. So far the government appears to be standing by the same targets outlined in my first chart, perhaps because the explosion in development over the past few years had put it on an even faster track. Whatever happens from here, one thing remains key: without public support, the energy transition will not work."
"
Share this...FacebookTwitterFakta Menarik Akun Pro Di Situs BandarQQ – Banyak sekali pembicaraan mengenai akun pro situs bandarqq. Beberapa orang mengatakan bahwa akun pro hanyalah fiktif belaka. Namun sebagian orang juga ada yang percaya dengan adanya akun pro ini. Membingungkan, ya. Lalu, menurut kamu bagaimana? Apakah benar-benar ada akun pro itu? Atau memang hanya fiktif?
Nah, sebelum membahas lebih lanjut, kamu mesti tahu nih bahwa akun pro hanya ada di beberapa level atau kalangan. Jadi, wajar saja jika masih banyak orang yang tidak mengetahuinya. CS judi online pun tidak akan tahu jika di situsnya memang tidak mengadakan akun pro. Berikut adalah beberapa ulasan mengenai bocoran akun pro di situs judi online BandarQQ yang tidak diketahui banyak orang:
Rahasia High Profit Pengguna Akun Pro
Pada umumnya, situs judi online memang memiliki rahasia dan juga cara curang dalam bermain. Namun banyak orang yang tidak mengetahuinya atau mereka tahu tetapi takut untuk memberitahu. Biasanya, para pemilik situs high profit-lah yang tahu rahasia dan cara curang seperti akun pro. Termasuk akun pro situs BandarQQ.
Yang perlu kamu ketahui tentang situs high profit ialah keadaan jika kamu menang maka pemilik situs dapat mengambil upah dari member yang menang saat bermain. Maka dari itu kamu tidak perlu heran jika pemilik akun pro sangat mudah menang.
Penyebab Anda Sering Kalah Meskipun Sudah Menggunakan Akun Judi Pro di Situs Agen BandarQQ
Harap diingat bahwa menggunakan akun pro tidak akan membuatmu selalu menang. Keuntungan memakai akun pro ini ialah kamu mempunyai kesempatan menang lebih besar dibandingkan menggunakan akun biasa. Bukan akan terus menang. Maka, wajar saja jika kamu masih bisa kalah. Namun jika kamu kalah berturut-turut dan tidak pernah menang sama sekali mungkin  ada masalah. Berikut ini ulasan penyebab mengapa kamu tidak menang-menang meski menggunakan akun pro termasuk akun pro situs BandarQQ!


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Belum Meng-upgrade Akun Pro
Kamu harus paham jika memakai akun pro, kamu wajib meng-upgradenya agar kelebihannya tidak berkurang.
Kurang beruntung
Kamu sudah meng-upgrade akun pro tetapi tetap kalah terus? Jika seperti itu kemungkinan besar adalah kamu sedang tidak beruntung. Karena, sekali lagi yang perlu kamu ingat, menggunakan akun pro termasuk akun pro situs BandarQQ juga bisa membuatmu kalah.
Penyebab Akun Pro Tidak Lagi Sakti
Akun pro memang sudah terbukti bisa membuat kamu mempunyai kesempatan menang lebih besar dibanding akun biasa. Namun ada masanya jika akun pro milik kamu tidak sakti lagi bahkan bisa membuat kamu sangat merugi! Berikut ini ulasan penyebab akun pro milik kamu tidak sakti lagi.
Nilai Deposit Milik Kamu Ternyata Menurun
Kamu tidak pernah meninggikan nilai depositmu. Ini adalah faktor yang paling memungkinkan jika akun pro milik kamu tidak lagi membawa keberuntungan. Termasuk akun pro situs BandarQQ. Maka, silakan naikkan nilai deposit yang kamu miliki.
Kamu Tidak  Pernah Lagi Melakukan Deposit
Penyebab berikutnya adalah akibat dari kesalahan kamu sendiri. Kamu tidak pernah lagi melakukan deposit. Banyak sekali penyebab orang tidak melakukan deposit lagi, salah satu contohnya karena sudah tidak pernah lagi bermain judi online.
Itulah beberapa penjelasan mengenai akun pro situs BandarQQ. Meski penjelasan ini mencakup semua situs judi online yang ada. Hal terpenting yang harus kamu ingat ketika bermain judi online adalah dengan tidak serakah karena keserakahan bisa mengantarkanmu pada kerugian. Semoga artikel ini bermanfaat, ya.
Share this...FacebookTwitter "
"Games can help people engage with science outside of the traditional realm of research and academia. And using games in ecological research is on the rise, helping ecologists answer questions they’d never be able to in a laboratory experiment. This is particularly true when it comes to answering questions about evolution, such as: which traits help organisms maximise their chance of survival?  Natural selection operates over incredibly long timescales as individuals pass on their genes from generation to generation. With a game, we can speed up selection and test how different processes influence survival in just a few clicks. Our citizen science study enlisted ordinary people to help researchers find out which colours and patterns help crabs to camouflage. Digital games were deployed at the Natural History Museum, London, featuring images of common crabs defending themselves in different environments.  Camouflage is one of the most common defences in nature, but many species change colour and pattern as they grow older. Shore crabs (Carcinus maenas) are no exception. Juvenile shore crabs are known for their bright and varied colouration, which helps them camouflage in a range of habitats. But, despite this diversity when young, they all become greener with age. Why this happens remained a mystery – until now. A team of Finnish and British ecologists combined forces with game developers and science communicators at a firm called FoAM Kernow. Together they created a touch-screen game in which museum visitors searched for camouflaged crabs. A series of crabs were displayed against photos of different coastal habitats, challenging players to find them as fast as possible. At the end of each round, players could see how fast they found them and which crabs hid most effectively, with the research results shared with participants in real time. Photos of crabs found in rockpools, mudflats and mussel beds were displayed against each environment on a sleek screen during the museum’s Colour and Vision exhibition. Visitors to the exhibition performed the role of predators in the crabs’ environment – each on a mission to find crabs hidden in the different habitats. With thousands of “predators” playing the game, the team were able to see which colours and patterns helped the crabs hide most effectively. As Ossi Nokelainen, a researcher at University of Jyväskylälead and lead author of our study, explained: The citizen science game enables us to explore crab survival – something that would be really difficult to measure in the wild. This helps us to better understand why the crabs change appearance as they age when reared in controlled conditions. The time taken to find each crab – and whether it was found at all – allowed the researchers to see how effective each crab’s camouflage was. Across all environments green crabs were the hardest to detect. They were the most likely to survive and took the longest to find, offering clues to why crabs get greener as they grow. While baby crabs can be camouflage specialists – fine-tuning their appearance to match their surroundings – as they grow it pays to get greener and avoid predators across a wider range of environments. This shift from specialist to generalist camouflage may explain why many other species also start to look more similar as they age. Unlike many citizen science activities, the game shared live results with participants as they played, generating what is quite possibly the fastest public engagement with a research finding."
"

Rarely is federal legislation something other than a vehicle for government overreach and aggrandizement. Despite its populist‐​sounding title, the Global Investment in American Jobs Act – introduced in the Senate last week – is one of those rarities. The bill calls for an assessment of U.S. policies that influence decisions by foreigners about investing in the United States.   
  
  
Properly modest in scope, the legislation simply authorizes to Commerce Department to produce a report that documents the importance of foreign investment, identifies home‐​grown impediments to such investment, and recommends policy changes that would make the United States a more attractive investment destination.   
  
  
What is so refreshing about the bill is that its premise is not that the practices of foreign governments or the greed of U.S. corporations that allegedly “ship jobs overseas” are to blame for U.S. economic stagnation – themes so prominent in the past couple of Congresses and the current White House. Rather, the premise is that U.S. policy and its accumulated residue have created a web of impediments that discourage foreign investment in the United States, and that changes to those policies could serve to attract new investment. This kind of thinking is long overdue.   
  
  
One of the themes of my 2009 Cato paper, “Made on Earth: How Global Economic Integration Renders Trade Policy Obsolete,” is that it is no longer apt to consider global commerce a competition between “Us” and “Them.” Trans‐​national production/​supply chains and cross‐​border investment have blurred the distinctions between “our” producers and “their” producers. Many products and services are created along supply chains that travel from idea conception to final consumption and that include value‐​added activities of varying degrees of labor‐, physical capital‐, and intellectual capital‐​intensity. Furthermore, the largest U.S. steel producer, Mittal, is a majority Indian‐​owned company with corporate headquarters in Luxembourg. The largest “German” steel producer, Thyssen‐​Krupp, recently completed construction of a $4 billion production facility near Mobile, Alabama for the purpose of serving U.S. demand for finished steel, particularly from the mostly foreign nameplate auto producers dotting the landscape of the American South. And, as of 2010, our beloved General Motors produces more vehicles in China than it does in the United States. So, really, who are “we” and who are “they”?   
  
  
Accordingly, instead of pursuing a 20th century trade policy model that seeks to secure market‐​access advantages for certain producers, policy should be recalibrated to reflect the 21st century reality that governments around the world are competing for business investment and talent, which both tend to flow to jurisdictions where the rule of law is clear and abided; where there is greater certainty to the business and political climate; where the specter of asset expropriation is negligible; where physical and administrative infrastructure is in good shape; where the local work force is productive; where there are limited physical, political, and regulatory barriers, etc. This global competition in policy is a positive development because — among other reasons — its serves to discipline bad government policy.   
  
  
We are kidding ourselves if we think that the United States is somehow immune from this dynamic and does not have to compete and earn its share with good policies. The decisions made now with respect to our policies on immigration, education, energy, trade, entitlements, taxes, regulations, industrial management, and the proper role of government in the economic sphere will determine the health, competitiveness, and relative significance of the U.S. economy in the decades ahead.   
  
  
Governments with the smartest policies will be the ones that secure the most investment, the strongest talent, and the best economic opportunities for their people. The legislation is step in the right direction.
"
nan
"It is often said that if something is repeated often enough, it becomes accepted as true. This has certainly been the case for the link between terrorism and the poaching of elephants for the ivory trade.  A wide range of public figures have repeated the claim that ivory plays a major role in bankrolling terrorist organisations in Africa. These include former US secretary of state Hillary Clinton, UK foreign secretary William Hague and Kenya’s president Uhuru Kenyatta.  The most recent voice to be added to the choir was that of cinema director Kathryn Bigelow. The Oscar-winning director teamed up with charity WildAid to create a short video asserting that trade in ivory is funding the Somali terrorist group al-Shabaab, responsible for the 2013 Westgate Mall attack in Kenya in which 67 people died.  As with any illegal activity, it is very difficult to obtain reliable data on the size of the ivory trade. Although there is evidence that it has been used to finance armed groups in Africa such as the Lord’s Resistance Army or the Janjaweed in Darfur, the allegations linking ivory to terrorist groups are much weaker. They essentially rest on a single report published by the Elephant Action League in 2012. The report asserts, based on a single unnamed “source within the militant group”, that al-Shabaab makes between US$200,000 and US$600,000 from ivory, up to 40% of its income. This over-reliance on a single source and the fact that only a short “journalistic summary” of the report was ever released, has led to scepticism.  Recently, a joint report by INTERPOL and the UN Environmental Program classified EAL’s claims as “highly unreliable” as they would require al-Shabaab to bring nearly all ivory poached from west, central and eastern Africa to a single Somali port. However, this same report establishes a solid link between al-Shabaab’s finances and another environmental crime: illegal charcoal production.  The trade in charcoal leads to widespread deforestation and is already driving erosion and desertification in parts of Somalia. Al-Shabaab’s main financing mechanism appears to be the taxing of charcoal coming to the port of Baraawe (and until recently Kismayo) south of Mogadishu, with the value of the trade estimated to be US$38–56m per year. This means that, even if the EAL’s inflated ivory estimates were true, the trade in charcoal would still generate 60 to 94 times more revenue for al-Shabaab.  We’ve known about the charcoal trade in the Horn of Africa for a while now – the UN, for instance, highlighted the issue in a 2013 monitoring report on the Somali conflict.  It is thus puzzling that some western political and conservation figures have decided to focus on the unproven link between ivory and terrorism instead of the more relevant and substantiated conservation issue. A possible (yet cynical) explanation is that those highlighting the issue are trying to gain notoriety by bringing together terrorism, a top issue for all western governments, and the elephant, one of the most widely used conservation flagship species.  This would surely generate more attention than the more abstract issue of desertification and a few obscure tree species. The increased visibility could then be used to generate extra votes, donations or simply a more environment-friendly image. If this was the case, then we would for example expect these efforts to focus on those more likely to vote or donate, instead of those more likely to buy ivory. In the case of Kathryn Bigelow’s video and the “Last days of ivory” campaign it spearheads, all materials are only available in English, a language not relevant for the key ivory markets in Southeast Asia.  All the first four actions proposed to those who visit the campaign’s website revolve around either sharing the campaign image and content on social networks or donating to the associated charities. This campaign does indeed appear to be targeting those who can donate rather than those who can directly impact the ivory trade. Those involved clearly have something to gain from pushing the link between ivory and terrorism beyond the available evidence. However, it is also clear that in the long run it is not only their own credibility that is at risk but that of a whole conservation movement. Conservationists have focused large on messages of doom and gloom that often sound as if holding humanity for ransom if the environmental crisis is not addressed. If we are serious about keeping the public’s trust, we must ensure that we are driven by evidence, not the hype, lest we become the boy who cried wolf."
"
Love it or hate it, WUWT gets traffic.

This month was 1,478,801 page views. This is up significantly from both January (1,324,097) and February (1,168,852).
As always, my sincere thanks to the many readers, commenters (even the angry ones, you know who you are 😉 ), moderators, and guest contributors that keep WUWT fresh and interesting.
– Anthony
UPDATE: Since I had a question about it, the numbers and graph above are from my internal WordPress.com traffic counter and stat system. They are actual counted pages views, not estimates like some external web traffic analysers.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9716d680',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Japan’s boffins: Global warming isn’t man-made
Climate science is ‘ancient astrology’, claims report
By Andrew Orlowski The Register UK (h/t) from WUWT reader Ric Werme
UPDATE: One of the panelists (Dr. Itoh) weighs in here at WUWT, see below.
Exclusive Japanese scientists have made a dramatic break with the UN and Western-backed hypothesis of climate change in a new report from its Energy Commission.
Three of the five researchers disagree with the UN’s IPCC view that recent warming is primarily the consequence of man-made industrial emissions of greenhouse gases. Remarkably, the subtle and nuanced language typical in such reports has been set aside.
One of the five contributors compares computer climate modelling to ancient astrology. Others castigate the paucity of the US ground temperature data set used to support the hypothesis, and declare that the unambiguous warming trend from the mid-part of the 20th Century has ceased.
The report by Japan Society of Energy and Resources (JSER) is astonishing rebuke to international pressure, and a vote of confidence in Japan’s native marine and astronomical research. Publicly-funded science in the West uniformly backs the hypothesis that industrial influence is primarily responsible for climate change, although fissures have appeared recently. Only one of the five top Japanese scientists commissioned here concurs with the man-made global warming hypothesis.
JSER is the academic society representing scientists from the energy and resource fields, and acts as a government advisory panel. The report appeared last month but has received curiously little attention. So The Register commissioned a translation of the document – the first to appear in the West in any form. Below you’ll find some of the key findings – but first, a summary.
Summary
Three of the five leading scientists contend that recent climate change is driven by natural cycles, not human industrial activity, as political activists argue.
Kanya Kusano is Program Director and Group Leader for the Earth Simulator at the Japan Agency for Marine-Earth Science & Technology (JAMSTEC). He focuses on the immaturity of simulation work cited in support of the theory of anthropogenic climate change. Using undiplomatic language, Kusano compares them to ancient astrology. After listing many faults, and the IPCC’s own conclusion that natural causes of climate are poorly understood, Kusano concludes:
“[The IPCC’s] conclusion that from now on atmospheric temperatures are likely to show a continuous, monotonous increase, should be perceived as an unprovable hypothesis,” he writes.
Shunichi Akasofu, head of the International Arctic Research Center in Alaska, has expressed criticism of the theory before. Akasofu uses historical data to challenge the claim that very recent temperatures represent an anomaly:
“We should be cautious, IPCC’s theory that atmospheric temperature has risen since 2000 in correspondence with CO2 is nothing but a hypothesis. ”
Akasofu calls the post-2000 warming trend hypothetical. His harshest words are reserved for advocates who give conjecture the authority of fact.
“Before anyone noticed, this hypothesis has been substituted for truth… The opinion that great disaster will really happen must be broken.”
Next page: (at the Register)  Key Passages Translated

UPDATE: From Kiminori Itoh, Prof., Yokohama National University.
Hi everybody!
I am one of the five who participated to the article in the JSER journal, which may have seemed to you as a mystery from Japan. At first, I thank you for picking up our activity in Japan. I am a regular reader of several climate blog sites, and had been making some contributions mainly to Climate Science of Prof. Pielke. Actually, the information I gave in the article largely owes the invaluable information shown at this site WUWT as well as Climate Science and Climate Audit. Thus, I felt I should explain a bit about the article of JSER because, unfortunately, it is written in Japanese although it has partly been translated into English. 
Some readers of WUWT might remember my name; I had written a guest blog in Climate Science several months ago, when Roger kindly suggested me to introduce my new book “Lies and Traps in Global Warming Affairs.” Yes, I am regarded as one of the most hard-core AGW skeptics in Japan, although I myself regard me as a realist in this issue.
The article of JSER has been composed of discussions between the five contributors, made through e-mail for several months, and was organized by Prof. Yoshida of Kyoto University (an editor of the JSER journal). Our purpose was to invoke healthy discussions on the global warming issue in Japan. The JSER journal was selected as a platform for this discussion just because Prof. Yoshida has a personal interest in this issue and he is an editor of the journal. 
Thus, it is not correct if one thinks that the discussion represents the opinion of the journal’s editors or of the society JSER. In fact, none of the five contributors belong to the JSER, and Prof. Yoshida kept his attitude neutral in the article.
All the contributors are well-established researchers in different fields and each has characteristic personal opinions on the AGW issue. Only one (Dr. Emori, National Institute of Environmental Sciences, Japan) represents IPCC. Other members are more or less skeptical of the conclusions of IPCC. For instance, as translated into English, Dr. Kusano made a severe critique on climate models; he himself is a cloud-modeler, so that his critique seems plausible. Prof. Akasofu is well known as an aurora physicist, Prof. Maruyama is famous for his ideas in geophysics, and I myself have sufficient academic record in environmental physical chemistry (more than 160 peer review papers).
We know that our try this time is small one, and its impact has a limitation especially due to language problem. Nevertheless, we believe that the discussion was useful and informative for everyone interested in the controversies associated with the AGW issue. In March, another article will come also in the JSER journal because the discussion received much interest from the readers of the journal. 
Any comments and opinions are welcome and very helpful for us. 
Thank you again.
Based on Dr. Itohs comments, I’ve amended the headline to be more reflective of his first hand account on the report. – Anthony



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e986925e8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
UPADATED AT 8:30AM PST Sept 2nd-
More on SIDC’s decision to count a sunspeck (technically a “pore”) days after the fact. NOAA has now followed SIDC in adding a 0.5 sunspot where there was none before. But as commenter Basil points out, SIDC’s own records are in contrast to their last minute decision to count the sunspeck or “pore” on August 21.
There is an archive of the daily SIDC “ursigrams” here:
http://sidc.oma.be/html/SWAPP/dailyreport/dailyreport.html
If you select the ursigrams for August 22 and 23, you get the reported data for the 21st and 22nd:
August 21:
TODAY’S ESTIMATED ISN  : 000, BASED ON 07 STATIONS.
SOLAR INDICES FOR 21 Aug 2008
WOLF NUMBER CATANIA    : 011
10CM SOLAR FLUX        : 067
AK CHAMBON LA FORET    : ///
AK WINGST              : 004
ESTIMATED AP           : 005
ESTIMATED ISN          : 000, BASED ON 14 STATIONS.
August 22:
TODAY’S ESTIMATED ISN  : 000, BASED ON 11 STATIONS.
SOLAR INDICES FOR 22 Aug 2008
WOLF NUMBER CATANIA    : 013
10CM SOLAR FLUX        : 068
AK CHAMBON LA FORET    : ///
AK WINGST              : 003
ESTIMATED AP           : 003
ESTIMATED ISN          : 000, BASED ON 11 STATIONS.
In both cases, the daily estimated “International Sunspot Number” based on multiple stations, not just the Catania Wolf Number, was 000. So how did SIDC end up with positive values in the monthly report?
UPDATED at 2:42 PM PST Sept 1st – 
After going days without counting the August 21/22 “sunspeck” NOAA and SIDC Brussels now says it was NOT a spotless month! Both data sets below have been recently revised.
Here is the SIDC data:
http://www.sidc.be/products/ri_hemispheric/
Here is the NOAA data:
ftp://ftp.ngdc.noaa.gov/STP/SOLAR_DATA/SUNSPOT_NUMBERS/MONTHLY
The NOAA data shows July as 0.5 but they have not yet updated for August as SIDC has. SIDC reports 0.5 for August. It will be interesting to see what NOAA will do.
SIDC officially counted that sunspeck after all. It only took them a week to figure out if they were going to count it or not, since no number was assigned originally.
But there appears to be an error in the data from the one station that reported a spot, Catania, Italy. No other stations monitoring that day reported a spot. Here is the drawing from that Observatory:
ftp://ftp.ct.astro.it/sundraw/OAC_D_20080821_063500.jpg
ftp://ftp.ct.astro.it/sundraw/OAC_D_20080822_055000.jpg
But according to Leif Svalgaard, “SIDC reported a spot in the south, while the spot(s) Catania [reported] was in the north.” This is a puzzle. See his exchange below.
Also, other observatories show no spots at all. For example, at the 150 foot solar solar tower at the Mount Wilson Observatory, the drawings from those dates show no spots at all:
ftp://howard.astro.ucla.edu/pub/obs/drawings/dr080821.jpg
ftp://howard.astro.ucla.edu/pub/obs/drawings/dr080822.jpg
Inquires have been sent, stay tuned.
Here is an exchange in comments from Leif Svalgaard.
——-
REPLY: So What gives Leif….? You yourself said these sunspecks weren’t given a number. I trusted your assessment. Hence this article. Given the Brussels folks decided to change their minds later, what is the rationale ? – Anthony
The active region numbering is done by NOAA, not by Brussels. The Brussels folks occasionally disagree. In this case, they did. Rudolf Wolf would not have counted this spot. Nor would I. What puzzles me is this:
21 7 4 3
22 8 4 4
The 3rd column are ’spots’ in the Northern hemisphere, and the 4th column are ’spots’ in the Southern hemisphere [both weighted with the ‘k’-factor: SSN = k(10g+s)]. But there weren’t any in the south. The Catania spot was at 15 degrees north latitude, IIRC. Maybe the last word is not in on this.
——–
Hmm….apparently there’s some backstory to this. There is a debate raging in comments to this story, be sure to check them. – Anthony
# MONTHLY REPORT ON THE INTERNATIONAL SUNSPOT NUMBER #
# from the SIDC (RWC-Belgium) #
#——————————————————————–#
AUGUST 2008
PROVISIONAL INTERNATIONAL NORMALIZED HEMISPHERIC SUNSPOT NUMBERS
Date Ri Rn Rs
__________________________________________________________________
1 0 0 0
2 0 0 0
3 0 0 0
4 0 0 0
5 0 0 0
6 0 0 0
7 0 0 0
8 0 0 0
9 0 0 0
10 0 0 0
11 0 0 0
12 0 0 0
13 0 0 0
14 0 0 0
15 0 0 0
16 0 0 0
17 0 0 0
18 0 0 0
19 0 0 0
20 0 0 0
21 7 4 3
22 8 4 4
23 0 0 0
24 0 0 0
25 0 0 0
26 0 0 0
27 0 0 0
28 0 0 0
29 0 0 0
30 0 0 0
31 0 0 0
__________________________________________________________________
MONTHLY MEAN : 0.5 0.3 0.2
========================================================
ORIGINAL STORY FOLLOWS:
Many have been keeping a watchful eye on solar activity recently. The most popular thing to watch has been sunspots. While not a direct indication of solar activity, they are a proxy for the sun’s internal magnetic dynamo. There have been a number of indicators recently that it has been slowing down.
August 2008 has made solar history. As of 00 UTC (5PM PST) we just posted the first spotless calendar month since June 1913. Solar time is measured by Coordinated Universal Time (UTC) so it is now September 1st in UTC time. I’ve determined this to be the first spotless calendar month according to sunspot data from NOAA’s National Geophysical Data Center, which goes back to 1749. In the 95 years since 1913, we’ve had quite an active sun. But that has been changing in the last few years. The sun today is a nearly featureless sphere and has been for many days:

Image from SOHO
And there are other indicators. For example, some solar forecasts have been revised recently because the forecast models haven’t matched the observations. Australia’s space weather agency recently revised their solar cycle 24 forecast, pushing the expected date for a ramping up of cycle 24 sunspots into the future by six months.
The net effect of having no sunspots is about 0.1% drop in the TSI (Total Solar Irradiance). My view is that TSI alone isn’t the main factor in modulating Earth’s climate. 
I think it’s solar magnetism modulating Galactic Cosmic Rays, and hence more cloud nuclei from GCR’s, per Svensmark’s theory. We’ve had indications since October 2005 that the sun’s dynamo is slowing down. It dropped significantly then, and has remained that way since. Seeing no sunpots now is another indicator of that idling dynamo.
Graph of solar Geomagnetic Index (Ap):

Click for a larger image
Earth of course is a big heat sink, so it takes awhile to catch up to any changes that originate on the sun, but temperature drops indicated by 4 global temperature metrics (UAH, RSS and to a lesser degree HadCrit and GISS) show a significant and sharp cooling in 2007 and 2008 that has not rebounded.In the 20 years since “global warming” started life as a public issue with Dr. James Hansen’s testimony before congress in June 1988, we are actually cooler.

Click for a larger image
Reference: UAH lower troposphere data
Coincidence? Possibly, but nature will be the final arbiter of climate change debate, and I think we would do well to listen to what it’s saying now.
Joe D’Aleo of ICECAP also wrote some interesting things which I’ll reprint here.
…we have had a 0 sunspot calendar month (there have been more 30 day intervals without sunspots as recent as 1954 but they have crossed months). Following is a plot of the number of months with 0 sunspots by year over the period of record – 23 cycles since 1749.

See larger image here.
Note that cluster of zero month years in the early 1800s (a very cold period called the Dalton minimum – at the time of Charles Dickens and snowy London town and including thanks to Tambora, the Year without a Summer 1816) and again to a lesser degree in the early 1900s. These correspond to the 106 and 213 year cycle minimums. This would suggest that the next cycle minimum around 2020 when both cycles are in phase at a minimum could be especially weak. Even David Hathaway of NASA who has been a believer in the cycle 24 peak being strong, thinks the next minimum and cycle 25 maximum could be the weakest in centuries based on slowdown of the plasma conveyor belt on the sun.
In this plot of the cycle lengths and sunspot number at peak of the cycles, assuming this upcoming cycle will begin in 2009 show the similarity of the recent cycles to cycle numbers 2- 4, two centuries ago preceding the Dalton Minimum. This cycle 23 could end up the longest since cycle 4, which had a similar size peak and also similarly, two prior short cycles.

See larger image here.
Will this mean anything for climate in our near future? Possibly.  But we’ll have to wait to see how this experiment pans out.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c91240c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Lake Baikal, the world’s oldest, deepest lake, is feeling the temperature of human-induced climate change. Situated in southern Siberia, Baikal occupies one of the fastest warming regions on the planet and, as a result, the lake itself has got warmer, seasonal ice is present for a shorter period of time and has got thinner, and its waters have become stratified for longer periods. These changes have already had an impact on the lake’s microscopic life, including phytoplankton and zooplankton.  Now, our new research has provided the first evidence that some of the lake’s unique microscopic plants are being outcompeted by species not unique to the lake, most likely due to climate change. Although these ecological changes are so far confined to the south basin of Lake Baikal, they may act as an early warning signal of what might happen across the rest of the lake in the coming decades. To place modern ecological observations into a wider and longer-term perspective, we looked at the mud which accumulates at the bottom of the lake. This preserves an environmental history that, with careful collection and analyses, can be used to reveal changes in the lake’s ecology.  We were especially interested in investigating long-term trends in a key group of organisms called diatoms. These tiny algae are invisible to the naked eye, being only a fifth of the breadth of human hair.  Most of the energy in Lake Baikal’s food web ultimately comes from photosynthesis by these tiny diatoms. They are also especially useful in reconstructing long-term changes in the lake’s ecology because they have shells made of silica, the same material as glass, which allows their fossils to be preserved in the lake mud. As with most plants and animals found in Baikal, these diatoms are mainly endemic – that is, they are found nowhere else in the world. Back in 2006, one of us (Anson) predicted that climate change would lead to a decline in Baikal’s large, heavy, slow-growing, endemic diatom species, as they would quickly sink out of the photic zone as the lake became increasingly stratified. His work suggested that they would be replaced by smaller, lighter, faster growing species (both endemic and those found elsewhere), able to better tolerate more stratified water.  In the current study, our colleague Sarah Roberts set out to test this hypothesis as part of her PhD, by extracting “cores” from the mud at the bottom of the lake, to see what diatoms were like in previous years.   Our findings were surprising. In the south of Lake Baikal our data showed that a significant change in the diatoms occurred at the very start of the 1970s, at the same time as the lake began to warm and ice thinned. As predicted, this change was manifested by a decline in diatoms with thick, heavy shells, alongside an increase in faster-growing species with lighter shells, such as Synedra acus, a species also found growing in many other lakes worldwide.  What we didn’t expect was the decline in other lighter, endemic species such as Crateriportula inconspicua.  When the heavy diatoms decline in abundance, dissolved silica needed to make their glass shells is now available for other diatoms to use. But because S. acus can tolerate higher water temperatures with fast growth rates, it quickly outcompetes the other smaller, endemic diatoms.  Why is this important? Climate change is already messing with ecosystems in other large, ancient lakes, such as Lake Tanganyika in East Africa. What happens to plankton has a knock on effect up the food web, causing fish to struggle and also, ultimately, those humans who depend on the ecosystem for their livelihood.  The increasing dominance by non-endemic diatom species in Lake Baikal has the potential to disrupt the lake’s own food web, through changes to the types of zooplankton and other fauna that feed on Baikal diatoms, which may ultimately impact on the endemic fish species that feed on the zooplankton communities themselves.  Elsewhere in the lake, we are also seeing an increase in algal mats along the coastline, linked to untreated sewage from coastal settlements. Interacting stressors from both nutrient enrichment along Lake Baikal’s coastline, and increasing surface water temperatures and stratification from climate change, could have untold consequences on biodiversity in one of the world’s unique ecosystems."
"

During the 2017 presidential campaign, then‐​candidate Donald Trump was open about his hostility toward Iran and his disdain for the Obama administration’s diplomacy with that country. Since January, the Trump administration has been engaged in an Iran policy review. News reports and leaks suggest the review is highly likely to recommend a more confrontational approach to Iran, whether within the framework of the Iranian nuclear deal or by withdrawing from it. This paper examines the costs of four confrontational policy approaches to Iran: sanctions, regional hostilities, “regime change from within,” and direct military action.



Increased economic sanctions are unlikely to succeed in producing policy change in the absence of a clear goal or multinational support. Indeed, sanctions on Iran are likely to meet with strong opposition from U.S. allies in Europe and Asia, who continue to support the nuclear deal. The second policy we examine — challenging Iranian proxies and influence throughout the Middle East — is likewise problematic. There is little coherent, effective opposition to Iran in the region, and this approach increases the risks of blowback to U.S. forces in the region, pulling the United States deeper into regional conflicts.



The third option, so‐​called regime change from within, is a strategy that relies on sanctions and on backing for internal Iranian opposition movements to push for the overthrow of the regime in Tehran. This approach is not feasible: regime change — whether covert or overt — rarely succeeds in producing a stable, friendly, democratic regime. The lack of any good candidates for U.S. support inside Iran compounds this problem. The final policy alternative we explore is direct military action against Iranian nuclear or military facilities. Such attacks are unlikely to produce positive outcomes, while creating the risk of substantial escalation. Worse, attacking Iran after the successful signing of the nuclear deal will only add to global suspicions that the United States engages in regime change without provocation and that it cannot be trusted to uphold its commitments.



We suggest an alternative strategy for the Trump administration: engagement. This approach would see America continue to uphold the nuclear deal and seek continued engagement with Iran on issues of mutual interest. Engagement offers a far better chance than confrontation and isolation to improve Iran’s foreign policy behavior and empower moderate groups inside Iran in the long term.



In July 2015, the P5+1—the United States, United Kingdom, France, Russia, China, and Germany—reached a diplomatic agreement with Iran to roll back and significantly limit the Iranian nuclear program in exchange for the lifting of economic sanctions. The Joint Comprehensive Plan of Action (JCPOA) was the result of years of meticulous diplomatic negotiations and represented an historic compromise between two long‐​standing adversaries, the United States and Iran. At the time, the Obama administration presented the agreement as a strict nonproliferation agreement that would extend Iran’s so‐​called breakout time—the time it would take Iran to “sprint” to the creation of a useable nuclear weapon—from a few months to a year or longer. Many also hoped that the JCPOA could help to reduce bilateral tensions and quiet calls for U.S. military action against Iran for the foreseeable future. The unexpected election of Donald Trump in 2016 dashed these hopes. With renewed tensions and open debate within the Trump administration as it conducts a “comprehensive review of our Iran policy,” the future of the JCPOA and of U.S.-Iranian relations is uncertain. 1 There are certainly many options for the Trump administration if it wishes to take a more confrontational approach to Iran, four of which are examined in this paper. Yet each is difficult, costly, and carries far higher risks than continuing a policy of engagement.



The JCPOA has been successful, placing strong restrictions on Iran’s ability to engage in even peaceful nuclear development. Iran removed 98 percent of its stockpile of enriched uranium, dismantled two‐​thirds of its uranium enrichment centrifuges, disassembled the core of its heavy water reactor (a potential source of weapons‐​grade plutonium), and converted two major enrichment sites into peaceful research facilities. In addition, Iran agreed to engage in uranium enrichment exclusively at a single facility—the Natanz complex—and to produce only low‐​enriched uranium for 10 years. Because uranium must be enriched to 90 percent for use in a nuclear weapon, Iran’s agreement to restrict enrichment to 3.67 percent constitutes a significant barrier to weapons development. Iran also agreed to limit its stockpile of low‐​enriched uranium to 300 kilograms for 15 years, making it extremely difficult to covertly enrich excess material. 2



To ensure compliance with the JCPOA’s restrictions, Iran agreed to submit what remained of its nuclear program to what Georgetown University’s Ariane Tabatabai describes as “the most intrusive inspections regime ever voluntarily agreed to by any party.” 3 International monitors perform daily inspections of all of Iran’s declared facilities, with some facilities subject to 24‐​hour video surveillance. As critics note, these inspections and many of the deal’s other restrictions eventually expire, phased out over the next 10 to 25 years. 4 As part of the deal, however, Iran rejoined the Nuclear Nonproliferation Treaty (NPT) and ratified its Additional Protocol, a provision that mandates inspections of Iran’s civilian nuclear facilities. In doing so, Iran made a commitment to never become a nuclear weapons state and agreed to monitoring under the NPT indefinitely, far beyond the life of the JCPOA.



Indeed, more than two years after the adoption of the JCPOA, Iran is in full compliance with the deal. Though there has been some debate about the interpretation of certain issues—Iranian missile testing and the extent of U.S. sanctions relief—the deal continues to be implemented by both sides. As of this writing, the International Atomic Energy Agency (IAEA) has reported eight times that Iran is meeting its obligations under the deal. 5 Even the Trump administration, despite public denigration of the agreement, has formally certified that Iran is fulfilling its JCPOA commitments. In exchange, economic sanctions related to Iran’s nuclear program have been lifted, including United Nations and European Union sanctions on Iran’s energy sector and a variety of U.S. secondary sanctions related to Iran’s financial and energy sectors. 6 In addition, Iran has regained access to wealth stored in offshore banks previously interdicted by sanctions. 7



Nonetheless, the change in presidential administration has altered the political climate surrounding the nuclear deal in Washington, D.C. There have been prominent calls from both within the Trump administration and outside it to kill the JCPOA. As a candidate, Donald Trump himself repeatedly boasted that his “number‐​one priority is to dismantle the disastrous deal with Iran,” which he described in his typical hyperbole as “the worst deal ever negotiated.” 8 The recertification process (required every 90 days) has become increasingly politicized as a result: in July 2017, some advisers persuaded the president to refuse certification of Iran’s compliance with the JCPOA, only for other advisers to succeed in persuading him, at the last minute, to accept the IAEA’s conclusions and certify compliance. 9 Trump told journalists following the episode that he intends not to repeat the incident, reportedly informing White House staff that “he wants to be in a place to decertify 90 days from now and it’s their job to put him there.” 10 As David S. Cohen, former deputy director of the Central Intelligence Agency (CIA), notes, President Trump’s “reported demand for intelligence to support his policy preference to withdraw from the Iran nuclear deal risks politicizing intelligence analysis, with potentially grave consequences.” 11



Calls to end the deal have also come from outside the administration. In July, Sens. Tom Cotton (R-AR), Ted Cruz (R-TX), David Perdue (R-GA), and Marco Rubio (R-FL) wrote a letter to Secretary of State Rex Tillerson to “urge that you not certify … that Iran is complying with the terms of the [JCPOA].” 12 John Bolton, United Nations ambassador under George W. Bush and an early candidate to be Trump’s secretary of state, called for bombing Iran’s nuclear facilities months before the JCPOA was signed. 13 In July 2017, he wrote, “withdrawing from the JCPOA as soon as possible should be the highest priority.” 14



Opponents of the deal have little factual basis for their arguments: the IAEA has repeatedly found Iran in compliance with the deal’s restrictions, and the Joint Commission of the JCPOA has not identified any violations. 15 Instead, opponents typically argue that Iran is violating the “spirit” of the deal, pointing to Iran’s ballistic missile tests or its support for violent groups throughout the Middle East. 16 Yet the JCPOA was narrowly written specifically to exclude non‐​nuclear questions; it was never intended to solve all problems in the U.S.-Iranian relationship. Ironically, if any JCPOA signatory is in violation of the deal, it may be the United States. 17 At the G-20 summit in July, President Trump reportedly urged fellow world leaders to stop doing business with Iran, an action that violates the American commitment under the JCPOA to “refrain from any policy specifically intended to directly and adversely affect the normalization of trade and economic relations with Iran.” 18



President Trump appears determined to undermine the JCPOA. The administration is considering using the deal’s “snap inspections” provision—which allows inspectors to demand access to undeclared sites in Iran reasonably suspected of illicit enrichment activity—to make Iran appear noncompliant. 19 In the absence of any clear evidence of illicit enrichment activity, Iran would likely decline the Trump administration’s demand to inspect undeclared military sites, allowing the White House to portray Iran as violating the deal. As Mark Fitzpatrick, executive director of the International Institute for Strategic Studies, notes, this approach is “the route that White House political operatives suggest as a way to meet President Trump’s pre‐​determination not to again certify that Iran is in compliance, even when the facts clearly say otherwise.” 20 This approach also plainly misuses the relevant provisions of the JCPOA: as Daryl Kimball, director of the Arms Control Association put it, the Iran deal’s “special access provisions were designed to detect and deter cheating, not to enable [a] false pretext for unraveling the agreement.” 21 The administration appears to be simply “seeking trumped up reasons to sink [the] Iran deal.” 22



The Trump administration’s approach to Iran approximates the Bush administration’s approach to Iraq in the lead up to the 2003 invasion. Fitzpatrick compares the two situations, noting that “unfounded assumptions, false claims, and ideologically‐​tinged judgements are driving a confrontational approach that could well lead to another war in the Middle East.” 23 As in the case of Iraq, the risk exists for politicization of intelligence findings. As Steve Andreasen and Steve Simon, both former members of the National Security Council, describe in a recent op‐​ed in the _New York Times_ : “It’s a good bet that [administration officials] will cherry‐​pick facts to give the president what he wants: an excuse to scuttle the Iran deal.” 24



President Trump’s commitment to a harder line against Iran—independent of the nuclear deal—is obvious, though the Trump White House’s vicious internal power struggles suggest clear differences inside the administration on the best approach. In June, for example, the _New York Times_ reported that the administration was ramping up a covert action program against Iran, and that “Mr. Trump has appointed to the National Security Council hawks eager to contain Iran and push regime change, the groundwork for which would most likely be laid through CIA covert action.” 25 Yet Trump’s National Security Adviser H. R. McMaster fired the council’s former senior director for intelligence, Ezra Cohen‐​Watnick, in August. Cohen‐​Watnick had previously expressed to administration officials “that he wants to use American spies to help oust the Iranian government.” 26 Along with Derek Harvey, who was the administration’s top Middle East official on the National Security Council, Cohen‐​Watnick had also advocated broadening U.S. involvement in the Syrian civil war as a means of pushing back against Iran. McMaster likewise fired Harvey in July 2017.



Prominent Iran hawks remain in the administration, and some go well beyond arguing for abrogating the JCPOA to make the case for a regime change policy toward Iran. In June, Tillerson testified before the House Foreign Relations Committee that the administration intended to “work toward support of those elements inside of Iran that would lead to a peaceful transition of that government,” 27 though other high‐​level administration officials have denied this is current policy. 28 While he was a member of Congress in 2016, Trump’s current CIA director, Mike Pompeo, publicly called for the United States to “change Iranian behavior, and, ultimately, the Iranian regime.” 29 Senator Tom Cotton (R-AR)—known to be close to the Trump administration—likewise has stated that “the policy of the United States should be regime change in Iran.” 30 Defense Secretary James Mattis as recently as June described Iran as “the most destabilizing influence in the Middle East.” 31



Outside the federal government, other hawkish voices have also made forceful calls for regime change. Soon after Trump was inaugurated, the well‐​connected conservative think‐​tank Foundation for the Defense of Democracies (FDD) submitted a memo to Trump’s National Security Council that argued for “coerced democratization” in Iran, a euphemism for regime change. 32 John Bolton said in a speech in July, “The behavior and the objectives of the regime are not going to change, and therefore the only solution is to change the regime itself.” 33



The debate on Iran in Washington today includes many options, some—though not all—of which begin with killing the JCPOA. Deliberately scuttling the JCPOA would have negative ramifications. The international community and Iran, recognizing U.S. intransigence, could conceivably continue to uphold the nuclear deal without the United States, isolating the United States from allies and handicapping its pursuit of unrelated diplomatic initiatives, notably the question of North Korea’s nuclear program. Alternatively, U.S. termination of the JCPOA could motivate Iran to unburden itself from the deal’s restrictions, expel international monitors, and begin once again to pursue a nuclear weapons capability in earnest. Either possibility puts the United States in a weaker, more dangerous position. Given the momentum in Washington behind pursuing a more hostile approach toward Iran, this policy analysis will explore the likely costs and consequences of four different approaches to confronting Iran, whether as alternatives to the JCPOA or supplementary to it.



The first approach we assess is applying economic pressure in the form of ratcheting up sanctions on Iran, including those the international community agreed to lift under the JCPOA. The second approach looks at the options for challenging Iranian influence in the Middle East, particularly its proxies in Iraq and Syria. The third approach considers the viability of what is called “regime change from within,” where the United States would support internal opposition groups in an effort to undermine or overthrow the government in Tehran. The fourth and final approach we evaluate is military action against Iran, most likely in the form of limited airstrikes against Iranian nuclear or other military facilities. We conclude by proposing a fifth strategy for the Trump administration: uphold U.S. commitments under the JCPOA, refrain from adding new sanctions, and engage with Tehran where U.S. and Iranian interests overlap. There is no silver bullet that can solve the problems in the U.S.-Iranian relationship, but continued engagement carries lower costs and a higher chance of success than any of the other approaches examined here.



Opponents of the JCPOA frequently argue that they could negotiate a better deal through the aggressive use of U.S. sanctions. These sanctions would be extraterritorially applied, forcing European companies to adhere to U.S. law, in theory making Iran willing to concede more of its nuclear program or to make other security and governance concessions. For example, former Connecticut senator Joe Lieberman proposed in December that President Trump “designate the entire Iranian Revolutionary Guards Corps as a foreign terrorist organization … support legislation in Congress punishing sectors of the Iranian economy … propose measures to curb Iranian access to U.S. dollars … and then to walk away, with cause, from the JCPOA.” 34 Such arguments are not restricted only to those who wish to abrogate the JCPOA. Various authors argue that while there are no grounds to “tear up” the deal, the president and Congress should nonetheless seek to impose new sanctions on Iran related to its regional activities and support for the Assad regime in Syria.



Indeed, Congress has already acted in this regard, passing an extensive sanctions bill in July 2017, including North Korean, Russian, and new Iranian sanctions. The bill, “Countering America’s Adversaries through Sanctions Act,” targets a number of new individuals and entities—particularly in relation to Iran’s ballistic missile program—and includes an arms embargo and several new reporting requirements. 35 Congress made last minute changes to the bill to ensure that it did not technically violate the JCPOA, 36 yet as Senator Bernie Sanders (I-VT) pointed out when justifying his vote against the bill: “I believe that these new sanctions could endanger the very important nuclear agreement that was signed between the United States, its partners, and Iran in 2015. That is not a risk worth taking.” 37 Sanders is correct; new sanctions on Iran for its missile programs and human rights abuses raise tensions within the framework of the JCPOA while adhering to the narrowest possible definition of its terms. In response to the new sanctions bill and the threat of further sanctions, Iranian leaders voted to increase the state’s military budget and threatened to restart the nuclear program, highlighting the escalatory potential of new sanctions. 38



Opponents of the JCPOA support the imposition of new sanctions, particularly the designation of the Islamic Revolutionary Guard Corps (IRGC) and IRGC‐​associated businesses, often with little regard for whether new sanctions could torpedo the deal or worsen relations. Council on Foreign Relations Senior Fellow Ray Takeyh has repeatedly said renewed sanctions are the first step in a broader strategy of pressure on Iran, arguing that “we must return to the days of warning off commerce and segregating Iran from global financial institutions. Designating the Revolutionary Guards as a terrorist organization and reimposing financial sanctions could go a long way toward crippling Iran’s economy.” 39 Likewise, the editors of the conservative _National Review_ advised the Trump White House to abrogate the deal through sanctions: “Better to declare an end to this diplomatic farce … and establish a robust sanctions regime that might actually force Tehran to change its ways.” 40



The central problem with this option—whether as a replacement for the JCPOA or in addition to it—is the utter lack of international support. Though often overlooked, the JCPOA is in reality a multinational arms control agreement, negotiated by the P5+1, the five permanent members of the United Nations Security Council, plus Germany. The other parties to the deal have been unequivocal in affirming that Iran is indeed abiding by its commitments under the deal. On August 3, a spokeswoman for European Union foreign policy chief Federica Mogherini told a press conference: “So far, we consider that all parties have been implementing their commitments under the deal.” 41 Sergei Lavrov, Russian foreign minister, likewise confirmed Iran’s compliance and questioned the Trump administration’s motives, saying in August that the Trump administration “continue[s] calling these agreements wrong and erroneous, and it’s a pity that such a successful treaty is now somewhat being cast into doubt.” 42



European support for the deal is strong. As Carl Bildt, former prime minister of Sweden, noted in an opinion piece in August, canceling the deal would be a nonstarter in Europe: “Europe would certainly not go along with this, for one because it would risk undercutting the elaborate inspections systems that the agreement depends on. But primarily because Europe has seen that the deal actually works … and Europe has absolutely zero appetite for a new cascade of conflicts in a region on its doorstep.” 43 As a result, European leaders are also keen to prevent the imposition of further non‐​nuclear U.S. sanctions that could potentially undermine the deal. Indeed, on July 11, Mogherini told reporters: “The nuclear deal doesn’t belong to one country; it belongs to the international community. We have the responsibility to make sure that this continues to be implemented.” 44



It is unlikely that any additional U.S. sanctions would be successful without multinational support. The United States has long had an extensive array of sanctions focused on Iran, including on weapons procurement and development, U.S.-Iranian trade, and terrorist financing. Yet the long‐​term effect of these sanctions on the Iranian economy was relatively minimal prior to 2005. Technology sanctions have undoubtedly been successful in slowing progress on nuclear and missile‐​related projects but have done little to impact Iran’s import and development of conventional weapons. 45



Two changes in the mid‐​2000s substantially increased the efficacy of sanctions on Iran. First, the Treasury department aggressively pursued a strategy of outreach, lobbying (and threatening) foreign banks to ensure that U.S. sanctions would be adhered to extraterritorially. Second, the European Union decided in 2012 to embargo Iranian oil exports. This decision was motivated by increasing concerns over Iran’s nuclear program, even though it was politically and economically costly for the Europeans. In 2010 alone, Iran’s exports to the EU totaled $19 billion, 90 percent of which were energy related. 46 By March 2013, Iran’s oil exports had dropped from 2.5 million barrels per day to 1 million barrels per day, resulting in an Iranian budget deficit of $28 billion that year. 47 While U.S. sanctions alone were relatively ineffectual, these punitive economic costs helped to drive Iran to the negotiating table.



Proponents of increased sanctions therefore typically advocate for more assertive enforcement of secondary sanctions penalties against European and Asian companies. A recent report from the Washington Institute for Near East Policy, for example, called for the United States to step up the extraterritorial enforcement of existing sanctions on terror financing and IRGC‐​affiliated companies, arguing that enforcement and public warnings could discourage European companies from re‐​entering the Iranian market. As Stuart Levey, at the time undersecretary for terrorism and financial intelligence, described the use of extraterritorial sanctions prior to the JCPOA: “Those who are tempted to deal with targeted high‐​risk actors are put on notice: if they continue this relationship, they may be next.” 48 Yet the decision to sanction Iran was costly for European companies. A number of companies, most notably French energy company Total, which signed a $5 billion investment deal with Iran and with China’s National Petroleum in July to develop the South Pars gas field, have begun to re‐​enter the market following the successful conclusion of the JCPOA. 49 In the absence of any concrete evidence of Iranian cheating on the deal, European and Asian governments are likely to push back strongly against new U.S. barriers to trade and investment in Iran, and on the excessive extraterritorial application of existing sanctions.



Another problem with sanctions is that they are rarely successful in producing policy change. Indeed, though targeted sanctions may impose costs on the targeted regime, it is less clear that these costs actually produce policy change. 50 Proponents of increased sanctions point to high profile successes like the JCPOA, while skeptics point to the many cases, from Syria to Zimbabwe, where sanctions have failed to produce policy change. More broadly, academic studies have repeatedly shown sanctions to be ineffective in achieving policy change. As Arne Tostensen and Beate Bull note in the journal _World_ _Politics_ , “The voluminous literature that has accumulated over the years tends to conclude that sanctions are rarely effective, even though exceptions have been documented.” 51 In one of the earliest broad‐​based studies of comprehensive sanctions, for example, researchers found an average sanctions success rate of only 34 percent. 52 Even the research on more recent “smart sanctions,” which are presumed to be more effective thanks to their “targeted” nature, shows that they are also largely ineffective. A wide‐​ranging study of United Nations targeted sanctions found them to be effective in only 10–20 percent of cases, 53 while another survey of post‐​9/​11 U.S. sanctions found them to be effective in only 36 percent of cases. 54



Policy change is especially unlikely when sanctions do not have clear, attainable goals or when the issue is of prime national security importance to the target state. 55 Sanctions focused on economic issues such as trade often seem to be qualitatively different than those focused on security. 56 When University of Chicago’s Robert Pape examined sanctions as an alternative to the use of force, he found they had only been successful in around 5 percent of national security–related cases. 57 Sanctions also tend to fail when they are unilateral; as the Washington Institute’s Katherine Bauer notes, even with the power of U.S. extraterritorial sanctions, “there are limits to U.S. jurisdiction and the ability to compel foreign compliance.” 58 Further sanctions on Iran thus fall into a worst‐​case scenario: security‐​focused sanctions with no clear goals other than securing “a better deal” or weakening the Iranian regime. In the absence of strong support from European or other Security Council nations, there is very little chance that further sanctions will compel Iran’s leaders to capitulate.



An alternative option is a deliberate strategy of challenging Iranian proxies throughout the Middle East. That option would not necessarily require the Trump administration to abrogate the JCPOA. Indeed, as Brookings Institution Senior Fellow Daniel Byman recently noted in congressional testimony: “Because the JCPOA … has put Iran’s nuclear program on the back burner, there is an opportunity to focus on Iran’s support for militant groups and other problems Iran causes in the region.” 59 This approach runs counter to Washington’s current regional strategy: though there are arenas where the United States is engaged in hostilities with Iranian‐​associated proxies—such as U.S. support for the Saudi‐​led campaign in Yemen—America’s anti‐​ISIS campaign typically means that it is de facto fighting on the same side as Hezbollah and other Shi’a militias. The most moderate alternative proposals call for U.S. support for regional allies, such as military and diplomatic support for a peace settlement in Yemen designed to split the Houthi rebels from Tehran’s limited support. 60 Other options include increased maritime presence to help disrupt Iranian arms shipments. 61 Still others call for building the capacity of regional actors: one recent report from the Center for a New American Security suggests maintaining U.S. influence in Iraq and increasing U.S. logistical support for the conflict in Yemen, in hopes of marginalizing Iranian influence in those conflicts. 62



However, there are also a variety of more aggressive proposals. Two senior former administration officials on the National Security Council, Derek Harvey and Ezra Cohen‐​Watnick, were reportedly in favor of direct U.S. military action against Iranian proxies in Syria. 63 Escalating clashes between U.S. troops and militias in southern Syria in recent months, including U.S. airstrikes on several militias, suggest that such clashes will happen even in the absence of a formal policy change. Several recent policy papers also make the argument for a more formalized anti‐​Iran strategy in Syria, often using proxies to challenge Iranian‐​allied groups. The Washington Institute’s Nader Udowski, for example, argued in June 2017 for “a new U.S. policy, the chief component of which should be a strategy targeting Iran’s Quds force and its Shi’a militias.” 64 Similarly, Max Peck of the Foundation for Defense of Democracies has argued that the Trump administration should seek to codify in law that the United States seeks the overthrow of the Assad regime in Syria, and “increase the costs of Iran’s engagement by maintaining the pressure on Assad … through its support for the armed opposition.” 65



Perhaps the most bellicose option is actively increasing U.S. participation in the war in Syria and Iraq. A report from the Institute for the Study of War (ISW) called for the United States to “seize and secure a base in southeastern Syria … create a de facto safe zone … then recruit, train, equip, and partner with local Sunni Arab anti‐​ISIS forces.” The report called for American troops to “fight alongside” these forces. 66 The goals would include not only “defeating al Qaeda, as well as ISIS,” but also “expelling Iranian military forces and most of Iran’s proxy forces from Syria.” This strategy extends to Iraq: as a follow‐​on report argued, America should also “take urgent measures to strengthen Iraqi Prime Minister Abadi,” and work to minimize Iranian influence in Iraq. 67 Though the extent of American military involvement varies widely across these proposals, they all share a common theme: direct or indirect military action against Iranian proxies in Syria, Iraq, Yemen, and elsewhere.



The central problem with this approach is that there is no coherent anti‐​Iranian axis in the Middle East to rely upon in a campaign to challenge Iranian influence in the region. Indeed, observers have often described the region using sectarian narratives—portraying conservative Sunni states in conflict with Iran’s more revolutionary Shi’a axis—that are largely exaggerated.



For example, despite Saudi efforts to form a united regional front against Iran, the conflicts of the Arab Spring have frequently seen the states of the Gulf Cooperation Council (GCC) act against each other’s interests. 68 In Syria, the conflict between Saudi and Qatari proxies helped to radicalize and doom the anti‐​Assad opposition, while a Qatari‐​Emirati rivalry fueled the Libyan conflict. Today’s GCC crisis only serves to highlight this problem: though clearly motivated by a desire to rein in Qatar’s independent foreign policy, the Saudi and Emirati embargo has in reality driven Qatar closer to Iran and Turkey, undermining a common GCC front. 69



Other regional attempts to form anti‐​Iranian movements have likewise failed. A widely‐​publicized Saudi Arabian attempt in December 2015 to create an Islamic Military Alliance to fight terrorism—which pointedly included no Shi’a majority states—has largely failed to develop since that time. 70 Nor is there any guarantee that regional partners will actually promote U.S. interests if the United States increases its support; the actions of allies in the region have all too often served to destabilize and worsen conflicts in Syria, Yemen, and elsewhere, rather than improve them.



Indeed, the lack of a solid anti‐​Iran coalition among existing U.S. partners—capable of achieving America’s often expansive foreign policy goals—is a key reason why the most extreme options for regional confrontation with Iran often involve fabricating an effective anti‐​Iranian bloc from whole cloth, whether that is the creation of a “credible and moderate Syrian opposition,” a regional “multinational Joint Task Force with Arab partners targeted at countering … the IRGC,” or “a new Syrian Sunni Arab partner … to conduct population‐​centric counterinsurgency.” 71 Each of these options is likely to fail. Previous U.S. efforts to create regional coalitions to fight terror groups have been largely unsuccessful. The 2014 collapse of the Iraqi army in the face of ISIS advances is also a salutary lesson; years of training commitments and substantial blood and treasure on the part of the U.S. military were not enough to overcome deeper societal problems like corruption. 72 Without coherent, effective local proxies, and given the major political differences that divide U.S. regional allies, any attempt to build an anti‐​Iranian force or coalition in the region is likely to falter.



A strategy of regional pushback against Iran is also likely to pull the United States more deeply into a variety of regional conflicts and increase the risks of blowback to U.S. troops in the region. The United States is already heavily overcommitted in the Middle East, with tens of thousands of troops engaged in conflicts in Iraq, Syria, Afghanistan, Libya, and Yemen, and stationed at permanent bases elsewhere throughout the region. Indeed, despite the Obama administration’s attempts to draw down American commitments to Middle Eastern conflicts, the number of troops engaged in fighting Middle East conflicts has been increasing again since 2014. 73 A stepped‐​up campaign against Iranian proxies throughout the region will require further troop increases, both in direct combat roles and to train and support local forces.



It is these troops who will bear the brunt of any Iranian military response to this strategy. Several hundred U.S. troops were killed by Iranian‐​associated groups in Iraq during the post‐​invasion occupation, a number likely to rise in any new conflict with these groups. 74 And while Hezbollah has been largely occupied in recent years with fighting on behalf of the Assad regime, if faced with a concerted campaign against it by U.S.-allied forces, it is likely to respond with the kind of asymmetric attacks that have characterized their long‐​running conflict with Israel. 75 Indeed, one potential response to a concerted attack on Iranian proxies throughout the region is retributive attacks on Israel; during the 2006 war, Hezbollah enjoyed substantial success against Israeli forces, disabling a number of tanks and even an Israeli warship. 76 The potential for Iranian retaliation against U.S. troops, regional partners, or shipping in the region suggests that a strategy of regional confrontation with Iran will not make the region safer or more stable, but will instead introduce additional conflict and uncertainty.



Another possible option for dealing with Iran is an explicit U.S. policy of regime change. This is not a new idea; for decades, hawks in Washington have called for regime change in Tehran. Justifications have ranged from the 1979 hostage crisis to Iran’s nuclear program in the mid‐​2000s to the anti‐​regime protests known as the Green Revolution after 2009. 77 Yet the failure of U.S. regime change campaigns in both Iraq and Libya to produce a stable, democratic state has led most proponents of regime change to back away from overt military options and instead suggest that the Trump administration pursue “coerced democratization” or “regime change from within.” In this approach, the United States would pressure the Iranian regime and simultaneously back groups that oppose it—whether the exiled extremist National Council of Resistance of Iran (NCRI), pro‐​democracy Green Revolution factions, or ethnic minorities within Iran—a strategy advocates often compare to Reagan’s support for civil society groups in the Soviet Union. As Reuel Gerecht and Ray Takeyh argue in a _Washington Post_ op‐​ed: “Today, the Islamist regime resembles the Soviet Union of the 1970s … if Washington were serious about doing to Iran what it helped to do to the U.S.S.R., it would seek to weaken the theocracy by pressing it on all fronts.” 78



Another proponent of “coerced democratization,” the Foundation for Defense of Democracies’ Mark Dubowitz, urged President Trump to “go on the offensive against the Iranian regime” by “weakening the Iranian regime’s finances” through “massive economic sanctions,” while also “undermin[ing] Iran’s rulers by strengthening pro‐​democracy forces” inside Iran. 79 This option appears to be gaining traction in the Trump administration’s ongoing Iran policy review and has received public support from Tillerson. CIA Director Mike Pompeo also favored such an approach during his time in Congress. Yet there are important reasons to doubt that such a strategy would actually yield constructive results in Iran or benefit U.S. national interests.



Regime change often fails, particularly when it is covert. According to one study of covert regime change operations by the United States during the Cold War, such efforts succeeded only one‐​third of the time. 80 Indeed, as an administration official said in August, “With Iran, they are looking at regime change but coming up empty. There are no good plans, no decapitation strikes possible.” 81 Arming or funding for local insurgencies also rarely succeeds; a leaked CIA report commissioned in 2012 found that most past attempts to covertly arm insurgencies had minimal impact on long‐​term outcomes and often backfired. 82



Even when successful in unseating one government and establishing another in its place, foreign‐​imposed regime change “generally does not improve relations between interveners and targets. Rather, it often makes them worse,” according to Georgetown University’s Alexander B. Downes and Boston College’s Lindsay A. O’Rourke. 83 Changing the leadership of a state typically fails to alter that country’s perception of its interests, and foreign‐​imposed regimes tend to diverge from the preferences of the intervener as they begin to face domestic political pressures. Contrary to the depiction of many regime change advocates, the Iranian regime enjoys substantial public support, and the population would not welcome a U.S.-imposed government. Any new regime that tried to implement policies that reflect U.S. interests instead of Iranian interests would “attract the ire of domestic actors,” leading to an unstable government viewed as illegitimate by the population. 84



Research shows that “when a country overthrows another’s government, it increases the likelihood of civil wars and usually doesn’t establish a democracy.” 85 The recent experiences of the United States in Iraq, Afghanistan, and Libya only confirm this finding. Sixteen years of U.S. military presence have done little to stabilize war‐​torn Afghanistan. 86 The war in Iraq essentially destroyed the Iraqi state, killing hundreds of thousands of Iraqis and displacing millions more. More than 4,400 U.S. troops were killed in combat, and more than 30,000 were wounded, with direct costs estimated to exceed $2 trillion and indirect costs as high as $4 trillion. 87 A widespread insurgency and civil war led to the rise of the Islamic State, prompting further U.S. intervention to fight against the group. In Libya, the U.S. choice to overthrow the regime of Muammar Gaddafi on humanitarian grounds resulted in a lengthy civil war and the deaths of more Libyans than would likely have perished without the intervention. 88 The likelihood of successful regime change and a subsequent stable, democratic state in Iran are vanishingly small.



Though regime change proponents highlight a variety of groups inside Iran as potential candidates for U.S. support, none are truly viable. The exiled opposition group Mujahideen‐​e‐​Khalq (MEK) (or its political wing, the NCRI) is one such example. The MEK began in the 1960s and 1970s as a paramilitary Marxist‐​Islamic resistance group opposed to the former Shah of Iran, the authoritarian ruler put in power following a 1953 coup sponsored by the United States and Great Britain. The group allied with Saddam Hussein during the 1980s Iran‐​Iraq War, and analysts widely agree that it is an undemocratic group that has no popular support inside Iran. 89 Indeed, the MEK has largely tried to win external support for its agenda of regime change in Iran. Until 2012, it was even designated a terrorist organization by the U.S. State Department and had lobbied hard over the years to win support from prominent current and former U.S. officials to have that designation removed. 90 It has won primarily the support of those who favor a hardline approach to Iran, such as former CIA directors James Woolsey and Porter Goss, former New York City Mayor Rudolph Giuliani, former governors Howard Dean and Ed Rendell, former U.N. Ambassador John Bolton, and former House Speaker and close Trump confidant Newt Gingrich. Yet in the absence of popular support outside certain Washington circles, backing the group in a bid to overthrow the Iranian regime would likely fail. 91



Regime change advocates also suggest supporting the so‐​called Green Movement that emerged amid the protests over the contested Iranian presidential elections in 2009. Unfortunately, according to Ariane Tabatabai and Madison Schramm, the Green Movement “essentially faded away a few months after the elections” and “was never a cohesive faction.” 92 Green Movement leaders Mir Hossein Mousavi and Mehdi Karroubi remain under house arrest in Iran today, and have made clear that their goal was to dispute the 2009 election results, not to overthrow the government. In fact, the best hope for the Green Movement is to avoid association with the United States; whatever popular support it continues to have would quickly evaporate with any whiff of U.S.-backing for regime change. As Michael Axworthy of the University of Exeter writes, “Given the long history of foreign meddling in the country (the CIA‐​inspired coup that removed Prime Minister Mohammad Mosaddeq in 1953 is just one example), any suspicion of foreign backing is political poison in Iran.” 93



The third option—seeking to stoke discontent among Iran’s minority populations—is similarly infeasible. Iran’s ethnic minorities include Kurds (10 percent), Baluchis (2 percent), Arabs (2 percent), and Azeri Turks (16 percent). 94 But Iran is not a country beset by ethnic, cultural, and religious cleavages in the way the former Yugoslavia was. Neighboring Iraq, with its mix of Shia, Sunni, and Kurds, was a comparatively disjointed state held together by a powerful centralized dictatorship. Iran is very different. Any strategy that seeks to foment political upheaval in Iran via these various minority groups ignores the fundamental cohesion that characterizes Iran as a national unit. 95 If anything, such an approach would be more likely to bolster Iranian nationalism than to subvert it. As Vali Nasr, dean of the Johns Hopkins School of Advanced International Studies and an Iranian‐​American, told the _New Yorker_ in 2008, “Iran is an old country—like France and Germany—and its citizens are just as nationalistic. The U.S. is overestimating ethnic tension in Iran … working with the minorities will backfire, and alienate the majority of the population.” 96



Direct military action against Iran is the least likely of the options being considered under the Trump administration’s policy review. Indeed, the focus on nonmilitary options among Iran hawks is likely a response to the widespread distaste among the American public for engaging in another open‐​ended regime change war in the Middle East. Yet some have argued that the Trump administration should “rebuild military leverage over Iran,” including “contingency plans to neutralize Iran’s nuclear facilities,” engage in regional military exercises, and direct the U.S. navy to “fully and responsibly utilize rules of engagement to defend themselves and the Persian Gulf against rising Iranian harassment.” 97



There are various contingencies in which U.S. policymakers may face a decision on the use of military force against Iran, whether it is a purposeful strike against Iran’s nuclear facilities in the wake of U.S. withdrawal from the JCPOA, or a more gradual escalation following military confrontations in Syria, the Gulf, or elsewhere. As the Trump administration considers these options, however, it would do well to remember that the lack of good military options was the key reason behind the Bush and Obama administrations’ decision to pursue diplomacy with Tehran in the first place.



The United States should only undertake military action against another state if its core security interests are threatened. Yet there is no plausible near‐​term scenario in which Iran poses a direct threat to the U.S. homeland. Nor do Iranian actions in the Middle East pose a significant threat to U.S. interests in the region. Taking military action against Iran to thwart the purported threat of its nuclear program would harken back to the preventive war doctrine adopted by the Bush administration after the September 11th terrorist attacks and codified in the 2002 National Security Strategy. 98 Though proponents of military action often describe such action as “preemptive,” one RAND report notes that “generations of scholars and policymakers have defined preemption more restrictively,” limiting it to cases of imminent threat. 99 This is a crucial difference; as the authors highlight, international law holds that truly preemptive attacks are an acceptable use of force in self‐​defense, while preventive attacks are not. As the historian and former Kennedy administration adviser Arthur Schlesinger Jr. put it when criticizing the Bush administration’s case for war against Iraq, this doctrine of preventive war “is alarmingly similar to the policy that imperial Japan employed at Pearl Harbor, on a date which, as an earlier American president said it would, lives in infamy. Franklin D. Roosevelt was right, but today it is we Americans who live in infamy.” 100 With no imminent threat from Iran, there is no legal justification for direct military action.



At the very least, the Trump administration is constitutionally obligated to seek approval from Congress for any military action against Iran. Trump himself may disagree. He previously declined to seek or secure congressional authority for his missile strike on a Syrian military base controlled by the Assad regime in April 2017 and has repeatedly made public statements arguing that military action should be kept secret to preserve the tactical advantage of a surprise attack. If Trump does seek congressional approval for military strikes on Iran, he is likely to face strong opposition from many Democratic members of Congress and at least some Republicans. Senator Chris Murphy (D-CT) argued in February that “Trump and his most radical advisers are begging for war with Iran. This would be a disaster of epic scale, perhaps eclipsing the nightmare of the Iraq war.” 101 Congressional Democrats, already concerned about the administration’s domestic policy proposals, are unlikely to cut him a blank check on Iran.



Even small‐​scale military attacks on Iran—whether targeted strikes on nuclear facilities or clashes with Iranian forces in the Gulf or elsewhere—are likely to lead to escalation. In March 2012, the Pentagon held a classified war simulation “to assess the repercussions” of an Israeli attack on Iran’s nuclear facilities. The results showed that such a targeted strike would provoke immediate Iranian retaliation against U.S. military bases and naval assets in the region, drawing the United States into “a wider regional war.” 102 General James Mattis, now Trump’s secretary of defense, was then head of Central Command and supervised the war game. The _New York Times_ reported that Mattis told aides a strike “would be likely to have dire consequences across the region and for U.S. forces there.” Following a similar war game in 2004, retired Air Force Colonel Sam Gardiner concluded, “There is no military solution for the issues of Iran.” 103



It is not clear that a narrow or targeted strike is even possible. To strike Iran’s nuclear facilities, the United States would also need to bomb Iran’s air defense systems and command and control facilities, which itself carries risks of escalation. Writing in 2006, retired General Thomas McInerney suggested one such plan for attacking Iran’s nuclear facilities, requiring a massive commitment of 700 aircraft, 500 cruise missiles, and 28,000 bunker‐​buster bombs in the initial 36–48 hours. 104 Moreover, airstrikes of this kind, to accomplish any long‐​term objective, could not be limited to a single one‐​off mission. As explained in a 2012 study by the Iran Project, a nongovernmental organization founded to improve official contacts between the American and Iranian governments, for targeted strikes to “fulfill the stated objective of ensuring that Iran never acquires a nuclear bomb, the United States would need to conduct a significantly expanded air and sea war over a prolonged period of time, likely several years.” 105



Under bombardment from the world’s most dominant military superpower and uncertain of U.S. intentions, Iran would be likely to engage in retaliatory strikes against U.S. bases and military assets in Iraq, Syria, Bahrain, Qatar, and the United Arab Emirates. Iran’s Shahab‐​3 intermediate range ballistic missile can hit targets up to 2,000 kilometers away, while its Soumar cruise missile can potentially hit targets up to 2,500 kilometers away, meaning all U.S. forward‐​deployed bases in the Middle East and at least some bases in Europe are within range for conventional retaliation. 106 Likewise, the potential for asymmetric retaliation should not be underestimated. As Afshon Ostavar of the Naval Postgraduate School notes, “While Iran’s neighbors have poured billions of dollars into conventional weaponry, Iran has invested in comparatively cheap proxy forces that have proven effective in numerous theaters.” 107 Proxy groups such as Hezbollah or even Iran’s Quds force, a special unit of the IRGC, could engage in terrorist attacks against U.S. forces or allies in the region.



Anything beyond a limited military strike would have even more dire and counterproductive consequences. Taking military action to topple the Iranian regime, for example, would require a massive, lengthy, and costly military commitment. America’s experience in Iraq should be instructive in this context: Bush administration officials and their allies in the think‐​tank community and news media made bold predictions about the ease with which America would win the war, that Iraq would be reborn as a functioning democracy, and that the costs to the United States in lives and dollars would be minimal. These predictions proved wrong. In addition to bolstering Iran’s strategic position, the war helped to destabilize the region and to exacerbate America’s terrorism problem. A 2006 National Intelligence Estimate concluded that “the American invasion and occupation of Iraq … helped spawn a new generation of Islamic radicalism.” 108 The war had “become the ‘cause celèbre’ for jihadists, breeding a deep resentment of U.S. involvement in the Muslim world and cultivating supporters for the global jihadist movement.” 109



A large‐​scale ground war in Iran would be immensely damaging. Comparisons to Iraq are illuminating. The U.S. invasion was initially successful against a relatively ineffectual Iraqi military with approximately 389,000 men under arms. But U.S. forces have struggled in the years since to control territory, build a functioning Iraqi state, and deal with mass insurgency among the population of around 37 million. In comparison, Iran has a larger (about 523,000 active duty) and more effective military, a bigger population (80.3 million), and territory more than three times the size of Iraq. 110 A study by the Iran Project concluded: “If the United States decided to seek a more ambitious objective, such as regime change in Iran or undermining Iran’s influence in the region, then an even greater commitment of force would be required to occupy all or part of the country.… Given Iran’s large size and population, and the strength of Iranian nationalism, we estimate that the occupation of Iran would require a commitment of resources and personnel” greater than the costs of the wars in Afghanistan and Iraq combined. 111



A direct military attack on Iran, whatever the specific goals, is likely to be counterproductive in terms of nuclear nonproliferation. Military action short of regime change cannot eliminate Iran’s nuclear program or the knowledge behind its existence. 112 Given U.S. interventions in recent years, even targeted strikes may be seen by Tehran as a precursor to more intensive military action that must be deterred. A 2010 Defense Intelligence Agency study concluded that the main goal of Iran’s military strategy is regime survival, with a key focus on deterrence. 113 As Kenneth Pollack, a former CIA and National Security Council analyst, noted in 2006: “The Iraq example coupled with the North Korea example probably is part of the motivation for some in Iran to get a nuclear weapon.” 114 The 2011 U.S. intervention in Libya only intensifies this dilemma for Iran; Muammar Gaddafi voluntarily gave up his nascent nuclear program before being removed by a joint American‐​European intervention. Thus, while targeted strikes could delay Iran’s ability to develop nuclear weapons by destroying infrastructure, they would probably incentivize Iran to redouble its enrichment efforts under the conviction that only a nuclear deterrent can ensure its future survival.



This logic also implies broader strategic costs to an attack on Iran: it would exacerbate the problem of nuclear proliferation more generally. As the current Director of National Intelligence Dan Coats recently acknowledged at the Aspen Security Forum, U.S. actions against Saddam Hussein’s Iraq and Muammar Gadhafi’s Libya have made it clear to other states, like North Korea, that a nuclear deterrent may be the best way to ensure regime survival in the context of a war‐​prone United States. 115 North Korea itself confirmed this logic, releasing a statement after a 2016 nuclear test arguing that “the Saddam Hussein regime in Iraq and the Gaddafi regime in Libya could not escape the fate of destruction after … giving up nuclear programs of their own accord.” 116 As Nobel laureate Thomas Schelling has famously pointed out, American nonproliferation policies are ironically a prime driver of nuclear proliferation. 117 If, after successfully negotiating a nuclear deal, the United States then engages in an aggressive war against Iran despite Tehran’s full compliance with the JCPOA, other potential proliferators would have no reason at all to believe that the United States can be trusted to negotiate in good faith.



Though the Trump administration’s Iran policy review appears predestined to produce a more belligerent approach towards Iran, each of the options explored in this paper has significant flaws. Indeed, each option is unlikely to achieve its stated objectives, while at the same time creating an unacceptably high risk of exacerbating the very problems the Trump administration seeks to resolve. At a fundamental level, a more assertive U.S. policy towards Iran—whatever the details—will inevitably intensify Iranian fears about the country’s national security, worsening the very behaviors that the United States seeks to forestall. Even adopting one of these more hostile approaches to Iran while nominally upholding the JCPOA presents greater problems than embracing the nuclear deal and using it as a vehicle for further engagement designed to temper Iranian behavior.



As this paper highlights, it is doubtful that ratcheting up economic sanctions will alter Iranian policies in a more constructive direction, especially in the absence of international cooperation. Likewise, by pushing back harder against Iranian influence throughout the Middle East, the United States would incur substantial long‐​term costs in exchange for negligible gains in regional security. Moreover, a more aggressive approach could lead to unintended military escalation. Supporting internal opposition groups to pressure the regime or foment domestic upheaval is a hopeless strategy, given Iran’s domestic political realities and America’s long history of failed regime change endeavors. Finally, direct military action would have little public support, no legal basis, and most likely produce profoundly negative consequences for regional security and American interests.



Such actions would effectively return U.S.-Iranian relations to the cycle of enmity in which they were trapped prior to the negotiation of the JCPOA, with the nuclear issue dominating as a justification for continued hostility. Indeed, prior to the JCPOA, American allies in the region, particularly Saudi Arabia and Israel, often used the issue of Iran’s nuclear program to steer American policy toward Iran in a more confrontational direction. In private conservations with U.S. officials early in the Obama administration, then‐​king Abdullah bin Abdulaziz al‐​Saud pushed U.S. military action against the Iranian regime. 118 From 2010 to 2012, there were reports that Israel was close to initiating military strikes against Iranian nuclear facilities, knowing it would likely trigger U.S. involvement. Israeli Prime Minister Benjamin Netanyahu’s cabinet officials reportedly blocked him from taking this step. 119



Maintaining and strengthening the JCPOA will help to minimize the future potential for such pressure. Though he fought hard to subvert the JCPOA, for example, Netanyahu has been relatively silent since its adoption. Carmi Gillon, former head of the Israeli security agency Shabak, wrote in July that, thanks to the JCPOA “the threat of an Iranian nuclear weapon is more remote than it has been in decades.” Gillon added, “the majority of my colleagues in the Israeli military and intelligence communities supported the deal once it was reached, [and] many of those who had major reservations now acknowledge that it has had a positive impact on Israel’s security and must be fully maintained by the United States and the other signatory nations.” 120



If the United States is to avoid returning to high levels of tension and conflict in the U.S.-Iranian relationship, it must avoid the more belligerent options explored in this paper. The alternative—the option most likely to produce a positive outcome for all parties—is to uphold the JCPOA, carefully enforce its terms and conditions, and build on it to further engage Iran where its interests overlap with the United States. Pursuing greater diplomacy and engagement with Iran is, ironically, low‐​hanging fruit at this time. Iranian President Hassan Rouhani, who in his first term helped shepherd the JCPOA to fruition, won reelection this year by a wide margin, receiving 57 percent of the vote (compared to 38.5 percent for his chief opponent). 121 The idea of greater engagement with the West was a key component of Rouhani’s electoral platform; both centrists like Rouhani and reformers like former President Mohammed Khatami have argued in favor of what they describe as “JCPOA 2.0,” a series of internal policy compromises that will allow Iran to continue to engage with the West and begin to reintegrate into the global economy. 122



The key to reaping the benefits of a more conciliatory approach is recognizing that Iran is not a unitary actor. Iranian politics, though not fully democratic, are dynamic and competitive, and include various factions, from conservative hardliners to moderate reformists. The nuclear deal is widely popular in Iran, but antagonism from the Trump administration will bolster the prominence of Iranian hardliners who felt Tehran capitulated too much in the negotiations and who use fears of U.S. duplicity to undermine the idea of constructive engagement with Washington. 123 Similarly, perceptions that the United States is failing to live up to its side of the bargain—or is taking new steps that may undermine Iranian security—weaken political support for pragmatic reformists who see value in making concessions to the West in exchange for sanctions relief and integration with the outside world. Ultimately, unlike the more aggressive policy options explored in this paper, further engagement with Iran when possible will strengthen Iran’s more moderate political factions and weaken hardliners, providing a more hopeful future for U.S.-Iranian relations.
"
nan
"

State officials in Ohio have been complaining recently about potential revenue losses from the growth of untaxed Internet shopping. “We figure we’re losing over $200 million annually from direct marketing, catalog and Internet sales,” says Clare Long, Ohio’s deputy tax commissioner. Across the nation, departments like Long’s have been fighting for years to force out‐​of‐​state mail order companies to collect sales taxes. Now electronic commerce is in the cross hairs. What state officials propose is — you guessed it — more taxes. 



The officials are worried because an estimated 27 million U.S. households (a number that’s getting bigger every day) now use the Internet regularly. In the officials’ eyes, that’s too many consumers who might make purchases without the government’s getting a cut. Buyers are supposed to pay a “use tax” in lieu of a sales tax on all out‐​of‐​state purchases, but few volunteer. 



At first glance, the proposal sounds reasonable: why not tax identical items the same regardless of how they’re purchased? From the tax collector’s perspective, that makes sense. But in the real world, there are several reasons why allowing states to tax out‐​of‐​state electronic commerce is bad policy. 



First, there is no immediate danger of large revenue losses for traditional retailers, and by extension, for state tax authorities. Because they cater to a customer’s desire for a hands‐​on experience, local stores don’t charge for shipping and offer immediate gratification and so will probably always dominate retailing. What’s more, shopping is for many people a pleasurable social experience that cannot be duplicated online. Thus, Internet sales won’t destroy “real” retailers, just as catalog sales haven’t. 



National data support that conclusion. In an era of almost no inflation, state budgets grew by 5 percent in fiscal year 1997 and by more than 6 percent in fiscal year 1998. The last fiscal year ended with about $21 billion more in tax collections than originally anticipated. It appears that states will enjoy a sizable revenue windfall this year as well. If electronic commerce is undermining state revenues, it’s an undetectable trend. Electronic commerce certainly hasn’t slowed the flood of surplus money pouring into Columbus — expected to be around $400 million this year. 



Second, it’s not fair to force out‐​of‐​state firms to act as tax collectors when they don’t benefit from state services. When an Ohio business collects sales taxes, there is a clear linkage between the taxes paid, the services provided, and legislative representation. After all, local firms benefit from police and fire protection, roads and waste collection and other state services, so it is proper that they help cover those costs. And local firms can make their voice heard directly through lobbying and membership in groups like the Chamber of Commerce. 



Remote sellers, on the other hand, don’t enjoy any of those advantages. If the state wants more of the taxpayers’ money, it should collect it itself and not try to push the burden onto out‐​of‐​state businesses. 



Finally, differentiated tax rates create healthy competition that helps keep local rates under control. For example, some residents of Manhattan drive to Delaware to avoid sales taxes — an option that has undoubtedly curbed the profligate fiscal habits of New York politicians. Electronic commerce similarly guards against excessive taxation. When sales tax rates get too high, it’s important that Ohioans have a shopping alternative. 



The idea that government won’t find some way to keep the tax dollars flowing is laughable. So let’s be honest about what’s going on here: allowing states to tax out‐​of‐​state electronic commerce would be the equivalent of a tax increase. States would fatten already overflowing coffers without ever having to bring the issue to a vote at home. That’s a dream scenario for state legislators but a nightmare for taxpayers. 



If states are concerned about local retailers, they can effectively address the issue by moving tax rates downward. Minnesota policymakers have raised one such interesting possibility, proposing to eliminate the sales tax on certain products that are easily acquired online. Specifically targeted are intangible goods that can be downloaded, such as software, music and books. 



Sure, limiting states’ taxing authority can lead to unequal taxation. But such limitations are a crucial component of American federalism. Absent those restraints, confiscatory tax rates — which are the true injustice — would get worse. To improve its business climate Ohio should cut taxes, not scheme to collect more.
"
"
Here is a weather curiosity. We’ve been hearing a lot about snowfall in the northern hemisphere this year. In Oslo, they have given up on trying to pile it up so they have resorted to dumping it in the sea. If this happened in Seattle they’d probably get into a tizzy for polluting Puget Sound with fresh water snow. And it is not just Oslo, the problem seems widespread. Here are some other news stories in London, OT Geneva, Ohio Chardon, OH Wasatch, UT Chicopee, MA and Rochester, NY where they say the piles are making driving dangerous. In Wenatchee, WA they want to spray warm sewage water on the snow to melt it.  I know they could use the USHCN temperature sensor at the sewage treatment plant there to check the temperature to make sure conditions are right. Yeah, that’s the ticket! – Anthony

From Reuters Environment Blog by Alister Doyle
It looks more like an Ice Age than global warming.
There is so much snow in Oslo, where I live, that the city authorities are resorting to dumping truckloads of it in the sea because the usual storage sites on land are full.
That is angering environmentalists who say the snow is far too dirty – scraped up from polluted roads — to be added to the fjord. The story even made it to the front page of the local paper (’Dumpes i sjøen’: ‘Dumped in the sea’).
In many places around the capital there’s about a metre of snow, the most since 2006 when it was last dumped in the sea. Extra snow usually gets trucked to sites on land, where most of the polluted dirt is left after the thaw. Those stores are now full — in some the snow isn’t expected to melt before September.
But are these mountains of snow a sign that global warming isn’t happening?
Unfortunately, more snow might fit projections by the U.N. Climate Panel, which says that northern Europe is likely to get wetter and the south drier as temperatures rise this century.
“By the 2070s, hydropower potential for the whole of Europe is expected to decline by 6 percent, with strong regional variations from a 20 to 50 percent decrease in the Mediterranean region to a 15 to 30 increase in northern and eastern Europe.” it said in a 2007 report (page 60 of this link).
So people in northern Europe may have to buy more snow shovels than parasols to cope with global warming?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e98486bb9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterAs winters get harsher and the snow piles up, more and more scientists are now warning of global cooling. Reader Matt Vooro has compiled a list (see below) of 31 prominent scientists and researchers who have words that governments ought to start heeding.
UPDATE: Another one for the list – Professor Paar, from Croatia’s Zagreb University
Cooling seems to be the trend. Photo source: NOAA. 
Lately, the clueless among warmist scientists, governments and the MSM have been running around in deep snow with their global warming blinders on, denying the cold around them. Governments, entrusted to serve the citizens, really ought to start listening up and planning accordingly.
———————————————————————————————–
Are we headed for global warming or cooling?
By Matt Vooro
For many years now a good number of non-AGW scientists, meteorologists, engineers, researchers and the like have looked at the possibilities of a cooling planet. I enclose some of the ones that I have noted  in my research. Indeed there is a significant number of scientists, academics, meteorologists and researchers who disagree with IPCC’s belief that the globe is very likely headed for unprecedented global warming due to man-made greenhouse gases.
The climate of this planet oscillates between periods of approximately 30 years of warming followed by approximately 30 years of cooling. Rather than 100 years of unprecedented global warming as predicted by IPCC, the global temperatures have leveled off and we seem to be heading for cooler weather.  
Lawrence Solomon in his article of 16 June, 2010 in the Toronto National Post commented on Professor Mike Hulme’s article about IPCC. The article can be found here at probeinternational.org. Hulme is a Professor of Climate Change in the School of Environmental Sciences at the University of East Anglia – the university of Climategate fame — is the founding Director of the Tyndall Centre for Climate Change Research and one of the UK’s most prominent climate scientists. Quoting Hulme, Solomon said:
The UN’s Intergovernmental Panel on Climate Change misled the press and public into believing that thousands of scientists backed its claims on manmade global warming, according to Mike Hulme, a prominent climate scientist and IPCC insider. The actual number of scientists who backed that claim was “only a few dozen experts” he states in a paper for Progress in Physical Geography, co-authored with student Martin Mahony.”
Professor Hulme’s paper can be found at fabiusmaximus
It would appear that IPCC underestimated the repetitive and significant impact of normal planetary cycles like the PDO, AO, AMO, NAO, ENSO, DEEP OCEAN CURRENTS [MOC], SOLAR CYCLES and UNEXPECTED PERIODS OF VOLCANIC ASH.
This is understandable as IPCC never had a mandate to study all causes of global warming but only the man induced component which seems to be dwarfed by natural planetary factors, which other scientists are now finding out. Read papers.ssrn.com/sol3/papers.
Here is a list of 31 different international climate scientists, academics, meteorologists, climate researchers and engineers who have researched this topic and who disagree with AGW science and IPCC forecasts, and are projecting much cooler weather for the next 1-3 decades.

The List

1. Don Easterbrook, Professor Emeritus, Dept. of Geology, Western Washington University.
Setting up of the PDO cold phase assures global cooling for next approx. 30 years. Global warming is over.  Expect 30 years of global cooling, perhaps severe 2-5°F.”
He predicts several possible cooling scenarios: The first is similar to 1945-1977 trends, the second is similar to 1880-1915 trends and the third is similar to 1790-1820 trends. His latest article states:
Expect global cooling for the next 2-3 decades that will be far more damaging than global warming would have been.”
Read here, here and here.
2. Syun Akasofu, Professor of Geophysics, Emeritus, University of Alaska, also founding director of ARC
He predicts the current pattern of temperature increase of 0.5C /100 years resulting from natural causes will continue with alternating cooling as well as warming phases. He shows cooling for the next cycle until about 2030/ 2040.
And again a new paper ON THE RECOVERY FROM LITTLE ICE AGE – Read here.
3. Prof. Mojib Latif, Professor, Kiel University, Germany
He makes a prediction for one decade only, namely the next decade [2009-2019] and he basically shows the global average temperatures will decline to a range of about 14.18 C to 14.28 C  from 14.39 C  [eyeballing his graphs].
He also said that “you may well  enter  a decade or two of cooling relative to the present temperature level”, however he did not indicate when any two decades of cooling would happen or whether the  second decade after the next decade will also be cooling. Read here and here.
4. Dr. Noel Keenlyside from the Leibniz Institute of Marine Sciences at Kiel University. The BBC writes:
The Earth’s temperature may stay roughly the same for a decade, as natural climate cycles enter a cooling phase, scientists have predicted.”
A new computer model developed by German researchers, reported in the journal Nature, suggests the cooling will counter greenhouse warming.”
Read here news.bbc.
5. Professor Anastasios Tsonis, Head of Atmospheric Sciences Group University of Wisconsin, and Dr. Kyle Swanson of the University of Wisconsin-Milwaukee. msnbc writes:
We have such a change now and can therefore expect 20 -30 years of cooler temperatures”
This is nothing like anything we’ve seen since 1950,”
Kyle Swanson of the University of Wisconsin-Milwaukee said. “Cooling events since then had firm causes, like eruptions or large-magnitude La Ninas. This current cooling doesn’t have one.”
Swanson thinks the trend could continue for up to 30 years.”
Also read The mini ice age starts here at dailymail.co.uk/.
6. William M Gray, Professor Emeritus, Dept of Atmospheric Sciences, Colorado State University
A weak global cooling began from the mid-1940’s and lasted until mid-1970’s. I predict this is what we will see in the next few decades.”
Read colostate.edu.
7. Henrik Svensmark , Professor DTU, Copenhagen. Henrik Svensmark writes:
Indeed, global warming stopped and a cooling is beginning. No climate model has predicted a cooling of the Earth, on the contrary. This means that projections of future climate is unpredictable.”
Read here.
8. Jarl R. Ahlbeck, D.Sc., AboAkademi University, Finland
Therefore, prolonged low solar activity periods in the future may cause the domination of a strongly negative AO and extremely cold winters in North America, Europe and Russia.”
Read here.
9. Dr. Alexander Frolov, Head of Russia’s state meteorological service Rosgidromet. The Daily Mail.co.uk quotes Frolov:
‘From the scientific point of view, in terms of large scale climate cycles, we are in a period of cooling.
‘The last three years of low temperatures in Siberia, the Arctic and number of Russia mountainous regions prove that, as does the recovery of ice in the Arctic Ocean and the absence of warming signs in Siberia.”
And writes:
Mr. Tishkov, deputy head of the Geography Institute at Russian Academy of Science, said: ‘What we have been watching recently is comparatively fast changes of climate to warming, but within the framework of an overall long-term period of cooling. This is a proven scientific fact’.” 
10. Mike Lockwood, Professor of Space Environmental Physics, University of Reading, UK. Read BBC News here:   
The UK and continental Europe could be gripped by more frequent cold winters in the future as a result of low solar activity, say researchers.”
11. Dr. Oleg Pokrovsky, Voeikov Main Geophysical Observatory: Ria Novosti writes:
There isn’t going to be an ice age, but temperatures will drop to levels last seen in the 1950s and 1960s.
Right now all components of the climate system are entering a negative phase.  The cooling will reach it’s peak in 15 years. Politicians who have geared up for warming are sitting on the wrong horse.
The Northeast Passage will freeze over and will be passable only with icebreakers.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Pokrovsky also claims that the IPCC, which has prophesized global warming, has ignored many factors. He also noted that most American weather stations are located in cities where temperatures are always higher.
We don’t know everything that’s happening. The climate system is very complex and the IPCC is not the final truth on the matter.”
Read here NoTricksZone.
12. Girma Orssengo, b.Tech, MASc, PhD 
These cool and warm PDO regimes correlate well with the cooling and warming phases of GMTA shown in Figure 3.
The model in Figure 3 predicts global cooling until 2030. This result is also supported by shifts in PDO that occurred at the end of the last century, which is expected to result in global cooling until about 2030 [7].”
Read WUWT and read here, and
In this article, a mathematical model was developed that agrees with observed Global Mean Temperature Anomaly(GMTA), and its prediction shows global cooling by about 0.42 deg C until 2030. Also, comparison of observed increase in human emission of CO2 with increase in GMTA during the 20th century shows no relationship between the two. As a result, the claim by the IPCC of climate catastrophe is not supported by the data.”
‘Fossil fuels allowed man to live his life as a proud human, but the IPCC asserts its use causes catastrophic.’ “
Read here at WUWT.
13. Nicola Scafetta, PhD.
Empirical evidence for a celestial origin of the climate oscillations and its implications
The partial forecast indicates that climate may stabilize or cool until 2030-2040.”
Read here
14. Dr William Livingston, astronomer  & solar physicist; and 15. Dr Matthew Penn – astronomer &  solar physicist
Astronomers Dr. William Livingston and Dr. Matthew Penn and a large number of solar physicists would say that now the likelihood of the Earth being seized by Maunder Minimum is now greater than the Earth being seized by a period of global warming.”
Read here: http://algorelied.com/?p=2706.
16. Joe d’Aleo – Executive Director of Certified Consultant Meteorologists. Read here:  Intellicast.com
Longer term the sun is behaving like it did in the last 1700s and early 1800s, leading many to believe we are likely to experience conditions more like the early 1800s (called the Dalton Minimum) in the next few decades. That was a time of cold and snow. It was the time of Charles Dickens and his novels with snow and cold in London.”
Also see various other articles about Global Cooling under ICE AGE at Ice Cap
17. Harry van Loon, Emeritus at NCAR and CORA, 18. Roland Madden, Senior scientist at NOAA, Deputy Head of Climate analysis, 19. Dave Melita, Head Meteorologist at Melita Weather Associates, and 20. William M Gray, Professor Emeritus, Dept of Atmospheric Sciences, Colorado State University
These scientists came to the same conclusions— the global warming trend is done, and a cooling trend is about to kick in.
Read here!
21. Dr. David Archibald, Australia, environmental scientist:
In this presentation, I will demonstrate that the Sun drives climate, and use that demonstrated relationship to predict the Earth’s climate to 2030. It is a prediction that differs from most in the public domain. It is a prediction of imminent cooling.”
See Warwick Hughes and David Archibald
22. Dr Habibullo Abdussamatov, Head of Space Research, Lab of Pulkov Observatory. See iceagenow.com:
In his presentation called The Sun Dictates the Climate, he indicated that there would be an ice age kind of temperatures in the middle of the 21st century. He showed a graph called The forecast of the natural climate change for the nearest 100 years and it showed the globa temperatures dropping by more than 1°C by 2055. According to him, a new ice age could start by 2014.”
And read here.
23. Dr Fred Goldberg, Swedish climate expert. People Daily:
We could have an ice age any time, says Swedish climate expert.”
and read: We could have an ice age any time, says Swedish climate expert
24. Dr. George Kukla, a member of the Czechoslovakian Academy of Sciences and a pioneer in the field of astronomical forcing, Read Ice Age Now:
In the 1970s, leading scientists claimed that the world was threatened by an era of global cooling.
Based on what we’ve learned this decade, says George Kukla, those scientists – and he was among them — had it right. The world is about to enter another Ice Age.”
25. Peter Clark, Professor of Geosciences at OSU: Read iceagenow.com: 
Sometime around now, scientists say, the Earth should be changing from a long interglacial period that has lasted the past 10,000 years and shifting back towards conditions that will ultimately lead to another ice age.”
26. James Overland, NOAA. Read physorg.com:
‘Cold and snowy winters will be the rule rather than the exception,’ said James Overland of the US National Oceanic and Atmospheric Administration.”
27. Dr. Theodore Landscheidt. Predicted in 2003 that the current cooling would continue until 2030 [Read here]:
Analysis of the sun’s varying activity in the last two millennia indicates that contrary to the IPCC’s speculation about man-made global warming as high as 5.8°C within the next hundred years, a long period of cool climate with its coldest phase around 2030 is to be expected.”
28. Matt Vooro, P. Eng. The icecap.us:
We seem to be in the same climate cycle that we were back in 1964-1976.The last two winters [2008, 2009] have been very similar to those we had back then with all the extra snow and cold temperatures. Once the extra warming effect of the current 2009/2010 El Nino is finished, watch for colder temperatures to return due to the impact of the negative PDO, AMO, AO, NAO, ENSO/La Nina, major volcanic ash and changing solar cycles.”
Good source of articles and data on global cooling, see: Isthereglobalcooling.com
29. Thomas Globig, Meteorologist, Meteo Media weather service. Read here at WUWT:
‘The expected cold for the next month will bring this down significantly by year end. ‘The year 2010 will be the coldest for ten years in Germany,’ said Thomas Globig from the weather service Meteo Media talking to wetter.info. And it might even get worse: ‘It is quite possible that we are at the beginning of a Little Ice Age,’ the meteorologist said. Even the Arctic ice could spread further to the south.”
30. Piers Corbyn, Astrophysicist. From http://wattsupwiththat.com/2010/12/27/piers-corbyn-goes-global-cooling/
Predicting in November that winter in Europe would be “exceptionally cold and snowy, like Hell frozen over at times,” Corbyn suggested we should sooner prepare for another Ice Age than worry about global warming. Corbyn believed global warming “is complete nonsense, it’s fiction, it comes from a cult ideology. There’s no science in there, no facts to back [it] up.”
31. Dr. Karsten Brandt, Director of donnerwetter.de weather service.
It is even very probable that we will not only experience a very cold winter, but also in the coming 10 years every second winter will be too cold. Only 2 of 10 will be mild.
Read here.
32. Joe Bastardi – Accuweather meteorologist and hundreds of other meteorologists (i.e. expert forecasters who outperform climatologists hands-down in seasonal forecasting).
http://www.accuweather.com/ukie/bastardi-europe-blog.asp
Other global cooling articles:
John Holdren Ice Age Likely
BBC News
Global Warming Debunked
Star Tribune
nationalpost.com
canadafreepress.com
news.yahoo.com
======================================================
Share this...FacebookTwitter "
"
This is from the Huffington Post. One can only hope that Kerry will follow through. For a quick primer on Kerry’s grasp of climate science, see this WUWT article: Kerry Blames Tornado Outbreak on Global Warming and a rebuttal Increasing tornadoes or better information gathering? I get a kick out of Kerry’s line “This has to stop”. Okay then, please debate Mr. Will, put a stop to it Mr. Kerry! –  Anthony
John Kerry
U.S. Senator from Massachusetts
Posted February 27, 2009 									| 04:47 PM (EST)
Facts Are Stubborn Things: George Will and Climate Change-
To paraphrase the conservative columnist’s favorite president, “There you go again, George.”
George Will has been one of my favorite intellectual sparring partners for a long time, a favorite more recently because he had the guts to publicly recognize the disaster that was George W. Bush’s presidency.
But in his latest Washington Post column, George and I have a pretty big loud disagreement.
Don’t get me wrong. I’m happy to see Will embracing the idea of recycling, but I’m very troubled that he is recycling errors of fact to challenge the science on global warming.
I’m even more troubled that Will used his February 15th column not only to cast doubt on sound science, but also to denigrate the work of two fine scientists.
Let’s be very clear: Stephen Chu does not make predictions to further an agenda. He does so to inform the public. He is no Cassandra. If his predictions about the effects of our climate crisis are scary, it’s because our climate is scary.
Likewise, John Holdren is a friend of mine and one of the best scientific minds we have in our country. Pulling out one minor prediction that he had some unknown role in formulating nearly three decades ago, as Will did in his February 15th column, and then using that to try to undo his credibility as a scientist may be a fancy debating trick, but it’s just plain wrong when it comes to a debate we can’t afford to see dissolve into reductio ad absurdum hijinx. (A side note: The incident in question occurred in 1980, which, as I recall, was just about the time Ronald Reagan made the claim that approximately 80 percent of our air pollution stems from hydrocarbons released by vegetation and that, consequently, we should “not go overboard in setting and enforcing tough emissions standards from man-made sources.”)
Dragging up long-discredited myths about some non-existent scientific consensus about global cooling from the 1970s does no one any good. Except perhaps a bankrupt flat earth crowd. I hate to review the record and see that someone as smart as George Will has been doing exactly that as far back as 1992. And it’s especially troubling when the very sources that Will cites in his February 15th column draw the exact opposite conclusions and paint very different pictures than Will provides, as the good folks at ThinkProgress and Media Matters for America have demonstrated so thoroughly.
This has to stop. A highly organized, well-funded movement to deny the reality of global climate change has been up and running for a long time, but it doesn’t change the verdict: the problem is real, it’s accelerating, and we have to act. Now. Not years from now.
No matter how the evidence has mounted over two decades — the melting of the arctic ice cap, rising sea levels, extreme weather — the flat earth caucus can’t even see what is on the horizon. In the old Republican Congress they even trotted out the author of Jurassic Park as an expert witness to argue that climate change is fiction. This is Stone Age science, and now that we have the White House and the Congress real science must prevail. It is time to stop debating fiction writers, oil executives and flat-earth politicians, and actually find the way forward on climate change.
This is a fight we can win, a problem we can overcome, but time is not on our side. We can’t waste another second arguing about whether the problem exists when we need to be debating everything from how to deal with the dirtiest forms of coal as the major provider of power in China to how to vastly increase green energy right here at home.
“Facts are stupid things,” Ronald Reagan once said. He was, of course, paraphrasing John Adams, who could have been talking about the science on global change when he said, “Facts are stubborn things.”
Stubborn or stupid — lets have a real debate and lets have it now.
I know George Will well, I respect his intellect and his powers of persuasion — but I’d happily debate him any day on this question so critical to our survival.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e97b846cc',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

People who don’t think the federal government sometimes exaggerates things a wee bit obviously did not survive the Y2K crisis. Or perhaps they merely sizzled away in the record heat endured by our fair Republic, as recently reported by the Department of Commerce, which has pronounced 1998–99 the warmest years for which we have adequate records — 1998 comes in as hottest and 1999 as second warmest. 



One Y2K lesson is that what is said in Washington isn’t necessarily what is, depending on what “is” means. In the case of the nation’s or the globe’s temperature, our government has chosen to trumpet one particular climate history out of several that are available. Not surprisingly, the government tells us about the hottest, while the rest are not remarkable at all. 



The heated pronouncement, which actually came from the National Climatic Data Center (NCDC), is not a result of cooking the books. Instead, it is a result of very selective reading. 



The particularly hot data set, known as the Historical Climate Network (HCN), comes from several hundred rural weather stations selected from the roughly 16,000 official sites that are available. But let’s consider two other data sets, one from NASA and the other from the very same NCDC. 



NASA’s record is compiled by James Hansen, director of the Goddard Institute for Space Studies. While the HCN has 1999 as the second‐​hottest year on record (records go back only to 1895), Hansen’s record ranks it as a very unremarkable 14th warmest, putting it nowhere close to the blazing year of 1934, when there was a real drought (as opposed to 1999, when less of the country than normal experienced extreme dryness). 



NCDC’s “other” history is known as “Climatological Division” (CD) record. It uses virtually all of the available data, aggregates them into 344 geographic units (CDs) that are thought to have some climate homogeneity, and then totals them, adjusting for the area occupied by each CD. In this record, both 1998 and 1999 temperatures fall beneath those of 1934. Instead of being second warmest, 1999 will be between fourth and eighth, depending on December data that have not yet been entered. 



The peculiar thing about the discrepancy between the CD and HCN records is that the former includes a few stations that are known to be artificially warming because cities have a way of growing up around their weather stations. Why the HCN should now be running warmer than this record is a mystery to everyone. But why the feds only trumpeted the HCN heat should be a mystery to no one. They are trying to whip up hysteria in order to shame the Senate into approving the United Nations’ Kyoto Protocol on climate change. Currently, 11 senators (of the required 67) support this economically disastrous treaty. 



The United Nations itself recently made a similar pronouncement about 1998–99 for global temperature. Their record goes to 1860. (Ask yourself how the U.S. record goes back only to 1895 while the U.N.‘s global history starts in 1860, and you will get an idea of how reliable are our historical statements about “global” warming.) 



Needless to say, the U.N.‘s story was blatted by every media network that serves it. But there are two other measures of global temperature that, like the NCDC and NASA records for the United States, were not so hot. 



The first is University of Alabama climatologist John Christy’s satellite history, which has been carefully corrected for instrument and orbit changes, that shows 1999 to be slightly cooler than the average for the 21 years in which the platforms have been taking our temperature. There are 12 warmer years and eight cooler ones in this history, which shows a slight warming trend only because of the big 1998 El Niño. (This means that the decade from 1998 to 2007 will very likely show a cooling trend.) 



The satellite temperatures are known to closely track those measured by weather balloons in the atmospheric layer from 5,000 to 30,000 feet — a zone forecast by computer models of global warming to be heating even more rapidly than the surface. This record extends back to 1958. Fifteen years were warmer than 1999 and 27 were cooler. 



The resolution of the difference between the U.N.‘s surface temperatures and those measured by the satellites and the weather balloons may spell the end of the global warming crisis. More and more, it appears that the reason they diverge is that warming is trapped largely in very cold airmasses in Siberia that don’t extend up to 5,000 feet, which is the altitude at which the balloon record begins. No one has yet to hear Russians clamoring for a return to the climate of the Stalin era. 



As Casey Stengel used to say, “You could look it up.” The NASA data are at www​.giss​.nasa​.gov/data, the NCDC CD history is at ftp​.ncdc​.noaa​.gov/pub, and the satellite record is at ftp://​vor​tex​.atmos​.uah​.edu/msu. 



Any way you look at it, governments have been less than truthful in telling the whole story about the heat of 1999. But what do you expect? After all, it is the year 2K.
"
"Southern Africa’s game farms are private reserves that house wildlife such as giraffes, zebras and antelope to be used for restocking national parks, meat production or trophy hunting. But these farms have a problem. Warthogs and porcupines want to move around the reserves too, and they have an annoying habit of making large holes under the boundary fences to burrow their way in and out. For cheetahs, these holes are an ideal way to get inside and prey on the valuable game.  Cheetahs are considered pests in these reserves and, in Namibia, game farmers end up killing more cheetahs than livestock farmers do. These game farms needed to find a way to let the warthogs in while keeping the big predators out. The solution is simple but unusual – have you ever considered whether your cat flap might be used by other animals besides your feline friend? It turns out “swing gates” (a technical term for a glorified cat flap installed along a fence line) are ideal. According to research I carried out with colleagues in Namibia, recently published in the African Journal of Ecology, warthogs and porcupines have learned how to use these gates, but big cats such as cheetahs and leopards don’t seem to have figured this out yet.  It could be that the big cats see an intact fence and do not bother to investigate the integrity of it, whereas warthogs may be more inquisitive and spend more time rummaging around the fence line looking for holes. This is great news for both livestock and wildlife farmers because it now stops movement of large carnivores into farms and can limit the amount of expensive antelope or buffalo that are killed by predators.  With the threat removed, farmers now do not need to resort to hunting carnivores to limit the damage that they cause on the farms, and it also means that they no longer kill hole-digging species like warthogs, aardvarks and porcupines to limit the number of fence breaches. Our study determined that the number of holes dug by burrowing animals under game fences decreased over time and this was most evident when the swing gates were easily accessible and ideally placed. This means installing gates in open areas with harder soil, dense vegetation, close to the watering holes where water-loving, perpetually-thirsty warthogs like to hang out. What’s more, swing gates are far cheaper than electric fencing – the conventional way to stop animals burrowing their way in. So it’s really a win for both people and wildlife. Electric fencing requires continual application of weed killer to stop the grass from short-circuiting the electric current, but the only maintenance that is needed with swing gates is the occasional hole-filling from the rogue warthog that does decide to dig a new hole under the fence. If we want to conserve cheetahs and promote co-existence between humans, wild game and wild predators, then using fences to exclude big cats from their natural habitat won’t work in the long term. But for the time being, this is a quick and simple fix. Farmers can add swing gates to their tool box of effective yet non-lethal techniques to combat predation of livestock. Laurie Marker, founder of the Cheetah Conservation Fund and one of the study’s co-authors, points out why it is so crucial that this works for game farmers too. Predators, she says, create large financial losses on game farms. “Swing gates will enable us time to work on a permanent solution that will enable all species to peacefully coexist on the same land, such as the development of conservancies”.  In these conservancies wildlife is allowed to roam free, without high game fences. As Marker points out, “neighbouring farmers and land occupiers then manage the resources collectively, allowing for predators to be managed within the larger landscape system.” In the meantime, you better watch out what other critters enter your house via your cat flap, as it might not just be Felix that is coming inside."
"

The Dayton Peace Agreement, signed in Paris on Dec. 14, 1995, formally ended the civil war in Bosnia. Three years later, Dayton’s goal of creating a unified, multiethnic Bosnian state remains as elusive as ever. But that should have been expected. According to University of Chicago political scientist John Mearsheimer, “History records no instance where ethnic groups have agreed to share power in a democracy after a large‐​scale civil war. The democratic power‐​sharing that Dayton envisions has no precedent.” 



That’s not to say that Dayton hasn’t led to any success. The fighting has stopped, and so far more than 3,600 pieces of heavy weaponry have been removed. But those few successes reveal Dayton for what it really is: a complicated cease‐​fire, not a solution to Bosnia’s long‐​term problems. The country still is deeply fractured, divided into two semiautonomous “entities” separated by a monstrosity called the Inter‐​Entity Boundary Line. One entity, the Serb Republic, now almost is entirely Serb. The other, the Muslim‐​Croat Federation, is made up of rival enclaves that maintain a tense coexistence. Nearly 90 percent of the Serbs who lived in the Muslim‐​Croat Federation before 1992 were expelled or have left. 



The prospect for ethnic reintegration is not promising. By the end of 1997, only 19 percent of Bosnia’s 2.3 million refugees and displaced persons had returned home. Moreover, the total number of returnees in 1998 is expected to be only 11 percent of that of 1997. Even more telling, during the last three years only 55,000 Bosnians have returned to areas where they would be in the minority. At the same time, 80,000 Bosnians have moved from areas where they were in the minority to areas where they would be in the majority. That means there are 25,000 fewer Bosnians living in integrated communities today than when Dayton was signed three years ago. Moreover, 85 percent of Bosnians recently polled still say they will not vote for a candidate from another ethnic group. 



Nevertheless, the Clinton administration insists that the Dayton agreement will not be adapted to the reality that has existed in Bosnia for three years — de facto ethnic separation. That unwillingness to rethink Dayton is ill‐​conceived. 



But Bosnia’s costs are higher than many realize. It and other noncombat operations around the world have been diminishing U.S. national security by creating an operations tempo that undercuts U.S. military readiness. 



First, it ensures that billions of taxpayer dollars will be spent in vain trying to superimpose an imaginary Bosnia (united) over the real Bosnia (divided). The United States already is paying about half of the costs of the Bosnia peacekeeping operation, which includes 6,900 U.S. combat troops in Bosnia, plus 3,100 support personnel in Croatia, Hungary and Italy. By the end of fiscal year 1999, Washington will have spent $10.64 billion on the mission. 



But Bosnia’s costs are higher than many realize. It and other noncombat operations around the world have been diminishing U.S. national security by creating an operations tempo that undercuts U.S. military readiness. In fact, during the last decade, the U.S. Army has been used in 29 significant overseas operations, compared with 10 during the preceding 40 years. The strain of that pace has shown up in negative trend lines across all military services in a number of readiness categories. 



For example, to relieve the European‐​based units that have carried out most of the Bosnia mission so far, peacekeeping duties have been shifted to Ft. Hood’s First Cavalry Division, one of the premier U.S.-based combat divisions. As one staff member of the House National Security Committee observed: “The Army is disassembling one of its most ready, most fearsome war‐​fighting divisions. The action shows how the requirements of Bosnia are detracting from the military’s ability to do high‐​intensity conflicts.” 



Bosnia and other overseas operations also have caused the U.S. Air Force’s readiness to slip. The units that fly over Bosnia and the Persian Gulf have priority for plane rotation, support equipment and pilots. As a result, fighter squadrons based in the United States are at their lowest readiness level in years. In 1992, 86 percent of U.S.-based fighter jets were designated “mission capable.” In 1998 only 75 percent were. 



Even more worrisome, there is mounting evidence that peacekeeping and other noncombat operations have adversely affected retention of soldiers, sailors and pilots. The Pentagon reports that first‐​term soldiers assigned to peacekeeping in Bosnia generally reenlisted at the same rate as their counterparts stationed elsewhere in Europe last year. But soldiers stationed in Bosnia were offered a tax‐​exempt reenlistment bonus, which artificially inflated their retention rate. The gap in retention rates for midcareer soldiers stationed in Bosnia was more noticeable. They reenlisted at a rate 6.1 percent lower than that of their counterparts stationed elsewhere in Europe. 



Or take the Air Force. Since 1996, it has performed hundreds of peacekeeping missions in 11 countries, with the Bosnia operation being one of the largest. These mundane and repetitive missions have affected pilot morale negatively because there is no compelling national interest to keep them motivated. 



“We’re not really fighting the country’s wars; we’re just acting like the world’s policeman,” explains one pilot who is a veteran of both Bosnia and Saudi Arabia. This year, nearly 45 percent of eligible Air Force pilots did not renew their service contracts, up dramatically from 14 percent in 1994. Such an anemic retention rate cannot long be sustained without compromising U.S. military readiness. Last year the Air Force had 45 fewer pilots than needed. This year the number has grown to 700, and it’s expected to reach 2,000 by 2002. 



There also is increasing evidence that peacekeeping operations — as distinguished from actual national defense — deter prospective recruits from joining the military. And a strong economy, with plenty of private‐​sector jobs, has made it even tougher for the military to find recruits to replenish its shrinking ranks. 



For fiscal year 1998, both the Navy and the Air Force failed to meet their recruiting goals. The Army was more successful, but only because its recruiting target was lowered significantly. The Navy fell short of its annual recruitment target by 13 percent, and it recently was reported that the Navy has 18,022 fewer sailors at sea than it needs. 



The recruitment problem likely will worsen with the current demographic downturn in the prime recruiting pool: males between the ages of 18 and 21 who are physically fit high‐​school graduates and who scored in the upper half of the military’s standardized entry examination. That population currently consists of 15 percent fewer people than it did in the mid‐​1980s. 



Washington’s unwillingness to revise Dayton also may be paralyzing Bosnian reconstruction. In election after election Bosnian voters cast ballots for nationalist candidates to counterbalance the perceived political power of their ethnic rivals who, in turn, vote for nationalist candidates for the same reason. Such circular logic is built into Dayton because the agreement requires three ethnic groups, each of which fears the political ambitions of the others, to operate under the fiction of a unified state. The political obstructionism and stalemates brought on by upholding that fiction have crippled Bosnia’s efforts to emerge from a communist economy. In fact, three years and $4.35 billion in reconstruction aid later, Bosnia has yet to privatize any part of its economy. 



Ironically, because so much property in Bosnia still is government owned, NATO troops may be paying as much as $40 million a year to rent deployment and storage space from government‐​owned companies in Bosnia. That money is then pocketed by the nationalist party that happens to exercise control over the local or regional government and its institutions. What is puzzling about these payments is the obvious contradiction. NATO allies effectively are subsidizing the very nationalist political parties that Western officials consider the principal obstacles to peace in Bosnia. As the top Western diplomat in charge of implementing the Dayton agreement, Carlos Westendorp has asked, “How can they pay money to these people when we are supposed to be here promoting democracy?” 



A more prudent and viable U.S. policy now would be to convene a “Dayton II” conference that recognizes the reality that has existed on the ground since 1995: a three‐​way partition. That would allow Bosnian Croats, Muslims and Serbs to escape the current atmosphere of perpetual political confrontation and nationalist rancor and concentrate on rebuilding normal lives. The conference could be organized by the European Union, the Organization for Security and Cooperation in Europe, or the Dayton agreement’s Peace Implementation Council to work out the details of formalizing Bosnia’s divisions and to update Dayton’s arms‐​control, demilitarization and human‐​rights provisions accordingly. 



On the military side, arrangements could be made to replace NATO’s current 32,000-strong Stabilization Force with a European Force, or EFOR, to oversee the transition. The EFOR operation could be conducted with Western European Union troops with, perhaps, a prominent eventual role for the Southeast European Brigade, a new regional security initiative being developed by seven NATO and non‐​NATO countries in or near the Balkans. With a few exceptions — such as providing logistics support, cargo airlift and sealift, and space‐​based communications and intelligence — U.S. forces could be extracted from Bosnia before the Dayton agreement’s fourth anniversary in December 1999. 



No doubt critics will point out that this would allow separatism to prevail over multicultural cosmopolitanism. Ideally, the people of Bosnia should enjoy equal rights regardless of religion or ethnic background, and it is tragic that they have refused to uphold that principle. But a multiethnic Bosnia prospering in a climate of liberal toleration is not a realistic expectation; there is simply too much enmity and suspicion on all sides. Sometimes even an ugly divorce is preferable to preserving a futile and destructive marriage — especially when the union is forced. Most important, a negotiated partition is the last, best chance to create a relatively stable environment that will allow for the timely departure of U.S. forces from Bosnia.
"
"
Share this...FacebookTwitter25 Years Ago
Challenger 1986
The story of NASA’s (2nd) worst disaster. There was a “consensus” to launch.
 




<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->







They will never be forgotten.
Share this...FacebookTwitter "
"From the moment world leaders claiming to want to fight climate change arrived in private jets, the 2019 World Economic Forum in Davos attracted controversy. With global inequality growing and the threat of environmental destruction looming ever larger, the jets are getting larger and more expensive. The director of one private charter company says surging demand for his planes is partly down to “business rivals not wanting to be seen to be outdone by one another”. This contradiction between rhetoric and action goes far beyond the use of private aircraft. It reveals the broader problems of allowing billionaires to not only control a disproportionate amount of the world’s wealth but also shape its political and economic agenda. While recognising the need for change, their solutions will almost always lead to a defence of the status quo from which they profit so handsomely. Davos is just the tip of the iceberg of the more pervasive problem of “charitable billionaires”. On the surface, it would appear that having “the 1%” direct their wealth toward worthy causes is both laudatory and necessary. But such philanthrocapitalism promotes market-friendly policies and devalues the ability of democratic governments to provide social welfare or meet the needs of their citizens. As one critic noted:  the problems capitalist philanthropists claim to be solving are rooted in the same economic system that allows them to generate such enormous wealth in the first place. An obvious answer may then be to simply dismiss the World Economic Forum and Davos altogether. Stopgap solutions such as seeking to address economic insecurity through employee “mindfulness and well-being” could be mocked, at best.  However, the ethos of Davos still has much to offer. It represents an effort to bring together global leaders and influencers to address problems that go beyond national borders. If anything, it would appear that we need more of these trans-national spaces for discussing technological innovation and social transformation. For Davos to truly be effective though, it must stop serving as a space for elites to defend their status and power. It must no longer be a forum, for instance, for CEOs like Tim Cook of Apple to defend his company and his executive allies from the legitimate “big tech backlash”. Rather, it should be an opportunity to force billionaires to invest in ambitious progressive solutions for solving the very problems they are primarily responsible for causing. Climate change is a crucial place to start. At the moment, the most that pro-environmental voices such as New Zealand PM Jacinda Ardern can do is plead with elites to “get on the right side of history and embrace ‘guardianship’ of the earth”. Alternatively, Davos could be used to promote a more ambitious green agenda and directly challenge the power of the billionaire class and the politicians who continue to prop them up. Leading up to Davos, US congresswoman Alexandria Ocasio-Cortez made headlines by calling for a “Green New Deal”. It proposes to have the US become a global leader in  “decarbonising” its economy within ten years. Further, it will achieve these aims through massive public investment, a federal jobs guarantee, and a dramatic increase in the taxing of high income. The growing public support for this plan has also been a platform for progressive lawmakers to question the huge US military budget and tax cuts. Why, they ask, are war and wealth being prioritised over the housing, healthcare or the environment? This is a good beginning. Yet it would benefit from having the support of global movements and the capital of global elites. In an alternative world, Davos could be just such a venue for an international commitment to innovation and progressive change. Rather than just having countries fight to tax their richest citizens and corporations, they could also mandate that they have to invest a percentage of their profits in the ideas and agenda proposed by leading experts and activists around the world every year. In this case, it would be for helping to fund countries and communities to put in place a “green new deal”. The roots of such an alternative are already growing in events like the “World Social Forum” which is an attempt to bring together community and political leaders with leading thinkers to imagine a different “world” to the corporate-friendly one supported by the World Economic Forum. Critically, it advocates “glocal” solutions, customised to local conditions. Davos, in this regard, could be a yearly corrective where those most affected by elite policies could put forward the specific solutions for them to rectify it. For this to happen it would mean transforming the very ethos of Davos from one of “idea sharing” to that of democratic accountability and justice. It entails delinking social value and influence from economic wealth and the political influence it buys. Instead, Davos could be an annual opportunity for experts and “the people” to propose the best and most cutting edge ideas for redistributing this wealth to where it is truly needed and can do the most good. Scientists warn that we have only 12 years potentially left to avert a “climate change catastrophe”. It is why we need to create an alternative Davos “for the many, not the few” as soon as possible."
"

When we began drafting this study of U.S. military spending and force posture, we had no way of knowing the tremendous challenge that COVID-19 would pose. It has wreaked havoc on the economy. It has disrupted every facet of American life. The impact will reverberate for generations. The global pandemic—and the U.S. government’s response to it—has threatened the lives and liberties of Americans as well as the United States’ standing in the world.



This disaster is a call to action. The threat posed by nontraditional security challenges, including pandemics, climate change, and malicious disinformation, should prompt a thoroughgoing reexamination of the strategies, tactics, and tools needed to keep the United States safe and prosperous.



As of this writing in late April 2020, and well before the full impact of COVID-19 is known, it seems obvious to us that the United States can no longer justify spending massive amounts of money on quickly outdated and vulnerable weapons systems, equipment that is mostly geared to fight an enemy that might never materialize. Meanwhile, the clearest threats to public safety and political stability in the United States are very much evident and all around us. Just how demonstrations of force or foreign stability operations contribute to U.S. national security is particularly questionable at a time when a microscopic enemy has brought the entire world to a standstill.



This analysis mostly examines where the U.S. military was as of December 31, 2019, with a few observations from early 2020. Where it will be on December 31, 2020, will be guided by a critical set of questions. The authors, and the entire team of scholars in the Cato Institute’s Defense and Foreign Policy Studies Department, intend to help frame those questions—and to answer as many as possible—over the coming year.



Security politics will be different in the future, but the goal of security policy hasn’t changed and is clearly outlined in this report: to identify the most effective and efficient means for advancing Americans’ safety and prosperity. That entails ending the forever wars, terminating needless military spending, rethinking the fundamentals of strategic deterrence, and focusing the entire defense establishment on innovation and adaptation.



 **Online Policy Forum | June 4, 2020 1:00 PM to 3:00 PM EDT**



Even before COVID-19, military spending was unsustainable. In a new Cato report, scholars argue that nontraditional security challenges require a thorough reexamination of national defense strategy. The report prioritizes ending the forever wars, terminating needless military spending, rethinking the fundamentals of strategic deterrence, and focusing the entire defense establishment on innovation and adaptation.



Budgetary and strategic inertia has impeded the development of a U.S. military best suited to deal with future challenges. Over the past several decades, the military has repeatedly answered the call to arms as American foreign policy privileges the use of force over other instruments of power and influence. The era of near endless war has now stretched into its third decade. Going forward, Washington should realign national security objectives and motivate allies and partners to become more capable as America’s relative military advantage wanes and the focus inevitably turns to domestic priorities, including public health.



As policymakers transition from primacy and unilateral military dominance, and beyond the post‐​9/​11 wars in the greater Middle East, the force must also be reoriented. The defense establishment’s most urgent requirement is prioritization. The nation’s resource constraints are real, and hard choices cannot be postponed. In particular, all military branches should emphasize innovation over the preservation of legacy systems and practices. This will require cooperation from Congress, which must address the budget pathologies that stifle new thinking and keep the Pentagon locked into old ways of doing business. Senior defense officials must orient the future force around a different approach to power projection, one less dependent on permanent forward bases, and toward a renewed focus on the requirements for strategic deterrence. The services must also think anew about how to best capture and use information.



Despite recent challenges and setbacks, most importantly the COVID-19 outbreak and response, the United States still enjoys many advantages, including a dynamic economy, political stability, and favorable geography. Securing the United States from future threats should sustain and build on those advantages. Restraining the impulse to use force, imposing limits on military spending, and relying more heavily on diplomacy, trade, and cultural exchange would relieve the burdens on our overstressed military. The ultimate objective should be to build an agile and adaptable military that can address a range of future challenges but is used more judiciously in the service of vital U.S. interests and to deter attacks against the homeland.



Building a modern military requires a clear conceptualization of the realities of international conflict and tight alignment with a country’s foreign policy. Strategic planners must have a clear‐​eyed view of both the threats facing the country and the tools necessary to defend its vital interests. Planners in the United States should take account of the country’s fortunate circumstances, including its geography, dynamic economy, and political stability, and recognize that maintaining these advantages does not require a massive military apparatus that is constantly active in nearly every part of the world.



For decades, however, U.S. national security policy has been oriented around a military‐​centric approach, variously called primacy, liberal hegemony, or deep engagement. Primacy is based on the idea that U.S. military power explains the absence of a major‐​power war since the end of World War II and the attendant rise in productivity and living standards. Harvard political scientist Samuel Huntington predicted in 1993, for example, that “a world without U.S. primacy will be a world with more violence and disorder and less democracy and economic growth.”1 Former secretary of state George Shultz put it even more succinctly in the 2016 documentary _American Umpire_ : “If the United States steps back from the historic role [it has] played since World War II, the world will come apart at the seams.”2



Such sentiments reflect why, despite the fact that the United States enjoys relative safety, U.S. officials see only grave and urgent dangers. They see any challenge to U.S. military dominance as a threat to global liberty and peace. The 2018 _National Defense Strategy_ (NDS), for example, notes that the “central challenge to U.S. prosperity and security is _the reemergence of long‐​term, strategic competition_ by … revisionist powers.” The goal then, according to the NDS, is to “remain the preeminent military power in the world.”3 The 2017 _National Security Strategy_ (NSS) goes further, noting that the “United States must retain overmatch—the combination of capabilities in sufficient scale to prevent enemy success and to ensure America’s sons and daughters will never be in a fair fight.”4



And while the United States is purportedly orienting around great power competition against China and Russia, the post‐​9/​11 conflicts grind on. The National Defense Authorization Act (NDAA) for Fiscal Year 2020 makes clear that the Pentagon envisions those conflicts continuing indefinitely.5 Today’s U.S. military budget, after adjusting for inflation, vastly exceeds that of the Cold War and now approaches levels during the height of the wars in Iraq and Afghanistan in the early 2010s (see Figure 1). Operationally, the Pentagon has been bogged down in Afghanistan and caught in the ongoing struggle between Saudi Arabia and Iran for dominance in the Persian Gulf region and beyond; in December 2019, the Trump administration was considering sending an additional 14,000 troops to the Middle East, including a substantial ground presence in Saudi Arabia, for the first time in nearly 17 years.6



Perceptions of looming threats or fear of potential peer competitors should not distract from the obvious need to take a strategic pause and reconsider the United States’ core defensive needs, especially during a global pandemic and associated economic disaster.7 Washington should realign its national security ends and means to better match the emerging geopolitical reality—especially America’s waning relative military power.8 The desire for one‐​sided “overmatch” is understandable but impractical given the extensive commitments it entails. The time is ripe to make a clean break from the past.



The dramatic shock of COVID-19 adds urgency to the need for new strategic priorities. This report acknowledges that the nation’s resource constraints are real and that the United States faces a period of grave economic uncertainty. The Pentagon is not immune to these pressures. Politicians are unlikely to undertake a concerted campaign to build public support for massive increases in taxes or deep cuts to popular domestic programs in order to fund a military that an ambitious grand strategy calls for, and they would likely fail if they tried. The U.S. military is spending beyond its means due mostly to inertia and strategic indecision. To that end, this report is founded on three pillars: articulating a force that meets the _realities of the geopolitical situation_ and contemplating the current budget pathologies that impede change; reexamining _force construction_ ; and evaluating the posture needed for _modern strategic deterrence_. These pillars drive the recommendations contained herein with an aim toward developing a more realistic and prudent military budget.



The Trump administration’s budget proposal for fiscal year (FY) 2021 aims for “U.S. military dominance in all warfighting domains—air, land, seas, space, and cyberspace,” echoing its FY 20 budget proposal, which supported “dominance across all domains.”9 This view is consistent with the desire for “overmatch” in the NSS. Overmatch requires the United States to “restore our ability to produce innovative capabilities, restore the readiness of our forces for major war, and grow the size of the force so that it is capable of operating at a sufficient scale and for ample duration to win across a range of scenarios.”10



This quest for global dominance is taking place as the United States’ capacity for sustaining supremacy is waning. The NDS observes, for example, that “we are emerging from a period of strategic atrophy, aware that our competitive military advantage has been eroding. We are facing increasing global disorder, characterized by decline in the long‐​standing rules‐​based international order—creating a security environment more complex and volatile than any we have experienced in recent memory.”11



The NDS does not treat this diagnosis as a recognition of the limits of American military power but rather as a rallying cry to marshal additional national resources and maintain the globe‐​spanning posture to which Washington has grown accustomed. Raging against the dying light of uncontested military primacy will run into severe budgetary and strategic obstacles.



The international order faces many challenges, and these cannot be reversed by attempting to restore U.S. dominance across all domains and in all regions. Instead, U.S. grand strategy should encourage allies and partners—the leading beneficiaries of global peace and stability—to take a greater role in sustaining it. The United States cannot be the world’s police force or coast guard.



The United States needs a prudent military strategy that can protect U.S. interests without turning into an open‐​ended pursuit of anachronistic, grand goals. “Overmatch” extending across all regions, domains, and weapons systems is simply untenable. The “America First” view of primacy focuses on military hardware and manpower, not the elements of smart power that have traditionally been the real sources of American strength and influence.12 Simply put, the United States today is overinvested in the military. As a recent Cato book explains, “a less expansive foreign policy agenda will allow the United States to reduce military spending significantly.”13 Washington should take advantage of the current period of relative geopolitical stability to adopt a military posture consistent with grand strategic restraint.14 Such a reorganization would bring much‐​needed coherence to U.S. military strategy.



The recommendations in this report are not driven by perceptions of waste and bloat within the U.S. defense establishment—though there is certainly much of that. Rather, the authors assess international politics today as well as the probable nature of future threats and fix on what is required to defend the U.S. homeland and vital interests.



The current approach relies heavily on the use of force and coercion at the expense of other instruments of power and influence. A military‐​centric strategy seems particularly ill‐​suited to a post–COVID-19 world.15 The primary tools of American global engagement under a grand strategy of restraint should be trade, diplomacy, and cultural exchange. The military instrument, while still vital, should be geared toward defense, in the strictest sense of the word, enabling allies and partners to counter adversaries. A grand strategy of restraint would leverage innovation and modernization to refocus on a narrower range of future challenges and to rethink how strategic deterrence could better serve the needs of the nation.



Adopting a new grand strategy, and fashioning a new force posture to suit, also requires a reconsideration of the value of forward deployment. The United States should reduce its permanent overseas presence, especially in forward‐​operating bases that will be vulnerable if conflict erupts. Under a strategy of restraint, the U.S. Navy and Air Force would be a surge force capable of deploying to crisis zones if local actors prove incapable of addressing threats.



The United States can support allies and prepare for future combat by enabling others to defend themselves and their interests. U.S. force planning should be oriented around how the U.S. military can contribute to such operations from a distance as U.S. interests dictate. In those rare instances where vital national interests necessitate the deployment of U.S. personnel well outside of the Western Hemisphere, Pentagon planners must ensure adequate facilities and resources to resupply their operations. Relying on forward‐​deployed forces as we currently do risks inadvertently creating a security dilemma that encourages prospective rivals to match such deployments. By focusing on modernization and interoperability, U.S. forces could assist others while reducing the risk of escalation. Equally important, an over‐​the‐​horizon posture would reduce demands on the U.S. military—especially on active‐​duty personnel.



A grand strategy of restraint calls for a less active conventional military, one that is not deployed in permanent bases or routinely engaged in offensive operations on multiple continents. Even so, restraint is not synonymous with disarmament; the United States will continue to rely on nuclear weapons to deter some strategic attacks. However, the current concept of “strategic deterrence” and the role of nuclear weapons in U.S. defense strategy would have to change. The main problem with Washington’s approach to strategic deterrence—as with U.S. military strategy in general—is that it suffers from mission creep.



At its core, “strategic deterrence” is preventing a first use of nuclear weapons against the U.S. homeland or an ally. But that is not the only behavior that U.S. officials currently seek to deter. The 2018 _Nuclear Posture Review_ (NPR), for example, says that the United States would consider using nuclear weapons to respond to “significant non‐​nuclear strategic attacks” against U.S. or allied civilians, infrastructure, and early warning capabilities.16



An overly broad definition of strategic threats drives demands for a large and diversified nuclear arsenal and missile defense capability in order to have many flexible response options.17 As a result, Washington’s approach to strategic deterrence places great weight on adversary capabilities. For example, according to the 2018 NPR, “Moscow’s perception that its greater number and variety of non‐​strategic nuclear systems provide a coercive advantage in crises at lower levels of conflict.”18 This is the supposed justification for new, low‐​yield U.S. nuclear weapons.19 Similarly, the _2019 Missile Defense Review_ cites the threat of hypersonic glide vehicles—high-speed maneuvering warheads that take an unpredictable rather than ballistic route to their target and that China and Russia are developing as a response to U.S. missile defense expansion—as a rationale for deploying more missile defense sensors on satellites.20



Having a flexible nuclear arsenal and missile defense system that can be tailored to respond to the unique characteristics of different threats sounds sensible. However, the failure to prioritize produces a kind of paralysis. In a world where dangers loom around every corner, doing anything less than deterring all of them at once is considered a failure. This encourages wasteful spending and invites potential adversaries to create counter strategies that increase the likelihood of inadvertent nuclear escalation. Such moves damage deterrence instead of strengthening it. Deterrence under restraint would have a narrower set of objectives and clearer priorities and would privilege clarity and reliability over flexibility.



If the United States would prefer to engage adversaries at a distance, strategists need to rethink how the future force should be organized. Improving the ability of the different services to communicate with one another and have smoothly functioning command and control during a conflict is especially critical.21 A traditional focus on raw firepower and the impulse to base personnel and equipment at great distances from the United States will likely need to give way to an emphasis on developing a technologically proficient force that relies on new layers of sensors (radar, sonar, etc.) that can direct long‐​range attacks and control unmanned vehicles at greater distances. Another overlooked capability in debates over the defense budget is the redundancy of reconnaissance systems—the ability of America’s intelligence‐​gathering satellites and aircraft to perform their functions if they are disrupted.



The current approach of massive investment in the military, displays of force, and direct challenges to multiple adversaries in their respective regions is often counterproductive. As Sen. Angus King (I-ME) notes, with respect to Iran “the unanswered question is who is provoking whom. As we escalate sending more troops, moving aircraft carriers, we view it as preventative and as defensive. They view it as provocative and leading up to a preemptive attack.”22 Michael O’Hanlon of the Brookings Institution made a similar point in 2017. Arguing for a new approach to European security, O’Hanlon explained that the United States “may be able to help ratchet down the risks of NATO‐​Russia war … by recognizing that NATO expansion, for all its past accomplishments, has gone far enough.”23 U.S. messaging must be consistent, and budgetary maneuvers should not introduce justifications for war. The overarching recommendation here is to halt policies that exacerbate regional security dilemmas and to restructure U.S. military power accordingly. Such a restructuring is made more difficult, however, by the rigidity of the budgeting process.



Spending patterns driven by inertia and habit privilege the military, the use of force, and coercion over diplomacy and other instruments of American power. Accordingly, the Pentagon’s budget continues to reflect strategic errors of the past, including searching for a peer competitor, continuing support for a counterproductive war on terror, and propping up dangerous and unreliable strategic partners. To complicate matters, Congress and the White House are sparring over new distractions, including potentially diverting funds from the military budget for border wall construction24 We refer to these distractions as “budget pathologies”: abnormalities and malfunctions inherent in how the U.S. government secures funding for the military, a process that often impedes the creation of a viable national strategy. The executive branch initiates many of these pathologies, but Congress also plays a key enabling role by not exercising its traditional power of the purse.



On December 9, 2019, for example, a House and Senate conference committee passed the FY 20 NDAA. The bill authorized $738 billion for national defense spending, and President Trump proudly signed it into law.25 The U.S. government continues to spend and act as if its wars in the Middle East will never end. Secretary of Defense Mark Esper described such operations as not “necessarily unusual” and noted that “we continue … ‘to mow the lawn.’ And that means, every now and then, you have to do these things to stay on top of [the threat].”26 In fact, these operations represent sunk costs and reflect misguided assumptions about what actually makes Americans safe and prosperous.



The U.S. military budgeting process is supposed to reflect a delicate balance between executive‐​level strategic guidance, Department of Defense (DOD) budget requests related to the overarching strategy, and legislative approval and appropriations to fund the requests. The actual process of funding the nation’s military, however, bears no resemblance to that ideal. A recent, clear sign of just how badly this process has broken down was revealed when the Trump administration tried to strip $3.6 billion from existing Pentagon projects to fund improvements for physical barriers at the U.S.-Mexico border. The funds earmarked to be ripped away would have served to upgrade and maintain the surface fleet, improve basic services on military bases, and expand the nation’s offensive cyber capabilities.27



This is just one example. There are many pathologies—spending decisions that serve partisan or parochial interests but do not advance U.S. security—that consistently undermine the entire federal budget, not merely what winds up in the Pentagon’s coffers. The most serious problem pertains to the unwillingness of American elected officials to reconcile spending and revenue. Despite the occasional attempt to reverse the tide, nothing has had lasting success. When Congress passed the Budget Control Act in 2011, the annual budget deficit stood at $1.3 trillion. Four years later the annual deficit fell to $438 billion. However, this figure has risen each subsequent year, exceeding $984 billion at the end of FY 19—the highest since 2012.28



Very few Americans appreciate the scale of the federal government’s spending. A poll taken in early 2017 found that only 1 in 10 Americans could correctly identify the amount spent on the military within a range of $250 billion.29 And yet, according to a 2019 Gallup survey, only 1 in 4 Americans believe that U.S. military spending should increase at all, while a slightly higher percentage (29 percent) thinks the United States is spending too much.30



The main budgetary problem for the Pentagon, therefore, is political. It refuses to budget based on what is possible and realistic and instead spends to satisfy perceptions of need (often indistinguishable from desires) with too little consideration of constraints and tradeoffs. While most Americans want a military that is prepared to prevail in combat, we all must take account of the resources available to make that a reality, both now and into the future. 



Beyond this overarching problem of ends misaligned to means, the Pentagon budgeting process is afflicted by two other related pathologies: overseas contingency operations funds and reprogramming. Both allow the government to spend without consequence and fail to distinguish between needs and wants.



Supplemental appropriations to pay for wars are not a novel idea. In fact, the first was passed in 1818. Historically, however, legislators moved such “emergency” spending (today known as “nonbase nonrecurring” or “contingency” funding) for unforeseen operations back to the base military budget within a few years once leaders had a clearer idea of operational needs.31



The DOD has received $2 trillion in overseas contingency operations (OCO) funding since September 11, 2001.32 In December 2019, Congress appropriated $71.5 billion for the OCO budget in FY 20.33 To put these numbers in perspective, in 2020, if OCO were its own government agency, it would have the fourth largest budget in terms of discretionary spending.34 The use of OCO funding for almost two decades following 9/11 has systematically undermined the established appropriations process. Supplemental appropriations fund activities unrelated to the wars but are not counted as part of the base DOD budget. In other words, reliance on OCO funding lets the military services avoid setting priorities that should guide long‐​term strategy and makes it too easy to undertake present‐​day combat operations without formal legislative consent and funding. Other departments and agencies have also gotten into the habit; even the U.S. Agency for International Development and the Department of State now rely on OCO funding to supplement their base budgets.35 Aside from its blatant dishonesty, OCO represents a larger pattern of runaway U.S. government spending and especially the legislative branch’s tendency to avoid oversight of either Pentagon spending or the nation’s perpetual conflicts. 



The other factor fueling the abuse of OCO funding is the Budget Control Act (BCA) of 2011. That legislation set limits on discretionary budget authority from 2012 through 2021 to slow the growth of public debt after the 2008 financial crisis.36 The spending limits are supposed to be enforced through what is commonly called sequestration. Under sequestration, any appropriations that go above set funding levels—or “caps”—are canceled.37 However, funding designated for OCO, ostensibly for counterterrorism efforts, including the wars in Afghanistan, Iraq, and Syria, is exempt from BCA caps and separate from the Pentagon’s base budget (hence “nonbase”).38 In other words, executive branch officials and legislators have a massive loophole for expanding military spending while seeming to abide by discretionary spending limits. The BCA’s OCO exemption allows elected officials to feign concern about out‐​of‐​control federal spending without doing anything to stop it.



Force development and planning requires funding that is on‐​time, stable, and proportional to the scope of military operations. Using OCO to skirt the BCA’s budget caps does not reflect a military establishment that can prioritize according to coherent long‐​term strategies. This technique has allowed civilian leaders to evade tough choices, including how to resolve ongoing conflicts and whether to enter new ones. While the war on terror presents unique challenges, using OCO only helps perpetuate the cycle of U.S. involvement in never‐​ending conflicts.



With BCA caps officially expiring in 2021, policymakers may be less tempted to rely on OCO funding. However, moving OCO back into the base budget inevitably raises concern about overall spending increasing at an unreasonable rate. Congress should move enduring costs back to the base budget without increasing topline military spending. Presenting the DOD with less budget flexibility should spur more creativity and budget management, not less, while still allowing the military to rely on supplemental funding for truly dire, unforeseen overseas expenses. When such emergencies arise, Congress can authorize additional funds as necessary.



Agencies often reprogram funds to deal with unforeseen challenges, but it is technically illegal to spend taxpayer dollars in ways not explicitly authorized by Congress.39 However, “as there are no government‐​wide reprogramming rules,” note Georgetown University researchers Michelle Mrdeza and Kenneth Gold, “prohibitions against reprogramming funds within an appropriations account … vary among agencies and appropriations subcommittees.”40 The Government Accountability Office (GAO) concurs. Agencies have the “implicit” authority to shift funds within a department or agency as long as the intended use of the funds remains broadly within the same goal.41 Regulations governing DOD require congressional approval for any funds reprogrammed over 20 percent or $20 million over the original allocation.



Reprogramming has become a national security issue as the executive branch seeks ways to seize control of the budget from Congress. The Trump administration’s threat to use funds allocated for other purposes to build the border wall, for example, contributed to the longest government shutdown in history in winter 2018–2019.42 Although the DOD continued operations, the budget impasse adversely affected many contractors, researchers, and production line managers.43



Past NDAAs restricted reprogramming funds for priorities that Congress expressly declined to fund, but the FY 20 NDAA did not include such language. A loophole in U.S. law allows for unassigned military construction funds to be used for construction projects during periods of national emergency.44 Other legislative language allows the secretary of defense to provide support for counterdrug activities to other departments and agencies.45 These two provisions provide the leeway to reprogram a significant amount of funding. Yet, the president can declare almost anything a “national emergency” at will.46



Thus, the moves to reprogram funds defy Congress’s traditional power of the purse and allow federal agencies to use money from the DOD budget to support domestic political initiatives. Such efforts create a dangerous precedent, both in undermining constitutional checks and balances and potentially limiting the funds vital to the nation’s defense.



These budgetary pathologies insulate the U.S. military from resource constraints, allowing it to proceed mostly by inertia. But the U.S. military also remains mired in the post‐​9/​11 Global War on Terror. Nearly two decades of continuous operations have put enormous strain on the force. The military branches continue to lower eligibility requirements to meet their recruitment goals and have increased retention bonuses to discourage services members from leaving.47



More ominous developments include rising suicide rates among veterans and active‐​duty service members, an increase in reported sexual assaults, and the need for expanded counseling to deal with post‐​traumatic stress disorder and other psychological challenges.48 In short, the well‐​being of U.S. service members is a pressing national concern.



The force of the future is likely to be smaller, particularly in terms of numbers of personnel in uniform, and thus will need to be more adaptable. That, in turn, will require increasing the academic aptitude and physical fitness standards for recruits.49 A focus on improving the force—as opposed to simply growing it—through retention programs for critical staff and expanded educational and retraining opportunities is key to creating a healthy and socially viable military. This should be a DOD‐​wide imperative.



Beyond recruitment and retention, each service branch confronts its own unique challenges. Pentagon officials must reconceptualize how the U.S. military plans to fight. The wars of the recent past, against chiefly nonstate actors in the greater Middle East, South Asia, and Sub‐​Saharan Africa, are unlikely to be an adequate guide for future conflicts.



In particular, the potential for direct engagement with technologically capable adversaries in contested environments means that the era of U.S. dominance can no longer be assumed. Within that framework, the following sections outline a few key choices that service leaders need to make.



There are two clear challenges for the joint forces of the United States: standardizing a system for operations across multiple domains (e.g., land, sea, air, space, and cyber) and pushing innovation. Addressing the first challenge demands that every branch of the U.S. military agrees to a joint, all‐​domain command and control (C2) system. As combat systems and advanced artificial intelligence (AI) platforms continue to develop, they must be seamlessly integrated within and between all U.S. forces. Currently, however, each military branch is pursuing its own C2 design. For example, the Navy has the Naval Integrated Fire Control‐​Counter Air system, and the Air Force has the Advanced Battle Management System.50 This duplicates effort, wastes funds, and impedes unifying C2.51



Defense contractors and other interested parties will lobby for their respective systems, but the choice should be based on the ability to implement the system across all services, agreement among the branches, and a clear standard for cybersecurity. Because a standard C2 platform is the optimal solution for the modern battlefield, all U.S. forces should streamline and upgrade to ensure that they meet the new compatibility standards. The U.S. military should not move forward with designing protections for these networks, and redundancy for forward C2 deployment, without first establishing a joint system. It is premature to estimate the eventual cost of such a unified system, but deciding on this system now will inevitably save money by facilitating coordination between every branch of the U.S. military.



The DOD, for its part, must decide on a platform, take bids on delivery of the system, and obtain executive branch and congressional approval on a process and timeline for implementation. Congress should use its legislative authority to ensure compliance through reporting requirements.52



The second overarching challenge for the U.S. military is the need to prioritize innovation. This entails empowering individuals at all levels to bring forward new ideas and establishing a process to deliver design options through a full development cycle in the most expeditious and cost‐​effective ways. Service members have a critical role to play in determining future priorities since these systems and platforms will have a direct impact on their daily lives and their ability to function on the battlefield.



Following the Army’s example, each branch of the military should develop its own Futures Command to push for branch‐​wide innovation. The mission of a Futures Command is modernization. It does away with old “industrial age” approaches, which are mostly piecemeal and often slowed by bureaucracy, and puts them all under one roof with a set of defined goals. If each branch has its own innovation command center, the Pentagon would be well‐​positioned to coordinate across branches. A futures reserve unit in each branch would prove critical given the recent effort to recruit and fund PhDs in the military and DOD.53 Members of the armed forces with advanced degrees could then naturally transition into the reserve system to support innovation.



The service branches should also develop practices for curating the massive amounts of data generated for AI systems. Given the high probability that this technology will be critical to future fights, branches should use “data wranglers”—individuals whose primary task is to collect information that can be plugged into various systems.54 There is currently no method to identify U.S. service members able to work with data, generate statistical analysis, and assure the accuracy of data.



In addition, Kessel Run‐​type programs in each branch could be successful for fostering innovation, as it has been for the Air Force. In the _Star Wars_ universe, the Kessel Run refers to an impossible task that is completed in a short time. The Air Force had that in mind when it set out to develop software quickly and in response to an uncertain environment.55 The inability to negotiate contracts with external parties who will build software or hardware in a timely and efficient manner are typically the main impediments to developing innovative programs in the military.



U.S. defense planners should consider what the nation’s defense needs will be in the future, but too often their efforts are stymied by inertia or shortsighted demands that defense programs serve domestic political and economic interests. The United States should be investing in innovation and research rather than stale production lines for weapons that have outlived their usefulness or new weapons that can never meet their design objectives.56 Developing weapons platforms should be based on the needs of the future military, not short‐​term concerns, such as the parochial interests of defense‐​industry workers or the politicians who shield them. The U.S. military must abandon weapons platforms that cost too much to maintain and retrofit and have limited or no value in future conflicts.



Future increases to the DOD’s research and development (R&D) budget should be funded by reducing spending on outdated weapons systems. As part of a renewed push for R&D, the U.S. government should revisit its approach to basic research funding. Instead of bolstering the National Science Foundation and encouraging scholars to seek trivial connections to national security in research projects, the DOD should be granted additional authority to invest in other public and private research startups and incubators through the individual service research offices (e.g., the Office of Naval Research). These funds should not be restricted and should be open to every research university and think tank capable of doing advanced research that will help drive innovation within the defense ecosystem.



This is not an argument for expanding federal funding for research but rather extending existing research opportunities to a much wider pool of qualified institutions. For too long the U.S. government has steered research funding to federally funded R&D centers. This has driven up R&D costs while failing to integrate the talent and ingenuity of research institutions outside traditional networks. The United States must leverage its deep technological base to meet coming challenges; as of now the U.S. government’s vision of research and research funding is tied to past processes that have a decidedly mixed record of delivering essential equipment and materials in a timely and cost‐​effective manner.



Research should be focused on applying novel technical capabilities to the modern battlefield. The idea that the United States has fallen behind China in the AI arms race is only true based on a measurement of research quantity, not quality. And such claims do not take full account of the vast array of innovative enterprises in the United States, most of which are completely outside the federal government’s control or purview.



For example, Google recently published a paper demonstrating quantum supremacy, when a quantum device (such as a quantum computer) can solve a problem that no traditional computer realistically can.57 This represents a leap over classical computing power by orders of magnitude, but U.S. defense planners must think about how to employ these tools in combat. AI is only as useful as the data fed into the algorithms.58 Moving forward with a clear vision of how the U.S. military can leverage AI and quantum power, therefore, requires investments in basic data science education, data assurance and retention, and data integrity.



These proposals are generally cost‐​neutral as they entail reorganization of existing lines of effort. Ensuring that the U.S. military develops multidomain battle systems without redundancy, establishing a clear process for managing data on the battlefield, and putting platform development in the hands of the individual soldier, sailor, airman, and Marine are all clear needs as relevant as massive outlays for modern weapons platforms. Reorganizing around Futures Command groups and using data wranglers would enable all service branches to innovate as the United States still enjoys a number of political, economic, and strategic advantages relative to prospective rivals.



Since its formal inception in 1947, the Air Force has fended off challenges to its place in the structure of the U.S. military, and a few respected scholars still call for its abolition.59 Many critics, however, aim to fix apparent inefficiencies within the force rather than doing away with it. A recent Center for Strategic and International Studies report, for example, notes that while spending on the Air Force has reached new heights, its force capabilities—as measured by the number of aircraft in its inventory—have fallen to an all‐​time low.60 This is partly explained by the overall focus on quality over quantity but is also due to the fact that the Air Force is more than just planes, just as the Army is more than the infantry and the Navy is more than surface ships. Still, the Air Force has struggled to introduce new aircraft. The service’s experience with the F-35 Lightning II aircraft, a fifth‐​generation fighter jet that is significantly more advanced than its predecessors and supposed to replace several other aircraft currently in service, has not been promising. In general, the Air Force has spent a lot of money to get less capacity.



A change of direction is in order. The structure and capabilities of the Air Force should maximize operational readiness, taking into consideration procurement difficulties associated with current weapons systems still under production.61 The bitter experience with the F-35, which will be delivered to the force nearly a decade late and at an inflation‐​adjusted cost well above original estimates, is only one sign of the overall challenge facing the Air Force.62 The service needs capable aircraft at a cost that will allow it to purchase them in adequate quantities, and it needs to obtain them in a timely fashion.



Per the objectives spelled out in the 2018 _National Defense Strategy_ (NDS), the U.S. Air Force is tasked with dominating the air, outer space, and cyberspace by using advanced and emerging technology. The Air Force needs to be an innovative service to keep up with the rapid pace of technical change. Specifically, the service should focus on countering China and Russia’s investments in anti‐​access/​area‐​denial systems, including long‐​range surface‐​to‐​air missiles.63



This will be difficult. As previously noted, the Air Force’s rising budgets have coincided with a declining number of active aircraft, along with fewer pilots and Air Force civilian employees.64 Such trends signal broader challenges with basic budgetary management, including the expanding costs of operation and maintenance. In other words, today’s Air Force paradoxically does less while spending more. This is perplexing to say the least.



While the service has emphasized incorporating advanced technology for air and space operations, overall readiness and pilot training have decreased substantially, contributing to a steady rise in aircraft mishaps.65 These operational problems are exacerbated by a shortage of qualified maintenance technicians. According to the GAO, the Air Force does not have a strategy to improve retention. If the Air Force is unable to hold onto its best people, it will struggle to adapt to changing operating environments (including outer space and cyberspace) and new technology (such as AI and quantum computing).66 The Air Force must undertake a service‐​wide initiative to reverse this trend, especially by incentivizing qualified personnel to remain in the force.



With respect to hardware, the Air Force is developing the F-35A, the B-21 Raider long‐​range bomber, and the KC-46A Pegasus tanker aircraft while also seeking to replace current intercontinental ballistic missiles and developing a Space Force, which is still officially under the Air Force’s auspices. That is unsustainable. The service’s goals must be aligned to present and future realities and should take account of the demands of modern combat. As the airspace in which the Air Force operates becomes increasingly crowded and contested, this places a premium on unmanned vehicles that can loiter and are capable of executing strike, surveillance, and resupply missions.



Forward basing poses both operational and doctrinal challenges to air operations because long‐​range precision strikes by an adversary can decimate aircraft and fuel supplies long before U.S. aircraft can engage the target. What good is a force of 100 F‐​35s if they never leave the ground?



A focus for now on drones and a reliance on a revitalized F-15 Eagle aircraft through the F-15EX platform is certainly warranted. The recent move to establish the 16th Air Force, which is focused on cyberspace and electronic warfare, is also a welcome development.67 On the whole, however, the Air Force is trying to do too much, including a focus on space, support for counterterror operations, unmanned reconnaissance, nuclear deterrence, transport, air defense, air‐​to‐​air combat, ballistic missiles, and precision bombing. A strategic pause and reset are desperately needed.



The Army’s strategy, posture, and budget should reflect and adapt to evolving geopolitical circumstances. The U.S. Army posture assessment fails to do that, placing dominance through military overmatch, as outlined in the NDS, at the forefront of the Army’s vision.68 Day‐​to‐​day operations, ongoing conflicts, allied engagement, and crisis response all continue to put unnecessarily high demands on the force. A realistic assessment of threats would allow the Army to prioritize and eliminate or offload unnecessary missions. Enabling and encouraging allies to do more in their respective regions would reduce the Army’s requirements, including especially numbers of active‐​duty personnel.



In 2018, the Army created the Army Futures Command.69 This organization has been critical for pushing the service to modernize. It originally established six priorities:



Of these, long‐​range precision fires (i.e., modern artillery) and networked air and missile defense are critical. The United States should divest from other outdated weapons systems—including, in particular, the Abrams tank—that are unlikely to serve a major purpose on the future battlefield, or at least in the battlefields that are truly critical to U.S. security and prosperity.



Above all, the active‐​duty U.S. Army should be substantially smaller and postured mostly for hemispheric defense. A grand strategy of restraint would eliminate most permanent garrisons on foreign soil and rely more heavily on reservists and National Guard personnel for missions closer to the U.S. homeland. Such a posture would reduce the likelihood that U.S. troops would be drawn into protracted civil conflicts that do not engage core U.S. national security interests. That, in turn, would generate substantial savings over the next decade.



Developing better and modern versions of artillery is another key task for the Army. That would allow the U.S. military to support allies from a distance, when U.S. leaders deem such assistance appropriate, while also ensuring that U.S. troops mostly remain out of harm’s way when such missions are not truly essential for U.S. security.



The development of better unmanned vehicles for long‐​range fires in support of ground operations is also critical. While drones for surveillance and precision strikes are useful, in a future war the United States will need functional unmanned vehicles that can deliver artillery support and fire weapons from a distance, minimizing harm to U.S. forces. Future platforms used to deliver long‐​range fires also need the ability to be undetected despite increased sensors employed by adversaries.



Finally, the Army needs to develop better air and missile defensive platforms to protect forward‐​operating units. These tools would benefit the entire U.S. military, but the greatest gain would go to the Army, whose ability to fight will be challenged by opponents’ long‐​range munitions. The Army needs portable sensors ready to detect incoming fires. A modern military is too vulnerable to long‐​range attack, including from artillery, ballistic missiles, and drones. Real‐​time battlefield awareness is essential, as is the need to defend our allies once the U.S. commits to pulling back from forward deployment. Thinking about this critical function is more important than developing a new helicopter or other vertical lift platform (e.g., tilt‐​rotor aircraft) or a next‐​generation tank. If the U.S. military cannot protect its forces in the field from short‐​range ballistic and cruise missiles, units will not survive long enough to bring these new weapons to bear against the enemy.



To meet current recruitment goals, the Army has waived certain requirements and increased enlistment bonuses.71 If these reforms draw capable people into the service, then they should continue, but careful oversight is needed. An emphasis on quality, rather than quantity, could reduce turnover, ensure new enlistees complete their requisite training, and ultimately improve retention.



A focus on readiness could also help. Service members should know that they have adequate support to complete their missions and be confident that policymakers will not send them to fight open‐​ended wars that are not vital to U.S. national security. A failure to meet those basic requirements has driven qualified personnel from the force. No branch of the U.S. military has reached its readiness goals, however, and the budget priority has since shifted to modernization. While the increase in research, development, testing, and evaluation is an important step in creating a more lethal and agile force, a failure to meet readiness goals will impede force transformation.



The Army needs to rethink the size of the force needed given the effort to modernize overall. At a time when two successive presidential administrations have pledged to draw down operations in the greater Middle East, the United States should refocus on establishing a lean and agile ground force that can retain the best people while allowing the marginal performers to transition out. This process of attrition should be used to reduce the size of the active‐​duty Army by 20 percent over the next decade. Recruiters need to employ what marketers call “microtargeting” to ensure that the U.S. Army has high‐​quality soldiers that can innovate on the battlefield, not just follow orders.72 Eliminating unnecessary forward bases, improving existing facilities, and rethinking education and training would be easier with reductions in the size of the force.



In recent years, the U.S. Navy has operated under the assumption that it can get all that it wants without a clear articulation of what it needs—though the situation may be changing. An October 2019 Congressional Budget Office (CBO) report warned that the Navy “would not be able to afford its 2020 shipbuilding plan.” CBO estimated that the Navy would need $28.8 billion per year for new‐​ship construction, more than double the historical average of $13.8 billion per year (in 2019 dollars).73 This is hardly the first time that CBO has observed the looming gap between the Navy’s plans and fiscal realities.74 Although the sea service has avoided a bitter reckoning, the responsible course would bring its goals in line with its available resources.



In early December 2019, Acting Navy Secretary Thomas Modly publicly reaffirmed his commitment to achieving a 355‐​ship Navy, and he separately issued a memo to the fleet calling for a plan to achieve it by the end of the next decade.75 But more recent evidence suggests that the Trump administration has scaled back its shipbuilding plans and backed away from the 355‐​ship goal. The president’s budget submission for FY 21 actually cut $4.1 billion from shipbuilding.76 Navy leaders acknowledge the tradeoffs between operations and maintenance and money for new construction. “We definitely want to have a bigger Navy, but we definitely don’t want to have a hollow Navy either,” Modly told _Defense News_. “If you are growing the force by 25 to 30 percent, that includes people that have to man them. It requires maintenance. It requires operational costs. And you can’t do that if your top line is basically flat.”77



Many strategy documents simply assume that considerably more money _must_ be made available to the military—and leave it to the politicians to figure out how.78 The Heritage Foundation, for example, calls for a 400‐​ship Navy even as it concedes that such a force “may be difficult to achieve based on current DOD fiscal constraints and the present capacity of the shipbuilding industrial base.”79



The Navy should reject such advice, prioritize among competing desires, and focus on what is genuinely needed to achieve vital national security objectives. In the near term, this means prioritizing current operations. High‐​profile disasters at sea, including the tragic accidents aboard USS _John S. McCain_ and USS _Fitzgerald_ , which claimed 17 sailors’ lives in 2017, raised obvious questions about the state of the surface Navy. A GAO report released two years before the _McCain_ and _Fitzgerald_ incidents concluded that “the high pace of operations the Navy uses for overseas‐​homeported ships limits dedicated training and maintenance periods,” which had “resulted in difficulty keeping crews fully trained and ships maintained.”80



The Navy must expand both its capacities and capabilities. Prioritizing less‐​expensive vessels could make up for certain shortfalls and grow the fleet at a faster rate. Newer platforms would also translate to less maintenance time, further increasing the number of vessels ready for service at any given time. On occasion, the Navy has gone in a different direction, privileging very high‐​end platforms that often take many years to reach the fleet. In the interim, this leaves more older ships in service longer, along with their additional repair and maintenance costs.



The Navy has made recapitalizing the ballistic missile submarine (SSBN) fleet—the _Columbia_ -class SSBNs that will replace the _Ohio_ -class—its top shipbuilding priority. The tradeoffs are most apparent with respect to fast‐​attack submarines (SSNs).81 Although these vessels are unsuited to perform many routine Navy missions—including escort operations and visible presence—they are critical and should be maintained in some quantity.



Other hard choices cannot simply be imagined away. This report focuses on two key acquisitions programs to highlight tradeoffs within the surface fleet: the _Gerald R. Ford_ -class aircraft carrier (CVN) and the new guided‐​missile frigate FFG(X).82



As designed, _Ford_ -class ships are the largest and most capable warships on the planet. But little else about the ships—including whether their actual performance matches their designed capabilities or when the ships will attain full operability—can be predicted with any confidence. Former Navy Secretary Richard Spencer staked his reputation on ensuring that the advanced weapons elevators—large lifts that transport bombs and missiles from inside the ship to the flight deck—aboard USS _Gerald R. Ford_ (CVN-78) would all work before the ship set out for trials. They didn’t—only 4 of 11 were operational by the end of October 2019.83



Three other critical technologies—the ship’s new electromagnetic aircraft launching system, an advanced arresting gear used to recover aircraft on deck safely, and a dual band radar—have also failed to meet the service’s expectations.84 A December 2018 report by DOD’s director of operational test and evaluation (DOT&E) identified a host of concerns, ranging from “poor or unknown reliability of systems critical for flight operations” to inadequate crew berthing.85



Most damning, perhaps, were the DOT&E’s conclusions pertaining to the ship’s core mission: the ability to launch and recover aircraft at high tempo and over extended periods (sorties, in Navy jargon). The report warned, “Poor reliability of key systems … on CVN 78 could cause a cascading series of delays during flight operations that would affect CVN 78’s ability to generate sorties.”86 In the end, DOT&E concluded that the Navy’s sortie generation requirements for the _Ford_ were based on “unrealistic assumptions.”87



Other critics fault a systemic lack of accountability throughout the Navy. Industry analyst Craig Hooper wrote in October 2019, “The naval enterprise struggles to bring bad news to the higher levels of the chain of command. It is a habit that perpetuates something of a complacent ‘not my problem’ or career‐​protecting sluggishness in the face of avoidable disaster.” This has ramifications that go well beyond catapults and arresting gear.88



As difficult as the design and development process for the Navy’s capital ship has been, however, even tougher questions swirl around the employment of these massive platforms. In an era of defense dominance, when adversaries can use relatively cheap but accurate weapons to attack large and exquisite platforms, how will the carriers perform? Not well, according to some knowledgeable critics, including retired Navy Capt. Henry J. Hendrix, who in 2013 warned, “The queen of the American fleet is in danger of becoming like the battleships it was originally designed to support: big, expensive, vulnerable—and surprisingly irrelevant to the conflicts of the time.” The national security establishment, he concluded, had ignored “clear evidence that the carrier equipped with manned strike aircraft is an increasingly expensive way to deliver firepower” and that the ships might struggle “to operate effectively or survive in an era of satellite imagery and long‐​range precision strike missiles.”89



National Defense University’s T. X. Hammes imagines an even more dramatic transformation that would merge “old technologies with new to provide similar capability at a fraction of the cost.” Specifically, Hammes proposes using container ships loaded with hundreds or thousands of drones and cruise missiles—but very few people—to eventually take the place of the iconic flattops hurling and recovering manned aircraft. “Flying drones,” Hammes writes, “can provide long‐​range strike, surveillance, communications relay, and electronic warfare” and can be launched and recovered vertically. Cruise missiles deployed in standard shipping containers, meanwhile, could effectively convert “any container ship—from inter‐​coastal to ocean‐​going” into “a potential aircraft carrier.”90



For now, Congress has conspired to thwart any fundamental reconsideration of the centrality of the aircraft carrier to the modern surface fleet. The 11‐​carrier legislative mandate remains despite serious concerns about the _Ford_ ’s timeline and even as “the Navy is finding it increasingly difficult to deploy carriers and keep them on station.”91 A reckoning has been postponed but cannot be avoided forever.



According to the _Force Structure Assessment_ issued in December 2016, the Navy seeks to procure 52 small surface ships, 20 of which are to be a new class of guided‐​missile frigates, the FFG(X).92 Some analysts contended that reactivating the _Oliver Hazard Perry_ -class frigates, the last of which was retired in 2015, would help the Navy achieve its force structure goals faster, but the decision to commission new vessels signaled the Navy leadership’s commitment to modernization.93



The Navy requested $1.28 billion in its FY 20 budget to procure the first FFG(X), awarding conceptual design contracts to five different companies.94 Despite the purported reduction in scheduling, risk, and price with the Navy’s approach to the FFG(X), the CBO predicted in October 2019 that the total cost of the 20‐​ship program will be closer to $23 billion than the Navy’s estimated $17 billion.95



Although the House and Senate fulfilled the administration’s request for $1.28 billion in procurement, plus another $59 million for research and development in the FY 20 NDAA, doubts remain about this program’s ability to fill the capability gaps in the fleet.96 The key questions will revolve around unit cost and the length of the design, development, and build phases. Congress has put significant pressure on the Navy to implement cost‐​effective capabilities on realistic timelines. If the Navy is truly committed to expanding fleet capacity quickly, and with minimal risk, it is imperative that it hold the line against anything likely to lead to costly delays.



Before the Navy can decide what it needs, however, it must decide what it’s going to do. The Trump administration’s _National Security Strategy_ (NSS) and NDS would appear to be good news for the Navy. Both documents focus on the rise of peer or near‐​peer competitors, chiefly China and Russia, with reference also to regional rivals such as North Korea and Iran. These types of adversaries would privilege the need for naval and air power over ground forces, which have been geared to fighting nonstate actors and insurgents over the past two decades.



The U.S. Navy has an extraordinarily ambitious set of objectives, and the demands placed on the service already exceed its ability to meet them. These demands mostly originate with the various regional combatant commands and further reflect a long‐​standing assumption that the Navy’s forward presence is essential to global security. The Heritage Foundation’s _Index of U.S. Military Strength_ , for example, argues that “the Navy must maintain a global forward presence both to deter potential aggressors from conflict and to assure our allies and maritime partners that the nation remains committed to defending its national security interests and alliances.”97



What the Heritage Foundation casts as a requirement is a choice. Strategic requirements are not handed down from on high but reflect the dominant strategic paradigm. A commitment to maintaining the free movement of raw materials, essential commodities, and finished goods was a core mission for the U.S. Navy during the Cold War and was driven by a concern that a globe‐​straddling Soviet Navy was both motivated to close—and capable of closing—critical sea lanes of communication and maritime choke points.98



Today, the situation is much different. Most international actors, including even modern rivals such as Russia and China, depend on the free flow of maritime trade and are therefore highly incentivized to try to keep these waterways open. For decades, however, U.S. allies and partners have neglected their own maritime forces, coming to rely on the U.S. Navy deploying small, surface combatants in their home waters. In effect, therefore, the U.S. Navy has been operating as a global coastal constabulary.



This practice should stop. U.S. policy should aim to encourage these nation‐​states to play a key role in securing access to vital sea‐​borne trade. The presumption that the U.S. military must be constantly on station, including in waters thousands of miles away from the Western Hemisphere, merits scrutiny, not least because the U.S. Navy alone cannot meet the demands of being a de facto coast guard for all other nations—nor is it in America’s interest to try.



Sea‐​lane control in the modern era aims to ensure the free flow of goods and is primarily defensive. The aim should be to prevent others from limiting access to the open oceans while not threatening to deny anyone else the peaceful use of those same seas. That mission can and should be shared with other countries, most of whom will be operating near their shores, and thus highly motivated—and able—to defend their sovereign waters.



Marine Corps Gen. David Berger’s appointment as the 38th commandant of the service was met with a question by a Marine Corps major: “Sir, who am I?”99 With a founding mission of being able to carry out contested amphibious operations, it is unclear today who the United States is preparing to invade, and how it would do so. Would the United States deploy landing craft like those used on Normandy beaches in 1944 or at Inchon Korea in 1950 during an age of highly sophisticated surface‐​to‐​surface missiles? Does the U.S. military have functional aviation or naval vehicles that can support large modern amphibious invasions?



The response to these sorts of questions was dramatic and forceful. Berger’s _Commandant’s Planning Guidance_ (CPG) sought to kill many sacred cows and institute a new path forward where the Marines would focus on sea denial, interoperability with the Navy, and wargaming to understand current and future combat options.100 The CPG stated, without evocation, “the Marine Corps will be trained and equipped as a naval expeditionary force‐​in‐​readiness and prepared to operate inside actively contested maritime spaces in support of fleet operations.”101



The Marine Corps’ decision not to request amphibious platforms in the 2020 budget was formalized in the CPG, which called amphibious operations “impractical and unreasonable.” Such conclusions recognize the need for a swift, agile force that can operate in forward positions without the resources and protection of the core force.



While a future great power war in the Asia‐​Pacific is possible, the probability of a near‐​term conflict is very low; this supports the decision to move the Marine Corps from a focus on amphibious operations. In fact, recent reports note that China’s navy is rethinking its spending plans given the economic uncertainty brought on by the trade war with the United States.102 China is not a peer competitor; its grandiose naval ambitions remain unfulfilled, as massive investment would be needed for surface ships, landing craft, advanced weapons platforms, aircraft, and personnel—all when the demands of an expanding middle class are increasingly going unmet. And those domestic challenges all preceded the COVID-19 pandemic that began in late 2019 that has wreaked havoc on China’s economy.



As Marine Corps planners recognize, there is a great need for large numbers of cheap autonomous naval systems that can overwhelm the enemy, and these are preferable to expensive and manned systems.103 Berger stated, “I see potential in the ‘Lightning Carrier’ concept … however, [I] do not support a new‐​build CVL [light aircraft carrier].”104 The CPG suggests a possible focus on high‐​mobility artillery systems to deny sea access and landing routes.



The Navy and Marine Corps should not be pushing new amphibious platforms when they are unable to maintain their current craft in a steady state of readiness.105 If the Marines are truly the “first to fight,” they need to focus on modernization, rework force structure for quality over quantity, and reset their priorities after years of focus on the Global War on Terror. Senior leaders in the Marine Corps have the correct vision, but implementing their plans within a change‐​resistant bureaucracy will be a challenge.



The United States has failed to undertake a much‐​needed reevaluation of its approach to strategic deterrence. The nuclear triad, the array of land‐, air‐, and sea‐​based capabilities that can deliver nuclear weapons to targets, has been a fixture since the early Cold War. Since then the triad has become dogma. A reexamination of its value considering technological developments, advances in intelligence, surveillance, and reconnaissance, and changes in adversary capabilities is overdue.



That hasn’t occurred under the Trump administration, which seems to be settling on a kitchen‐​sink approach to solving the country’s alleged “deterrence gaps” vis‐​à‐​vis other great powers.106 Its 2018 _Nuclear Posture Review_ retains the triad and adds two new capabilities—a low‐​yield warhead for the Trident (the nuclear‐​armed ballistic missile carried by U.S. submarines) and a new nuclear sea‐​launched cruise missile—to the Obama administration’s nuclear modernization plan. A 2017 report from the CBO estimated that this plan would cost roughly $1.2 trillion over 30 years.107 That 30‐​year estimate is likely to increase as programs face unforeseen problems and delays. The United States is also trying to improve its capabilities for defeating ballistic and cruise missile threats to both forward‐​deployed forces and the American homeland.108



These investments in nuclear weapons and missile defense demonstrate that strategic deterrence remains central to U.S. strategy, but is the United States making the right policy choices? What are the threats the United States wants to deter, and can nuclear weapons and missile defense help mitigate them? Raising these questions reveals that some elements of the nuclear modernization plan are superfluous and that some missile defense choices are likely to push rivals to develop destabilizing counterstrategies.



Most of the nuclear modernization plan’s spending will fund new delivery platforms—aircraft, submarines, and missiles—with some money going toward updated nuclear warheads. The plan is not meant to expand the arsenal; as new systems get introduced, old ones will be phased out.



Supporters of the nuclear modernization plan claim that it will only eat up a small portion of overall military spending. That is true given the very high topline for the budget, but this does not imply that nuclear modernization will be cheap and easy. Initial cost estimates are already growing. For example, recent delays in the B61-12 nuclear gravity bomb life extension program (LEP) will add an extra $600–$700 million, and the W80-4 nuclear warhead LEP’s estimated project cost had increased from $9.4 billion in November 2017 to $12 billion by summer 2019.109 Delivery platforms are also prone to cost overruns. The B-2 Spirit bomber program (which wildly overran its initial cost projections) offers a cautionary tale for its secretive and expensive successor, the B-21 Raider.110



Before developing new nuclear capabilities, we need to decide whether they are necessary for strategic deterrence. Arguments about the relatively low price of systems are hardly compelling if the United States does not need to buy them in the first place. The B61-12 gravity bomb, for example, is superfluous given U.S. efforts to develop an air‐​launched cruise missile that could hold the same targets at risk from long distance.111 The decision to deploy a low‐​yield tactical warhead for the Trident missile rests on faulty understandings of Russian nuclear strategy.112 Similarly, the United States should eliminate the nuclear mission for the F-35, cut the purchase of new intercontinental ballistic missiles in half, and delay procurement of the B-21 for 10 years.113



Increased spending on strategically dubious capabilities also extends to missile defense. The _2019 Missile Defense Review_ calls for a wide‐​ranging expansion of missile defense capabilities to counter both rogue states and great powers.114 That includes expanding the stock of existing interceptors and developing new technology to counter offensive capabilities that U.S. adversaries have fielded to defeat existing U.S. defenses.115 Rather than enhancing strategic deterrence, America’s missile defense posture is encouraging adversaries to develop new offensive platforms that increase the risk of conventional conflicts going nuclear.



Adjusting American grand strategy toward restraint would mandate a different approach to strategic deterrence. Modernizing the U.S. nuclear arsenal is important, but the pursuit of maximum flexibility to deter an amorphous set of strategic threats will waste billions of dollars on capabilities the United States doesn’t need. The primary goal of strategic deterrence, preventing nuclear first use against America and its allies, would remain the same under restraint. Instead of pursuing flexibility to respond to a wide variety of threats, the three pillars of strategic deterrence under restraint are removal of peripheral threats through diplomacy; shifting a greater defense burden to allies; and adopting a conventional military posture that enables deterrence by denial—discouraging enemy action by denying a quick and easy victory.



Greater reliance on diplomacy could contain or remove potential threats that current U.S. military doctrine casts as strategic imperatives. For example, the Joint Comprehensive Plan of Action with Iran allowed the United States to reduce nuclear proliferation risks through diplomacy.116 The case also illustrates the negative consequences of abandoning diplomacy. Since the Trump administration’s withdrawal from the agreement, the region has witnessed a constant tit‐​for‐​tat escalation of tensions.117 Arms control agreements with other great powers such as China and Russia are another important feature of restraint’s approach to strategic deterrence. Arms control measures can help set guardrails on the most dangerous aspects of great power competition, allowing for a degree of strategic trust and stability that is important for averting nuclear disaster.



Another key component of a redesigned U.S. strategic deterrent would entail empowering allies to respond to the coercive activities of regional rivals. Given the stakes involved for all parties, the deterrent threats by local actors might prove more credible than those issued by a distant United States.118 Under restraint, regional disputes might prove less likely to escalate into great power conflict, and more capable local deterrent forces would help reduce—though not eliminate—demands on the U.S. military and U.S. taxpayers.



China and Russia demonstrate how effective asymmetric strategies—those that avoid matching an opponent’s capabilities but instead try to exploit weaknesses with other means—can frustrate an otherwise stronger foe that depends on power projection to achieve its interests.119 U.S. allies in East Asia, for example, don’t need to build a lot of expensive aircraft or ships to defend themselves from China’s growing air and naval forces. A mix of unmanned systems, long‐​range precision strike conventional weapons, and strong air defense could be an effective and affordable counter to Chinese power. Encouraging allies to develop their own asymmetric capabilities would empower them to contribute more to deterring regional conflicts. Gradually reducing the forward deployment of U.S. forces could facilitate this transition.120



The United States would still have an interest in deterring nuclear first use against its allies—or the use of nuclear weapons in any context. But stronger, more capable allies armed with conventional weapons, combined with a reduced forward‐​deployed U.S. military presence, would shorten the list of strategic threats that U.S. officials feel obliged to deter or eliminate.



The third pillar of a new U.S. strategic deterrence posture under restraint is a greater reliance on conventional weapons to deter other great powers. Instead of threatening an attacking country through punishment (damaging the attacker’s population and economy) this approach would depend on a concept known as deterrence by denial, which resists enemy action by denying a quick, easy military victory for the aggressor.121 Credibly increasing the costs of aggressive action would leverage U.S. advantages in sensors, regional missile defense, and conventional long‐​range precision strike to deter military action that U.S. allies are unable to address.122 Allies equipped with similar capabilities would further improve deterrence by denial.



Such an approach would reduce the risk of inadvertent nuclear escalation in conventional conflicts by focusing on defeating military units rather than engaging in deep strikes against an adversary’s command and control networks.123 Technical developments in both the United States and its potential great power adversaries have blurred the lines between conventional and nuclear forces. The military strategies adopted by the United States, China, and Russia that emphasize early, deep conventional strikes further increase the escalation risks.124



Under this new approach, nuclear weapons and homeland missile defense would play reduced roles. On the missile defense side, U.S. defense planners should pivot to improving regional systems and increasing the stock of associated interceptors while moving away from expanded homeland missile defense.126 That would make it harder for great power adversaries to both initiate and prevail in quick, limited conflicts. U.S. leaders would also face less pressure to rapidly escalate to conventional attacks against Chinese or Russian territory. The U.S. way of war emphasizes strikes against command and control facilities, some of which are located far behind a country’s borders. Such strikes could be interpreted as an attack on a country’s leadership or an effort to reduce the effectiveness of its nuclear forces. If U.S. forces could deflect an initial attack against land‐​based, anti‐​access/​area denial weapons such as surface‐​to‐​air and anti‐​ship missile batteries, it could reduce the incentive to target adversary command and control early in a conflict.



The United States should take advantage of a strategic pause, adopt a grand strategy of self‐​reliance and restraint, and develop a comprehensive plan for dealing with peer and near‐​peer competitors and rivals. For at least two decades, the U.S. military has been trapped in a cycle of small‐​scale wars and nation‐​building fiascos that have eroded America’s unique advantages. Reconstructing U.S. security, therefore, requires a conscious decision to remove U.S. forces from past conflicts, and a fundamental reconceptualization of how the United States will use its forces in the future. Security budgets need to view U.S. power along economic, diplomatic, and cultural dimensions. These alternatives are often more effective than force and can produce a positive lasting impact by creating a period of stability that endures and that can be sustained by many like‐​minded actors, not merely the U.S. military.



Diplomacy, for example, has grown stagnant, but the Trump administration seems determined to hasten its demise.127 President Trump has scaled back on many diplomatic initiatives, but the COVID-19 pandemic laid bare the shortcomings of the military‐​centric approach. The United States can divest some of its legacy military apparatus and focus on innovating for the future while also investing a small fraction of these funds to deal with a range of threats to public safety that are not amenable to military solutions. The U.S. government will almost certainly need to prepare for a role in coordinating supply and delivery of vital equipment in future disasters and pandemics. Our true strategic reserve is more than the manpower that the military can marshal and the expertise in delivery, logistics, and analysis that the military can offer. The capacity and the expertise of the American people is a strength that will see us through crises.



This report has outlined a plan for moving the United States toward a more sustainable national security posture predicated on restraint.128 The budgeting process and the design and development of new military systems are riddled with inefficiencies that have wasted time and money that could be put toward fixing the social and structural problems the military faces. Conventional forces should be modernized for future fights, not geared toward sustaining the war on terror. Finally, the United States needs a modern approach to strategic deterrence that places greater emphasis on denying the ability of other great powers to project offensive military forces by using conventional capabilities rather than the nuclear triad.



Security comes through prudence, not overwhelming force, permanent alliances, or massive investments in weapons platforms. Defending the United States requires a judicious application of the many instruments of American power, not reckless overseas military adventures that have cost too many lives and too much treasure. A clear consideration of U.S. capabilities, appreciation of our fortunate geopolitical situation, and confidence in our ability to address future challenges will allow the United States to build and maintain a leaner and more efficient military, one that is more than capable of defending U.S. vital interests and deterring attacks against the homeland.



 **Advanced Battle Management System (ABAMS):** the technical engine that would manage all communications, orders, and sensors used by the Air Force



 **Anti‐​access/​area‐​denial (A2/AD):** an operational concept that complicates an opponent’s ability to use air, naval, and land power at long distance; typically entails the use of land‐​based sensors and precision strike systems to target opponent ships, aircraft, and bases



 **Aircraft carriers (CVNs):** the largest ships in the U.S. Navy and the centerpiece of U.S. fleet operations, capable of carrying about 60 aircraft of varying types



 **Arresting gear:** mechanical system that rapidly decelerates aircraft when landing on a platform such as an aircraft carrier



 **Ballistic missile submarines (SSBNs):** the sea‐​based leg of the nuclear triad, these vessels carry Trident missiles, each capable of delivering up to eight nuclear warheads



 **Command and control (C2):** set of organizational and technical processes employed to accomplish missions



 **Dual band radar:** combines radar systems into one integrated system for easier operation, maintenance, upgrade, and targeting



 **Fast‐​attack submarines (SSN):** the U.S. Navy’s primary undersea platform, capable of both offensive action at sea or against targets on land; also used for intelligence gathering



 **Frigates (FFG):** mixed‐​armament warship lighter than a destroyer; typically focused on anti‐​ship and anti‐​submarine warfare



 **FFG(X):** class of future multimission guided‐​missile frigates



 **Integrated Fire Control‐​Counter Air System (NIFC-CA):** the Navy’s multidomain battle management system



 **Low‐​yield nuclear weapon:** a nuclear weapon with a relatively small explosive yield thought to be useful for limited nuclear operations on the battlefield or for controlling escalation



 **Maritime choke points (e.g., straits and narrows):** a heavily trafficked narrow waterway



 **Micro‐​targeting:** direct marketing methods utilizing datamining techniques to segment consumers by tastes or attributes



 **Nuclear Posture Review (NPR):** major policy document that sets out the nuclear strategy and policies of a new administration; typically includes overviews of the U.S. nuclear arsenal, arms control policy, and nuclear strategy broadly defined



 **Operational readiness:** capacity of a unit to perform its designated combat or combat support function



 **Smart power:** strategic use of both hard (military) and soft power (diplomacy and trade) to achieve foreign policy ends



Eric Gomez is director of defense policy studies; Christopher Preble is vice president for defense and foreign policy studies; Lauren Sander is external relations manager for defense and foreign policy studies; and Brandon Valeriano is a senior fellow at the Cato Institute. 



ShowHide

Endnotes



1 Samuel P. Huntington, “Why International Primacy Matters,” _International Security_ 17, no. 4 (Spring 1993): 83.



2 George Shultz, _American Umpire—Teaser_ , trailer for film by James Shelley and Elizabeth Cobbs, 2016, 0:47, https://​vimeo​.com/​1​4​6​7​22638.



3 Office of the Secretary of Defense, _Summary of the 2018 National Defense Strategy of the United States of America: Sharpening the American Military’s Competitive Edge_ (Washington: Department of Defense, 2018), pp. 2, 4. Emphasis in original.



4 Office of the President of the United States of America, _National Security Strategy of the United States of America_ (Washington: The White House, December 2017), p. 28.



5 National Defense Authorization Act for Fiscal Year 2020, S. 1790, 116th Cong. (2019).



6 Gordon Lubold and Nancy A. Youssef, “Trump Administration Considers 14,000 More Troops for Mideast,” _Wall Street Journal_ , December 4, 2019; and Jared Malsin, “U.S. Forces Return to Saudi Arabia to Deter Attacks by Iran,” _Wall Street Journal_ , February 26, 2020.



7 John Glaser and Christopher Preble, “High Anxiety: How Washington’s Exaggerated Sense of Danger Harms Us All,” Cato Institute Study, December 10, 2019.



8 Jennifer Lind and Daryl G. Press, “Reality Check: American Power in an Age of Constraints,” _Foreign Affairs_ 99, no. 2 (March/​April 2020): 41–48.



9 Office of Management and Budget (OMB), _A Budget for America’s Future_ (Washington: Government Publishing Office, February 2020), p. 34; and OMB, _A Budget for a Better America: Promises Kept. Taxpayers First._ (Washington: Government Publishing Office, March 2019), p. 23.



10 Office of the President, _National Security Strategy_ , p. 28.



11 Office of the Secretary of Defense, _2018 National Defense Strategy_ , p. 1.



12 Smart power is the strategic use of both hard (military) and soft power (diplomacy and trade) to achieve foreign policy ends.



13 John Glaser, Christopher A. Preble, and A. Trevor Thrall, _Fuel to the Fire: How Trump Made America’s Broken Foreign Policy Even Worse (and How We Can Recover)_ (Washington: Cato Institute, 2019), p. 181.



14 See, for example, Barry R. Posen, _Restraint: A New Foundation for U.S. Grand Strategy_ (Ithaca, NY: Cornell University Press, 2014); and A. Trevor Thrall and Benjamin Friedman, eds., _U.S. Grand Strategy in the 21st Century: The Case for Restraint_ (New York: Routledge, 2018).



15 Mark Hannah, “Stop Declaring War on a Virus,” _War on the Rocks_ , April 17, 2020.



16 Office of the Secretary of Defense, _Nuclear Posture Review_ (Washington: Department of Defense, February 2018), p. 21. The Nuclear Posture Review (NPR) is a major policy document that sets out the nuclear strategy and policies of a new administration. A typical NPR includes overviews of the U.S. nuclear arsenal, arms control policy, and nuclear strategy, broadly defined.



17 The 2018 NPR is explicit about the demand for flexibility in nuclear deterrence. It refers to this as “tailored deterrence.” See Office of the Secretary of Defense, _Nuclear Posture Review_ , pp. vii–viii.



18 Office of the Secretary of Defense, _Nuclear Posture Review_ , p. 53.



19 Low‐​yield nuclear weapons are nuclear weapons with a relatively small explosive yield (a smaller amount of energy released when detonated) thought to be useful for limited nuclear operations on the battlefield or controlling escalation.



20 Office of the Secretary of Defense, _2019 Missile Defense Review_ (Washington: Department of Defense, January 2019), p. xvi.



21 Command and control refers to the actual efficacy of commanding officers and leaders to move and direct troops on the battlefield as well as the infrastructure that enables the commander to provide direction to their troops (i.e., radios and tactical operation centers providing intelligence and support).



22 Angus King, interview by Andrea Mitchell, _Andrea Mitchell Reports_ , MSNBC, May 23, 2019, https://​www​.youtube​.com/​w​a​t​c​h​?​v​=​S​s​f​a​I​B​w2f7A.



23 Michael O’Hanlon, “President Trump Might Be on to Something with Russia,” _USA Today_ , December 19, 2017.



24 Benjamin Denison, “Confusion in the Pivot: The Muddled Shift from Peripheral War to Great Power Competition,” _War on the Rocks_ , February 12, 2019.



25 Senate and House Armed Services Committees, _FY2020 NDAA Summary_ , December 2019, https://www.armed-services.senate.gov/imo/media/doc/FY20%20NDAA%20Conference%20Summary%20_%20FINAL.pdf.



26 Quoted in Shawn Snow, “Esper Says U.S. Forces Combating ISIS in Libya ‘Continue to Mow the Lawn,’” _Military Times_ , November 14, 2019.



27 Claudia Grisales, “These Are the Military Projects Losing Funding to Trump’s Border Wall,” NPR, September 4, 2019. 



28 Bureau of the Fiscal Service, _Final Monthly Treasury Statement: Receipts and Outlays of the United States Government: For Fiscal Year 2019 through September 30, 2019, and Other Periods_ (Washington: Department of the Treasury, September 2019). The deficit in fiscal year 2011 was $1.3 trillion according to the Congressional Budget Office. See Elizabeth Cove Delisle et al., “Federal Budget Deficit for Fiscal Year 2011: $1.3 Trillion,” Congressional Budget Office, November 8, 2011.



29 “Foreign Policy Poll,” Charles Koch Institute, January 2017, https://​mk0qeluyepi9​drvw7c​ng​.kin​stacdn​.com/​w​p​-​c​o​n​t​e​n​t​/​u​p​l​o​a​d​s​/​2​0​1​7​/​0​2​/​C​h​a​r​l​e​s​-​K​o​c​h​-​I​n​s​t​i​t​u​t​e​-​a​n​d​-​C​e​n​t​e​r​-​f​o​r​-​t​h​e​-​N​a​t​i​o​n​a​l​-​I​n​t​e​r​e​s​t​-​J​a​n​-​2​0​1​7​-​F​o​r​e​i​g​n​-​P​o​l​i​c​y​-​P​o​l​l​-​1.pdf, p. 22. See also “New Poll: Americans Crystal Clear: Foreign Policy Status Quo Not Working,” Charles Koch Institute, February 7, 2017.



30 Lydia Saad, “Demand Wanes for Higher Defense Spending,” Gallup, March 12, 2019; and Mark Hannah and Caroline Gray, _Indispensable No More? How the American Public Sees U.S. Foreign Policy_ (New York: Eurasia Group Foundation, November 2019).



31 F. Matthew Woodward, _Funding for Overseas Contingency Operations and Its Impact on Defense Spending_ (Washington: Congressional Budget Office, October 2018), pp. 3, 4.



32 Neta C. Crawford, “United States Budgetary Costs and Obligations of Post‐​9/​11 Wars through FY2020: $6.4 Trillion,” 20 Years of War: A Costs of War Research Series, Brown University, November 13, 2019, p. 3; and Elizabeth Field, _Overseas Contingency Operations: Alternatives Identified to the Approach to Fund War‐​Related Activities_ , GAO-19–211 (Washington: Government Accountability Office, January 2019), p. 1.



33 Senate and House Armed Services Committees, _FY2020 NDAA Summary_ , p. 1.



34 “OCO Is Fourth Largest ‘Agency,’” Taxpayers for Common Sense, February 10, 2020. The Trump administration’s 2021 budget request, _A Budget for America’s Future_ , outlines plans to reduce the overseas contingency operations budget each fiscal year (FY 2021 proposal is $69 billion), but Taxpayers for Common Sense also noticed the steady proposed increases to the base budget that will counterbalance this decrease in OCO. See “More OCO Details—the Devil’s in the Footnotes!,” Taxpayers for Common Sense, February 10, 2020.



35 Emily M. Morgenstern, “Foreign Affairs Overseas Contingency Operations (OCO) Funding: Background and Current Status,” Congressional Research Service In Focus, December 30, 2019.



36 Brendan W. McGarry, _The Defense Budget and the Budget Control Act: Frequently Asked Questions_ , CRS Report R44039 (Washington: Congressional Research Service, September 30, 2019). The Budget Control Act did not address the largest share of U.S. federal spending, so‐​called mandatory programs such as Medicare, Medicaid, and Social Security.



37 McGarry, _Defense Budget and the Budget Control Act_ , pp. 4–5. Congress has repeatedly amended the Budget Control Act (BCA) to change discretionary spending limits through the Bipartisan Budget Acts (BBA) of 2013, 2015, 2018, and 2019. The original cap for defense spending in 2020 under the BCA was to be $630 billion and was raised to $667 billion in the most recent BBA. For discretionary spending, which accounts for most defense spending, the Office of Management and Budget calculates the percentage and dollar amount to be taken from affected programs to achieve the total mandatory cut required by the BCA. See “FAQs on Sequester: An Update for 2020,” posted on the House Committee on the Budget’s website, https://​bud​get​.house​.gov/​p​u​b​l​i​c​a​t​i​o​n​s​/​r​e​p​o​r​t​/​F​A​Q​s​-​o​n​-​S​e​q​u​e​s​t​e​r​-​A​n​-​U​p​d​a​t​e​-​f​o​r​-2020.



38 Brendan W. McGarry and Emily M. Morgenstern, _Overseas Contingency Operations Funding: Background and Status_ , CRS Report R44519 (Washington: Congressional Research Service, September 6, 2019), p. 6.



39 For background, see Office of the General Counsel, _Principles of Federal Appropriations Law, Chapter 2: The Legal Framework, Fourth Edition, 2016 Revision_ , GAO-16–463SP (Washington: Government Accountability Office, 2016).



40 Michelle Mrdeza and Kenneth Gold, “Reprogramming Funds: Understanding the Appropriator’ Perspective,” Government Affairs Institute at Georgetown University, https://​gai​.george​town​.edu/​r​e​p​r​o​g​r​a​m​m​i​n​g​-​f​u​n​d​s​-​u​n​d​e​r​s​t​a​n​ding/.



41 Susan J. Irving (associate director of budget issues at Government Accountability Office) to Steve Horn (chairman of the Subcommittee on Government Management, Information and Technology under the Committee on Government Reform and Oversight), June 7, 1996, B-272080, https://​www​.gao​.gov/​a​s​s​e​t​s​/​9​0​/​8​5​6​2​0.pdf.



42 Mihir Zaveri, Guilbert Gates, and Karen Zraick, “The Government Shutdown Was the Longest Ever. Here’s the History,” _New York Times_ , January 25, 2019.



43 According to the White House, the deal that broke the impasse took $1.4 billion in the fiscal year 2019 budget bill, $3.6 billion from military construction projects, $2.4 billion from the Department of Defense counterdrug account, and $600 million from a Treasury Department forfeiture fund to fund the border wall construction. “President Donald J. Trump’s Border Security Victory,” Fact Sheet, The White House, February 15, 2019.



44 See “Construction Authority in the Event of a Declaration of War or National Emergency,” 10 U.S. Code § 2808.



45 The definition of a national emergency in U.S. code simply means a declaration of emergency by the president. See “Support for Counterdrug Activities and Activities to Counter Transnational Organized Crime,” 10 U.S. Code § 284.



46 See “Termination of Existing Declared Emergencies,” 50 U.S. Code § 1601.



47 Matthew Cox, “Army Scaling Back Recruiting Goals after Missing Target, Under Secretary Says,” Mil​i​tary​.com, March 21, 2019.



48 Jamie Crawford, “Military Sexual Assaults Increase Sharply, Pentagon Report Finds,” CNN, May 2, 2019; and Patricia Kime, “Active‐​Duty Military Suicides at Record Highs in 2018,” Mil​i​tary​.com, January 30, 2019.



49 The fiscal year 2020 National Defense Authorization Act increased overall military end strength by 1,400 troops. See Pat Towell, _FY2020 National Defense Authorization Act: P.L. 116–92 (H.R. 2500, S. 1790)_ , CRS Report R46144 (Washington: Congressional Research Service, January 2, 2020), p. 8.



50 Integrated Fire Control‐​Counter Air System is the Navy’s multidomain battle management system; the Advanced Battle Management System is the technical engine that manages all communications, orders, and sensors that the Air Force uses.



51 Dan Gouré, “A New Joint Doctrine for an Era of Multi‐​Domain Operations,” _Real Clear Defense_ , May 24, 2019; and Grant J. Smith, “Multi‐​Domain Operations: Everyone’s Doing It; Just Not Together,” _Over the Horizon_ , June 24, 2019.



52 Theresa Hitchens, “Navy, Air Force Chiefs Agree to Work on All Domain C2,” _Breaking Defense_ , November 12, 2019.



53 Examples of these efforts include the Commandant of the Marine Corps Strategist Program (https://​www​.usm​cu​.edu/​A​c​a​d​e​m​i​c​-​P​r​o​g​r​a​m​s​/​C​M​C​-​F​e​l​l​o​w​s​-​S​t​r​a​t​e​g​i​s​t​s​-​F​o​r​e​i​g​n​-​P​M​E​-​O​l​m​s​t​e​d​-​S​c​h​o​l​a​r​s​/​C​o​m​m​a​n​d​a​n​t​-​o​f​-​t​h​e​-​M​a​r​i​n​e​-​C​o​r​p​s​-​S​t​r​a​t​e​g​i​s​t​-​P​r​o​gram/) and Department of Defense STEM scholarships (https://​dod​stem​.us/​s​t​e​m​-​p​r​o​g​r​a​m​s​/​s​c​h​o​l​a​r​ships).



54 The military doesn’t currently use the term “data wrangler,” but it is a common term in the film industry used to identify the person responsible for collecting and storing digital footage. This process is much the same in the military, where all data generated needs to be collected, transformed, stored, and analyzed. In short, the U.S. military needs to establish a system to identify and task battlefield data managers.



55 See Kessel Run (website), U.S. Air Force, https://​kessel​run​.af​.mil/​roles.



56 For example, the Pentagon has repeatedly tried to terminate production of new M1 Abrams tanks, but Congress continued to fund them over these objections. See Associated Press, “Army: Thanks but No Tanks,” _Politico_ , April 28, 2013. Similarly, Congress authorized 12 more F-35 Lightning II aircraft than the Trump administration requested in fiscal year (FY) 2020. See Towell, _FY2020 National Defense Authorization Act_ , pp. 20–21. The continued legislative requirement for aircraft carriers similarly complicates long‐​range shipbuilding plans. The Department of Defense’s request for the _Columbia_ -class ballistic missile submarine for FY 20 was $1.7 billion for procurement and $533 million for research and development; however, Congress authorized $1.8 billion for procurement and $548 million in research and development for FY 20. See Towell, _FY2020 National Defense Authorization Act_ , p. 10.



57 Quantum computing is the computational method utilizing superposition and entanglement to process calculations orders of magnitude faster than current microprocessors. Note: Quantum supremacy is different from quantum advantage, which is when a quantum device solves a problem _faster_ than a traditional computer. Thanks to James Knupp for clarifying this concept. See Frank Arute et al., “Quantum Supremacy Using a Programmable Superconducting Processor,” _Nature_ 574 (2019): 505–510. 



58 Benjamin Jensen, Scott Cuomo, and Chris Whyte, “Wargaming with Athena: How to Make Militaries Smarter, Faster, and More Efficient with Artificial Intelligence,” _War on the Rocks_ , June 5, 2018. 



59 See, for example, Robert M. Farley, _Grounded: The Case for Abolishing the United States Air Force_ (Lexington: University Press of Kentucky, 2014).



60 Todd Harrison, _The Air Force of the Future: A Comparison of Alternative Force Structures_ (Washington: Center for Strategic and International Studies, October 2019).



61 Operational readiness is the capacity for a unit to perform its designated combat or combat‐​support functions.



62 Dan Grazier, “F-35: Is America’s Most Expensive Weapon of War the Ultimate Failure?,” _National Interest_ , March 19, 2019; Kristin Houser, “Hard Landing: U.S. Military’s Trillion‐​Dollar F-35 Fighter Jet Is Almost Unflyable,” _Futurism_ , June 13, 2019; Michael P. Hughes, “What Went Wrong with the F-35, Lockheed Martin’s Joint Strike Fighter?,” _The Conversation US_ , June 14, 2017; Jonathan Lowell, “A U.S. Air Force Pilot Describes How He Landed His F-35 Safely after a Mid‐​Air Power Failure,” _Business Insider_ , August 27, 2019; and Eric Tegler, “WTF-35: How the Joint Strike Fighter Got to Be Such a Mess,” _Popular Mechanics_ , July 27, 2018. The program initially called for 2,000 aircraft of all variants by the end of fiscal year 2019 but was to have produced only 500 over that period: Michael J. Sullivan, _F-35 Joint Strike Fighter: Action Needed to Improve Reliability and Prepare for Modernization Efforts_ , GAO-19–341 (Washington: Government Accountability Office, April 2019), p. 6. The F-35A model, flown by the Air Force, is set to drop from $89.2 million to $77.9 million in 2022: Marcus Weisgerber, “Price of F-35 Falls, but Not as Much as Pentagon Hoped,” _Defense One_ , October 29, 2019.



63 Anti‐​access/​area‐​denial is an operational concept that complicates an opponent’s ability to use air, naval, and land power at long distance, which typically entails the use of land‐​based sensors and precision strike systems to target opponent ships, aircraft, and bases.



64 Harrison, _The Air Force of the Future_.



65 Harrison.



66 Brenda S. Farrell, _Military Personnel: Strategy Needed to Improve Retention of Experienced Air Force Aircraft Maintainers_ , GAO-19–160 (Washington: Government Accountability Office, February 2019).



67 Rachel S. Cohen, “USAF’s New Info Warfare Group Coming into Focus,” _Air Force Magazine_ , September 18, 2019. Regarding the F-15EX, see Kyle Mizokami, “After Nearly 20 Years, the Air Force Will Fly Brand New F‐​15s,” _Popular Mechanics_ , January 29, 2020.



68 Pete Geren and George W. Casey Jr., _A Statement on the Posture of the United States Army 2009_ (Washington: U.S. Army, May 2009).



69 For more information about Army Futures Command, visit https://​www​.army​.mil/​f​u​tures.



70 The Army later added two additional cross‐​functional teams, “Synthetic Training Environment” and “Assured Positioning, Navigation and Timing.”



71 Dave Philipps, “As Economy Roars, Army Falls Thousands Short of Recruiting Goal,” _New York Times_ , September 21, 2018.



72 Microtargeting is a direct marketing technique that segments consumers by tastes or attributes.



73 Eric J. Labs, _An Analysis of the Navy’s Fiscal Year 2020 Shipbuilding Plan_ (Washington: Congressional Budget Office, October 2019), p. 3.



74 The prior year, for example, the Congressional Budget Office similarly concluded that the cost of the Navy’s plan for new‐​ship construction ($26.7 billion) would nearly double its historical average of $13.6 billion. See Eric J. Labs, _Analysis of the Navy’s Fiscal Year 2019 Shipbuilding Plan_ (Washington, Congressional Budget Office, October 2018), p. 3. A report 10 years earlier had reached a similar conclusion: the estimated costs to fulfill all the Navy’s wishes were nearly double what it was likely to receive given historical funding averages. See Dale Eisman, “Navy’s Shipbuilding Wish List Sails into Troubled Waters,” _The Virginian‐​Pilot_ , March 15, 2008.



75 David B. Larter, “Acting US Navy Secretary: Deliver Me a 355‐​Ship Fleet by 2030,” _Defense News_ , December 9, 2019.



76 Rebecca Kheel, “Pentagon Proposes $704B Budget with Boost for Nukes, Cuts to Ships,” _The Hill_ , February 10, 2020.



77 Quoted in David B. Larter, “In a Quest for 355 ships, US Navy Leaders Are Unwilling to Accept a Hollow Force,” _Defense News_ , January 13, 2020. See also Nick Blenkey “Acting Secnav Commits to 355 Ship Navy, but Not at $2 Billion Apiece,” _MarineLog_ , January 10, 2020.



78 See, for example, Eric Edelman et al., _Providing for the Common Defense: The Assessment and Recommendations of the National Defense Strategy Commission_ (Washington: United States Institute of Peace, November 2018), p. 63.



79 Dakota L. Wood, ed., _2020 Index of U.S. Military Strength with Essays on Great Power Competition_ (Washington: Heritage Foundation, November 2019), p. 349.



80 John Pendleton, _Navy Force Structure: Sustainable Plan and Comprehensive Assessment Needed to Mitigate Long‐​Term Risks to Ships Assigned to Overseas Homeports_ , GAO-15–329 (Washington: Government Accountability Office, May 2015); see also Geoff Ziezulewicz, “Navy’s 7th Fleet No Stranger to High Ops Tempo,” _Navy Times_ , August 21, 2017.



81 Ballistic missile submarines (SSBNs) are the sea‐​based leg of the nuclear triad. These vessels carry Trident missiles, which are each capable of delivering up to eight nuclear warheads. Fast‐​attack submarines (SSN) are the U.S. Navy’s primary undersea platform, capable of offensive action both at sea and against targets on land.



82 Aircraft carriers (CVNs) are the largest ships in the U.S. Navy and the centerpiece of U.S. fleet operations, capable of carrying about 60 aircraft of varying types. The “N” in the hull classification of a CVN denotes that it employs nuclear propulsion. Guided‐​missile frigates (FFG) are a mixed‐​armament warship lighter than a destroyer, which are typically focused on anti‐​ship and anti‐​submarine warfare. The FFG(X) is the next generation of multimission guided‐​missile frigates.



83 Kyle Mizokami, “USS Ford Will Set Sail with Only 2 out of 11 Weapon Elevators,” _Popular Mechanics_ , October 12, 2019.



84 Arresting gear is the mechanical system that rapidly decelerates aircraft when they land on a platform, such as an aircraft carrier. Dual band radars combine radar systems into one integrated system for easier operation, maintenance, upgrade, and targeting.



85 Robert F. Behler, _Director, Operational Test and Evaluation: FY 2018 Annual Report_ , Department of Defense, December 2018, p. 131.



86 Behler, _FY 2018 Annual Report_ , p. 134.



87 Justin Katz, “As Navy Touts $14.9B Dual Carrier Buy Contract, DOT&E Report Calls Out ‘Unrealistic Assumptions’ about CVN-78,” _Inside Defense_ , February 1, 2019.



88 Craig Hooper, “The Most Expensive Ship in the World Is Broken. The U.S. Navy Secretary Should Be Held Accountable,” _Forbes_ , October 16, 2019.



89 Henry J. Hendrix, _At What Cost a Carrier?_ , Disruptive Defense Papers (Washington: Center for a New American Security, March 2013), p. 3.



90 T. X. Hammes, “We Need to Start Thinking Differently about Maritime Airpower—and We Can,” _Task and Purpose_ , September 19, 2018.



91 Paul McLeary, “All 6 East Coast Carriers in Dock, Not Deployed: Hill Asks Why,” _Breaking Defense_ , October 28, 2019.



92 _Executive Summary: 2016 Navy Force Structure Assessment (FSA)_ (Washington: U.S. Department of the Navy, December 15, 2016), p. 2; and Ronald O’Rourke, _Navy Frigate (FFG[X]) Program: Background and Issues for Congress_ , CRS Report R44972 (Washington: Congressional Research Service, April 28, 2020), p. 1.



93 John Cole and Thomas Ulmer, “Bad Idea: Reactivating the U.S. Navy’s Oliver Hazard Perry‐​Class Frigates,” _Defense 360_ , December 7, 2017; and David B. Larter, “Don’t Reactivate the Old Frigates, Internal US Navy Memo Recommends,” _Defense News_ , November 12, 2017.



94 David B. Larter, “The US Navy’s New, More Lethal Frigate Is Coming into Focus,” _Defense News_ , January 28, 2019. In late April 2020, the Navy selected a design by Italian shipmaker Fincantieri to be built at the Marinette Marine shipyard in Wisconsin. David B. Larger, “The US Navy Selects Fincantieri Design for Next‐​Generation Frigate,” _Defense News_ , April 30, 2020.



95 Labs, _Analysis of the Navy’s Fiscal Year 2019 Shipbuilding Plan_ , p. 25.



96 O’Rourke, _Navy Frigate (FFG[X]) Program_ , p. 26, Table 3.



97 Wood, _2020 Index of U.S. Military Strength_ , p. 375.



98 Maritime choke points are heavily trafficked, narrow waterways such as straits and narrows.



99 Leo Spaeder, “Sir, Who Am I? An Open Letter to the Incoming Commandant of the Marine Corps,” _War on the Rocks_ , March 28, 2019.



100 David H. Berger, _Commandant’s Planning Guidance: 38th Commandant of the Marine Corps_ (Washington: U.S. Marine Corps, 2019).



101 Berger, _Commandant’s Planning Guidance_. The Marine Corps _Force Design 2030_ offers more details about how the service will turn Berger’s guidance into reality. See _Force Design 2030_ (Washington: U.S. Marine Corps, March 2020). To learn more about how the 2030 force design could help the service implement a restraint‐​focused grand strategy, see Eric Gomez, “Marine Corps Changes Inch U.S. Closer to a Restraint‐​Friendly Military Posture,” _Cato at Liberty_ (blog), Cato Institute, March 24, 2020.



102 Minnie Chan, “China’s Navy Is Being Forced to Rethink Its Spending Plans as Cost of Trade War Rises,” _South China Morning Post_ , May 26, 2019.



103 Scott Cuomo et al., “How the Marines Will Help the U.S. Navy and America’s Allies Win the Great Indo‐​Pacific War of 2025,” _War on the Rocks_ , September 26, 2018.



104 Quoted in Richard R. Burgess, “Marine Commandant Berger: Force Design Is Top Priority,” _Seapower Magazine_ , July 18, 2019.



105 Sam LaGrone and Megan Eckstein, “Failure of Two Ships to Participate in RIMPAC Highlight Amphibious Readiness Gap,” _USNI News_ , August 1, 2018.



106 For more on the “deterrence gap” concept, see Keith B. Payne, “The Emerging Nuclear Environment: Two Challenges Ahead,” National Institute for Public Policy Information Series no. 436, January 2, 2019; and Office of the Secretary of Defense, _Nuclear Posture Review_ , p. 55.



107 Michael Bennett, _Approaches for Managing the Costs of U.S. Nuclear Forces, 2017 to 2046_ (Washington: Congressional Budget Office, October 2017), p. 1; and Office of the Secretary of Defense, _Nuclear Posture Review_ , p. 55.



108 The exact amount of fiscal year (FY) 2019 and FY 20 appropriations for the Missile Defense Agency (MDA) was $10.491 billion and $10.452 billion, respectively. For a breakdown of funding by program, see Wes Rumbaugh, “FY 2020 Missile Defense Agency Budget Tracker,” Missile Threat, Missile Defense Project, Center for Strategic and International Studies, December 30, 2019. The Trump administration proposed $20.3 billion for missile defense and defeat in its FY 21 budget submission, including $9.2 billion for the MDA. See Jon Harper, “Budget 2021: Trump Proposes Flat Pentagon Budget,” _National Defense_ , February 10, 2020.



109 Rachel S. Cohen, “B61-12 Nuclear Warhead Delay Drives Up Price Tag,” _Air Force Magazine_ , September 25, 2019; and Sara Sirota, “GAO: B61-12 LEP First Production Unit Delayed, W80-4 LEP Cost Estimate Increased,” _Inside Defense_ , June 18, 2019.



110 As Kingston Reif and Mandy Smithberger note, “the B-2 bomber program overran its cost so badly that a mere 20 aircraft emerged from a $40 billion program [that originally] intended to buy 135 to 150 aircraft.” Kingston Reif and Mandy Smithberger, “America’s New Stealth Bomber Has a Stealthy Price Tag,” _Defense One_ , May 21, 2018.



111 Dennis Evans and Jonathan Schwalbe, _The Long‐​Range Standoff (LRSO) Cruise Missile and Its Role in Future Nuclear Forces_ (Laurel, MD: Johns Hopkins Applied Physics Laboratory, 2017), p. 8.



112 Olga Oliker and Andrey Baklitskiy, “The Nuclear Posture Review and Russian ‘De‐​Escalation:’ A Dangerous Solution to a Nonexistent Problem,” _War on the Rocks_ , February 20, 2018; and Olga Oliker, “U.S. and Russian Nuclear Strategies: Lowering Thresholds, Intentionally and Otherwise,” in _America’s Nuclear Crossroads: A Forward‐​Looking Anthology_, eds. Caroline Dorminey and Eric Gomez (Washington: Cato Institute, July 2019), pp. 37–46.



113 For more on these recommendations, see Caroline Dorminey, “Buying the Bang for Fewer Bucks: Managing Nuclear Modernization Costs,” in _America’s Nuclear Crossroads_, pp. 1–15.



114 Eric Gomez, “It Can Get You into Trouble, but It Can’t Get You Out: Missile Defense and the Future of Nuclear Stability,” in _America’s Nuclear Crossroads_, p. 17.



115 Michael D. Griffin and Rebeccah L. Heinrichs, “Ensuring U.S. Technological Superiority: An Update from Under Secretary Michael D. Griffin,” (interview, Hudson Institute, Washington, August 23, 2019); and Patrick Tucker, “Trump’s New Missile Policy Relies Heavily on Largely Unproven Technologies,” _Defense One_ , January 17, 2019.



116 Kelsey Davenport, “The Joint Comprehensive Plan of Action (JCPOA) at a Glance,” Arms Control Association, May 2018; and Maggie Tennis, “Preserving the U.S. Arms Control Legacy in the Trump Era,” in _America’s Nuclear Crossroads_, pp. 81–84.



117 Ben Hubbard, Palko Karasz, and Stanley Reed, “Two Major Saudi Oil Installations Hit by Drone Strone, and U.S. Blames Iran,” _New York Times_ , September 14, 2019; and Dan Lamothe, “U.S. to Send 1,800 Additional Troops to Saudi Arabia to Boost Defenses against Iran,” _Washington Post_ , October 11, 2019.



118 David Barno and Nora Bensahel, “Fighting and Winning in the ‘Gray Zone,’” _War on the Rocks_ , May 19, 2015.



119 Stephen Biddle and Ivan Oelrich, “Future Warfare in the Western Pacific: Chinese Antiaccess/​Area Denial, U.S. AirSea Battle, and Command of the Commons in East Asia,” _International Security_ 41, no. 1 (Summer 2016): 7–48; and Michael Kofman, “It’s Time to Talk about A2/AD: Rethinking the Russian Military Challenge,” _War on the Rocks_ , September 5, 2019.



120 Ted Galen Carpenter and Eric Gomez, “East Asia and a Strategy of Restraint,” _War on the Rocks_ , August 10, 2016.



121 For more, see Eric Gomez, “The Future of Extended Deterrence: Are New U.S. Nuclear Weapons Necessary?,” in _America’s Nuclear Crossroads_, p. 58.



122 Terence Roehrig, _Japan, South Korea, and the United States Nuclear Umbrella: Deterrence after the Cold War_ (New York: Columbia University Press, 2017), p. 15.



123 Barry R. Posen, _Inadvertent Escalation: Conventional War and Nuclear Risks_ (Ithaca: Cornell University Press, 1991), pp. 2–3; and Caitlin Talmadge, “Would China Go Nuclear? Assessing the Risk of Chinese Nuclear Escalation in a Conventional War with the United States,” _International Security_ 41, no. 4 (Spring 2017): 53–55.



124 For information about technical developments that blur the nuclear/​conventional distinction, see James M. Acton, “Escalation through Entanglement: How the Vulnerability of Command‐​and‐​Control Systems Raises the Risks of an Inadvertent Nuclear War,” _International Security_ 43, no. 1 (Summer 2018): 63–65. On military strategies that increase escalation risks, see Talmadge, “Would China Go Nuclear?,” p. 53; and Tong Zhao and Li Bin, “The Underappreciated Risks of Entanglement: A Chinese Perspective,” in _Entanglement: Russian and Chinese Perspectives on Non‐​Nuclear Weapons and Nuclear Risks_ , ed. James M. Acton (Washington: Carnegie Endowment for International Peace, 2017), pp. 58–59.



125 Gomez, “It Can Get You into Trouble, but It Can’t Get You Out,” pp. 25–28.



126 For a brief explainer on what a space sensor layer for missile defense could look like, see “Missile Defense Tracking System, Space Sensor Layer (SSL), Hypersonic and Ballistic Tracking Space Sensor (HBTSS),” Glob​alse​cu​ri​ty​.org, https://​www​.glob​alse​cu​ri​ty​.org/​s​p​a​c​e​/​s​y​s​t​e​m​s​/​h​b​t​s​s.htm.



127 William J. Burns, “The Demolition of U.S. Diplomacy: Not Since Joe McCarthy Has the State Department Suffered Such a Devastating Blow,” _Foreign Affairs_ , October 14, 2019, https://www.foreignaffairs.com/articles/2019–10-14/demolition-us-diplomacy.



128 There are several issues that we did not deal with in this analysis, but the Cato Institute intends to issue defense policy and budget analyses annually, with each report focusing on three to four core challenges. Next year, for example, will include a focus on two relatively ignored aspects of the budgeting process: cybersecurity and technology, in general, and workforce issues, including retention, recruitment, and education.
"
"

My new Cato Policy Analysis, “In Pursuit of Happiness Research: Is It Reliable? What Does It Imply for Policy,” was released today. If you’re wondering why we need long papers about the hazards of happiness research, look no further than Bill McKibben’s new essay in _Mother Jones_: 



According to new research emerging from many quarters, … our continued devotion to growth above all is, on balance, making our lives worse, both collectively and individually. Growth no longer makes most people wealthier, but instead generates inequality and insecurity. Growth is bumping up against physical limits so profound—like climate change and peak oil—that trying to keep expanding the economy may be not just impossible but also dangerous. And perhaps most surprisingly, growth no longer makes us happier.



There’s about five kinds of wrong in this one short passage. One of them is generated by the fact that McKibben is apparently ignorant of the most recent work on happiness — much of which directly contradicts his claim that we are not getting happier with growth. You’ll find the up‐​to‐​date scoop in my new paper. (Here’s a bite‐​sized taste.)   
  
  
If you’re worried about this whole business about measuring happiness and using the results to determine public policy, you’re not alone. Darrin McMahon in the elegant lead essay of this month’s _Cato Unbound_ casts a skeptical eye over the entire enterprise. But in today’s installment, Swarthmore psychologist Barry Schwartz, author of _The Paradox of Choice: Why More Is Less_ comes to the defense of the politics of happiness, and argues (in the McKibben vein), that more wealth can actually make us worse off. Is it true? Tune in to _Cato Unbound_ on Friday when Ruut Veenhoven, Director of the World Database of Happiness, will drop the latest data.   
  
  
Or, if you’re anxious, you can get plenty of secondhand Veenhoven in my paper.
"
nan
"Groundwater is the biggest store of accessible freshwater in the world, providing billions of people with water for drinking and crop irrigation. That’s all despite the fact that most will never see groundwater at its source – it’s stored naturally below ground within the Earth’s pores and cracks. While climate change makes dramatic changes to weather and ecosystems on the surface, the impact on the world’s groundwater is likely to be delayed, representing a challenge for future generations. Groundwater stores are replenished by rainfall at the surface in a process known as “recharge”. Unless intercepted by human-made pumps, this water eventually flows by gravity to “discharge” in streams, lakes, springs, wetlands and the ocean. A balance is naturally maintained between rates of groundwater recharge and discharge, and the amount of water stored underground. Groundwater discharge provides consistent flows of freshwater to ecosystems, providing a reliable water source which helped early human societies survive and evolve.  When changes in climate or land use affect the rate of groundwater recharge, the depths of water tables and rates of groundwater discharge must also change to find a new balance. The time it takes for this new equilibrium to be found – known as the groundwater response time – ranges from months to tens of thousands of years, depending on the hydraulic properties of the subsurface and how connected groundwater is to changes at the land surface. Estimates of response times for individual aquifers – the valuable stores of groundwater which humans exploit with pumps – have been made previously, but the global picture of how quickly or directly Earth’s groundwater will respond to climate change in the coming years and decades has been uncertain. To investigate this, we mapped the connection between groundwater and the land surface and how groundwater response time varies across the world. We found that below approximately three quarters of the Earth’s surface, groundwater response times last over 100 years. Recharge happens unevenly around the world so this actually represents around half of the active groundwater flow on Earth.  This means that in these areas, any changes to recharge currently occurring due to climate change will only be fully realised in changes to groundwater levels and discharge to surface ecosystems more than 100 years in the future. We also found that, in general, the driest places on Earth have longer groundwater response times than more humid areas, meaning that groundwater stores beneath deserts take longer to fully respond to changes in recharge. In wetter areas where the water table is closer to the surface, groundwater tends to intersect the land surface more frequently, discharging to streams or lakes.  This means there are shorter distances between recharge and discharge areas helping groundwater stores come to equilibrium more quickly in wetter landscapes.  Hence, some groundwater systems in desert regions like the Sahara have response times of more than 10,000 years. Groundwater there is still responding to changes in the climate which occurred at the end of the last glacial period, when that region was much wetter. 


      Read more:
      The global race for groundwater speeds up to feed agriculture's growing needs


 In contrast, many low lying equatorial regions, such as the Amazon and Congo basins, have very short response times and will re-equilibrate on timescales of less than a decade, largely keeping pace with climate changes to the water cycle.  Geology also plays an important role in governing groundwater responses to climate variability. For example, the two most economically important aquifers in the UK are the limestone chalk and the Permo-Triassic sandstone.  Despite both being in the UK and existing in the same climate, they have distinctly different hydraulic properties and, therefore, groundwater response times. Chalk responds in months to years while the sandstone aquifers take years to centuries. In comparison to surface water bodies such as rivers and lakes which respond very quickly and visibly to changes in climate, the hidden nature of groundwater means that these vast lag times are easily forgotten. Nevertheless, the slow pace of groundwater is very important for managing freshwater supplies.  The long response time of the UK’s Permo-Triassic sandstone aquifers means that they may provide excellent buffers during drought in the short term. Relying on groundwater from these aquifers may seem to have little impact on their associated streams and wetlands, but diminishing flows and less water could become more prevalent as time goes on.  This is important to remember when making decisions about what rates of groundwater abstraction are sustainable. Groundwater response times may be much longer than human lifetimes, let alone political and electoral cycles."
"The Trump administration has offered oil companies a chunk of the American west and the Gulf of Mexico that’s four times the size of California – an expansive drilling plan that threatens to entrench the industry at the expense of other outdoor jobs, while locking in enough emissions to undermine global climate policy. Energy companies have leased 9.9m acres from the unprecedented 461m acres put up for rent by the Trump administration, according to a new analysis from the Wilderness Society. The fossil fuels extracted from those leases could equal half a year of emissions from China, the world’s top carbon polluter. The administration has jump-started this plan, independent government analysts say, by offering energy leases at bargain rates. That has lured drilling companies to pristine lands where an outdoor economy had already grown up around wildlife and the natural landscape. Trump’s Democratic opponents vow to close public lands to new drilling. But they probably can’t stop the extraction that Trump has already started. Despite a glut in US oil supply, the federal government has proposed leasing places like the Slickrock Bike Trail in Moab, Utah, where visitors pedal through petrified sand dunes and ancient seabeds. Locals have warned the potential air and water pollution aren’t worth damaging this national treasure. “If we really get to the point that we need to burn the Picassos to heat the house for an hour, we could still do that, but there’s no reason to lease these parcels now when they have a higher and better use,” said Ashley Korenblat, CEO of Western Spirit Cycling and managing director of the not-for-profit group Public Land Solutions. The leases are being signed as world scientists stress that pollution from oil, gas and coal needs to decline rapidly to avert catastrophe. Conservationists say the developed land will never again be wild and experts have repeatedly shown the sales aren’t even earning a fair return for taxpayers, costing the federal government and states billions of dollars. “We’re in an era now where fundamental questions need to be raised about whether there should be more leasing or not. Millions of acres are already under lease that are not being developed,” said David Hayes, the former deputy secretary of the interior department under President Barack Obama. “So why is the administration going so hard and fast over putting additional acreage up?” Just as the Obama administration halted coal leasing on federal lands, Hayes argues the federal government should consider a moratorium for oil and gas. At the least, Trump officials should not allow drilling on frontier and sensitive areas, said Hayes, who is now the director of the State Energy and Environmental Impact Center at New York University School of Law. Nearly all the Democrats running for presidentwould ban fossil fuel extraction on federal land. Chase Huntley, energy and climate change director with the Wilderness Society, said the “tremendous area” the administration has offered to private companies and its disregard for the environment and climate “suggests the administration’s real interest here: which is advancing their agenda of energy dominance regardless of who it hurts”. The interior department and its Bureau of Land Management, which handles onshore leasing, did not respond to requests for comment. Neither did the trade group for the US oil and gas industry, the American Petroleum Institute. Much of the drilling on Trump-leased areas won’t happen now, while global supply is high and oil and gas prices are so low that the acres wouldn’t turn a large enough profit, but the Wilderness Society report shows that as a portion of the leases are acted upon they will be responsible for substantial pollution. On the low end, the leases could result in emissions equal to the annual output of Brazil. Pete Erickson, a senior scientist with the Stockholm Environment Institute, reviewed the analysis and called the low-end estimates “conservative”. Earth is already 1.1C hotter than it was before humans began to burn fossil fuels for industry. To keep the planet from climbing to 1.5C hotter, oil emissions would need to drop from 13 gigatons of carbon dioxide per year to less than 8 gigatons by 2030, according to an analysis by the climate change news organization Carbon Brief. Gas emissions would need to decline from 8 gigatons to fewer than 5 gigatons. In a report ordered by the Obama administration, the US Geological Survey in 2018 found that public lands account for 24% of the country’s emissions and vegetation on the same lands absorbs 15% of the nation’s carbon dioxide pollution. But the Trump administration has largely refused to consider what its actions on public lands will mean for the climate crisis. Last month the White House proposed changes to how it applies a bedrock environment law – the 50-year-old National Environmental Policy Act. Under the revisions, the government would no longer need to count climate impacts when approving infrastructure such as pipelines or leasing land for oil and gas drilling. The administration has also made revisions to rules that protected the environment but proved inconvenient for the oil and gas industry. “If there’s anything in the way of oil and gas leasing, it’s going to be pushed aside,” Hayes said, noting the reinterpretation of the Migratory Bird Act, changes to an agreement for protecting sage grouse, the shrinking of national monuments in Utah and the opening of sensitive portions of the Arctic National wildlife refuge and National Petroleum reserve to development. Independent analyses have found the US is not earning a fair amount from oil and gas leasing either. The Congressional Budget Office in 2016 concluded the federal government could increase its share of income from onshore leases by up to $1.2bn over 10 years. That figure would be about twice as large if it counted the money that would go to states. Audits by the Government Accountability Office have drawn similar conclusions that the government could demand more money. If the government had raised royalty rates years ago, it could have earned up to $12bn more for taxpayers between 2009 and 2019, according to the non-partisan fiscal policy group Taxpayers for Common Sense. States would have received about half that income. “We have policies that govern the way we lease our federal lands for oil and gas development that are a century old, and they just haven’t kept pace. And we have others that haven’t been looked at in decades. This has led to really a tremendous giveaway [to the oil and gas industry],” said Autumn Hanna, vice-president of the non-partisan fiscal policy group Taxpayers for Common Sense.  In addition to expanding leasing opportunities, the Trump administration has opened lands that were once protected from industry. Last week the interior department finalized plans that allow mining, drilling and other development on lands recently removed from Bears Ears and Grand Staircase-Escalante national monuments in Utah. The landscapes there feature “classic red rock canyons”, “forested mesas”, and “fantastical geologic features”, ranging from standing rock pillars to streams that flash-flood several times a year, according to Steve Bloch, the conservation director at the Southern Utah Wilderness Alliance. The region has a dense concentration of prehistoric sites, where researchers are able to study the cultures of indigenous tribes. Bloch said that doesn’t matter to the federal government. “It’s clear from the administration’s approach to public lands, to their energy dominance agenda, that they believe the highest and best use of these places is for fossil fuel development,” Bloch said. Of the 9.9m acres leased, 2.2m acres are in Wyoming and 1.3m acres are in Alaska. Nearly 5m acres are in the Gulf of Mexico. Adam Kolton, executive director of the Alaska Wilderness League, said “at the broadest level, what we’re witnessing is a sort of wholesale turnover of America’s Arctic to the oil and gas industry”. Kolton said drilling on public lands was once done alongside arguments that the US needed to be energy independent, but the country is now a net exporter of oil. He believes the expansion of drilling, particularly in the Arctic, will be among Trump’s most damaging and longest-lasting environmental legacies. “Right now there’s an attempt to not just roll back what was done under Obama in Alaska, but to turn back the clock half a century and look to an era where oil, mining and logging operations were unchecked,” Kolton said. "
"
Share this...FacebookTwitterHere’s a scary clip produced by some green Marxists at the German Federal Office of the Environment, who seem to have taken it upon themselves to tell the rest of us how to live. If this represents the government’s view and the target it has in mind for its citizens, then it’s awfully spooky. (Clip is in German – main points are described below).

It’s back to central planning, housing project living, and being told exactly how to live – all the failed experiments of the past, all combined and packaged as a religion that promises salvation.
Remember how the communists promised paradise for workers? This is the same pipe dream. The problem with all of it is that it is not based on the fundamental laws of economics, and so it will definitely fail. Many of the diagnoses offered in the first half of the clip are not even true, and the remedies proposed thereafter contradict each other.
The makers of this clip are sure that the world’s problems are because we are all behaving badly, and that we have to be made to behave differently – dictated – in a way that suits their world view. The world is threatened, says the film, by population growth and our consumption, and demands a system of redistribution.
Here are some of the main points:
 1. We are all spoiled. We cannot, or simply do not want to, go without the things we use daily. We are using more than what nature is capable of giving. We are using much too much. We are living way beyond our means. 
2. We are addicted to a huge array of products that we believe make our lives easier. We are leading lives of profligacy, which requires gigantic amounts of resources like, water, raw material and energy. The planet will soon be depleted. 
The disgruntled among us have been saying this for at least 2 centuries. Yet, everyday we keep finding more than we need. Things are better today than ever.
3. The guilty parties are North America and Europe. The industrialised countries use more than their fair share. Each German uses 60 tons of material annually, Americans 130 tons. This can no lonager be tolerated. 
This is called prosperity. Obviously a concept that the clip’s makers have an aversion to. Note that most material is not consumed – but used. It doesn’t disappear.
4. By 2030, 20% of the most important raw materials will reach their production limits. 
5. All this consumption is leading to many things, particularly climate change and species extinsction. 
6. Expanding deserts, multiplication od disaeases, wars and resource shortages are threatening future generations.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The clip doesn’t say which materials. And actually deserts are shrinking. They tend to grow when the planet cools.
7. People in industrialised nations are only able to be so wasteful because people in other poor countries have to go without raw materials.
8. Our level of prosperity is not possible for everyone. 
That’s false. It’s people in developing countries that need to learn how to use raw materials. This comes with free markets and education. Any normal person with a 4th grade education or more knows that the planet gets destroyed in undeveloped countries, and protected in developed countries.  The solution is for the rest of the world to live like we do. The clip claims:
9. We have to change our thinking radically. Only through a drastic reduction in our consumption of resources through dematerialisation will we be able to sustainably secure our future. We have to consume less.
10. For example, many of the things that we seldom use can be borrowed among friends. Things like the electric drill, lawnmower. We have to redesign our tools so that they have longer lifetimes, or can be repaired, easily dismantled and to recycle.
This is so stupid – economic ignorance now in full display. Imagine if we designed a long life copy machine to last 20 years. It would be obselete in three years, and so it would wind up consuming too much energy and operating too slowly during the remaining 17 years. This film was made by technical and economic illiterates.  The clip then suggests we need to:
11. Use public laundermats, buy second hand clothes, ride bicycles and have fun at “sustanaible parties”. 
WTF? Yeah right, When I was young I worked years with the target of getting away from having to do that. This is back to Soviet living. And the clip says we ought to think about using kooky ideas like:
12. Wind-powered ships, wind-powered buildings.
13. Powering everything with renewables.
Get ready to pay a fortune for an unreliable and primitive source of energy. Indeed get ready to fork it all over. The film then has the temerity to say:
14. Today’s prices do not reflect the ecology. That has to change.
15. Taxes on resources have to go up!
Yes, they want to take everything away from us. Everybody has to get along with much less – except for the state, of course. For them it’s more! more! more! Now we know what all this is about – state power. Power and wealth to the state, and not the individual. This clip is an example of the propaganda, one that despises humanity, that we are getting in Germany for our tax euros.
Folks, you’ve got to start talking to your local and regional politicians and business leaders about this. The higher-ups are all drugged up with their “let’s-take-over-and-save-the-planet” fantasies. It has to start from the botrtom up, like the tea party candidates.  
It’s not the people that “need to drastically change their thinking”. It’s the intoxicated politicians that do.
 
 
Share this...FacebookTwitter "
"

There’s a war on. And it cannot be successfully prosecuted without a delicate combination of U.S. leadership and international cooperation. Anything that threatens either threatens all. Unfortunately, two upcoming environmental issues do both.



Although it seems a lifetime, it was a bit over two months ago when European leaders couldn’t say enough bad things about President Bush, when he wisely said “no” to the United Nations’ Kyoto Protocol on global warming. That was during the last “Conference of the Parties” (COP) to the treaty, in Bonn, Germany. Bush knows the whole thing costs a fortune and, even if climate change is a big deal, the Protocol has no detectable effect on anything but our economy. Hopefully, the events of the last month provide a bit of needed perspective on how big this deal is.



In their obsession with pleasing radical greens (who just got clobbered in a Hamburg election), the euros modified Kyoto in Bonn. The purpose was to get the Japanese to go along, which will satisfy a requirement necessary to make it legally binding. In doing so, they made the climatically inconsequential treaty more irrelevant. They also left out a lot of details that need to be cleaned up at the next COP.



That COP begins on October 29 in, of all places, Marrakech. Last July, the United States intimated that despite our opposition to Kyoto we would attend and maybe pony up a new plan.



Let’s kill this meeting now, before it harms our wartime alliance. All it will do is re‐​open old wounds. And don’t even think about the security issues of getting a few thousand people from the hated West together in an Islamic nation, however friendly and moderate the current leaders might be.



The consequence of alliance‐​building is that we’ve been offering some pretty generous terms, such as debt‐​forgiveness and dropping of economic sanctions. We can only hope that our European allies didn’t make any demands about Kyoto. And if we don’t go to Marrakech, that will be a very good sign that they didn’t.



The second green threat to the war effort is on the home front. Career bureaucrats at the Environmental Protection Agency don’t seem to get it: This is not the time to saddle the nation with new rules that could constrict our energy security.



Right now, they’re busily working away on new regs to restrict a basket of three emissions: sulfur dioxide, nitrogen oxides and airborne mercury. The first two are substantially regulated already, and no one has been able to find one death from the last one. But we know that mercury is toxic, and despite the obvious lack of morbidity and mortality, so the logic goes: We must regulate.



The problem is that while sulfur and nitrogen oxides are already largely scrubbed from coal combustion, apparently the only way to keep the mercury out of the air is to stop burning it. Coal produces over 50 percent of our electricity. We have so much of it in the ground (despite attempts by the Clinton Administration to lock it up, as it did in Utah) that we are called the Saudi Arabia of coal.



In a war, prudence plans for the worst contingencies. One of those is that things can and do go bad, and that we could wind up with an embargo — in reality or de facto — on Middle Eastern oil. A supertanker is a wonderful target for, say, an airliner‐​cum cruise missile. Not likely, you say?



In 1973, oil was embargoed, fuel prices skyrocketed, and soon after, the economy went into a sharp and severe recession. Recall that the shock was so great that one of his first acts as president was for Jimmy Carter to declare that energy security was “the moral equivalent of war.” Now, consider prosecuting a real war in this environment.



In the worst‐​case scenario, if supplies are restricted, the United States can buy more oil — at potentially great expense — from non‐​Arab vendors. But how does it get shipped, once one supertanker blows up? In that world we can formulate alternative fuels out of the strategically inexhaustible coal. No one likes the idea. It’s dirty, but so is war, where a lot of things can happen that no one likes. Let’s not place this industry in harm’s way before such an effort might be required.



Yes, there’s only a small chance of this. But it is larger than the probability of the events of September 11.



The point is simple: It’s time to back off on things like Kyoto and regulations that have the potential to tie our hands in any way. Halfhearted war efforts are for losers.



Finally, for those concerned about ultimate environmental degradation, take solace. Big wars, however noble, leave behind big governments and even bigger alliances, which will be only too eager to impose all kinds of new regulations once the smoke clears away. The residuum from the last one is called the United Nations, which gave us the Kyoto Protocol.
"
"BP’s new chief executive has set an ambitious target to shrink the oil firm’s carbon footprint to net zero by 2050 by cutting more greenhouse gas emissions every year than produced by the whole of the UK. Bernard Looney, who replaced Bob Dudley as chief executive this month, said it was clear that BP needed to change. He said BP would aim to become a net zero company by 2050 or sooner by tackling “all the carbon we get out of the ground as well as all the greenhouse gases we emit from our operations”. BP is following the lead of other large oil firms by setting a target to reduce its contribution to the climate crisis, which will require the company to remove more than 400m tonnes of carbon emissions a year from its oil and gas business. Looney expects BP will “invest more in low-carbon businesses – and less in oil and gas – over time”, but will not set out the detail of how BP plans to meet the goals until an investor meeting in September. “Today is about a vision, a direction of travel,” he told an audience of investors and industry analysts. “I appreciate you want to see more than a vision. We don’t have that for you today, but we will in September. The direction is set. We are heading to net zero. There is no turning back.” The former head of BP’s oil production business said BP would still be producing oil and gas in 2050, but less than it produces today. The net zero strategy will require BP to cut or offset around 360m tonnes of greenhouse emissions created by the oil and gas it produces every year through measures such as tree-planting, and carbon capture technologies. Looney also assured investors that BP would safeguard the $8bn paid out in shareholder dividends every year by becoming “a force for good as well as a provider of competitive returns”. Helge Lund, the chairman, said: “The board supports Bernard and his new leadership team’s ambition for BP. Aiming for net zero is not only the right thing for BP, it is the right thing for our shareholders and for society more broadly.” BP’s green goals have divided environmental campaigners. Some welcomed the company’s ambition; others criticised the absence of a clear strategy. Murray Worthy, a senior campaigner at Global Witness, said BP’s net zero pledge “looks like an attempt to grab some positive headlines by a new CEO but with little of substance to show how it will achieve these grand claims. “Saying that they will invest more in low-carbon tech and less in oil and gas ‘over time’ is not a credible plan for reaching net zero.” The ambitions were welcomed by shareholder pressure group Climate Action 100+, the Institutional Investors Group on Climate Change (IIGCC), green investor body Follow This and the Church of England Commissioners, which have all collaborated with BP’s executives to help tackle the climate crisis. Stephanie Pfeifer, a member of Climate Action 100+ and the chief executive of the IIGCC, said investors would continue to look for progress from BP including how it would invest more in non-oil and gas businesses and ensure its lobbying activity supports delivery of the Paris agreement. An investigation by the Guardian last year revealed that the company planned to grow its production of oil and gas by about a fifth between 2018 and 2030, despite warnings that an increase in fossil fuel production would put the world on track for catastrophic global heating and a runaway climate crisis. “I know many may doubt our intentions, based on seeming inconsistencies between what we say and what we do. I get that,” Looney said. “We are taking steps to more firmly and visibly align our intentions with our actions and become much more transparent.”  BP said it aimed to play a more active role in lobbying for policies that would spur action on the climate crisis, and would cut its spending on corporate reputational sponsorship and redirect the funds towards promoting climate action. Looney set out the plans for a greener BP alongside one of the most radical corporate overhauls for the company in decades. The shakeup will replace BP’s traditional distinction between upstream, which produces oil and gas, and downstream, which refines the fossil fuels to sell petroleum products. Instead there will be four new business areas: production and operations, customers and products, gas and low-carbon energy, and innovation and engineering. “We’ll still be an energy company but a different kind of energy company,” Looney said."
"
Share this...FacebookTwitter
Only 8 days to go!
A new paper published today in Atmospheric Chemistry and Physics suggests that the relationship proposed by Henrik Svensmark is supported. Svensmark proposes that changes in the sun’s magnetic field modulate the density of Galactic Cosmic Rays (GCRs) which in turn seed cloud formation on Earth, which changes the albedo/reflectivity to affect Earth’s energy balance and hence global climate. Read more here at WUWT:
Cosmic rays linked to rapid mid-latitude cloud changes, B. A. Laken , D. R. Kniveton, and M. R. Frogley.
This will make the 3rd International Energy and Climate Conference that much more interesting as Svensmark himself will be one of the distinguished speakers, along with Fred Singer and others.

Roster of Speakers
Prof. Dr. Fred Singer, USA, Atmospheric Physicist, Chairman IPCC
Prof. Dr.Dieter Ameling, Former President of Business Union Steel and Chairman Stahl institute VDEh.
Prof. Dr. Robert Bob Carter, Australia, Geologist
Prof. Dr. Vincent Courtillot, France, Geophysicist
Prof. Dr. Karl-Friedrich Ewert, Germany, Geologist
Prof. Dr. Ian Plimer, Australia, Geologisist
Prof. Dr. Werner Kirstein, Germany, Dipl. Physicist & Geography
Prof. Dr. Horst Lüdecke, Germany, Press Spokesman for EIKE
Prof. Dr. Nir Shaviv, Israel, Astrophysicist
Prof. Dr. Henrik Svensmark, Denmark, Atmospheric Sciences
Prof. Dr. Jan Veizer, Canada, Paleo-geologist
Dr. Emmanuel Martin, France, Economist
Dr. Horst Borchert, Germany, Physicist
Dr. Lutz Peters, Germany, Author of Klima 2055
Dipl.-Ing. Michael Limburg, Germany, Vize President EIKE
Dipl. Meteorologist Klaus Puls, Germany
Günther Ederer, Germany, Business Journalist and Film Producer


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Climate conference location
Berlin, Germany
10117 Berlin Mitte, only a few steps away from the S-Bahn and Metro stations Friedrichstraße.
Conference dates and times
Starts Friday 3 December 2010 at 1 p.m. and ends at approx 8 p.m.
Starts Saturday 4 December 2010 at 8 a.m. and ends at approx 4 p.m.
Conference fees
120€ private participant (70€ day ticket)
250€ company representative
220€ for a student sponsored by company
Please book before 26 November 2010. After that a 30€ surcharge gets levied.
Info, registration, tickets
Email: info@berlinmanhattan.org
Fax: (+49) 30 69 20 800 39
More information here: 3rd International Energy and Climate Conference.

Share this...FacebookTwitter "
"

Since 9/11, the U.S. government has poured a breathtaking amount of resources into investigating suspected terrorism operations within the United States. A key component of these investigations is known as “ghost‐​chasing” — the thousands of leads and tips investigated daily, and classified as “threats,” despite the fact that only one in 10,000 fails to be false.



These efforts are often criticized on the basis of civil liberties abuses. But, convinced that terrorism is an “existential” threat, many people are perfectly comfortable overlooking these abuses. In their new book, _Chasing Ghosts: The Policing of Terrorism_ , Cato’s John Mueller and Mark G. Stewart of the University of Newcastle, Australia, take aim at the very premises of U.S. counterterrorism operations. Is terrorism truly a significant threat? Are most would‐​be terrorists actually skilled enough to pull off an attack? Is it true that we can “never be safe enough”? Mueller and Stewart examine the methods of the FBI, National Security Agency, the Department of Homeland Security, and local policing agencies, revealing the government’s exaggerated claims about the “threats” they divert. The question, they write, is not whether any real terrorists exist — but whether the chase is worth the cost.



 **You Might Be a “Lukewarmer” If …**  
When it comes to global warming, most people think there are two camps: “alarmist” or “denier” being their respective pejoratives. Either you acknowledge the existence of manmade climate change and consider it a dire global threat, or you deny it exists at all. But there’s a third group: the “lukewarmers.” As Cato scholars Pat Michaels and Paul C. Knappenberger write in their new ebook, _Lukewarming: The New Climate Science that Changes Everything_ , “Lukewarmers believe the evidence of some human‐​caused climate change is compelling, but it is hardly the alarming amount predicted by models.”



Lukewarmers are skeptical that government pacts, like those sought at the 2015 United Nations Climate Change Conference in Paris, will do much to temper climate change’s effects. They also tend to question the incentive structure of climate science, where scientists are vying for millions of dollars of government funding — meaning that any proposal that global warming’s effects have been overforecast “threatens to derail everyone else’s gravy train.” This, they argue, has brought about “a systemic distortion in the direction of alarmism.” Lukewarmingtells a different story — one that ends with optimism. “Lukewarmers know,” they write, “that economic development is the key in adaptation to the vagaries of weather and climate, even climate change induced by people.”



 **Property Rights after _Kelo_**  
When Cato published the first edition of _Cornerstone of Liberty: Property Rights in 21st Century America_ , the infamous Supreme Court case of _Kelo v. New London_ had only recently been decided, declaring that the government can seize private property by eminent domain under a broad definition of “public use.” In the decade since, by one estimate, the government has taken over a million homes from their owners. Cato adjunct scholar Timothy Sandefur of the Pacific Legal Foundation and his wife Christina, vice president for policy at the Goldwater Institute, set out to revise the book for its second edition — but, as they write, “So much has happened in the years after _Kelo_ that what started as a simple update to this book became a complete renovation.”



As in the first edition, the Sandefurs narrate the heartrending stories of Americans forced from their homes, explaining along the way how property rights became eroded. But this updated edition also contains a wealth of new material on the ever‐​changing threats to property owners. The Sandefurs conclude by examining the backlash from _Kelo_ and suggesting a new path forward. As Washington Post columnist George Will wrote, “Not since Babe Ruth and Lou Gehrig has there been a one‐​two punch quite like Timothy and Christina Sandefur. Both lawyers. Both authors. Both helping shape the country.”



 **Paving the Path to Growth**  
“If you could wave a magic wand and make one or two policy or institutional changes to brighten the U.S. economy’s long‐​term growth prospects, what would you change and why?” Brink Lindsey, Cato’s vice president for research, posed this question to 51 prominent economists and policy experts for his ebook _Reviving Economic Growth_.



Their ensuing essays constitute a “brainstorming” session from an eclectic group of contributors, featuring libertarian, progressive, and conservative perspectives. “By bringing together thinkers one doesn’t often see in the same publication,” writes Lindsey, “my hope is to encourage fresh thinking about the daunting challenges facing the U.S. economy — and, with luck, to uncover surprising areas of agreement that can pave the way to constructive change.”



In a second ebook, _Understanding the Growth Slowdown_ , Lindsey and his contributors dive yet again into the pressing questions surrounding the disappointing performance of the U.S. economy in recent years. Lindsey asks whether this could be more than a temporary trend, but rather the “new normal” — and if so, why. “The U.S. economy is a phenomenon of mind‐​boggling complexity,” Lindsey observes. These collected essays don’t aim to provide all the answers, but to provoke new ideas — without which an economic revival will certainly not be possible.
"
"

 **The oceans are heating up 40% faster than scientists realized** screamed _Business Insider_ last Saturday (January 12). Two days earlier _The New York Times_ broke the story with “the oceans are heating up 40 percent faster on average than a United Nations panel estimated five years ago.” It’s all from a January 10 article in _Science_ by Lijing Cheng, of the Chinese Academy of Sciences in Bejing, along with three American coauthors, titled “How Fast are the Oceans Warming?”   
  
  
Scary. Not. “40 percent” is a straw man.   
  
  
The subject of all this attention is the change in the heat content of the world’s oceans. This is obviously related to their temperature—something that has proven rather difficult to measure precisely on the centennial scale because of changes in measurement techniques and data sources. (Quants: heat (in joules) divided by the heat capacity (joules required to warm the ocean a degree) gives temperature change).   
  
  
At the outset, it’s important to note that this is not an original research article. It’s a “Perspectives” piece, kind of like a sciency op‐​ed that cites a collection of refereed publications (in this case, with a large number of self‐​citations) that determine the “perspective” of the writers. Quoting from Dr. Roy Spencer’s blog on January 16:   




For those who read the paper, let me warn you: The paper itself does not have enough information to figure out what the authors did…



Further, Spencer notes:   




One of the conclusions of the paper is that Ocean Heat Content (OHC) has been rising more rapidly in the last couple decades than in previous decades, but this is not a new finding, and I will not discuss it further here.   
  
  
Of more concern is _the implication that this paper introduces some new OHC dataset that significantly increases our previous estimates of how much the oceans have been warming._   
  
  
As far as I can tell, **this is not the case**.



The “United Nations panel” in the first paragraph is, of course, its Intergovernmental Panel on Climate Change (IPCC), and in their most recent (2013) science compendium they most certainly did **not** estimate that the heat content of the ocean is 40% less than what it is from Cheng et al’s “perspective.” In that report, they noted five different publications, but found problems with four of them and only conferred credibility upon the highest figure, published by Dominguez et al. in 2008. _The Cheng et al. study is only 11% higher than that, not 40%_. To repeat, the average of the five studies mentioned in the 2013 IPCC report is 40% below the new Cheng et al. figure, but the one that the IPCC found most credible in fact differs from Cheng et al. by only 11%.   
  
  
The 40% figure is therefore a straw man.   
  
  
It’s also noteworthy that the “40 percent” claim is nowhere in the _Science_ Perspective. It’s from a guest post by Cheng et al. in “Carbon Brief,” principally funded by the European Climate Foundation, which describes itself as “a major philanthropic initiative to help Europe foster the development of a low‐​carbon society and play an even stronger international leadership role to mitigate climate change.”   
  
  
_Another Perspective_   
  
  
It is obviously very important to understand historical changes in ocean heat content. Another way to do this would be with the new “reanalysis” data sets, which combine heretofore separate atmospheric observations in the past via a dynamic model. Obviously as one goes further back in time, important data, such as vertical weather balloon soundings drop out, as they did in the 1930s. One important note: the model is modulated with the changes in atmospheric radiation consistent with human emissions of greenhouse gases, ozone, and aerosols, as well as changes in solar radiation.   
  
  
(The relevant paper is by Patrick Layloyaux of the European Center for Medium‐​Range Weather Forecasts, the same people who produce the daily “Euro” model that mid‐​Atlantic forecasters love so much in snow situations. He has 14 co‐​authors, with the majority being from the ECMWF.)   
  
  
Here’s what the ECMWF simulates for the historical heat content (in Joules/​square meter) of the upper 300 meters (984 feet) of the globe’s oceans:   






  
  
  
_Oceanic heat content (joules/​square meter) or the upper 300 meters of the ocean. From Layloyaux et al., 2018._   
  
  
Somehow “ocean heat content as high as it was 75 years ago” isn’t quite so alarming. 
"
"Evidence of the devastating impacts of anthropogenic climate change are stacking up, and it is becoming horrifyingly real. There can be no doubt that the climate crisis has arrived. Yet another “shocking new study” led The Guardian and various other news media this week. One-third of Himalayan ice cap, they report, is doomed. Meanwhile in Australia, record summer temperatures have wrought unprecedented devastation of biblical proportions – mass deaths of horses, bats and fish are reported across the country, while the island state of Tasmania burns. In some places this version of summer is a terrifying new normal. The climate disaster future is increasingly becoming the present – and, as the evidence piles up, it is tempting to ask questions about its likely public reception. Numerous psychological perspectives suggest that if we have already invested energy in denying the reality of a situation we experience as profoundly troubling, the closer it gets, the more effort we put into denying it. While originally considered as a psychological response, denial and other defence mechanisms we engage in to keep this reality at bay and maintain some sense of “normality” can also be thought of as interpersonal, social and cultural. Because our relationships, groups and wider cultures are where we find support in not thinking, talking and feeling about that crisis. There are countless strategies for maintaining this state of knowing and not-knowing – we are very inventive. The key point is that it prevents us from responding meaningfully. We “succeed” in holding the problem of what to do about the climate crisis at a “safe” distance. As the crisis becomes harder to ignore – just consider the current batch of shocking reports – individually and culturally we will dig deeper to find ways to strategically direct our inattention. The standard narrative for a piece like the one I’m writing here, as a social scientist, is to now say something about how the crisis could be better communicated. The billion-dollar question, of course, is whether this most recent disaster can be used to motivate real change. No doubt it is important to keep this kind of commentary up. It is key that we consider how to give the climate crisis traction in a culture so accomplished at distancing us from uncomfortable realities.  But let’s be honest. No one really knows what works. We have never been here before. And I’m starting to think that more of this kind of analysis is, perversely, another example of distancing us from that crisis. Intellectualising terrifying climate crisis stories as an issue for “communicators” and “the public” is another way of detaching ourselves from their reality, from the relevance to me and you. So let’s cut through all that and stop invoking an imaginary audience. Many terrible things are happening as a result of climate change – their happening is being reported. How are you receiving it? How does it feel? Are you shocked, horrified, scared, bored, tired? What do you do with the terror? Do you compartmentalise it somewhere “safe”? Perhaps like me, you know you care. You attach importance to climate change, you want to act correctly, avoid risking other lives, damaging homes and habitats. Perhaps you know you are scared too – scared of contemplating what we have already lost or of what will happen as the crisis gets closer still. Scared of what you are being asked to give up. Add in some residual guilt and you might then engage in a defence of some kind, consciously or otherwise – telling yourself that others are more responsible, there is nothing we can do, everybody else seems to be carrying on as normal. As the crisis deepens, the walls close in, you might double down on those defences. So where do we go from here? How might this knowledge help us – you and me? We must make a commitment, but not of the kind you might imagine. The shocking reality of the climate crisis is making its way into the webs of everyday life, emotions, thought processes, relationships, hopes, dreams and fears. Perhaps we should commit to letting it, as an alternative to doubling down on our denial.  We can do this individually, but more important is collectively acknowledging our fears about actual and anticipated losses. Fears about the loss of species and habitats, but also our established ways of life. This leads to more constructive questions, about what we want to hang on to, what are our obligations? I don’t have ready answers to these questions, but I am still confident we can find ways to keep doing the things we really care about – for ourselves, each other, the places we live in. But we need to talk about these choices. Such a process is still miles apart from many “sustainability” agendas. Halting the climate crisis is still predominately framed as a matter for individual choice and change – use less plastic, cycle to work, fly less. But the behavioural response required is way more complicated than that. When it comes to the climate crisis, the personal is political. I am talking about a politics that grows from opposition and critique of our current systems. This is evident in young people organising school strikes and protesters willing to get arrested for their direct action. But we also need to pay more attention to what is lost, to who and what we care for, to other possible ways of being.  Some conservation scientists, at least, see recent cultural change as a hopeful sign of a growing sense of care and responsibility. So stop feeling guilty, it’s not your fault. Be attentive to what’s going on, so that you might notice what you care about and why. What are you capable of, and what might we be capable of together, when we aren’t caught between knowing and not knowing, denial and distress?  See what obligations emerge. There are no guarantees. But what else do we do?"
"

from the Calgary Herald: Canadian mini-satellite may solve carbon puzzle (h/t to WUWT reader “Freezedried”)
Tom Spears Canwestnews Service
Friday, February 27, 2009
While NASA lost a $285-million US satellite this week, a Canadian microsatellite that does the same job is chugging along happily in orbit –at 1/1,000th the cost.
The 30-centimetre-long University of Toronto satellite is searching for the “missing” carbon dioxide–the vast amount of Earth’s main greenhouse gas that somehow vanishes each year.
That’s what NASA’s OCO(orbiting carbon observatory) satellite would have done, if it had survived launch on Tuesday. The big difference: Canada built and launched its tiny version for $300,000.
The OCO launched but failed to reach orbit. (see WUWT story here)
The CanX-2 micro satellite, shown slightly smaller than actual size (10 x 10 x 34 cm) 
Details on the hardware are here
Meanwhile, the U of T’s CanX-2 is cruising 700 kilometres above Earth “and functioning really well,” after some glitches that followed its launch last April, said Ben Quine, the director of space engineering at York University–which made an instrument aboard the tiny CanX. Its job, like OCO’s, is to find Earth’s missing greenhouse gas.
“The measurement principle is almost exactly the same as the one for the OCO,”he said. “It’s very sad when you lose a spacecraft, but it also means that we are the only people in orbit with one-kilometre resolution on the ground.”
That means York’s Argus instrument can look at details below. A Japanese satellite does the same job, but can’t look at features less than 10 kilometres wide.
The problem is that where carbon dioxide comes from, and where it is sucked out of the atmosphere, remains poorly understood.
“Clearly, if we’re going to do something about climate change, we need to understand where CO2 is produced and particularly where it’s absorbed.That’s much less clear,” Quine said.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e97edcb62',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The Great Barrier Reef could be about to experience its most widespread outbreak of mass coral bleaching ever seen, according to an analysis from the US government’s National Oceanic and Atmospheric Administration. But the analysis, seen by Guardian Australia, says while bleaching could hit the entire length of the world heritage-listed reef, the impacts may not be as intense as previous major outbreaks.  Pockets of bleaching are being seen in areas including Lizard Island, north of Cooktown, and more than 1,100 kilometres south-east at Heron Island, off Gladstone. On Thursday, the Great Barrier Reef Marine Park Authority, conservationists and scientists warned that heat stress was building along most of the 2,300-kilometre reef along Queensland’s north coast. Temperatures would need to drop from current levels over the next two weeks if the reef was to avoid a third mass bleaching event in five years. Dr William Skirving, of Noaa’s Coral Reef Watch, prepared a briefing on the “status of heat stress on the GBR” on 19 February. The agency’s observations and model forecasts suggested that “2020 is likely to be the most extensive coral bleaching event that we have seen so far” on the reef. He told Guardian Australia: “We are pretty confident that it’s likely there will be bleaching right up and down the reef and you will find very few reefs that don’t have some bleaching.” While bleaching would be extensive, the intensity of the heat may not reach previous major bleaching events, Skirving said. In 2016 and 2017, back-to-back bleaching killed about half the reef’s corals. He said: “I’m expecting a bit of mortality but I don’t think this will get to the levels where we have seen large-scale mortality.” Corals bleach if they sit in unusually warm waters for long periods. The algae that provides food and the coral’s colour separate from the animal, leaving behind a visible white skeleton. Severe bleaching can kill some corals, and weaken others. Extreme heat can also kill corals almost immediately. Skirving’s analysis identified a key period from 26 February to 4 March when widespread bleaching was likely to hit. During this time, tides would be weak, meaning there was less mixing of the waters that help dissipate heat through the water column. At the same time, the unusually high ocean temperatures would remain. He said: “We will see temperatures rocket through through that period. But if it does get really bad is yet to be seen – it depends on rain and wind – and neither of those can be precisely predicted.” His analysis said only a significant weather event “such as a cyclone” during that eight-day period would save the reef from widespread bleaching. The world’s oceans have taken up about 93% of the extra energy caused by rising levels of greenhouse gases in the atmosphere, according to the UN’s climate science panel. According to the panel, it had high confidence that tropical coral reefs “are projected to reach a very high risk of impact at 1.2°C [of global warming], with most available evidence suggesting that coral-dominated ecosystems will be non-existent at this temperature or higher.” Prof Ove Hoegh-Guldberg, a marine biologist at the University of Queensland, whose early work helped to explain the link between coral bleaching and climate change, said in the past two weeks, “the risk factors for major bleaching events, like elevated water temperatures” had “increased dramatically”. Skirving said it appeared large-scale bleaching events were happening more frequently on the reef, and listed 1983, 1987, 1998, 2002, 2016, 2017 and “now likely 2020”. While reefs could “bounce back” from bleaching events, there was concern individual reefs would not have enough time to recover between each outbreak. Dr Lyle Vail, director of the Australian Museum’s Lizard Island Research Station, said on Friday there was already bleaching on some corals there. “Alarm bells are ringing, we’re in troubled waters,” he said. Lizard Island was badly hit by bleaching in 2016. Vail added: “The coral recovery was coming along nicely so it’s hard to see it bleaching again, fingers crossed for some cloud cover and rain to cool things down.” Several scientists working at Heron Island and the nearby One Tree Island have also been sharing images of corals beginning to bleach. Dr Tracy Ainsworth, a coral biologist at UNSW, has been with a team of scientists at a research station on Heron Island since 15 January. “We have been recording bleaching in the lagoon and reef flat and some exposed areas of the reef, and that’s concerning. A lot of the reef is fine though in the wave-exposed areas.” I've been on Heron Island for a month measuring community calcification and production with our team @CoralTrace @BillLeggat1 @jesseb3rg @thedivinggoose. Sadly, we've seen corals in the shallows begin to bleach and are monitoring the situation closely @UNSWScience @UON_research pic.twitter.com/su0vSPK0q1 Early signs of #coralbleaching at shallow sites on #MaggieIsland on Saturday. Seawater temperatures peaked at 31.7 C at the @aims_gov_au weather station in Cleveland Bay. #itsgettinghotinhere https://t.co/pCai9FmNvi pic.twitter.com/aBOp66d1VR Several colonies of mostly Acropora showing signs of #coralbleaching in the lagoon of #OneTreeIsland in the Southern #GreatBarrierReef. Observed during fantastic coral field work with Emily Howells and Andy Davis pic.twitter.com/jmUYLOgJMS"
nan
nan
"Tim Flannery (The age of the megafire is here, and it’s a call to action, Journal, 7 February) writes: “As far as swift climate action is concerned, all good choices have gone up in smoke”. That may not be the case, however. There has been abundant support by now for the claim made by Martin Fleischmann and Stanley Pons in 1989 to have observed nuclear fusion at ordinary temperatures, but the hope that such a fossil-fuel-free process might contribute usefully to energy production has not been fulfilled because it is very unpredictable, and we do not as yet know the conditions needed to produce large amounts of energy. Suitably funded research on a large scale might lead to a resolution of this issue.Prof Brian JosephsonEmeritus professor of physics, University of Cambridge  • Join the debate – email guardian.letters@theguardian.com • Read more Guardian letters – click here to visit gu.com/letters • Do you have a photo you’d like to share with Guardian readers? Click here to upload it and we’ll publish the best submissions in the letters spread of our print edition"
"
Given the thousands of comments made here weekly, I’ve decided to add a new feature to WUWT: Quote of the Week. It will be posted on Sundays.

A commenter on WUWT summed up Earth Hour in a succinct  way:
I will be thinking about the 1.8 billion people on Earth who have no  access to electricity, and how insane they must think we are.
From commenter “007” on the WUWT Poll: What are you going to do for “Earth Hour”? thread.
Anyone that wants to submit a better feature logo that the simple one I cobbled together above is certainly welcome to do so. – Anthony
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e978cbff6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"

The heinous bombing of the Alfred P. Murrah federal building in Oklahoma City has understandably raised public fears of terrorism. As is common after sensational crimes, some persons have revived their call for a bigger federal government and a narrower Constitution. This Policy Analysis examines various restrictions on civil liberty which have been proposed as a response to Oklahoma City.



Since draconian legislation is sometimes justified as being what the people demand, two points should be kept in mind. First, a large fraction of the population, not just a tiny fringe, is afraid of the federal government. According to a recent Gallup poll, 39 percent of Americans believe “the federal government has become so large and powerful it poses an immediate threat to the rights and freedoms of ordinary citizens.” If the word “immediate” is omitted, 52% of the population is afraid of the federal government.



Contrary to stereotypes about “angry white men,” people fearful of misuse of federal power tend to be female more than male, black more than white, and liberal more than conservative. Repressive measures, rather than reassuring the American public, will intensify the fears which are already widely shared.



Second, in the aftermath of a tragedy, it is not hard for insta‐​polls to report large majorities in favor of various repressive measures, especially when described at a high level of generality, with all the repressive details left out. (For example, “Should the government have more power to keep an eye on terrorist groups?” will obtain higher poll numbers than “Should the government be allowed to infiltrate non‐​violent, non‐​criminal dissident groups and be allowed to use wiretaps without a court order”?) In the long run, public officials are expected to exercise judgement, and not blindly rush into measures which may have short‐​term popularity. The most thorough public opinion survey of Americans’ attitudes towards Constitutional rights included this question: “Suppose the President and Congress have to violate a Constitutional principle to pass an important law the people wanted. Would you support them in this action?” Twenty‐​eight percent said yes, “because the Constitution shouldn’t be allowed to stand in the way of what the people need and want.” Forty‐​nine percent said no, “because protecting the Constitution is more important to the national welfare than any law could possibly be.”(1)



Indeed, the precise reason for putting certain fundamental rights in the Constitution is to protect them from transient majorities.(2) In long‐​term perspective, the herding of American citizens of Japanese descent into concentration camps during WWII was a horrible human rights violation.(3) But at the time, public opinion and the press heavily favored the concentration camps, despite the total lack of evidence that these Americans were disloyal. And certainly national security was in far graver danger in early 1942 than it is today.



But because terrorism, like child abuse (or Communism in previous decades) provokes such intense concerns, there is temptation to be careless in choosing the weapons to fight these evils. From the Alien and Sedition Acts in 1798 to the Palmer Raids in 1920 to McCarthyism in the 1950s, cynical politicians who have manipulated popular fears of aliens and radicals have done tremendous damage to the lives of innocent people and to the fundamental principles of Americanism.



Today, Congress stands poised to repeat the mistakes of the past, as vast numbers of people are smeared with guilt by (very tenuous) association. It is not often that one sees the Presbyterian Church and the American Friends Service Committee standing shoulder‐​to‐​shoulder with the National Rifle Association and Americans for Tax Reform.



That such diverse groups can find common ground, along with other organizations, to warn about the dangers of proposed legislation sharply curtailing civil liberty should indicate just how serious is the current threat to our Constitution.(4) President Clinton was right to characterize the Oklahoma City bombing as “an attack on our way of life.” If Oklahoma City becomes a pretext for the constriction of the Bill of Rights, then we will have handed terrorism a victory which it could never have won for itself.



 **There is No Terrorism Crisis**



“By enabling terrorists to appear much stronger than they really are, the media often find themselves working Pour le roi de Prusse,” observed one historian.(5) Contrary to the imagery of some irresponsible segments of the media (and their Congressional analogues), there is no need to legislate an atmosphere of panic and hysteria. According to the State Department, international terrorist attacks are at their lowest level in 23 years.(6) In the United States in the last eleven years, according to the FBI, there have been only two international terrorist incidents. (One was the World Trade Center bombing; the other was a trespassing incident at the Iranian mission to the United Nations, in which five critics of the Iranian regime took over the mission’s offices, and refused to leave.)(7)



As for incidents of domestic terrorism, there were none in the United States in 1994, nor were there any preventions of terrorist incidents. In 1993, there were 11 incidents classified by the FBI as “terrorist.” Nine of those eleven incidents took place one night in Chicago when animal rights activists set off small incideniary devices in four department stores that sell fur.(8)



Combining domestic and international terrorism, and also accounting for suspected terrorist acts, the total terrorist incident count in the United State is as follows:



 **Terrorist Incidents in the United States**



1994 0 0 1 1993 11 7 2 1992 4 0 0 1991 5 4 1 1990 7 5 1 1989 4 7 16 Of these incidents, only one (the 1993 World Trade Center bombing) was classified as international in origin.(9)



The Oklahoma City bombing was one of the most terrible single crimes in American history, but it was just that: an isolated, single crime. It was emphatically not part of a trend towards increasing terrorism.



 **The British Tragedy**



More government secrecy, more police powers to detain people at will, less governmental accountability, and less freedom are not novel responses to terrorism. They are precisely the approach that has been taken in Great Britain since the early 1970s. The British lesson should be a caution to American politicians who feel confident that the only thing wrong with anti‐​terrorism policy is that the Bill of Rights has been taken too far.



In 1974, Irish Republican Army terrorists bombed pubs in Birmingham, killing twenty‐​one people. Home Secretary Roy Jenkins introduced the Prevention of Terrorism (Temporary Provisions) Bill. Approved without objection in Parliament, the Bill was supposed to expire in one year, but has been renewed every year. The Bill included a smorgasboard of civil liberties restrictions, most of which are now being proposed, with some variation, in the United States.



Under the Bill, the police may stop and search without warrant any person suspected of terrorism. They may arrest any person they “reasonably suspect” supports an illegal organization, or any person who has participated in terrorist activity. An arrested person may be detained up to forty‐​eight hours and then for five more days upon the authority of the Secretary of State.



Of the 6,246 people detained between 1974 and 1986, 87 percent were never charged with any offense. Many detainees reported that they were intimidated during detention and prevented from contacting their families.



The Prevention of Terrorism Bill also makes it illegal even to organize a private or public meeting addressed by a member of a proscribed organization, or to wear clothes indicating support of such an organization.(10)



The Act allows the Secretary of State to issue an “exclusion order” barring a person from ever entering a particular part of the United Kingdom, such as Northern Ireland or Wales. Persons subject to this form of internal exile have no right to know the evidence against them, to cross‐​examine or confront their accusers, or even to have a formal public hearing.(11)



The European Court of Human Rights ruled the Prevention of Terrorism Act to be in violation of Article Five, Section Three of the European Convention on Human Rights, which requires suspects to be “promptly” brought before a judge.(12) Nevertheless, the British government refuses to abandon its preventive detention policy, and evades the European Court’s ruling by invoking Article 15’s provision for countries to ignore the Convention on Human Rights “in time of war or other emergency threatening the life of the nation.”(13)



One of the most important lessons from Britain is that even a huge dose of restrictions on civil liberties, such as the Prevention of Terrorism Bill, does not long remain “sufficient” in the eyes of the government. At least in regard to civil liberties, the Domino Theory has proven correct, as one traditional Anglo‐​American freedom after another has fallen under the government’s assertion of the need for still more anti‐​terrorist powers.



In Northern Ireland the jury has been “suspended” for political violence cases; judges in the Diplock courts hear the cases instead. Confessions are admitted without corroboration. Confessions are extracted through “the five techniques”: wall‐​standing, hooding, continuous noise, deprivation of food, and deprivation of sleep. Convictions may be based solely on the testimony of “supergrasses” (police informers).(14)



In 1988, the Thatcher government enacted additional laws restricting civil liberties. Television stations were forbidden to broadcast in‐​person statements by supporters of a legal political party, Sinn Fein.(15) The ban even applied to rebroadcasts of archive films taped many decades ago, such as footage of Eamon de Valera, the first president of Ireland. A confidential British Broadcasting Corporation memo announced the government’s intention to keep journalists from broadcasting any statement by U.S. Senator Edward Kennedy supporting Sinn Fein.(16) The BBC also banned Paul McCartney’s “Give Ireland Back to the Irish,” and a song by another group urging the release from prison of the Guildford Four (discussed below).(17)



A suspect’s decision to remain silent under interrogation may now be used against him in court. Although terrorism in Northern Ireland was the stated basis for the change, the change will also apply in England and Wales. No‐ one who has seen Great Britain’s slide down the slippery slope can feel confident that repressive measures introduced solely for terrorism will not eventually seep into the ordinary criminal justice system.



 **Wiretaps do not even need judicial approval.(18)**



The Security Service Act of 1989 provides: “No entry on or interference with property shall be unlawful if it is authorized by a warrant issued by the secretary of state.” If committed pursuant to an order from the secretary of state, acts such as theft, damage to property, arson, procuring information for blackmail, and leaving planted evidence are not crimes.(19)



As in America, gun prohibitionists have hitched their wagon to “anti‐​terrorism,” with little regard for an actual terrorist nexus. Although British laws regarding possession of actual firearms were already quite severe, the Firearms Act of 1982 introduced restrictive licensing for imitation firearms which could be converted to fire live ammunition.(20) The sponsor of the new law against imitation firearms promised that it would help stem “the rising tide of crime and terrorism”–although there had never been a crime or terrorist act committed with a converted imitation weapon.(21) The first time the Prevention of Terrorism Act was used was after another pub bombing, in the English town of Guildford. Four people were arrested, held incommunicado in prison for a week, and coerced into false confessions by administration of drugs and by threats against their families. While the “Guildford Four” were being held, the police used the time to fabricate evidence against them.



Although members of the Irish Republican Army already in prison confessed to the Guildford bombings, the Guildford Four were tried, convicted, and sentenced to life in prison.



Several leading English statesmen, including Roy Jenkins, felt that the defendants had been framed. A campaign to free them continued for fifteen years, until, upon discovery of police notes of fabrication of evidence, the Guildford Four were released from prison.(22)



The Birmingham bombings that had led to the Prevention of Terrorism Act resulted in the conviction of a group of defendants called the “Birmingham Six.” Amnesty International charged that their confessions were extracted under torture. The forensic scientist whose testimony convicted the Birmingham Six later admitted that he lied in court. The Birmingham Six confessed while being held incommunicado by the police; the various confessions were so factually inconsistent that they could not have been true.



(Civil libertarians fear that the Birmingham case is only one of many instances of police obtaining coerced confessions.(23))



The Birmingham Six were also eventually freed.



Britain, fortunately, has no death penalty. In America, where before anyone had even been indicted President Clinton announced that the perpetrators of the Oklahoma City bombing should be executed, the federal death penalty would mean that vindication of persons wrongfully convicted of terrorism might be post‐​mortem.



To state the obvious, all the legislation has hardly immunized Britain from terrorism. But Britain has, in two decades, eviscerated the magnificent structure of liberty and limited government that took over a millennium to construct. For centuries, “the rights of Englishmen” were proudly held up in contrast to the absolutism of the Continent. Far from being an examplar to the world, the modern “anti‐​terrorist” United Kingdom has been found culpable of human rights violations under the European Convention on Human Rights more often than any other member of the Council of European States.(24) To a student of Britain’s magnificent history in the story of freedom, it is a pitiful sight to see modern Britons forced to turn to Brussels and the European Court of Human Rights as the last protector of what were formerly the unquestioned rights of Englishmen.



Britain was once the freest nation in the world; today, it is one of the unfreest in Western Europe. As Britain illustrates, no matter how great a country’s tradition of freedom, freedom can be lost in less than a generation if public officials, and the public, allow terrorism to destroy their traditional way of life.



 **Weakening Restraints on FBI Political Surveillance**



Within days after the Oklahoma City bombings, conservative talk show host Rush Limbaugh began casting blame on civil libertarians such as former Ohio senator Howard Metzenbaum who had promoted strict guidelines on FBI surveillance of dissident groups in the United States.(25) Other persons have also called for abolition of the remaining limitations on FBI investigations.



First of all, there is at present no evidence that the FBI wanted to spy on anyone suspected in Oklahoma City bombing, but was prevented from doing so by the current guidelines. Thus, persons demanding the abolition of FBI guidelines are demanding a “solution” for which there is no demonstrated problem.



Second, the FBI guidelines exist for a very good reason. Before the guidelines were implemented, the FBI spied on literally hundreds of thousands of Americans who were doing nothing more than exercising their Constitutional right to question government policies. Victims of these abuses ranged from Dr. Martin Luther King, Jr., to the Ku Klux Klan, to the Congress on Racial Equality and the civil rights movement. The Counter‐​intelligence Programs (COINTELRPO) invaded the Constitutional rights of American people who simply were expresssing in public what Secretary of Defense Robert McNamara had concluded in private. Far from being confined to a single type of dissident, or to a few years of excess, FBI abuses dated back to the 1940s and were pervasive until brought to light by fifteen months of hearings before Senator Frank Church’s special committee in 1975–76. Altogether, there were 675 FBI operations against civil rights, white supremacist, or anti‐​war groups, which led to only four convictions.(26)



Even after all the public hearings, and the implementation of guidelines, the FBI continued to abuse the rights of dissident Americans, through a massive surveillance of people in CISPES (Committee in Solidarity with the People of El Salvador) who opposed to President Reagan’s policy in El Salvador in the mid‐​1980s. The CISPES investigation, justifiably regarded today as shameful, would have been lawful if the anti‐​terrorism bills current being considered had been law.



The first set of FBI guidelines were implemented by President Ford’s attorney general Edward Levi in 1976. In 1983, the “Levi guidelines” were replaced by President Reagan’s attorney general William French Smith. These “Smith guidelines” were far less restrictive. FBI director William Webster stated that the Smith guidelines “should eliminate any perception that actual or imminent commission of a violent crime is a prerequisite to investigation.” Thus, the recent highly‐​publicized claim of a former FBI official “you have to wait until you have blood in the streets before the bureau can act” is patent nonsense.(27)



In fact, the Reagan/​Smith guidelines, which are still in force, nowhere require the completion of a violent crime. Rather they state that a:



domestic security/​terrorism investigation may be initiated when facts for circumstances reasonably indicate that two or more persons are engaged in an enterprise for the purpose of furthering political or social goals wholly or in part through activities that involve force or violence and a violation of the criminal laws of the United States.



Specifically, the guidelines already allow investigations based upon mere words:



When, however, statements advocate criminal activity or indicate an apparent intent to engage in crime, particularly crimes of violence, an investigation under these Guidelines may be warranted unless it is apparent from the circumstances or the context in which the statements are made, that there is no prospect for harm.



While the Smith guidelines would prevent infiltration of Second Amendment groups simply because they are sharply critical of government policy, the guidelines do not now prevent infiltration of groups which actually threaten violence. For example, in Virginia, a group of fifteen men who allegedly wanted to resist the federal government managed only three meetings before being arrested for weapons violations as a result of government infiltrator’s secret tape recordings.(28)



Rather than being obliterated, guidelines on FBI domestic surveillance should be brought up to full strength.



A statutory version of the Levi guidelines should be enacted.



Persons who eager to “unleash” the FBI against dissident groups who are not threatening illegal activity might first want to go through the mental exercise of imagining their worst nightmare as President. Liberals might imagine Pat Buchanen or Pat Robertson. Conservatives could imagine Dianne Feinstein or Jesse Jackson. In such a scenario, would we want the FBI free to spy on whomever the President does not like? Under Presidents Nixon, Johnson, and Kennedy, who were far more moderate than Jesse Jackson or Pat Buchanan, the FBI did so, with baleful results.



An official at the Treasury Department, who works closely with the BATF, warned that there is “a tremendous potential for abuse” in administration proposals to loosen controls on the FBI.(29)



It must be remembered that many of America’s greatest organizations were, in their day, radical extremists. The abolitionists were extremists, as were the suffragettes, the civil rights movements, and many of the opponents of the War in Vietnam. If these groups seem vindicated by history, they were bitterly attacked in their day as radical and anti‐​American.



Finally, before any additional powers are granted to the FBI, it is appropriate to investigate FBI abuses of existing powers, including the events in Waco.(30) At the least, it is well‐​established that the FBI used a chemical warfare agent which is banned in international warfare, against children indoors, even though Army and manufacturer manuals specifically warn that the agent indoors is flammable, and can severely injure unprotected children. In securing Attorney General Reno’s consent, the FBI falsely told her that the chemical warfare agent was “a mild form of teargas.” The FBI also ignored the advice of its own behavioral experts, and pressured at least one of them to reverse his advice, so as to justify an assault. This fact too was concealed from the Attorney General.



 **FBI Foreign Jurisdiction**



It has been proposed that the FBI’s foreign jurisdiction be expanded. Firstly, the expansion is unnecessary, since the CIA can operate overseas against terrorists. Second, allowing domestic American law enforcement agents to operate on foreign soil against foreign soil against foreign citizens creates a dangerous precedent, and will inevitably lead to demands for reciprocity. Do we really want the Russian secret police, or even the Mexican federales, operating on American soil? The Clinton bill also removes most of the limitations regarding use (including overseas) of American trainers for foreign law enforcement, and removes the restriction against American tax dollars being used to pay the salaries of foreign police.(31) Internationalizing criminal law is even more dangerous to civil liberty than is federalizing it.



 **Felonizing Support for Peaceful Activities of Foreign Organizations**



 **Presidential Designation of “Terrorist” Groups**



The Clinton and Dole bills empower the President to designate “foreign terrorist” organizations which are illegal for Americans to provide any “material support.”(32) Recently, the Clinton administration has retreated from its insistance that the Presidential designation be unreviewable. At the least, the potential for judicial review will reduce the risk of the terrorist designation being used against domestic dissident groups. (Since they would be able to show in court that they were not foreign.) But it should be remembered that American courts have historically been extremely deferential to Presidential foreign policy decisions. If there were even a scintilla of evidence in favor of the President’s designation of a foreign group as “terrorist,” then it is virtually certain that courts would not overturn the designation.



Again, the reader might consider imagining this legislation in the hands of one’s worst political nightmare.



An organization which provides support to the government of Israel or to the Israeli Defense Forces (which are considered “terrorist” in some political circles) could be outlawed, as could (by a different President) a group which provides support to Palestinian refugees.



 **Material Support**



Current federal law appropriately forbids the providing of material support to any foreign terrorist organization.(33) The law forbids investigations of people for violating this law unless there is some reasonable suspicion that they have violated or may violate the law.



The restriction should of course be retained; targetting people for FBI investigations when there is not a scintilla of suspicion is not only an invitation to harassment of dissidents, it is a waste of law enforcement resources.



One important distinction between the Clinton and Dole bills is that the Dole creates an explicit exception to the “material support” statute: “ ‘Material support’…does not include humanitarian assistance to persons not directly involved in such violations.”(34) Thus, sending a Christmas food package to an I.R.A. or A.N.C. prisoner would constitute material support, but giving money to a fund which assisted the orphaned children of I.R.A. or A.N.C.



members would not be, under the Dole approach.



Under the Clinton bill, however, the donor to the I.R.A. orphanage would be a federal felon, subject to ten years in prison, as would be a person who spent five dollars to attend a speech of a visiting lecturer from the African National Congress.



When pressed about this fact at recent Congressional hearings, a Clinton administration spokesperson acknowledged that minor support for the A.N.C.‘s peaceful activities could have been felonized, but that the American people should simply trust the President not to abuse the immense power which President Clinton was requesting.



But as President Lyndon Johnson put it: “You do not examine legislation in light of the benefits it will convey if property administered but, in light of the wrongs it would do and the harms it would cause if improperly administered.”



The “terrorism” bills’ overbreadth is astonishing. The Palestine Liberation Organization is permanently defined as a terrorist organization by the proposal, no matter what its future conduct.(35) Thus, if the P.L.O. should live up the peace treaty that it signed with Israel, President Clinton would be guilty of providing “material support” to a terrorist organization should he invite Yassir Arafat to the White House and give him a free meal and a night’s lodging.



 **Licensed Donations**



Theoretically, a license can be procured allowing humanitarian contributions to the blacklisted group. The licensing procedure is, however, very difficult to comply with. Not only does recipient group have to open its books to the Treasury Department, so does the donor. In other words, if a person wants to make a $50 contribution to buy clothes for Palestinian orphans, the person must make his financial records open for inspection, and be able to show “the source of all funds it receives, expenses it incurs, and disbursements it makes.”(36) There is no limitation that the complete accounting of receipt, expenses, and disbursements be limited to the charitable donation. Virtually no‐​one in the United States keeps such detailed records. Knowing that a charitable donation to a politically blacklisted group would expose the donor to a nightmare audit, few donors would be courageous or foolish enough to give anyway.



In addition to criminal penalties of up to ten years in prison, civil fines of $50,000 per offense may be imposed, and in civil prosecutions, the government may, upon approval of the court, introduce secret, classified evidence which remains hidden from the defendant.(37) (The Clinton and Dole bills grant similar authority to use secret evidence in proceedings under the International Emergency Economic Powers Act, which gives the President unilateral authority to regulate or prohibit all foreign exchange transactions, all imports and exports of securities and currency and foreign currency transactions, and all banking transactions involving foreigners.(38))



 **The Constitutional View**



The Constitution mandates that if a person is to be punished for association with a group which has unlawful objectives, the government must prove that the individual specifically intended to further the unlawful objectives.(39) What the Clinton/​Dole bills propose is a return to practices which the Supreme Court outlawed over half a century ago.



Then, the Immigration and Naturalization Service attempted to deport labor organizer Harry Bridges because of his affiliation with the Communist party. Bridges had supported only lawful Communist activities, rather than the party’s unlawful ends. The INS argued that if an organization had unlawful purposes, the fact that a supporter had supported only lawful purposes was irrelevant. The Supreme Court disagreed, and dismissed the case.(40)



More recently, the Court declared unconstitutional a law that was “a blanket prohibition of association with a group having both legal and illegal aims.” Unless there was proof that the defendant specifically intended to support the group’s illegal aims, the prohibition was a violation of “the cherished freedom of association protected by the First Amendment.”(41)



 **Defining Everything as “Terrorism”**



Current federal law already provides a comprehensive, realistic definition of “terrorist activity.”(42) Some proposals define virtually any crime as “terrorism.” For example, the Clinton and Dole “terrorism” bills define as “terrorism” virtually every violent or property crime, whether or not related to actual terrrorism. The bills impose a prison terms of up to twenty‐​five years (for property damage, more for violent crimes) for “terrorist” offenses which are defined as follows: any assault with a dangerous weapon, assault causing serious bodily injury, or any killing, kidnapping, or maiming, OR any unlawful destruction of property.(43) Snapping someone’s pencil, breaking someone’s arm in a bar fight, threatening someone with a knife, or burning down an outhouse would all be considered “terrorist” offenses. Any attempt to perpetrate any of these terrorist crimes would be subject to the same punishment as completed offense.



Even a threat to commit the offense (i.e. “One of these days, I’m going to snap your pencil.”) is a felony subject to ten years in federal prison.(44) Again, the extra federal power granted by the legislation is superfluous to genuine anti‐​terrorism. It is already a serious federal felony to make a real terrorist threat, as by threatening to set off a bomb, or to assassinate the President.(45)



In order for the offense to be considered “terrorism,” all that would be necessary would be jurisdictional predicate that would cover almost every crime. The jurisdictional predicate requires one of any of the following: the crime “affects commerce in any way” (not necessarily interstate commerce); the criminal used “any facility used in any manner in commerce”; the victim was “traveling in commerce” (again, not necessarily interstate); the victim was a federal employee, or the property damaged was federal; the victim was not an American national; or any of the offenders “travels in commerce.”(46) If anyone involved in the crime meets the jurisdictional predicate, then jurisdiction is invoked for the entire crime.(47)



Finally, in order for a prosecution to take place, the Attorney General must certify in writing that the offense “transcended national boundaries” and was intended to intimidate a foreign government or “a civilian population, including any segment thereof.”(48) There is no provision for review of whether the Attorney General’s certification was even remotely accurate. Nor is there any requirement that there be an actual international border crossing.



Just because the law allows it, the federal government probably will not prosecute every Canadian tourist who snaps a policeman’s pencil or everyone who scratches anti‐​war graffiti on post office tables. The proponents of these bills may expect that the essentially limitless discretion granted to the federal government will not be abused. But a fundamental principle of American law has always been that the law should control the government; citizens should not be at the mercy of the good judgement of government officials. As the Supreme Court put it, “It could certainly be dangerous if the legislature could set a net wide enough to trap all possible offenders, and leave it to the courts to step inside and say who could rightfully be detained, and who should be set a large.”(49)



The justification for federalizing all of the criminal law is that such federalization is necessary to make sure that every possible terrorist crime is covered. For example, it is asserted that the bombing of a Jewish hospital in, for example, St. Louis, might not be covered by current federal law. In fact, the federal arson statute has successfully been applied to the burning of a trailer that was hooked up to a power system which was part of the interstate electricity grid.(50) Thus, the fact that the hospital drew power from the same electrical grid would justify application of the current federal arson law, without the need for a new statute. Even if it is possible to imagine some bizarre hypothetical crime that would not be covered by the (very expansive) interpretation of current federal criminal statutes, every conceivable terrorist crime is subject to severe punishment under current state criminal laws.



The dangers posed by the hidden federalization of the entire criminal law (all the way down to petty vandalism) become all the greater when coupled with the bill’s other provisions to make the overbroad federal RICO,(51) money laundering,(52) and wiretapping laws(53) applicable to “terrorist” offenses and to authorize use of the military in domestic law enforcement for “terrorism.”(54) No bail is allowed even if it is uncontroverted that the accused will not flee and will pose no danger to anyone.(55)



Likewise, mandatory prison sentences, with no possibility of probation, are required for “terrorist” crimes, no matter what the circumstances.(56)



Having used state law definitions to define petty property crimes as “terrorism,” the bills then forbid defendants from invoking state constitutional law protections of the state where the alleged offense took place.(57)



Turning every state and local petty property crime (or even a local violent crime) into a federal felony may be unconstitutional, as the Supreme Court recently ruled in the Lopez “gun‐​free‐​school‐​zones” case. Putting aside questions of Constitutionality, it is inappropriate that the draconian federalization of state crimes be pushed through Congress under the mask of anti‐​terrorism.



 **Resisting Foreign Dictatorships**



Solicitude for foreign governments should not blind us to the fact that most governments in the world are dictatorships. Under the principles on which America is based, governments without the consent of the governed have no legitimacy, and it is the right of the people of that nation to overthrow the dictatorship.



Yet the Clinton and Dole bills define as “terrorism” any act which plans the destruction of government property in foreign nation with which the United States is “at peace.”(58) Thus, if Chinese refugees living in the United States planned a jailbreak to liberate political prisoners in China, they would be guilty of “terrorism.” If Americans in 1940 had plotted the destruction of railways leading to Nazi concentration camps, they too would have been guilty of “terrorism.” And so would the countless American Jews who smuggled firearms to the Jewish resistance movement in Palestine in the 1940s, making possible the eventual establishment of the state of Israel. Had such a “terrorism” law been universal in 1776, the Dutch, French, and other private citizens who provided material assistance to the American Revolution (even though their governments were at peace with the British Empire) would have been “terrorists” too. It ill becomes a nation which was born in violent revolution with foreign assistance to felonize the very types of charity which allowed our own nation to become free. Resistance to dictatorships and empires is not terrorism.



 **Wiretapping**



Various proposals have been offered to expand dramatically the scope of wiretapping. For example, the Clinton bill defines almost all violent and property crime (down to petty offenses below misdemeanors) as “terrorism” and also allow wiretaps for “terrorism” investigations.(59)



Other proposals would allow wiretaps for all federal felonies, rather than for the special subet of felonies for which wiretaps have been determined to be especially necessary. Notably, wiretaps are already available for the fundamental terrorist offenses: arson and homicide. Authorizing wiretaps for evasion of federal vitamin regulations, gun registration requirements, or wetlands regulations is hardly a serious contribution to antiterrorism, but amounts to a bait‐​and‐​switch on the American people.



Currently, FBI wiretapping, bugging, and secret break‐ ins of the property of American groups is allowed after approval from a seven‐​member federal court which meets in secret.(60) Of the 7,554 applications which the FBI has submitted in since 1978, 7,553 have been approved.(61)



Making the request for vast new wiretap powers all the more unconvincing is how poorly wiretap powers have been used in the past. Terrorists are, of course, already subject to being wiretapped. Yet as federal wiretaps set new record highs every year, wiretaps are used almost exclusively for gambling, racketeering, and drugs. The last known wiretap for a bombing investigation was in 1998. Of the 976 federal electronic eavesdropping applications in 1993, not a single one was for arson, explosives, or firearms, let alone terrorism. From 1983 to 1993, of the 8,800 applications for eavesdropping, only 16 were for arson, explosives, or firearms.(62) In short, requests for vast new wiretapping powers because of terrorism are akin to a carpenter asking for a pile driver to hammer a nail, while a hammer lies nearby, unused.



Even more disturbing than proposals to expand the jurisdictional base for wiretaps are efforts to remove legal controls on wiretaps. For example, wiretaps are authorized for the interception of particular speakers on particular phone lines. If the interception target keeps switching telephones (as by using a variety of pay phones), the government may ask the court for a “roving wiretap,” authorizing interception of any phone line the target is using. Yet while roving wiretaps are currently available when the government shows the court a need, the Clinton and Dole bills allow roving wiretaps for “terrorism” without court order.(63) (Again, remember that both bills define “terrorism” as almost all violent or property crime.)



The Foreign Intelligence Surveillance Act (FISA) provides procedures for authorizing wiretaps in various cases. These procedures have worked in the most serious foreign espionage cases.(64) Yet the Clinton and Dole bills would authorize use of evidence gathered in violation of FISA in certain deportation proceedings.



 **Warrantless Data Gathering**



Proposals have also been offered to require credit card companies, financial reporting services, hotels, airlines, and bus companies to turn over customer information whenever demanded by the federal government.(65) Document subpoenas are currently available whenever the government wishes to coerce a company into disclosing private customer information. Thus, the proposals do not increase the type of private information that the government can obtain; the proposals simply allow the government to obtain the information even when the government cannot show a court that there is probable cause to believe that the documents contain evidence of illegal activity.(66)



Similar analysis may be applied to proposals to increase the use of pen registers (which record phone numbers called, but do not record conversations, and thus do not require a warrant). If a phone company has a high enough regard for its customers’ privacy so as to not allow pen registers to be used without any controls, the government may obtain a court order to place a pen register. Business respect for customer privacy ought to be encouraged, not outlawed.



 **Curtailing First Amendment Rights of Computer Users**



For some government agencies, the Oklahoma City tragedy has become a vehicle for enactment of “wish list” legislation that has nothing to do with Oklahoma City, but which it is apparently hoped the “do something” imperative of the moment will not examine carefully.



One prominent example is legislation to drastically curtail the right of habeas corpus.(67) Although Supreme Court decisions in recent years have already sharply limited habeas corpus,(68) prosecutors’ lobbies want to go even further. Two obvious points should be made: First, habeas corpus has nothing to do with apprehending criminals; by definition, anyone who files a habeas corpus petition is already in prison. Second, habeas corpus has nothing to do with Oklahoma City in particular, or terrorism in general.



A second example, of piggybacking irrelevant legislation designed to reduce civil liberties are current FBI efforts to outlaw computer privacy.



If a person writes a letter to another person, he can write the letter in a secret code. If the government intercepts the letter, and cannot figure out the secret code, the government is out of luck. These basic First Amendment principles have never been questioned.



But, if instead of writing the letter with pen and paper, the letter is written electronically, and mailed over a computer network rather than postal mail, do privacy interests suddenly vanish? According to FBI director Louis Freeh, the answer is apparently “yes.”



Testifying before the Senate Judiciary Committee about Oklahoma City, director Freeh complained that people can communicate over the internet “in encrypted conversations for which we have no available means to read and understand unless that encryption problem is dealt with immediately.”(69) “That encryption problem” (i.e. people being able to communicate privately) could only be solved by outlawing high quality encryption software like Pretty Good Privacy”.



First of all, shareware versions of Pretty Good Privacy are ubiquitous throughout American computer networks. The cat cannot be put back in the bag. More fundamentally, the potential that a criminal, including a terrorist, might misuse private communications is no reason to abolish private communications per se. After all, people whose homes are lawfully bugged can communicate privately by writing with an Etch-a-Sketch”.(70) That is no reason to outlaw Etch‐ a‐​Sketch.



Although Mr. Freeh apparently wants to outlaw encryption entirely, the Clinton administration has been proposing the “Clipper Chip.” The federal government has begun requiring that all vendors supplying phones to the federal government include the “Clipper” chip. Using the federal government’s enormous purchasing clout, the Clinton administration is attempting to make the Clipper Chip into a de facto national standard.(71)



The clipper chip provides a low level of privacy protection against casual snoopers. But some computer scientists have already announced that the chip can defeated. Moreover, the “key”–which allows the private phone conversation, computer file, or electronic mail to be opened up by unauthorized third parties–will be held by the federal government.



The federal government promises that it will keep the key carefully guarded, and only use the key to snoop when absolutely necessary. This is the same federal government that promised that social security numbers would only be used to administer the social security system, and that the Internal Revenue Service would never be used for political purposes.



Proposals for the federal government’s acquisition of a key to everyone’s electronic data, which the government promises never to misuse, might be compared to the federal government’s proposing to acquire a key to everyone’s home.



Currently, people can buy door locks and other security devices that are of such high quality that covert entry by the government is impossible; the government might be able to break the door down, but the government would not be able to enter discretely, place an electronic surveillance device, and then leave. Thus, high‐​quality locks can defeat a lawful government attempt to bug someone’s home, just as high‐​quality encryption can defeat a lawful government attempt to read a person’s electronic correspondence or data.



Similarly, it is legal for the government to search through somebody’s garbage without a warrant; but there is nothing wrong with privacy‐​conscious people and businesses using paper shredders to defeat any potential garbage snooping. Even if high‐​quality shredders make it impossible for documents to be pieced back together, such shredders should not be illegal.



Likewise, while wiretaps or government surveillance of computer communications may be legal, there should be no obligation of individuals or businesses to make wiretapping easy. Simply put, Americans should not be required to live their lives in a manner so that the government can spy on them when necessary.



Thus, although proposals to outlaw or emasculate computer privacy are sometimes defended as maintaining the status quo (easy government wiretaps), the true status quo in America is that manufacturers and consumers have never been required to buy products which are custom‐​designed to faciliate government snooping.



The point is no less valid for electronic keys than it is for front‐​door keys. The only reason that electronic privacy invasions are even discussed (whereas their counterparts for “old‐​fashioned” privacy invasions are too absurd to even be contemplated), is the tendency of new technologies to be more highly restricted than old technologies. For example, the Supreme Court in the 1920s began allowing searches of drivers and automobiles that would never have been allowed for persons riding horses.



But the better Supreme Court decisions recognize that the Constitution defines a relationship between individuals and the government that is applied to every new technology. For example, in United States v. Katz, the Court applied the privacy principle underlying the Fourth Amendment to prohibit warrantless eavesdropping on telephone calls made from a public phone booth– even though telephones had not been invented at the time of the Fourth Amendment.(72) Likewise, the principle underlying freedom of the press– that an unfettered press is an important check on secretive and abusive governments–remains the same whether a publisher uses a Franklin press to produce a hundred copies of a pamphlet, or laser printers to produce a hundred thousand. Privacy rights for mail remain the same whether the letter is written with a quill pen and a paper encryption “wheel,” or with a computer and Pretty Good Privacy.



Efforts to limit electronic privacy will harm not just the First Amendment, but also American commerce. Genuinely secure public‐​key encryption (like Pretty Good Privacy) gives users the safety and convenience of electronic files plus the security features of paper envelopes and signatures. A good encryption program can authenticate the creator of a particular electronic document–just as a written signature authenticates (more or less) the creator of a particular paper document.



Public‐​key encryption can greatly reduce the need for paper. With secure public‐​key encryption, businesses could distribute catalogs, take orders, pay with digital cash, and enforce contracts with veriable signatures–all without paper.



Conversely the Clinton administration’s weak privacy protection (giving the federal government the ability to spy everywhere) means that confidential business secrets will be easily stolen by business competitors who can bribe local or federal law enforcement officials to divulge the “secret” codes for breaking into private conversations and files, or who can hack the clipper chip.



 **The New Star Chamber**



Although the United States has suffered exactly one alien terrorist attack in the last eleven years, special harsh rules for aliens are at the top of the “antiterrorism” agenda. The most ominous proposals are those that allow secret evidence for deportation cases in which the government asserts that secrecy is necessary to the national security.(73) Georgetown University Law Professor David Cole calls the secret court the new “Star Chamber,” since its powers resemble those of the inquisitorial court which the British monarchy, in violation of the common law, used to terrorize dissident subjects. Star Chamber was one of the most hated abuses of the British government.



Modern Star Chamber proceedings are to be before a special court (one of five select federal district judges)(74), after a an ex parte, in camera showing that normal procedures would “pose a risk to the national security of the United States.”(75) Based upon further ex parte, in camera motions, evidence which the government does not which to disclose may be withheld from the defendant, who will instead be provided a general summary of what the evidence purports to prove. In other words, secret evidence may be used.(76) Of course any of the “showings” that the government makes in camera and ex parte may be based on allegations regarding the unreviewable claims of a secret informant.



Wiretap evidence is usable even if it was illegally obtained.(77) Normal procedural rules allowing for disclosure of circumstances relating to illegally obtained evidence are abolished.(78)



Legal aliens do not, of course, have the full scope of Constitutional rights guaranteed to American citizens; for example, they cannot exercise rights associated with citizenship, such as voting, or serving on a jury. But it is well‐​settled that legal aliens enjoy the same right to freedom of speech as do citizens. Likewise, legal aliens have always been accorded the same due process protections in criminal cases. After all, the Fifth Amendment’s guarantee of Due Process protects “all persons,” not just “all citizens.”(79)



Procedures like those proposed in the Clinton and Dole bills have already been found unconstitutional. As the District of Columbia Court of Appeals, put it:



Rafeedie–like Joseph K. in The Trial–can prevail before the [INS] Regional Commmissioner only if he can rebut the undisclosed evidence against him, i.e., prove that he is not a terrorist regardless of what might be implied by the government’s confidential information. It is difficult to imagine how even someone innocent of all wrongdoing could meet such a burden.(80)



The argument for allowing secret evidence in deportation proceedings is that otherwise the identity or operational mode of a confidential informant might be jeopardized. First of all, the very purpose of the Constitution’s Confrontation Clause is to prevent people’s lives from being destroyed by the type of secret accusations which had characterized the European justice systems.



Moreover, the argument against endangering the secrecy of confidential accusers in deportation cases proves too much. The very same argument applies in every other case, including criminal violence or drug sales cases. Obeying the Confrontation Clause in those cases may likewise impede the short‐​term interests of law enforcement. But the Constitution has conclusively determined that a criminal justice system without a right of confrontation poses a far greater long‐​term risk to public safety than does requiring the government to disclose the reason why it wants to imprison, execute, or deport someone.



Simply put, confidential informants often lie.



Informants are rarely good citizens who come forward to help prevent a crime. Rather, informants are criminals who have been caught, and have turned informant in order to protect themselves from prosecution; informants have every reason to lie and falsely accuse people.(81)



Confidential informants who are not professional criminals may have other reasons for lying. The type of miscarriage of justice that can occur based on confidential informants was illustated in 1950 case, in which the Supreme Court held that secret evidence could be used to prevent an alien from entering the United States.(82) (She was married to an American.) When the alien was granted a hearing, it was discovered that the confidential informant was her husband’s angry ex‐​girlfriend.



Some persons who would oppose Star Chamber proceedings for criminal trials might approve of such procedures in deportation hearings since deportation is, under most circumstances, a less severe sanction than prison. Yet if the alien cannot find a country that wants to take him (or if the State Department can quietly convince other countries not to take him), then the alien may be imprisoned for the rest of his life in the United States, at the sole discretion of the Attorney General, without even the right to ask for a writ of habeas corpus based on governmental violation of statutes.(83)



Finally, some persons may accept Star Chamber for legal resident aliens under the presumption that such procedures would never be used against American citizens. Yet if there is anything the experience of Great Britain proves, it is that special, “emergency” measures implementented in a limited jurisdiction (such as Northern Ireland) soon spread throughout the nation. Cancers always start small. If one international terrorist incident in eleven years is a sufficient interest to justify a Star Chamber for certain terrorism suspects, then it is hard to resist the logic that crimes which actually are widespread (such as homicide, rape, or drug trafficking) should be entitled to their own Star Chamber.



 **More Informants**



One of the reasons that many people are so frightened of the federal government is how it already uses informants to attempt to infiltrate suspicious organizations. One of the most notorious cases which helped create the militia movement was started by the attempt to creat an informant.



Randy Weaver was a white separatist who lived with his family in a remote cabin in northern Idaho. There was no indication that he had ever advocated or participated in illegal violence. When he was approached by federal agents who wanted him to infiltrate violent white supremacist groups and serve as an informer, he refused. He was later entrapped (a jury later found) by repeated pestering from undercover agents into selling undercover BATF agents two shotguns whose barrels had been shortened (at the request of the undercover agents) to a fraction of an inch below the 18″ legal limit. Weaver failed to appear for a court hearing resulting from the illegal firearms sale; as it later turned out, the order to appear which had been mailed to him gave an incorrect date for the hearing. A fugitive arrest warrant was issued for Weaver.



United States Marshals showed up one day in August 1992. The Weavers’ three dogs (two collies and a labrador) began barking, and Randy Weaver, his friend Kevin Harris, and Weaver’ fourteen‐​year‐​old son Sammy grabbed their guns to run and investigate.



The Marshals, wearing camouflage and carrying silenced machine guns, did not identify themselves or their purpose, but they did shoot one of the dogs. Sammy Weaver returned fire, and was promptly shot by a Marshal. Sammy turned and fled, with his nearly severed arm flopping as he ran. Sammy was promptly shot in the back. Nearby, Kevin Harris concluded that if he fled, he too would be shot; Harris fired his rifle in the direction of the marshal who had shot Sammy; the bullet killed the marshal who had shot Sammy Weaver.



Randy Weaver had only heard the shooting, but had not seen what had happened. “Come on home, Sam. Come home,” he yelled over and over. At last, Sammy called “I’m coming, Dad.” Those were apparently the last words Sammy Weaver said before he died.



Harris’s shot had disordered the Marshals, and Weaver and Harris used the opportunity to retreat to their cabin.



Later that day, Randy Weaver and his wife Vicki picked up Sammy’s dead body and carried it to a building near the cabin, where they prepared their son’s body for burial.



Over 300 government agents, led by the FBI “Hostage Rescue Team” descended on Ruby Ridge, Idaho, where Weaver’s two‐​story cabin was located. Commanding the FBI at Ruby Ridge was Richard M. Rogers, who would later serve as a field commander at Waco.(84)



The FBI rules of engagement allow use of deadly force only when necessary to protect an innocent person from imminent peril. But on the plane out to Idaho, Rogers wrote new rules of engagement for Ruby Ridge. The new rules allowed FBI snipers to shoot any adult who was armed. Since virtually everyone besieged in Idaho went outside armed (in full compliance with the laws of Idaho, and of most other states, because the armed people were on their own property) everyone was a target outside the cabin.



At Weaver’s trial in 1993, HRT Director Rick Rogers was unable to cite any authority allowing the FBI, in violation of state law, to shoot people who were posing no threat to anyone. (A provision in the 1994 federal crime bill, removed during the bill’s final movement through Congress, would have immunized federal agents from state criminal prosecution for crimes committed while on the job.)



As at Waco, a siege ensued, with the “Hostage Rescue Team” surrounding the residence of people who, far from being held hostage, simply wanted to be left alone.



At about six p.m. the next day, sixteen‐​year‐​old Sarah Weaver, her father Randy, and Kevin Harris walked out to the nearby shed to pay their last respects to Sammy. They were carrying firearms. Standing by the open door was Mrs. Vicki Weaver, holding her 10 month old daughter Elisheba.



FBI sniper Lon T. Horiuchi said that he could hit a quarter at 200 yards. Horiuchi fired, and hit Randy Weaver in the shoulder. Horiuchi later testified that Weaver was shot to keep Weaver from shooting at a helicopter overhead.



At the subsequent trial, Associate Marshal Service Director Wayne Smith testified that no helicopter was over the Weaver cabin that day, and the judge threw out the charge that Weaver had aimed a firearm at a helicopter. Sarah Weaver, Randy Weaver, and Kevin Harris fled back towards the cabin.



Sniper Horiuchi fired again, this time at a person he said he thought was Kevin Harris. (Although Harris was not even alleged to have raised any gun at any helicopter.) Horiuchi later testified that he could not identify his target clearly because he could not see through the curtains of the door. After Horiuchi had testified, the government (illegally late) turned over Horiuchi’s official report of the shooting; the drawing showed two figures standing in an open door.(85)



The FBI sniper’s .308 slug crashed into Vicki Weaver’s head with such force that skull bone fragments ricocheted into Harris, as the slug exited her body and entered his.(86) Vicki Weaver’s body fell to its knees, and her head came to rest on the floor, like a person at prayer. Randy Weaver took baby Elisheba from her arms, and lifted his wife’s head; half her face had been blown away. Her dead body was laid out on the cabin floor, and covered with a blanket.



An FBI psychological profile, prepared before the attack, called Vicki Weaver the “dominant member” of the family, thus implying that if she were “neutralized” everyone else might surrender.(87)



During the next week, “the FBI used the microphones to taunt the family. ‘Good Morning Mrs. Weaver. We had pancakes for breakfast. What did you have?’ asked the agents in at least one exchange. Weaver’s daughter Sarah, 16, said the baby, Elisheba, often was crying for its mother’s milk when the FBI’s messages were heard.”(88)



Bo Gritz, a highly‐​decorated American soldier in Vietnam, who is now a talk‐​show host and a right€wing political figure, offered to try to negotiate with Weaver.



Eight days after Vicki Weaver was shot, Gritz succeeded in convincing Weaver to surrender based on a promise that Weaver could meet with famed criminal defense attorney Gerry Spence.



Spence agreed to take the case pro bono, and in April 1993, Kevin Harris went on trial for murder, with Randy Weaver charged with conspiracy to commit murder.(89) As with the Branch Davidians, the government attempted to portray Weaver as a political and religious zealot who prophesied and then sought to create a holy war with federal agents, even though his clear goal had been to avoid government agents.(90) Weaver and Harris claimed self‐ defense, and that the government unjustifiably fired first.



With no defense evidence even introduced, the jury acquitted the accused of all charges of criminal violence, and the court fined the federal government for falsifying evidence, for withholding evidence, and for lying.(91) Weaver was convicted only of his failure to appear for the court hearing growing out of the BATF sting.(92)



The Justice Department conducted an internal review of the incident which strongly condemned governmental actions, and recommended criminal prosecution. The report has never been released the public. Its recommendations were over‐ ruled by high‐​ranking Justice Department officials.



Instead, trivial sanctions were imposed. For example, Larry Potts, the supervisor of the siege, who had approved the “shoot‐​to‐​kill” rules of engagement was given a censure, the same punishment inflicted on FBI Director Louis Freeh for losing his portable phone. Potts was then promoted to the second‐​ranking position at the FBI. The new training center for US Marhsals in New Orleans was named the “William F.



Degan” center, in honor of the marshal who had killed Sammy Weaver.



If President insists that wishes to convince the tens of thousands of Americans who belong to militia, the millions who support the patriot movement, and the 39 percent who told the Gallup poll that they think the federal government is an immediate threat to their liberty, then the President should stop the government from acting like a terrorist organization, and then slapping itself on the wrist. Rather than encouraging more use of informants, Congress should create a special prosecutor to investigate homicides perpetrated by the federal government, starting with the Weaver case.



Preserve Our National Commitment to Freedom of Speech Many people, particularly people who abhor “right‐​wing” political viewpoints, have asserted that talk show hosts, commentators, and others who speak strongly about the need to restrain the federal government are indirectly responsible for the events in Oklahoma City. Such claims are disgraceful.



When President Kennedy was assassinated in Dallas in 1963, some people attempted to link the assassination to the climate of “hate” which characterized the intense Southern opposition to President Kennedy’s legislative program, including civil rights. But quite plainly, Southern segregationsists, wrong as they were on policy matters, had nothing to do with the President’s murder.



In 1970, anti‐​war radicals blew up a math building at the University of Wisconsin. These radicals lived in an “Amerika” where important intellectual, political, and media voices proclaimed that the Vietnam war was immoral, illegal, and imperialist, and the American government was guilty of crimes against humanity. The young Bill Clinton enunciated some of these views. Yet it would be improper to blame the opponents of the Vietnam war, including young Mr. Clinton, for the criminal acts of the Wisconsin bombers.



Today, the Southern Poverty Law Center (SPLC) di
"
"The sale of the most polluting fuels burned in household stoves and open fires will be phased out in England from next year to clean up the air, the government has said. Plans to phase out the sale of house coal and wet wood have been confirmed as part of efforts to tackle tiny particle pollutants known as PM2.5, which can penetrate deep into lungs and the blood and cause serious health problems.  Wood burning stoves and coal fires are the single largest source of PM2.5, contributing three times as much of the pollution as road transport, the Department for Environment, Food and Rural Affairs (Defra) said. Sales of two of the most polluting fuels, wet wood and house coal, will be phased out from 2021 to 2023, to give householders and suppliers time to move to cleaner alternatives such as dry wood and manufactured solid fuels. These produce less smoke and pollution, and are cheaper and more efficient to burn, officials said. The environment secretary, George Eustice, said: “Cosy open fires and wood-burning stoves are at the heart of many homes up and down the country, but the use of certain fuels means that they are also the biggest source of the most harmful pollutant that is affecting people in the UK. “By moving towards the use of cleaner fuels such as dry wood we can all play a part in improving the health of millions of people. This is the latest step in delivering on the challenge we set ourselves in our world-leading clean air strategy. “We will continue to be ambitious and innovative in tackling air pollution from all sources as we work towards our goal to halve the harm to human health from air pollution by 2030.” Sales of all bagged traditional house coal will be phased out by February 2021, and the sale of loose coal direct to customers via approved coal merchants will end by February 2023. Sales of wet wood in units of under two cubic metres will be restricted from February 2021, to allow for existing stocks to be used up. Wet wood sold in larger volumes will need to be sold with advice on how to dry it before burning from this date, the government said. Manufacturers of solid fuels will also need to show they have a very low sulphur content and only emit a small amount of smoke. Prof Stephen Holgate, special adviser on air quality at the Royal College of Physicians, said: “We know that air pollution causes significant health issues across the life course. It is key that the government does everything it can to improve the air we all breathe. Today’s announcement on domestic burning is a welcome step forward, and will in time, play a role in reducing the pollution associated with PM2.5.“Inhaling combustion particles from any source is harmful, but more so than ever when it’s directly within your home. Burning coal for heat and power has to stop and strong guidance is needed to insist that if wood is burnt in approved stoves, it is non-contaminated and dry.” John Maingay of the British Heart Foundation said: “Wood and coal burning accounts for 40% of harmful levels of background PM2.5 in the UK, and our research has shown that toxic PM2.5 can enter the bloodstream and damage our heart and circulatory system. “Phasing out sales of coal and wet wood is a vital first step towards protecting the nation’s health from toxic air … however, we must not stop there. Air pollution is a major public health challenge, and it requires an urgent and bold response.” • This article was amended on 24 February 2020 to clarify that the ban applies only in England, not to the whole of the UK."
"The ten hottest years on record were all during the past two decades and the hottest global ocean temperatures ever were recorded in 2018 – a heat increase from 2017 equivalent to 100 million times that of the Hiroshima bomb. Climate change is here and it’s already wreaking havoc. The polar bear – something of a poster child for climate change – is just one of countless victims in this warming world. It’s thought that if global temperatures continue to rise by an average of 4.5°C since pre-industrial times, which is likely to happen if we do nothing to reduce our carbon emissions, half of the world’s wildlife could be lost from Earth’s most biodiverse places. As ocean temperatures melt ice sheets – the hunting grounds of polar bears – these large carnivores have to search new areas for food, which is why 52 polar bears “invaded” a Russian town in February 2019, looking for their next meal. Locals were frightened to go outside – with good reason: polar bears can, and do, hunt people.  Unfortunately, climate change is only going to make these negative interactions between humans and wildlife more common. Already, while Australia heats up, wildlife is seeking refuge in towns. Kangaroos have swarmed human settlements in search of food and flying foxes have had to be hosed down by locals to stop them from overheating. In southern Africa, more frequent droughts have meant thirsty elephants have raided villages to eat crops and pilfer water from storage tanks. Most wild animals are naturally averse to being so close to humans, so their incursions into our lives shows how desperate they are getting. As climate change begins to take its toll on humans, by reducing crop productivity for example, we are likely to become less tolerant of these sorts of human-wildlife conflicts. Poor African villagers who have had their entire yearly crop destroyed by a herd of hungry elephants can hardly be blamed for wanting to get rid of the problem by killing the animals. Sadly, elephants – like most other species – are already experiencing precipitous declines in their populations and this is almost exclusively due to human activities. Climate change will exacerbate conflicts over natural resources between and within species – ourselves included. For example, some observers have suggested climate change was partly responsible for the Arab Spring uprisings, as droughts forced people from rural areas into overcrowded cities and inflamed tensions. If conflicts within our own species can’t be overcome, there is little hope for mitigating conflicts with other species – especially as resources become scarcer. But there is a small glimmer of hope – there are effective methods to reduce damage caused by wildlife. Polar bears can be scared away from human settlements by flares and water tanks can be made elephant-proof. These technical fixes can help limit immediate conflict between wildlife and humans in the short term, providing much-needed relief in poor communities from the damaging effects of intruding wildlife. Realistically however, technical fixes to human-wildlife conflict are only a temporary stopgap. To truly address the issue, we must focus on the root cause. Carbon emissions must be reduced – not only for the sake of wildlife but for the survival of humans too.  Wildlife habitat must be protected to ensure that species have space and food without needing to enter human settlements. Equally, societies must address their insatiable demand for natural resources, reduce overconsumption and excessive waste. Much of this is easier said than done, of course. Without political will and sufficient funding all of this falls short. Global leaders must step up to the task – and it is partly up to ordinary people to pressure them to act. Movements such as the Extinction Rebellion and the school students organising global strikes against climate change are an encouraging start and must be built upon. We need to cause an uproar like our lives depend on it – because they do. We have no planet B, as the refrain goes – and neither do the planet’s 8.7m other species."
" Labor has to take the initiative in defending Australia against the dangers of climate change because the summer of catastrophe has highlighted our national vulnerability and because business and the states are now demanding national leadership, according to Anthony Albanese. As revealed by Guardian Australia, the Labor leader will use a speech to a progressive thinktank on Friday to commit the ALP to adopting a net-zero target by 2050 if it wins the next federal election, without the use of carryover credits from the Kyoto period.  While Scott Morrison is holding off from making a commitment to carbon neutrality by 2050, partly because of an internal brawl within the Coalition and partly because the prime minister says Australia should not sign up to targets in the absence of costings, Albanese will say on Friday that adopting net-zero “should be as non-controversial in Australia as it is in most nations”. According to the Labor leader’s speech notes circulated in advance, Albanese will commit Labor to adopting “a real target, with none of the absurd nonsense of so-called carryover credits that the prime minister has cooked up to give the impression he’s doing something when he isn’t”. “That’s not acting. It’s cheating, and Australian’s aren’t cheaters. A Labor government will never use Kyoto carryover credits.” Albanese will point out – as some in the government have noted publicly in recent weeks – that Australia accepted the net-zero pathway when the Coalition signed and ratified the Paris agreement. “This is what the world agreed to in Paris – Australia included.” On Friday the finance minister, Mathias Cormann, confirmed the government “will be finalising a longer-term target in time for COP26” – the next round of climate talks in Glasgow in November. “But what we will do as part of our process is to ensure that the agenda we determine to achieve any such target is environmentally effective and economically responsible,” he said. Cormann accused Labor of committing to targets without setting out the cost, warning that Albanese is “making the same mistake” as former leader Bill Shorten. Earlier, Labor’s climate change spokesman, Mark Butler, told Radio National the opposition would set out a detailed policy about how to achieve targets and its cost “well before” the next election. Butler argued that the cost of reducing emissions should not be divorced from the cost of inaction and noted Melbourne University research had found actions to reduce emissions have a benefit cost ratio of 20 to one. He also noted the CSIRO had found a net zero emissions path “will deliver higher wages and lower energy bills” in modelling relied on by the New South Wales Berejiklian government when it set its target of zero emissions by 2050. In his speech, Albanese will say that “whether the current [federal] government accepts it or not, this goal is fast becoming the reality”. “All states and territories in Australia have already promised to operate in a carbon-neutral way by 2050. “The Business Council of Australia is calling for it. AGL, Santos, BHP, Amcor, BP, Wesfarmers, Telstra and others all agree. Seventy-three countries, including the UK, Canada, France and Germany, many with conservative governments, have already adopted it as their goal. Australia should too.” Albanese will argue the lesson of the summer is preparation can help avert further tragedy, “and only positive, forward-thinking leadership can steer us through”. Courtesy of bushfires that claimed 33 lives and destroyed 3,000 homes, caused more than 1m animal deaths and saw more than 12m hectares burned, Australians have learned “we’re now living in dangerous times”. Albanese will say climate change was a factor in the bushfires, and people touched by the tragedies of the summer now understood Australia had a lot to lose. “But the good news is we also have a lot to gain.” The Labor leader will argue taking action on climate change means “more jobs, lower emissions and lower energy prices”. “We should be a clean-energy superpower, harnessing the wind and sun to spark a new manufacturing boom and power generations of jobs; developing a hydrogen industry; creating manufacturing jobs here in Australia in new industries that provide well-paid jobs. “Instead we have the government talking nonsense that they themselves have dismissed previously.” The shadow cabinet took the decision to lock in behind a net-zero target before parliament resumed for 2020. The decision – Labor’s first significant move on climate policy post-election – comes as the opposition battles internal tensions about abatement targets and the future of coal. Guardian Australia revealed on Thursday that concerns were expressed during the shadow cabinet deliberation about the risks of setting a concrete target in the absence of a roadmap to get there. There were also concerns about how Labor would answer the inevitable questions about the cost of action. Labor has not yet taken a decision about what its interim emissions reduction target will be, and that decision looms as a future flashpoint. The Morrison government will blast Labor for committing to a target without a cost-benefit calculation. But on Thursday, Labor’s Senate leader, Penny Wong, told the ABC the Coalition was not being transparent about the costs of inaction. She said the path set by the Coalition was “a path which will impose, and is imposing, greater costs on Australians than the path of taking action”. “We saw [the costs] over these recent months. We’ve seen them in the tragedies we’ve seen. We’ve seen them in the costs of drought and bushfires and natural disasters. “But there’s more than the human cost. There is an economic cost, and what we know from what many people have told us is that it will cost us far more not to act than to act.”"
nan
"Pine martens are returning to areas of the UK after an absence of nearly a century. Following releases in mid-Wales during 2015, reintroductions are proposed in north Wales and southern England for 2019. The pine marten is a small native carnivore that inhabits a range of woodland habitats. It’s an excellent climber and often nests within tree cavities. This opportunistic predator has a varied diet including fruit, eggs, songbirds and small mammals. By the 1920s, pine martens were virtually extinct in the UK after centuries of persecution to protect game birds and poultry. Only a population in north-west Scotland and small numbers in northern Wales and England survived. With UK legal protection, their range has expanded since the 1980s, increasing their encounters with the grey squirrel. Since George Monbiot penned “how to eradicate grey squirrels without firing a shot” in 2015, the media has courted the charismatic mammal as the saviour for the UK’s embattled red squirrels. The media message is simple: the return of pine martens will herald the decline or even eradication of grey squirrels, which, since their arrival from North America in 1876, have caused regional extinctions of the native red squirrel. That’s because pine martens supposedly prefer eating greys, while leaving reds alone. The optimism around pine martens in the UK originated from research in Ireland and Scotland. In Scotland, scientists studied forests containing pine martens, red squirrels and grey squirrels. The more pine martens they recorded using a woodland area, the more likely they were to find red squirrels and the less likely grey squirrels were to be there. Like earlier Irish studies, this suggested that pine martens suppress grey squirrel populations to the overall benefit of red squirrels.  However, that’s not quite the whole story. There’s a desire in the media to find heroes and villains in nature which simplifies the situation and obscures the potential impact of a returning predator on British wildlife and livestock. Sadly, ecology and conservation are rarely simple and the restoration of pine martens will not always follow a script. The Scottish pine marten researchers make clear that pine martens sometimes eat red squirrels. In a small number of other studies conducted elsewhere in Europe, reds were in fact a significant seasonal component of pine marten diet – up to 53% in one case. It’s therefore incorrect to suggest, as some conservation groups have, that dietary studies show pine martens very rarely eat red squirrels. The reality is that predation rates reflect the relative abundance of red squirrels to other prey, encounter rates and local habitat characteristics. Why grey squirrels have declined in the presence of pine martens remains uncertain. The impact of martens on greys may vary geographically and it’s unwise to simply extrapolate the findings from Scotland and Ireland to the rest of the British Isles without a note of caution. Suggesting the pine marten is the best long-term solution for grey squirrel control in England is premature and requires more research to confirm. Pine martens have been absent from much of England for around 100 years, a period of significant agricultural and urban change. Landscapes have altered dramatically and many potential prey species have regionally declined. Pine marten predation upon these could therefore prove to be locally significant. This should not be a barrier to reintroducing pine martens. Instead, it reinforces the need for informed discussions with all interest groups likely to be affected. We must acknowledge that as a last resort, lethal control of predators may be necessary to conserve rare species such as some ground nesting birds. As the pine marten becomes more common in the UK and Ireland, inevitably there will be scenarios where lethal intervention is unavoidable. A pine marten predating a seabird colony was shot in 2018 under licence to protect an internationally important breeding population. Measures to prevent predation of poultry or game birds are frequently recommended where pine marten restoration is occurring. These include the installation of electric fencing, cutting back branches overhanging pens and ensuring that wire netting has no holes martens could get through. While these management recommendations are useful, many people may find it difficult to implement them. As a result, any negative impacts of a returning arboreal predator will fall heavily upon a handful of poultry owners. The return of the pine marten may also complicate the conservation or reintroduction of other species. Although the location and other details are confidential, there were concerns that a pine marten was adversely affecting a red squirrel conservation programme after an individual was found to be regularly visiting release enclosures. As pine martens naturally spread from Scotland into northern England, adaptive and measured responses will be needed to responsibly manage their return. An approach to conservation that’s media-friendly but built on limited evidence rarely works, and certainly won’t in pine marten restoration."
"

TUCSON, July 31 (UPI) — Scientists  confirmed Thursday that water, considered an essential building block of life,  does indeed exist on the planet Mars.An analysis of a soil sample collected by the Phoenix lander  detected traces of water, which exists as ice just below the red soil on the  Martian surface.

“We’ve seen evidence for this water ice before in  observations by the Mars Odyssey orbiter and in disappearing chunks observed by  Phoenix last month,” scientist William Boynton said in a  written statement released by NASA and the Jet Propulsion Lab, “but this is the  first time Martian water has been touched and tasted.”
Boynton is lead scientist for the Thermal and Evolved-Gas  Analyzer team based at the University of Arizona.
Details of the composition of the water were not immediately  released. The sample came from a 2-inch deep trench carefully carved by the  lander’s robotic arm.
The presence of water is one of more dramatic discoveries  made by the Phoenix since it touched down on Mars near the pole May 24. NASA  announced it had secured funding to extend the Phoenix mission through Sept. 30.
More here: http://phoenix.lpl.arizona.edu/



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d675474',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Climate change is one of the few scientific theories that makes us examine the whole basis of modern society. It is a challenge that has politicians arguing, sets nations against each other, queries individual lifestyle choices, and ultimately asks questions about humanity’s relationship with the rest of the planet. The Intergovernmental Panel on Climate Change published its synthesis report on November 2, a document that brings together the findings from the IPCC’s three main working groups. It reiterates that the evidence for climate change is unequivocal, with evidence for a significant rise in global temperatures and sea level over the last hundred years. It also stresses that we control the future and the magnitude of shifting weather patterns and more extreme climate events depends on how much greenhouse gas we emit.  This is not the end of the world as envisaged by many environmentalists in the late 1980s and early 1990s, but it will mean substantial, even catastrophic challenges for billions of people. Greenhouse gases absorb and re-emit some of the heat radiation given off by the Earth’s surface and warm the lower atmosphere. The most important greenhouse gas is water vapour, followed by carbon dioxide and methane, and without their warming presence in the atmosphere the Earth’s average surface temperature would be approximately -20°C.  While many of these gases occur naturally in the atmosphere, humans are responsible for increasing their concentration through burning fossil fuels, deforestation and other land use changes.  Although carbon dioxide is released naturally by volcanoes, ecosystems and some parts of the oceans, this release is more than compensated for through the carbon absorbed by plants and in other ocean regions, such as the North Atlantic. Had these natural carbon sinks not existed, CO2 would have built up twice as fast as it has done. Records of air bubbles in ancient ice show us that carbon dioxide and other greenhouse gases are now at their highest concentrations for more than 800,000 years. The IPCC presents six main lines of evidence for climate change. We have tracked the unprecedented recent rise in atmospheric carbon dioxide and other greenhouse gases since the beginning of the industrial revolution. We know from laboratory and atmospheric measurements that greenhouse gases do indeed absorb heat when they are present in the atmosphere. We have tracked significant increase in global temperatures of 0.85°C and sea level rise of 20cm over the past century. We have analysed the effects of natural events such as sunspots and volcanic eruptions on the climate, and though these are essential to understand the pattern of temperature changes over the past 150 years, they cannot explain the overall warming trend. We have observed significant changes in the Earth’s climate system including reduced snowfall in the Northern Hemisphere, retreat of sea ice in the Arctic, retreating glaciers on all continents, and shrinking of the area covered by permafrost and the increasing depth of its active layer. All of which are consistent with a warming global climate. We continually track global weather and have seen significant shifts in weather patterns and an increase in extreme events. Patterns of precipitation (rainfall and snowfall) have changed, with parts of North and South America, Europe and northern and central Asia becoming wetter, while the Sahel region of central Africa, southern Africa, the Mediterranean and southern Asia have become drier. Intense rainfall has become more frequent, along with major flooding. We’re also seeing more heat waves. According to the US National Oceanic and Atmospheric Administration (NOAA) between 1880 and the beginning of 2014, the 13 warmest years on record have all occurred within the past 16 years. The continued burning of fossil fuels will inevitably lead to further climate warming. The complexity of the climate system is such that the extent of such warming is difficult to predict, particularly as the largest unknown is how much greenhouse gas we will emit over the next 85 years. The IPCC has developed a range of emissions scenarios or Representative Concentration Pathways (RCPs) to examine the possible range of future climate change. Using scenarios ranging from buisness-as-usual to strong longer-term decline in emissions, the climate model projections suggest the global mean surface temperature could rise by between 2.8°C and 5.4°C by the end of the 21st century. The sea level is projected to rise by between 52cm and 98cm by 2100, threatening coastal cities, low-lying deltas and small islands. Snow cover and sea ice are projected to continue to reduce, and some models suggest that the Arctic could be ice-free in late summer by the latter part of the 21st century. Heat waves, extreme rain and flash flood risks are projected to increase, threatening ecosystems and human settlements, health and security. These changes will not be spread uniformly around the world. Faster warming is expected near the poles, as the melting snow and sea ice exposes the darker underlying land and ocean surfaces which then absorb more of the sun’s radiation instead of reflecting it back to space in the way that brighter ice and snow do. Indeed, such “polar amplification” of global warming is already happening. Changes in precipitation are also expected to vary from place to place. In the high-latitude regions (central and northern regions of Europe, Asia and North America) the year-round average precipitation is projected to increase, while in most sub-tropical land regions it is projected to decrease by as much as 20%, increasing the risk of drought. In many other parts of the world, species and ecosystems may experience climatic conditions at the limits of their optimal or tolerable ranges or beyond. Human land use conversion for food, fuel, fibre and fodder, combined with targeted hunting and harvesting, has resulted in species extinctions some 100 to 1000 times higher than background rates. Climate change will only speed things up. The IPCC synthesis set in stark terms the global challenge of reducing greenhouse gas emissions. To keep global temperature rise below 2°C then global carbon emission must peak in the next ten years and from 2070 onward must be negative: we must start sucking out carbon dioxide from the atmosphere.   Despite 30 years of climate change negotiations there has been no deviation in greenhouse gas emissions from the business-as-usual pathway so many feel keeping the climate change to less than 2°C will prove impossible. The failure of the international climate negotiation, most notably at Copenhagen in 2009, set back meaningful global cuts in emissions by at least a decade.  Anticipation is building for the Paris conference in 2015 and there are some glimmers of hope.   China, now the largest greenhouse gas polluter in the world, has discussed instigating a regional carbon-trading scheme which if successful would be rolled out across the whole country. Meanwhile the US, which has emitted a third of all the carbon pollution in the atmosphere, has placed the responsibility for regulating carbon dioxide emissions under the Environment Protection Agency, away from political wrangling in Washington.  Support and money are also needed to help developing countries mitigate carbon emissions and adapt to inevitable climate change. Trillions of dollars will be invested in energy over the next 15 years to keep pace with increasing demand – what we must do is ensure that it is directed towards developing cheap, clean, secure energy production rather than exploiting fossil fuels. We must also prepare for the worst and adapt. If implemented now, much of the costs and damage that could be caused by changing climate can be mitigated. Climate change challenges the very way we organise our society. It needs to be seen within the context of the other great challenges of the 21st century: global poverty, population growth, environmental degradation, and global security. To meet these challenges we must change some of the basic rules of our society to allow us to adopt a much more global and long-term approach and in doing so develop a solution that can benefit everyone."
nan
"Car sales have raced to a ten-year high according to new figures that are being celebrated as part of wider signs of a UK recovery. In the latest monthly figures published by the Society for Motor Manufacturers and Traders 425,861 new cars were registered; a rise of 5.6% and the biggest September sales figure since 2004. These numbers represent the 31st consecutive month of growth in the new car market and bring registrations for the year so far to nearly 2m. The steady rise of new car sales is being taken as vote of confidence for the UK economy, suggesting that consumers feel comfortable enough to spend their cash. Indeed, new car sales are now at their highest since before the recession – recovering at a greater rate than most of Europe.  In part, September’s soaring sales are reflective of the fact that it is always a popular month for new cars, as it brings with it the latest registration plate. But those readily parting with their cash should be wary of the too-good-to-refuse finance deals.  These deals are the main driver of improving sales figures. Last year, almost three quarters of new cars were sold to consumers on credit. Volkswagen alone reports its loans and leasing business has increased 40% in the last two years. And these personal contract purchases are usurping the second-hand car market. Propping up the market with such deals may be unsustainable in the long run as many consumers will get a nasty shock when they realise they do not have the equity they imagined in the car and are not in a position to buy. Not only might they be put off future purchases but they find themselves unable to afford a new car following the three-year contract period.  Our insatiable appetite for new cars also raises environmental concerns. How can we ensure that our love of new wheels is sustainable? Admittedly, new petrol and diesel cars are more efficient than their forebears and have shown ever improving emissions levels since the turn of the century, but there is no room for complacency if the UK is to meet strict European Union targets for 2020. And while overall sale volumes remain extremely small at just over 3,000 (less than 1% of September’s total sales), electric cars have seen an increase of a whopping 426% on 2013. Plus, alternatively fuelled vehicles are increasingly grabbing market share from internal combustion engines as a result of more models becoming available. Private car ownership, though, remains destructive for the environment. Petrol and diesel cars still emit damaging levels of CO2 that cause localised pollution and contribute to worldwide climate change. Most electrics will be powered from fossil-fuel fired power stations, potentially causing twice the global warming of internal combustion engines. And mass-produced vehicles of all stripes require intensive production processes for their steel bodies and use up finite mineral reserves.  Even the growth in electric car sales is not necessarily good news for the environment. Their limited range means most drivers see them as suitable only for use within cities, not between them. They are often used as second cars, so supplementing not replacing larger ones. And, within urban areas, use of electric cars will replace buses, trains, trams, metros, cycling and walking – each more environmentally sustainable than any private vehicle. Some of us need to own a car, most notably in rural areas where 73% of drivers in rural areas rely on a car for shopping and 81% require one for work, in contrast with figures of 46% and 48% for towns and cities. But, with most of the UK living in urban areas, the number of cars being bought reflects desire rather than a need for new vehicles. And, with the biggest growth in owners being among those on lower incomes, the credit boom is surely responsible, meaning we need to question just how much of a good news story the new car sales figures are in the long term."
"
The Zogby poll results mirrors the recent Gallup poll It’s the economy, stupid. Even so, with opinion on Cap and trade in the minority it seems plans are in place to move forward.
On Earth Day, Secretary Chu warmly embraced the administration’s cap-and-trade proposal, stating, “We must state in no uncertain terms we have a responsibility to our children to curb emissions from fossil fuels…”
Q. President Obama wants to impose cap-and-trade laws that would limit the total carbon dioxide emissions allowed to be released into the environment. These laws would turn carbon dioxide into a commodity allowing those that pollute less to sell credits to those that pollute more. These credits would be traded on commodities markets. According to congressional testimony given by the Director of the nonpartisan Congressional Budget Office, “decreasing emissions would also impose costs on the economy – much of those costs will be passed along to consumers in the form of higher prices for energy and energy intensive goods.” Some have estimated these costs to be $800 to $1300 more per household by 2015. Knowing this, do you support or oppose cap-and-trade laws?

Support                 30%
Oppose                  57%
Not sure                13%
Q. Which course of action should America take with regards to energy
policy?
Make energy cheaper by developing all sources of U.S. energy, including coal, nuclear power, offshore drilling and drilling in the Arctic National Wildlife Refuge                            54%
Reduce America’s production of fossil fuels that might cause global warming                                   40%
Not sure                                                           6%
The O’Leary Report/Zogby poll was conducted April 24-27 of 3,937 voters nationwide and has a margin of error of plus-or-minus 1.6 percentage points. Slight weights were added to party, age, race, gender, education to more accurately reflect the population. Margins of error are higher in sub-groups.
Brad O’Leary is publisher of “The O’Leary Report,” a bestselling author, and is a former NBC Westwood One talk show host. His new book, “Shut Up, America! The End of Free Speech,” is now in bookstores. To see more poll results, go to www.olearyreport.com.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e968b9be0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The megafires of Australia’s summer “are a harbinger of life and death on a hotter Earth”, a climate summit has said in a forceful declaration for urgent and dramatic climate action. The Climate Emergency Summit, held in Melbourne this week and of which Guardian Australia was a partner, released a declaration saying the warming world was a clear threat to Australian society and civilisation.  “The climate is already dangerous – in Australia and the Antarctic, in Asia and the Pacific – right around the world. The Earth is unacceptably too hot now,” the declaration said. “If the climate warms 1.5 degrees above pre-industrial levels, the Great Barrier Reef will likely be lost, sea levels could rise metres and massive global carbon stores such as the Amazon and Greenland, will hit tipping points, releasing millions of tonnes of carbon into the atmosphere.” Signatories to the declaration included Ian Dunlop, Carmen Lawrence, John Hewson, Tim Costello and Kerryn Phelps. It warned that even the Paris agreement emissions reduction targets would put the world on a path to 3.5C warming by 2100, and 4C to 5C warming “when long-term climate-system feedbacks were factored in”. Does climate change cause bushfires? The link between rising greenhouse gas emissions and increased bushfire risk is complex but, according to major science agencies, clear. Climate change does not create bushfires, but it can and does make them worse. A number of factors contribute to bushfire risk, including temperature, fuel load, dryness, wind speed and humidity.  What is the evidence on rising temperatures?  The Bureau of Meteorology and the CSIRO say Australia has warmed by 1C since 1910 and temperatures will increase in the future. The Intergovernmental Panel on Climate Change says it is extremely likely increased atmospheric concentrations of greenhouse gases since the mid-20th century is the main reason it is getting hotter. The Bushfire and Natural Hazards research centre says the variability of normal events sits on top of that. Warmer weather increases the number of days each year on which there is high or extreme bushfire risk. What other effects do carbon emissions have? Dry fuel load - the amount of forest and scrub available to burn - has been linked to rising emissions. Under the right conditions, carbon dioxide acts as a kind of fertiliser that increases plant growth.  So is climate change making everything dryer?  Dryness is more complicated. Complex computer models have not found a consistent climate change signal linked to rising CO2 in the decline in rain that has produced the current eastern Australian drought. But higher temperatures accelerate evaporation. They also extend the growing season for vegetation in many regions, leading to greater transpiration (the process by which water is drawn from the soil and evaporated from plant leaves and flowers). The result is that soils, vegetation and the air may be drier than they would have been with the same amount of rainfall in the past. What do recent weather patterns show? The year coming into the 2019-20 summer has been unusually warm and dry for large parts of Australia. Above average temperatures now occur most years and 2019 has been the fifth driest start to the year on record, and the driest since 1970. Is arson a factor in this year's extreme bushfires? Not a significant one. Two pieces of disinformation, that an “arson emergency”, rather than climate change, is behind the bushfires, and that “greenies” are preventing firefighters from reducing fuel loads in the Australian bush have spread across social media. They have found their way into major news outlets, the mouths of government MPs, and across the globe to Donald Trump Jr and prominent right-wing conspiracy theorists. NSW’s Rural Fire Service has said the major cause of ignition during the crisis has been dry lightning. Victoria police say they do not believe arson had a role in any of the destructive fires this summer. The RFS has also contradicted claims that environmentalists have been holding up hazard reduction work. “National security analysts warn that 3C may result in “outright social chaos”, and 4C is considered incompatible with the maintenance of human civilisation. “Climate change must be accepted as an overriding threat to national and human security, with the response being the highest priority at national and global levels.” The declaration called on governments to commit to rapidly reducing greenhouse gas emissions to zero, to drawing down carbon concentrations already in the atmosphere, and to integrating adaptation and resilience measures into restructured national and global economies. The executive director of Micah Australia, Tim Costello, told the Guardian the declaration was a rallying cry to emphasise the critical nature of the climate challenge. “This summit was people from military, agriculture, from politics, from economics: we’re all frustrated, we want to see action and a breakthrough, we’re all working hard in our areas, but none of us actually know what will be the tipping point, when it will finally be widely realised that this is an emergency and we have to decarbonise,” Costello said. “Like people understood the emergency of war, there will be a suspension of politics and human rights if we don’t deal with climate.” He said party politics had failed Australia, and shown itself incapable of dealing with the climate emergency. And, he argued, climate change as an existential threat had long been a reality for communities across Australia’s region. “I have seen the poorest communities already losing lives and livelihoods for years from climate change. Now it is our existential challenge after these bushfires, whereas at the Pacific Islands Forum our prime minister was told ‘it is only economic for you, it is existential survival for us’.” The declaration said Australia’s political leaders were especially culpable, guilty of short-term political expediency, which had left Australians acutely exposed to the impacts of climate change. “The first duty of a government is to protect the people, their well-being and livelihoods. Instead, Australian governments have left the community largely unprepared for the disasters now unfolding, and for the extensive changes required to maintain a cohesive society as climate change impacts escalate.” The declaration argued it was in Australia’s self-interest to demand greater global action on climate change, and a continued reliance on fossil fuel resources was unsustainable, both economically and environmentally. Australia was the world’s fourth largest carbon polluter, exports included, and one of the countries most exposed to climate change, the declaration said. “It makes no sense to build our economy on fossil fuel resources, practices and technologies which are unsustainable, particularly when Australia has some of the best clean energy resources and opportunities in the world.”"
"
Share this...FacebookTwitterSteffen Hentrich of the German liberal institute presents a comparison of the safety of various sources of energy. Much of the media, many politicians and a host of activists have all told us that nuclear energy is too dangerous to be used by man.
Is this claim founded on solid data and facts, or is it run-away hysteria? You be the judge. Shown are the number of deaths per terawatt-hour of energy produced.

Source: http://nextbigfuture.com/2011/03/lowering-deaths-per-terawatt-hour-for.html
By far, probably to James Hansen’s glee, coal, with its dangerous mining work, is the most dangerous of all. Surprisingly, even hydro, wind, and solar power are far more hazardous than nuclear.
Photovoltaic power generation kills, per terawatt-hour, 11 times more people than nuclear. 
Using the arguments of the anti-nuclear activists, we can say there needs to be an immediate moratorium on windmills, photovoltaic panels and biomass plants. Compared to nuclear, they’re simply too deadly.
So what does all this mean? Hentrich writes:
With this in mind, nuclear energy alone becomes the scapegoat, and this illustrates the glaring denial of reality in politics and in the public. This also shows that a rapid stop of nuclear power generation will in no way reduce the risks involved in power generation.”
Indeed, stopping nuclear power makes the power generation industry much more dangerous, and not safer.
Share this...FacebookTwitter "
"As the electric saw cuts into the base of the horn of the live rhino lying at my feet, I feel an uncomfortable guilt. The rhino shakes and judders and there is an unpleasant smell reminiscent of burning hair. I glance nervously at the friends around me, clad in khaki and camouflage.  But luckily for this rhino, I wasn’t a poacher and there was no blood or bounty – I was there as part of a conservation drive. Just over a month ago I was involved in a local operation to de-horn white rhinoceros in South Africa. The idea is that by removing the horn, we remove the motive for poaching. However, I have a conflict of emotions about this process: taking the horn from a rhino is what the bad guys do. In an ideal world we wouldn’t do this, but it’s for the good of both the individual rhino and the species. At the end of the operation, our rhino walks off to go about its life. Much like your hair and nails, rhino horn is made up of keratin – removing it is painless and it will slowly grow back. But if poachers get to our rhino first, they will almost inevitably kill it to cut off its horn. And its death is unlikely to be painless. Another method to put off poachers is to relocate the rhino. I’ve been involved in transporting rhino hundreds of miles away in massive crates on the back of giant trucks to move them to a safer location.  De-horning and translocation both require rhino capture, helicopters, fast 4x4 drivers, wildlife vets, and a general lack of fear of a very large animal (which is never totally asleep). It is an epic and dangerous operation; for the rhino and the team. Yet, despite these drastic and costly measures on the ground, Africa continues to lose rhino to poaching at an alarming and unsustainable rate, fuelled by demand from the medicinal black market in Asia. Wildlife (or environmental) crime goes way beyond the White Rhinoceros, but this species serves as a flagship for Africa and for conservation. If we cannot save the iconic rhino from extinction at the hands of an illegal trade, then what hope have we for controlling the bushmeat killing fields, saving baby chimpanzees from abduction into the pet or entertainment trades, halting ecosystem-destroying industrial pollution, or preventing the loss of great swathes of forest and their dependent biodiversity? The list goes on.  Wildlife crime is pervasive across the world and ultimately undermines the functioning of ecosystems that we (the human species) depend on for life. Wildlife crime is often international; goods are trafficked across borders from where the natural resource is available to where it is desired. This is why we need an effective international investigative and enforcement network and why we need it operating on the ground in Africa. Interpol aims to connect enforcement agencies and partners across international boundaries – and it recently announced the formation of a new dedicated wildlife crime team in East Africa.  It is not that nations with wildlife crime lack motivation to solve their problems. I have seen first-hand the prevention and enforcement efforts that the Department of Environment and Nature Conservation, the South African government unit tasked with protecting biodiversity, employ to tackle rhino poaching. But current efforts to tackle wildlife crime across the continent and beyond are clearly not sufficient, and any assistance or improved resourcing can only help.  The budget of African enforcement and conservation organisations is dwarfed by the money that international criminal organisations command. One kilogramme of rhino horn is estimated to be worth US$65,000. That is more expensive than the equivalent weight of gold or cocaine. With an adult white rhinoceros averaging a mass of six kilogrammes across its two horns, the horn of a single rhino can represent a staggering US$390,000 to the seller. The support that Interpol can bring will help to redress the resourcing imbalance. The impetus for the creation of the new Interpol team stems from a recent report highlighting increasing losses of African elephants, alongside rhino, to poaching in East Africa. Global ivory seizures reached a new high in 2013 and, like the rhino, elephants are similarly in serious need of effective conservation action.  Sharing of intelligence between agencies in Africa and Asia and ports-of-call in between is likely to be the most effective route to successfully reducing the kill rate of the African elephant and the white rhino, the two largest land mammals on Earth.  Attack the problem from both ends of the chain; target the poachers in Africa (supply) and the dealers in Asia (demand), while providing long-term environmental education and introducing appropriate legalisation of some wildlife products, and we may be better able to save the elephant, rhino, and other species. By setting up the dedicated environmental crime team, which will be based in Nairobi, Interpol can provide the resourcing and support to boost local efforts and integrate response. Basing the team in Kenya is also a good move. Kenya has one of the highest elephant and rhino populations in Africa. This makes it important for wildlife, but also vulnerable to poaching. Kenya lost at least 59 rhino and 302 elephant to poaching in 2013 alone.  Kenya is home to both the southern and northern subspecies of white rhinoceros in addition to the black rhinoceros. Only a few days ago, the death of Suni, the first captive-born northern white rhino, means that this branch of the rhino family is now probably functionally extinct. Suni was one of only two surviving males (the other, Sudan, is 40 (at the time of publication) – fairly old in rhino years – and there is now a population of just six individuals remaining. The northern subspecies was pounded into its current depressing state by years of illegal slaughter. The depletion of this subspecies is an indication of what happens when we fail to act effectively against illegal wildlife crime. We ultimately lose the wildlife. Further investment in fighting wildlife crime, and international collaboration, in tandem with the aforementioned long-term actions, gives us hope that we can avoid the same fate befalling other species. This article was amended on August 24 2016 to correct the surviving northern white rhino male to Sudan, age 40."
"Curious Kids is a series for children of all ages, where The Conversation asks experts to answer questions from kids. All questions are welcome: find out how to enter at the bottom of this article.  Why do we have different seasons at specific times of the year? – Shrey, age nine, Mumbai, India Over the course of a year, the Earth goes on a journey around the Sun. The reason we have seasons is because, during its journey around the Sun, the Earth is tilted.  The Earth’s tilt affects the amount of daylight each hemisphere gets, which in turn makes the temperature hotter or colder.  For example, if you live in the northern hemisphere – that’s north of the equator, like in Europe, USA, or India – then winter happens in December, January and February. That’s when the northern hemisphere is tilted away from the Sun, and the days are shorter.  For anywhere south of the equator, such as Australia or Latin America, it’s summer during these months. That’s because the southern hemisphere is tilted toward the Sun, and the days are longer.  Every season has a middle point. In summer and winter, these midpoints are called solstices. The summer solstice is the longest day, and shortest night, of the year. The winter solstice is the shortest day of the year, and the longest night.  In spring and autumn, the midpoints are called the equinoxes. At the spring and autumn equinoxes, day and night are the same length.  For thousands and thousands of years – right back to the Stone Age – people have known how to work out when the solstices and equinoxes happen throughout the year.  Indeed, they built hundreds of amazing stone circles – like the famous Stonehenge – all over Europe, which marked certain times of the seasons across the year.  These days, we even know how to calculate the seasons on other planets. For example, the next Spring equinox on Mars is on the 23rd March.  To understand how this works, imagine a small ball (representing the Earth) moving around a lightbulb (the Sun) in a circle. Let’s say the ball has a line drawn around the middle, representing the equator. If you have these things at home, you can try this yourself.  As the ball moves around the lightbulb, the half closest to the light will be lit, while the other half will be in darkness. One full circle around the lightbulb represents one full year on Earth.  As you move the ball around the lightbulb, try spinning it between your fingertips, so that the light always shines directly onto the equator.  If the Earth span like this, day and night would be the same length all year round, and there would be no seasons. Now, take that small ball and tilt it at an angle, so that the light from the bulb no longer shines directly on the equator. If you are doing this at home, it might help to colour in either the top or bottom half of the ball.  Now the hemispheres of the ball will get different amounts of light at any one time. The hemisphere tilted away from the bulb gets less light, and the hemisphere tilted towards the bulb gets more.  That means it’s “summer” in the hemisphere tilted towards the lightbulb, and “winter” in the hemisphere tilted away. Keeping the ball at the same angle, move it to the other side of the light bulb. The hemisphere that was tilted away from the bulb is now tilted towards it. So, the hemisphere that was in “winter” when you started moving the ball, is now in “summer”, and the hemisphere that was in “summer” is now in “winter”.  The same thing happens as the Earth moves around the Sun, which is what gives us different seasons at specific times of the year.  Remember, the decrease in sunlight and colder temperatures you get during winter is not because the hemisphere is further away, but because the sun is above the horizon for a much shorter time.  Hello, curious kids! Have you got a question you’d like an expert to answer? Ask an adult to send your question to us. You can: * Email your question to curiouskids@theconversation.com 

* Tell us on Twitter by tagging @ConversationUK with the hashtag #curiouskids, or

* Message us on Facebook. Please tell us your name, age and which town or city you live in. You can send an audio recording of your question too, if you want. Send as many questions as you like! We won’t be able to answer every question, but we will do our best. More Curious Kids articles, written by academic experts: Why has nobody found any life outside of Earth? – Anna, age 12, Sydney, Australia People say that everything is made of molecules. Are feelings made of molecules? Is sound made of molecules? – Claire, age six, Bristol, UK Do cats and dogs understand humans when they make miaowing or barking noises? – Mila, age 11 and Alex, age eight"
"
A Yogi Berra moment – “it’s deja vu all over again…”
From NHC Public Advisory #25
DATA FROM AN AIR FORCE RECONNAISSANCE AIRCRAFT INDICATE THAT MAXIMUM SUSTAINED WINDS HAVE INCREASED TO NEAR 150 MPH…240 KM/HR…WITH HIGHER GUSTS.  GUSTAV IS AN EXTREMELY DANGEROUS CATEGORY FOUR HURRICANE ON THE SAFFIR-SIMPSON HURRICANE SCALE. SOME FLUCTUATIONS WITH AN OVERALL SLIGHT STRENGTHENING IS FORECAST DURING THE NEXT 24 HOURS…AND GUSTAV COULD REACH CATEGORY FIVE INTENSITY DURING THIS PERIOD. GUSTAV IS FORECAST TO REMAIN A MAJOR HURRICANE THROUGH LANDFALL ALONG THE NORTHERN GULF COAST.
Here is my own hurricane track imagery of Gustav and Hanna:

Click for a Hi Definition image
I’m sure this will become mass-media fodder again for the ever popular “global warming causes more damaging hurricanes”, but it is important to note that NHC’s own science officer, Christopher Landsea, co-authored a paper that claims otherwise. So have other scientists.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9cec94d3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

When Senate Majority Leader‐​to‐​be Tom Daschle asked Jim Jeffords what he wanted in return for switching political sides, the junior senator from the Green Mountain State asked to be chairman of the Environment and Public Works Committee.



That’s the only take‐​home prize Jeffords gets for his defection, so it must be important. And telling.



One can imagine, after this promise, what happened at Jeffords’ meeting with Messrs. Bush and Cheney: “OK, Senator, what does it take to keep you with us?”



Jeffords: “Change your position on the Kyoto Protocol.”



Cheney: “Hit the road, Jim.”



The truth is that Kyoto and global warming will be the focus of Jeffords’ Environment Committee. Expect him to parade witness after witness, on the hottest summer days, decrying Bush & Co. for being “out of touch” with the world on this issue. Robert Watson, head of the United Nations’ Intergovernmental Panel on Climate Change, which bills itself as the “consensus of scientists,” is sure to get top billing.



When he appears, Watson will trot out the U.N.’s new “Third Assessment Report” on climate change, a compendium of more than 1,000 pages that’s due to hit the streets in a couple of months. With this report, Jeffords will show us how loony the current Administration is about planetary heating.



Unlike the U.N.’s first and second Assessments, published in 1990 and 1996, this one purports to tell world leaders what to do about climate change. Here’s what the “consensus of scientists” prescribes (it can be found on page 12 of the “Policymakers Summary” of the new report):



“emissions/​carbon/​energy taxes, tradable or non‐​tradable permits [indirect taxes], subsidies [which require taxes], deposit/​refund systems [taxes and regulations], technology or performance standards [regulations by fiat], product bans [let’s outlaw coal!], voluntary agreements [ha!], government spending and investment [taxes], and support for research and development [paid for by taxes].”



Who is “out of touch” here? Each of these general policy prescriptions requires confiscation of individual wealth to cool the planet or adapt to warming. Apparently, the “consensus of scientists” is that people and markets are too stupid to do this on their own. Did it ever occur to the United Nations that if global warming is as terrible and costly as it thinks it is, there will be a substantial market for technologies and products that would reduce that cost? The U.N. thinks we would rather sit around and fry, unless we are taxed into mending our evil ways.



In fact, there’s good scientific evidence that we adapt well to hot days. Some of today’s most in‐​demand technology, in fact, is designed to prevent death from heat stroke, an affliction that was more common decades ago. The technology is called “air conditioning.”



Are we quietly adapting or passively frying? My University of Virginia colleague Robert Davis and I recently looked at heat‐​related mortality data from major American cities for the last 40 years. At first, we found what everyone seems to know: In some cities, mainly older ones, daily death rates skyrocket on exceedingly hot days. Therefore, the United Nations tells us, if we heat things up more, “several thousand” more people will die every summer in North America because of global warming. The truth is that cities have been heating up by themselves, without global warming, for hundreds of years, compromising our ability to measure the earth’s true temperature.



But when we looked at the trends in heat‐​related deaths, we found that in most cities, the deaths occurred early on–in the 1960s and 1970s. By the time we get to the 1990s, we have engineered heat‐​related deaths out of most cities, with electrically driven air conditioning. In fact, the last big urban die‐​off, in Chicago in July 1995, occurred largely because there was a power failure. Our results have been presented at several peer‐​screened professional conferences and written up many times, all to favorable review.



We don’t expect Jeffords to invite us before his Committee, because we’re “out of touch,” too. Our results show that free markets, not taxation, create the capital that people use to invest in technologies that shield them from the vagaries of our naturally hostile environment. And there’s the problem: When it comes to the environment, Jeffords, the Kyoto Protocol, and the United Nations believe more in taxation and coercion than they do in free markets and free will.
"
"

If you’re free Friday morning, you might want to hop on over to the Russell Senate Office Building to learn about the amazing, inexplicable, short‐​sighted market bias against straw‐​bale buildings and the need for the feds to do something about it. The Environmental & Energy Study Institute, the sponsor of this event, 



Invites you to learn how the ‘new but old’ method of straw‐​bale construction can help address some of our most serious national policy challenges, such as record energy prices and unemployment, inadequate supply of affordable housing, the threat of climate change, and pressing needs in transportation and infrastructure funding. The modern building industry places heavy demands on the energy and transportation sectors. Straw is a locally‐​sourced, widely available, and renewable resource that builders, architects, engineers, and home owners are turning into affordable, safe, durable, and energy‐​efficient buildings in many climates. The following presenters will discuss the benefits of using this American invention, the regulatory barriers and institutional biases against straw‐​bale construction, and the role of the federal government in resolving these issues.



And that parable about the three little pigs? A PR smear spun by “Big Brick” no doubt.
"
"

Pundits, politicians and the press have argued that global warming will bring disaster to the world, but there are good reasons to believe that, if it occurs, we will like it. Where do retirees go when they are free to move? Certainly not to Duluth. 



People like warmth. When weather reporters on TV say, “It will be a great day,” they usually mean that it will be warmer than normal. The weather can, of course, be too warm, but that is unlikely to become a major problem if the globe warms. Even though it is far from certain that the temperature will rise, the Intergovernmental Panel on Climate Change (the U.N. body that has been studying this possibility for more than a decade) has forecast that, by the end of the next century, the world’s climate will be about 3.6 degrees Fahrenheit warmer than today and that precipitation worldwide will increase by about 7 percent.



The scientists who make up this body also predict that most of the warming will occur at night and during the winter. In fact, records show that, over this century, summer highs have actually declined while winter lows have gone up. In addition, temperatures are expected to increase the most towards the poles. Thus Minneapolis should enjoy more warming than Dallas; but even the Twin Cities should find that most of their temperature increase will occur during their coldest season, making their climate more livable.



Warmer winters will produce less ice and snow to torment drivers, facilitating commuting and making snow shoveling less of a chore. Families will have less need to invest in heavy parkas, bulky jackets, earmuffs and snow boots. Department of Energy studies have shown that a warmer climate would reduce heating bills more than it would boost outlays on air conditioning. If we currently enjoyed the weather predicted for the end of the next century, expenditures for heating and cooling would be cut by about $12.2 billion annually.



The cost to Americans of building dikes and constructing levees to mitigate the damage from rising seas would be less than $1 billion per year, an insignificant amount compared to the likely gain of over $100 billion for the American people as a whole.



Most economic activities would be unaffected by climate change. Manufacturing, banking, insurance, retailing, wholesaling, medicine, educational, mining, financial and most other services are unrelated to weather. Those activities can be carried out in cold climates with central heating or in hot climates with air conditioning.



Certain weather‐​related or outdoor‐​oriented services, however, would be affected. Transportation would benefit generally from a warmer climate, since road transport would suffer less from slippery or impassable highways. Airline passengers, who often endure weather‐​related delays in the winter, would gain from more reliable and on‐​time service.



The doomsayers have predicted that a warmer world would inflict tropical diseases on Americans. They neglect to mention that those diseases, such as malaria, cholera and yellow fever, were widespread in the United States in the colder 19th century. Their absence today is attributable not to a climate unsuitable to their propagation but to modern sanitation and the American lifestyle, which prevent the microbes for getting a foothold.



It is actually warmer along the Gulf Coast, which is free of dengue fever, than on the Caribbean islands where the disease is endemic. My own research shows that a warmer world would be a healthier one for Americans and would cut the number of deaths in the U.S. by about 40,000 per year, roughly the number killed on the highways.



According to climatologists, the villain causing a warmer world is the unprecedented amount of carbon dioxide we keep pumping into the atmosphere. As high school biology teachers emphasize, plants absorb carbon dioxide and emit oxygen.



Researchers have shown, moreover, that virtually all plants will do better in an environment enriched with carbon dioxide than in the current atmosphere, which contains only trace amounts of their basic food. In addition, warmer winters and nights would mean longer growing seasons. Combined with higher levels of CO2, plant life would become more vigorous, thus providing more food for animals and humans. Given a rising world population, longer growing seasons, greater rainfall, and an enriched atmosphere could be just the ticket to stave off famine and want.



A slowly rising sea level constitutes the only significant drawback to global warming. The best guess of the international scientists is that oceans will rise about 2 inches per decade. The cost to Americans of building dikes and constructing levees to mitigate the damage from rising seas would be less than $1 billion per year, an insignificant amount compared to the likely gain of over $100 billion for the American people as a whole.



Let’s not rush into costly programs to stave off something that we may like if it occurs. Warmer is better; richer is healthier; acting now is foolish.
"
"
Share this...FacebookTwitterNOAA 8 inch rain gage. Source: http://www.crh.noaa.gov/iwx/?n=coop_station
German Weather Service meteorologist Christoph Hartmann writes what I think is a surprising essay on measuring precipitation, and the errors in doing so. Indeed Hartmann says precipitation may be understated by up to 50%, or much more at some locations.
As Hartmann explains, measuring precipitation is by no means an exact science, and results have to be taken with a lump of salt.
There are many sources of errors, and in his essay here he looks at just two main sources: wind and instrumentation.
But first, let’s take a look at how precipitation is measured. In his previous essay he described two types of precipitation measuring gages. In Germany precipitation is measured with the unit of liters/m², e.g. 25.4 liters is an inch of rain.
Two methods of measuring precipitation
Hartman explains that precipitation is generally measured by a rain gage with a known opening area, for example 200 cm² in Germany, which is positioned 1 meter above the ground surface. The gage funnel catches the precipitation and leads it to either
1) a graduated measuring tube or a
2) an optical drop counter 
Optical rain gage (drop counting). Source: atmos.washington.edu
With the measuring tube system, the tube is graduated and the amount of precipitation can be simply read off. With the optical rain gage (drop counter), the amount of precipitation is derived from the number of drops. If the precipitation is snow or ice, then the measuring tube or optical gage are brought inside and the captured precipitation is melted and measured.
Wind and errors up to 400%
Hartmann explains that the biggest sources of error are wind-related. This is easily seen when measuring snowfall. Just before a snowflake falls into the gage, air turbulence sucks it back out tosses it overboard. Just taking a look around after a blizzard, it’s easy to imagine how difficult it is to measure snowfall. Places exposed to wind are barren, while other places are covered by meter-deep snowdrifts. How much snow really fell?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Hartmann says measurement errors of up 400% can occur over time when measuring powdery snowfall in alpine, polar or windy areas.
One way to reduce error is to place the instrument in a wind-protected area. By measuring the wind speed, it is then possible to adjust precipitation measurements. But Hartmann writes:
Wind effects lead to an under-estimation of the actual fallen precipitation. The level of deviation depends on the speed of the wind and the type of precipitation.
Because wind speeds are factored into precipitation measurements, climatological precipitation trends without taking changes in wind speeds into account should always be deduced very carefully.
The second problem encountered arise from the two above described measurement instruments, especially with the optical rain gage, writes Hartmann. With frozen precipitation, the gages are heated up in order to melt the precipitation. But this involves evaporation. And under torrential rains, the optical gage becomes much less accurate. The result, writes Hartmann:
Under equal precipitation amounts, the optical gage measures less precipitation than the measuring tube, both in summer and in winter.
So if two different stations use different instruments, them they will show different precipitation amounts even when the actual precipitation is the same. In summary, Hartmann writes his stunning conclusion:
In total these two sources of errors lead to a precipitation deficit of 5 to 15% for liquid precipitation, and between 20 and 50% for solid [frozen] precipitation. In very windy locations, the deficits are substantially more.
Because instruments measure less precipitation than what actually falls, it means we have a worldwide precipitation deficit solely because of the measurement method.
What does it all mean? Are many of the reported droughts solely the product of faulty readings? And we all thought that the network of temperature measurement stations was a mess. This is a huge open floodgate to potential climatological data manipulation and bogus assertions. See here for example: motherjones – the coming mega-drought (h/t NTZ reader DirkH).
Share this...FacebookTwitter "
"

The administration continues to hype global warming despite the larger fish sizzling on Washington’s grill. Every month Vice President Gore holds a press conference with a federal climatologist in tow to show that yet another record has been set. But there’s a story behind this story that the 2,000 environmental journalists out there have somehow managed to miss, despite the fact that all the information they need is available from government sources on the Internet. 



The tawdry misuse of history began, as predicted by many of us, as few days (eight) after New Year’s as were required to pull together enough data to spell W-O-L-F. And seven monthly wolves later, it turns out that Gore is really not talking about the globe’s temperature after all, and the science he’s peddling hasn’t even been peer reviewed. 



Scientists suspected something was funny with the first federal pronouncement, in January, that said that temperatures in 1997 were the warmest ever measured. Other temperature histories, such as those of NASA, showed no such thing. Global satellite readings put 1997 smack in the middle of the range for the last two decades. 



It turns out that the new history cited by Gore was developed for political impact. It was developed, according to a paper released on June 9 by the National Climatic Data Center (NCDC), a unit of the Department of Commerce, because “climate change questions involve a body politic and busy elected officials [shouldn’t that be “one busy elected official”?] attuned to rapid information delivery.” Further, “Timely climatic information provided when there is a maximum of interest [read: when it’s hot] may be the best way” to communicate about climate. 



Scientists suspected something was funny with the first federal pronouncement…that said that temperatures in 1997 were the warmest ever measured…Global satellite readings put 1997 smack in the middle of the range for the last two decades. 



This “history” was never peer reviewed. It was sent around via e‐​mail by NCDC, which explained, “Our methodology was not documented in the open refereed literature. This [memorandum] is an attempt to provide that documentation.” 



Sending an e‐​mail to everyone is not quite the same as peer review. 



Nor was this history a record of global temperature after all. Instead, according to NCDC, it is an “index” that combines three different measures, kind of like putting fruit salad in a blender. 



The three measures were land surface temperatures, which by definition are hardly global; sea surface temperatures taken from ships; and data from a network of buoys whose deployment was begun in the mid‐​1980s. The last two measurements are very different from the first, and in order to create the desired fruit salad, NCDC adjusted the sea surface temperature data up by 25 percent after 1982. That certainly might make things appear to be a bit warmer in recent years! 



In point of fact, the sea surface temperature data are increasingly at odds with air temperatures taken over the ocean. No one knows the reason for this, but the air temperatures just happen to match up perfectly with those recorded by NASA’s satellites, which happen to match up perfectly with the Weather Bureau’s (what it was called before it became a “service”) weather balloons. None of those records shows a lick of global warming in the last 20 years. 



Parenthetically, we might note that recent reports about the satellite data being in error are themselves in error. Annual temperature averages taken by weather balloons look exactly like those measured by the satellites. So the satellite cannot be wrong unless, somehow, thermometers in the 1,125,000 weather balloons launched over the last 20 years have been making exactly the same mistakes in temperature measurement as the satellites. 



In order to reassure all the recipients of their e‐​mail that their new blended‐​index approach was a good idea, NCDC observed that the three records jammed together looked an awful lot like NASA scientist James Hansen’s global temperature history. “The match is very good,” they wrote. But the Hansen history does not remove the effect of urban warming, which is known to bias global temperatures by about 0.2 degree. No wonder it’s so hot. 



So NCDC had a choice: Either use sea surface temperature data that disagree with marine air temperatures and data from satellites and weather balloons, or use one of those three mutually agreeable records that all show no warming. Guess which choice they made for “busy elected officials”? 



This isn’t temperature measurement, it’s hot air.
"
"For decades now, GDP has been the standard measure of a nation’s well-being. But it is becoming clear that an economic boost may not be accompanied by a rise in individual happiness. While there are many reasons for this, one important factor is that as nations become richer, environmental features such as green space and air quality often come under increasing threat. The mental health benefits of access to parks or waterfronts, for instance, have long been recognised but more recently researchers have also started to look at the role air pollution can play in our general mental health and happiness. With more tangible outcomes such as health, cognitive performance or labour productivity, the adverse effects of poor air are significant and well-established. The link to infant mortality and respiratory disease is well known, and the World Health Organisation estimates that around 7m deaths are attributable to air pollution each year. But while many people will die and many more will acquire a chronic health condition, focusing on objective indicators such as these may still understate the true welfare cost. This is because there is now good evidence of a direct link between air quality and overall mental health and happiness. This evidence comes from a diverse array of studies in different countries and using different analytical approaches. The most compelling of these studies track the same people over time, and find that changes in the air quality in these people’s neighbourhoods are related to changes in their self-reported happiness. One particularly innovative study looked at what happened when large power plants in Germany were fitted with equipment designed to reduce emissions. Researchers had access to happiness data from a long-term survey of a panel of around 30,000 Germans, and categorised everyone by whether they lived upwind or downwind of a power plant (or nowhere near).  The research found that those downwind underwent a significant improvement in their happiness levels after the installation, while their upwind neighbours did not benefit. This sort of comparison – a natural experiment that would be impossible and perhaps unethical to replicate in a lab – helps to ensure that the improvement in happiness was due to the improvement in air quality as opposed to other factors. Economists and scientists are continually on the lookout for new ways to test the association. One example, recently published in Nature Human Behaviour, comes from China. Researchers looked at the sentiment expressed in 210m geotagged messages on the microblog platform Sina Weibo (a Chinese equivalent to Twitter). Given they knew where these tweets had been sent from, and how happy or sad they were, the researchers were then able to match the tweets to a daily local air quality index, providing a real-time connection between air pollution and happiness. Analysing data from 144 Chinese cities, they found that self-reported happiness was significantly lower on days with relatively higher pollution levels. This study adds to a pile of research which suggests that air pollution can be detrimental to happiness – but we still need more research on why this is. While health is undoubtedly a factor, we know from studies that control for health status that air pollution affects happiness over and above any indirect effects on physical condition. Some possible reasons for the direct link include aesthetics such as haze, smell and even taste, as well as anxiety about personal health or the health of others. Air pollution has also been a focus of several studies on cognitive impairment, but it is still too early to say if it really plays a role in brain health.  Improving the well-being of citizens remains an obvious and important aim of public policy. To date, the principal focus has been on material well-being but many social scientists and indeed policy makers now argue that we need to take account of how people think and feel about the quality of their life. This is not to ignore material factors like income or physical health. Rather, a comprehensive picture of societal well-being needs to integrate objective indicators with subjective measures like happiness. Doing so will help ensure that we take account of the total cost of environmental degradation such as air pollution. And we will all be better off as a result."
"
 Part II: Where does global warming rank among future risks to public health?

Guest essay by Indur M. Goklany
In Part 1, we saw that at present climate change is responsible for less than 0.3% of the global death toll. At least 12 other factors related to food, nutrition and the environment contribute more. All this, despite using the World Health Organization’s scientifically suspect estimates of the present-day death toll “attributable” to climate change,
Here I will examine whether climate change is likely to be the most important global public health problem if not today, at least in the foreseeable future.
This examination draws upon results generated by researchers who are prominent contributors to the IPCC consensus view of climate change.  I do this despite the tendency of their analyses to overstate the net negative impacts of climate change as detailed, for instance, here, here and here.
Specifically, I will use estimates of the global impacts of climate change from the British-government sponsored “Fast Track Assessments” (FTAs) which have been published in the peer reviewed literature. Significantly, they share many authors with the IPCC’s latest assessment. For example, the lead author of the FTA’s study on agricultural and hunger impacts is Professor Martin Parry, the Co-Chairman of the IPCC Work Group 2 during the preparation of the IPCC’s latest (2007) assessment.  This Work Group was responsible for the volume of the IPCC report that deals with impacts, vulnerability and adaptation.
I will consider “the foreseeable future” to extend to 2085 since the FTAs purport to provide estimates for that date, despite reservations.  In fact, a paper commissioned for the Stern Review (p.74) noted that “changes in socioeconomic systems cannot be projected semi-realistically for more than 5-10 years at a time.” [Despite this caution, Stern’s climate change analysis extended to at least 2200.]
In the following figure, using mortality statistics from the WHO, I have converted the FTAs’ estimates of the populations at risk for hunger, malaria, and coastal flooding into annual mortality. Details of the methodology are provided here.

In this figure, the left-most bar shows cumulative global mortality for the three risk categories in 1990 (the baseline year used in the FTAs). The four “stacked” bars on the right provide mortality estimates projected for 2085 for each of the four main IPCC scenarios. These scenarios are arranged from the warmest on the left (for the so-called A1FI scenario which is projected to increase the average global temperature by 4.0°C as indicated by the number below the stacked bar) to the coolest on the right (for the B1 scenario; projected temperature increase of 2.1°C).  Each stacked bar gives estimates of the additional global mortality due to climate change on the top, and that due to other non-climate change-related factors on the bottom. The entire bar gives the total global mortality estimate.
To keep the figure simple, I only show estimates for the maximum (upper bound) estimates of the mortality due to climate change for the three risk factors under consideration.
This figure shows that climate change’s maximum estimated contribution to mortality from hunger, malaria and coastal flooding in 2085 will vary from 4%-10%, depending on the scenario.
In the next figure I show the global population at risk (PAR) of water stress for the base year (1990) and 2085 for the four scenarios.

A population is deemed to be at risk if available water supplies fall below 1,000 cubic meters per capita per year.
For 2085, two bars are shown for each scenario. The left bar shows the net change in the population at risk due to climate change alone, while the right bar shows the total population at risk after accounting for both climate change and non-climate-change related factors. The vertical lines, where they exist, indicate the “spread” in projections of the additional PAR due to climate change.
This figure shows that climate change reduces the population at risk of water stress! This is because global warming will decrease rainfall in some areas but serendipitously increase it in other, but more populated, areas.
The figure also suggests that the warmest scenario would result in the greatest reduction in net population at risk.
[Remarkably, both the IPCC’s Summary for Policy Makers and the original source were reticent to explicitly point out that climate change might reduce the net population at risk for water stress. See here and here (pages 12-14 or 1034-1036).].  Thus, through the foreseeable future (very optimistically 2085), other factors will continue to outweigh climate change with respect to human welfare as characterized by (a) mortality for hunger, malaria and coastal flooding, and (b) population at risk for waters stress.
In the next post in this series, I will look at a couple of ecological indicators to determine whether climate change may over the “foreseeable future” be the most important problem from the ecological perspective, if not, as we saw here, from the public health perspective. 



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e969a1285',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The oil and gas industry has had a far worse impact on the climate than previously believed, according to a study indicating that human emissions of fossil methane have been underestimated by up to 40%. Although the research will add to pressure on fossil fuel companies, scientists said there was cause for hope because it showed a big extra benefit could come from tighter regulation of the industry and a faster shift towards renewable energy.  Methane has a greenhouse effect that is about 80 times more potent than carbon dioxide over a 20-year period and is responsible for at least 25% of global heating, according to the UN Environment Programme. In the past two centuries, the amount of methane in the atmosphere has more than doubled, though there has long been uncertainty about whether the source was biological – from agriculture, livestock or landfills – or from fossil fuels. There were also doubts about what share of fossil methane was naturally released and what share was from industry. Earlier estimates were based on intermittent, bottom-up monitoring of oil and gas companies and comparisons with geological evidence from the end of the Pleistocene epoch, about 11,600 years ago. For a more accurate comparison, a team at the University of Rochester in the US examined levels of methane in the pre-industrial era about 300 years ago. This was achieved by analysing air from that period trapped in glaciers in Greenland. The sample – made up of about a tonne of ice – was extracted with a Blue Ice Drill, capable of producing the world’s biggest ice cores. The findings, published in Nature, suggest the share of naturally released fossil methane has been overestimated by “an order of magnitude”, which means that human activities are 25-40% more responsible for fossil methane in the atmosphere than thought. This strengthens suspicions that fossil fuel companies are not fully accounting for their impact on the climate, particularly with regard to methane – a colourless, odourless gas that many plants routinely vent into the atmosphere. An earlier study revealed methane emissions from US oil and gas plants were 60% higher than reported to the Environmental Protection Agency. Accidents are also underreported. A single blowout at a natural gas well in Ohio in 2018 discharged more methane over three weeks than the oil and gas industries of France, Norway and the Netherlands released in an entire year. At the time, the company said it was unsure of the size of the leak. The immense scale was only revealed a year later when scientists analysed satellite data provided by the European Space Agency. Fracking also appears to have worsened the problem. Atmospheric methane had started to flatten off at the turn of the century, but rose again after a surge in fracking activity in the US and elsewhere. The industry, however, continues to claim that the energy source can be used as a “bridge fuel” because it has lower carbon emissions than oil or coal, but this fails to account for leaks and flares of methane and other gases during extraction. For almost three decades, world governments have met every year to forge a global response to the climate emergency. Under the 1992 United Nations Framework Convention on Climate Change, every country on earth is treaty-bound to “avoid dangerous climate change”, and find ways to reduce greenhouse gas emissions globally in an equitable way. Cop stands for conference of the parties under the UNFCCC. The UK will host Cop26 this November in Glasgow. In the Paris agreement of 2015, all governments agreed for the first time to limit global heating to no more than 2C above pre-industrial levels, and set out non-binding national targets on greenhouse gases to achieve that. However, these targets are insufficient, and if allowed to stand would lead to an estimated 3C of heating, which scientists say would spell disaster. For that reason, the Cop26 talks in Glasgow are viewed as the last chance for global cooperation on the emergency, with countries expected to come with tough new targets on emissions. The negotiations will be led by environment ministers and civil servants, aided by UN officials. Nearly every country is expected to send a voting representative at the level of environment secretary or equivalent, and the big economies will have extensive delegations. Each of the 196 nations on earth, bar a few failed states, is a signatory to the UNFCCC foundation treaty. The Cops, for all their flaws, are the only forum on the climate crisis in which the opinions and concerns of the poorest country carry equal weight to that of the biggest economies, such as the US and China. Agreement can only come by consensus, which gives Cop decisions global authority. Fiona Harvey Environment correspondent Growing calls for tighter controls will be strengthened by the new study. The lead author, Benjamin Hmiel, said the paper was cause for optimism because it showed that action on methane – which has a relatively short shelf life, persisting in the atmosphere for about nine years – could give a strong short-term boost to efforts to stabilise the climate. “Placing stricter methane emission regulations on the fossil fuel industry will have the potential to reduce future global warming to a larger extent than previously thought,” Hmiel said. “Methane is important to study because if we make changes to our current methane emissions, it’s going to reflect more quickly.” Other scientists who were not involved in the research concurred there were positive implications in the findings, but only if governments were able to rein in fossil fuel companies, which has not been the case until now. “This indicates that the fossil fuel sector has a much more polluting impact beyond being responsible for the overwhelming majority of carbon dioxide emissions. This is worrying and overall bad news,” said Dr Joeri Rogelj, a climate change lecturer at the Grantham Institute. The good news, Rogelj said, was that measures to prevent leaks, reduce flaring and switch to renewables would be more effective than expected. “What this study shows is that we can have a bigger impact on methane in the atmosphere than earlier thought. This allows us to set climate policy priorities right.” Dave Reay, the executive director of the Edinburgh Centre for Carbon Innovation, said one of the key messages from the study was that the old bottom-up method of measuring methane emissions was “woefully inadequate”. “We knew fossil fuel extraction – including fracking – was a major part of global methane emissions, but this impressive study suggests it is a far bigger culprit in human-induced climate change than we had ever thought,” he said. “If correct, gas, coal and oil extraction and distribution around the world are responsible for almost half of all human-induced methane emissions. Add to that all the carbon dioxide that is then emitted when the fossil fuels are burned, and you need look no further for the seat of the climate emergency fire.”"
"Glasgow recently became the first European university to join the rapidly-expanding fossil free divestment movement. Following hot on the heels of the Australian National University, Glasgow promised to move £18m of investment over the next ten years. The international, grass-roots, student-led fossil-free movement now has the support of religious, medical and charitable bodies across the world (181 and counting). These organisations have divested because they can no longer endorse the activities of the fossil fuel sector. The movement is inspired by the success of the anti-apartheid divestment campaign, where financial and moral pressure on companies doing business with South Africa contributed to the fall of the apartheid regime. The campaign is beginning to rattle fossil fuel companies. A fight-back has begun. Pro-coal Australian prime minister Tony Abbott has called divestment “stupid”. Academics, too, have criticised the campaigners as hypocritical. Such criticisms are wrongheaded. Anyone who cares about climate change should support the divestment campaign. Viewed at a global level, existing solutions aren’t working. The ability of market-based instruments to reduce carbon emissions is more a matter of faith than empirical evidence. Carbon reductions from renewables, while growing fast, are offset by increases elsewhere. Greater efficiency stimulates growth and consumption, not parsimony. Existing measures are like “squeezing a balloon”: reductions in one place lead to increases elsewhere. The Fifth IPCC assessment warned that we have five times more fossil fuel reserves than we can safely extract if we are to stand a decent chance of staying under 2°C warming. This puts the question starkly: how can we leave this carbon in the ground? The divestment movement confronts the core logic – licence, extract, profit – of fossil fuel companies. One key tactic to make it harder for them to extract carbon is to erode their political legitimacy. Fossil fuel companies use their economic clout to sow doubt about climate science. They lobby for generous subsidies and flout indigenous rights. They commission toys and sponsor art at the Tate, the British Museum, the Royal Shakespeare Company and other cultural institutions to normalise the presence of big oil in our everyday lives. By divesting, organisations such as the World Council of Churches send a strong message: we find your activities immoral. The moral case for divestment is based on the clear environmental damage and the undemocratic power of these corporate behemoths. By stigmatising fossil fuel companies, the divestment movement aims to reduce their political room for manoeuvre. When mainstream figures such as the governor of the Bank of England says fossil fuel reserves can’t be burnt, or the Rockefellers start divesting from fossil fuels for financial reasons, people take notice. The financial case for divestment is based on the carbon bubble. The financial health of fossil fuel companies relies on 2795 gigatonnes of “unburnable” carbon – reserves that have to stay in the ground if we are to have a decent chance of staying under 2C warming. This creates enormous financial risk, as a change in policy (or indeed in climate) could leave these reserves and their associated infrastructure stranded. Long-term financial sustainability is at odds with carbon investment. So far, £30 billion has been divested; small beer compared to the £441 billion spent on exploration by the top 200 companies in 2012. For deeper success, divestment will need to break out beyond churches and charities to affect wider market norms. If this happens, debt will likely become less accessible and capital-intensive projects at the margins less feasible. This can only be a good thing for the climate. Eventually, campaigners hope fossil fuels will face a regulatory and legislative environment that forces the whole company – not just the green-tinged outliers – to move beyond petroleum, or to make way for those who will. All this fossil fuel bashing will be too much for some. “We all use fossil fuels, you included!” says the critic when she leaps to the defence of big oil. This is true, as far as it goes, but naïve. Energy use is not a matter of individual choice – whether we like it or not we are locked into world systems whose very life-blood is oil. We can’t choose a decentralised grid, renewable supply, or decent cycling infrastructure, thanks to historic legacies and the continued power of big oil. We need divestment to work because fossil fuel companies distort politics and stand in the way of a sustainable future. “We should engage fossil fuel companies, not demonise them,” runs another counter-argument. Investor engagement can work, but only if clear goals and timelines are set. Research that helps companies extract more efficiently just gets carbon out of the ground faster; working with companies on renewables, carbon capture and storage, or low-carbon technology can work, but does nothing to transform the core business of big carbon. And when the laws of coercive competition squeeze, big carbon will always retreat to its core business. We are well past the point where the good delivered by fossil fuel companies outweighs the environmental, social, and economic negatives. We need any and all tactics to achieve a post-carbon world. Divestment puts fossil fuel companies in the spotlight, names them responsible for climate change, and confronts their power. Divestment should be supported by everyone who cares about climate change."
"
Share this...FacebookTwitterEd Caryl has submitted another essay, which indicates that CO2 is not as strong a driver as many would like to have us believe.
CO2 is Cool!
By Ed Caryl
The global warming amount if CO2 doubles has been a bone of contention for the last 30 years. The IPCC has settled on figures in the range from 1.5 to 4.5 degrees, with a most probable rise of 3°C. One of the figures widely quoted is based on CO2 rising from 295 ppm in 1900, to 365 ppm in 2000, while temperature rose 0.57 degrees. (For those readers allergic to math formulas, apologies are offered, but keep reading, the result is what is important, and the math is done for you.) Because the CO2 affect is agreed by most to be logarithmic, the formula for the temperature rise if CO2 doubles, when based on CO2 concentration and temperature rise over time, is:
ln2/(ln(CO2 at end of period/CO2 at beginning of period))/(the change in temperature). (ln is the
natural logarithm). Substituting the numbers from above, we get:
 ln2/(ln(365/295)/0.57) = 1.85°C
 This formula was used to calculate the CO2 doubling-temperature (the temperature rise if CO2 in the atmosphere doubles) over a slightly longer period, from 1880 to 2010, using the Law Dome (Antarctic ice samples) and Mauna Loa Hawaii CO2 levels spliced together, and the GISS (Goddard Institute for Space Studies) global surface temperature figures.
The Law Dome and Mauna Loa CO2 data overlap from 1960 to 1978, and agree quite closely over those years, so splicing them seems quite valid. First, here (from the sources above) is the temperature and the CO2 plotted together:

Figure 1. Plot of GISS global temperature anomaly and atmospheric CO2.
The result of CO2 doubling using the above formula with the beginning and end numbers in Figure 1 is:
 ln2/(ln(389.78/290.7)/0.91 = 2.15°C. This is higher, but still less than the IPCC estimate of 3°C.
But what if we take different time periods for the calculation? A shorter period 50-year calculation results in a noisy chart because there is great variation in temperature from year to year, with negative as well as positive temperature changes, and in the years before 1970, the CO2 rise was very slow so the ratios are small numbers. Here is the plot of the CO2-doubling temperature rise using the above formula with a sliding 50-year window beginning with the period from 1880 to 1930 and ending with 1960 to 2010:

Figure 2. CO2 sensitivity over time using a 50-year window applied to the data in Figure 1. The red trace is a 10-year moving average on the calculated sensitivity. The black line is the linear trend.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The calculation results in large positive and occasionally negative numbers when a shorter period is used. Non-CO2 influences are visible, the warming in the late 30s and 40s, and the later cooling and warming again in the 50s 60s and 70s. These are visible in Figure 1, above. The Atlantic Multi-decadal Oscillation (AMO), the Pacific Decadal Oscillation (PDO), and others, cause these temperature cycles. These 60 to 70-year ocean temperature cycles will increase a sensitivity figure based on a 100-year window.
Volcanoes, La Niña, and El Niño events and above described ocean cycles, as well as the solar cycle, affect the plot because they perturb the system away from equilibrium. Anything that adds or subtracts from the temperature is indistinguishable from the affects of CO2. CO2 is not the only thing affecting temperature. Many people have forgotten this simple fact. The plot in Figure 2 is really the total climate sensitivity, not just from CO2.
The figure widely used for CO2 doubling is only stable if the window used is long. It is only accurate if other factors are not pushing the numbers up. If shorter periods are taken, wildly different numbers result, both positive and negative. In recent years, the numbers settle to just over 1.0°C because the CO2 differences over the 50-year windows are getting larger. What is the real Climate Sensitivity? It appears to be less than 1.0°C for CO2 doubling.
Keep in mind that these plots were generated using GISS global surface temperature data. This data has been criticized for including too many sites influenced by urban warming and for being adjusted upward in recent years, and downward for earlier years. This seems to be the case, as any upward bias would tilt the above plot in the upward direction, as is seen at the end of the plot.
In an effort to check for this, the same formula was used on UHA satellite temperature data for the last 31 years. If the window is the 30 years from 1979 to 2009, the result is 1.75°C, if from 1980 to 2010, the answer is 2.05°C. Again, a short window gives a noisy answer. If 2011 is cooler than 2009, the calculation will be less than 1.75°C. Caution must be used when selecting data for these calculations. As an example, a calculation using the window from 1980 to 2008 results in a negative –0.2°C sensitivity.
Here are the temperature and CO2 plots:

Figure 3. Satellite temperature and Mauna Loa CO2 plots.
The sensitivity was plotted using a sliding 20-year window on the 31 years of satellite data available. Here is that result:

Figure 4. CO2 sensitivity using satellite temperature data and a 20-year sliding window.
The plot is very noisy with two points going negative. We will really need 20 or 30 more years of satellite temperature data to see a valid result, but the 30-year window suggests that the sensitivity as measured by the satellite data will still be less than 2°. Keep in mind that many of the ocean cycles, such as the AMO and PDO, are on the order of 60 years in length, and recently have been in their positive phases. The satellite data is only half this long.
In summary, all these plots show that CO2 sensitivity is probably 1°C or less for a doubling of CO2. We will need a few more years of good temperature data to pin that down.
Meanwhile, all the people claiming sensitivities of 3° or more need to calm down. The above plots and calculations rule that out.
A CO2 climate sensitivity of 1°C for CO2 doubling is not very important. Also remember that this figure includes all the supposed positive feedbacks, because if they exist, they have had an influence on the temperature already. Keep in mind that CO2 will probably never double in the atmosphere for the simple reason that we will run out of easily available fossil carbon long before then. But this is fodder for the next article.
Share this...FacebookTwitter "
"
Old Radar Sites In Greenland Show Icecap Growth Over the Years
(And let’s not forget what we’ve learned about the temperature reporting from the DEW line Radar Stations – Anthony)
By Joseph D’Aleo, CCM, AMS Fellow
Though the ice may be melting around the edges of the Greenland Icecap in recent years during the warm mode of the AMO much as it did during the last warm phase in the 1930s to 1950s, snow and ice levels continue to rise in most of the interior. Johannessen in 2005 estimated an annual net increase of ice by 2 inches a year. 

(Above: Recent Ice-Sheet Growth in the Interior of Greenland, Ola M. Johannessen, Kirill Khvorostovsky, Martin W. Miles, Leonid P. Bobylev, Science Express on 20 October 2005 Science 11 November 2005: Vol. 310. no. 5750, pp. 1013 � 1016, DOI: 10.1126/science.1115356)
A Canadian Icecap emailer noted during the cold war there were two massive radar sites built on the Greenland icecap now abandoned. They are called Dye-2 and Dye-3. When built they sat high above the snow, recent pictures show how the snow is building up around them, proving the snow build-up in recent times. This demonstrates this snow accumulation over time.
Dye-2 and 3 were among 58 Distance Early Warning Line radar stations built by America between 1955-1960 across Alaska, Canada, Greenland and Iceland at a cost of billions of dollars. Their powerful radars monitored the skies constantly in case Russia decided to send bombers towards America. After extensive studies in late 1957, the USAF selected sites for two radar stations on the ice cap in southern Greenland. Dye-2 was to be built approximately 100 miles east of Sondrestrom AB and 90 miles south of the Arctic Circle at an altitude of 7, 600 feet, and Dye-3 was to be located approximately 100 miles east of DYE II and slightly south at an elevation of 8,600 feet.
The selected locations for the new radar sites were found to receive from three to four feet of snow fall each year. Since the winds were constantly blowing with speeds as much as 100 mph, this snow accumulation constantly formed large drifts. To overcome this potential problem, it was decided that the Dye sites should be elevated approximately twenty feet above the surface of the ice cap.
Dye 3 was built in 1960. From a distance the structure, with its onion-shaped dome, looks like a Russian orthodox church. Dye 3 was an ice core site and previously part of the DEW line in Greenland.  (The Distant Early Warning (DEW) Line: A Bibliography and Documentary Resource List Arctic Institute of North America, Page 23). As a Distant Early Warning line base, it was disbanded in years 1990/1991. The Dye 3 cores were part of the GISP (Greenland Ice Sheet Project initiated in 1971) and, at 2037 meters, was the deepest of the 20 ice cores recovered from the Greenland ice sheet as part of GISP. Samples from the base of the 2km deep Dye 3 and the 3km deep GRIP cores revealed that high-altitude southern Greenland has been inhabited by a diverse array of conifer trees and insects within the past million years. (Eske Willerslev, et al. (2007) Ancient Biomolecules from Deep Ice Cores Reveal a Forested Southern Greenland Science 317 111-114)
The first image below is  from 1972.

See larger image here.
Here it is in 2006.

See larger image here.
In looking back at the time the sites were abandoned, one console operator lamented “We were very busy during this time and I was sad to see it end. I remember thinking of all the waste,” he said. The site is slowly disappearing into the snow. Its outbuildings are no longer visible and drifting snow will consume it completely one day, but that day appears to be decades away.” Read more here.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99c3a0f3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Today at 00 GMT (5PM PST) a new month started. Every time a new month of statistics starts being logged by WordPress for Watts Up With That, I say to myself, “there’s no way I’ll get this sort of traffic again”. And yet, again I’m surprised that WUWT not only met last months stats, but significantly exceeded them.
Thank you again, loyal readers.

Click for full sized image
It was one year ago that I moved from the Typepad blog to WordPress, and as you can see from above, the growth has been steady, except for one month, April. which had a slight dip.
For September 2008 the total was 846,193 page views, up from 667,215 page views in August 2008.
But there is a caveat, I think the real numbers are just shy of 800,000, because on the weekend of 09/20 and 09/21 I got quite a bit of unexpected traffic that I’m not sure is real or not. During that time, we got a lot of Spam on one particular older entry comparing UAH, RSS, HadCRUT, and GISS, but not anywhere near the numbers specific to that post, shown below:
Blog Stats Increase due to DOS “something”

Saturday 09/20     23,486
Sunday   09/21 25,319
Monday  09/22       1,006
Total: 49,811
You can read about it here in this entry
I checked with WordPress support, twice, and they assured me that the numbers are real, saying:
Hi,
Our stats expert has had a look and found no evidence of a DoS or anything untoward.  He says “the most plausible reason is an email newsletter featuring the URL, or else some other non-browser app loading the URL such as a feed reader. I have not been able to find any evidence of of a DDOS attempt or other “foul play.”
Separately, I’ve checked our security logs and see no other signs of activity that would normally indicate a blog under attack.
In short: we’re quite sure the traffic is genuine and doesn’t correspond with an attack of any kind.
Kind regards,
Alex
WordPress Support
Even so, I’m unconvinced. I got not one single comment added on that posting during the onslaught of traffic, almost 50,000 page views, which tells me the numbers aren’t real, no matter what WordPress support says.
Therefore I have decided to take the step of publishing an “adjusted” set of numbers this month. The difference is that instead of inflating the numbers, such as GISTEMP and USHCN adjustments do, I’m reducing them to what I consider a truly representative value for the month.
Raw WUWT September numbers:           846,193 page views
WUWT Spam Uncertainty numbers:           -49,811 page views (from 09/20 to 09/22)
Final Adjusted WUWT September numbers:    796,382 page views
Still, not too shabby.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9bea60b7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Scientists tell us that the world is warming and greenhouse gas emissions are to blame. Yet climate change framed by scientists, politicians and economists as a straightforward pollution problem will neither convince sceptics nor advance the difficult decision-making process. Mark Maslin, a climatologist at University College London, recently offered his analysis of climate denial and the sceptics’ reluctance to accept the science. In an article published here he correctly observed that “the lack of acceptance of the science of climate change is neither due to a lack of knowledge, nor due to a misunderstanding of science”. Yet by framing climate change as “a massive pollution issue that shows the markets have failed and it requires governments to act collectively to regulate industry and business” he continues the vicious circle. If climate science provides the main argument for governments to regulate private enterprise, sceptics will continue to deconstruct the supposedly legitimising science. Therein lies a problem not only with Maslin’s generally well-informed analysis, but also with the orthodox political approach to climate change. In the US and anglophone countries, any efforts to enforce emission regulations to solve the “massive pollution issue” are regularly stunned by the sceptical critique of the “underlying” scientific facts. It should be clear by now that climate change, although frequently described by scientists, economists and politicians as a straightforward pollution problem, requires more than a technocratic “solution”. By now, everything from trade policy or global inequality to animal extinctions or indigenous peoples’ rights has been woven into the tangled knot of climate change politics. To be sure, scientists have done a great job of bringing climate change onto the political agenda: because of new scientific knowledge, carbon-consuming power plants or gas-guzzling cars that were once tolerated and applauded are increasingly perceived as undesirable or immoral. Yet the orthodox approach to climate change, epitomised in the Kyoto Protocol, has proven ill-suited to adequately address the wicked problem of sovereign states all pursuing their own national, carbon-consuming interests. However useful it has been in bringing the topic onto the political agenda, the scientific framing of climate change as a global pollution problem to be fixed by governmental intervention has run aground. So if you don’t want to talk with sceptics about the science of climate change, then you probably shouldn’t call it a massive pollution issue either, or you will be reminded also of the benefits of industrial pollution. The debate climate scientists are understandably tired of – but unknowingly contribute to – is American at heart. Politicians in the US put a higher premium on science to resolve their ideological conflicts than anywhere else – just look at the dozens of Congressional hearings on climate science over the past three decades.  Similar debates are going on over petrochemical pesticides such as DDT, genetically modified organisms (GMOs), or the smoking ban. In each of these cases the protagonists wanted to rely on the authority of science to settle their value disputes. The various parties are committed to different ethical and ideological positions but they are united in their focus on science and claim to derive the legitimacy of their policy position in one way or another from scientific evidence. Why should we ban GMOs? Because science says so. Unless it doesn’t. Many of those who oppose the power of agro-giants have taken the bait and discuss scientific uncertainties rather than the political economy of GMOs. The authority of science, it is understood, trumps the opponent’s “irrational” value judgements. However, in political practice, the scientific framing of these debates – how harmful is DDT, do GMOs affect human health, does second-hand smoke cause lung cancer? – has made them only worse. To be sure, a science-based approach to decision-making does have great normative force. It allows governments to claim the high ground, a place from which they can be seen to be acting for the benefit of all. But in the case of climate change it does not work, simply because we are not dealing with a pollution problem to be solved cost-benefit style. Climate change is not a hole in the earth’s ozone layer caused by a set of manageable chemicals. This time an international treaty won’t do the trick. Of course the climate controversy as it thrives in the US has now taken on an almost global significance. Facilitated by the English language and internet media in which it is conducted, the debate over climate science gives support to those in other countries where US climate sceptics’ talking points have become part of the political rhetoric. While we can appreciate efforts to understand the reluctance of accepting the science of climate change, we should pay attention also to how the problem is framed. While the pollution narrative has served to keep climate scepticism alive and scientists on their toes it has led to very little real progress – emissions are still rising and a global treaty is yet another climate conference away. The only rational solution would be to drop the “science says” arguments altogether and foster pragmatic climate policies that do not hinge on scientific truth. What a relief this would be for science and scientists to reclaim their right to be wrong.  These policies do not tackle global warming directly via emission reduction targets, as if one could control the global thermostat. Environmental and human health benefits rather emerge as positive side-effect of policies dealing primarily with energy security and the modernisation of inefficient energy producers. In this view climate policy should be connected with established institutions and forms of decision-making, for instance with national health policy-making. The first line of this article originally read “Scientists tell us the world is warming and that a climate catastrophe is imminent. They’re probably right.” It was changed on February 6 at the author’s request."
"
While sunspots are often cited as the main proxy indicator of solar activity, there is another indicator which I view as equally (if not more) important. The Average Planetary Magnetic index (Ap), the strength of which ties into Svensmark’s cosmic ray theory modulating Earth’s cloud cover. A weaker Ap would mean less cosmic rays are deflected by the solar magnetic field, and so the theory goes, more cosmic rays provide more seed nuclei for clouds in Earth”s atmosphere. More clouds mean a greater albedo and less terrestrial solar radiation, which translates to lower temperatures.
I’ve always likened a sunspot to what happens with a rubber band on a toy balsa wood plane. You keep twisting the propeller beyond the normal tightness to get that extra second of thrust and you see the rubber band start to pop out knots. Those knots are like sunspots bursting out of twisted magnetic field lines.
The Babcock model says that the differential rotation of the Sun (the sun being a viscous fluid, the poles rotate at a slower rate than the equator) winds up the magnetic fields of it’s layers during a solar cycle. The magnetic fields will then eventually tangle up to such a degree that they will eventually cause a magnetic break down and the fields will have to struggle to reorganize themselves by bursting up from the surface layers of the Sun. This will cause magnetic North-South pair boundaries (spots) in the photosphere trapping gaseous material that will cool slightly. Thus, when we see sunspots, we are seeing these areas of magnetic field breakdown.

Sunspots are cross connected eruptions of the magnetic field lines, shown in red above. Sometimes they break, spewing tremendous amounts of gas and particles into space. Solar flares and coronal mass ejections (CME’s) are some examples of this process. Sometimes they snap back like rubber bands. The number of sunspots at solar max is a direct indicator of the activity level of the solar dynamo.
As many of you may recall, a few months ago, I had plotted the Average Geomagnetic Planetary Index (Ap) which is a measure of the solar magnetic field strength but also daily index determined from running averages of eight Ap index values. Call it a common yardstick (or meterstick) for solar magnetic activity.

Click for a larger image
I’ve updated the graph today, to include July 2008 Ap data as you can see below:

Click for a larger image
Source data, NOAA Space Weather Prediction Center:
http://www.swpc.noaa.gov/ftpdir/weekly/RecentIndices.txt
As you can see, the Ap Index has continued along at the low level (slightly above zero) that was established during the drop in October 2005. As of July 2008, we now have 34 months of the Ap hovering around a value between 5 to10, with occasional blips of noise.
Since it is provided in the same dataset, I decided to also plot the smoothed Ap Index.
I also plotted my own 24 month smoothing window plot, shown in magenta.

Click for a larger image
I also decided to update the plot of the 10.7 centimeter band solar radio flux, also a metric of solar activity. It is in the same SWPC dataset file as the Ap Index, in columns 8 and 9. The smoothed 10.7 CM flux value provided by SWPC has also dropped about the same time continues a downward trend.
I also provided my own 24 month wind smoothed value which is plotted in magenta.

Click for a larger image
Note the lower flux values during this solar minimum than the last
We continue to remain in a deep solar minimum, and with the forecasts being modified to push back the real “active” start of Solar cycle 24, it remains anybody’s guess as to when the sun will come out of it’s funk.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d0d54cd',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Guest Post by Steven Goddard

The Third Little Show
Mad dogs and Englishmen go out in the midday sun The Japanese don’t care to, the Chinese wouldn’t dare to Hindus and Argentines sleep firmly from twelve till one But Englishmen detest-a siesta
– Noel Coward – 1931
Persistence is the British trait which kept the Shackleton crew alive and helped England withstand the Nazi’s throughout World War II.  It keeps the Catlin Crew going and kept Lewis Pugh relentlessly paddling his kayak over Arctic Ice towards the pole.   And it is the same trait which keeps the UK Met Office forecasting warm summers year after year.  The Met Office forecast 2007 to be the warmest year ever globally, and a hot summer in the UK. 
Instead it turned out to be a cool summer and the rainiest on record in England.  Similar story for summer 2008 and winter 2008-2009 .  Yet in fine British tradition the Met Office remains undaunted –
The coming summer is ‘odds on for a barbecue summer’, according to long-range forecasts. Summer temperatures across the UK are likely to be warmer than average and rainfall near or below average for the three months of summer.
Chief Meteorologist at the Met Office, Ewen McCallum, said: “After two disappointingly-wet summers, the signs are much more promising this year. We can expect times when temperatures will be above 30 °C, something we hardly saw at all last year.”
The last 30C day in London was July 26, 2006 – that was over 1,000 days ago.  But you have to admire their grit and determination to get the global warming message across to the ignorant British population.
“The definition of insanity is doing the same thing over and over and expecting different results.”
– Attributed to Albert Einstein

Darts anyone?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9664a685',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterSome of us may be wondering whatever happened to the dana who we all love and miss so much. Well, Lubos Motl at the Reference Frame has a nice little update on the adventures of dana:H/t: Mindert Eiting
Why Dana 1981 Hasn’t Proved…
By Lubos Motl
Dana1981 is a 30-year-old Prius driver and the owner of several other alternative vehicles who has mistakingly received a bachelor degree in astrophysics and a master degree in physics, so he or she became a self-described environmental scientist who is “passionate” about the climate hysteria.
Clearly, such people shouldn’t be admitted as college students because they’re incapable of rational thinking. The presence of people like him dramatically cripples the intellectual atmospheres at the world’s universities…”
Read more!
PS: My advice to you dana is: I wouldn’t mess with Lubos, as he would certainly do physics circles and orbits around you. Dana, you’ve only proved one thing, and you may realize what that is when you get older.
Share this...FacebookTwitter "
"Curious Kids is a series for children of all ages, where The Conversation asks experts to answer questions from kids. All questions are welcome: find out how to enter at the bottom of this article.  Where did the first seed come from? – Alice, age six, Beverley, UK Hi Alice. This is a clever question. As I’m sure you know, plants use seeds to spread their young and make new plants. But plants haven’t always used seeds to do this. Seeds came together bit-by-bit over a really long time, as plants evolved. To understand how this happened, you need to know that all living things change slowly over time, to get better at surviving in their environment – this process is called evolution.  Here’s how it works: when a living thing has a feature which works well, it will be able to live longer and have more young. These young will probably have similar features, thanks to their parents.  Plants started using seeds to spread their young somewhere between 385m and 365m years ago. Before seeds existed, plants had other ways of doing this.  Back then, most plants used spores. Some plants today, such as algae, mosses and ferns, still do. You might have spotted the tiny brownish dots on the underside of fern leaves – these are spores.  Spores are different from seeds in a few ways. A spore is made of just one part – a single cell – while a seed contains many cells, each with different jobs to do.  Another difference is that spores only have one parent plant, while seeds have two.  This means that, after a seed starts sprouting, it can grow into a plant, just like its parents.  But spores have to work a bit harder: once they’ve travelled away from their parent plant, they grow into a little green plate of cells, which scientists call a “gametophyte”. Then, two gametophytes must join together, before they can grow into a plant.  It’s easier for gametophytes to join together when its wet – and that’s why plants that use spores usually need to grow in wet places.  For example, horsetails are a very ancient type of plant, which like to grow along lakes, rivers and ponds: they have very strange spores with four “legs” which help them to move and travel further away.  Scientists believe that an extinct seed fern, called Elksinia polymorpha, was the first plant to use seeds.  This plant had cup-like features, called “cupules”, that would protect the developing seed. These cupules grew along the plant’s branches.  Today, plants with seeds do things a little differently. There are two main types: “angiosperms” and “gymnosperms”.  Angiosperms are flowering plants – their seeds develop inside of fruit, like apples, tomatoes or even rose hips or holly berries.  Gymnosperms, such as pine trees, grow their seeds inside a hard cone.  Seeds have evolved because they are better at helping plants to survive than spores are. For example, seeds contain a food source to help the new plant grow.  They also have a hard coat, which helps them to live longer in different conditions: this means plants with seeds can life in lots of different places, from hot, dry deserts to cool, rainy places.  Seeds are so good at helping plants to spread their young that most plant species on Earth today use seeds.  Hello, curious kids! Have you got a question you’d like an expert to answer? Ask an adult to send your question – along with your name, age and town or city where you live – to curiouskids@theconversation.com. Send as many questions as you want! We won’t be able to answer every question, but we’ll do our best. More Curious Kids articles, written by academic experts: Why do spiders have hairy legs? - Audrey, age five, Melbourne, Australia Why do we have different seasons at specific times of the year? – Shrey, age nine, Mumbai, India How is water made? – Clara, age eight, Canberra, Australia"
"
Share this...FacebookTwitterHere’s one of my favorites – by Joe Walsh.
I’ve added the lyrics to the song – see below.
Some warmists tried to tell us yesterday that climate scientists lead humble, low-paid lives. Some probably do, but the ones we are familiar with don’t live bad at all. This is for them.

 
Warming’s been good to me so far
I have a data center
Know its high price
Don’t really work there
Man is that nice


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




I sponge off NASA
Fudge behind the wall
I have the government
Pay for it all
They say I’m crooked but it’s not on my dime
I’m just hiding the clues at the scene of the crime
Warming’s been good to me so far
My Teraflop
Runs a model and drive
I lost my data
Now I contrive
I have a big lab
Adjust in the back
I delete emails
In case I get hacked
I’m forging new records
My pols can’t wait
They send me money
And tell me I’m great
So I got me an offer
No cold records on the wall
Temperature keeps climbing
Doubt it will fall
Lucky I’m sane after all I’ve been through
(Everybody sing) It’s warm (He’s cool)
I can’t screw up but sometimes I still do
Warming’s been good to me so far
I fly to conferences
Sometimes I see Gore
It’s hard to believe
No one listens no more
It’s tough to cover
This fudging and shame
Everybody’s suspicious
Data’s the same
They say I’m crooked but it works every time
(Everybody sing) Oh yeah (Oh yeah)
I keep adjusting guess I’ll never know why
Warming’s been good to me so far…
Share this...FacebookTwitter "
"
Share this...FacebookTwitterReader Bernd Felsche reminds us of this excellent post by Christoper Hitchens here, which is indeed worth bringing up again today, particularly in light of the dark closed minds that have lately attempted to claim that no matter what the weather does today, man is the culprit.
An identical mindset infected so called leading intellectuals back during the Little Ice Age. As Dr. Baliunas explains, during the Medieval Warm Period, Europe enjoyed a warm climate, which allowed society to flourish.
But around the 15th century, the Little Ice Age began and the weather deteriorated. Agriculture suffered and people starved. The low-point was from 1550 to 1700. The intellectuals back then were sure it was man’s fault. A mass wave a institutionally legalized executions ensued.
Dr. Baliunas (emphasis added):
How unusual was this very intense period of the Little Ice Age? On the afternoon of August 3rd, 1562, a thunderstorm struck Central Europe across a front several hundred km long. After raging for several hours, the storm unleashed a terrific hail that continued until midnight. It destroyed crops. It destroyed vineyards, birds and unprotected horses and cows. Dirists noticed something we hear today. They said for 100 years such a storm had never been seen. The storm was deemed so unusual in this period of superstition, that it had to be unnatural; it had to be supernatural.
Thus superstition and witchcraft bred a precautionary response: Eradicate those responsible for the storm and the period of new storminess. Now it was well known that people could cook weather with the help of Satan. So thus did extreme conditions of the severest part of the Little Ice Age contribute to Europe’s most horrific period of mass executions a witch trials. This was completely legal and it was undertaken, administered by highly educated upper social strata. These were institutionally legalized executions for sorcery.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Sounds very familiar today, doesn’t it? How does it compare to PIK, RC and GISS? Watch this speech by Dr Baliunas. Humanity is always only a step away from with-burning and Holocausts. It has happened time and again. The only thing that saves us, is the strong rule of democracy. How often have we heard leading “intellectuals” question democracy lately? We see many similarities today in the thinking of the upper social strata. “Man is responsible” – especially the deniers.

Share this...FacebookTwitter "
"The dairy sector in the UK is going through a period of high uncertainty. Not only are suppliers having to cope with retail price wars and the fact that milk prices are being reduced by the increasing alignment of domestic and world dairy prices, they are also facing the fact that milk quotas will be eliminated in March, which promises to fully expose the sector to market forces.  So it is not surprising that recent discussions have focused on how the dairy sector in the UK should adapt. Some view the future as inevitably going towards mega-dairy farms, imitating those existing in the US, where thousands of cows are milked under huge sheds without seeing the sun, in something that is closer to an intensive factory than a farm.  Whether this comes to pass depends on whether these mega-dairy farms are the only solution to maintain the competitiveness of the sector and therefore its survival. In the UK there are just a handful of dairy farms with herds of over 1000 cows, whereas in the US they can be ten times that size. The prospect raises two questions: are farms whose practices resemble mega-dairies, including feeding cows in-house all year round, more profitable, and does this necessarily mean low animal welfare standards? First some context. The average price received by farmers in 2014 (up to October) was 32.17p/litre (though it can vary by over 10p/litre depending on the nature of a farmer’s supply contract). This is much about 6.7 times higher than the average milk price received in 1973, though in fact the price has fallen since then once you adjust for inflation. This doesn’t take into account the effect of the cost of production, however. If the cost of production has fallen faster than the wholesale price of milk, this might not be a problem.   Unfortunately historic production costs are not available. But we do have historic feedstuff costs going back to 1988 from the department of agriculture, which are the highest component of costs (more than double labour, which is second). So these can give us some idea of what has been happening. As can be seen from the chart that I have used these figures to plot below, this suggests that the true price of milk that farmers receive has fallen since 1995.  The true farmers’ price of milk The decrease in milk prices reflects how competitive the sector has become, partly because retailers drive down prices and partly because of increasing competition from other food products. The chart below shows the decline in purchases of milk products, driven by a strong decline in the consumption of full-fat milk that has not been replaced by skimmed milks (note that cheese consumption has been rising but not very strongly).     UK weekly dairy consumption per capita  This drop in demand for milk products has led to a structural adjustment in the sector, reducing both the number of dairy cows and dairy farms in the UK. Here’s the trend going back to the mid-1990s: Dairy farms and dairy cows Notice that the number of cows has been falling more slowly than the number of farms. This implies that the number of cows per farm has been growing, which might suggest that we are heading towards mega-farms. It is also worth pointing out that there has not been a corresponding drop in the volume of milk being produced. This is because yields per cow have been growing steadily thanks to a combination of technological advances and more high-yielding cows.  Milk production and yields A recent UK survey by Scotland’s Rural College found that of the country’s 863 dairy farms, the traditional British dairy management style of all-summer grazing and winter-only indoor feeding was practised by less than a third of respondents, and on average, herd sizes were larger within systems that feed indoors. This would mean that the proportion of cows being milked in such farms will be rather higher.  This decline in grazing has happened in many countries. Reasons include difficulty in controlling feed rations for high-yielding animals, uncertainty of grass supply in some countries, practical difficulties such as walking distances and lying times, and the availability of a stable labour force. High-yielding cows can also demand up to five times as much energy, which can be difficult to achieve by a grazing and silage-based diet. This can mean that some additional feeding is required. But if this is the trend, what about profitability? Interestingly well managed grazing-based farms do not seem to be less profitable than those that rely more on indoor feeding. Milkbench+, the dairy benchmarking agency, found it is possible to produce milk efficiently at almost any scale and at any level of outputs, as detailed in this figure:  Profitability and herd size But while the above figures are based on profitability per litre of milk produced, a very important point is that most probably very small herds will not provide sufficient income for owners. This can make the size of the enterprise a key aspect, depending on the farmer’s objectives.  Grazing-based systems have on average fewer cows than farms with indoor-based feeding systems. And certainly, housing and management style can affect the welfare of dairy cows. It has been reported that the British public is opposed to indoor dairy systems – though many will not be aware that traditional British systems have cows spending winters indoors anyway.  The Farm Animal Welfare Council says that housed dairy cows in the UK can have an acceptable standard of welfare as long as suitable housing is provided together with skilled animal husbandry and veterinary practice. Nevertheless, continually housed dairy cows can be susceptible to a range of health issues in feet and legs and are at greater risk of health disorders such as mastitis and retained placenta.  There are techniques that may lower the incidence of some health issues, however. And dairy cows maintained in grazing systems may also be at risk of health issues such as lameness and milk fever and are also exposed to prevailing weather conditions. There are still a variety of dairy management systems in use but there does certainly seem to be a clear trend towards farming that feeds more time indoors. If larger farms don’t necessarily mean more profitability – or lower welfare standards, thanks to our UK legislation – it may well be that farmers are making up for lower margins by seeking to generate more income from their holdings.  A couple of positive closing thoughts: We always have the option of buying our dairy products from farmers that support higher welfare standards. And a shift towards larger indoor farms is unlikely to result in US-style mega-dairy farms – we simply don’t have the space. Hard Evidence is a series of articles in which academics use research evidence to tackle the trickiest public policy questions."
"**Spain's King Felipe VI has begun ten days of quarantine after coming into contact with a person who tested positive for coronavirus.**
Palace sources say the king, 52, was in ""close contact"" with the individual on Sunday, but gave no further details.
The monarch's wife and the couple's two daughters will continue their activities as normal.
Spain has recorded nearly 1.6 million cases and 43,131 deaths since the pandemic began.
Last week, the World Health Organization warned that Europe, which is once again at the centre of the pandemic, faced a ""tough"" six months ahead.
However, recent results from a number of vaccine trials have given hope and on Tuesday the Spanish government is due to meet to discuss plans to vaccinate the population.
Last week, Prime Minister Pedro SÃ¡nchez said the country hoped to offer the vaccine to ""a very substantial part"" of its population within the first half of 2021."
"

Even yellow journalists know it’s a good idea to use the refereed scientific literature as the basis for science stories, so it was disconcerting to see a bona fide green journalist like the Washington Post’s Joby Warrick give a great deal of ink to a nonrefereed speech — not even a paper — delivered in San Francisco by federal climatologist Jonathan Overpeck. 



The reason for all the fuss soon became obvious. At the December meeting of the American Geophysical Union, Overpeck said that the so‐​called Medieval Warm Period was local, not global. In other words, the warming that was so substantial that it allowed the Vikings to colonize Greenland and North America was not created by a general planetary warming. That implies that the cooling that followed — known as the Little Ice Age — was similarly non‐​global; otherwise the Warm Period would have shown up, in comparison with temperatures in succeeding centuries as, well, global warming. 



Overpeck’s speech prompted handsprings of joy from our greener friends. Now, instead of saying that the decade of the 1990s (and, in particular, 1998) is the warmest in 600 years (which goes back to the beginning of the putative Warm Period), they can say it’s the warmest in 1,200 years. This story will be in print on or about January 4, 1999, and allows them to declare that the warm terror is here and higher taxes are needed pronto to stop the burning of fossil fuels. 



Others might say, “big deal, sure am glad that I haven’t spent a lick on heating oil and it’s almost Christmas. Think I’ll go and buy some stuff for the missus.” 



Like Tip O’Neill’s politics, climate is local. 



Still others may correctly deduce that Overpeck has created a big problem for those who warn of impending apocalypse. If he is right (a large IF), then regional climate naturally varies tremendously, whether or not the globe warms. In other words, climate changes so dramatic that they promoted the Viking exploration are simply the way of things. And ditto for their flipside — large regional coolings like the Little Ice Age. That event sent the Rhone Glacier in the Alps some 5,000 feet further downslope than it is today and prompted winter carnivals on the frozen Thames. 



Poignant testimony to the social consequences of this regional swing can be found in Kalaallit Nunaat (the politically correct term for Greenland these days), where masonry churches, once built in pastures, are now encased in ice. While KN’s climate clearly changed in ways that were tremendously important to society at the time of the Vikings, that apparently had nothing to do with global warming or cooling. 



Instead, Overpeck says, those changes occurred as purely internal oscillations of the climate system, with no external global change. If we accept that notion, what does it really mean? 



It means that large, regional climate changes have occurred and will occur whether or not the planet warms. That is the kind of change people and plants care about, because no one can sense the global temperature. Like Tip O’Neill’s politics, climate is local. So those who would seek to impose costs on society to prevent climate change had better demonstrate that warming the planet will make large regional excursions more, not less, likely. 



Recently, I explored this notion in a paper in the refereed journal Climate Research. Relying upon historical data (and explicitly ignoring computer models of climate because of their patent unreality), I found that temperature variability between seasons and between years has significantly declined in the second half of this century. And there have been a few warm years in that period, too. 



So when I looked at the variability as a function of the planet’s annual temperature, I found that the cool years were more variable and the warmer ones less. Conclusion? Warming the planet decreases variability on a year‐​to‐​year scale. Cooling the planet makes things more variable. 



That’s pretty good evidence that what human beings are doing to the climate makes things more predictable and equable than before. 



Want more? When the carbon dioxide concentration of the atmosphere was at its highest level since animals first appeared, the biggest animals in history roamed the earth: dinosaurs. Those beasts required a tremendous amount of vegetation to reach their enormous size. Carnivores, like T. Rex, were supported by the massive herbivores. How many tons of vegetation were ultimately required to feed them, considering it had to pass through huge lunks like Apatosaurus (that’s Brontosaurus to you intellectual dinosaurs)? The toasty earth had to have been greener than casino felt. 



What’s more, when the dinos were around, the climate was so stable that they were cold blooded! They’d probably still be here today, except for the fact that they went extinct when the earth got clobbered by a small asteroid. The asteroid raised a huge cloud of dust and killed them with global cooling, which made the climate more variable, resulting in an undependable food supply. 



Our greener friends might become extinct too, if they tout Overpeck’s findings as good news for their side.
"
"Although streaming remains the most popular way people listen to music, old formats like cassettes and vinyl have both seen an increase in sales. In fact, vinyl has seen a remarkable sales increase of 1,427% since 2007, selling around around 4m LPs in 2018 in the UK alone. Since the popularity of vinyl shows no signs of stopping soon, this means that more non-recyclable discs will be manufactured – which could have a negative impact on the environment. Although album covers are generally made of recyclable cardboard, records were originally made of shellac, before non-recyclable vinyl was used as a replacement. Shellac is a natural resin secreted by the female Kerria lacca bug, which was scraped from trees to produce gramophone records. Since shellac isn’t from fossil fuel-derived feedstock (chemicals, such as ethylene, used to make substances like plastic), its carbon footprint was lower than that of modern records.  Shellac records were brittle and prone to water and alcohol damage though, so PVC plastic records were developed to last longer. In ideal conditions (low oxygen, without movement), discarded PVC could take centuries to decompose. However, the environmental conditions of most landfill sites (which have varying soil acidity and temperatures) can cause discarded PVC albums to leach plasticisers (solvents added to plastics to make them more flexible and resilient). They may even outlive the site itself or escape into the environment as pollutants.  Modern records typically contain around 135g of PVC material with a carbon footprint of 0.5kg of CO₂ (based on 3.4kg of CO₂ per 1kg of PVC). Sales of 4.1m records would produce 1.9 thousand tonnes of CO₂ – not taking transport and packaging into account. That is the entire footprint of almost 400 people per year.   In the 80s, records were replaced by CDs, which promised durability and better sound quality. CDs were made of layered polycarbonate and aluminium, which has slightly less environmental impact than PVC, and are manufactured using less materials than records. However, CDs can’t be recycled because they’re made of mixed materials that are difficult and uneconomical to separate into their component parts for recycling. CDs were also encased in fragile polycarbonate cases, which, despite being a single material, aren’t widely recycled. They also aren’t as indestructible as many people first thought, which meant many ended up in landfills.    As new formats of music emerged throughout the years – first albums, then cassettes, CDs, and now streaming services – there was a cycle of waste and destruction as old technologies were replaced by new ones. But we didn’t move to CDs by choice – it was simply what companies manufactured at the time. While high-quality CDs could last for 50 to 100 years under ideal conditions, this isn’t true for many low-quality, cheap CDs. These were easily damaged by direct exposure to sunlight and heat, warped by fast-changing temperatures, gravity, scratches, fingerprints and smudges – subsequently resulting in them getting thrown out. Current digital technology gives us flawless music quality without physical deterioration. Music is easy to copy and upload, and can be streamed online without downloading. Since our digital music is less tangible than vinyl or CDs, surely it must be more environmentally friendly?  Even though new formats are material-free, that doesn’t mean they don’t have an environmental impact. The electronic files we download are stored on active, cooled servers. The information is then retrieved and transmitted across the network to a router, which is transferred by wifi to our electronic devices. This happens every time we stream a track, which costs energy. Once vinyl is purchased, it can be played over and over again, the only carbon cost coming from running the record player. However, if we listen to our streamed music using a hifi sound system it’s estimated to use 107 kilowatt hours of electricity a year, costing about £15.00 to run. A CD player uses 34.7 kilowatt hours a year and costs £5 to run.  So which is the greener option? It depends on many things, including how many times you listen to your music. If you only listen to a track a couple of times, then streaming is the best option. If you listen repeatedly, a physical copy is best; 
streaming an album over the internet more than 27 times will likely use more energy than it takes to produce and manufacture the same CD.  If you want to reduce your impact on the environment, then vintage vinyl could be a great physical option. For online music, local storage on phones, computers or local network drives keeps the data closer to the user and will reduce the need for streaming over distance from remote severs across a power-hungry network. In a world where more and more of our economy and social relations happen online, records, and other vintage music formats, buck that trend. Instead, the record revival shows us what we want to see in our media and material world more widely – experiences that hold their value and with loving care endure. Older music formats have a sense of importance and permanence attached to them, belonging to us in a way that our virtual purchases simply don’t.  It seems that whatever the format, owning copies of our favourite and most treasured music, and playing them over and over again, might just be the best option for our environment."
nan
"If you take an interest in ethical consumerism and plan to treat someone special this February 14, what dilemmas lie ahead? You might already be conscious of getting child labour and slave-free chocolate, a recycled card, even fair trade gold, and perhaps vintage or conflict-free diamonds if it’s a very special year. But what about your flowers?  This year one of us (Jill Timms) will spend her Valentine’s Day looking at sustainable supply chains in Lake Naivasha, Kenya, where hundreds of flower workers will be recovering from their busiest time of year.  Across the world, 250m rose stems will be produced for the day. Of those exported to the EU, 38% are from Kenya, where flower export values have trebled this decade. Governments in Ethiopia, Tanzania and more recently Uganda and Rwanda, are also pursuing expansion, with flowers now accounting for 10% of East African exports. That part of the world has a natural abundance of heat and space, and lots of available cheap labour. Flowers could help the regional economy to “bloom”. However, there are significant social and environmental challenges, such as the massive population growth around Lake Naivasha which contributes to pollution and has helped cut the lake’s volume in half.  Our own research project on sustainable flowers focuses on stakeholders from different parts of the supply chain. But you definitely have a role to play here too, and it begins with asking questions of the flowers you buy. Here are our top five: Geography matters. Some flowers travel by sea, some cargo plane and others in the hold of passenger jets, all with very different carbon footprints. For instance more than 90% of UK flowers are imported, mostly from the Netherlands, although Kenya and Columbia are increasingly important suppliers. Chemical sprays freeze flowers to extend life, and they often travel via the Dutch flower hub. Historically the Netherlands has been the industry powerhouse, but now works hard to retain this in the face of direct supermarket buying, growth in Chinese, East African and South American production, and criticism of the extra “flower miles” involved in transporting via Holland. So provenance is important, but you may struggle to know this. Flowers are not always labelled, labels don’t always specify origin or may list the Netherlands if bought at auction, and bouquets include flowers from multiple sources. Even when the origin is known, things can still be unclear as sustainability issues vary widely by country and flower. Of course, very short supply chains are possible for some varieties (the shortest being from your garden, if you have one). But this sort of localised growing does not satisfy the demand for volume, variety and year-round supply, or indeed guarantee sustainability in terms of energy, pesticide use and so on. In response to ethical concerns, “certification” schemes are becoming more common. Yet we find consumers, florists and even wholesalers are often unaware or misunderstand these, with Fairtrade still being the only one with wider recognition. We are working with bodies including the British Florist Association to educate florists about standards, and wholesalers like Fleurmetz to review how certification can be more visible. You can help by asking your florist if their flowers are certified. If they don’t know, ask to see delivery boxes.  In the UK, about 60% of flowers are bought from supermarkets, with the rest mostly from florists. Supermarkets have their pros and cons. Flowers tend to be better labelled, and they are more likely to cut out the auctions and buy direct from growers, which assures provenance and means they can influence standards. However the supermarkets might not share this information, and their demands on price, volume and the short time from field to market can put inordinate pressure on farms.  In contrast, the demise of the high street, Brexit uncertainty and increased online and supermarket competition, has led to “support your local florist” campaigns. Interestingly, some florists have responded by using sustainability as a selling point. Certifications can help you support farms that claim good practice, but could your purchase also promote development – a familiar argument for global trade? Of course it depends how it is done. For example, the Ethiopian government attracted lots of foreign investment in flower farming. However, incentives included controversial land use agreements that led to civil unrest in 2016, with several foreign-owned flower farms badly damaged or burnt to the ground. There is always a trade-off. Flowers grown in greenhouses in Holland use enormous amounts of energy, but travel less. Lake Naivasha roses enjoy natural heat and light, but are flown many miles and can be chemically treated to survive. So your priorities need to guide your purchase: environmental issues include carbon footprint, chemical use, ecological degradation and water use; social issues include health and safety standards, gender discrimination, precarious employment and land rights.  Accordingly you might choose locally-grown seasonal or organic  flowers, or seek growers who support community development or rights for women workers. Eco-florists such as Wild and Wondrous are raising awareness of alternative practices. Take in your own vase to avoid cellophane packaging or ask for reusable and recycled options like StemGem. When presenting your blooms, take inspiration from the #nofloralfoam campaign. Treat your flowers well by refreshing water and trimming, keep them out of heat and sunlight, then recycle as green waste to make their journey worthwhile. St Valentine’s is a day to express our love, so demonstrate yours for people and planet. The supply chains are complex, but our simple advice is to ask questions."
"

The recent Earth Summit, the United Nations Conference on Environment and Development (UNCED), in Rio de Janeiro was to have established an ambitious environmental agenda for the 21st century. The conference addressed virtually every field of human endeavor: energy, industry, water and land use, agriculture, human health, and transportation, among others. An international congregation of tens of thousands of bureaucrats, environmentalists, technocrats, planners, and others arrived in Rio to guide humanity on its course toward the brave new world defined by Agenda 21, the Rio Declaration, the Conventions on Climate Change and Biological Diversity, and the Forest Principles.



The principal theme for UNCED was “sustainable development,” a concept popularized by the report of the World Commission on Environment and Development. That report, entitled Our Common Future, was presented to the United Nations in 1987 by commission chairman Gro Harlem Brundtland, the Labor prime minister of Norway.



Our Common Future was compiled in response to a 1983 request of the UN General Assembly for “a global agenda for change.” Heavily influenced by former West German chancellor Willy Brandt’s North‐​South Report, the commission called for restructuring mankind’s future so that economic growth would be “based on policies that sustain but expand the environmental resource base.”[1]



“Sustainable development” means different things to different people. Its definition is intentionally vague to increase the possibility of compromise on thorny issues on which reasonable people may differ. To those inclined toward balancing economic and environmental goals–as are the authors of this study–“development” implies economic growth, and “sustainable” implies full consideration of environmental factors. It is becoming abundantly clear, however, that to others the term implies virtually no additional economic development.[2] The latter position is based on the argument–made with great emotion but insufficient facts and analysis–that the current path of development is clearly unsustainable because the planet is about to choke on humanity’s wastes and there is not enough land to meet everyone’s demands. Even though that faction, inherently suspicious of technological change and economic growth, was unable to obtain all that it wanted, it is today much closer to its goal as laid out explicitly in the early drafts of the Agenda 21 documents.



The draft Agenda 21 documents specified various measures that nations “should” undertake. Ultimately, the conference shied away from that heavy‐​handed approach and decreed those measures to be optional. If acted upon faithfully, many of those measures would impose national planning on virtually every economic sector, regulate almost all human activity, and subordinate all social and economic goals to environmental goals. Thus, while calling for a massive transfer of wealth and technology from developed to developing nations, the recommended measures would deny all nations the means of creating more technology and wealth by putting in place legal and institutional frameworks that would be incompatible with either technological progress or economic growth. Proponents of those measures, in part because of their refusal to balance environmental and socioeconomic goals, would needlessly reduce agricultural productivity in the name of sustainability–even as they lament that the world has too many people to feed and that agriculture uses too much land and denudes forests, thereby threatening species and reducing biodiversity. Such measures, in addition to being poor social and economic policy, would be counterproductive; they would cause more environmental harm than good, and they might even bring about the very catastrophe environmentalists strive to avert.



In particular, the Convention on Biological Diversity, which the United States rightly refused to sign, does not even address the single most important reason for the loss of biological diversity and deforestation: the loss of habitat and land conversion to meet fundamental human needs for food, clothing, and shelter. Moveover, the convention would reduce the incentives to research and develop the very technologies that would–if anything could–help meet the competing demands made on land by human beings and other species.



Using as an example the historical increase in U.S. agricultural productivity, this study will show that–but for technological progress–all our forestlands and croplands, including those that would have been only marginally productive, would have had to have been plowed to produce the quantities of food we produce today. The accompanying wholesale destruction of forests and other natural habitats and the reduction of biodiversity would have resulted in environmental problems that would have dwarfed those we face today and matched environmentalists’ worst nightmares.



The only way to feed, clothe, and shelter the greater world population that the future will inevitably bring–while limiting deforestation and loss of biodiversity and carbon dioxide sinks–is to increase, in an environmentally sound manner, the productivity of all activities that use land.[3] Such increases are possible only within a legal, economic, and institutional framework that relies on free markets, fosters decentralized decisionmaking, respects individual property rights, and rewards entrepreneurship. Such an approach is the best hope for a world facing severe pressure on its land base, yet it has been noticeably absent from recent, much‐​publicized strategies that purport to lead us to sustainable development, conserve biological diversity, and combat global deforestation.
"
"
Share this...FacebookTwitterOh the irony!
Potsdam is the home of the alarmist Potsdam Institute for Climate Impact Research, headed by Professor Hans-Joachim Schellnhuber and Stefan Rahmstorf, who has been very busy lately trying to convince the rest of the world that the cold is due to the warming, at least that’s what his models are saying (now).
Snowiest December ever
Potsdam is now well on its way to recording its snowiest December since records began in 1893. The Meteorological Station Potsdam Telegrafenberg has been recording a wide variety of weather parameters since 1893, and looking at the records, no December compares to the one they are having now.
Daily snow cover in Potsdam: Source: http://saekular.pik-potsdam.de/2007_en/index.html
As Rahmstorf and Schellnhuber watch crews remove snow every day at the PIK, they really must be cursing all the white stuff that was never supposed to happen.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Some December data at the PIK – records smashed
On December 21 the station had a snow cover of 33 cm, a December record, smashing the old record of 23 cm set 97 years ago in 1913.
This December will have had snow cover 30 of 31 days (No thaw is in sight) – the 2nd most, just behind 1969 (see Figure below), which saw 31 days of snow on the ground. Currently there is 23 cm of snow cover, which was the old record.
Number of days with snow cover in December. Source; http://saekular.pik-potsdam.de/2007_en/index.html
The average so far for this December has been about 16 cm of snow cover. With the current forecast, that too will not change. If anything it may increase. That will make it an all-time record.
It’s been one of the coldest Decembers in Potsdam since records started. Temperatures for this December in Potsdam have averaged near -4°C so far.
The snow has not only been frequent in Potsdam, but all over Germany. Many cities are poised to set new records for most days with snow cover on the ground. Read here.
Share this...FacebookTwitter "
"If you’re committed to the Paris agreement – to keep the increase in global average temperature to well below two degrees above pre-industrial levels, and pursue efforts to limit the increase to 1.5 degrees – then at a minimum, logically, scientifically, you’re committed to net-zero carbon emissions by 2050. So far, at least 77 countries have committed to the target, as has every state and territory in Australia. The fact that prime minister Scott Morrison is pushing back hard against the calls for such a target sends yet another strong signal that his government still denies the need to tackle climate change. Sensing it must be seen to do something, but committed to doing nothing substantive, the government is arguing that investing in technology is the superior pathway to… to… to what? Are billions of dollars of public funds about to be allocated to a strategy that delivers on an unspoken goal? This passion for technology is newfound and insincere. In truth, our government has a long history of undermining climate technologies. In the three years to 2016, the government ripped just shy of $1bn from the Australian Renewable Energy Agency (Arena), the body charged with helping early stage technologies through to commercial launch. The funding of a feasibility study for a coal power station in Collinsville and the foreshadowed gift of $11m to extend the life of the 42 years old Vales Point coal power station in the Hunter, demonstrate just how reluctant the Coalition is to let go of last century’s energy technologies. One of the most promising and critical new technologies is the rapid maturation of the electric vehicle, but who can forget the government’s pushback against EVs during last year’s election? pic.twitter.com/GIvJffJ5EJ Last November I visited the Leilac zero carbon cement project Belgium – an exciting project given that cement is responsible for 7% of global emissions, more than twice as much as aviation. The new process captures most of the carbon dioxide that’s ordinarily released to the atmosphere during cement manufacture. The technology, which can be powered by renewable energy, was developed in Bacchus Marsh, Victoria and was lured to Europe on the back of a €12 million grant and a price on carbon. In the alternate universe where Arena and our carbon price weren’t smashed by ideological attacks, that world-changing technology would be proudly Australian made. While there’s plenty of valuable research and development in our future, especially for the difficult to decarbonise sectors of cement, steel and aviation, the truth is that we already have the technology to deal with around 70% of global emissions. The pathway is simple – electrify everything and swap fossil fuels for renewables. These technologies have come down in cost not because of boffins in laboratory coats, but because of innovation born of sustained deployment and ruthless competition. Mike and Annie Cannon-Brooke’s Resilient Energy Collective is a case study for how far we’ve come. In just a handful of weeks the group has put together an emergency power product for restoring power to bushfire affected communities. The solar-powered, battery-backed system can be installed in a single day, and will be rolled out to 100 communities in as many days. The energy supply companies partnering in the project are stunned that the infrastructure is being rolled out in hours not months. Community members are amazed that they’re using solar power at night. Likewise, Aemo, our grid operator, has just released a blueprint for reducing electricity sector emissions by 85%, using existing technologies and without compromising reliability. Industry is champing at the bit to implement such a plan — they just need a minister who believes in the end goal and is committed to resolving the roadblocks. In reality, the call for technology before action is a specious distraction designed to paper over the plan to take no action. The greatest proponent of the frame is Danish political scientist Bjorn Lomborg, one of a small cadre of almost respectable climate obfuscationists. In the lead up to the Copenhagen climate conference in 2009, Lomborg handpicked a panel of ancient Nobel laureates to rank 16 climate solutions. The four proposed carbon tax schemes were ranked dead last, and the top three projects deemed worthy of consideration were “marine cloud whitening”, energy research and development and “stratospheric aerosol insertion”. The top-ranked solution would involve a global fleet of “1,900 unmanned ships spraying sea water mist into the air to thicken clouds” and reflect the sun’s rays back into space. The third solution involves fleets of planes spraying sulphur dioxide into the sky. The chemical would mimic the effects of volcanoes “reacting with water to form a hazy layer … spread around the globe … scattering and absorbing incoming sunlight”. The first three years of the Coalition government focussed on tearing down climate policy. The next three used endless reviews that came to nothing – as intended. In July 2014, Tony Abbott finally made good on his promise to dismantle Australia’s carbon price mechanism, our most effective and efficient climate policy. In doing so, not only did he throw away the best tool we had, he cheated Australian farmers out of earning billions from exporting carbon credits to Europe. In 2015, Abbott managed to slash the renewable energy target – assisted in the background by Angus Taylor, the man now charged with reducing emissions – cutting future activity under the target by 40%. The only half decent action has been the emissions reduction fund, called a fig leaf of a policy by the party’s once and future leader Malcolm Turnbull in 2009, whereby taxpayers, not polluters, buy carbon offsets. To date, the ERF has bought just 50m offsets, which doesn’t even cover the increase in emissions from just the LNG sector during the last 5 years. Now the government is talking about a “technology investment target”, whatever that means. Will we be subjected to another barrage of lies that some magical technology exists to cut coal emissions? Remember CCS and HELE? Hopefully by now we all now know that “clean coal” is as real as healthy cigarettes. If Scott Morrison is genuine about climate action, then sure, he should start by restoring the billion dollars ripped out of Arena. In fact, let’s give them a few hundred million a year to help Australian ideas reach their potential and give us a whole new export sector to replace the inevitable decline in coal exports. We have the resources, people and smarts to position Australia for great success in a carbon-constrained global economy. At this point, the roadblocks to effective and affordable action are social and political, not technological. So here we are again. Another strategy to kick the can down the road. The Finkel review bought the government a year of doing nothing in 2017, as did the national energy guarantee in 2018. The hollow climate solutions package helped the government escape scrutiny in 2019, however the “Black Summer” and the approaching November’s COP26 conference in Glasgow – where countries are expected to lift their commitments in the direction of the Paris agreement’s goals – leave the government with nowhere to hide. Simon Holmes à Court is senior advisor to the Climate and Energy College at Melbourne University and sits on the board of the Smart Energy Council"
"Ten years on from the devastating Boxing Day earthquake and tsunami, our understanding of very large earthquakes has grown enormously. From satellites monitoring changes on the Earth’s surface to drilling deep below the ocean floor, new techniques are constantly being developed to help us figure out why earthquakes are sometimes so big, and so deadly.  The 2004 earthquake ruptured the fault marking the contact between two tectonic plates at a subduction zone, where one plate slides beneath another – the locations on Earth where the very largest earthquakes and tsunami are generated. Before 2004, the last earthquake of magnitude nine or larger occurred in 1964, in Alaska. However in the past decade there have been several very large quakes, including Chile in 2010 and the 2011 Tohoku-oki earthquake in Japan that caused the Fukushima nuclear power station meltdowns.  We know a great deal more about these recent events than we did the Alaska earthquake as, not surprisingly, the technology we use is a lot more advanced than in the 1960s. Satellites can now track the movement of the Earth’s surface during, before and after an earthquake using GPS, and they can track a tsunami wave in the deep ocean. Techniques to record the earthquake waves and to resolve the details of the earthquake fault slip are now much more sophisticated. And we can gain much higher resolution details of geological layers and structure under the seafloor, helping us find out more about earthquake faults. The 2004 earthquake occurred when the Indian plate slipped beneath the Sunda plate. Though scientists were well aware this was an active fault line, the plate slipped over a much larger area and further into the shallow fault zone than anticipated. The 2011 Japan earthquake and tsunami that followed was even more unexpected in character: the fault slipped into its very shallowest section and the slip was the greatest ever recorded on a fault – the fault (and in turn the overlying seabed) moved by up to 50-60m. To put this into context, in a “typical” large earthquake, the fault slips by about 5-10m and still causes very damaging effects. For the first time, equipment on the seafloor offshore Japan in 2011 measured the changes in pressure as the seafloor moved, and was able to record the magnitude of the fault slip. This very large and shallow slip certainly amplified the tsunami, leading to devastating results. These recent earthquakes have truly called into question the existing models for very large earthquake generation and have resulted in scientists rapidly generating new ideas and re-examining the hazard potential at other subduction zones around the world. Although we now have very sophisticated techniques to remotely record the earthquake process and the geology below the seafloor, we really need to sample the fault rocks themselves, where the real action goes on and where earthquakes are generated, typically 5km or more below the surface or seafloor. Sampling this zone “in situ” is one of the holy grails of modern fault and earthquake studies – and the only way to do this is through ocean drilling using the same techniques as the petroleum industry.  In the past 10-15 years projects tackling active faults and the earthquake process, particularly at subduction zones, have increased in number and in the boldness of their objectives, drilling in very challenging conditions.  Drilling is ongoing on the tectonic plate margin of Japan as part of the NanTroSEIZE programme, south of the region affected by the 2011 earthquake. Scientists have drilled lots of boreholes to sample different parts of the system and the project is currently 2km from the primary target fault zone. This part of the Japanese plate boundary generates damaging tsunami-genic earthquakes about every 150-200 years and the project will not only sample the fault rocks but also continuously monitor the fault to help reduce potential risk.  Further north along the Japan coast, and soon after the 2011 earthquake, a team of scientists rapidly prepared a ship to drill through the shallow fault where the extremely large slip occurred. The rock samples they brought back to the ship were unexpectedly weak and we now believe this is why there was such large and shallow slip.  Weakness of the earthquake fault may be the answer for the surprising 2011 earthquake but what about other subduction zones?  A drilling expedition will sail to the tectonic margin off Sumatra in 2016 to drill into the sediments supplying the fault zone that generated the 2004 earthquake. Here we hypothesise that the sediments may be especially strong and that this resulted in shallower fault slip than expected – the opposite to what has been proposed for the Japan 2011 earthquake fault.  If both theories are correct, we will need to revisit global subduction zones to re-assess sediment and fault properties and how these faults may slip in future. Since the 2004 earthquake and tsunami we’ve learned a lot but, as is often the case in science, we now have new questions to answer. However new technology is giving us the ability to answer them – this is a very important time to be an Earth scientist."
"
Share this...FacebookTwitterHat tip: Readers Edition.
Michael Krüger at skeptic site Readers Edition brings our attention to an interview (in German) with skeptic scientist Professor Werner Kirstein of the Institute for Geography at the University of Leipzig on MDR German Public Radio. (Expect MDR to receive hellfire and brimstone for daring to air such blasphemy).

In the interview Professor Kirstein is asked about the cold winters being a sign of warming. He said that it just doesn’t fit, and that years ago the same scientists predicted the opposite, and reminds us what climate experts said 3 or 4 years ago:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In Germany we will rapidly see no more snow in the winter.”
So much for that prediction.
Professor Kirstein also cautioned against placing too much emphasis on 2010 being among the hottest years on record, believing the claim is “a joke” and saying that determining a global average is a tricky business and in the end is only a theoretical value. The radio moderator then attempts to make Kirstein look like a denier in asking:
And so what you are saying is that there is no climate change?”
To which Kirstein answered that indeed there is – from natural causes, citing the extreme temperature variations of the past repeating ice ages. Kirstein disputes the notion that man-made CO2 is driving the climate today, saying CO2 has at best extremely little to do with temperature, and it will not do any good to reduce it. He views political attempts to reduce CO2 as an effort to steer the economy into a certain direction and as a way to earn money.
Near the end of the clip, Kirstein says flat out that it all comes down to a belief – a religion.
Transcript (in German) of the interview, Read here.
Professor Kirstein was also once featured at NTZ not long ago, Read here.
Share this...FacebookTwitter "
"It is not uncommon for advocates of a socialist alternative to capitalism to hear others say “it’s a nice idea but it does not work in practice”, or that the Soviet experience proved, once and for all, that capitalism is the best socio-economic model we can get.  Erik Olin Wright, who died on January 23 2019, begged to differ. Wright devoted his career as a sociology professor at the University of Wisconsin-Madison to breathing new life into the study of alternatives to capitalism. He did so with a consistent concern, namely to harness social scientific reasoning to construct a better socio-economic system. And yet, by the end of his life, Wright had reached a level of international acclaim that few Marxist theorists ever achieve. With the global financial crisis prompting a widespread search for alternatives to capitalism, and ideas he championed such as the universal basic income moving into the mainstream, Wright was a key figure behind the resurgent left on both sides of the Atlantic. His ideas are more relevant now than ever. In 2010, Wright poured his decades of research into one of his major works entitled Envisioning Real Utopias. In this immensely lucid, accessible and important book, Wright made his most comprehensive attempt at formulating a set of ideas and strategies that could liberate humanity from its current social, economic and political restrictions – what we might term an “emancipatory” social science. He constructed it around three main axes: a diagnosis of capitalism, a look at some alternatives to capitalism and a theory of transformation. Here, Wright wished to show the reader why a socialist alternative is not only desirable but also something achievable.  While there are different possible pathways towards the socialist transformation of society, Wright told us, we are best off conceptualising alternatives to the status quo on the basis of the “anti-capitalist potential” of things that actually exist. These “real utopias”, as he called them, include such organisations as workers’ cooperatives and even Wikipedia. They are “real” simply by virtue of existing. And they are “utopian” because the values upon which they rest, along with the practices they uphold, provide insights into an emancipatory alternative that has the potential to be brought into existence. But Envisioning Real Utopias is more than a book. It is, as I see it, akin to a manual for thinking about the practice of socialism. For Wright did not limit himself to demonstrating why capitalism ought to, and indeed can be, superseded by something better. He also highlighted how three core socialist values – equality, cooperation and freedom – can co-exist in practice. As such, he provided the reader with a guide for not merely thinking about emancipation but, also and crucially, for realising it.  Its realisation, Wright thought, would depend on the formation of institutions that can guarantee “social empowerment” or, to put it differently, individuals’ capacity to exert control, collectively, over the ownership and use of economic resources and activities. In a workers’ cooperative, for example, workers are effectively empowered by owning an equal share of the organisation and having an equal voice in decision making.  In the case of Wikipedia, one finds highly collaborative practices between a core of paid employees and volunteers. Decisions, such as what to include in different encyclopedic entries, are the result of a deliberative process between editors (which any member of the public can choose to become), without the need for a body specifically devoted to editorial control.  Wright acknowledged the fact that “social empowerment” could take several forms but insisted that, the higher its degree, the more an economy and society would be regarded as socialist.  His analytical framework can therefore be used to assess the degree of social empowerment that particular organisations have the potential to realise. This is in fact what Wright set out to do with worker cooperatives and Wikipedia. Despite their currently limited capacity to radically change things within a capitalist economy, both provide insights into the kind of institutions capable of guiding society towards greater empowerment.  It is worth remembering here that capitalism began its journey within the margins of the feudal economy – and what Wright provided us with are the analytical resources to understand how socialism could potentially emerge from within the margins of the capitalist economy. Those reading the book will also be in a position to appreciate something that characterised many of Wright’s works, namely the capacity to combine a passionate stance with intellectual rigour. He was indeed critical of capitalism and passionate about emancipation, but also firmly devoted to the task of formulating a robust critical social science. Whether Wright studied inequality, the basic income, or alternatives to capitalism, he would do so with admirable assiduity, clarity and precision. The greatest achievement of Envisioning Real Utopias is, as I understand it, its invaluable role in demonstrating that despite the reigning cynicism about the possibility for change, one need not seek refuge in a utopia in the clouds. Wright’s book is a stark reminder that elements of a vision for the future are often, if not always, found in the present. In his poignant and inspiring blog documenting the days before his death, Wright told us he did not fear death. He has now left us, but his work lives on – a work that inspires us not to fear capitalism’s death and to remain hopeful about socialism’s existence."
"Last year, humans emitted approximately 37 billion tonnes of carbon dioxide into the atmosphere – a disastrous and unsustainable figure. To avoid the worst effects of climate change we could capture some of that carbon as it is released by power plants and store it permanently below ground. Better still, some of that waste carbon dioxide could be converted into useful chemicals or fuel.  These processes are known respectively as “carbon capture and storage” and “carbon dioxide utilisation”, and
both require large amounts of raw materials. As an example, carbon capture can involve running emissions over some metal, which then reacts with (and hence captures) the CO₂ before turning it into a different substance that can be stored or reused. To make a dent in climate change, the amount of metal required would be huge. For example, if 1 gram of a metal, in a metal catalyst, could capture 100 grams of coal-based carbon dioxide emissions (an optimistic scenario), around 1.5m tonnes of this metal would reduce global emissions by just 0.4%. So, although keeping carbon out of the atmosphere is important, it is equally important that we do it in a green and sustainable manner. If large amounts of a metal are ever used to significantly decrease carbon emissions, it must have a sustainable supply so that reserves are not depleted.  Sadly, many technologies seem to be ultimately unsustainable. For instance, a recent study by a team of Japanese scientists, highlighted by the Royal Society of Chemistry, described how a catalyst based on the metal rhenium converts carbon dioxide into carbon monoxide. Carbon monoxide is useful as it can be used to form chemicals and fuels such as hydrogen and methanol. The catalyst is indeed extremely active and can work with carbon dioxide at very low concentrations, but the system is still not ideal. Rhenium is very rare: mostly found in Chile and Kazakhstan, it is estimated to have an abundance of less than 10 parts per billion in the Earth’s crust (equivalent to 0.000001%). To put that in prospective, aluminium is 8 million times more abundant and accounts for about 8% of the Earth’s crust. Rhenium itself is mostly used to make turbine blades in aircraft jet engines. If this metal was employed to tackle climate change globally, resources would decrease and its price would increase. This would have a knock-on effect on industrial manufacturing.  Its low abundance also means that producing this catalyst would be expensive. It is therefore unlikely that a global business model for worldwide rhenium based carbon dioxide utilisation would be pursued. In another study, an American research team created a ruthenium catalyst which could transform carbon dioxide from the air into the fuel methanol. However, ruthenium is also incredibly rare, and would likely encounter the same availability and cost problems. Fortunately, it is possible to develop catalysts that are more sustainable and environmentally friendly. This ties in with the principles of “green chemistry” which has been around since the 1990s and has gone from strength to strength.  I am one of numerous researchers across the globe using relatively abundant, and thus more sustainable, metals for carbon dioxide conversion. Colleagues and I recently developed an aluminium catalyst, for instance. It makes sense to use aluminium as it is one of the most abundant metals in the Earth’s crust and has shown promise in carbon dioxide utilisation.  This catalyst can convert carbon dioxide into cyclic carbonates, commercially valuable products used in batteries, pharmaceuticals and polymers. The catalyst can also be “regenerated” once its reactivity has gone and can be reused multiple times.  But it’s not always straightforward to use more abundant metals and I admit I myself have dabbled in using less sustainable metals. These include chromium, a toxic form of which was the subject of the film “Erin Brockovich”, and platinum, another metal estimated to account for less than 0.000001% of the Earth’s crust.  I used these scarce metals because sustainability is not always a substitute for reactivity. Fundamental chemical differences between rare and abundant elements means that simple substitution will not necessarily create a catalyst. For example, my colleagues found that chromium was more reactive in some cases than aluminium in forming cyclic carbonates.  Researching rare metals is still an interesting area to explore and will lead to new chemical discoveries that abundant metals could not produce. The impressive catalytic activity of the rhenium and ruthenium catalysts must not be ignored.  The massive problem of climate change however means that we have to be more realistic and considerate when it comes to designing catalysts for large-scale industrial application. This is by no means an easy feat. Of course, just using abundant natural materials will not necessarily make our methods greener. A true evaluation of sustainability is tough and involves a complex assessment of the entire process, including factors such as raw materials used, energy required, operation costs and carbon saved. Ultimately, we must divert more effort towards sustainable climate change reduction as soon as possible. As David Attenborough said at the recent COP24 summit in Poland: “If we don’t take action, the collapse of our civilisations, and the extinction of much of the natural world, is on the horizon”."
"I walked my kids to school recently through the pretty North East English village of East Boldon and saw a red cross on a beautiful old rowan tree. I thought it might be the diesel haze of nearby commuter traffic confusing my senses, but no. One email to my local councillors later and my worst fears were realised. The rowan tree, a species which feeds flocks of birds each autumn with its berries and whose only crime is to be old and gnarly, is for the chop – and the birds’ autumn lifeline with it.  In the email to my councillors I pointed out that two of the three trees slated for removal in that particular copse do not need to be cut down, they just need a bit of remedial work. One is a small tree smothered with ivy that simply needs to be stripped away.  The third is dead and I suggested in my email that the grounds team cut it into logs and scatter them around the small woodland – allowing them to rot for the benefit of fungi and insects, instead of their usual annihilation in an industrial grinder.  As a lecturer in ecology with 25 years of experience and over 30 peer reviewed papers published, I expected my suggestions to be taken seriously. While the councillor was prompt to reply and polite, it was made clear that grounds management is none of my business. From my experience in the UK, grounds management teams employed by local councils seem unresponsive to expert advice. They also appear unsure how to reconcile having wild areas which are safe for human use but which remain useful for wildlife.  The small patches of wild and semi-wild areas in cities have been mostly stripped of their value to wildlife by over-intensive grounds management. Anything that has the vague look of irregularity is removed. No wood is allowed to lie on the ground, it must be tidied away. A 2013 report by the Royal Society for the Protection of Birds reviews the dramatic loss of wildlife habitat in the UK in great detail, emphasising the need to improve the semi-wild urban areas we still have for wildlife.   The wildlife value of a particular tree species in cities is often disregarded when a decision is made to remove it. In parks, plant species which are exotic to the UK such as the New Zealand cabbage tree (Cordyline australis) are intentionally planted because no native wildlife can use them, so they are low maintenance. Pansies and other colourful flowering plants used in urban park borders are often useless for pollinators. This is due to modifications made during breeding of such plants which restricts access to the nectar producing organs and sometimes leads to lower rates of nectar production.  Petals of many ornamental plants are bred to enclose the central nectaries – nectar producing organs – and sexual parts of the flower that produce pollen and are of use to pollinators. This breeding gives the flower a beautiful ball shape but limits their use to insects. I suspect such plants are only put in urban parks in the first place to please the aesthetics of pensioners – a key voter demographic for local councils.  Leaves are swept up immediately before their nutrients can return to the ground and the insects that lay their eggs on them are doomed to certain death. Road verges are cut back to the bone several times each year and the clippings are left lying, minimising their use as a habitat for wild flowers.  It doesn’t help that grounds management is often subcontracted to private firms in the UK. In these cases, grounds management is more likely to be insensitive to expert advice as the function is out of the hands of the democratically controlled body and with a private company that needn’t care what the public thinks.  Only a few UK councils that I am aware of are making progress in wildlife-sensitive grounds management. Councils such as Burnley, Dorset, Devon, Cornwall, East Sussex and Bristol have developed comprehensive pollinator action plans and are taking measures like allowing grass to grow longer in public areas, planting native, bee-friendly flowers and mowing verges less frequently.  The irony of it all is that measures to improve public areas for wildlife essentially involve less effort overall. All that really needs to be done is to allow public areas to be a little more unkempt, because unkempt areas are what nature likes. If local councils allow this to happen, the public shouldn’t berate them. We must be sympathetic to having natural and semi-natural areas that don’t look like Tellytubby land.  But I am sceptical. Local councils in the UK seem a long way from taking wildlife seriously. Unfortunately, the focus on human ideas of “neatness” at present dominates any love for nature. I fear that green spaces in urban areas will continue their decline as wildlife habitats."
"

On April 7, I wrote about global warming “hotheads,” who dominate the science profession for a lot of very sound economic, incentive‐​based reasons.



We don’t shell out multimillion‐​dollar grants to people who say something isn’t a problem. Recipients of this largess peer‐​review each other’s papers. There’s a lot of incentive to give a bad review to a manuscript downplaying the issue and to give a great one to the paper describing an upcoming apocalypse.



Anyone who disagrees with this should spend some time reading the “climategate” e‐​mails purloined from the Climate Research Unit at the University of East Anglia in November 2009.



Transport yourself from the biased world of peer‐​reviewed science to the biased world of amateur climatology on the Internet. They really aren’t very different; they’re just symmetrically opposite.



In the Internet world where the flatliners live, there’s no such thing as global warming. Where the hotheads reside, it’s everywhere and all the time. Some flatliners even doubt the whole notion of the greenhouse effect — the recycling of infrared radiation by water vapor and carbon dioxide (and a few other things) — that keeps the lower atmosphere about 60 degrees warmer than it would otherwise be.



Both groups are delusional. Hotheads are _convinced, mind you_ that the Koch Brothers are behind the flatliners, and flatliners are _convinced, mind you_ that there’s a hothead conspiracy coordinated by George Soros. Both view each other as murderers of the world: Hotheads will kill the economy, while flatliners will destroy civilization.



Here is the flatliner Holy Picture:





This is the global surface temperature departure from the 1961–1990 average, also from the CRU at East Anglia. (Despite all the climategate hubbub, this is still the reference standard in the business). It’s obviously flat, giving rise to the flatliner mantra: “No warming in 14 years — the same time in which the greatest increases in atmospheric carbon dioxide occurred.” Or, put in more stark terms, “Global warming is a commie plot.”



The flatline argument happens to be popular because it is occurring now, and few people outside of Aspie climate guys like me can remember the weather more than a year or two back.



Here’s a little secret about global warming. The central tendency of computer models using input that pretty much mimics the observed changes in carbon dioxide is to produce a constant (not an increasing) rate of warming.



There are many reasons for this. The response of surface temperature to an increment of carbon dioxide is logarithmic. So for every part per million (ppm) increase, a little less warming is generated. But the actual increase in its atmospheric concentration is a low exponent. When we started monitoring it at Mauna Loa in 1957, CO2 was growing at about 0.75 ppm annually. Now it’s growing at around 2 ppm.



The addition of a logarithmic response to an exponential increase in the cause of something can in fact add up to a straight line, which is very obvious in a suite of the temperature projections made by our friends at the U.N.



There are two warming periods in our recent history. One, in the early 20th century, could not have been caused by carbon dioxide, because we simply hadn’t put very much in the air back then. The second one, which begins in the mid‐​1970s, is much more suspicious because it has been accompanied by a cooling of the stratosphere and is accentuated at high latitudes in the Northern Hemisphere and in the winter, which is what one would expect from increasing CO2.



Here’s the East Anglia history since then, with a straight line fit to the data:





The fit of a constant trend to the overall data is striking, despite the fact that indeed there is no net warming in the last 14 years. In fact, fitting any simple curve to the data does no better than the straight line.



So much for the flatliners. They have lived in fortuitous times. Stay tuned for an analysis of the lukewarmers.
"
"There aren’t many corners of the world left untouched by humanity. Recent research has highlighted that just 23% of the planet’s land surface (excluding Antarctica) and 13% of the ocean can now be classified as wilderness, representing nearly a 10% decline over the last 20 years. And more than 70% of what wilderness remains is contained within just five countries.  Researchers from the US and Australia recently produced a global map to illustrate this decline, made by combining data on things such as population density, night-time lights and types of vegetation. The problem with such an approach is that the question of where wilderness begins and ends is not as simple as it may first seem. The data used to map wilderness is often collected in different ways for different parts of the world. For example, some datasets map roads all the way down to farm and forest tracks, while others may only record primary road networks. The definition of how far land has to be from these roads to be classified as wilderness can also vary. Meanwhile, knitting all this data into a single map often leads to compromises that reduce its usefulness, such as not including any blocks of wilderness below a certain size. So while global maps are useful for drawing attention to the attrition of wilderness areas, only the greater detail of national and local maps can really help us understand and respond to the threats that face our remaining wild areas. Scotland is perhaps the country with the most detailed wilderness mapping in the world today. It has been mapped at global, continental, national, regional and local scales, each one showing progressively more detail, and higher levels of accuracy and reliability. The Scottish government has been able to use these maps to define what should count as protected “wild land” in the most effective way. Early maps showed most wilderness was in the uninhabited highlands and suggested there were almost no wild areas around the main cities of Glasgow and Edinburgh. But by zooming in and reducing the size threshold of what counted as wilderness, the government identified smaller areas of wild land nearer to cities that are just as important for recreation, and landscape, habitat and ecosystem conservation. China is following suit with a similar approach and using national level mapping to define wilderness areas and help develop a new national park system. The country can be neatly divided in two as highlighted by what’s known as the “Hu Line”, a simple straight line that connects Ai-hui in the north-east to Teng-Chong in the south-west. East of this line, the country is densely populated and intensively farmed. To the west, human population is sparse and the land remains largely wild. Chinese geographers are now developing methods to cope with this marked polarity in the distribution of the country’s wilderness. As with Scotland, they need to identify those smaller pockets of wild ecosystems that remain within the otherwise fragmented and developed landscapes of the east. One thing that wilderness maps are particularly good at illustrating is how wild land is being lost to the demand for food, fuel, water, timber and minerals as the human population increases. Maps show that this mainly happens through the road construction associated with logging, oil and gas and mineral extraction. Images of the ongoing fragmentation of the Amazon rainforest provide a good example of how roads, once constructed, open up the landscape for agriculture. Despite the problems of global wilderness maps, there have been some attempts to overcome the impact of cross-border assumptions and inconsistencies. The variations in wilderness quality have been consistently mapped across Europe as part of an EU project to develop a register of the EU’s remaining wilderness areas. One thing that this map highlights is just how common it is to find wilderness areas at more northern latitudes that are too cold and dry for agriculture or forestry and at high altitudes where the land is too rugged to work. So we shouldn’t be surprised to see a similar pattern on the global map.  The scale of these kind of maps affects both the patterns we see and how we understand wilderness destruction. This in turn influences how we might respond to and manage the threats to the world’s remaining wild areas. While global maps grab the headlines, they also risk masking the detail in the underlying causes and so have limited use. They may be great for highlighting the problem, but should only be a starting point to encourage us to look deeper and help us appreciate the underlying drivers of these lost wilds."
"In the past few years, there’s been a resurgence in the idea of foraging for food. The practice of hand gathering plants and animals for bait, money or the table has long taken place, but more recently top chefs have been popularising the idea, while urban foragers have told of the lengths they go to to find wild food in big cities.  But why, in an age where most things we want or need are only a few clicks away, do many seek the thrill of finding their own food? Why do local commercial gatherers choose to pursue these ancient livelihoods when there are less arduous alternative careers?  Humans by nature are hunter gatherers and have always collected food for sustenance. Over the centuries we have found many uses for different species collected both inland and along the seashore, including bait, medicines, fertilisers and soaps. Some research suggests that the Omega-3 fatty acids Homo sapiens gleaned from foraging shellfish on the seashore is what made us more “intelligent” than other human races.  There are clear benefits to wild harvesting in the modern world. Not only does it cut the forager’s own food costs but there may be health advantages too, not least from the exercise involved. Research has found that various activities, such as childhood beachcombing or practising mindfulness at the seaside, can add value to the coast as a “therapeutic landscape”.  However, there is still little published on the personal values of traditional activities such as foraging. A growing body of work on land-based cottage industries, such as wild blueberry and mushroom picking, has found that the wild product trade can empower communities by allowing them to develop new local industries that they can control. It has also been suggested that foraging should be considered an important ecosystem service, due to the cultural benefits that people gain freely from the land. But this research often does not consider the importance of foraging to individuals, and ignores the work of coastal gatherers who collect different things – seaweed, for example – which may be used for more than just food. While we are beginning to understand the benefits of people’s interactions with nature, one area that researchers are still looking in to is the meaning of the practices to those who forage. Despite numerous mentions in books and newspapers, there has been surprisingly little research into the non-monetary value of these gathering activities, particularly on the seashore. In countries where harvesting traditions are proudly practised, communities have begun to express the important meanings that lie behind their wild harvesting, in order to keep cultural practices alive and to protect their target species. The locals who forage on the Puget Sound in Washington, US, for example, have demonstrated that harvesting is key to the sense of place of some shellfish harvesters, and a sense of belonging for some plant and mushroom pickers. Carrying on these traditions brings together family heritage, personal experiences and social connections. And the very act of foraging deeply connects people to their environment, the traditions of the area, and the practices of their homelands too. In my previous work as a marine ecologist, I worked with bait collectors, cocklers, seaweed pickers, citizen scientists, natural historians, commercial fishermen and recreational anglers across the UK. Every person gathered something different, each passionate about what they did, and all with a story to tell. However, it seemed that the cultural and heritage reasons for foraging weren’t thought about until there was a threat of either wild stocks  declining or new management rules.   I am now looking at what gathering means to foragers in a modern world, and seeking participants to take part in the research. As other researchers have asked of other communities around the world, I am looking in to why people in the UK continue to forage from its seashores. Is there an unspoken, deep meaning to foraging that adds something to the gatherer’s well-being? Or is it simply something done for money or food? In a time when well-being and conservation policy is interested in both the sustainability of society and the environment the modern personal, familial and cultural values associated with the ancient activity of gathering should not be forgotten. We need to tell the stories of gatherers – be they commercial or recreational foragers, young or old."
"
Don sent me his AGU paper for publication and discussion here on WUWT, and I’m happy to oblige – Anthony
Abstracts of American Geophysical Union annual meeting, San Francisco  Dec., 2008
Solar Influence on Recurring Global, Decadal, Climate Cycles Recorded by Glacial Fluctuations, Ice Cores, Sea Surface Temperatures, and Historic Measurements Over the Past Millennium 
Easterbrook, Don J., Dept. of Geology, Western Washington University, Bellingham, WA 98225,
Global, cyclic, decadal, climate patterns can be traced over the past millennium in glacier fluctuations, oxygen isotope ratios in ice cores, sea surface temperatures, and historic observations.  The recurring climate cycles clearly show that natural climatic warming and cooling have occurred many times, long before increases in anthropogenic atmospheric CO2 levels.  The Medieval Warm Period and Little Ice Age are well known examples of such climate changes, but in addition, at least 23 periods of climatic warming and cooling have occurred in the past 500 years. Each period of warming or cooling lasted about 25-30 years (average 27 years).  Two cycles of global warming and two of global cooling have occurred during the past century, and the global cooling that has occurred since 1998 is exactly in phase with the long term pattern.  Global cooling occurred from 1880 to ~1915; global warming occurred from ~1915 to ~1945; global cooling occurred from ~1945-1977;, global warming occurred from 1977 to 1998; and global cooling has occurred since 1998.  All of these global climate changes show exceptionally good correlation with solar variation since the Little Ice Age 400 years ago.
The IPCC predicted global warming of 0.6° C (1° F) by 2011 and 1.2° C (2° F) by 2038, whereas Easterbrook (2001) predicted the beginning of global cooling by 2007 (± 3-5 yrs) and cooling of about 0.3-0.5° C until ~2035.  The predicted cooling seems to have already begun. Recent measurements of global temperatures suggest a gradual cooling trend since 1998 and 2007-2008 was a year of sharp global cooling. The cooling trend will likely continue as the sun enters a cycle of lower irradiance and the Pacific Ocean changed from its warm mode to its cool mode.
Comparisons of historic global climate warming and cooling, glacial fluctuations, changes in warm/cool mode of the Pacific Decadal Oscillation (PDO) and the Atlantic Multidecadal Oscillation (AMO), and sun spot activity over the past century show strong correlations and provide a solid data base for future climate change projections. The announcement by NASA that the Pacific Decadal Oscillation (PDO) had shifted to its cool phase is right on schedule as predicted by past climate and PDO changes (Easterbrook, 2001, 2006, 2007) and coincides with recent solar variations. The PDO typically lasts 25-30 years, virtually assuring several decades of global cooling.  The IPCC predictions of global temperatures 1° F warmer by 2011,  2° F warmer by 2038, and 10° F by 2100 stand little chance of being correct. “Global warming” (i.e., the warming since 1977) is over!

Figure 1.  Solar irradiance, global climate change, and glacial advances. Click to enlarge
The real question now is not trying to reduce atmospheric CO2 as a means of stopping global warming, but rather (1) how can we best prepare to cope with the 30 years of global cooling that is coming, (2) how cold will it get, and (3) how can we cope with the cooling during a time of exponential population increase?  In 1998 when I first predicted a 30-year cooling trend during the first part of this century, I used a very conservative estimate for the depth of cooling, i.e., the 30-years of global cooling that we experienced from ~1945 to 1977.  However, also likely are several other possibilities (1) the much deeper cooling that occurred during the 1880 to ~1915 cool period, (2) the still deeper cooling that took place from about 1790 to 1820 during the Dalton sunspot minimum, and (3) the drastic cooling that occurred from 1650 to 1700 during the Maunder sunspot minimum. Figure 2 shows an estimate of what each of these might look like on a projected global climate curve.  The top curve is based on the 1945-1977 cool period and the 1977-1998 warm period.  The curve beneath is based on the 1890-1915 cool period and 1915-1945 warm period.  The bottom curve is what we might expect from a Dalton or Maunder cool period.  Only time will tell where we’re headed, but any of the curves are plausible.  The sun’s recent behavior suggests we are likely heading for a deeper global cooling than the 1945-1977 cool period and ought to be looking ahead to cope with it.

Figure 2. Global temperature variation 1900 to 2008 with projections to 2100. Click to enlarge.
The good news is that global warming (i.e., the 1977-1998 warming) is over and atmospheric CO2 is not a vital issue. The bad news is that cold conditions kill more people than warm conditions, so we are in for bigger problems than we might have experienced if global warming had continued. Mortality data from 1979-2002 death certificate records show twice as many deaths directly from extreme cold than for deaths from extreme heat, 8 times as many deaths as those from floods, and 30 times as many as from hurricanes. The number of deaths indirectly related to cold is many times worse.
Depending on how cold the present 30-year cooling period gets, in addition to the higher death rates, we will have to contend with diminished growing seasons and increasing crop failures with food shortages in third world countries, increasing energy demands, changing environments, increasing medical costs from diseases (especially flu), increasing transportation costs and interruptions, and many other ramifications associated with colder climate. The degree to which we may be prepared to cope with these problems may be significantly affected by how much money we waste chasing the CO2 fantasy.
All of these problems will be exacerbated by the soaring human population.  The current world population of about 6 ½ billion people is projected to increase by almost 50% during the next 30 years of global cooling (Figure 2).  The problems associated with the global cooling would be bad enough at current population levels.  Think what they will be with the added demands from an additional three billion people, especially if we have uselessly spent trillions of dollars needlessly trying to reduce atmospheric CO2, leaving insufficient funds to cope with the real problems.

Figure 3. Global population.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a333224',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The UK’s environment agency has confirmed quagga mussels, a fresh or brackish water mollusc, have been found in a reservoir near London.  Quagga were previously identified as one of the single greatest threats to UK wildlife, yet also one predicted to be highly likely to invade UK waters.  Originally found in the Dnieper River in Ukraine, these thumbnail-sized mussels have since invaded freshwater systems across the world, particularly in North America. Quagga form a dense mat of shells on river, lake and reservoir floors, taking up space that might be otherwise used by native species. They are big zooplankton eaters, which has a knock-on effect on the abundance and structure of food available to native fish. Quagga arrive in the UK with a costly reputation. Together with their close relative the Zebra mussel, they are known for clogging pipes, reducing water quality and fouling boats.  In the Great Lakes, quagga now blanket the bottom of Lakes Michigan and Huron, depriving creatures further up the food chain of vital nutrients. Lake Huron, once a fishing paradise, saw its salmon population crash after quagga took over and began competing with the smaller fish that salmon feed on. Even the Hoover Dam on the Colorado River has had to temporarily shut its turbines in order to clear out the mussels. Experience teaches us it is highly unlikely we can eradicate these species. But we have also learnt that there are many things we can do to reduce their effects and spread. Much of the reason that eradication of quagga mussel is unlikely is due to scale. If we have found them in one place they are likely to be in others. Scientists dealing with the early invasion of the zebra mussel in the Great Lakes of the US and Canada confirmed that eradication was unlikely once they realised the new population had already spawned several times before detection.  However, the Great Lakes are a huge interconnected waterbody and Britain has nothing of comparable size; there’s no need to give up just yet. If the UK has an opportunity to eradicate this current mussel invasion then action must be immediate and drastic. Given the scale of the eco-economic burden the quagga will bring once established, it is entirely reasonable to think outside our normal set of tools and apply extreme methods. There are emerging direct methods to try to kill off invasive mussels. In North America poisonous fertiliser and a new bacterial product are being trialled this year against zebra mussels. But some indirect methods may also work. In Scandinavia acid rain, caused by air pollution, and changes in soil chemistry following uplift have caused some coastal lakes to turn acidic, wiping out native mussel populations. Acute decreases in water pH as a management action will cause massive short term change in the biology of the invaded lakes. Fish become less fertile, some species may become extinct, and the makeup of local invertebrates and zooplankton will change. However it is possible that in low-pH water the quagga (and all other molluscs) will become locally extinct due to the effect of the acid on their shells. It’s a long shot. A second, perhaps more palatable, option is to manage fish populations to maximise their negative effects on quagga. The normal way to harvest fish involves taking only the largest fish down to some minimum size. An alternative idea is to remove those medium sized fish which prevent young fish from thriving, increasing their numbers so they can compete with and consume quagga. These are all untested ideas, but are based on observations of natural ecological dynamics (that is, acidification leading to extinction and competition leading to population decline). We largely know what causes the spread of non-native invasive aquatic species once they have arrived at our ports in the ballast of shipping vessels. Larvae have been found to hitch rides between lakes and rivers in fishing equipment – waders and fishing reels being particularly vulnerable – on boats and other water sports equipment.  It is vital that the UK authorities provide advice and equipment for recreational lake and river users in the south of England to wash their equipment as soon as they have finished at each site, and certainly before they switch to a new site. The realists will state clearly, and probably correctly, that the quagga is here to stay. Or at least that if such extreme measures for eradication above did work then it would only be a temporary measure until another invasion occurs, or another population that is already here is discovered. But given the huge economic cost of a full-scale quagga invasion, it would be better to have tried and failed than not to have tried at all."
"

Current image from Terra Satellite, rotated 90 degrees to improve view, plus annotation and world view inset added by Anthony
Source image is available here at the NASA Terra website
North Pole to remain frozen
By            Bill Scanlon, Rocky Mountain News
Originally published 02:57 p.m., August 29, 2008
Updated 02:57 p.m., August 29, 2008
Santa can rest easy.
It’s looking like the ice at the North Pole won’t melt to water next month, as had been feared. It would have been the first time in thousands of years that the most northerly place on the planet would have been ice-free.
“It’s quite unlikely at this point,” Walt Meier a research scientist at the University of Colorado’s National Snow and Ice Data Center, said today.
The ice in the Arctic Ocean is at near historic lows, and breaks records every couple of years due to human-caused global warming, the scientists at NSIDC say.
This spring, it was looking like the ice might retreat so far that the North Pole itself would be ice-free for at least a day in September – the height of the ice-melt season.
The chances were great enough that the scientists at NSIDC were laying almost even odds on it in an office pool.
But while global warming is playing an important role, seasonal variability does, too. And this summer turned out to be a little cooler than last summer, when the record for ice retreat was set, Meier said.
“We only have about two or three weeks more of ice melt, and it’s not going to make it to the North Pole,” Meier said.
Read the rest of the article here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d2f17a8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe chances are real, and both are threatening the planet Earth. Apophis and Yellowstone have been appearing in the media lately. The chances of a catastrophic event occurring in your lifetime are higher than you may think.
Illustration of an asteroid impacting the earth. (Source Wikipedia)
Apophis
Russian astronomers are predicting that the asteroid Apophis could collide with the planet earth on April 13, 2036, writes the online Voice of Russia.
Apophis’s length was earlier estimated to be 450 metres, but a better estimate based on spectroscopic observations at NASA’s Infrared Telescope Facility in Hawaii puts it at 350 metres. That’s still a big rock to be hit by.
‘Apophis will approach Earth at a distance of 37,000 – 38,000 kilometers on April 13, 2029. Its likely collision with Earth may occur on April 13, 2036,’ Professor Leonid Sokolov of the St. Petersburg State University said.”
According to Wikipedia, NASA has estimated the energy that Apophis would release if it struck Earth as the equivalent of 510 megatons on TNT. By comparison the impacts of the Tunguska event is estimated to be in the 3–10 megaton range. The 1883 eruption of Krakatoa was the equivalent of roughly 200 megatons, and the Chicxulub impact, believed by many to be a significant factor in the extinction of the dinosaurs, has been estimated to have released about as much energy as 100 million megatons. 
The bad news is that an impact by Apophis would destroy an area of thousands of square kilometres, and seriously disrupt the climate for a few years. The good news is that it would be unlikely to have long-lasting global effects. Also the chances of Apophis actually striking the earth are still remote.
Yellowstone
The other potential natural catastrophe is the Yellowstone super-volcano, reports National Geographic here. Yellowstone’s caldera covers a 40 by 60 kilometer swath of Wyoming, is an ancient crater formed after the last big blast, some 640,000 years ago. The magnitude of an eruption estimated by scientists would be 1000 times more powerful than the 1980 Mount St. Helens eruption of 1980, and would lead to dire consequences for the globe.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




See: “When Yellowstone Explodes“ in National Geographic magazine. Here’s how the last one looked:
Scientists calculate that the pillar of ash from the Yellowstone explosion rose some 100,000 feet, leaving a layer of debris across the West all the way to the Gulf of Mexico. Pyroclastic flows—dense, lethal fogs of ash, rocks, and gas, superheated to 1,470 degrees Fahrenheit—rolled across the landscape in towering gray clouds. The clouds filled entire valleys with hundreds of feet of material so hot and heavy that it welded itself like asphalt across the once verdant landscape.”
The Yellowstone crater today is rising at a record speed, forced up by a huge magma reservoir that is thought to be about 10 km below the surface, see below. It has risen 25 centimeters since 2004. NatGeo writes that roughly 3,000 earthquakes occur in Yellowstone each year.
But between December 26, 2008, and January 8, 2009, there were about 900 earthquakes, and the rate of rise then slowed for a time. Scientists believe the earthquakes may help to release pressure on the magma reservoir below the surface by allowing fluids to escape, and thus relieve some pressure.
Yellowstone super volcano. (Source: Wikipedia)
Yellowstone erupted 3 times in the last 2 .1 million years. The last eruption was about 640,000 years ago. German online FOCUS magazine writes:
 The Yellowstone volcano is considered to be dangerous because on a geological timescale, it is due to erupt.”
Worrisome odds
Yellowstone is not the only super-volcano threatening the planet. A FOCUS map shows 6 others. Although the chances are small that any one in particular will erupt soon – maybe 1 in a 1000, the odds increase to worrisome levels when all the catastrophe possibilities get factored in. If one identifies 10 potential catastrophic events, each with the odds of occurrence being 1 in 1000, then it means the odds of one happening reduce to 1 in a 100. That starts to get worrisome. It means there’s a pretty good chance one catastrophe will occur in the next 100 years.
People who were born just recently have a pretty good chance of witnessing such an event in their lifetime. And the longer the planet goes without a catastrophe occurring, the greater the chances become.
Share this...FacebookTwitter "
"The social media conversation over the climate crisis is being reshaped by an army of automated Twitter bots, with a new analysis finding that a quarter of all tweets about climate on an average day are produced by bots, the Guardian can reveal. The stunning levels of Twitter bot activity on topics related to global heating and the climate crisis is distorting the online discourse to include far more climate science denialism than it would otherwise.  An analysis of millions of tweets from around the period when Donald Trump announced the US would withdraw from the Paris climate agreement found that bots tended to applaud the president for his actions and spread misinformation about the science. The study of Twitter bots and climate was undertaken by Brown University and has yet to be published. Bots are a type of software that can be directed to autonomously tweet, retweet, like or direct message on Twitter, under the guise of a human-fronted account. “These findings suggest a substantial impact of mechanized bots in amplifying denialist messages about climate change, including support for Trump’s withdrawal from the Paris agreement,” states the draft study, seen by the Guardian. On an average day during the period studied, 25% of all tweets about the climate crisis came from bots. This proportion was higher in certain topics – bots were responsible for 38% of tweets about “fake science” and 28% of all tweets about the petroleum giant Exxon. Conversely, tweets that could be categorized as online activism to support action on the climate crisis featured very few bots, at about 5% prevalence. The findings “suggest that bots are not just prevalent, but disproportionately so in topics that were supportive of Trump’s announcement or skeptical of climate science and action”, the analysis states. Thomas Marlow, a PhD candidate at Brown who led the study, said the research came about as he and his colleagues are “always kind of wondering why there’s persistent levels of denial about something that the science is more or less settled on”. The researchers examined 6.5m tweets posted in the days leading up to and the month after Trump announced the US exit from the Paris accords on 1 June 2017. The tweets were sorted into topic category, with an Indiana University tool called Botometer used to estimate the probability the user behind the tweet is a bot. Marlow said he was surprised that bots were responsible for a quarter of climate tweets on an average day. “I was like, ‘Wow that seems really high,’” he said.  The consistent drumbeat of bot activity around climate topics is highlighted by the day of Trump’s announcement, when a huge spike in general interest in the topic saw the bot proportion drop by about half to 13%. Tweets by suspected bots did increase from hundreds a day to more than 25,000 a day during the days around the announcement but it wasn’t enough to prevent a fall in proportional share. Trump has consistently spread misinformation about the climate crisis, most famously calling it “bullshit” and a “hoax”, although more recently the US president has said he accepts the science that the world is heating up. Nevertheless, his administration has dismantled any major policy aimed at cutting planet-warming gases, including car emissions standards and restrictions on coal-fired power plants. The Brown University study wasn’t able to identify any individuals or groups behind the battalion of Twitter bots, nor ascertain the level of influence they have had around the often fraught climate debate. However, a number of suspected bots that have consistently disparaged climate science and activists have large numbers of followers on Twitter. One that ranks highly on the Botometer score, @sh_irredeemable, wrote “Get lost Greta!” in December, in reference to the Swedish climate activist Greta Thunberg. This was followed by a tweet that doubted the world will reach a 9-billion population due to “#climatechange lunacy stopping progress”. The account has nearly 16,000 followers. Another suspected bot, @petefrt, has nearly 52,000 followers and has repeatedly rejected climate science. “Get real, CNN: ‘Climate Change’ dogma is religion, not science,” the account posted in August. Another tweet from November called for the Paris agreement to be ditched in order to “reject a future built by globalists and European eco-mandarins”.[* See footnote]. Twitter accounts spreading falsehoods about the climate crisis are also able to use the promoted tweets option available to those willing to pay for extra visibility. Twitter bans a number of things from its promoted tweets, including political content and tobacco advertising, but allows any sort of content, true or otherwise, on the climate crisis. A Twitter spokesperson disputed the accuracy of the Brown University research. “Non-peer reviewed research using our public API can often be deeply flawed,” the spokesperson said, adding that “sweeping assessments” of users based on signals such as location and tweet content are routinely made by outside groups. “To be clear - none of these indicators are sufficient to determine if something is a bot. Looking for accounts that look similar to those disclosed as part of our archives is an equally flawed approach, given many of the bad actors mimic legitimate accounts to appear credible. This approach also often wrongly captures legitimate voices who share a particular political viewpoint that one disagrees with.” Research on internet blogs published last year found that climate misinformation is often spread due to readers’ perception of how widely this opinion is shared by other readers. Stephan Lewandowsky, an academic at the University of Bristol who co-authored the research, said he was “not at all surprised” at the Brown University study due to his own interactions with climate-related messages on Twitter. “More often than not, they turn out to have all the fingerprints of bots,” he said. “The more denialist trolls are out there, the more likely people will think that there is a diversity of opinion and hence will weaken their support for climate science. “In terms of influence, I personally am convinced that they do make a difference, although this can be hard to quantify.” John Cook, an Australian cognitive scientist and co-author with Lewandowsky, said that bots are “dangerous and potentially influential”, with evidence showing that when people are exposed to facts and misinformation they are often left misled. “This is one of the most insidious and dangerous elements of misinformation spread by bots – not just that misinformation is convincing to people but that just the mere existence of misinformation in social networks can cause people to trust accurate information less or disengage from the facts,” Cook said. Although Twitter bots didn’t ramp up significantly around the Paris withdrawal announcement, some advocates of action to tackle the climate crisis are wary of a spike in activity around the US presidential election later this year. “Even though we don’t know who they are, or their exact motives, it seems self-evident that Trump thrives on the positive reinforcement he receives from these bots and their makers,” said Ed Maibach, an expert in climate communication at George Mason University. “It is terrifying to ponder the possibility that the Potus was cajoled by bots into committing an atrocity against humanity.” * This footnote was added on 11 March 2020. Following publication, the owner of Twitter account @petefrt contacted us to say that the account had been suspended because Twitter said the owner appeared to be managing multiple accounts to achieve artificial amplification. However, on appeal, Twitter has conditionally unsuspended the account. The owner says @petefrt is their only account and has asked Twitter to detail any other accounts associated with that account so they may be deactivated."
"

“This is the age of maximal surveillance,” says Bruce Schneier — the so‐​called “security guru” who spoke at Cato’s Second Annual Surveillance Conference in October. Surveillance is now ubiquitous and virtually unescapable for those who wish to enjoy the conveniences of modern life. And while the government downplays the importance of its access to citizen’s metadata, as Schneier observed, this data is about us — everything about us. “Metadata reveals who we are,” he said. “Google knows more about me than I know — because Google remembers better.”



Throughout the day‐​long conference, experts from around the country discussed the perils of national and global surveillance, as well as prospects for encryption and other tools to protect privacy in an ever‐​changing technological landscape. Sen. Patrick Leahy (D-VT), a longtime proponent of surveillance reform in Congress, lauded the passage of the USA Freedom Act, a reform bill first introduced at Cato’s surveillance conference in 2013 and signed into law in 2015. He particularly praised Cato’s role as a consistent champion for privacy rights. “I want to thank the Cato Institute,” he said. “You worked very hard on this — when we had people starting to back away, you helped give them courage.”



 **Immigration from 1965 to 2015**  
Fifty years ago, President Lyndon B. Johnson signed the Immigration Act of 1965 — a defining component of the American legal immigration system. Its passage meant that, after years of discriminatory Progressive Era immigration policies, immigrants from Western Europe no longer had legal preference over immigrants from places like Asia and Eastern and Southern Europe. At the same time, however, the Act introduced new limitations on immigrants from countries like Mexico and Canada. To commemorate the anniversary of this law, Cato hosted a conference, “Fifty Years after Reform: The Successes, Failures, and Lessons from the Immigration Act of 1965.”



Rep. Ruben Gallego (D-AZ), the son of immigrants from Colombia and Mexico, opened the morning by praising immigration as an engine of economic growth. Jim Gilmore, a 2016 Republican presidential candidate and former governor of Virginia, warned that deporting all illegal immigrants would require turning America into a “police state.” And Bill Richardson, the former governor of New Mexico, called for expanding visas for high‐​skilled workers. “While American businesses try to adapt and grow amidst economic and technological revolutions, our outdated immigration policies are holding us back from our full potential,” he said.



 **Preparing for the UN’s Climate Change Conference**  
Just before November’s highly anticipated gathering of world leaders in Paris for the United Nations Climate Change Conference, Cato hosted its own conference: “Preparing for Paris: What to Expect from the U.N.‘s 2015 Climate Change Conference.” Speakers discussed what is at stake in Paris, where leaders will attempt to negotiate a climate change agreement; the potential legal implications of such an agreement; as well as the latest scientific developments in climate science. “There is increasing evidence that the threat from global warming is overstated,” said Judith Curry of the Georgia Institute of Technology.



She denounced the “stifling” of more moderate positions on the effects of climate change, saying that anyone who diverges even slightly from the Intergovernmental Panel on Climate Change consensus is considered a “denier.” Richard Tol of the University of Sussex, whom Cato’s Pat Michaels called the “preeminent environmental economist in the world,” delivered the keynote address, in which he predicted that not much will happen in Paris. “For the last 20 or 25 years, governments have tried to reduce greenhouse gas emissions,” he said — to little success. “We should be dismayed,” he said, that so much money has been wasted on these efforts.



 **Rethinking Monetary Policy**  
This year’s 33rd Annual Monetary Conference, attended by over 200 people, was the first hosted by Cato’s new Center for Monetary and Financial Alternatives — a center dedicated to moving monetary and financial regulatory policies toward a more rules‐​based, free‐​market system. The conference featured distinguished speakers like St. Louis Fed president James Bullard, Richmond Fed president Jeffrey Lacker, and Stanford economist John B. Taylor. Bullard gave the opening address, in which he argued that a stable interest rate peg is “a realistic theoretical possibility.”



Claudio Borio of the Bank for International Settlements, whom _The Economist_ has called “one of the world’s most provocative and interesting monetary economists,” proposed challenging some of the “deeply‐​held beliefs” of monetary policy, including the idea that monetary policy is “neutral,” or that deflations are always disastrous. Rep. Bill Huizenga (R‐​Mich.), who chairs the House Financial Services Subcommittee on Monetary Policy and Trade, discussed his legislation requiring the Fed to adopt an explicit policy rule, among other reforms. “The Fed ultimately must be accountable to the people’s representatives, as well as to the hard‐​working taxpayers themselves,” he said.



 **Evaluating the TTIP**  
“It’s been quite a year for trade policy,” Cato’s Dan Ikenson remarked at the beginning of Cato’s conference, “Will the Transatlantic Trade and Investment Partnership Live Up to Its Promise?” “I think we are about to embark on a robust debate in the United States about the TTIP.” Cato’s conference helped prime for that debate, featuring leading trade experts who analyzed TTIP’s status and geopolitical implications. The day opened with a keynote address from Shawn Donnan of the Financial Times, who predicted that the deal may prove difficult to get through the Obama administration. Despite the fact that negotiations began in 2013, he said, “A lot of the conversations feel like they’re just getting started.” The conference, which was broadcast on C-SPAN, continued with discussions of what is at stake in the negotiations — from GMO regulations, to labor and environmental standards, to intellectual property issues.



Speakers including Michelle Egan, a professor at the American University’s School of International Service, and Swedish economist Fredrik Erixon, succinctly explained some of the complex issues under negotiation, like standards‐​related trade barriers and TTIP’s effect on global trade policy. The participants also wrote essays on crucial aspects of TTIP, all of which are available online at www​.cato​.org.
"
"To take a look at the cars on your street, it might not be apparent that the automotive industry is going through one of its most dramatic periods of change. Yet rising purchasing power in emerging markets, tightening emission regulations and advances in electronics are driving a major shift in how vehicles are powered, constructed and driven.   Nothing will supersede the internal combustion engine but vehicles will increasingly resemble computers integrated with home applications, their surrounding infrastructure and each other. The line between car-maker and tech company will become increasingly blurred. By 2020, what a car looks like will depend very much on where you live. The biggest story in the next five years however is not an issue of technology but economics: hundreds of millions of people in emerging markets will buy cars, often for the first time. In 2009 China overtook the US as the world’s largest car market, and by 2016 India and Brazil will displace Japan in fourth and fifth place respectively. This is an enormous opportunity for manufacturers in emerging markets to acquire economies of scale and launch themselves on the world stage. Within a few years we are likely to see these new brands spread into developed economies. The signs are already there. Companies such as the Indian manufacturer Tata, which bought Jaguar-Land Rover, or China’s Geely, which now owns Volvo, are well placed to use these high-status brands and dealership networks to move into mature markets. This won’t just mean different brands on the same old cars either. Car makers in emerging markets have to deal with price-sensitive first-time consumers, urban congestion, poor air quality and (in some cases) concerns over fuels costs. This creates a fertile space for innovation in smaller vehicles, where China is already dominant, and alternative fuels where emerging economies stand to gain a technological lead. Recent launches of models such as the long-range, low-cost electric Denza model in China and the super-low cost e2o in India highlight this trend. The fact air pollution affects all nations, rich or poor, means there is real momentum behind carbon emission regulations. Yet for all the hype about fuel cells and electric cars, the first priority for car makers will not be to gamble on anything radically new but to make the best of what they’ve got. In practice, that means making the engine smaller, reducing aerodynamic and rolling drag and above all, making everything lighter. The recently launched BMW i3 shows how seriously this is being taken. Using sports car-style carbon-fibre materials for a (nominally) mass-market vehicle reduces weight enough to enable a large battery pack. Although the i3 is expected to have modest sales, it is widely seen as firing the starting pistol for suppliers and venture capitalists to invest in these more exotic materials.  Lighter materials, at cheaper costs are the key to both extending the life of large conventionally fuelled vehicles and unlocking the potential of battery electric propulsion. Plug-in and pure electric vehicles have proven most popular in wealthy suburban environments such as California or the Netherlands where people can charge cars in their driveway overnight. The relative ratio of electricity to fuel costs also plays a part: in Norway cheap power and tough “polluter pays” policies have seen the adoption of electric vehicles soar, whereas sales disappoint in Germany with its more expensive electricity supply. Towards the end of the decade we’ll reach a tipping point. The cost of batteries is anticipated to halve and energy density to almost double, reducing costs and improving driving distances to the point the when EVs become a much more affordable option. At the other end of the spectrum, compressed natural gas is an option for long-distance travel. Long used in Asia and South America, CNG emits less CO2 than petrol engines and fewer harmful pollutants than diesel and can be filled either quickly or slowly depending on the purpose of the filling station. Hydrogen fuel cells represent perhaps the greatest hope (and gamble) for the industry. Like petrol engines they are quick to fuel and carry a lot of energy by volume which makes them suited for long-range driving – all this while emitting only water-vapour.  Yet despite tumbling costs fuel cell vehicles are currently the most expensive on the market and the fuelling infrastructure is costly to establish. Hyundai and Toyota have launched models targeted at the Japanese and California markets and other manufacturers are eagerly watching. If these cars achieve even modest sales, other brands are sure to invest strongly in the technology (if they are not doing so already). The launch of BMW’s DriveNow car-sharing scheme in London highlights another trend, the growth of the connected car and “pure mobility” services as an alternative to car ownership. In dense urban centres, smartphones have enabled car fleet operators to devise ever more sophisticated services. Uber’s cab services or car sharing schemes such as Daimler’s Car2Go or Autolib’ in Paris provide an alternative to costly car-ownership and limited on-street parking. For now, this is largely confined to large, rich cities and compared to public transport the impact is minimal. Yet continued smartphone penetration should see these models adopted in emerging markets too and especially in Russia. By 2020 we may see this process start to step up, as Nissan and Google vie to be the first to commercialise autonomous vehicles. Although likely to be relatively slow moving and still limited to big cities, autonomous vehicles would revolutionise the efficiency, safety and attractiveness of car sharing. The increasingly computerisation of vehicles also points towards a perhaps more significant, if unseen trend in vehicle development. Today, change across the vehicle fleet takes a decade or more, mediated by the development and production cycles of car-makers and the churn of the accumulated fleet. Yet integration with cloud-based services means that cars are no-longer a closed system, but capable of being remotely monitored and upgraded. Tesla has made a virtue of providing real-time vehicle upgrades to improve performance and India’s Mahindra & Mahindra has adopted the slogan of “fixing your problem, before you notice it”.  As automotive manufacturers start to converge with consumer electronics and cloud-based services development cycles are contracting with BMW speaking of four months from idea to production for new applications. This also poses the question of whether car-makers will even remain the chief players in auto-mobility. In the future might we prefer to buy our transport services from Google? The ground-work is being laid now and by 2020 we may start to see an answer."
"It has long been news that overfishing persists in many of the world’s oceans. Fish and invertebrate stocks have been over-exploited for our ever-hungry, growing human population, leaving some species in dangerous decline. The establishment of marine protected areas (MPAs) across the globe has been hailed as the silver bullet for conservation, with reports of increased catch, and spillover of recovered populations into adjacent fisheries, helping to replenish overfished stocks. 
But there may be unintended consequences if these areas are left unchecked. As populations of certain species are restored, disease can increase too.  Lundy Island, off the coast of Devon, was the UK’s first MPA. It was established as a marine nature reserve in 1986, incorporated a no take zone in 2003 and was designated a marine conservation zone in 2010.  Four years of monitoring from 2003 to 2007 saw a marked increase in commonly fished species, such as lobster, inside the no take zone when compared to fished areas. But in 2010, a study of Lundy called for a cost-benefits review of marine reserves, after it was found that shell disease in European lobsters may be increasing inside the protected area, supposedly caused by the high density of certain species.  We returned to Lundy the following year to monitor the populations of European lobster. When we compared a fished area to the eight-year-old, unfished, no take zone, we found more abundant, and larger lobsters inside the no take zone This phenomenon is a well known upshot of establishing MPAs and one of the reasons they are celebrated. Local fishermen agreed that since the no take zone was implemented, there has been an increase in catch around the area.  But in the same survey, we found that there was a higher probability of lobsters being injured inside the Lundy no take zone. Injury is thought to be induced by the European lobsters’ aggressive and solitary nature, so naturally in areas of high density such as the no take zone we expected to find a lot. Still, injury is known to be a precursor to disease. The shell of a lobster is its first line of defence and once breached, this may give rise to entry of pathogens.  This is crucial to understand because other studies have shown that pathogens in marine ecosystems are on the rise, a phenomenon which may be exacerbated by climate change.  In the past, disease in American lobsters is thought to have contributed to the collapse of a lobster fishery in southern Massachusetts. It is important to monitor disease and understand the effects on populations elsewhere in the world, especially those species which are commercially exploited. Our study is interesting in that it introduces the idea that un-fished populations in marine parks may eventually reach a threshold at which conditions become unhealthy. This may even introduce the possibility of controlled fishing in long-standing no take zones.  This may be a controversial move but studies have shown high abundance in marine reserves may render animals vulnerable to disease particularly because infections can no longer be “fished out”. A total ban on fishing is certainly positive in allowing recovery of populations back to unexploited densities, but they may have a finite time span of success. There is no doubt that fishery closures and marine protected areas do help contribute to the conservation of species, but the important message here is that we must monitor them closely. In November 2013, the UK designated 27 new MPA sites. Monitoring species richness, abundance and disease in these areas will be crucial to avoid any unwanted byproducts such as disease increase."
"
Share this...FacebookTwitterWe want to be fair, and so I’m obligated to inform readers that Professor Stefan Rahmstorf, has testily replied to EIKE’s report, which I wrote about just 2 days ago. Rahmstorf retorts at his website: Headlines From Absurdistan here.
First, recall that Rahmstorf is that alarmist scientist who projects an oddball sea level rise of 1.4 meters over the next 90 years, something that almost all scientists dismiss as nonsense (Rahmstorf may not be aware that sea level rise has indeed been slowing down; a few years ago the rate was 3.4 mm per year). Read more here.
Sea level rise is slowing. TOPEX U. of Colorado
Rahmstorf appears to be miffed that EIKE has labelled him as a lowly sceptic. Let us look at what he has written in reply.
1. First off, we’re all bought off. In his retort he first whines about all the money that big business is pouring into the sceptic machinery and to US denier politicians. (I haven’t seen a cent of it). Rahmstorf writes:
Also European companies don’t pinch their pennies when it comes to buying up candidates for the US Senate who deny anthropogenic climate change.
2. Next he points out that the focus of his 2003 paper was an analysis of  ice core data from Greenland with respect to the timing of climate changes during the ice age only (Dansgaard-Oeschger events).
3. He then claims that EIKE is confusing local with global temperature fluctuations. Rahmstorf writes:
The Greenland data are mainly characterised by fluctuations in the Atlantic currents (the Dansgaard-Oeschger events), which practically have no impact on the global temperature.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




4. In the Holocene in Greenland, like the northern latitudes overall, there was a cooling trend (it was warmer earlier), which was caused by regional solar radiation due to orbital cycles – not a global phenomena.
5. He counters the sceptic argument that past warmings show that today’s warming is natural using the following analogy:
It is as logical as saying that there cannot be forest arson today because there were completely natural forest fires in the past, too.
Of course there were warmer climates in the past than today (but as far as we know, not in the last 2 million years).
Today is the hottest it’s been in the last 2 million years! That’s quite the claim. Rahmstorf then poses the question:
Doesn’t the internet report [by EIKE] simply confirm the wisdom that the deniers of anthropogenic climate change no longer have any arguments – and so they have to resort to the most absurd possible twisting of the facts?
Finally, Rahmstorf takes one last shot at EIKE:
The source of all this nonsense, by the way, is the EIKE “climate sceptic” lobby group with the cute name of “European Institute for Climate and Energy”. For more information read Süddeutsche or the Spiegel.
Needless to say, Rahmstorf cites two lefty sources that don’t exactly say the nicest things about EIKE, calling it a hack organisation that’s headquartered in a mailbox. Funny though how Rahmstorf takes EIKE so seriously that he feels compelled to reply to the “mailbox operation” in less than 72 hours. Must be quite the mailbox.
Share this...FacebookTwitter "
"Even under the most conservative climate change scenarios, sea levels 30cm higher than at present seem all but certain on much of the UK’s coast by the end of this century. Depending on emission scenarios, sea levels one metre higher than at present by 2100 are also plausible.  The knee-jerk reaction to sea level rise has traditionally been to maintain the shoreline’s position at all cost, by building new flood defence structures or upgrading old ones. More than US$10 billion per year is already spent worldwide on “grey” infrastructure such as concrete walls and levies to protect against coastal flooding. Equally large are the costs incurred when coastal defences fail. The United Nations has called on governments to relocate public facilities and infrastructure from flood-prone areas, while the UK Climate Change Committee has urged the government to “set out how and when the hard choices that have to be made on the coast are going to happen”. The traditional approach of “grey” engineered sea defences locks society into ever increasing costs of replacement and maintenance. The alternatives are “nature-based solutions” to coastal flooding and erosion, which work with natural processes to reduce flood risk and incorporate ecosystems into flood defence. Rather than seeing the coast as a static line, these alternatives rethink the coastlines as zones with valuable habitats such as beaches, dunes and wetlands that act as carbon stores, places for recreation and natural buffers against the waves. Schemes such as the Wild Coast Project at Wallasea on the UK’s east coast have restored salt marshes where land had been reclaimed for agriculture years earlier. The tide and waves now regenerate salt marsh where it had been embanked and drained. If designed well, such schemes create new habitat which can reduce the height and intensity of storm surges and lower flood risk.  This technique for managing sea level rise can be thought of as allowing nature the space to create new coastal habitats within well-defined boundaries, akin to flooding a “sandpit”. In this sandpit, enough coastal space is vacated by humans to give natural processes room to respond to sea level rise by creating new wetlands further inland where once the terrain was dry. While such interventions do not enable full control over water levels and waves, they are designed to keep them at a safe distance from humans. Nature may be allowed to have some freedom to “play in the sandpit” created for it and people may not care what type of salt marsh or mudflat forms at Wallasea. But, as with “grey” infrastructure, humans ultimately build the sandpit by setting its boundaries.   The future of the world’s coastlines, however, is uncertain as the coast is inherently dynamic. Every wave and tide shapes the coast such that it determines how the next wave and tide can shape it. Though people may not notice it, the coast and the habitats which line it are never fixed and in fact change a great deal over a single human lifetime. People may create pockets of space for nature and think they are in control when in fact humans never were and it is doubtful they ever can be.  This is amply illustrated by the freshwater grazing marshes at Blakeney in Norfolk, on the UK’s east coast, where embankments were breached during the 2013 storm surge. A storm surge at sea forced salt water through an embankment into the Blakeney Freshes nature reserve, a unique freshwater wetland. Though unintended, salt water flooding of embanked areas like the Freshes can create a new ecosystem there and prevent flood waters from rising in adjacent areas, where people would have come to harm.  The Blakeney Freshes illustrate the importance of allowing sufficient space for nature to decide the boundaries of its “sandpit”. The more space given to it and the wider the buffer zone of coastal landforms, the lower the risk of flooding to areas that lie further inland.  As with weather forecasts, predicting how complex natural processes will interact at the coast is difficult – certainly over years and decades. It is time people stopped pretending that nature can be controlled at all, whether through “grey” or “green” engineering schemes. The best option is to watch and learn.  The means for monitoring exist, with ever-improving measurement technology, data transmission and high-resolution satellite imagery, such as the European Commission’s Copernicus programme. Responding to sea level rise may be as simple as allowing space for the flow of water and sediment during extreme events. Society can do so by relinquishing the need to control the process and restricting development near the coast, creating areas that nature can “claim” with whatever habitat it wishes to “build” there. As a salt marsh turns into a tidal flat, a freshwater field into a salty lagoon, there is the opportunity to stand back, watch and learn to better understand how and why those changes happen and how people can benefit from change rather than fight a losing battle to prevent it."
"The Antarctic has registered a temperature of more than 20C (68F) for the first time on record, prompting fears of climate instability in the world’s greatest repository of ice. The 20.75C logged by Brazilian scientists at Seymour Island on 9 February was almost a full degree higher than the previous record of 19.8C, taken on Signy Island in January 1982. It follows another recent temperature record: on 6 February an Argentinian research station at Esperanza measured 18.3C, which was the highest reading on the continental Antarctic peninsula. These records will need to be confirmed by the World Meteorological Organization, but they are consistent with a broader trend on the peninsula and nearby islands, which have warmed by almost 3C since the pre-industrial era – one of the fastest rates on the planet. Scientists, who collect the data from remote monitoring stations every three days, described the new record as “incredible and abnormal”. “We are seeing the warming trend in many of the sites we are monitoring, but we have never seen anything like this,” said Carlos Schaefer, who works on Terrantar, a Brazilian government project that monitors the impact of climate change on permafrost and biology at 23 sites in the Antarctic. Schaefer said the temperature of the peninsula, the South Shetland Islands and the James Ross archipelago, which Seymour is part of, has been erratic over the past 20 years. After cooling in the first decade of this century, it has warmed rapidly. Scientists on the Brazilian antarctic programme say this appears to be influenced by shifts in ocean currents and El Niño events: “We have climatic changes in the atmosphere, which is closely related to changes in permafrost and the ocean. The whole thing is very interrelated.” The impacts vary across Antarctica, which encompasses the land, islands and ocean south of 60 degrees latitude. This region stores about 70% of the world’s fresh water in the form of snow and ice. If it were all to melt, sea levels would rise by 50 to 60 metres, but that will take many generations. UN scientists predict oceans will be between 30cm and 110cm higher by the end of this century, depending on human efforts to reduce emissions and the sensitivity of ice sheets. While temperatures in eastern and central Antarctica are relatively stable, there are growing concerns about west Antarctica, where warming oceans are undermining the huge Thwaites and Pine Island glaciers. Until now, this has led to a relatively low amount of sea-level rise, but this could change rapidly if there is a sustained jump in temperature. The Antarctic peninsula – the long finger of land that stretches towards Argentina – is most dramatically affected. On a recent trip with Greenpeace, the Guardian saw glaciers that have retreated by more than 100 metres in Discovery Bay and large swathes of land on King George Island where the snow melted in little more than a week, leaving dark exposed rock. While some degree of melt occurs every summer, scientists said it had been more evident in recent years, with temperatures rising more quickly in winter. This is believed to be behind an alarming decline of more than 50% in chinstrap penguin colonies, which are dependent on sea ice. Schaefer said monitoring data from these areas could indicate what is in store for other parts of the region. “It is important to have sentinel areas like the South Shetlands and the Antarctic peninsula because they can anticipate the developments that will happen in the future, the near future,” he said."
"
Share this...FacebookTwitterYesterday I just happened to be listening to German NDR news radio (during my after-lunch snooze) and heard an interview with meteogroup’s Frank Abel, one of Germany’s better known meteorologists.Regrettably I can’t post the German interview audio due to copyright reasons. But you can get an mp3 audio clip e-mailed to you by calling Frau Renate Genz-Kreher, at the Hamburg studios, Tel.: (+49) 40/4156-2788. Just pick up the phone – I’m sure she speaks pretty good English
The NDR newsman starts the interview with a list of dramatic weather events that have occurred in northern Germany recently, then questions Abel if these events were something we’d have to get used to. Abel answered (paraphrased):

It’s really to early to say. Such events have happened in the past. A couple of months ago Austrian experts cautioned against premature conclusions on climate change and rainfall. It is quite certain that global temperatures are rising now but it is unsure how this impacts rainfall. We have to be careful not to spread too much certainty on the topic.
In a nutshell, all these reports claiming climate change leads to more frequent extremes are premature. There’s too much uncertainty out there.
The newsman then asks why northern Germany has had such lousy weather lately. Abel explains that it has to do with so-called 5B Lows. These are Lows that form in the Alps and move up to the Baltic Sea, which leads to warm moist air from southeast Europe colliding with much cooler air and thus results in heavy deluges. The same phenomenon happened in 2002, and in Poland in 1997 – all caused by so-called 5B Lows.
Of course the newsman doesn’t really care about that. He wants to here that it’s due to global warming. So he presses Abel.
Also for us here in North Germany it feels like it has rained harder than ever before. What has come down over the last few weeks is just unbelievable. Can you, as a weather expert, confirm this?
Abel agrees that it has rained a lot, and that August was one of the wettest months on the records, and then explains some of the geographic factors and goes into the current high water situation that people in some regions have to deal with right now. But he doesn’t deliver the goods. So the newsman persists:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




So is it the changes in climate that are behind these heavy rains?
Abel refuses to say yes.
The situation is that nobody is really certain, and you have to be very careful…
The newsman interrupts:
Yes, but what do you think?
Frank Abel:
I can’t say. If you look back at the past, you’ll certainly find years and phases where you had similar conditions and people said ‘this has never happened”‘ Man has a relatively short memory for these things. Many have forgotten how we had similarly wet summers at the end of the 1970s.
It really sucks when the interviewee doesn’t cooperate. Next time the newsman will know better and invite a more reliable expert – like Prof Mojib Latif, who we now know will say anything. Damn meteorologists!
Don’t get me wrong, I don’t know what Abel’s position is on global warming…I’d say he’s a warmist, but not an alarmist. You’d have to ask him yourself (privately, off the record). He aint no Joe Bastardi – pretty sure about that.
I live in northern Germany and what do I think of the current weather? It was one of the shortest summers I can remember my 20 years here. We had about 3 weeks of hot weather from end of June to middle of July, and that was it. After that it was cool and rainy. This September has been really cool, with very few days getting up over 20°C. It’s rained a lot – but that’s what it does in northern Germany. That’s nothing new.
Share this...FacebookTwitter "
"Over the past two years, 37 experts from around the world have battled to develop a diet that is both sustainable and healthy. They integrated existing knowledge on the impact of diet on diseases, including cardiovascular disease, diabetes and cancer, with the impact of current food production systems on the environment.   The result of this “EAT-Lancet Commission” is a seminal report, published on January 16, on Food in the Anthropocene. The diet they came up with is likely to split opinion, as it would fundamentally impact on many people’s daily lives. When comparing it to what is typically consumed in the UK, for instance, there are some dramatic changes recommended.  The one which has hit most of the headlines is a reduction in the consumption of red meat (beef, pork and lamb) from approximately 12% of total energy intake to a mere 1%. Poultry does a little better, but still falls by more than half, along with major reductions in the intake of fish, eggs and dairy produce. The potential impact diets in North America (the largest consumers of meat and dairy products) is even greater.   These animal products are to be replaced largely by nuts, pulses and legumes. While total intake of grains (wheat, rice and corn) remains largely unchanged, refined products are to be replaced by wholemeal varieties. With veganuary fresh in the mind, such changes may not appear too radical. However, while social media may have us believe that vegetarianism, or even veganism, are becoming the norm, still only about 12% of the UK population follow a meat-free diet.  While there is little doubt that eating less red meat is good for your health, it is also important to recognise that for the poorest in society, meat and dairy products represent affordable sources of both calories and a range of micronutrients. In parts of the world where malnutrition is still common, increasing the availability of such products would certainly have significant health benefits.   Across much of the world, people aspire to the meat and dairy-rich diets that dominate most of high-income countries. This is seen most acutely in China, where traditional diets, rich in rice and other plant materials, are being rapidly replaced with meat dishes. The country now has more than 400m pigs – around half the global population. Changing such attitudes, and ensuring the most vulnerable people continue to obtain appropriate nutrition, is at least as big a challenge as manipulating agricultural production across the globe. Animal production is often perceived as an unsustainable use of natural resources. Many animals are fed on food suitable for human consumption, demand a high proportion of the planet’s water resources and then produce methane, one of the most potent greenhouse gases. It is hard to argue that the highest income countries should not reduce their meat and dairy.  However, other animals (including cows, sheep and goats) graze pasture that is unfit to grow human-edible crops, turning grass and other plants into high value protein. If the UK, for example, were to abandon such livestock production, are its residents really ready to see radical changes in a rural environment associated with its “green and pleasant land”? Could the uplands of Scotland, Wales and Northern England really be turned towards the production of peanuts and soya beans? The EAT-Lancet report cannot be ignored. It starkly points out the catastrophic consequences for the planet of continuing with “business as usual”. Yet it is important that, as agriculture becomes more efficient and sustainable, we still consider the health and well-being of the most vulnerable in society. This includes not only the one billion people who remain in danger of malnutrition in areas such as Sub-Saharan Africa and South Asia, but also many of the poorest people within the large urban conglomerates of our wealthy countries.  If adopted, the report suggests that the dietary changes recommended could save more than 11m lives each year. While most would welcome a longer and healthier life, these represent more mouths to feed in an ageing population. As people get older, their nutritional needs change and about a third of people living in care homes in the UK suffer from some degree of malnutrition. Ironically, this is often associated with a lack of high-quality protein, such as is found in meat and dairy products.  This perhaps raises even more profound questions over what we wish to achieve for the future of humanity."
"
Now I’ve heard everything. Talk about your “Kyoto protocol”. The original source of this silliness comes from the city of Kyoto. In June, in a bid to reduce greenhouse gases and perhaps become a nationally  designated “model environmental city,” the municipal government indicated it would request convenience stores to “voluntarily refrain” from staying open all night.
No Slushee for you!
You can read the complete story here in Japan Today. The worst part about this is the complete lack of understanding about where the major energy use is. Closing the store may result in some energy savings from lighting, but the main power use, refrigeration systems, and that Slushee machine, will still operate.
No more midnight Slushee! Maybe the real reason is the “exploitation of the polar bear” on the cup.
Here is more, a response from the  Japan Franchise Association
Convenience stores defend  24-hour operations
September 27th, 2008 by Jame, Japanprobe.com

Facing attack from critics that want convenience stores to shut down at night  as a measure to prevent global warming, the Japan Franchise Association has  responded by stating that convenience stores play a crucial  role as safe havens for lost children and victims of crime:
More than 13,000 cases of women finding refuge in convenience stores across  the country were reported during fiscal 2007. Nearly half of them occurred after  11 p.m. and about 40 percent were due to stalkers and molesters, the association  said.
In addition, there were 6,000 cases of lost children requiring assistance and  12,000 cases of elderly people found wandering the streets alone.
The 12 companies that comprise the JFA operate around 42,000 convenience  stores.
Explaining the significance of convenience stores, a JFA official said they  provide a “substitute for ‘koban’ (police boxes) and streetlights in the middle  of the night.”
The National Police Agency says that koban and “hashutsujo” police branch  offices are located at about 13,000 places across the country, but that number  is down by around 1,000 from five years earlier.
In addition, the JFA has also stated that convenience stores with limited  nighttime hours would still have to keep on their refrigeration systems when  closed, so the reduction in greenhouse gas emissions would be  negligible.
From the Japan Times article:
Behind moves to limit 24-hour business is concern about the environmental impact of round-the-clock operations. “Definitely, 24-hour operations eat up electricity,” he said.
Although acknowledging that some people are active late at night, for example because of their jobs, Ando went on to claim “the vast majority have standard lifestyles and get up in the morning and come home from school or work and sleep at night.”
“With no time left to waste to combat global warming, we are very concerned about whether it is really good (that stores) stay lit up even past midnight,” Ando said.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c80c691',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Opinion studies indicate that many Americans are terrified of flying. But there’s good news from the Federal Aviation Administration for those nervous Nellies. Last year there were 14 million commercial airline flights carrying 615 million passengers. How many deaths? A big fat zero. The chances of dying by electrocuting yourself in the bathtub, being crushed to death by the airbag in your car or being hit on the head by a meteorite were about the same as by taking a ride on an airplane. 



America has become a nation of worrywarts. We worry about everything from global warming to the amount of fat in our breakfast cereal to the radiation emanating from our computers. A recent poll featured in USA Today confirms that we are concerned about the strangest things. More than half of Americans say that successful cloning is one of their biggest dreads for the 21st century. What is completely inexplicable is that 55 percent of Americans cited “technology” as one of their worries. How’s that for unfounded paranoia? Technology saves lives, it doesn’t cost them. 



Maybe our greatest risk in the modern age is that of worrying ourselves to death. The risks we now face in our daily lives are minuscule compared with the risks of earlier times. In fact, the world is a much safer place today than it was at any time in history. Many of the things we fret about today, such as flying on a commercial airline, are not dangerous at all. If you don’t believe me, consider the following statistics: 



  
  




  
  




  
  




What else do you worry about? War? Floods and tornadoes? AIDS? Nuclear accidents? Acts of terrorism? Sudden Infant Death Syndrome? Contaminated air and water? Heart attack? Believe it or not, the death rate from every single one of those menaces is down–in most cases way down. In the last 20 years modern medicine has made giant strides in treating and preventing heart disease. It remains a major killer only because, as life expectancies increase, we become more prone to this degenerative disease. 



Worrywarts have shifted their phobias to obscure and distant threats posed by things like global climate change, alien invasions, cloning and secondhand smoke. Most of the threats that were really serious and frightening earlier in this century are no longer problems at all. My parents often told gruesome tales of growing up in the 1930s when polio was still a dreaded killer. Everyone knew someone who had been paralyzed and confined to an iron lung or a wheelchair. The Centers for Disease Control tells us that there was not a single reported case of polio in the United States in 1997. 



One of the leading causes of death in America today is poverty. The best way to continue to reduce the risk of injury and early death is to promote economic growth and rising living standards through free‐​market capitalism. Paradoxically, many of the safety, health and environmental regulations that come out of the Environmental Protection Agency and the Occupational Safety and Health Administration do not reduce risk. They are so cost‐​ineffective that they make us poorer and thereby put people at greater risk of death than if we had never issued those regulations at all. Smart regulations save lives. Dumb regulations cost lives. 



Now, if after reading this you are still nervous about the sky falling and the world (or your own world) coming to an end, my advice is to board an airplane. These days, that’s probably the safest place you can be.
"
"There aren’t many animals high up in the Himalayas, but the odd bird passes by. Each year bar-headed geese (Anser indicus) travel from their breeding grounds in China and Mongolia to spend winter in the Indian subcontinent. It’s a journey that takes them through some of the tallest mountains in the world and there are even old reports of these geese flying over Annapurna and even once over Mount Everest. We knew these birds were well adapted to flying at lofty altitudes but until now we assumed they hit some chosen top altitude and flipped on cruise control. However, new research published in the journal Science by myself and an international team of scientists found otherwise. After tracking the geese we discovered they actually stick close to the mountains, following the peaks and valleys like a rollercoaster. It’s an unexpected approach to a tricky problem. At higher altitudes, air gradually becomes less dense and the atmospheric pressure drops. This means birds must flap their wings ever faster, deeper or both in order to generate the lift and thrust required to keep them flying fast through the air.  At the same time, the availability of the oxygen required to sustain these flights is reduced. At 5500m – just above Everest Base Camp – a lung full of air will contain around half the oxygen obtained at sea level. Herons, egrets and other birds that fly high above the ground, but not necessarily over mountains, often climb steadily and then level off at a specific altitude (typically between 2,000 and 7000m) when they find suitable tailwinds which help propel them along at high speeds.  Bar-headed geese could follow a similar strategy. After all, their lungs are 25% larger than other geese and their cardiac and flight muscles contain more blood vessels – all ideal adaptations for a low-oxygen environment. Their blood even contains slightly modified haemoglobin that gives it a higher affinity for oxygen than many other birds.  However, when we tracked these geese we found they seldom maintained a constant altitude at all. They might descend many hundreds of meters only to have to climb again later in the same flight. The birds mostly stayed within 100m or so of the undulating terrain and only flew as high as was necessary. Due to the many ups and downs recorded on longer flights, we termed this the “rollercoaster” strategy. To understand how and why bar-headed geese made these flights we developed a customised data logger which we implanted into their abdomens (after catching them in Mongolia) where it stayed all year. We put a coloured ring around their necks so that we could identify them the following year.  The logger would record flight altitudes, body temperature, heart rate and body motions (used to determine wingbeat frequency and overall accelerations). Both heart rate and wingbeats went up with increasing altitude and decreasing air density, indicating that at high altitudes even horizontal flight was hard work.  Using this data we calculated that, without a helpful tailwind, staying at high altitude would not be worth the extra flapping. For the geese the most efficient route is to track the underlying terrain and stay in reasonably dense air. This helps to reduce the energy used up flying while at the same time making more oxygen available.  Extra flapping uses a great deal of energy. Even just a 5% increase in flap frequency results in an average 19% increase in heart rate and a 40% increase in oxygen consumption. This is because, unusually, bar-headed geese increase the size of their flaps along with their frequency.  Given we know what the air pressure at any given altitude is likely to be, we can calculate the flaps necessary for the goose to stay airborne. However the increasingly extreme extra effort involved in eking out those extra flaps means even this species will eventually hit its ceiling. Our calculations say “peak goose” could be somewhere nearer 8,000m than 9,000m (perhaps depending on the weather), they simply don’t have the heart or the lungs to keep flapping beyond this point. So the Mount Everest (8848m) sightings might just about be plausible, perhaps with assistance from updrafts deflected from the valleys, but seem unlikely. I would love to hear from anyone with some good evidence of geese at these altitudes. I would certainly not expect them to stay there for very long! What is clear is that these birds have evolved to keep their flight costs to a minimum by regularly returning to lower altitudes whenever possible and only flying as high as the terrain dictates. Bar-headed geese are exceptionally well adapted to flying at high altitude when necessary, but they chose to make it the exception rather than the rule."
"Europe’s largest asset manager is backing a shareholder vote urging Barclays to stop offering loans to fossil fuel companies. Amundi, an influential investor with more than €1.65tn (£1.4tn) in assets under management, is the latest shareholder to throw its weight behind a resolution calling on Barclays to phase out services to energy companies that fail to align with Paris climate goals.  It comes amid rising concerns over Barclays’ role as Europe’s largest financier of fossil fuel companies. The resolution, spearheaded by campaign group ShareAction, was filed in January by a group of 11 pension and investment funds managing more than £130bn worth of assets. The Church of England has also thrown its weight behind the climate resolution, which is the first to be lodged against a UK bank. A spokesperson for Amundi, whose headquarters are in Paris, told the Guardian that the firm would vote in favour of the resolution at Barclays’ annual investor meeting in May. “According to its voting policy for 2020, Amundi reiterates its priority on the question of the energy transition and the decarbonisation of the economy. And as such, we are favourable to any resolutions from shareholders that ask issuers to be more transparent around environmental and social factors. Therefore, the resolution of ShareAction is perfectly in line with our policy,” the spokesperson said. While Amundi holds a minor stake in Barclays of about 0.02%, its decision to publicly support the resolution could put pressure on other asset managers to follow suit. Some of Barclays’ largest shareholders, including BlackRock, have refused to disclose whether they will vote in favour of the resolution. That is despite BlackRock’s chairman and chief executive, Larry Fink, having pledged last month to strengthen the asset management firm’s “commitment to sustainability and transparency in our investment stewardship activities”. Instead, BlackRock plans to disclose key votes after they are cast. Legal & General Investment Management, which holds a 2% stake in Barclays, has also declined to comment on its voting plans. ShareAction’s campaign manager, Jeanne Martin, welcomed Amundi’s support. “It’s further proof that the most powerful investors are dissatisfied with Barclays’ financing of extreme fossil fuels such as coal, tar sands and fracking, and failure to keep pace with its competitors,” she said. A recent study commissioned by campaign groups, including the Rainforest Action Network, singled out Barclays as the largest financier of fossil fuels in Europe, with lending and underwriting to carbon-intensive companies and projects totalling $85bn (£64bn) between 2015 and 2018. Barclays is expected to disclose its position on the resolution in March. A spokesperson said: “We continue to engage with ShareAction and other stakeholders on this issue and will make a further statement at the appropriate time.”"
"
Set Phasers on Stun
March 29th, 2009 by Roy W.  Spencer, Ph. D.


I’ve been receiving a steady stream of e-mails asking when our latest work on  feedbacks in the climate system will be published. Since I’ve been trying to fit  the material from three (previously rejected) papers into one unified paper, it  has taken a bit longer than expected…but we are now very close to submission.
We’ve tentatively decided to submit to Journal of Geophysical Research (JGR)  rather than any of the American Meteorological Society (AMS) journals. This is  because it appears that JGR editors are somewhat less concerned about a paper’s  scientific conclusions supporting the policy goals of the IPCC — regulating  greenhouse gas emissions. Indeed, JGR’s instructions to reviewers is to not  reject a paper simply because the reviewer does not agree with the paper’s  scientific conclusions. More on that later.
As those who have been following our work already know, our main conclusion  is that climate sensitivity has been grossly overestimated due to a mix up  between cause and effect when researchers have observed how global cloud cover  varies with temperature.
To use my favorite example, when researchers have observed that global cloud  cover decreases with warming, they have assumed that the warming caused the  cloud cover to dissipate. This would be a positive feedback since such a  response by clouds would let more sunlight in and enhance the  warming.
But what they have ignored is the possibility that causation is actually  working in the opposite direction: That the decrease in cloud cover caused the  warming…not the other way around. And as shown by Spencer and  Braswell (2008 J. Climate), this can mask the true existence of negative  feedback.
All 20 of the IPCC climate models now have positive cloud feedbacks, which  amplify the small about of warming from extra carbon dioxide in the atmosphere.  But if cloud feedbacks in the climate system are negative, then the climate  system does not particularly care how much you drive your SUV. This is an issue  of obvious importance to global warming research. Even the IPCC has admitted  that cloud feedbacks remain the largest source of uncertainty in predicting  global warming.
Significantly, our new work provides a method for identifying which direction  of causation is occurring (forcing or feedback), and for obtaining a more  accurate estimate of feedback in the presence of clouds forcing a temperature  change. The method involves a new way of analyzing graphs of time filtered  satellite observations of the Earth (or even of climate model  output).
Well…at least I thought it was new way of analyzing graphs. It turns  out that we have simply rediscovered a method used in other physical sciences:  phase space analysis.  This methodology was first introduced by Willard Gibbs in 1901.
We found that by connecting successively plotted points in graphs of how the  global average temperature varies over time versus how global average radiative  balance varies over time, one sees two different kinds of structures emerge:  linear striations, which are the result of feedback, and spirals which are the  result of internal radiative forcing by clouds.
But such a methodology is not new. To quote from Wikipedia on the subject of  ‘phase space’:
“Often this succession of plotted points is analogous to the system’s  state evolving over time. In the end, the phase diagram…can easily elucidate  qualities of the system that might not be obvious  otherwise.”
Using a simple climate model we show that these two features that show up in  the graphs are a direct result of the two directions of causation: temperature  causing clouds to change (revealed by ‘feedback stripes’), and clouds causing  temperature to change (revealed by ‘radiative forcing spirals’).
The fact that others have found phase space analysis to be a useful  methodology is a good thing. It should lend some credibility to our  interpretation. Phase space analysis is what has helped us better understand  chaos, along with its Lorenz attractor, strange attractor, etc.
And the fact that we find the exact same structures in the output of  the IPCC climate models means that the modelers can not claim our interpretation  has no physical basis.
And now we can also use some additional buzzwords in the new article…which  seems to help from the standpoint of reviewers thinking you know what you are  talking about. The new paper title is, “Phase Space Analysis of Forcing and  Feedback in Models and Satellite Observations of Climate  Variability”.
It just rolls of the tongue, doesn’t it?
I am confident the work will get published…eventually. But even if it didn’t,  our original  published paper on the issue has laid the groundwork…it would just take  awhile before the research community understands the implications of that  work.
What amazes me is the resistance there has been to ‘thinking out of the box’  when trying to estimate the sensitivity of the climate system. Especially when  it has been considered to be ‘thinking in the box’ by other sciences for over a  century now.
And it is truly unfortunate that the AMS, home of Lorenz’s first  published work on chaos in 1963, has decided that political correctness is  more important than the advancement of science.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e976ea2e7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterI’m wrong with my last post. There is some climate material in those Wikileaks documents. I started sifting through and found this one. Nothing earth-shattering, but you see the deals being done behind the scenes.
May 8, 2009
Here we see China was never going to accept any targets on CO2 emissions. But they did promise to bring “action items”!
http://cablegate.wikileaks.org/cable/2009/05/09BEIJING1247.html
Climate Change
————–
8. (C) UK DCM Wood said the UK Environment and Science
Minister had recently had talks with Chinese officials on
climate change. In the lead up to Copenhagen, China would
not agree to targets on emissions but was willing to be
constructive and would come to Copenhagen with a package of
action items related to nuclear power, renewable energy and
reforestation. Wood said his impression was that China could
be induced to do more on climate change. “
They knew long before that Copenhagen was going to fail. 


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




There are likely more about climate in there. It’ll take an army to go through it. There are some real doozies in their on other topics of Iran, gas and oil projects, etc. I’d say a gold mine of info for many corporations too. This is going to be worse than I thought.
Expect lots of “action items” in Cancun to save face.
==================================================================
Another Wikilieaks climate document found: Jan 29, 2008
Merkel pushing for aggressive measures. Here we see that Merkel is a flaming warmist.
http://cablegate.wikileaks.org/cable/2008/01/08BERLIN122.html
13. (C) Chancellor Merkel and the rest of Germany’s political
leadership remain serious about pursuing aggressive
international measures to meet the challenges of global
warming.  Merkel has made climate change a priority of her
Chancellorship and enjoys the overwhelming domestic support
on this.  Merkel’s support for mandatory, targeted global
limits on greenhouse gas (GHG) emissions and an international
cap-and-trade regime reflects a deep-seated belief that only
drastic, concerted efforts on the part of the international
community can slow — and ultimately reverse — the human
contribution to global warming.  If anything, Steinmeier
supports tougher standards.  While the Germans have been
willing to consider alternative solutions, such as new
technologies for clean coal and renewables, fundamental
differences in our approaches to the issue of climate change
remain, and could lead to more public disagreement in the
future.  For example, while Germany will send a delegation to
the January 30 Major Economies Meeting (MEM), the German
Government remains skeptical about the value that the Major
Economies Process (MEP) adds to the UNFCCC track. The Germans
are particularly concerned about the need to avoid
duplication of effort in the various other climate
change-related forums, including the UNFCCC and the G-8.
TIMKEN JR 
Share this...FacebookTwitter "
"
Guest post by Steve Goddard
There is a considerable amount of misinformation propagated about the greenhouse effect by people from both sides of the debate.  The basic concepts are straightforward, as explained here.
The greenhouse effect is real.  If there were no greenhouse gases in the atmosphere, earth would be a cold place.   Compare Mars versus Venus – Mars has minimal greenhouse gas molecules in its’ atmosphere due to low atmospheric pressure, and is cold.  By contrast, Venus has a lot of greenhouse gas molecules in its’ atmosphere, and is very hot.  Temperature increases as greenhouse gas concentration increases.  These are undisputed facts.
Heat is not “trapped” by greenhouse gases.  The earth’s heat balance is maintained, as required by the laws of thermodynamics.

outgoing radiation = incoming radiation – changes in oceanic heat content
The image below from AER Research explains the radiative balance.

http://www.aer.com/scienceResearch/rc/rc.html
About 30% of the incoming shortwave radiation (SW) is reflected by clouds and from the earth’s surface.  20% is absorbed by clouds and re-emitted back into space as longwave (LW) radiation.  The other 50% reaches the earth’s surface and warms us.  All of that 50% eventually makes it back out into space as LW radiation, through intermediate processes of convection, conduction or radiation.  As greenhouse gas concentration increases, the total number of collisions with GHG molecules increases.  This makes it more difficult for LW radiation to escape.  In order to maintain equilibrium, the temperature has to increase.  Higher temperatures mean higher energies, which in turn increase the frequency of emission events.  Thus the incoming/outgoing balance is maintained.
It has been known for a long time that even a short column of air contains enough CO2 to saturate LW absorption.  This has been misinterpreted by some skeptics to mean that adding more CO2 will not increase the temperature.  That is simply not true, as higher GHG densities force the temperature up.  There is no dispute about this in the scientific community. See the graph below:

Click for larger image
As Dr. Hansen has correctly argued, increases in atmospheric temperature cause the ocean to warm up.  Thus changes the oceanic heat content become the short term imbalance in the incoming/outgoing equilibrium equation, which is not shown in the AER diagram.
The image below shows GHG absorption by altitude and wavenumber.  As you can see, there is a strong absorption band of CO2 at 600/cm.  That is what makes CO2 an important greenhouse gas.

http://www.aer.com/scienceResearch/rc/m-proj/lbl_clrt_mls.html

The important greenhouse gases are: H2O, CO2, O3, N2O, CO and CH4.  The reason why the desert can get very cold at night is because of a lack of water vapor.  The same is true for Antarctica.  The extreme cold in Antarctica is due to high albedo and a lack of water vapor and clouds in the atmosphere, which results in almost all of the incoming radiation returning immediately to space.
An earth with no CO2 would be very cold.  The first few tens of PPM produce a strong warming effect, and increases after that are incremental.  It is widely agreed that a doubling of CO2 will increase atmospheric temperatures by about 1.2C, before feedbacks.  So the debate is not about the greenhouse effect, it is about the feedbacks.
Suppose that the amount of reflected SW from clouds increases from 20% to 21%?  That would cause a significant cooling effect.  Thus the ability of GCM models to model future temperatures is largely dependent on the ability to model future clouds.  Cloud modeling is acknowledged to be currently one of the weakest links in the GCMs.  Given the sensitivity to clouds, it is perhaps surprising that some high profile climate scientists are willing to claim that 6C+ temperature rises are established science.
So the bottom line is that the greenhouse effect is real.  Increasing CO2 will increase temperatures.  If you want to make a knowledgeable argument, learn about the feedbacks.  That is where the disagreement lies.
“Lisa, in this  house we obey the laws of thermodynamics“
– Homer Simpson


Addenddum:

The GHG/stoplight analogy

Suppose that you have to drop your child at school at 8:00 and have to be  at work at 8:30.  There are 10 stoplights between the school and the office.   Your electric car has a fixed maximum speed of 30MPH.  It takes exactly 30  minutes to drive there.

If the city adds another stoplight (analogous to more CO2) the only way you  can make it to work on time is to run traffic lights and/or get the city to make  the traffic lights more efficient at moving cars (analogous to higher  temperature.)  The radiative balance has to be maintained in the atmosphere, so  the outgoing radiation has a fixed amount of time to escape, regardless of how  many GHG molecules it encounters.   Otherwise, Homer and your boss will be very  angry at you for violating the laws of thermodynamics.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9826bf2a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterWhen it comes to renewable energy, you can call them “Jesus technologies”. These are technologies that went the way of the dinosaurs a long time ago due to their inefficiency and impracticality. But in order to serve a political purpose, they seem to keep getting resurrected every 30 years or so. I came up with “Jesus technology” from Bishop Hill’s  Caspar and the Jesus-Paper“, a paper that died often but kept coming back.
Sure in some cases these primitive technologies make sense, but for the wide-scale application in a power grid, they make little sense, cost the consumer dearly, and even put the energy supply at risk.
Klaus-Dieter Humpich of the Friedrich Naumann Foundation For Freedom has an excellent piece on renewable energy called The Dirty Secret of Wind and Solar. It’s in German, and so if you can read the language, it is worth taking the time to do so. What follows is a summary.
Humpich’s essay starts by reminding us that electrical energy is very difficult and enormously costly to store. Therefore, the wildly fluctuating supply of wind and solar energies requires having conventional back-up systems in place, ready to fire up or throttle down at a moment’s notice whenever the sun and wind intensities change. Humprich writes that solar and wind are referred to as “additive energy forms” in the energy business, and not as “alternatives”. Alternatives would suggest that they replace conventional fuels, which is not the case. They only add to conventional fuels, and hence they are called additive supplies.
Wind and solar are a nightmare to control
The problem with wind energy is that a wind generator’s output varies with the third power of the wind velocity, P = kV³. That means a wind generator produces only 1/8 of it’s rated energy if the wind speed is cut in half. So whenever the wind speed changes, the power grid must be compensated by conventional power plants that are on constant stand-by. On gusty days, as more wind parks get added to the grid, it becomes more and more of a nightmare to keep the grid stable. The result: you get a grid that behaves like a wild bronco. Humpich writes:
A power control engineer would say that these are real disturbances with steep gradients (e.g. changes in power output due to a wind gust through a wind park).”
The once easy-to-manage, steady, conventional-fuel power supply and corresponding consumption have since been intruded on by a third, highly unstable and unpredictable player.
Standby conventional power plants have low efficiencies
So when the wind suddenly dies down, reserve conventional plants have to jump in quickly, meaning they’ve got to be always on stand-by. These power plants thus rarely run at their peak efficiencies, and often at outputs well below their peak efficiency. The result? Little, if any, savings in fossil fuel consumption gets achieved. Now we know why the concept of wind being an alternative really isn’t so.
The energy that gets produced by a wind generator, is in part lost to reduced efficiencies by the standby conventional plants. All the investment and resources to install the massive system wind and solar park infrastructure has only lead to saving a fraction of what they originally were promised to save.
Humpich writes:
You always have to keep conventional power plants running alongside in order to keep the grid stable. That means it consumes fuel that does not even get used. Be it that the plant is running only at partial capacity – at a sub-optimal efficiency – or is “throttled”, which means the generated steam does not even get sent to the turbines to be converted into power but rather is simply sent back to the condenser.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Result: consumption of fuel – for nothing.
Mixed power grids and systems are less efficient
So why go through all the trouble if it isn’t worth it? It’s all in the bookkeeping. As long as the energy from renewable sources gets accounted as having replaced the equivalent in conventional energy, then it appears interesting and the business of CO2 emission certificates looks especially lucrative.
As Humprich explains, it’s a bit like a brochure for a new car claiming the car gets 45 mpg. But as we know, that number is only under certain ideal conditions. In reality, with all the stop-and-go driving in city traffic, etc., the car’s fuel efficiency turns out to be much less.
It’s the same concept with a grid that is powered only using steady conventional fuel. An efficiency close to that advertised by the “manufacturer'” can indeed be reached when operated near ideal conditions. But when you mix in wind and solar parks, the efficiency is spoiled – you’re in “city traffic”. It drops considerably.
So what exacly is the efficiency of a conventional power plant operating on a “mixed grid”? Does the generated wind and solar energy replace a corresponding amount of fossil fuel? The answer is of course “no”. To determine the exact amount, it is necessary to conduct comprehensive simulations or actual field measurements. Humprich provides an example. A combination natural gas fired/steam typically has an efficiency of 57%. But when it is used as a back up for wind and solar energy, it no longer operates under ideal conditions, and so the efficiency drops to a measly 36%.
Rotten in Denmark
Denmark is a country that has a large supply of renewable energy. And it is also long known that when storms rage over Denmark, its power grid has to be stabilized by conventional power plants in neighboring Germany and Sweden.
Today wind and solar energy are incresingly being stabilized by gas-fired steam power plants, and so  it means more business for the gas industry. Humprich writes:
Maybe that’s why the two leading propagandists for wind and solar energy today are representatives of gas. In the USA, in any case, the gas industry is the leading sponsor of the ‘climate industry’. But this is not reprehensible. If you wish to promote another product (natural gas) onto an established market (coal and nuclear), then a lot of arm-twisting is needed. In this respect, gas-guys like Schröder, Fischer and Co. become real vacuum cleaner salespeople, who happen to get get paid generaously for their sales pitches.”
Further Literature:
Kent Hawkins: Wind Integration Realities – Case Studies of the Netherlands and of Colorado, Texas, Master Resource.
C. le Pair & K. de Groot: The impact of wind generated electricity on fossil fuel consumption.
K. de Groot & C. le Pair: The hidden fuel costs of wind generated electricity.
Share this...FacebookTwitter "
"
I’m sure we’ll see other dead people talking about climate change soon. Jerry Garcia perhaps? Albert Einstein, John Lennon, why maybe even John Wayne could be utilized. I can see him now, “Listen up Pilgrim…climate change is gonna kill you unless you get off your sorry butt and do something about it!”.
We all know these long dead people had opinions about climate change, but they just never had a chance to express them before they died. Right?
Anything to “save the planet”, including putting words in dead people’s mouths. – Anthony
h/t to Jeff Alberts
Greenpeace Resurrects JFK for Global Warming Ad Campaign
Web video depicts dead president warning climate change ‘threatens our very existence,’ claims ‘technology and renewable energy offers the last remaining hope.’
By   Jeff Poor
Business & Media Institute
10/30/2008 1:31:03 PM
There’s something a little creepy about historical figures being brought back to life to promote climate change alarmism, but the over-the-top environmentalists at Greenpeace have no qualms with using it as a tactic.
A video posted on Greenpeace’s YouTube site portrays former President John F. Kennedy, who was assassinated Nov. 22, 1963, in Dallas, making a plea for environmental activism to save the planet from the perils of global warming.
“When man first walked upon the moon, it defined a generation,” Kennedy is depicted saying. “As this new millennium dawns, we face a greater challenge – climate change threatens our very existence. What further disasters will convince world leaders that the existing technology and renewable energy offers the last remaining hope for sustainable future?”
The ad is part of a Greenpeace campaign labeled “Energy [R]evolution” that sets greenhouse gas goals far in excess of the Kyoto treaty. The ad promotes the eradication of coal-fired plants, using more expensive unproven sources of energy, blames industrialized nations for the plight of poor nations, and calls for a radical overhaul of the European auto industry.
Watch the video:

Other aspects of the campaign include an energy policy based on “equity and fairness,” reduction of greenhouse gas emission by “up to 30 percent below 1990 levels,” “strict mandatory efficiency standards” for home and office appliances and the phasing out of nuclear power, which has been deemed as a greenhouse gas-free energy solution to the climate change issue by some.
The ad makes an emotional plea by linking climate change disaster scene after disaster scene, including a post-Hurricane Katrina shot from Reynoir St. in Biloxi, Miss, followed by shots of children, calling for an “energy revolution.”
“Hollow words and spineless resolution have failed,” the voice continued. “Now is the time for an energy revolution. Will we look into the eyes of our children and tell them that we had the opportunity but lacked the courage? Will we look into the eyes of our children and tell them that we had the technology, but lacked the vision? Or, will we look into the eyes of our children and tell them that we faced our challenge and that we fought – we fought for the energy revolution?”
Kennedy is considered by some historians to be a great orator. However, the choice to use his likeness in the ad is curious because, although he is credited for laying some the foundation of modern federal environmental policy, it was his Republican rival and successor, President Richard Nixon, who made the Environmental Protection Agency a reality in 1970.
This isn’t the first time environmental messages have been linked to historical images. Al Gore’s “We Can Solve It” campaign had one commercial spot that began with video from the D-Day invasion of Normandy and included clips from the moon landing and the civil rights movement. Time magazine doctored the famous Iwo Jima photograph by Joe Rosenthal of the Marines raising the American flag and replaced the flag with a tree to push the “war on global warming.”
The use of deceased celebrities in advertising has been considered controversial by some. A recent DirectTV ad starring Craig T. Nelson as the father in the 1982 movie “Poltergeist,” shows the daughter – played by Heather O’Rourke – reciting the movie’s memorable line, “Theeeyyy’re heeerrre” However, O’Rourke died in 1988 and some critics claimed that crossed the line.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b67d40d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Many people that have have an interest in the interaction between the Sun and Earth have been keeping a watchful eye on several metrics of solar activity recently. The most popular of course has been sunspot watching.
The sun has been particularly quiet in the last several months, so quiet in fact that Australia’s space weather agency recently revised their solar cycle 24 forecast, pushing the expected date for a ramping up of cycle 24 sunspots into the future by six months.
On August 31st, at 23:59 UTC, just a little over 24 hours from now, we are very likely to make a bit of history. It looks like we will have gone an entire calendar month without a sunspot. According to data from NOAA’s National Geophysical Data Center, the last time that happened was in June of 1913. May of 1913 was also spotless.
With the current space weather activity level of the Sun being near zero, and the SOHO holographic imaging of the far side of the sun showing no developing spots that would come around the edge in the next 24 hours, it seems a safe bet to conclude that August 2008 will be the first spotless month since June 1913.
Here is the sun today,  at 09:14UTC August 30th:

Click for a very large image
Some people who watch the sun regularly might argue that August wasn’t really spotless, because on August 21st, a very tiny plage area looked like it was going to become a countable sunspot. Here is an amateur astronomer’s photo of the event:

August 21st, 2008 spots – Photo: Pavol Rapavy
But according to solar physicist Leif Svalgaard, who regularly frequents this blog:
According to NOAA it was not assigned a number on Aug.21st nor on Aug.22.
So without an official recognition or a number assigned, it should not be counted in August as actual sunspot.
It has also been over a month since a countable sunspot has been observed, the last one being on July 18th. Since then, activity has been flat. Below is a graph of several solar metrics from the amateur radio propagation website dxlc.com for the past two months:

Click image for original source
They have a table of metrics that include sunspots, and their data also points to a spotless August 2008. See it here: http://www.dxlc.com/solar/indices.html
So unless something dramatic happens on the sun in the next 24 hours, it seems a safe bet that August 2008 will be a spotless month.
Update: As commenter Jim Powell points out,
There was a stretch of 42 spotless days from 9/13/1996 to 10/24/1996. Today we have equaled this period. Check out Jan Janssens spotless days page http://users.telenet.be/j.janssens/Spotless/Spotless.html.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9cd73fd2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

My youngest daughter, Abby, is graduating from college in a month. So like most university seniors, she’s been in the throes of a job search. After an interview in Boston two Fridays ago, she was sitting at a gate in Logan Airport waiting for a flight to D.C. (She still likes to visit her parents!) When the aircraft arrives, who should alight from it but Elizabeth Warren. As she walks through the gate area, the waiting passengers proceed to stand up and give her an ovation, calling out, “Thank you, Senator!” and “We love you, Senator!” Oh, boy. I’m proud to say Abby’s voice was dripping with disdain when she told me this story. And, no, she didn’t join in the ovation.



How I wish people — of all political and ideological persuasions — wouldn’t have such misplaced faith in politicians. One of the great lessons I learned from Cato, in my many years as a donor, is that it’s the power and advancement of ideas that will create positive change in our world and build a free and prosperous society. The outcome of elections and the machinations of politicians alone won’t do it. And so much of our politics is partisan tribalism: both Democrats and Republicans support elected officials of their respective party even when they abdicate on issues that would appear core to that party.



I have plenty of Republican friends who defended George Bush for years despite the out‐​of‐​control spending and growth of government under his administration, including initiatives that would have left them outraged had a Democratic president been responsible. Steel tariffs, No Child Left Behind, Medicare Part D, and TARP are just a few examples.



And where are the angry Democrats protesting the sorry civil liberties record of the Obama administration? PATRIOT Act abuses such as national security letters and warrantless eavesdropping or data collection got them exercised when Bush was president — today, their silence is deafening. And partisans on each side seem to believe in “executive power for me, but not for thee.…” There have been frightening grabs of presidential power under each of the last two administrations. This has elicited complaints from both sides of the aisle: but, with few exceptions, from the left only of Bush, and from the right only of Obama.



As believers in markets, we know people respond to incentives. If partisan voters don’t insist that the people whom they elect adhere to principle, why would they? As a result, no matter which party holds power, the results are similar: too much spending, too much regulation, an unbridled Federal Reserve, a bias toward military intervention, and too little respect for civil liberties. In fact, a friend of Cato’s once shared a brilliant analogy. During campaigns, we hear from the marketing departments of each party, and they sound very different. But with a handful of key exceptions we get similar results when they’re in power: he speculated that they must each outsource to the same place when governing!



And let’s not have too much faith that simply pitching bad leaders overboard will change things dramatically for the better. Last summer, we were paid a visit here at Cato by Kim Kataguiri, an impressive and courageous 20‐​year‐​old Brazilian who has catalyzed the protests against President Dilma Rousseff and the drive for her impeachment. It will be wonderful to see a corrupt and ineffective leader get the comeuppance she deserves. But the vice president faces corruption charges, too, so his ascension wouldn’t likely change much. Rather, it will take a change of values to transform Brazil and allow it to reach its potential.



Politics is ultimately a necessary ingredient for the world to move in the direction we want. But a country and a world steeped in liberty can’t be accomplished politically without changing the terms of the debate and the climate of ideas: precisely Cato’s role. Scott Rasmussen once spoke at a Cato event, and contended that politicians only follow — and don’t lead — the rest of the country. It is the very contempt in which citizens hold the political class that made him optimistic about the future despite the current policy environment. It’s our job to continue making the compelling case for freedom through the media, in the academy, and to the policy community. Our objective is to lead policymakers in the direction of liberty. Only when they get there will they deserve ovations.
"
"
Share this...FacebookTwitterPia Heinemann reports in Die Welt today, Ocean Acidification Does Not Lead To Species Die-Off, on a new study appearing in the latest edition of Science. The study contradicts the assumption that ocean acidification leads to species die-off, surprising scientists.Abstract in Science here
Manmade emissions of CO2 are thought to be partly absorbed by the oceans, which in turn would acidify and pose a huge threat to calcareous organisms like corals and plankton. This is the horror story that has been widely circulating in the media for the last couple of years, and with ever-growing alarmism, at a time the dangers of global warming are turning out to be wildly exaggerated.
Italian and Swiss scientists have found answers by looking at 120 million year old sediment deposits. The team directed by Elisabetti Erba of the University of Milan describes new findings in the latest issue of Science.
It is not unusual for CO2 concentrations in the atmosphere to surge after large volcanic eruptions. This has happened often in the past.
They examined microscopic fossils and nannoplankton from a time period just after large volcanic eruptions 120 million years ago, when the air’s CO2 content rose to about twice today’s level. Their studies contradicted their expectations. Die Welt writes:
Contrary to what was expected, no large-scale die-offs occurred among the organisms when acidification increased. The species simply adapted: They formed smaller shells and remained small.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




They endured the changes far better than first thought.
Heinemann writes that the study also delivered yet another surprise:
Apparently, the oceans acidify with a delay. After the volcanoes erupted and the surface water pH value began to sink, it took 25,000 to 30,000 years longer for the CO2 effect to reach the sea bottom.
These new findings deal a massive blow to those hoping to exploit ocean acidification as the next disaster scenario to replace the discredited catastrophic AGW story. Expect the MSM to bury or spin the story.
Update/Note: Keep in mind that the plankton and coral studied were from 120 million years ago, meaning the species has since survived climate extremes and changes that were off the charts when compared to today’s mild natural changes. They’ve handled much colder and much warmer conditions with widely varying ocean chemistry.
Update 2: The Alfred Wegener Institute in Germany [Read here] is planning years of research on acidification, costing millions of euros, to study a bogus non-problem. They’ve teamed up with neutral Greenpeace, and so you can be sure they’ll come up with “catastrophic” findings and demand more money for reasearch. Whatever it takes to bilk the taxpayer out of money.
Share this...FacebookTwitter "
"

The British government’s decision to begin extradition hearings against former Chilean dictator Gen. Augusto Pinochet has triggered a heated right‐​left quarrel in the United States and around the world. 



Pinochet’s conservative defenders argue that his military regime, although brutal at times, saved Chile from the scourge of communism, and that the retired, 83‐​year‐​old general should be allowed to return to his home in Santiago. 



Pinochet’s liberal detractors, on the other hand, compare him with Adolf Hitler and argue that he should be extradited to Spain and prosecuted for genocide and terrorism. Liberals also see the Pinochet case as an opportunity to move toward a universal system of criminal justice. 



Lost in the right‐​left controversy over Pinochet’s record is a more important issue. The Pinochet affair teaches us ominous lessons about what we can expect from the planned International Criminal Court (ICC), the global criminal justice organ approved last summer in Rome by 120 nations.  




The Pinochet affair is … an alarming harbinger of the ICC [and] … portends a very hazardous future. 



The first lesson of the Pinochet affair is that the ICC’s prosecutions will likely be ideologically based. Indeed, does anyone seriously believe that Mikhail Gorbachev will be arrested next time he travels to Western Europe and tried for the bloody war in Afghanistan or for the KGB’s cruel excesses during his six‐​year tenure? Was anyone startled by Italy’s decision not to extradite Abdullah Ocalan, the recently captured Marxist leader of the Kurdish Workers’ Party, to Turkey, where he is wanted as a terrorist? Or the German government’s decision not to demand Ocalan’s extradition to Germany where a number of his alleged murders took place? Last, was anyone surprised by the Spanish authorities’ decision to go after Pinochet, but at the very same moment, allow Cuban dictator Fidel Castro rome around Spain freely? 



The second lesson the Pinochet affair teaches us about the ICC is that its activities could threaten the fragile peace of transitional democracies. Chile and other new democracies around the world have withstood recent political and economic strains, but the arrest of Pinochet has reopened old wounds and many Chileans believe that terrorist violence could erupt if he is not allowed to return home. “I have already been threatened by ultra‐​left‐​wing terrorists,” says Sen. Hernan Larrain of the rightist Independent Democratic Union Party. His worst fear is that a British decision to go ahead with extradition will polarize the Chilean population, creating a climate for violence. Many people on the left also fear that terrorist violence could be aimed against them, since armed right‐​wing groups and former secret policemen are reportedly reorganizing. There is also widespread fear that the Chilean military will reassert control if the political situation deteriorates markedly. 



In addition to unsettling peace around the world, the ICC’s activities could make achieving peace in the first place more difficult. Consider the case of Palestine Liberation Organization leader Yasser Arafat, who was in the United States negotiating the Wye River Agreement the same week Pinochet was arrested in Britain. Would Arafat have negotiated at all if he thought the American government might arrest him when he arrived and try him for acts of PLO terrorism? Or would the recent Irish Peace Settlement have been negotiated if the Catholic or Protestant leaders faced criminal prosecution? 



The third lesson the Pinochet affair teaches us about the ICC is that its authority may be interpreted in expansive ways. For example, although Pinochet’s secret police are implicated in the deaths or disappearances of 3,000–4,000 people over 17 years (an average of about 250 people a year) he is being charged with the crime of genocide, defined by the 1948 Genocide Convention as systematic killing with “intent to destroy, in whole or in part, a national, ethnical, racial, or religious group.” Proponents of the genocide charge against Pinochet maintain that the elimination of political opponents through assassinations and imprisonment constitutes a kind of “ideological genocide.” So much for original intent. 



The Pinochet affair is thus an alarming harbinger of the ICC. Specifically, the planned court stands to produce an arbitrary and highly politicized “justice” and open a Pandora’s box of political folly and malleable laws. The Pinochet affair, in short, portends a very hazardous future.
"
"

Junk science from Harvard, purveyed by a senior official from the United Nations and printed on the front page of The New York Times. Who would have thought such a thing possible? 



James J. McCarthy, a Harvard oceanographer, recently took a summer tourist cruise to the North Pole. When his ship, a Russian icebreaker, followed open water ever northward (as icebreakers do), it eventually wound up in a few‐​square‐​mile patch of water at 90 degrees North.



McCarthy didn’t publish this in the peer‐​reviewed literature. It would not have been accepted, because it is not at all unusual. Instead, he called The New York Times, which dutifully ran a front‐​page story on Aug. 19 by John Noble Wilford, whose first words were, “The North Pole is Melting.” The Times went on to note that “the last time scientists can be certain that the pole was awash in water was more than 50 million years ago.”



This is all nonsense, and especially troubling because McCarthy is co‐​chair of the United Nations’ Intergovernmental Panel on Climate Change (IPCC) section on “adaptation and impacts” of climate change. The Times report is not science; we scientists don’t take a single observation of anything and then draw large systematic conclusions. The famous Supreme Court decision on junk science, Daubert v. Merrell Dow, identified this type of statistical irrelevancy as trash. 



Why didn’t McCarthy get online and check IPCC’s own temperature histories for the region? They are probably no more than four mouse clicks away. Had he done so, he would have found that summer North Pole temperatures are no different than they were for several decades in the early 20th century, long before dreaded economic growth could have warmed the region. Further, based on fossil evidence, most climatologists think the period from 4,000 to 7,000 years ago averaged at least 2 C warmer than the current era at high latitude. That’s three millennia. It seems pretty obvious that the summer polar ice back then would have been considerably more abraded than it is today, and maybe even gone completely in certain years. The climatic consequences? No one can find them, except that the period was noteworthy for the flowering of agriculture and the rise of civilization. 



McCarthy could have also read the latest issue of the scholarly journal Climatic Change, in which University of Colorado climatologist Mark Serreze writes that not only has there been no net change in summer North Pole temperatures over the last 70 years, there has been no change in annual North Pole temperatures.



Times reporter John Noble Williams could have done this too. Instead, after receiving a firestorm of criticism for the irresponsibility of his first report, he tried to back down in a modified, limited hang‐​out sort way. His August 29 Times article notes that McCarthy now “would not argue with critics who said that open water at the pole was not unprecedented.”



Instead, he went to the Serreze article and disgorged another distortion: “The data scientists are now studying reveal evidence that on average Arctic temperatures in the winter have risen 11 degrees over the past 30 years.”



This is more junk. Both Serreze and the IPCC winter data show that a) 30 winters ago, in 1969, temperatures were around their lowest for the entire 20th century, and b) the net rise since then is 1.5 C. In terms of the departure from normal, they are currently running around 0.7 C above the standard reference mean, an inconsequential number. Wilford picked a small region of the Arctic where temperatures rose a great deal and said that this area applied “on average” to the entire Arctic. But that warm spot is balanced by many areas in which temperatures have fallen, which is how we achieve the unremarkable average change that has been observed.



These things are not difficult to check. But it’s easier to unquestioningly print the pronouncements of a Harvard professor bearing U.N. authority, even if he’s speaking from a cruise ship with a sample size of one. If I were McCarthy’s dean at Harvard, I’d be livid. And if I were Robert Watson, head of the United Nations’ climate panel, I’d find a new co‐​chair for the section on “adaptation and impacts” before the current one completely destroys the panel’s credibility.
"
"
Share this...FacebookTwitterThis is how ZDF German Public television envisioned how life in the year 2000 could appear back in 1972. Hat-tip Michael Miersch here.
Part 1 (Youtube video): Living and going to work (
The first part shows a man named Mr B living in a futuristic, electronic apartment. In the year 2000 people have to work only 25 hours per week, and can live with artificial hearts. Breakfast is prepared automatically and that all food is free of poisons because it is organically produced, free of pesticides and chemicals.
There are no printed newspapers – people get their news from a “printer” twice a day. Everyone lives in huge high rise apartment buildings with 2000 units, each equipped with international satellite TV that can be watched 24 hours per day. Shopping is done by radio-teleshopping; purchases are booked direct from his bank account.
Mr B communicates with friends using a “TV telephones” (smoking is still politically correct).
People don’t visit each other anymore – they converse via TV screen. Sociologists warn of the isolation of man by technology.
Mr B. does not use a conveying sidewalk to go the short distance to the train station when he goes to work, he walks. The air is now clean again because pollution was banned in 1990. Some even called for the death penalty for polluters. He works in another city 80 km away. No problem though, the jet-engine powered 500 km/hr commuter monorail train needs only 15 minutes. At the station he rents an electric city-car, which are readily available at all transportation hubs.
Part 2: At work in the year 2000, and the environmental hell of 1972.
The electric cars are automatically navigated. At work a massive network of people-conveyors take him through the huge maze of buildings. He works at a databank center that sells data to customers. All data is stored at a massive data storage centers and systems. For example, the databanks deliver critical data to politicians almost instantly so that they can always make the right decisions.
Everything is automated, and so Mr B has lots of time on his hands at work – no stress. That’s the way it is for millions of highly skilled workers like him, who only need to sit around and monitor the automated systems. As he sits around, he thinks about what he’ll do when he retires at the age of 50.
Retirement in the year 2000 is a problem too, as people have yet to figure out how spend all that free time on their hands. This is how ZDF imagined life could be in the year 2000 back in 1972.
At the 6 min mark, the show returns back to the reality of 1972. Here ZDF bemoans that 14 million cars jam the streets of West Germany. They pollute the air and threaten to choke the citizens in a sea of metal, exhust and noise. Cars are a symbol of freedom, but in reality they condemn the people to being stuck in traffic jams. The car – it kills 17,000 and injures 500,000 every year. Millions of tons of sulfur dioxide, carbon monoxide and lead are blown yearly into the atmosphere.
Air pollution in urban areas is already at the allowable limits. The SST pollutes the entire stratosphere and creates extreme noise – all to save 3 hrs of flight time. ZDF says we’ve poisoned the biosphere and food chain with our pesticides. Industry has polluted the water so much that clean water will be a luxury product in just a few years. Germany’s Lake Constanz will soon look like Lake Erie. It will take 50 to 100 years before Lake Erie returns to a natural condition.
Part 3: Man is destroying the planet – we have precious little time to save it


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Part 3 looks at life and the environment as it was in 1972. The film starts with:
Industrialization brought prosperity to many, but a threat to all. Industrialization favors the concentration of capital and assets in the hands of a few.
The clip then describes the growing gap between the haves and have-nots. Up to 800,000people (from 60 million) live in poverty. The film says that by1980, half a million students will jam into the universities, where only half will be able find housing, and classroom will be overfilled, leading to riots and civil unrest.
ZDF complained that the field of medicine is outdated, and Germany is falling behind. 30% of all hospital beds were made in the 1920s.
ZDF then asks a series of pessimistic questions. Will the political system be able to quickly enough make the decisions necessary to bring the country forward to meet the challenges of the year 2000?  How will the family survive? How will the workplace change? Automation threatens to turn workers into mere monitors.
For millions there isn’t going to be any work.”
Another problem is that the economy produces more food then what is needed. Europe destroys the surplus of food while people starve in other places on the globe. ZDF then juxtaposes this with military spending, which amounts to 600 billion German Marks annually. “For the first time in history, man is capable of destroying the planet.
The only choice is either we live together, or die together.”
ZDF then complains that technology is advancing too quickly; we can’t keep up and that we don’t really know what we’re getting ourselves into. Since WWII, millions of tons of concrete have been transformed into living units. Now everywhere the landscape is littered with high rise apartment buildings. No one thought about the impact on man and society. No research is being done to see where all this is taking us. The ZDF clip ends with a quote:
‘The capability of man to thoughtlessly destroy the environment is practically without limits,’ says an American scientist. Poiliticans face the challenge of stopping the destruction before it becomes too late. For that we have very little time.”
End
———————————————————————————-
Today we see that life is much better and different than what ZDF predicted in 1972. Lake Erie is also clean again. Sulphur dioxide and carbon monoxide have long since been replaced by life-giving CO2 as the big threat to humanity and nature. The dire prophesies never came true – even though there was a consensus among the “experts”.
HAPPY NEW YEAR EVERYBODY!
Share this...FacebookTwitter "
"
You may remember last month I posted adjusted numbers for WUWT due to a SPAM attack, dropping my September count by about 50K:

SEPTEMBER NUMBERS Click for full sized image
For September 2008 the total was 846,193 page views, up from 667,215 page views in August 2008.
But there is a caveat, I think the real numbers are just shy of 800,000, because on the weekend of 09/20 and 09/21 I got quite a bit of unexpected traffic to one old post that I’m not sure is real or not. During that time, we got a lot of Spam on that particular older entry comparing UAH, RSS, HadCRUT, and GISS, but not anywhere near the numbers specific to that post, shown below:
Blog Stats Increase due to DOS “something”

Saturday 09/20 23,486
Sunday   09/21 25,319
Monday  09/22   1,006
Total: 49,811
Raw WUWT September numbers:           846,193 page views
WUWT Spam Uncertainty numbers:           -49,811 page views (from 09/20 to 09/22)
Final Adjusted WUWT September numbers: 796,382 page views
Today at 5PM (00UTC) I tallied my October numbers, and while they are slightly lower than last month’s total, I didn’t have any perceptible spam attacks like last month, so I don’t need to adjust October’s numbers:

OCTOBER NUMBERS Click for larger image
At 826,633 page views for October, that would put me up slightly from last month’s adjusted numbers of 796,382, but there was also an extra day involved (September hath 30 days). Since that difference is close to my daily average, we’ll just call it “no change” for October.
Of course many of you are either hoping for “change” or “no change” in November. 😉
Thanks again to all of you, and especially those whom have donated recently to help keep this effort going. My UHI test in Reno was funded in part through your donations. Another trip this weekend to retrieve two dataloggers that have been out for 45 days will also be assisted by those same donations. Again my thanks.
Have a wonderful Halloween!- Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b32455e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Earth Hour in California – Success or Bust?


Guest post by Russ Steele, NCWatch
At our house we set the timer to remind us to turn on all the visible out side lights.  We have multiple security lights on the garage and the barn that come on when the sun goes down. My friend George Rebane has evidence that he turned on his lights for Earth Hour at Ruminations. I should have done the same, but was working on a sea level issue in R and forgot. I am glad I set the timer to remind me to turn off the outside house lights at 9:30.
The real question is did it Earth Hour make a difference one way or the other?
Roger Sowell had a good idea, he download the the graph below from www.caiso.com, the California Independent System Operator.  CAISO is in charge of receiving power from power generating plants, and distributing the power throughout the state grid to the various end users.
California power use 3-28-09 from CAISO  - Click for a larger Graphic 
Now compare the graph from Saturday 3/28/09 to the one on Sunday 3/29/09 shown below, note the similar slopes during the same time period. Note that annotations were added by Anthony Watts on both graphs.
California power use 3-29-09 from CAISO  - Click for a larger Graphic 
Roger notes:
The light gray line is the forecasted power usage, shown in Megawatts.  The red line is the actual power consumed.  Around 1900 hours, 7 p.m., the load was approximately 24,000 MW.  By 8:00, the load increased smoothly to just over 26,000 MW.  Then the load began a steady decrease right on through the night, ending at around 22,000 MW at almost midnight. 
 
There was no apparent decrease in the power load throughout the state, from 8:30 to 9:30 p.m.  No step changes, nothing, nada, zip, zilch.
There you have it, scientific data showing that the Earth Hour was a total bust in California.  If you look close, you can see a little bump up above the forecast demand, which tracked very closely with actual power consumed prior to the witching hour 8:30 to 9:30. But, it is clear that power consumption did not drop, it stayed up. Maybe all those protesters forgot to turn off the lights.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e975adbdb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Guest post by Steven Goddard
I have been noticing in recent weeks that NSIDC extent is much closer to their 1979-2000 mean than NANSEN is to their 1979-2007 mean.  This is counter-intuitive, because the NANSEN mean should be relatively lower than NSIDC – as NANSEN’s mean includes the low extent years of the 2001-2007 period.  Those low years should have the effect of lowering the mean, and as a result I would expect the NANSEN current extent to be equal to or above the 1979-2007 mean.
(For exclusive subsets A and, B where subset A has a mean value of 14 and subset B has a mean value less than 14, then the mean of the full set AB must also be less than 14.)

NANSEN shows extent more than 500,000 km2 below the 1979-2007 mean


NSIDC shows extent less than 200,000 km2 below the 1979-2000 mean

I overlaid the NANSEN graph on top of the NSIDC graph below, and it is easy to see how large the discrepancy is.  In fact, the NSIDC mean sits at about one standard deviation below the NANSEN mean – which makes little sense given their base time periods.  It should be the opposite way.

(Note – the NANSEN and NSIDC measuring systems are not identical, and I had to make a shift along the Y-axis to line them up.  However, the X and Y scales are identical for both graphs in the overlay image.)


NANSEN and NSIDC combined
As mentioned above, one might expect that the current NANSEN extent would actually be above the 1979-2007 mean.  But something odd happened with the NANSEN data on December 13, 2008.  Overnight it lost about 500,000 km2 of ice, as Anthony captured in the blink comparator below.

Is it possible that there is still an error in the NANSEN data?  The discrepancy in the offset from the mean vs. NSIDC is rather large – nearly large enough to place California inside.  What are your thoughts?
I asked Dr. Walt Meier from NSIDC his opinion, and he replied (as always) courteously and promptly.  His answers are below:

Nansen uses a different algorithm to calculate the sea ice extent. The algorithms differ in the way combine the raw data together to estimate extent. As long as one uses the same algorithm, the stories are all the same, but the details can differ, more so at certain times of year. When there is a diffuse, broken up ice edge and melt is starting is one such time.
I suspect the Bering Sea is probably the region resulting in most of the differences. While our algorithm shows the region has mostly “ice-covered” the ice cover there is very fragmented, broken-up, and thin.
….
The other thing that’s important to mention is that I was referring simply to discrepancy between how close the current lines are to climatology. However, there is also generally an “offset” between algorithm outputs – a bias or mean difference between the algorithms that is fairly consistent throughout the record. That is why NSIDC’s climatology is different than the Nansen climatology.
The important thing to remember is that there is a good consistent record from the passive microwave data as long as you consistently use the same algorithm and the same processing. But you can’t mix and match products.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96aa8eda',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

When a business isn’t making money, it usually tries to market new products and attract new investors. When a bureaucracy’s reason for existence is threatened, it typically generates new missions. One difference between businesses and bureaucracies, though, is that businesses have to satisfy customers and investors. Because bureaucracies are under no such constraint, they require more oversight. 



The most recent evidence for this is _Environmental Diplomacy_ , the State Department’s first annual report on the environment and foreign policy. According to the report, the end of the Cold War has reduced the demand for the agency’s traditional tasks and given it time to ponder other threats to U.S. vital interests. And what do you know? The world is filled with environmental crises that the State Department must solve.



What are the environmental problems that warrant the attention of the State Department? According to the report, they fall into five areas: climate change, toxic chemicals, species extinction, deforestation and marine degradation. In each area, terrifying scenarios abound that necessitate reliance on the State Department to save the day. For example, “Forests four times larger than Switzerland are lost every year. 70% of the world’s marine fish stocks are fully to over‐​exploited. The people of the world annually release 23 billion tons of CO2 into the air … The range of impacts [from CO2 release] is likely to include: threats to human health including increases in heat‐​related deaths and illnesses, and in the incidence of infectious diseases.… There is no way to estimate the potential benefits that may come from millions of species yet to be studied, or yet to be discovered. And there is no way to estimate the health, economic, and spiritual costs to our children who could inherit a world robbed of a drug to cure AIDS, stripped of a strain of disease‐​free wheat, or bereft of the wonder of such diverse creatures as tigers and sea turtles.”



Let’s avoid speculating on the desperation that led the State Department to produce a post‐​Cold War mission that required the invocation of children, AIDS, wheat, sea turtles and tigers all in the same sentence. Instead, consider the paucity of evidence for the existence of those problems. Take, for instance, the panic over global warming. Observed warming since the late 19th century is only .5 degrees centigrade rather than the 1990 prediction of the United Nations’ climate change panel of 1.3 to 2.3 degrees centigrade, according to Patrick Michaels, professor of environmental sciences at the University of Virginia. Greenhouse physics predicts that the driest air masses — those in the polar regions during the winter night — should respond first and most strongly to CO2 emissions. In fact satellite data confirm that over the last 18 years the globe has cooled in general, but the coldest winter regions in Siberia and Canada have warmed. Are warmer winters in the Arctic Circle a problem?



If the State Department is serious about solving environmental problems, it should educate the world about the establishment of property rights, the enforcement of contracts, and the role of courts in resolving dispute. And then it should get out of the way.



Although the State Department claims there is no way to estimate the potential benefits from species yet to be discovered, the loss of biodiversity as a threat to pharmaceutical discoveries has been studied by economists for Resources for the Future. In a February 1996 article in the _Journal of Political Economy_ , they estimate the pharmaceutical benefits from saving one additional plant species from extinction to be, at most, about $10,000. That translates into an estimated willingness of pharmaceutical companies to pay about $8 an acre for land in tropical regions. The authors conclude that both the value of species preservation for use in pharmaceutical research and, by extension, the incentive to conserve threatened habitat are “negligible” in comparison with other development uses.



Let’s assume, however, that some of the problems are real and as serious as the report alleges. What will the State Department do if we give it more money to save us from those impending disasters? It will “help stabilize” regions where pollution contributes to political tensions. It will “enable nations … to work cooperatively to develop initiatives to attack regional environmental problems.” It will “strengthen our relationship with allies by working together on internal environmental problems.” In short, it will expend a amount of hot air to create domestic political capital.



If _Environmental Diplomacy_ were a prospectus for a company trying to raise additional money in the stock market for its new mission to solve environmental problems, would you invest your money? I think not, because the solution to environmental problems is not more meetings or negotiations but property rights. To the extent environmental problems exist, both within and between countries, the cause is the lack of adequately defined and enforced property rights. If people own resources, they seek to conserve their value rather than waste them. Rhinos and tigers are slaughtered to near extinction because no one owns them. Water and air resources are used with little thought because no one owns them.



If the State Department is serious about solving environmental problems, it should educate the world about the establishment of property rights, the enforcement of contracts, and the role of courts in resolving dispute. And then it should get out of the way. No one need decide the difficult tradeoffs about the use of land, energy, water or air. Prices will communicate the value of those commodities to all and they will be used to everyone’s best advantage. Short of that, the State Department should stop preaching and concentrate on developing a coherent post‐​Cold War national security strategy.
"
"You don’t need to observe politics for too long before realising that hypocrisy is the natural scent of the politics. It is a stench that pervades much of what is said and policy that is enacted. There are two particular types – the hypocrisy where politicians pretend they care about something and then do nothing, and the one where they pretend to think something and do the opposite. We have seen it this week with former foreign affairs minister Julie Bishop suddenly discovering climate change. She told Guardian Australia “that I’ve always been of the view that Australia, as a leading industrialised and developed nation, with one of the best standards of living in the world, needs to be a leader in the international response to climate change”. The nation as one responded with a resounding, “Seriously, what?” Name one instance during her time as foreign minister or even just as a member of the Howard or Abbott-Turnbull governments where she ever proffered such a view or supported such a policy position. Her very last mention of “climate change” in parliament came in 2017 when she lovingly talked up “high efficiency, low emissions” coal plants and decried in loud tones the ALP’s “obsession with a 50% renewable energy target” which she said was “destroying business confidence in South Australia, threatening jobs and threatening industries”. Gotta love that leadership. Or perhaps she was leading in 2015 when she told parliament: “I think it is important not to engage in hyperbole when one is talking about climate change. I remember in 2011 when the deputy leader [Tanya Plibersek] tried to scare the senior citizens on the Central Coast by saying that they were going to be subject to the ravages of climate change.” Or maybe she suggested being a “leader in the international response” was what she was doing when she told parliament that the government’s 26% to 28% target of emissions cuts, “is a responsible contribution” and that “what is not responsible is Labor’s endorsement of a carbon reduction target of up to 60% on 2000 levels by 2030”. Do nothing but say you care about doing lots. The other hypocrisy was writ large across the LNP this week with regards to the budget surplus. Gone is the boasting of a surplus already delivered, now the Treasurer says “well the surplus has never been an end in itself” and “delivering a surplus has not been our priority in the face of these crises” and “our focus is on is not necessarily delivering the surplus”. Last year, when David Speers questioned Frydenberg about the government’s boast of delivering a surplus that had actually yet to happen, Speers suggested “there might be a collapse in China, there might be another terrible drought … things can happen that could prevent you actually achieving this”. The treasurer responded by arguing: “Well this is not a wafer thin surplus. This is a very significant surplus.” Don’t expect to hear such arguments being made now. Now the talk is of things outside the control of the government – indeed they will use all the arguments they criticised the ALP for making when the GFC smashed the economy and the budget. Hypocrisy? Yep. But here’s the thing – we actually want them to be hypocrites, it is better for the nation if they are – so long as it is this brand of hypocrisy. The absolute worst thing the government could do right now is desperately try to deliver a surplus. Forget stories such as those that appeared on the front page of the Australian this week about new taxes – nothing on the revenue side will help the government now because any new taxes will only come into effect next financial year at the earliest. But they can stop spending right now. The only reason the government has been able to project a surplus is because they project that they will be awash with tax revenue. The government was not projecting a budget surplus because of big austerity campaigns, but because of factors outside their control that looked set to deliver a massive increase in tax. In the face of less than expected revenue, the only way to still deliver a surplus is to cut government spending – but that would further reduce economic activity and thus further reduce revenue, which would necessitate further cuts. The smart thing to do is to accept a budget deficit, because that will help keep the economy afloat. Will the government attempt to sell such a move with hypocritical statements? For sure. But that is better than the alternative. And so it is with climate change. Members of the LNP have spent more than a decade warning about not getting ahead of the rest of the world, or that a carbon price is actually a tax, or that we need to think more about adaptation than lowering emissions, or that electric cars will kill weekends or that renewable energy will deliver blackouts. It would be a measure of gross hypocrisy for them now to start shouting about the need to do more on emissions, that we need to lead the world and we need to put a price on carbon and to stop faffing about. The LNP account for around 99% of everyone in Australian politics who is to blame for the lost decade of climate change policy inaction. Were they to turn around now and argue there is no time to waste, we would be able to fill the MCG with print outs of transcripts that demonstrate their hypocrisy. And yet, bring it on. Because while hypocrisy should be addressed, I would much prefer the hypocrisy of a Liberal party pretending that their past actions did not contradict their new outlook on climate change than the current version we have of the LNP saying we need to do something but then purposefully and perpetually doing the opposite. The hypocrisy of their position on the budget surplus looks to have been forced on them. With luck all sides may use this as a moment to remember that it is actually true that “the surplus has never been an end in itself”. And on climate change we can only hope the LNP discover a new fragrance of hypocrisy. Yes it would be best if they were to admit their errors, and apologise for the past decade of waste that has cost us all so greatly. But if they start to act on reducing emissions and actually do attempt to lead the world, then I’ll take the emissions cuts and live with the hypocrisy."
"Royal Bank of Scotland’s new chief executive is renaming the group NatWest in a corporate overhaul designed to put its 2008 government bailout, and the fallout from a a string of scandals, behind it. The lender said it was ditching the 293-year-old RBS company name, saying it was the right time to make a change at the parent company and reflect that NatWest is its biggest brand. However, the existing RBS bank branches – most of which are in Scotland – will keep their name, as will Ulster Bank in Northern Ireland. The chief executive, Alison Rose, who took over in November, described her strategy as the “start of a new era”. The bank’s reputation was shattered after its near collapse in 2008, when the government was forced to shell out £45bn to save the bank, which is still 62% taxpayer-owned. Since then the RBS brand has been sullied by claims that it mis-sold toxic mortgage-backed securities in the lead-up to the global credit crunch and that it pushed small businesses towards failure in order to sell their assets for profit. Despite ditching the Scottish name, the chairman, Howard Davies, said the bank’s headquarters would remain in Edinburgh. “We’re not unscrewing any brass plaques at this point,” he said. However, Rose, the first woman to run one of the “big four” lenders, will be based in London. The strategy was revealed as the bank released its full-year earnings, which showed that 2019 pre-tax profits surged by 93% to £3.1bn from £1.6bn a year earlier. Its results were boosted by its disposal of a stake in the Middle Eastern bank Alawwal last year. This is the third consecutive year of profit for RBS since its bailout in 2008. Shareholders will pocket another £968m in dividends as a result of the bank’s strong performance, £600m of which will go to the government coffers. Rose was widely expected to reveal job losses on Friday but refused to disclose how many colleagues or bank branches would be axed as part of plans to cut costs by £250m this year. Its NatWest Markets investment bank has been singled out for a “significant transformation”, with plans to exit parts of the business and cut its basket of risky assets in half. Rose said it was part of efforts to build a “smaller and simpler bank”. As part of the strategy, Rose announced that RBS will also stop lending and offering underwriting services to major oil and gas producers that do not have credible transition plans in line with Paris climate agreement targets, which aim to limit global heating to below 2C. The group is threatening similar moves against companies with more than 15% of their activities related to coal unless they have similar plans prepared. RBS also pledged to fully phase out coal financing by 2030 and is aiming to “at least halve” the climate impact of its lending activity by the end of the decade. The value of loans to oil, gas and coal companies is relatively small compared to its peers, worth about £6bn. At its rival Barclays, lending and underwriting to carbon-intensive companies and projects totalled $85bn (£64bn) between 2015 and 2018, according to a study commissioned by campaign groups including Rainforest Action Network.  However, Rose said it was a step in the right direction: “On our balance sheet, only 1% of our lending is to the oil and gas sector, and in coal it’s marginal, down at 0.3%. But, for me, the key message here is we’re all going to need to work together; no one organisation is going to solve the climate challenge on their own and we’re very keen to work with business, with regulators, with industry to help make the plans to transition over the next 10 years.”"
"
Share this...FacebookTwitterPDO how entering the cool phase. Will it cool the planet in the decades ahead?
Will the new 2011-2020 decade be warmer or cooler than the last one?
That’s what we’re betting on (FOR CHARITY).
This bet is also known as the Honeycutt Climate Bet for Charity, after Rob Honeycutt who first proposed this bet. You can find the entire background here.
First of all Charles Zeller has also pledged $5,000 for the warm side, and has already paid half of it to Doctors Without Borders. So whatever happens from now on, I will consider that this blog has led to some kind of succees in that it has played a role in leading to this generous donation. The money will no doubt alleviate much pain and suffering among those of us who happened to be born in unfortunate economic and social conditions. It may even save a life or more. So hats off to Charles.
If you wish to join the Climate Bet of the Decade, i.e. Honeycutt Climate Bet For Charity, click here to see how. It’s real easy: just leave a reader comment and I’ll put your name, e-mail address, and amount on a list. That’s it. You’ll appear in the next update.
Here’s the latest list. I have to admit that the warmists, though small in number, are a generous bunch.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




They have pledged over $14,000.00 so far. The coolists have also made a good number of pledges, and have in fact far surpassed all my earler expectations. No matter who wins, some good will surely come out of this.
The cool side: (I hope my math is correct).

And the warm side:

If you wish to up your bet, just say so. If you wish to cancel it, well there I can’t help you. Pledged is pledged! 🙂
Share this...FacebookTwitter "
"
As many of you may know, I produce a variety of weather imagery maps for web and broadcast in SD and HD. Since there is a lot of interest in the path of hurricane Gustav, I thought I’d post a near-live image, which will update every 30 minutes.

Click image for full size or animate this image: Click for loop>>>   
What is interesting to note, is that as of this writing, Gustav seems to be losing organization. The eye, which was well defined just before making landfall on Cuba, seems very nebulous. Watch and wait.
Update: 3:30PM PST, while there was some weakening earlier, it now looks like signs of increased angular momentum are showing up in the satellite imagery. A defined eye may appear again.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9cb0b9fe',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Mr. President, justices of the Constitutional Court, distinguished guests, ladies and gentlemen, good morning.



I’m honored to be here today to speak as you celebrate the 70th anniversary of Italy’s Constitution. In a world of political and constitutional uncertainties, it’s altogether fitting that you should be marking this milestone in Italy’s history, and I wish you many more such celebrations in the years to come.



Before I begin, let me thank President Giorgio Benvenuto, our event coordinator Professor Luigi Troiani, and their colleagues at the Pietro Nenni Foundation for bringing this occasion about and for inviting me to speak before you. When they visited me in Washington in October, we talked about work I’d done over the years on American constitutional theory and history. And we talked in particular about the Cato Institute’s annual Constitution Day Symposium, which I started 16 years ago to mark the day in 1787 when the American Constitution was completed and sent out to the states for ratification. Since that Constitution has endured now for more than 230 years, Senator Benvenuto and Professor Troiani thought it might be useful to share some of that experience with you, especially as it might bear on European constitutionalism. And so we settled on the topic I’ll discuss this morning, “American Constitutional Theory and History: Implications for European Constitutionalism.”



That’s a large subject, of course, with many moving parts drawn from several disciplines. And the inevitable ambiguities of translation complicate understanding even further. So rather than dive straight into my argument, I’ll first say a word about my subtitle and then outline the argument so you’ll know where I’m going.



By European constitutionalism I’m alluding not to the national constitutions of the European nations, about which I know little, but to the European Union Treaties that serve as a “Constitutional Charter” for the European Communities. As an American constitutionalist looking east and seeing everything from Brexit to Grexit, plus events in European capitals, I’m struck by the tension in the EU between exclusion and inclusion in its many forms, including individualism and collectivism. Those themes underpin my talk today. The issues surrounding them are universal. They’re at the heart of the human condition.



In America we wrestled with them at our founding over 200 years ago, again in the aftermath of our Civil War, and yet again with the advent of Progressivism, which culminated in our New Deal constitutional revolution. And we’re still wrestling with them. Because America was founded on philosophical principles — First Principles, coming from the Enlightenment — it’s particularly appropriate that we look at that experience to shed such light as we can on this more recent European constitutional experience.



But my more immediate concern is this: In liberal democracies today — nations constituted in the classical liberal tradition — we see the same basic problem, albeit with significant variations. It’s that the growth of government, responding mainly to popular demand, has raised seemingly intractable moral and practical problems. First, increasing intrusions on individual liberty; and second, the unwillingness of people to pay for all the public goods and services they’re demanding. So governments borrow. And that’s led to massive public debt that saddles our children and grandchildren, to bankruptcy, and to the failure of governments to keep the commitments they’ve made.



In Italy, we need only look east, to the birthplace of democracy. But Greece isn’t alone in this. Nor are we in America immune. Cities like Detroit have gone bankrupt. So too, just recently, has the American territory of Puerto Rico. The state of Illinois has a credit rating today just above junk status, and Connecticut and New Jersey, among other states, aren’t far behind. At the national level, America’s debt today exceeds $20 trillion — that’s trillion — more than double what it was only a decade ago. And our unfunded liability vastly exceeds that.



What has this got to do with constitutionalism? A great deal. Constitutions are written, after all, to discipline not only the governments they authorize but the people themselves. The point was famously stated by James Madison, the principal author of the U.S. Constitution. “In framing a government which is to be administered by men over men, the great difficulty lies in this,” he wrote: “you must first enable the government to control the governed; and in the next place oblige it to control itself. A dependence on the people is, no doubt, the primary control on the government,” Madison concluded, “but experience has taught mankind the necessity of auxiliary precautions.“1



The principal such precaution, of course, is a well‐​written constitution. But no constitution is self‐​executing. It’s people who ultimately execute constitutions. In the end, therefore, the issue is cultural, a point I’ll come back to.



America’s Founders were deeply concerned with the problem of undisciplined, unlimited government. After all, they’d just fought a war to rid themselves of distant, overbearing government. In drafting the Constitution, therefore, they weren’t about to impose that kind of government on themselves. In fact, during the ratification debates in the states, there were two main camps — the Anti‐​Federalists, who thought the proposed Constitution gave the government too much power, and the Federalists, who responded by pointing to the many ways the proposed Constitution would guard against that risk. The Federalists eventually won, of course, but the point I want to secure is that there wasn’t a socialist in the group! There were _limited_ government people, the Federalists, and _even more_ limited government people, the Anti‐​federalists.



So under a Constitution that hasn’t changed all that much, how did we go from limited to effectively unlimited government? The answer lies in the fundamental shift in the climate of ideas that began with Progressivism at the end of the 19th century, which the New Deal Supreme Court institutionalized in the 1930s. To illustrate that, I’ll first look closely at America’s founding documents: the Declaration of Independence, signed in 1776; the Constitution, ratified in 1788; the Bill of Rights, ratified in 1791; and the Civil War Amendments, ratified between 1865 and 1870, which corrected flaws in the original Constitution. Together, those documents constitute a legal framework for individual liberty under limited government, however inconsistent with those principles our actual history may have been.



Then I’ll show how progressives rejected the libertarian and limited government principles of America’s Founders and how they eventually turned the Constitution on its head, not by amending it but through political pressure brought to bear on the Supreme Court. The problems that have ensued include the ones just noted: less liberty, increasing debt. But perhaps of even greater importance, for eight decades now the Supreme Court has struggled to square its post‐​New Deal decisions with the text and theory of the Constitution. That amounts to nothing less than a crisis of constitutional legitimacy.



And again, the basic reason for that crisis is the fundamental shift in outlook. Many Americans today no longer think of government as earlier generations did. Whereas the Founders saw government as a “necessary evil,” to be restrained at every turn, many today think that the purpose of government is to provide them with vast goods and services, as decided by democratic majorities.



 **The Importance of Theory**



I come, then, to the first important point I want to flag. You cannot understand the U.S. Constitution unless you understand the moral and political theory that stands behind it. And that was outlined not in the Constitution but in the Declaration of Independence.2 The Constitution was written in a context, as were the later Civil War Amendments, and that context was one of natural law, Anglo‐​American common law, and even elements of Roman Law, all of which is captured succinctly in those famous words of the Declaration that I’ll quote in a moment. Indeed, President Abraham Lincoln’s famous Gettysburg Address, written in the throes of a brutal Civil War, begins with these words: “Fourscore and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal.” Lincoln was reaching back to the Declaration, not to the Constitution.



Yet no less than my good friend and Italy’s gift to American constitutionalism, the late Justice Antonin Scalia, all but dismissed the Declaration as “philosophizing,” contrasting it with the Constitution’s “operative provisions.“3 And his conservative colleague when the two served on the nation’s second highest court, the late Judge Robert Bork, wrote that “the ringing phrases [of the Declaration] are hardly useful, indeed may be pernicious, if taken, as they commonly are, as a guide to action, governmental or private.“4 Is it any wonder that there’s constitutional confusion in America today when the document that’s essential to understanding it plays little or no part in that understanding?



With that outline now before us, let me flesh out the argument. I’ll do that by focusing on the underlying moral, political, and legal principles at stake, after which I’ll offer just a few reflections on how they might illuminate issues in the European context. Again, I want to show how the shift from limited to effectively unlimited government took place in America, despite very few constitutional changes. I should note, however, that it will be some time before I get to the Constitution. If a proper understanding of the Constitution requires a proper understanding of the theory behind it, and if that theory is found implicitly in the Declaration, then that should be our initial focus, and will be for some time.



That will take us into some of the deeper reaches of moral and political theory, so please bear with me. My hope is that at the end you’ll better understand the Constitution itself — and especially the broad principles that underpin it.



So let’s get started. The first thing to notice about the American constitutional experience is how relatively different its beginnings were from those of many other nations. Constitution making and remaking in many nations has taken place in the context of an often stormy history stretching back centuries, even millennia. By contrast, America was a _new_ nation. We came into being at a precise point in time, with the signing of the Declaration of Independence. True, American patriots had to win our independence on the battlefield. And before that we had a colonial history of roughly 150 years. But America was created not by a discrete people but by diverse immigrants with unique histories all their own.



A second, crucial feature distinguishing America’s constitutional experience is that it unfolded during the intense intellectual ferment of the Enlightenment, including the Scottish Enlightenment, with its focus on the individual, individual liberty, and political legitimacy, all of which reflected the sense of “a new beginning.” Indeed, the motto on the Great Seal of the United States captures well the spirit of America’s origins: _Novus ordo seclorum, “a_ new order of the ages.”



 **The Declaration of Independence**



Let’s turn, then, to that new order, as outlined in the Declaration. Penned near the start of our struggle for independence, the Declaration in form is a _political_ document. But were it merely that, it would not have so endured in our national consciousness. Nor would it have inspired countless millions around the world ever since, leading many to leave their homelands to begin life anew under its promise, including millions from Italy who now enrich America. It has so inspired because, fundamentally, it is a profound _moral_ statement. Offered from “a decent Respect to the Opinions of Mankind” and invoking “the Laws of Nature and of Nature’s God,” it was written not only to declare but to _justify_ our independence. And it did so not simply by listing the king’s “long Train of Abuses and Usurpations,” which constitute the greater part of the document, but by first setting forth the moral and political vision that rendered those acts unjust.



And so we come to those famous words that flowed from Thomas Jefferson’s pen in 1776, words that capture fundamental principles concerning the human condition:



We hold these Truths to be self‐​evident, that all Men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty, and the Pursuit of Happiness — That to secure these Rights, Governments are instituted among Men, deriving their just Powers from the Consent of the Governed.



The first thing to notice about that justly famous passage is that its propositions are asserted as “truths,” not mere opinions. The Founders were not moral relativists. They were confident in their claims. And why not? Their truths were said to be “self‐​evident,” grounded in universal reason, accessible by all mankind — and the evidence supports that.



Notice too the structure of the passage: There are two parts — and the order is crucial. The moral vision comes first, defined by equal rights. The political and legal vision comes second, defined by powers, as derived from the moral vision. And right there is the second major point I want to flag: Unlike today, where politics, grounded in will, so often determines what rights we have, for early Americans, morality, grounded in reason, determined our rights. The Founders were concerned fundamentally with moral and political _legitimacy_. Rights first, government second, as the means for securing our rights.5



Given that order of things, the Founders were engaged in what’s called “state‐​of‐​nature theory,” a rudimentary form of which can be found in the writings of your own Seneca,6 a fuller discussion much later in those of Thomas Hobbes7 and, especially, John Locke8 — often said to be the philosophical father of America.



State‐​of‐​nature theory is a thought experiment. The idea is to show how, without violating any rights, a legitimate government with legitimate powers might arise from a world with no government. Thus, the first step is to show, from pure reason, what rights we’d have in such a world.



For that, as the Declaration implies, we turn to the natural law tradition — more precisely, the natural rights strain coming from the Reformation and the Enlightenment. Simply put, natural law stands for the idea that there’s a “higher law” of right and wrong, grounded in reason, from which to derive the positive law, and against which to criticize that law at any point in time. There’s nothing suspect about that idea, as modern moral skeptics argue. We appeal to natural law when the positive or actual law is morally wrong. In America, the abolitionists, the suffragists, and the civil rights marchers all invoked our natural rights in their struggles to overturn unjust law.



The origins of this law are in antiquity. Many of its particulars are found in Roman Law, especially the law of property and contract. Over some 500 years in England, prior to the American Revolution, this law was refined and reduced to positive law by common‐​law judges consulting reason, custom, and what they knew of Roman Law as they adjudicated cases brought before them by ordinary individuals.9 And John Locke drew largely on that body of common‐​law rights as he crafted a theory of natural rights, much as Jefferson drew on Locke when he drafted the Declaration.



To correct a common misunderstanding, these are the rights we hold _against each other_ , and would hold in a state of nature. Later, once we create a government, they’ll serve as rights we hold against that government, and likely be included in a bill of rights.



To discover and justify these rights in detail, as I and others have done,10 we’d need to delve into the complex issues of moral epistemology and legal casuistry, and this isn’t the occasion for that. Suffice it to say that when that foundational work is finished, the conclusion one reaches is the same one America’s Founders reached intuitively, that our basic right is the right to be free from the unjustified interference of others, and all other rights are derived from that basic right, as the facts may warrant. What results approximates largely the judge‐​made common law of property, torts, contracts, and remedies, a law that defines our private relationships, as it did in early America, both before and long after the Revolution. It’s a law that says, in essence, that each of us is free to pursue happiness, by his own subjective values, either alone or in association with others, provided we respect the equal objective rights of others to do the same. In short, it’s a live‐​and‐​let‐​live law of liberty.



And I can summarize it with three simple rules, so simple that even a child can understand them.



Rule 1: Don’t take what belongs to someone else. That’s the whole world of property, broadly conceived as Locke did — our property in our “Lives, Liberties, and Estates.”



Rule 2: Keep your promises. That’s the whole world of contracts and associations.



Rule 3. If you’ve wrongly violated rules 1 or 2, give back what you’ve wrongly taken or wrongly withheld. That’s the whole world of remedies.



There’s a fourth rule, but it’s optional: Do some good. You’re free not to be a Good Samaritan, but you should be one if you’re a decent human being and the cost to you is modest. Unlike much continental law, Anglo‐​American law never compelled strangers to come to the aid of others.11 It didn’t because individual liberty is its main object. And it saw that there’s no virtue in forced beneficence. We’re free to criticize those who don’t come to the aid of others, and we should, even as we defend their right not to.



Why have I mentioned this fourth, voluntary rule? Again, it’s because, when we start from a theoretical state of nature, we need to know what rights we do and do not have for government to enforce once we bring it into the picture. And the Good Samaritan is the modern welfare state writ small. If there’s no right to be rescued, there’s no correlative obligation for government to enforce. Recognizing that raises important questions about the very legitimacy of the welfare state.12



 **Leaving the State of Nature and the Problem of Political Legitimacy**



To get to the Constitution, however, we need now to take the last step in the argument. We need to derive a legitimate government with legitimate powers — and that’s not easy. I’ve said little about enforcement so far. The Declaration says that government’s purpose is to secure our rights, its _just_ powers derived “from the consent of the governed.” Thus, the Founders invoked the social contract, which grounds political legitimacy in consent.



But there are well‐​known problems with consent‐​based social‐​contract theory as a ground for political legitimacy. The question is how to move legitimately from self‐​rule to collective rule. Unanimity will achieve legitimacy, of course, but rarely if ever do we get it. Majoritarianism won’t solve the problem, because it amounts to tyranny over the minority that hasn’t consented. Nor will the social contract work, except for those in the original position who agree thereafter to be bound by the will of the majority. Nor, finally, will so‐​called tacit consent work — “you stayed, therefore you’re bound by the majority” — because it puts the minority to a choice between two of its rights, its right to stay where it is, and its right not to be ruled by the majority, precisely what the majority must justify on pain of circularity. As for elections, an occasional vote hardly justifies all that follows.



As a _practical_ matter, the social contract argument may be the best we can do, but recognizing its infirmities leads to a compelling conclusion — and to the third basic point I want to flag, namely, that there is an air of illegitimacy that surrounds government as such. Government is not like a private association that we join or leave at will. It’s a _forced_ association. Its very definition entails force. And once we recognize its essential character, that should compel us, _from a concern for legitimacy_ , to do as much as we can through the private sector where it can be done voluntarily and hence in violation of the rights of no one, and as little as possible through the public sector where individuals will be forced into programs they may want no part of.



In short, as a _moral_ matter, there’s a strong presumption against doing things through government. We should turn to government not as a first but only as a _last_ resort, when all else fails.



Still, we can refine this conclusion. We can distinguish three distinct powers in decreasing degrees of legitimacy. The first is the police power — the power, through adjudication or legislation, to more precisely define and enforce our rights. As such, it’s bound by the rights we have to be enforced, although it includes the power to provide limited “public goods” like national defense, clean air, and certain infrastructure, goods described by non‐​excludability and non‐​rivalrous consumption, as economists define them.13



When we leave the state of nature, we give government that power to exercise on our behalf. But because we had the power in the state of nature — Locke called it the “Executive Power” each of us has to secure his rights — to that extent it’s legitimate. Only the anarchist who would prefer to remain in the state of nature can be heard to complain. Fortunately, there are few of those.



Less legitimate is the eminent domain power — the power to condemn and take private property for public use after paying the owner just compensation — because none of us would have such a power in the state of nature. Such legitimacy as this power enjoys, at least in America, is because we gave it to government when we ratified the Constitution’s Fifth Amendment, which includes the Takings Clause; and it’s “Pareto optimal,” as economists say, meaning that at least one person is made better off by it — the public, as shown by its willingness to pay — and no one is made worse off — the owner, provided he’s indifferent as to whether he keeps the property or gets the compensation.



The third great governmental power, ubiquitous today, is the least legitimate. In fact, from a natural rights perspective, it enjoys no legitimacy. It’s the redistributive power, and it takes two forms, material and regulatory. Through redistributive taxation, government takes from _A_ and gives to _B_. Through redistributive regulation, government prohibits _A_ from doing what he would otherwise have a right to do or requires him to do what he would otherwise have a right not to do, all for the benefit of _B_. Those powers describe the modern redistributive and regulatory state. No one would have them in the state of nature. How then could government get them legitimately, since governments, in the classical liberal tradition, get whatever powers they have from the people, who must first _have_ those powers to yield up to government?



There are three main answers. First, if that redistribution arose through unanimous consent, there would be no problem, but again, rarely if ever does that occur in the public sector. Second, majorities gave governments those powers. That raises the classic problem of the tyranny of the majority. And third, special interests have learned how to work the system for their benefit, as public choice economists have long explained.14 That’s the tyranny of the minority — and the main source today of such schemes.



We can conclude this examination of the moral foundations of the classical liberal vision by imagining a continuum, with anarchy or no government at one end — our state of nature — and totalitarianism at the other end, where everything possible is done through government. At the anarchy end, individuals are free to plan and live their lives as they wish, alone or in cooperation with others. They will soon find, however, that there are some things best done collectively, like the provision and enforcement of law, national defense, clean air and water, limited infrastructure, and the like — public goods — and most will consent to the public provision of such goods. But as we move up the continuum toward totalitarianism and try to bring more and more _private_ goods under _public_ provision — education, health care, child care, jobs, housing, ordinary goods and services — people start voting with their feet. The Berlin Wall was not built to keep West German workers out of the workers’ paradise to the east.



The moral, political, and legal vision implicit in the Declaration of Independence is closer to the anarchy end of that continuum. America’s Founders envisioned a land in which people were free to live as they wished, respecting the equal rights of others to do the same, with government there to secure those rights and do the few other things it was authorized to do.



That basic moral vision is perfectly universalizable. How to secure it through the rule of law is another matter. Certain basic legal principles are themselves universalizable and are common to most legal systems, but whether a nation has a parliamentary system as in much of Europe, or a republican form of government as in America, or some other arrangement is not a matter of natural law. Let’s now see how the Founders framed a constitution to secure the Declaration’s moral vision.



 **The Constitution**



After we declared independence, and during our struggle for it, we lived under our first constitution, the Articles of Confederation. As its name implies, it was a loose agreement among the 13 states, authorizing a national government that hardly warranted the name. Three main problems lay ahead. Surrounded on three sides by great European powers, our national defense was painfully inadequate. Second, states were erecting tariffs and other barriers to free interstate trade. And finally, our war debts remained unpaid. After 11 years, the Framers met in Philadelphia to draft a new Constitution.



The main problem they faced was how to strike a balance. They needed to give the new government enough power to address those problems and accomplish its broad aims, yet not so much as to risk our liberties. Those aims were set forth in the Constitution’s Preamble:



We the People of the United States, in Order to form a more perfect Union, establish Justice, insure domestic Tranquility, provide for the common defence, promote the general Welfare, and secure the Blessings of Liberty to ourselves and our Posterity, do ordain and establish this Constitution for the United States of America.



Notice: States aside, regarding the proposed new government, we’re right back in the state of nature, about to “ordain and establish” a constitution to authorize it and bring it into being. All power rests _initially_ with “we the people.” _We_ bring the constitution and the government that follows into being through ratification. _We_ give it its powers, such as we do. The government does not give us our rights. We already have our rights, natural rights, the exercise of which creates and empowers this government.



So how does Madison strike the balance between power and liberty in service of those aims? First, through federalism: Power was _divided_ between the federal and state governments, with most power left with the states, especially the general police power, the basic power of government to secure our rights, as just discussed. The powers we delegated to the federal government concerned national issues like defense, free interstate commerce, rules for intellectual property, a national currency, and the like.



Second, following Montesquieu, Madison _separated_ powers among the three branches of the federal government, with each branch defined functionally. Pitting power against power, he provided for a bicameral legislature, with each chamber constituted differently; a unitary executive to enforce national legislation and conduct foreign affairs; and an independent judiciary with the implicit power to review legislative and executive actions for their constitutionality — a novel institution at that time, and a crucial one as time went on.



Third, although the Constitution left most of the rules for elections with the states, it provided for periodic elections to fill the offices set forth in the document, thus leaving ultimate power with the people.



But while each of those provisions and others struck a balance between power and liberty, the main restraint on overweening government took the name of the _doctrine of enumerated powers_. And I can state it no more simply than this: If you want to limit power, don’t give it in the first place. We see that doctrine in the very first sentence of the Constitution, after the Preamble: “All legislative Powers _herein granted_ shall be vested in a Congress .…” By implication, not all powers were “herein granted.” Look at Article I, section 8, and you’ll see that Congress has only 18 powers or ends that the people have authorized. And the last documentary evidence from the founding period, the Tenth Amendment, states that doctrine explicitly: “The powers not delegated to the United States by the Constitution, nor prohibited by it to the States, are reserved to the States respectively, or to the people.” In other words, the Constitution creates a government of delegated, enumerated, and thus limited powers. If a power is not found in the document, it belongs to the states — or to the people, never having been given to either government.



As I noted earlier, when the Constitution was sent out to the states for ratification, it met stiff resistance as Anti‐​Federalists thought it gave too much power to the national government. Only after the Federalists agreed to add a bill of rights was it finally ratified. During the first Congress in 1789, Madison drafted 12 amendments, 10 of which were ratified in 1791 as the Bill of Rights. It sets forth rights that are good against the federal government, such as freedom of religion, speech, press, and assembly, the right to keep and bear arms, to be secure against unreasonable searches and seizures, to due process of law, to compensation if private property is taken for public use, to trial by jury, and more.



But it’s important to note that the Bill of Rights was, as Justice Scalia said, an “afterthought.“15 Unlike with many European constitutions, which begin with a long list of rights, many aspirational, the Framers saw the Constitution’s _structural_ provisions as their main protection against overweening government.16 And on that score, it’s crucial to mention the Ninth Amendment, which reads: “The enumeration in the Constitution, of certain rights, shall not be construed to deny or disparage others retained by the people.”



The history behind that amendment is instructive. During the ratification debates, there were two main objections to adding a bill of rights. First, it would be unnecessary. “Why declare that things shall not be done,” asked Alexander Hamilton, “which there is no power to do?“17 Notice, he was alluding to the enumerated powers doctrine as the _main_ protection for our liberties: Where there is no power, there is a right.



But second, it would be impossible to list all of our rights, yet by ordinary principles of legal construction,18 the failure to do so would be construed as implying that only those rights that were listed were meant to be protected. So to guard against that, they wrote the Ninth Amendment, which reads, again, “The enumeration in the Constitution, of certain rights, shall not be construed to deny or disparage others retained by the people.” Notice: “retained by the people.” You can’t retain what you don’t first have to be retained. They were alluding to our natural rights, which we retained when we left the state of nature, save for those we gave up to government to exercise on our behalf, like the police power.



For a proper understanding of the Constitution, the importance of the Ninth Amendment, which speaks of retained rights, and the Tenth Amendment, which speaks of delegated powers, cannot be overstated.19 Taken together, as the last documentary evidence from the founding period, they recapitulate the vision of the Declaration. We all have rights, enumerated and unenumerated alike, to pursue happiness by our own lights, to plan and live our lives as we wish, provided we respect the rights of others to do the same, and federal and state governments are there to secure those rights through the limited powers we’ve given them toward that end. There, in a nutshell, is the American vision, reduced from natural to positive law.



There was a problem, however. There were too few checks on the states, where most power was left. And the reason was slavery. To achieve unity among the states, the Framers made their Faustian bargain. They knew that slavery was inconsistent with their founding principles. They hoped it would wither away in time. It didn’t. It took a brutal civil war to end slavery, and the Civil War Amendments to complete the Constitution by incorporating at last the grand principles of the Declaration, especially equality before the law.



The Thirteenth Amendment ended slavery in 1865. The Fifteenth Amendment, ratified in 1870, protected the right to vote. And the Fourteenth Amendment, ratified in 1868, defined federal and state citizenship and, for the first time, provided _federal_ remedies against a state’s violating the rights of its own citizens.20



Unfortunately, only five years after the Fourteenth Amendment was ratified, a deeply divided 5–4 Supreme Court eviscerated the principal font of substantive rights under the amendment, the Privileges or Immunities Clause.21 Thereafter the Court would try to do under the less substantive Due Process Clause what was meant to be done under privileges or immunities, and the misreading of the Fourteenth Amendment has continued to this day. Among other things, the upshot was Jim Crow racial segregation in the South, which lasted until the middle of the 20th century.



 **Progressivism**



We turn now to the great ideological watershed, the rise of Progressivism at the end of the 19th century. Coming from the elite universities of the Northeast, progressives rejected the Founder’s libertarian and limited government vision.22 They were social engineers, planners enamored of the new social sciences. Insensitive when not hostile to the power of markets to order human affairs justly and efficiently, they sought to address what they saw as social problems through redistributive regulatory legislation. They looked to Europe for inspiration: Bismarck’s social security scheme, for example, and British utilitarianism, which in ethics had replaced natural rights theory. The idea was that policy, law, and judgment were to be justified not by whether they protected our natural and moral rights but whether they _gave_ us positive rights purporting to produce the greatest good for the greatest number.



A particularly egregious example of that rationale concerned a sweetheart suit brought against a Virginia statute that authorized the sterilization of people thought to be of insufficient intelligence.23 Part of the bogus “eugenics” movement, the law was designed to improve the human gene pool. Writing for a divided Supreme Court in 1927, the sainted Justice Oliver Wendell Holmes upheld the statute, ending his short opinion with the ringing words, “Three generations of imbeciles are enough.” There followed some 70,000 sterilizations across the nation.



Some of what the progressives did was long overdue, like promoting municipal health and safety measures and attacking corruption. Yet they also sowed the seeds for later corruption, especially through regulatory schemes ripe for special interest capture, replacing markets with cartels.24 And their record on racial matters was abysmal.25



During the early decades of the 20th century, progressives directed their political activism mostly at the state level, but they often failed as the courts upheld constitutional principles securing individual liberty and free markets. With the election of Franklin Roosevelt in 1932, however, progressive activism shifted to the federal level. Still, during the president’s first term the Supreme Court continued mostly to uphold limits on federal power, finding several of Roosevelt’s programs unconstitutional.



With the landslide election of 1936, however, things came to a head. Early in 1937, Roosevelt unveiled his infamous Court‐​packing scheme, his threat to pack the Court with six new members. Uproar followed. Not even an overwhelmingly Democratic Congress would go along with the plan. Nevertheless, the Court got the message. The famous “switch in time that saved nine” justices followed. The Court began rewriting the Constitution, in effect, not through amendment by the people, the proper way, but by reading the document as it hadn’t been read for 150 years — as authorizing effectively _unlimited_ government.26



The Court did that rewrite in three basic steps. First, in 1937 it eviscerated the very centerpiece of the Constitution, the doctrine of enumerated powers. Then in 1938 it bifurcated the Bill of Rights and gave us a bifurcated theory of judicial review. Finally, in 1943 it jettisoned the non‐​delegation doctrine. Let me describe those steps a bit more fully so you can see the importance of recognizing and adhering to the theory that stands behind and informs a constitution.



The evisceration of the doctrine of enumerated powers involved three clauses in Article I, section 8, where Congress’s 18 legislative powers are enumerated: the General Welfare Clause, the Commerce Clause, and the Necessary and Proper Clause. All were written to be shields against government. The New Deal Court turned them into swords of government through which the modern redistributive and regulatory state has arisen.



In relevant part, the General Welfare Clause, the first of Congress’s enumerated powers, authorizes Congress to tax to pay for the “general Welfare of the United States.” As Madison wrote in _Federalist 41_ , that qualifying language was simply a general heading under which Congress’s 17 other powers were subsumed, for which Congress may tax, but only if they serve the _general_ welfare _of the United States_ , not particular or local welfare.



Instead, the New Deal Court read the clause as an _independent_ power authorizing Congress to tax for whatever it thought might serve the “general welfare.“27 That reading could not be right, however, because it would enable Congress to tax for virtually any end, thus rendering Congress’s other powers superfluous, as Madison, Jefferson, and many others noted when the issue arose early in our history. Indeed, it would turn the Constitution on its head by allowing Congress effectively unlimited power. Such is the result from ignoring the document’s underlying theory of limited government.



Similar issues arose that year with the Commerce Clause, which in relevant part authorized Congress to regulate interstate commerce. Recall that under the Articles of Confederation states had begun erecting tariffs and other protectionist measures, and that was leading to the breakdown of free trade among the states. Thus, the Framers gave Congress the power to regulate — or _make regular_ — commerce among the states, largely by negating state actions that impeded free trade, but also through affirmative actions that might facilitate that end.28



Over several decisions, however, beginning in 1937,29 the New Deal Court read the Commerce Clause as authorizing Congress to regulate anything that “affected” interstate commerce, which of course is virtually everything. Thus, in 1942 the Court held that, to keep the price of wheat high for farmers, Congress could limit the amount of wheat a farmer could grow, even though the excess wheat in question in the case never entered commerce, much less interstate commerce, but was consumed on the farm by the farmer and his cattle. The Court held that the excess wheat he consumed himself was wheat he would otherwise have bought on the market, so “in the aggregate” such actions “affected” interstate commerce.30 Such were the economic theories of the Roosevelt administration.



The last of Congress’s 18 enumerated powers authorizes it “to make all laws which shall be necessary and proper for carrying into execution the foregoing powers.” Thus, the clause affords Congress _instrumental_ powers — the _means_ for executing its other powers or pursuing its other enumerated ends. “Necessary” and “proper” are words of limitation, of course: Not any means Congress desires will do. Yet the New Deal and subsequent Court’s, until very recently, have hardly policed those limitations.31



Turning now to the second step, despite the demise in 1937 of the doctrine of enumerated powers, one could still invoke one’s rights against Congress’s expanded powers. So to address that “problem,” the New Deal Court added a famous footnote to a 1938 opinion.32 In it, the Court distinguished two kinds of rights: “fundamental,” like speech, voting, and, later, certain personal rights; and “non‐​fundamental,” like property rights and rights we exercise in “ordinary commercial relations.” If a law implicated fundamental rights, the Court would apply “strict scrutiny” and the law would likely be found unconstitutional. By contrast, if non‐​fundamental rights were at issue, the Court would apply the so‐​called rational basis test, which held that if there were _some_ reason for the law, if you could _conceive_ of one, the law would be upheld. Thus was economic liberty reduced to a second‐​class status. None of this is found in the Constitution, of course. The Court invented it from whole cloth to make the world safe for the New Deal programs.33



Finally, in 1943 the Court jettisoned the non‐​delegation doctrine,34 which arises from the very first word of the Constitution: “ _All_ legislative Powers herein granted shall be vested in a Congress .…” Not some; all. As government grew, especially during the New Deal, Congress began delegating ever more of its legislative power to the executive branch agencies it was creating to carry out its programs. Some 450 such agencies exist in Washington today. Nobody knows the exact number.



That is where most of the law Americans live under today is written, in the form of regulations, rules, guidance, and more, all issued to implement the broad statutes Congress passes. Not only is this “law” written, executed, and adjudicated by unelected, non‐​responsible agency bureaucrats — raising serious separation‐​of‐​powers questions — but the Court has developed doctrines under which it defers to _agencies_ ’ interpretations of statutes, thus largely abandoning its duty to oversee the political branches. Governed largely today under administrative law promulgated by the modern executive state, we’re far removed from the limited, accountable government envisioned by the Founders and Framers.35



This completes my overview of American constitutional theory and history. From it, as I mentioned early on, the main lesson to be drawn is that culture matters. The Founders and Framers were animated by individual liberty under limited government. When the post‐​Civil War Framers saw the need to revise our original federalism, they did it the right way, by amending the Constitution to make it consistent with its underlying moral and political principles. The New Deal politicians, having less regard for the Constitution and its underlying principles, rejected that course, choosing instead to browbeat the Court into effectively rewriting the Constitution, undermining its moral and political principles in the process.



But don’t take my word for it. Here’s Franklin Roosevelt, writing to the chairman of the House Ways and Means Committee in 1935: “I hope your committee will not permit doubts as to constitutionality, however reasonable, to block the suggested legislation.“36 And here’s Rexford Tugwell, one of the principal architects of the New Deal, reflecting on his handiwork some 30 years later: “To the extent that these [New Deal policies] developed, they were tortured interpretations of a document intended to prevent them.” They knew exactly what they were doing. They were turning the Constitution on its head.37



Thus, the problem today is not, as so many America progressives think, too little government. It’s too much government, intruding on our liberties and driving us ever deeper into debt. And it isn’t as if our Founders didn’t understand that. As Jefferson famously wrote, “The _natural_ progress of things is for liberty to yield, and _government_ to gain ground.“38 The remedy is a good constitution, but it must be followed. And that takes good people.



 **A Few Implications for European Constitutionalism**



So what lessons might we draw from the American experience for European constitutionalism? Recall my mentioning earlier of being struck by the tension in the EU between exclusion and inclusion in its many forms, including individualism and collectivism. As we’ve seen, that same tension runs through America’s constitutional history as well. To address deficiencies in the Articles of Confederation, the original Constitution moved toward greater inclusion to form “a more perfect Union.” But the resulting federalism didn’t get the balance right either. It left too much power with the states, enabling the southern states to continue enforcing slavery. So the Civil War Amendments increased the inclusion, correctly. The adjusted federalism gave more power to the federal government, enabling it to block states from oppressing their own citizens — a higher power checking a subsidiary power.



But that balance, reflecting the nation’s underlying principles, was upended again by the far more inclusive New Deal constitutional revolution. Giving vastly more power to the federal government, contrary to the nation’s limited government principles, this change swept ever more Americans into public programs, leading many to want out. They wanted to be _excluded_ from the socialization of life, as reflected by the rise of the conservative and libertarian movements in the second half of the 20th century.



Are there parallels with post‐​War developments in Europe? To this sometime‐​student of European affairs, there seem to be, but the inclusion that began with the 1951Treaty of Paris and continued through the many treaties since makes it difficult if not impossible to speak of three distinct periods, as in America, much less point to a “golden mean” in this evolution akin to America’s post‐​Civil War settlement. In recent years, however, the impetus toward exclusion, in many forms, is unmistakable, Brexit being only the most prominent example, the ongoing refugee resettlement crisis being another.



Federalism _within_ nations is a delicate balance. Federalism _among_ sovereign nations, which is what the EU amounts to, is far more difficult, especially when cultural differences loom large. And on that score, here’s a paradox. Europeans have always been more comfortable than Americans with collectivization in the form of the welfare state, certainly within their respective nations.39 But with collectivization _among_ nations, cultural differences — rich and poor being only one axis — can easily exacerbate the cooperation that is required if collectivization is to work at all, much less with any measure of efficiency. The evidence suggests that the EU has gone too far in that direction. At the same time, the evidence is equally clear that the failure to make EU border security an EU responsibility, leaving it instead to individual members, has raised serious problems too.40



In America, border security became a federal government function once the Constitution was ratified. Within our borders, however, to keep states honest, the Founders instituted _competitive_ federalism, whereby states compete for the allegiance of citizens, and it’s largely worked as states with high taxes and excessive regulations lose firms and people to states with low taxes and reasonable regulations. People vote with their feet, musg as in the Schengen Area. But the federal income tax plus the direct election of senators, both enacted as constitutional amendments and both promoted by progressives, unleashed _cooperative_ federalism whereby federal and state officials collude, using federal funds and enacting federal regulations, to undercut state autonomy and the discipline that competitive federalism was meant to secure.41



Earlier I said that you can’t understand the American Constitution unless you understand the theory behind it. Well what’s the theory behind the treaties that comprise the EU Constitution? Peace through trade and cooperation, yes — given Europe’s long history of wars. But beyond that, what? We’ve seen how a radical shift in the climate of ideas in America, especially in the direction of collectivism, has led, as many lonely voices predicted, to a reaction that today reflects a deeply divided nation, unable to restrain its appetite for “free” goods and services, even in the face of crushing debt. The divisions surfacing recently in Europe are no accident. People and peoples yearn to breathe free — in an earlier understanding of that idea. The balance needed to ensure that freedom may be difficult to find. But to discover it, as we celebrate Italy’s Constitution today and reflect on Italy’s place within the larger European Community, we could do no better than to appeal to the First Principles that are the very foundation of civilized nations. Thank you.
"
"

‘Tis the season for Internet shopping! Well, sort of. It’s actually the season for all kinds of shopping, with record‐​breaking consumer spending this year. Online holiday sales are expected to top $2.3 billion, but that’s a mere snowflake in the overall blizzard of retail activity. 



Electronic commerce is catching on, however, and with more than 140 million people online, it’s no longer the exclusive domain of computer nerds. Thus, _Newsweek_ recently declared that “it’s beginning to look a lot like an e‐​Christmas.” On the same day, _U.S. News & World Report_ observed that “shoppers from east to west seem determined to avoid traffic jams at the mall, long lines at the post office, and last‐​minute dashes to the supermarket.” 



But Internet shoppers are also avoiding something else: sales taxes. And thanks to the newly enacted Internet Tax Freedom Act, “tax‐​free” will be the rule for at least three years. 



That leaves some observers and state officials decidedly short on holiday cheer. For example, a recent article by technology commentator James Ledbetter denounced restrictions on Internet taxation as “unfair” to those who shop in stores. Similarly, a wire service story accused Internet vendors of enjoying a “free ride” and warned that local retailing could cease to exist. Not to be outdone, Congress appointed a commission earlier this month to study the issue.  




Without doubt, limiting states’ taxing authority leads to unequal taxation. Nevertheless, such limitations are a crucial component of American federalism. 



Of course, not all Internet purchases are tax‐​free, just those made across state lines. Out‐​of‐​state Web vendors — like their catalog cousins — aren’t required to collect sales taxes except in states where they maintain a physical presence. In other words, electronic commerce is treated exactly the same as mail‐​order business. And the Supreme Court has held that states have no authority to tax mail‐​order sales outside their borders absent explicit congressional authorization. Instead, most states ask consumers to pay a “use tax” in lieu of a sales tax on all purchases. Few consumers volunteer. 



At first glance, the pro‐​tax case sounds reasonable: why should identical items be taxed differently depending on how they’re purchased? Theoretically, they shouldn’t be. But in the real world, there are several reasons why allowing states to tax out‐​of‐​state electronic commerce is bad policy. 



First, there is no immediate danger of large revenue losses for traditional retailers or, by extension, for state tax authorities. Local stores cater to a customer’s desire for a hands‐​on experience, offer immediate gratification, don’t charge for shipping, and so will probably always dominate retailing. What’s more, shopping is for many people a pleasurable social experience that cannot be duplicated online. Thus, Internet sales won’t destroy “real” retailers, just as catalog sales haven’t. 



Empirical evidence supports that conclusion. In an era of almost no inflation, state budgets grew by 5 percent in FY97 and by more than 6 percent in FY98. Over the past four years, state tax collections have exceeded expectations by about $25 billion. It appears that there will be a sizable revenue windfall this year as well. With revenues pouring in so rapidly, it’s just not credible that electronic commerce is undermining state tax collections. 



Second, differentiated tax rates create healthy competition that helps keep local rates under control. For example, some residents of Manhattan drive to Delaware to avoid sales taxes — an option that has undoubtedly curbed the profligate fiscal habits of New York politicians. Electronic commerce serves as a similar safety valve that guards against excessive taxation. 



If states and localities feel compelled to tax their citizens more heavily, they still have the tools to do it. There will always be income taxes, property taxes, gas taxes, hotel taxes and the like. Perhaps what states really seek isn’t equity or revenue security but rather a new source of funds that doesn’t require voter approval. 



Finally, there is something inherently unsettling about states’ exercising legal authority outside their jurisdictions. By what right can New York force a firm in Florida to act as its tax collection agent? Even if it were constitutionally permissible, it would set a dangerous precedent with enormous potential for conflict. 



Without doubt, limiting states’ taxing authority leads to unequal taxation. Nevertheless, such limitations are a crucial component of American federalism. Absent those restraints, confiscatory state tax rates — which are the true injustice — would worsen. To improve their business climate, states should cut taxes, not scheme to collect more. 



For its part, Congress should stubbornly refuse to bow to state demands for new taxing authority. The Internet Tax Freedom Act was a good start in ensuring that traditional principles of remote commerce apply to the online world; Congress should consider making it permanent. 
"
"A Dutch government scientist has proposed building two mammoth dams to completely enclose the North Sea and protect an estimated 25 million Europeans from the consequences of rising sea levels as a result of global heating. Sjoerd Groeskamp, an oceanographer at the Royal Netherlands Institute for Sea Research, said a 475km dam between north Scotland and west Norway and another 160km one between west France and south-west England was “a possible solution”. In a paper to be published this month in the American Journal of Meteorology, Groeskamp and Joakim Kjellsson of the Geomar centre for ocean research in Kiel, Germany, say the idea is affordable and technically feasible – if intended more as “a warning of the immensity of the problem hanging over our heads”. Based on existing projects, the scientists estimate the cost of building a so-called North Sea Enclosure Dyke at between €250bn and €500bn. Spread over 20 years, the annual cost to the 14 countries that would be protected by it would amount to just over 0.1% of their combined GDP, they calculate. Groeskamp said it also appeared technically viable. The depth of the North Sea between France and England rarely exceeded 100 metres, he said, while between Scotland and Norway it averaged about 127 metres, peaking at just over 320 off the coast of Norway. “We are currently able to build fixed platforms in depths exceeding 500 metres, so such a dam seems feasible,” he said. International experts agreed that the plans looked theoretically viable. “I guess it depends on what timescale we’re thinking of,” said Hannah Cloke, a professor of hydrology at the University of Reading. “If you look back hundreds and hundreds of years, then we’ve made some significant adaptations to our landscape, and the Netherlands is an example of that … We can, as humans, do amazing things.” She added that it was “good that we’re thinking outside the box. I think it is really important that we keep thinking about these ideas, because the future looks very scary. If you look back into the 1940s in the UK, the Thames Barrier probably seemed equally ridiculous. It depends what happens in the next 20-30 years, how bad it gets, and then perhaps we will need something like this.” However, Cloke cautioned that a dam may not be the best use of the money. “Maybe we should be thinking about making populations resilient to flooding in different ways, and also think about what we can do to stop the climate getting worse – invest in keeping ourselves safe in the long term.” The authors acknowledge that over time, their project would eventually turn much of the North Sea into a vast tide-free freshwater lake, radically changing its ecosystem. “We estimated the construction costs by extrapolating the costs for large dams in South Korea,” Groeskamp said. “But in the final calculation, we must also take into account factors such as the loss of income from North Sea fishing, the increased costs for shipping across the North Sea, and the costs of gigantic pumps to transport all of the river water that currently flows into the North Sea to the other side of the dam.” However, the costs and consequences of doing nothing about rising sea levels would ultimately be “many times higher”, they warned. The project “makes it almost tangible what the consequences of rising sea levels will be”, Groeskamp said. “A rise of 10 metres by the year 2500 is predicted, according to the bleakest scenarios. This dam is therefore mainly a call to do something about climate change now. If we do nothing, this extreme dam might just be the only solution.” While experts have criticised Boris Johnson’s plan to build a 20-mile, £15bn bridge between Scotland and Northern Ireland as fraught with possibly insurmountable technical difficulties, the Dutch have been successfully protecting themselves against the sea with dykes for centuries. Their original 32km Enclosure Dyke, or Afsluitdijk, officially opened in 1933, sealed off a large North Sea inlet that became the freshwater IJsselmeer lake, and subsequently allowed the largest land reclamation project in history. The Zuiderzee works – of which the Afsluitdijk was a key part – and Delta works, a vast series of dams, sluices, locks and storm-surge barriers protecting the south-west Netherlands from the sea, are widely seen as marvels of hydraulic engineering. The Intergovernmental Panel on Climate Change predicts that sea levels will rise by 30cm-60cm by 2100, even if the Paris climate accord pledges are met. If emissions continue on their present trends, it foresees an 84cm rise by 2100 and up to 5.4 metres by 2300. The threat is particularly acute in the Netherlands, about a third of which lies below sea level. Last year, the Dutch government assembled a committee of specialists to monitor the threat closely and devise possible responses. The government has warned that while it expects its own defences to hold until about 2050, bolstering them further could take years: the past three decades of Dutch sea defence works are designed to cope with a rise of only 40cm. • This article was amended on 13 February 2020 to clarify that the estimated cost of the project as a percentage of the GDP of the countries affected is annual and spread over 20 years."
"What can be done to limit global warming to 1.5°C? A quick internet search offers a deluge of advice on how individuals can change their behaviour. Take public transport instead of the car or, for longer journeys, the train rather than fly. Eat less meat and more vegetables, pulses and grains, and don’t forget to turn off the light when leaving a room or the water when shampooing. The implication here is that the impetus for addressing climate change is on individual consumers. But can and should it really be the responsibility of individuals to limit global warming? On the face of it, we all contribute to global warming through the cumulative impact of our actions.  By changing consumption patterns on a large scale we might be able to influence  companies to change their production patterns to more sustainable methods. Some experts have argued that everyone (or at least those who can afford it) has a responsibility to limit global warming, even if each individual action is insufficient in itself to make a difference.  Yet there are at least two reasons why making it the duty of individuals to limit global warming is wrong. Climate change is a planetary-scale threat and, as such, requires planetary-scale reforms that can only be implemented by the world’s governments. Individuals can at most be responsible for their own behaviour, but governments have the power to implement legislation that compels industries and individuals to act sustainably. Although the power of consumers is strong, it pales in comparison to that of international corporations and only governments have the power to keep these interests in check. Usually, we regard governments as having a duty to protect citizens. So why is it that we allow them to skirt these responsibilities just because it is more convenient to encourage individual action? Asking individuals to bear the burden of global warming shifts the responsibilities from those who are meant to protect to those who are meant to be protected. We need to hold governments to their responsibilities first and foremost. A recent report found that just 100 companies are responsible for 71% of global emissions since 1988. Incredibly, a mere 25 corporations and state-owned entities were responsible for more than half of global industrial emissions in that same period. Most of these are coal and oil producing companies and include ExxonMobil, Shell, BP, Chevron, Gazprom, and the Saudi Arabian Oil Company. China leads the pack on the international stage with 14.3% of global greenhouse gas emissions due to its coal production and consumption.  If the fossil fuel industry and high polluting countries are not forced to change, we will be on course to increase global average temperatures by 4°C by the end of the century. If just a few companies and countries are responsible for so much of global greenhouse gas emissions, then why is our first response to blame individuals for their consumption patterns? It shouldn’t be – businesses and governments need to take responsibility for curbing industrial emissions.  


      Read more:
      Climate action must now focus on the global rich and their corporations


 Rather than rely on appeals to individual virtue, what can be done to hold governments and industries accountable?  Governments have the power to enact legislation which could regulate industries to remain within sustainable emission limits and adhere to environmental protection standards. Companies should be compelled to purchase emissions rights – the profits from which can be used to aid climate vulnerable communities.  Governments could also make renewable energy generation, from sources such as solar panels and wind turbines, affordable to all consumers through subsidies. Affordable and low-carbon mass transportation must replace emission-heavy means of travel, such as planes and cars. More must also be done by rich countries and powerful industries to support and empower poorer countries to mitigate and adapt to climate change. All of this is not to say that individuals cannot or should not do what they can to change their behaviour where possible. Every little contribution helps, and research shows that limiting meat consumption can be an effective step. The point is that failing to do so should not be considered morally blameworthy.  In particular, individuals living in poorer countries who have contributed almost nothing to climate change deserve the most support and the least guilt. They are neither the primary perpetrators of global warming nor the ones who have the power to enact the structural changes necessary for limiting global warming, which would have to involve holding powerful industries responsible.  While individuals may have a role to play, appealing to individual virtues for addressing climate change is something akin to victim-blaming because it shifts the burden from those who ought to act to those who are most likely to be affected by climate change. A far more just and effective approach would be to hold those who are responsible for climate change accountable for their actions."
"

What’s worse than a public policy debate that turns bitter and impolite? Well, for one, having the courts step into the marketplace of ideas to judge which side of a debate has the best “facts.” Yet that’s what Michael Mann has invited the D.C. court system to do. In response to some scathing criticism of his methodologies and an allegation of scientific misconduct, the author of the infamous “hockey stick” models of global warming—because they resemble the shape of a hockey stick, with temperatures rising drastically beginning in the 1900s—has taken the global climate change debate to a record low by suing the Competitive Enterprise Institute, National Review, and two individual commentators. The good Dr. Mann claims that some blogposts alleging his work to be “fraudulent” and “intellectually bogus” were libelous. The D.C. trial court rejected the defendants’ motion to dismiss this lawsuit, holding that their criticism could be taken as a provably false assertion of fact because the EPA, among other bodies, have approved of Mann’s methodologies. In essence, the court seems to cite a consensus as a means of censoring a minority view. The defendants have appealed to the D.C. Court of Appeals (the highest court in the District of Columbia). Cato has filed a brief, joined by three other think tanks, in which we urge the court to stay out of the business of refereeing scientific debates. We argue that the First Amendment demands that failing to leave room for the marketplace of ideas to operate stifles academic and scientific progress, and that judges are ill‐​suited to officiate policy disputes—as history has shown time and again. The lower court clearly got it wrong here—and there are numerous cases where courts have more judiciously treated similarly harsh assertions for what they really are: expressions of disagreement on public policy that, even if hyperbolic, are among the forms of speech most deserving of constitutional protection. The point in this appeal is that courts should not be coming up with new terms like “scientific fraud” to squeeze debate over issues impacting government policy into ordinary tort law. Dr. Mann is not like a corner butcher falsely accused of putting his thumb on the scale or mixing horsemeat into the ground beef. He is a vocal leader in a school of scientific thought that has had major impact on government policies. Public figures must not be allowed use the courts to muzzle their critics. Instead, as the U.S. Supreme Court has repeatedly taught, open public debate resolves these sorts of disputes. The court here should let that debate continue outside the judicial system.
"
"As with Brexit in Britain, the outcome of a referendum in Hamburg came as a blow to the establishment. By the slimmest of majorities, voters defied the status quo, and the party that did the best job convincing the public it would honour their will collected the political rewards. The dynamic unleashed by the 2013 plebiscite on buying the local utility grids back from private providers helps to explain why the city state is expected to buck national trends at this Sunday’s elections.  Polls predict the ruling centre-left Social Democrats (SPD), who are in decline across Germany, will hang on to their Hamburg stronghold with relative ease. Their junior coalition partner, the Green party, is expected to increase its share of the vote, leading to a strengthened mandate for the current power-sharing deal. The CDU of Angela Merkel is likely to drop into third place while the far-right Alternative für Deutschland may even struggle to make it above the 5% threshold for entry into the Hamburg parliament. Yet a centre-left success in the city state is more likely to be the result of local factors than national trends. In September 2013, 51% of the electorate voted to “re-municipalise” the local electricity and district heating networks, as proposed by grassroots initiative “Our Hamburg Our Grid”. And while the energy debate has only played a minor role in this year’s election campaign, it has increased the profile of the two governing parties. For one, Hamburg has managed to get de-privatisation done. In Berlin, by contrast, the senate announced its intention to buy back the district heating grid but has so far failed to drive the project through the courts. Hamburg sealed a €650m (£545m) deal last September. For Hamburg’s SPD – which campaigned against re-municipalisation, and is widely seen as more pro-business than elsewhere in the country – the buy-back has helped show it can also apply its famed pragmatism to more traditionally leftwing projects. The local party has been quicker to discover its green heart than other SPD branches in the country. And on Thursday, Hamburg’s SPD mayor, Peter Tschentscher, surprised his Green coalition partners by proposing to shut one coal unit at the city’s Moorburg power plant and convert the second to gas, in addition building a 100MW electrolyser to produce green hydrogen at the site. “If we hadn’t listened to voters, we would have been punished,” said Matthias Ederhof, an SPD candidate on the regional party list. “We are on the up because voters see us as competent.” The re-municipalised electricity grid, for which Hamburg paid the Swedish energy group Vattenfall €610m, has been making the city senate a healthy profit since 2017. The Greens are finding that having utility grids in public hands allows them to propose initiatives that would otherwise have sounded like pipe dreams. Hamburg, a city whose wealth has historically grown around its inland port, has in recent years turned into a busy laboratory for renewable energy schemes. Hamburg is now the first state in Germany to write a legally binding date into its constitution, the year 2030, not just for halving its carbon emissions from 1990 levels, but also for phasing out coal power for district heating altogether. The 250,000 households in the city which use district heating are still powered by coal stations in Wedel and Tiefstack. In the future, the Greens propose, Hamburg homes will be heated entirely from renewable sources such as waste incineration, biomass or solar power. There is a proposal to build a large heat storage system underneath the harbour, capturing excess heat produced in the summer for use during the winter by pumping hot salt water hundreds of meters deep into the soil. Even an old air raid bunker in the Wilhelmsburg district has been transformed into a so-called “energy bunker”, with a large heat reservoir. The city has the highest number of charging points for electric cars in Germany, more than Berlin or Munich. If Hamburg’s utility grid was still owned by Vattenfall, such experiments would have been unthinkable, said Christian Maass, a former state secretary for the environment and urban planning. “Private monopolies become a problem especially when the state doesn’t have the power and capacity to regulate them. In Hamburg, it therefore made more sense for the senate to hold the monopoly, because politicians can be held to account much more effectively than private companies,” said Maass, who now works as consultant on renewable energy. “Expectations on state-owned energy or heating grids are much more long-term and less orientated around maximising profit. If you want to shape the future of a city, that’s an immeasurable advantage.”"
"
Title with apologies to Crosby, Stills, Nash, and Young.
In my last post, part 77 of “How not to measure temperature” I pointed out that the National Weather Service in Upton NY has a weather station that is way out of compliance due to the way it is setup and the proximity to bias factors such as the parking lot.
There are thousands of weather stations across the USA, some run by various agencies. Often we’ll see them at national parks with interpretive displays. This one I encountered in Ely Nevada on my last road trip to finish the Nevada USHCN station surveys was part of an air quality and environmental monitoring program jointly run by the Department of Energy (DOE) and the Desert Research Institute (DRI).
It is an impressive station with multiple state of the art sensors, solar power, and a datalogger with a satellite uplink to DRI’s HQ. You can look at hourly data from the station at the CEMP DRI website here.
It is located about 2 miles northeast of town on government property, BLM land:
Ely, NV Weather Station operated by DOE/DRI -click for larger image
What is unique about this station is that it has an interpretive exhibit with live data readouts. I applaud DRI/DOE for doing this. Here are what the they look like closeup:

Click for larger images to read the text on the interpretive displays
As I said, I applaud DRI/DOE for doing this. Taking the effort to make such a wonderful educational display is a good use of taxpayer funds.
Except, that is, when they miss one critical detail.
Ely, NV Parking Lot Education Weather Station operated by DOE/DRI - click for larger image
Yes, the expensive satellite uplinked state of the art interpretive educational weather station is sited in the middle of two asphalt parking lots. One is for RV storage, the other is the parking lot for the Ely District Office for the Bureau of Land Management.
Here is the the view to the northeast of how the temperature sensor sees the BLM land:
What the temperature sensor sees - click for a larger image
Here is the aerial view of the placement:
Aerial view - Ely, NV Weather Station operated by DOE/DRI -click for larger image
With the parking lots on both sides being active with cars and RV’s, I would imagine that a fairly variable albedo exists, especially on weekends and holidays.
This wouldn’t be so bad if it was only an educational station with an interpretive exhibit, as one could explain it was placed here for the convenience of viewing and science really doesn’t advocate measuring the temperature of parking lots.
Except that this station is used for an active science project. How much of the other data measurements and calculations for such things as Tritium dispersal, gaseous pollutant volumes, etc are dependent on the temperature, humidity, and dewpoint data gathered here, all of which would be affected by the siting?
Contrast it to the ASOS station siting at the airport across the road. The ASOS is about 1000 feet NW of the southern runway intersection which you can see here in Google Maps

Normally ASOS stations are much more poorly sited than state of the art stations, but this example  illustrates how spending tens of thousands of dollars on hi-tech measurement gear can be undone by lack of simple planning.
Happy Thanksgiving everyone!


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ad5d699',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _The Current Wisdom_ is a series of monthly posts in which Senior Fellow Patrick J. Michaels reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press.   
  
_The Current Wisdom_ only comments on science appearing in the refereed, peer-reviewed literature, or that has been peer-screened prior to presentation at a scientific congress.   
  
**History to Repeat: Greenland’s Ice to Survive, United Nations to Continue Holiday Party**   
  
This year’s installment of the United Nations’ annual climate summit (technically known as the 16th meeting of the Conference of the Parties to the Framework Convention on Climate Change) has come and gone in Cancun. Nothing substantial came of it policy-wise; just the usual attempts by the developing world to shake down our already shaky economy in the name of climate change. News-wise probably the biggest story was that during the conference, Cancun broke an all time daily low temperature record. Last year’s confab in Copenhagen was pelted by snowstorms and subsumed in miserable cold. President Obama attended, failed to forge any meaningful agreement, and fled back to beat a rare Washington blizzard. He lost.   
  
But surely as every holiday season now includes one of these enormous jamborees, dire climate stories appeared daily. Polar bear cubs are endangered! Glaciers are melting!!   
  
Or so beat the largely overhyped drums, based upon this or that press release from Greenpeace or the World Wildlife Fund.   
  
And, of course, no one bothered to mention a blockbuster paper appearing in _Nature_ the day before the end of the Cancun confab, which reassures us that Greenland’s ice cap and glaciers are a lot more stable than alarmists would have us believe. That would include Al Gore, fond of his lurid maps showing the melting all of Greenland’s ice submerging Florida.   
  
Ain’t gonna happen.   




The disaster scenario goes like this: Summer temperatures in Greenland are warming, leading to increased melting and the formation of ephemeral lakes on the ice surface. This water eventually finds a crevasse and then a way down thousands of feet to the bottom of a glacier, where it lubricates the underlying surface, accelerating the seaward march of the ice. Increase the temperature even more and massive amounts deposit into the ocean by the year 2100, catastrophically raising sea levels.   
  
According to Christian Schoof of the University of British Columbia (UBC), “The conventional view has been that meltwater permeates the ice from the surface and pools under the base of the ice sheet….This water then serves as a lubricant between the glacier and the earth underneath it….”   
  
And, according to Schoof, that’s just not the way things work. A UBC press release about his _Nature_ article noted that he found that “a steady meltwater supply from gradual warming may in fact slow down the glacier flow, while sudden water input could cause glaciers to speed up and spread.”   
  
Indeed, Schoof finds that sudden water inputs, such as would occur with heavy rain, are responsible for glacial accelerations, but these last only one or a few days.   
  
The bottom line? A warming _climate_ has very little to do with accelerating ice flow, but _weather_ events do.   
  
How important is this? According to University of Leeds Professor Andrew Shepherd, who studies glaciers via satellite, “This study provides an elegant solution to one of the two key ice sheet instability problems” noted by the United Nations in their last (2007) climate compendium. “It turns out that, contrary to popular belief, Greenland ice sheet flow might not be accelerated by increased melting after all,” he added.   
  
I’m not so sure that those who hold the “popular belief” can explain why Greenland’s ice didn’t melt away thousands of years ago. For millennia, after the end of the last ice age (approximately 11,000 years ago) strong evidence indicates that the Eurasian arctic averaged nearly 13°F warmer in July than it is now.   
  
That’s because there are trees buried and preserved in the acidic Siberian tundra, and they can be carbon dated. Where there is no forest today—because it’s too cold in summer—there were trees, all the way to the Arctic Ocean and even on some of the remote Arctic islands that are bare today. And, back then, thanks to the remnants of continental ice, the Arctic Ocean was smaller and the North American and Eurasian landmasses extended further north.   
  
That work was by Glen MacDonald, from UCLA’s Geography Department. In his landmark 2000 paper in _Quaternary Research_ , he noted that the only way that the Arctic could become so warm is for there to be a massive incursion of warm water from the Atlantic Ocean. The only “gate” through which that can flow is the Greenland Strait, between Greenland and Scandinavia.   
  
So, Greenland had to have been warmer for several millennia, too.   
  
Now let’s do a little math to see if the “popular belief” about Greenland ever had any basis in reality.   
  
In 2009 University of Copenhagen’s B. M. Vinther and 13 coauthors published the definitive history of Greenland climate back to the ice age, studying ice cores taken over the entire landmass. An exceedingly conservative interpretation of their results is that Greenland was 1.5°C (2.7°F) warmer for the period from 5,000-9000 years ago, which is also the warm period in Eurasia that MacDonald detected. The integrated warming is given by multiplying the time (4,000 years) by the warming (1.5°), and works out (in Celsius) to 6,000 “degree-years.”   
  
Now let’s assume that our dreaded emissions of carbon dioxide spike the temperature there some 4°C. Since we cannot burn fossil fuel forever, let’s put this in over 200 years. That’s a pretty liberal estimate given that the temperature there still hasn’t exceeded values seen before in the 20th century. Anyway, we get 800 (4 x 200) degree-years.   
  
If the ice didn’t come tumbling off Greenland after 6,000 degree-years, how is it going to do so after only 800? The integrated warming of Greenland in the post-ice-age warming (referred to as the “climatic optimum” in textbooks published prior to global warming hysteria) is over seven _times_ what humans can accomplish in 200 years. Why do we even worry about this?   
  
So we can all sleep a bit better. Florida will survive. And, we can also rest assured that the UN will continue its outrageous holiday parties, accomplishing nothing, but living large. Next year’s is in Durban, South Africa, yet another remote warm spot hours of Jet-A away.   
  
References:   
  
MacDonald, G. M., et al., 2000. Holocene treeline history and climatic change across Northern Eurasia. _Quaternary Research_ **53** , 302-311.   
  
Schoof, C., 2010. Ice-sheet acceleration driven by melt supply variability. _Nature_ **468,** 803-805.   
  
Vinther, B.M., et al., 2009. Holocene thinning of the Greenland ice sheet. _Nature_ **461** , 385-388.


"
"
A Guest Post By Steve Goddard
In my most recent article in The Register, and also posted here on WUWT, I incorrectly speculated  that NSIDC graphs appeared to show less growth in Arctic ice  extent than had actually occurred.  My calculations were based on counting ice  pixels from Cryosphere Today maps.  Since then, I have had further discussions  with Dr. Walt Meier at NSIDC and William Chapman at Cryosphere Today, to try to  understand the source of the problem.   Dr. Meier has confirmed that counting  pixels provides a “good rough estimate” and that NSIDC teaches pixel counting to  CU students as a way to estimate ice extent.  William Chapman has confirmed that  the projection used in CT maps is very close to what it appears and to what I  had assumed it to be.  It is an astronaut’s view from about 5,000 miles above  the north pole.
What I have learned
In 2008, CT and NSIDC maps show excellent agreement – as can be seen  in this video which overlays an  August 14 NSIDC map on top of the August 14 CT map.  The borders of ice extent  are nearly identical in the two maps. (The videos show overlays of the two maps.)

The discrepancy occurred in August, 2007, when agreement between  NSIDC and CT was not so good.  The equivalent video from August 15, 2007  shows that the CT map was missing a significant amount of low concentration ice  on the Canada/Alaska side.  I have since confirmed from AMSR maps and NASA  satellite photos that the NSIDC map is probably more accurate than the CT  map.
The reason that CT provides their side-by-side image viewer is apparently to encourage visitors to make a visual comparison of two dates,  which is exactly what myself and others here did when we observed the  discrepancy vs. NSIDC graphs.  The human brain is quite good at making estimates  of relative areas from images, and pixel counting is nothing more than a way to  quantify what has already been observed.  Since writing The Register piece I  have made adjustments to the CT pixel counts for map distortion, and as I  expected that makes the discrepancy slightly larger.

Because CT maps showed less ice in 2007, the increase in 2008 ice extent appears to be much greater. There is little doubt now  that the NSIDC reported ice growth is absolutely correct.  But  wasn’t the ice supposed to shrink this year due to an excess of  “thin first-year ice?”  In May, NSIDC’s mean forecast (based on previous year’s melt) was that Arctic ice extent would be 13% lower  than last year.  (NSIDC has more recently posted on their web site some reasons  why they believe the May estimates didn’t work out.)

Click for a larger image
The next graph shows the NSIDC May  forecast superimposed on the AMSR graph of what has actually happened this year.  Ice extent has increased rather than decreased, and is not  tremendously lower than most other years this decade.   Note that the AMSR and  NSIDC graphs bottom out at 2Mkm2, not zero, which creates the visual impression  that ice extent is lower than it actually is.

Click for a larger image
Looking at the NSIDC map below, several things are apparent.
Firstly, the widely  publicised news stories predicting a possible “ice free  Arctic,” an “ice free North  Pole,” and a record  low extent aren’t likely to happen. (See this WUWT post with a quote from Dr. Meier) The North Pole is nearly as far away  from ice free water as in any other year.  In order to get to ice free water  from the pole, one would have to travel over 500 miles of ice to near Svalbard.   Lewis Pugh’s kayak trip, as reported by the BBC August 30 – “Swimmer aims to kayak to N  Pole” is going to be an impossible one.  That critical error didn’t stop the  BBC from highlighting it on the front page of their web site this weekend.
Secondly, while Arctic ice is well below the 30 year mean,  it is above expectations and nowhere near gone.  NSIDC graphs show Arctic ice  extent at greater than 70% of normal – hardly a six sigma event.  As of August  1, NSIDC was even considering a possible return to normal extent this summer.  If not for a few weeks of stormy Siberian  weather, the map (and story) would be quite different today.

There has been lots of press publicity  for predictions of an ice free Arctic by  the year 2013 or within the next  ten years.  Looking at the CT ice area graph below, that would clearly  require some major non-linear changes to the 30 year trend.

Click for a larger image
So how do we know if the trend is linear,  exponential or sinuous? As seen below, many long-term GISS Arctic temperature  records (except for stations close to the Bering Strait) show the last 30 years  as being the warming leg of a possibly cyclical pattern – with current  temperatures no warmer than 70 years ago.  Most stations close to the Bering  Strait show a sharp upwards shift of several degrees (corresponding to the PDO  shift) in 1976, and relatively flat temperatures since.  University of Alaska data shows that on average, state temperatures have been nearly flat since 1977.  In  fact, parts of Alaska are coming off one of their coldest  summers on record – possibly corresponding to another shift of the PDO to  pre-1976 conditions.
Is it possible that the 30 year  satellite record coincidentally represents only one leg of a waveform?   Greenland temperature records would hint at that.  If you examine only one leg  of a waveform, you will absolutely come to the wrong conclusion about the long  term behaviour – just as some did during the 1970s ice age panic.

Click for a larger image – original source image from NASA GISS

We know from official US Weather Bureau  records that it was possible to sail as far as 81N latitude in ice-free water, during a  similar warm period in 1922.  That is about as close to the pole as you can sail  now.  Temperatures around Spitzbergen, Norway warmed a remarkable 12C during a  few years prior to 1922.  From the November, 1922 Weather Bureau report – “He says that he first noted wanner conditions in 1915, that since that  time it has steadily gotten warmer, and that to-day the Arctic of  that region is not recognizable as the same region of 1865 to 1917.“



Is there cause for concern?   Perhaps.  But unfortunately much of the press coverage has been little more than  science fiction so far.  How do we separate the science from the fiction?  Dr.  Meier has graciously agreed to answer that question (and others) in my next  article.
One thing we can state with a degree of  certainty, is that there likely will be more multi-year ice in 2009 than there  was in 2008.  This is  because the 2008 melt season is ending with more ice area  than 2007.  Barring asteroid impact, after the winter freeze there will be (by  definition) more multi-year ice than what we started with this year.  Any ice  which survives the summer will be classified as multi-year ice in 2009.  If next  year is cool like this year, is it unreasonable to hypothesise that ice extent  will again increase?  Or are we on a non-linear trend which will lead to  ice-free summers and a collapsing Greenland ice sheet?  Hopefully Dr. Meier can  help sort this out for us.







			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9cc3e7b7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Every country in the world is failing to shield children’s health and their futures from intensifying ecological degradation, climate change and exploitative marketing practices, says a new report. The report says that despite dramatic improvements in survival, nutrition, and education over the past 20 years, “today’s children face an uncertain future”, with every child facing “existential threats”. “In 2015, the world’s countries agreed on the sustainable development goals (SDGs), yet nearly five years later, few countries have recorded much progress towards achieving them,” says the report by a commission of 40 child and adolescent health experts from around the world. “Climate change, ecological degradation, migrating populations, conflict, pervasive inequalities, and predatory commercial practices threaten the health and future of children in every country,” it says.  The commission, convened by the World Health Organization (WHO), the United Nations children’s agency, Unicef, and medical journal the Lancet, calls for radical changes to protect children’s health and futures from the intensifying climate emergency. It also highlights the threat of predatory commercial practices, linking children’s exposure to marketing of fast food and sugary drinks to an 11-fold increase in childhood obesity, from 11 million in 1975 to 124 million in 2016. The report includes an index of 180 countries that compares data on survival, wellbeing, health, education and nutrition; as well as sustainability, with a proxy for greenhouse gas emissions, and equity, or income gaps. Norway, South Korea, the Netherlands, France and Ireland are found to be the best countries for a child to flourish in his or her early years. The Central African Republic, Chad, Somalia, Niger, and Mali are the bottom five in the list, based on the same ranking. But when performance is compared taking per capita carbon emissions into account, Burundi, Chad and Somalia are best performers, while the US, Australia and Saudi Arabia are among the bottom 10 countries. “When authors took per capita CO2 emissions into account, the top countries [on the child flourishing ranking] trail behind: Norway ranked 156, the Republic of Korea 166, and the Netherlands 160,” the report says. “Each of the three emits 210% more CO2 per capita than their 2030 target.” “The only countries on track to beat CO2 emission per capita targets by 2030, while also performing fairly (within the top 70) on child flourishing measures are: Albania, Armenia, Grenada, Jordan, Moldova, Sri Lanka, Tunisia, Uruguay and Vietnam,” the report says. The UK is ranked among the top 10 countries when it comes to child flourishing, but placed 133rd on “delivering on emissions targets”; it is “currently on track to emit 115% more CO2 than its 2030 emissions target”. Experts behind the report agree that “while the poorest countries need to do more to support their children’s ability to live healthy lives, excessive carbon emissions – disproportionately from wealthier countries – threaten the future of all children”. Stefan Peterson, Unicef’s chief of health, said children living in the poorest countries are facing the brunt of a changing climate, despite having a tiny carbon footprint. “These children face enormous challenges to their health and wellbeing, and are also now at the greatest disadvantage due to the climate crisis,” he said. “We need sustainable gains in child health and development, which means that big carbon emitters need to reduce their emissions for all children to thrive, poor and rich.” The report says: “If global warming exceeds 4C by the year 2100 in line with current projections, this would lead to devastating health consequences for children, due to rising ocean levels, heatwaves, proliferation of diseases like malaria and dengue, and malnutrition.” Anthony Costello, professor of global health and sustainable development at University College London, said the commission was calling for a radical rethink on global child health. “Climate change threatens our children’s future so we must stop carbon emissions as soon as possible,” he told the Guardian. “Our new index shows that not a single country performed well on both child development and emissions indicators. “We also call for greater regulation of marketing of tobacco, alcohol, formula milk, sugar-sweetened beverages and gambling to children, and of social media companies which target children through secret algorithms and the inappropriate use of their personal data.”  The report says children are at risk from harmful marketing. “Evidence suggests that children in some countries see as many as 30,000 advertisements on television alone in a single year, while youth exposure to vaping (e-cigarettes) advertisements increased by more than 250% in the US over two years, reaching more than 24 million young people.” Industry self-regulation has failed, said Costello, adding that in Australia, for instance, “children and adolescent viewers were still exposed to 51 million alcohol ads during just one year of televised football, cricket and rugby”. “The reality could be much worse still,” he said. “We have few facts and figures about the huge expansion of social media advertising and algorithms aimed at our children.” The commission calls on governments to put measures in place “to ensure children receive their rights and entitlements now and a liveable planet in the years to come”. “We live in an era like no other. Our children face a future of great opportunity, but they stand on the precipice of a climate crisis … our challenge is great and we seem to be paralysed,” it says."
"
Share this...FacebookTwitter
For Europe, the huge welcoming applause Palin got after being introduced just before her speech was loud thunder accompanied by tectonic tremors. The signs for catastrophic (political) climate change are clearer than ever.

UPDATED 8/29 12.15 p.m.This blog is supposed to focus on climate and energy, and not politics. But the future of both issues are tangled up in politics, and so it is unavoidable.
Make no mistake about it. The ruling elitists in Europe and Germany are about as far from America’s foundational principles as they have ever been. Looking at the German media headlines yesterday, Beck’s rally was labelled as a gathering of “ultra-conservatives”, “religious right”, “God-fearing, gun-toting Americans”-patriotic folks in T-shirts and caps who chant “USA! USA!” and “God bless America!”. It’s just so unsophisticated – so say the euro-elitists through their noses. Europe’s elite media are marginalising and villainizing American conservatism. Call it the Cold War of the 21st Century. That’s the reality.
The euro-media of course would never broadcast the speeches, fearing their listeners might draw the “wrong conclusions”. So the media select the pictures they think their viewers ought to see and deliver pre-packaged thoughts and commentary to tell them how to think. It’s the responsible way of informing citizens.
Europe’s spite for original American ideals is deep.
Should someone like Palin take the Presidency in 2012, then prepare for headlines from Europe that normally would be reserved for history’s worst dictators. It’ll be “freeze it, personalise it and polarize it” in über-doses. In secular environmental Europe, things like Christianity, God, freedom, small government, life, capitalism are extreme, and thus talk about such things must be reigned in and controlled by the heavy hand of the state and media.
America itself is deeply divided, and it’s going to stay that way. And there’s no question which side Europe’s elites in media and government align themselves with. It’s the believers of the USA’s Constitution, i.e. conservatism, against the rest of the world. It really is that precarious. Those who believe in the founding principles of America have very few allies left in the world.
The differences between the American right and European left (the American left is the European left) are now far beyond reconciliation. The two sides could not be more polarised. What’s the way out? What can be done when reconciliation has no chance? The American right, the believers of its Constitutional principles, are alone on the globe. Do you compromise? Do you try to change the mind of the rest of the world? Conflict and uprising seem to be pre-programmed.
Although American conservatism appears to be making a comeback now, the reality is that it is completely surrounded on all fronts, almost hopelessly outnumbered, with very few political weapons. The NGOs, the UN, the elite media, academia, Hollywood, Russia, China, Europe, the Third World, the “environmentalists”, the billionaires, the special interests, the powerful US Left and all dictators are arrayed against American conservatism. People are to be ruled by governments, and not vice versa.
As President, who could someone like Palin possibly find as an ally? Other than Poland, the Czech Republic, Israel and Georgia, you’d be hard pressed to name one that’s influential. I’m not saying Palin should not be elected for that reason, I’m just telling you what the world is like for the last bastion of American ideals.
The odds are very long.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterGuest writer Ed Caryl, author of One Of Our Hemispheres Is Missing and A Light In Siberia, now brings us his latest essay.
——————————————————————————————————————–
You Want Me To Believe What?
By Ed Caryl
The proponents of Anthropogenic Global Warming claim that man’s use of fossil fuels has released extra carbon dioxide (CO2) into the atmosphere, and that the extra CO2 is warming the earth catastrophically due to greenhouse effects, causing great “disruption” in climate.
There are many parts to this hypothesis. The statement starts out with a truth, and then gets hazier as it progresses, each step suggesting an increasing level of calamity, but a decreasing level of believability. So let us look at the claims one by one:

Man’s use of fossil fuels has released extra CO2 into the atmosphere.

Stipulated. The increase since the dawn of the industrial age is nearly 100 ppm.

The earth is warming in a catastrophic way.

Since the Maunder Minimum, the earth has warmed by perhaps 1°C. There is good evidence that any measurement of more warming than that has been tampered with or subject to confirmation bias. Only warming in the last 100 years, and in particular, in the last 50, can possibly be due to greenhouse effects. Supposedly, any AGW will be seen first in the Arctic. Yet, many Arctic weather stations show no warming.
The warming is due to greenhouse effects
Knut Ångstrom. Source: http://www.angstrom.uu.se/historia.php
There are nearly as many numbers cited for what a doubling of CO2 will do as there are scientists working in the field. The most often cited expert on the subject is Svante Arrhenius, even though he was a physical chemist, not an atmospheric scientist, lived and worked a hundred years ago, and considered atmospheric science a hobby. He published the first numbers in 1896, 4.7 to 6°C. These numbers were criticized by Knut Ångstrom (one of the first true atmospheric scientists) in 1900 as being much too high. Later, in 1906, Arrhenius adjusted that number downward to 1.6°C. A hundred years ago, there was no consensus, even in one man’s head. Yet today, Arrhenius’ first numbers are the ones most often cited, and Ångstrom’s criticism is forgotten.
The value is estimated by the IPCC Fourth Assessment Report (AR4) as likely to be in the range 2 to 4.5°C with a best estimate of about 3°C, but is very unlikely to be less than 1.5°C. Values substantially higher than 4.5°C cannot be excluded, but agreement of models with observations is not as good for those values.[1]
Other estimates range from 0.4°C to as high as 10°C. The later number seems obviously too high, given the current lack of warming, and even the IPCC seems to agree. Another, more complete list of estimates is here. Given the amount of confirmation bias among the workers in this field, it is surprising that the average estimate is still less than 3°C. Arrhenius’ 1906 number, 1.6°C, may end up closest to the truth.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The point here is that there is wide disagreement on what the affect of doubling CO2 will do. The numbers cover a range of more than 20. None of these numbers can be more than guesses, and they could all be wrong.
Oh, and other tiny points: Who says CO2 is going to double? And why?
And what is the role of water? Water vapor is a greenhouse gas also. And water, unlike CO2, exists in all three phases. As water changes phase, it takes up and gives up heat. I suspect that Dr. Michael Mann has never seen a southwest desert thunderstorm. Under those towering thunderheads, the temperature can drop from 40°C to 10°C,  in just a few minutes, accompanied by heavy rain and hail. Cubic miles of hot air rise in those clouds, expanding and cooling as it rises. The rising air gives up heat to condensing water droplets, and then more heat to fusing hailstones. All that heat is transferred ultimately to the top of the clouds to be radiated to space. This goes on 24/7, all around the world. Is this factored into the climate models?
AGW will cause great climate disruption
Here, the proof is in. It’s not happening! This is the core of the whole debate. James Hansen predicted in 1988 that the West Side Highway in New York City would be under water by 2008. The Battery tide gauge shows a two-inch rise, and the rise has been linear since the gauge was installed in 1856. Worldwide sea levels show the same modest rise at the same linear rate. If Michael Mann’s “hockey stick” temperature rise was true, we should see the same sharp rise in sea level. We do not.
Tornados – also not happening. The peak years were in the early 1970’s, and it has been downhill since.
Hurricanes? This is a bit tricky. Before the satellite era, hurricanes had to have been spotted by ship or plane. The Hurricane Hunter aircraft beginning in WWII covered ocean areas of interest to the military, but the Hurricane formation regions off the African coast were not watched until later. So we don’t have accurate counts until satellites watched everywhere, consistently. We do have some records of hurricanes that made landfall. Here is one long-term record for the Apalachee Bay, Florida. It shows a hurricane frequency peak 2500 years ago and another during the Medieval Warm Period. But it shows no increase in the present. Tropical cyclone formation is driven by sea-surface temperature, so it is reasonable to assume that tropical cyclone formation will be a function of ocean temperature cycles, such as the AMO and the Southern Oscillation. Currently, the Tropical Cyclone Energy is near the 30-year low. This is a point that Al Gore has given up, and he has removed this slide from his presentation.
What about drought, floods, heat waves, cold snaps, insect infestations and other Biblical plagues? All (except the first-born son one, unless you count the 10:10 video) have been mentioned, and all blamed on AGW. The problem is that at any given moment, somewhere in the world, a record is being broken. This is just statistics in action. Any noisy phenomenon, given enough time and space to act, will produce the occasional exceptional spike, but the record phenomenon have no pattern in time.
For a branch of science to have any validity, it must be testable. Tests of a science include: Does it make predictions that can be verified? Do those predictions match observations? So far, the predictions have not come to fruition.

The North Pole has not become ice-free
The South Pole ice is expanding
The icecaps at the poles are not collapsing. One iceberg doesn’t make a collapse.
The oceans are not flooding land anywhere.
The deserts are not expanding
The polar bears are doing just fine, thank you
So are the penguins, (except in South Africa, where they froze last winter).

So what is it again we’re supposed to believe?
And why?
Share this...FacebookTwitter "
nan
nan
"
Solar cycle 23 as seen from SOHO - click for larger image
Below is a note forwarded to me by John Sumption from Jan Janssens. For those who do not know him, Jan runs a very comphrehensive solar tracking website here.
Jan included the caveat:
This  topic’s sure to start another heated discussion on the solar blogs 
So I’m happy to oblige by posting it here. Jansen makes some good points about the possible first month that cylce 24 spots exceed cycle 23 spots. But when you are in a deep minimum like this one, it is hard to pinpoint the transition, because next month may bring the reverse condition. He writes:
Prior  to August 2008, only 3 SC24-sunspot groups appeared. This was in January, April  and May. During these 3 months, SC23-activity was higher than SC24-activity.  Based on the NOAA-numbering, there were respectively (SC23 to SC24) 2 to 1, 2 to  1, and 4 to 1 sunspotgroups visible. 
In  August, there were no sunspotgroups numbered by NOAA. However, on 21-22 August  “something” was visible well enough to be seen by several observers and to  prompt the SIDC to give a (preliminary) non-zero sunspotnumber for those days. 
This group had a SC24-polarity but appeared on a moderate latitude of 15  degrees. Based on previous cycle transits, it is not unusual that some “early”  new cycle groups appear this low. If one considers this as a sunspotgroup and  belonging to SC24, then August was the month during which SC24-activity  outnumbered SC23 activity. 
However, if one adheres strictly to the NOAA-numbering, then September  ***might*** be that month. I stress “might”, because -unless some group appears  tomorrow or tuesday- the score will still be 1 to 1: On September 11th, NOAA did  number an even tinier group than the August one, and it was a SC23 group (NOAA  1001). SC24-activity then wins on “points”, because the Wolfnumbers for 22-23  September produced by NOAA 1002 (SC24) were higher than the NOAA 1002  Wolfnumber. 
Last  but not least, I want to emphasize that SC24-activity will be considered higher  than SC23’s when its smoothed group (or Wolf) number exceeds that of the old  cycle. This might happen in the coming months (or whenever her Majesty the Sun  feels up to it 😉


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c69f706',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _The Current Wisdom_ is a series of monthly posts in which Senior Fellow Patrick J. Michaels reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press.   
  
_The Current Wisdom_ only comments on science appearing in the refereed, peer-reviewed literature, or that has been peer-screened prior to presentation at a scientific congress.   
  
**  
Better Model, Less Warming**   
  
Bet you haven’t seen this one on TV: A newer, more sophisticated climate model has lost more than 25% of its predicted warming! You can bet that if it had predicted that much more warming it would have made the local paper. __   
  
The change resulted from a more realistic simulation of the way clouds work, resulting in a major reduction in the model’s “climate sensitivity,” which is the amount of warming predicted for a doubling of the concentration of atmospheric carbon dioxide over what it was prior to the industrial revolution.   
  
Prior to the modern era, atmospheric carbon dioxide concentrations, as measured in air trapped in ice in the high latitudes (which can be dated year-by-year) was pretty constant, around 280 parts per million (ppm). No wonder CO2 is called a “trace gas”—there really is not much of it around.   




The current concentration is pushing about 390 ppm, an increase of about 40% in 250 years. This is a pretty good indicator of the amount of “forcing” or warming pressure that we are exerting on the atmosphere. Yes, there are other global warming gases going up, like the chlorofluorocarbons (refrigerants now banned by treaty), but the modern climate religion is that these are pretty much being cancelled by reflective “aerosol” compounds that go in the air along with the combustion of fossil fuels, mainly coal.   
  
Most projections have carbon dioxide doubling to a nominal 600 ppm somewhere in the second half of this century, absent no major technological changes (which history tells us is a very shaky assumption). But the “sensitivity” is not reached as soon as we hit the doubling, thanks to the fact that it takes a lot of time to warm the ocean (like it takes a lot of time to warm up a big pot of water with a small burner).   
  
So the “sensitivity” is much closer to the temperature rise that a model projects about 100 years from now – assuming (again, shakily) that we ultimately switch to power sources that don’t release dreaded CO2 into the atmosphere somewhere around the time its concentration doubles.   
  
The bottom line is that lower sensitivity means less future warming as a result of anthropogenic greenhouse gas emissions. So our advice… keep on working on the models, eventually, they may actually arrive at something close puny rate of warming that is being observed   
  
At any rate, improvements to the Japanese-developed Model for Interdisciplinary Research on Climate (MIROC) are the topic of a new paper by Masahiro Watanabe and colleagues in the current issue of the _Journal of Climate_. This modeling group has been working on a new version of their model (MIROC5) to be used in the upcoming 5th Assessment Report of the United Nations’ Intergovernmental Panel on Climate Change, due in late 2013. Two incarnations of the previous version (MIROC3.2) were included in the IPCC’s 4th Assessment Report (2007) and contribute to the IPCC “consensus” of global warming projections.   
  
The high resolution version (MIROC3.2(hires)) was quite a doozy – responsible for far and away the greatest projected global temperature rise (see Figure 1). And the medium resolution model (MIROC3.2(medres)) is among the Top 5 warmest models. Together, the two MIROC models undoubtedly act to increase the overall model ensemble mean warming projection and expand the top end of the “likely” range of temperature rise. 



FIGURE 1





Global temperature projections under the “midrange” scenario for greenhouse-gas emissions produced by the IPCC’s collection of climate models. The MIROC high resolution model (MIROC3.2(hires)) is clearly the hottest one, and the medium range one isn’t very far behind.   
  
The reason that the MIROC3.2 versions produce so much warming is that their sensitivity is very high, with the high-resolution at 4.3°C (7.7°F) and the medium-resolution at 4.0°C (7.2°F). These sensitivities are very near the high end of the distribution of climate sensitivities from the IPCC’s collection of models (see Figure 2). 



FIGURE 2





Equilibrium climate sensitivities of the models used in the IPCC AR4 (with the exception of the MIROC5). The MIROC3.2 sensitivities are highlighted in red and lie near the upper und of the collection of model sensitivities. The new, improved, MIROC5, which was not included in the IPCC AR4, is highlighted in magenta, and lies near the low end of the model climate sensitivities (data from IPCC Fourth Assessment Report, Table 8.2 and Watanabe et al., 2010).   
  
Note that the highest sensitivity is not necessarily in the hottest model, as observed warming is dependent upon how the model deals with the slowness of the oceans to warm.   
  
The situation is vastly different in the new MIROC5 model. Watanabe _et al_. report that the climate sensitivity is now 2.6°C (4.7°F) – more than 25% less than in the previous version on the model.[1] If the MIROC5 had been included in the IPCC’s AR4 collection of models, its climate sensitivity of 2.6°C would have been found near the low end of the distribution (see Figure 2), rather than pushing the high extreme as MIROC3.2 did.   
  
And to what do we owe this large decline in the modeled climate sensitivity? According to Watanabe _et al._ , a vastly improved handling of cloud processes involving “a prognostic treatment for the cloud water and ice mixing ratio, as well as the cloud fraction, considering both warm and cold rain processes.” In fact, the improved cloud scheme—which produces clouds which compare more favorably with satellite observations—projects that under a warming climate low altitude clouds _become a negative feedback_ rather than acting as positive feedback as the old version of the model projected.[2] Instead of enhancing the CO2-induced warming, low clouds are now projected to retard it.   
  
Here is how Watanabe _et al_. describe their results: 



A new version of the global climate model MIROC was developed for better simulation of the mean climate, variability, and climate change due to anthropogenic radiative forcing….   
  
MIROC5 reveals an equilibrium climate sensitivity of 2.6K, which is 1K lower than that in MIROC3.2(medres).... This is probably because in the two versions, the response of low clouds to an increasing concentration of CO2 is opposite; that is, low clouds decrease (increase) at low latitudes in MIROC3.2(medres) (MIROC5).[3]



Is the new MIROC model perfect? Certainly not. But is it better than the old one? It seems quite likely. And the net result of the model improvements is that the climate sensitivity and therefore the warming projections (and resultant impacts) have been significantly lowered. And much of this lowering comes as the handling of cloud processes—still among the most uncertain of climate processes—is improved upon. No doubt such improvements will continue into the future as both our scientific understanding and our computational abilities increase.   
  
Will this lead to an even greater reduction in climate sensitivity and projected temperature rise? There are many folks out there (including this author) that believe this is a very distinct possibility, given that observed warming in recent decades is clearly beneath the average predicted by climate models. Stay tuned!   
  
References:   
  
Intergovernmental Panel on Climate Change, 2007. Fourth Assessment Report, Working Group 1 report, available at http://www.ipcc.ch.&#13;  
  
Watanabe, M., et al., 2010. Improved climate simulation by MIROC5: Mean states, variability, and climate sensitivity. _Journal of Climate_ , **23** , 6312-6335.   

"
"The UK’s accounting watchdog has launched a major review into whether companies and their auditors are adequately reflecting the financial risks of the climate crisis in their accounts. The Financial Reporting Council, which sets reporting standards for all listed companies in the UK, plans to use the review to make sure companies are being clear with investors about their exposure to climate risks. The review could lead to tougher disclosure rules for UK listed companies and more scrutiny on the work of accounting firms – including KPMG, EY, Deloitte and PwC – in helping investors to identify climate risks. Sir Jon Thompson, the FRC’s chief executive, said company reports and accounts were “essential to understanding how the corporate world is responding to the challenge of climate change”. “Not only do boards of UK companies have a responsibility to report their impact on the environment and the risks of climate change to their business, but investors expect them to operate sustainably,” he said. “Auditors have a responsibility to properly challenge management to assess and report the impact of climate change on their business.” The FRC will also examine whether companies have adopted the recommendations put forward by the the Taskforce on Climate-related Financial Disclosures (TCFD), which was set up to highlight the financial exposure of companies to the risk of climate chaos. Mark Carney, the outgoing Bank of England governor, warned companies late last year to use their next two annual financial reports to road-test how they represent climate risks in their accounts. He said that while many of the largest banks and energy companies had made progress in how they report their climate risks, this progress is “uneven across sectors”. Investors are calling for better climate disclosure standards as they face increasing pressure to prove they are investing responsibly. Under a new voluntary City code, which came into effect this year, pension funds and asset managers must publish annual reports showing they have taken environmental, social and governance (ESG) issues into account when investing. The FRC, which governs the code, said it would help “align the approach of the whole investment community”. The FRC plans to select a sample of company financial reports from across different industries to assess to what extent they comply with reporting requirements in relation to the climate emergency. It will investigate whether auditors have assigned enough resources to analysing climate risks and whether the impact of a climate crisis has been appropriately reflected in company reports and accounts. The first set of accounts to face the FRC’s scrutiny under its new investigation will be taken from this year, which will be published from early 2021."
"

 _Global Science Report_ _is a weekly feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
As economic heavyweights assembled for their annual summit held by the World Economic Forum (WEF) in Davos, Switzerland, they were greeted by a call for $700 billion/yr of increased spending out to the year 2030 to “to close the green investment gap worldwide, leading to sustainable economic growth that attains global climate change goals.” They were told that this goal can be reached through an additional $36 billion/yr investment from the world’s governments (on top of the $96 billion/yr currently spent) that will “spur up to US$ 570 billion in private capital needed to avoid devastating climate impacts on economy.”   
  
This call was made by the WEF’s own Green Growth Action Alliance as it released its first Green Investment Report at the outset of the Davos conference.   
  
The Green Growth Action Alliance justified the call for the extra spending this way:   




Such investments are urgently needed to avoid the potentially devastating impacts of climate change and extreme weather events as witnessed in many parts of the world in 2012. Scientists agree that extreme weather has become the “new norm” and comes at a huge, and rising, cost to the global economic system. Without further action, the world could see a rise in average global temperatures by 4ºC by the end of the century. According to scientists, this could lead to further devastating impacts, including extreme heat waves, more intense tropical storms, declining global food stocks and a sea-level rise affecting hundreds of millions of people.



Using a poor excuse to call for a bad idea doesn’t seem much like progress.   
  
The science of global warming re extreme events is hardly compelling. The data noise, generated from both natural processes and from other human influences, largely overwhelms any anthropogenic greenhouse effect signal in most cases.   
  
However, compelling evidence is emerging that the magnitude of the climate sensitivity—that is, how much warming we should expect from a doubling of atmospheric carbon dioxide concentration—has been overestimated. Even if there was good scientific evidence that higher temperatures lead to a more “extreme” climate (there’s just about as much evidence for the opposite), an overestimate of the sensitivity would lead to an overestimate of extremes.   
  
And these overestimates are being used by the Green Growth Action Alliance to oversell the need to do something about climate change.   
  
In fact, there are much more pressing needs.   




For example, according to the International Energy Agency (IEA), there are currently 1.3 billion people globally without access to electricity. The IEA recognizes that getting these folks hooked up is imperative for economic growth:   




Energy alone is not sufficient for creating the conditions for economic growth, but it is certainly necessary. It is impossible to operate a factory, run a shop, grow crops or deliver goods to consumers without using some form of energy. Access to electricity is particularly crucial to human development as electricity is, in practice, indispensable for certain basic activities, such as lighting, refrigeration and the running of household appliances, and cannot easily be replaced by other forms of energy. Individuals’ access to electricity is one of the most clear and un-distorted indication of a country’s energy poverty status.



It would seem to us, that getting electricity to those without is a better way to achieve the Green Growth Action Alliance’s goal of “driving development and well-being” than is “reducing greenhouse gas emissions.”   
  
And the best way to get (cheap, reliable) electricity to large numbers of people is through electricity systems that are powered by greenhouse gas-emitting fossil fuels. This is not to say that there are not instances where boutique energy sources such as solar may provide a better solution, but just that those instances are minor compared to the magnitude of the task—which makes doubly bad Green Growth Action Alliance advice to shift substantial capital from fossil fuel projects to help fund its green solutions.   
  
The bottom line is this: Fossil-fuel energy leads to more people with electricity which leads to more economic growth which leads to richer, more stable, more resilient and more environmentally-friendly societies with greater wellbeing.   
  
Don’t get us wrong, we are all for market-driven innovation, but in Davos, the urgency for such innovation is being overhyped, and the situation is made worse by the urging for public sector spending in order to fuel it.


"
"Jeff Bezos, the Amazon founder and Washington Post owner, announced on Monday that he was donating $10bn to save the Earth’s environment – barely a month after it was revealed Amazon threatened to fire employees who spoke out about the company’s role in the climate crisis. The new Bezos Earth Fund will start distributing the money this summer, the multi-billionaire said in an Instagram post to his 1.4 million followers. “I want to work alongside others both to amplify known ways and to explore new ways of fighting the devastating impact of climate change on this planet we all share,” Bezos said in the post. “This global initiative will fund scientists, activists, NGOs – any effort that offers a real possibility to help preserve and protect the natural world. We can save Earth. It’s going to take a collective effort from big companies, small companies, nation states, global organizations and individuals.” The announcement appears to contradict Amazon’s actions towards employees speaking publicly about the climate crisis. In January, the Guardian revealed that several workers who called for stronger climate action by the company were warned to be quiet or face dismissal. Bezos, the world’s richest man with a personal net worth of $129.9bn according to Forbes, has clashed with US president Donald Trump, a frequent climate change denier, on many occasions, notably after the US withdrew from the Paris climate agreement. “Anybody today who is not acknowledging that climate change is real – that we humans are affecting the planet in a very significant and dangerous way – those people are not being reasonable,” Bezos told Amazon’s Smbhav summit for small and medium-sized businesses in India last month."
"
More Signs of the Apocalypse
From Medindia.com
Posted online: Friday, August 29, 2008 at 2:48:25 PM
Report Confirms Four Austrians Suffer Tick-borne Encephalitis from Cheese
Medical experts confirmed on Thursday that four people recently fell ill with tick-borne encephalitis (TBE) in western Austria after eating homemade goats’ cheese.
A shepherd in western Vorarlberg province, who had checked into hospital in July with flu-like symptoms, was found to have the illness following a blood test.
But the man said he had noticed no tick bites, the usual method of transmission, two experts from the Institute of Virology at Vienna Medical University wrote in an article published Thursday.
Doctors finally traced the cause of the illness to the cheese, which the shepherd had made from unpasteurised goat’s and cow’s milk on an isolated pasture at over 1,560-metre (5,120-feet) altitude.
Three other members of his family, who had not been on the pasture, also exhibited flu-like symptoms and headaches.
Further tests found that one of the goats, whose milk had been used to make the cheese, as well as other animals who had eaten leftovers, had developed TBE anti-bodies, meaning they had also been infected.
Ticks were believed until now to be found only below 1,350-metre altitude, but this may have changed due to global warming, the experts said.
Cases of TBE infections via dairy products were reported in recent years almost exclusively in Baltic countries.
Unfortunately, the article does not say just who the experts are.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d00014b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The Ukraine crisis caused relations between Russia and the EU to fall to their lowest point since the Cold War. But despite the bickering and outright conflicts, both still need each other: Europe relies on Russian gas to keep warm, and Russia in turn needs revenues. With winter on its way and capital flight from Russia reaching dangerous levels, the outlook should draw the EU and Russia back together. Gazprom is the only supplier of Russian natural gas to Europe. In 2013, it exported 161.5 billion cubic meters (bcm) of gas to the rest of the continent (Germany, Turkey and Italy were the biggest recipients). That works out to around 30% of Europe’s consumer market. This in turn means European consumers are responsible for around 40% of Gazprom’s annual revenues.  Russia currently exports gas to Europe through five routes: via Ukraine (supplying Slovakia, the Czech Republic, Hungary, and feeding into western Europe, as well as to south-eastern Europe and Turkey); via Belarus to the Baltic states, Poland and Germany; via the Blue Stream pipeline directly into Turkey; and via Nord Stream directly to Germany and Finland. Both parties are interdependent and should pull together an action plan for cooperation to gain from their relationship. However, dependencies vary from country to country. Most nations in eastern Europe rely heavily on Russian gas, while others, such as Spain, Portugal, Ireland or Denmark, don’t import any at all. The most precarious countries are those which rely totally on imports of gas from Russia specifically via Ukraine. Bulgaria, for instance, is particularly at risk. It has very small conventional gas reserves (5bcm per 2010 statistics) and imports 94% of its domestic consumption, it has very little storage capacity, and relies heavily on Russian gas imported through Ukraine. Moldova is highly vulnerable too. After disruptions to the supply of gas from Russia in 2006 and 2009, the EU aimed to wean itself off Russian gas. Connections between national gas grids were put in place, giving countries a back-up plan in case of further disruption.  New suppliers were also sought to replace Russia. In 2013 Europe imported 48.5 bcm of natural gas from North Africa, 13 bcm from Iran and Azerbaijan, and 46 bcm in liquefied natural gas (LNG), mainly from Qatar.  But to put these numbers in context, this year alone Gazprom plans to sell 157 bcm to Europe. All of Europe’s various options for new energy supply have their own problems. The US’ large shale gas reserves could end up heating Europe homes, however LNG terminals must be built on both continents before large amounts can be shipped across the Atlantic. This is unlikely to happen in the near future. It means high investment costs, which would be passed on to European consumers and industries – the EU might not like Russia, but it still likes cheap energy. Gas from Africa and the eastern Mediterranean could help, but this would require billions to be spent on new drilling, refining, ports and security. A proposal to bring gas from the Middle East and the Caspian Sea region to Europe via Turkey will become a reality by 2018-2020. This southern corridor is another option, but again there are lots of logistical and security hurdles to overcome. In reality, Europe’s options are limited. A more diversified supply is still some way off. The EU’s supply contracts are long term, and this doesn’t allow to make short-term changes that may account for political factors – Russia is guaranteed stable gas sales until at least 2022. Individual countries can’t even do much about it by themselves as a 1998 EU directive restricts the power of national governments. Italy couldn’t suddenly sign a big deal with Namibia, for instance, without first going through EU channels.  Since the USSR broke up, Russia has wanted to diversify its own exports, reducing its reliance on gas sent to Europe through the Ukraine. Thanks to new pipelines across Belarus and under the Black Sea, this is already happening: in the 1990s around 90% of gas exports from Russia to Europe went through Ukraine Pipelines. This was down to 70% in 2007 and to 50% in 2013.  Once the South Stream pipeline through Bulgaria is built, reliance on exports through the Ukraine will decrease even further, but will still remain significant.  For Russia, the obvious long-term solution is to find alternative gas buyers elsewhere in the world. This explains the recent Russia-China energy deals. However, Europe still demands more energy than emerging economies and has better connections with Russia. For the moment, despite Russia’s best efforts, Europe is the main source of Gazprom’s revenue and profits. Russia’s efforts to diversify away from Euros and Dollars are unlikely to take off in the short run.  The EU and Russia therefore need to get together in the short and medium future – before new suppliers or supply routes can be developed – to safeguard energy security for Europe and revenues for Russia. This mutual interdependency could eventually draw Europe and Russia back together, despite all the political rhetoric."
"A supercomputer designed to improve extreme weather and climate forecasting is to receive £1.2bn from the UK government towards its development. The technology will be managed by the Met Office, with more sophisticated rainfall predictions and improving forecasting at airports among its aims.  Data collected by the powerful device will also be used to predict storms more accurately, select the most suitable locations for flood defences and forecast changes to the global climate. The supercomputer is expected to be the most advanced of its kind dedicated to weather and climate in the world. The Met Office’s current supercomputer, which is due to reach its end of life in late 2022, is among the world’s 50 most powerful computers, and contains enough storage to hold more than 100 years of HD films. “This investment will ultimately provide earlier, more accurate warning of severe weather, the information needed to build a more resilient world in a changing climate and help support the transition to a low-carbon economy across the UK,” said Prof Penny Endersby, the Met Office’s chief executive. “It will help the UK to continue to lead the field in weather and climate science and services, working collaboratively to ensure that the benefits of our work help government, the public and industry make better decisions to stay safe and thrive.” The government hopes the technology will help communities better prepare for weather disruption such as that from recent storms Dennis and Ciara. The supercomputer itself is expected to cost £854m, with remaining funds going towards investment in the Met Office’s observations network and programme offices, over a 10-year period from 2022 to 2032. “Over the last 30 years, new technologies have meant more accurate weather forecasting, with storms being predicted up to five days in advance,” said Alok Sharma, the business and energy secretary and Cop 26 president. “Come rain or shine, our significant investment for a new supercomputer will further speed up weather predictions, helping people be more prepared for weather disruption from planning travel journeys to deploying flood defences.”"
"
Share this...FacebookTwitterMeteorologist Karsten Brandt of donnerwetter.de projects more cold winters ahead. (Photo: Donnerwetter.de)
So forget the Met Office and PIK, who have proven themselves to be quite the laughing stocks. All that good money flowing into these institutions, and such rubbish coming out.
Meteorologist Karsten Brandt of German weather service company donnerwetter.de provides us with an outlook for 2011 and the next 10 years ahead. Read here (in German).
More cold winters over the next 10 years
Lately we’ve been hearing a lot about the North Atlantic Oscillation (NAO) going negative, which appears to be linked to quiet solar activity. (On the other hand we’ve been hearing from AGW true believers that the negative AO is caused by sea ice changes, which are caused by Arctic warming, which is caused by man-made trace CO2 emissions).
Clearly though, the NAO plays an important role in Europe’s weather. Knowing what the NAO will do for the next 10 years allows you to make a predictions for Europe’s climate ahead. Karsten Brandt writes:
It is even very probable that we will not only experience a very cold winter, but also in the coming 10 years every second winter will be too cold. Only 2 of 10 will be mild.
Dr Brandt is not some lone guy out there making this kind of blasphemous prediction. A large number of meteorologists and climatologists are projecting the same, e.g. like Accuweather’s Joe Bastardi and warmist climatologist Mojib Latif. Even the Potsdam Institute for Climate Impact Research is joining the chorus, but claiming the cold is due to warming.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




2011 in Germany about 1°C cooler 
Over the short-term Dr Brandt says that making a reasonable forecast for the next 2 months is not that hard, and that one can generate a pretty good idea of how the rest of the year will develop. Last year much of western Europe had a cooler year. What ‘s the outlook for Germany for 2011? Brandt writes:
Compared to the last 20 years, 2011 will turn out with a high probability to be too cold. Instead of 11°C in the west, or 10°C in the north and east, we expect only 8-10 °C, i.e. 1°C colder than the last years. Especially the cold start in the year and the suspected once again colder end of the year will pull the year’s temperature down.
Hopefully municipalities will procure the necessary wintertime snow-clearing equipment and stockpile sufficient amounts of sand and road salt so that next winter citizens will not have to endure the horrific traffic conditions that they are now experiencing over much of Europe. And you home centers ought to think about ordering snow shovels next fall.
But don’t despair, Brandt says the chances for a warm spring and a warm summer month aren’t bad.
Finally, here’s what the National Weather Service is showing:

Share this...FacebookTwitter "
"When you hear about businesses with a high environmental impact or activities with a high carbon footprint, you are probably more likely to imagine heavy machinery, engines and oil rather than hairdressing. Yet hairdressing, both as a sector and as an individual activity, can have a massive carbon footprint.  Hairdressing uses high levels of hot water, energy and chemicals. Similarly, in our homes, heating hot water is typically the most energy intensive activity. For the cost of a ten-minute shower that uses an electric immersion heater, you could leave a typical television on for 20 hours.  So while it helps to turn lights and appliances off, the real gains in terms of reducing energy usage are in slashing our use of hot water. A quarter of UK emissions are residential and, of those, the vast majority come from running hot water. The longer it runs and the hotter it is, the more energy intensive (and costly) it is.  Most people use too much shampoo and wash their hair too often. A daily routine of shampooing your hair twice followed by a wash out conditioner uses annually about 14,222 litres of water and 1252kWh of energy, costs about £245, and has a carbon footprint of 500kg of carbon dioxide equivalents (CO₂e).  On the other hand, if you shampoo your hair twice a week (supplementing that with a dry shampoo if needed) and use a leave-in conditioner, you will use annually just 613 litres of water and 55 kWh of energy, produce a carbon footprint of 25kg of CO₂e, and cost yourself about £27 a year.  Research has also revealed how shampoo can contribute to pollution. Maybe this in part explains why sales of shampoo have fallen over the past few years in the UK – with many people choosing to wash their hair less often.  Washing your hair less doesn’t just save you money, it’s also much better for your hair condition. It can also help to limit the ageing effects of over exposure to hot water and chemicals on your skin.  My latest research project looks at the issue of sustainability across the hairdressing sector. Not only is the hair sector a high user of resources, but hairdressers probably talk to more people than any other occupation – and are in a great position to pass on advice about lower resource hair care.  From speaking with hairdressers, it seems that ever since the episode of Blue Planet II in which David Attenborough explained how a whale mother was still carrying her dead baby which, it was claimed, had been poisoned by plastics (though scientists working on the show have confirmed there was no actual evidence to prove this) salons have been seeing a massive increase in clients wanting to know that their hairdresser is doing their bit.  Our research has found that many hairdressers are keen to make changes that are better for the environment. The opportunity to present their industry as part of the solution rather than part of the problem is very attractive to hairdressers, as it boosts their sense of professional identity and pride in offering a well informed service.  A large focus of the project has been on equipping hairdressers with the skills and knowledge required for them to talk to their clients about sustainable hair care. There are many products out there that are better for the environment, not because they have “organic” or “eco” on the label, but because they reduce the need for hot water.  Dry shampoo is a great example. It is fast, convenient, and great at festivals and on the move. It also makes hair easier to style, is cheap and avoids the need for any hot water. Similarly, leave-in conditioner avoids the need for an extra rinse and again makes hair easier to style. It is also fantastic at giving body to fine hair, and saves water, energy, money and time.  Our ecohair project, run in association with the Vocational Training Charitable Trust and the Hair and Beauty Industry Authority, provides a sustainable stylist certificate at no cost, once hairdressers have completed the training programme. The salon owner can also obtain a sustainable salon certificate to let customers know these things are important to their business.  Getting certified as a sustainable salon has numerous benefits, and not just in terms of reputation. Adopting the changes as part of the scheme saves the typical salon 286,000 litres of water, 24150 kWh of energy and £5,300 a year.  And with new research showing the increased threat of climate change and the need for urgent behavioural change, it is great that simple alterations to our hair care routines – and where we choose to get our hair cut (you can find sustainable salons here) – can make such a difference to the planet we call home."
"
Share this...FacebookTwitterAlthough the Potsdam Institute For Climate Impact Research (PIK) gives the impression that it is a climate research facility, it also appears to have become an institute for formulating novel economic policy.
H/t: reader Ike
The PIK, commissioned by the German Federal Ministry for the Environment, Nature Conservation and Nuclear Safety, has produced and released a NEW SYNTHESIS REPORT that claims Europe can revitalize its economy by tackling the climate challenge, namely by raising the European climate target for emissions reductions from 20% to 30%. The report is titled:

A New Growth Path for Europe – Generating Prosperity and Jobs in the Low Carbon Economy”
Tipping point to prosperity at 30%
The PIK seems to be claiming there is an economic tipping point to prosperity at 30% emissions reduction. The onlineDie Zeit writes on the new PIK report:
Europe should reduce its greenhouse gas emissions by 30% instead of only 20% by 2020, which is the current plan. This is how the continent could overcome its economic stagnation.
If they stick to the 20% target, ‘then it would be like someone stuck in a hole who is digging deeper’.”
Based on climate-economic models!
The PIK claims a 30% reduction by 2020 would lead to higher growth and increased employment. These projections are based on “new model results”. (PIK models have an incurable habit of producing exactly what the PIK wants to see).
In the coming decade, Europe will need to accept the challenge of increasing economic growth while reducing both unemployment and greenhouse gas emissions. New model results show that these three goals can actually reinforce one another.”
Yet, there must be something terribly wrong with their models because a slew of European governments have been recently forced to do just the exact opposite, due real-life economics, and scale back subsidies to money-losing  green energy sources – especially solar.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




But the PIK has never been deterred by the harsh truths of reality, and claims:
Clear policies associated with a decisive move to a 30% target can be doubly beneficial for the climate and the EU economy.”
Their “new model results” also say their new plan would:
• increase the growth rate of the European economy by up to 0.6% per year.
• create up to 6 million additional jobs Europe-wide.
• boost European investments from 18% to up to 22% of GDP.
• increase European GDP by up to $842 billion (2004 dollars).
• increase GDP by up to 6% both in the old (EU15) and new (EU12) member states.”
For the first time in the academic modelling field
The above projections all sound so rosy. So just exactly what kind of brilliant mastermind plan did the PIK use to produce such rosy projections? (Hang on to your chair!):
For the first time in the academic climate modeling field, the present study has taken a state-of-the-art model of climate economics and enhanced it along those lines. The enhanced model includes:
• the fact that investments depend on subjective expectations, not on correct previsions of whatever future possibilities may arise.
• the fact that higher investments trigger higher learning-by-doing, thereby reducing unit costs.
• the resulting existence of different possible equilibria with different growth paths.
The new simulations show that 30% is achievable and can be economically beneficial by shifting the European Economy into a new, more advantageous equilibrium – a path of low-carbon growth.”
I don’t know about you, but I’d be a little wary of the “first-time in the academic climate modeling field” point, especially in a field as complex as economics. Thinking that these things work without glitches after just one tune-up sets a new standard in naiveness.
Apparently I’m not the only one who’s palm over face on this. Even Germany’s greenie Environment Minister Norbert Röttgen is not touching this with a 10-foot pole. According to Die Zeit:
Even Environment Minister Norbert Röttgen has placed little emphasis on the presentation of the new study: Instead of travellingng to Brussels himself, he sent his secretary Katherina Reiche.”
Like the old promises of government central planning, PIK’s plan is a roadmap to a disaster.

Share this...FacebookTwitter "
"
Share this...FacebookTwitterYesterday I wrote about the latest issue of FOCUS news magazine HERE.
I just picked up the new issue, out today, and read it. It’s an unmistakable departure from usual doom and catastropheism, which had generated so much of the fear and urgency needed to justify the rush to reckless policy making and profiteering.
FOCUS shows that the urgency is undue and that it’s time to get back to rationality, and away from all the mass hysteria that has taken policymaking on a ride on the crazy train.
Part 1: It’s getting warmer – that’s good!
In its first part, FOCUS describes the Sahara region as it was thousands of years ago – an area rich in wildlife and plants where humans settled and even built a 500 meter long by 5 meter wide protective wall 1000 years before Christ. Then came a climate catastrophe (naturally of course), in the form of cooling and drying. The once green paradise dried up into a desert wasteland. Today the Sahara desert, thanks to warming, is greening up again. Says Stefan Kröpelin, geo-archaeologist of the University of Cologne, who has been researching the region for 30 years now:
At the southern edge, vegetation in most places has been moving northwards since the end of the 1980s. Global warming here has been a blessing. If the trend continues at this pace, the Sahara will be green again in a few hundred years.”
The Sahara is just one example of the advantages of global warming. FOCUS also writes:
More and more renowned scientists are saying climate change does not lead to only catastrophes, rather it also brings with it rich advantages for both man and nature. But this will hardly play a role in Cancun. Politicians, scientists and media are too fixated on the problems of the future – from sea level rise, to storms, to the spread of tropical diseases (see page 86).
Studies worldwide show that many of the widespread horror scenarios are baseless.”
FOCUS continues to believe the science underpinning the theory that global temperatures will rise 2 – 4°C by the end of the century, and this being due to man’s activities. Here, FOCUS naively ignores the impacts of oceanic and solar cycles. But even so, it has, at least in this article, truly departed from the planet-is-going-to-hell-in-a-hand-basket narrative. That’s huge progress for traditional German journalism – make no mistake about it.
And what does Stefan Kröpelin say about climate models for the future?
I trust the data from the earth’s history more than any climate model.”
Josef Reichholf, Professor Emeritus of Ecology and Evolutionary Biology at the University of Munich speaks on biodiversity.
The earth’s history shows that warm periods are characterized by high levels of biodiversity. In general, the rule is: the warmer it is, the more biodiversity you get.”
FOCUS then writes about the warm and cold cycles the earth has experienced, in particular the ice ages that have occurred every 100,000 years and the massive ice sheets that once covered many parts of the globe. This part really puts the earth and climate in the right perspective. Readers see that things have been far more extreme than the puny half-degree fluctuations we are biting our nails over today. The earth does change naturally, and often dramatically.
FOCUS also writes about the Holocene optimums and minimums, and the challenges and benefits that man derived from them. Climate has always been changing. The Vikings even settled in Greenland in the year 982, FOCUS points out.
FOCUS also puts CO2 in the spotlight and discovers that it is not that “climate-killing” gas everyone has been making it out to be. Indeed, the gas is actually a fertilizer for plants. It makes the planet greener. It boosts agricultural yields, which means more food to feed the world.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




CO2 is often called a climate poison or climate killer, but: ‘It is an essential building block for photosynthesis, and thus the basis for all life,’ says Hans-Joacheim Weigel, Director of the Johann Heinrich von Thünnen Institute for Biodiversity.”
Part 2: The 7 scourges of the end time
FOCUS then looks at the 7 climate scourges alleged will occur as the world ends, or something, and debunks them one by one. Here I think FOCUS did a good job – a must read. Here I summarize with just a few key words.
FOCUS (in a few words):
1. Islands will sink: “Satellite photos show they are not”
2. Disappearing glaciers. FOCUS writes: “not a new phenomena”
3. Melting of the ice caps: “Highly improbable – very high temperatures would be needed”
4. Gulf Stream collapse: “NASA shows it’s not true”
5. More storms: “Chris Landsea shows it won’t be so.”
6. Polar bears will die: “They are growing in number and are highly adaptable – no big problem.”
7. Tropical diseases will spread: “DDT ban was a cause. And Paul Reiter says it’s BS.”
Part 3: Who’s who in climate science?
One interesting section of the FOCUS report was the part on who’s who in climate science? They list 11 names:
1. Rejendra Pachauri: “calls to resign”
2. Michael Mann: “doubts on his methodology”
3. James Hansen: “warning of AGW since the 1980s”
4. John Christy, Atmospheric Sciences:
Analyzes climate data from satellites at the University of Alabama. Is against alarmist statements from other scientists who warn of catastrophic temperature increases and sea level rises, and considers climate protection measures unnecessary.”
5. Stephen McIntyre, mining specialist:
The Canadian analyzed Mann’s hockey stick curve together with economist Ross McKitrick, who both found deficiencies in the methodology, which put the curve’s shape into question, as well as the claim that never in the last 1000 years has it been warmer than today. McIntyre became known when Mann refused to reveal his data. He is active with his blog Climate Audit.”
6. Stefan Rahmstorf: “A lead author of the IPCC 4AR report and many publications”
7. Björn Lomborg: “Recommends adaptation”
8. Mojib Latif: “Projected a pause in global warming”
9. Richard Lindzen:
Researches at MIT and one of the most prolific skeptics. He says the earth is never in equilibrium, and for that reason natural changes such as ocean currents or atmospheric cycles can explain the warming. For this reason it is senseless to attempt to fight climate change.”
10. Nicholas Stern: “Inaction is more expensive than action.”
11. Paul Crutzen: “Advocates geo-engineering.”
So much for the ballyhooed consensus and settled science. This FOCUS article shows that the science is hotly disputed, and more importantly, that the catastrophe scenarios are hysteria, and that warmer climates would bring real benefits.
For German journalism, this to me represents a watershed event. This piece breaks a lot of taboos in Germany. The reactions indeed will be worth following. Expect hellfire and vitriol from the greenshirts.
I wish I had time to write more, but this I think is a good overview of the extensive 14-page FOCUS piece. Go out and buy it.
Share this...FacebookTwitter "
"The asteroid that wiped out the dinosaurs set off an intense heat wave that briefly boiled the Earth’s atmosphere – but it didn’t burn off all the plants. Humanity has not been unlucky enough to observe at first hand the effects of a large impact, so to investigate whether a massive asteroid would spark off a global wildfire we had to turn to the laboratory. We have modelled, for the first time, the heat generated by the impact and what it meant for the planet’s plants. Our research is published in the Journal of the Geological Society. This all happened 65m years ago at the end of the Cretaceous period, when dinosaurs still roamed the earth. Suddenly, between 60 and 80% of all living species became extinct. Until the 1980s, this catastrophic loss of life was a mystery, but then scientists found a clue – traces of the element iridium in rocks of this age. Iridium generally falls to Earth with extraterrestrial objects. This suggested a massive asteroid collided with the planet and that this could be responsible for the mass extinction. Ten years later, scientists found the 65m year-old, 180km wide, Chicxulub crater on the Yucatan Peninsula in Mexico, which finally provided the smoking gun that could explain the apparent chaos at the end of the Cretaceous era. A crater that wide suggests that an asteroid or comet approximately 10km wide hit the Earth.  The impact would have released a huge amount of energy – equivalent to more than a billion Hiroshima bombs. The asteroid itself was vaporised as it smashed into the Earth and in doing so vaporised and blasted out particles of the rock that it hit.  A huge glowing ball of hot rock and vapour rushed up from the impact site at huge speeds, ejecting it way above the atmosphere up into space. As it hit the cold of space it decelerated, cooled and rained back through the atmosphere, re-solidifying and forming tiny droplets of rock known as “spherules”.  As these spherules fell through the atmosphere they were subject to the same frictional-drag which causes space shuttles to become super-heated as they return to earth. This in turn meant as the particles rained down through the atmosphere they delivered a massive blast of heat to the ground. Scientists call this a “thermal pulse”. This heat pulse has widely been suggested to have ignited global wildfires and has been cited as a cause of the mass extinction. New computer modelling techniques have enabled us to generate better estimates of the heat pulse resulting from this impact. We found it wasn’t evenly distributed across the surface of the Earth. Areas close to the impact site experienced a strong but very short-lived pulse – reaching a peak heat flux of around 50kW/m2 (20 times higher than a human can tolerate) for around one and a half minutes. Further away, the maximum heat flux was lower – a peak of 20kw/m2 – but lasted much longer – up to seven and a half minutes. We teamed up earth scientists with fire safety engineers to investigate whether this heat would lead to a massive global wildfire. The heat pulse from the asteroid impact was recreated using state-of-the-art apparatus usually used to test the flammability of furnishings and materials. This provided, for the first time, the ability to test whether the heat pulse from the impact could have set fire to the world’s plants. Our research reveals that the short sharp blast of heat felt closer to the impact could not have ignited live plants.  However the longer drawn out pulse further away from the impact may have started fires in some locations, implying that localised fires may have occurred. But critically “global firestorms” were unlikely. This turns our understanding of the heat pulse on its head as its effects would have been greater further away from the impact. Earth scientists will have to reassess their understanding of the fossil record of life. Until now they have assumed the heat pulse was strongest closer to the crater, but now patterns of extinction and survival must be reinterpreted by considering a more severe heat pulse further away."
"
Share this...FacebookTwitterThe blogosphere is now filled wth Arctic sea ice projections and opinions, and why not? It’s a game that adds fun to the science. For example Steve Goddard at WUWT projects 2010 will finish above 5.5 million sq km.
My forecast remains unchanged. 5.5 million, finishing above 2009 and below 2006. Same as it has been since May.
August 22, 2010: Source: UI Atmos-Cryosphere
I’d be happy if that became true because it would be real sand in the gears of the death-spiral worshippers. But seriously I doubt it will keep above or at the 5.5 million mark. That’s a real long shot.

Maybe a different dataset is being used for this claim. The shot above shows there is still plenty of potential for melt.
Weeks ago I projected, using my crystal ball, that we would finish a bit below 5 million. That’s turning out to be a wee bit too pessimistic, but not too much though.
On August 22, Arctic sea ice according to JAXA was at 5.628 million sq km. That means only 128,000 sq km over the 5.5 million mark with still about 20 days left before reaching the average low point. That’s a lot of time.
August 22, 2010
Over the last 8 years we have seen that from August 22 until reaching the low point, Arctic sea ice has lost an average of 535,000 sq km.
The minimum loss for that period was 279,000 sq km in 2006 and the maximum was 837,000 in 2008. Over the last 4 years, the average was a loss of 582,000 for the period.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Taking the average of the last 4 years, and we get a 2010 minimum ice extent of 5.628 – 0.582 = 5.04 million sq km. So, forget the 5.5 million mark being the minimum. It aint gonna happen.
The Arctic is not projected to get very cold in the next two weeks, see here: http://wxmaps.org/pix/temp2.html.
The temperature above 80°N latitude is also staying stubbornly near the freezing point. So expect Arctic sea ice, using JAXA, to get down closer to the 5 million mark.

Source: Danish Meteorological Institute.
Yes, of course, there are lots of other factors, wind, blah blah, blah, but only 100,000 sq. km of melt over the next 20 days could only happen if Al Gore showed up at the North Pole.
UPDATE: Steve Goddard at WUWT has backed off his 5.5 million sq km projection. http://wattsupwiththat.com/2010/08/24/sewa-ice-news-arctic-mid-week-update/
 
Share this...FacebookTwitter "
"

The lead story on the June 29 MSNBC News was that there were terrible floods in the United States and–interspersed in the middle of the story–that global warming is going to be even worse than we thought. This was one‐​sided, emotional science reporting even worse than I thought possible. 



The story was rooted in a recent study by Tom Wigley, introduced as “a respected climatologist.” Wigley’s study was financed by the Pew Foundation, which is running a multi‐​million‐​dollar campaign to hype global warming. 



Wigley says that sulfate aerosols will be legislated out of existence faster than previously thought. He champions the theory that sulfates reflect away the sun’s radiation, conveniently explaining why the planet has warmed so little despite the claims of warming doomsayers and their computers. 



Without sulfate aerosols, computer models indicate our hemisphere should have already warmed about 2.3 degrees Celsius as a result of the greenhouse effect. The observed warming this century is a scant 0.65 degrees. If the sulfate hypothesis fails, the argument devolves into what the “skeptics” have said for decades: the earth simply isn’t going to warm all that much. 



Having held a doctorate in climatology for two decades, I feel confident in saying that every one of my colleagues who has expressed an opinion to me dislikes Wigley, mainly because he seems arrogantly dismissive of some facts when they get in the way of his theories. He actively discourages the airing of points of view that conflict with his. 



In October 1994, at a global warming meeting called by Rep. John Dingell (D‐​Mich.), Wigley was confronted with the reality that satellites had found no warming. He merely waved his hands and said, “Oh come now, that’s just the satellite data.” Oh come now, Tom, it’s just the only global measure of temperature that exists! 



The sulfate aerosol theory is politically correct, because some explanation is needed for why the early climate models flopped so badly. They served as the basis for the United Nations climate treaty, recently modified in Kyoto to force the United States to reduce its greenhouse gas emissions by as much as 45 percent in 8.5 years. Even though the treaty would devastate the U.S. economy, Wigley thinks it is not enough, saying we need “nine more Kyotos.” 



Needless to say, Tom is very big with the folks like the Pew Foundation and the United Nations, both of which seem to care not a whit about the destruction of the American economic miracle as long as dreaded global warming is banished. 



He co‐​authored a famous 1996 paper that showed that, from 1963 through 1987, the behavior of the atmosphere did in fact increasingly resemble that of one in which greenhouse warming was being counteracted by sulfate cooling. But the data available for his study actually began in 1957 and ended in 1995. When all of the data on the critical region of the atmosphere was looked at, there was no change whatsoever! Wigley has never given a satisfactory explanation of why he ignored all the data. Should he write a response to this paper, I reserve the right to reply. 



Wigley does not like to be confronted with this in public. Resources for the Future, a Washington outfit big in the global warming game, recently held a forum featuring Wigley but none of his critics. When pressed, Wigley said that his critics did not belong on the stage with him because they generally did not publish their work in peer‐​reviewed literature. Hogwash. Wigley’s critics are among the most published in the business. 



Arizona State’s Robert Balling may be the most prolific living climatologist. Only a handful of papers have mathematically searched for predicted greenhouse signals–and several are mine. The five most prominent critics have published nearly a thousand articles, mostly in peer‐​reviewed journals. Critics include a member of the National Academy of Sciences, the former head of the American Physical Society, heads of major research laboratories and former presidents of other scientific and professional societies. The fact that they even appear in the literature at all, given the thousands of people who lose some of the $2.1 billion that we spend each year on global change research if it all goes kerblooey, is testimony to the cogency of their arguments. Maybe that’s why Wigley doesn’t want any opposition. 



Sulfates don’t do a good job of explaining the failure of the models noted above. NASA scientist James Hansen, who essentially ignited the greenhouse issue 11 years ago with his flamboyant congressional testimony, has become very skeptical about sulfates. University of Washington scientist Peter Hobbs found that sulfates off the East Coast are overwhelmed in their own plume by black carbon particles that absorb radiation and cancel sulfate cooling. And throughout the eastern United States, where sulfates have been in decline for the last 30 years, the temperature hasn’t budged during the entire century. 



Any or all of these observations could have been offered by “respected climatologists” if MSNBC had bothered to do a little legwork. The real “story behind the story” is why they didn’t.
"
"

And yet to play out, let’s also not forget Al Gore’s 2008 prediction: “Entire north polar ice cap will be gone in 5 years”
-Anthony
By Dennis  Avery  in the Canada Free Press
“2008 will be the hottest year in a  century:” The Old Farmers’ Almanac, September 11, 2008, Hurricanes, Arctic  Ice, Coral, Drinking water, Aspen skiing
We’re now well into the earth’s third straight harsher  winter-but in late 2007 it was still hard to forget 22 straight years of global  warming from 1976-1998. So the Old Farmer’s Almanac predicted 2008 would be the  hottest year in the last 100.
But  sunspots had been predicting major cooling since 2000, and global temperatures  turned downward in early 2007. The sunspots have had a 79 percent correlation  with the earth’s thermometers since 1860. Today’s temperatures are about on a  par with 1940. For 2008, the Almanac hired a new climatologist, Joe D’Aleo, who  says the declining sunspots and the cool phase of the Pacific Ocean predict  25-30 years of cooler temperatures for the planet.
“You could potentially sail, kayak or even  swim to the North Pole by the end of the summer. Climate scientists say that the  Arctic ice . . . is currently on track to melt sometime in 2008.” Ted  Alvarez, Backpacker Magazine Blogs, June, 2008.
Soon after this prediction, a huge Russian icebreaker got  trapped in the thick ice of the Northwest Passage for a full week. The Arctic  ice hadn’t melted in 2007, it got blown
into warmer southern  waters. Now it’s back. (Reference)
Remember too the Arctic has its own 70-year climate cycle.  Polish climatologist Rajmund Przbylak says “the highest temperatures since the  beginning of instrumental observation occurred clearly in the 1930s” based on  more than 40 Arctic temperature stations.
(This uneducated prediction may have been the catalyst for Lewis Pugh and his absurd kayak stunt that failed miserably – Anthony)
“Australia’s Cities Will Run Out of Drinking  Water Due to Global Warming.”
Tim  Flannery was named Australia’s Man of the Year in 2007-for predicting that  Australian cities will run out of water. He predicted Perth would become the  “first 21st century ghost city,’ and that Sydney would be out of water by 2007.  Today however, Australia’s city reservoirs are amply filled. Andrew Bolt of the  Melbourne Herald-Sun reminds us Australia is truly a land of long droughts and  flooding rains.
“Hurricane Effects Will Only Get Worse.” Live Science,  September 19, 2008.
So wrote the on-line  tech website Live Science, but the number of Atlantic hurricanes 2006-2008 has  been 22 percent below average, with insured losses more than 50 percent below  average. The British Navy recorded more than twice as many major land-falling  Caribbean hurricanes in the last part of the Little Ice Age (1700-1850) as  during the much-warmer last half of the 20th century.
“Corals will become increasingly rare on reef  systems.” Dr. Hans Hoegh-Guldberg, head of Queensland University (Australia)  marine studies.
In 2006, Dr.  Hoegh-Guldberg warned that high temperatures might kill 30-40 percent of the  coral on the Great Barrier Reef “within a month.” In 2007, he said global  warming temperatures were bleaching [potentially killing] the reef.
But, in 2008, the Global Coral Reef  Monitoring Network said climate change had not damaged the “well-managed” reef  in the four years since its last report. Veteran diver Ben Cropp said that in 50  years he’d seen no heat damage to the reef at all. “The only change I’ve seen  has been the result of over-fishing, pollution, too many tourists or people  dropping anchors on the reef,” he said.
No More Skiing? “Climate Change and  Aspen,” Aspen, CO city-funded study, June, 2007.
Aspen’s study predicted global warming would change the climate  to resemble hot, dry Amarillo, Texas. But in 2008, European ski resorts opened a  month early, after Switzerland recorded more October snow than ever before.  Would-be skiers in Aspen had lots of winter snow-but a chill factor of 18 below  zero F. kept them at their fireplaces instead of on the slopes.
*Sources:
Predictions  of 25-30 year cooling due to Pacific Decadal Oscillation:  Scafetta and West,  2006, “Phenomenological Solar Signature in 400 Years of Reconstructed Northern  Hemisphere Temperature Record,” Geophysical Research Letters.
Arctic Warmer in the 1930s:  R. Przybylak,  2000, “Temporal and Spatial Variation of Surface Air Temperature over the Period  of Instrumental Observation in the Arctic,” International Journal of Climatology  20.
British Navy records of Caribbean  hurricanes 1700-1850:  J.B. Elsner et al., 2000, “Spatial Variations in Major  U.S. Hurricane Activity,” Journal of Climate 13.
Predictions of coral loss:  Hoegh-Guldberg et al., Science, Vol.  318, 2007. Status of Coral Reefs of the World 2008, issued by the Global Coral  Reef Monitoring Network, Nov., 2008.
Aspen climate change study:  Climate Change and Aspen: An  Assessment of Potential Impacts and Responses, Aspen Global Change Institute,  June, 2007.
(1) Reader  Feedback | Click  here to get Canada Free Press in your email
Dennis T. Avery, is a senior fellow with  the Hudson Institute in Washington.  Dennis is the Director for Global Food  Issues ([url=http://www.cgfi.org]http://www.cgfi.org[/url]).  He was formerly a senior analyst for the Department of State. 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a091411',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The UK is to spearhead a major global crackdown on illegal timber and deforestation, with plans to form a coalition of developing countries against the trade as part of its hosting of crunch UN climate talks this year. Deforestation is a leading factor in rising global greenhouse gas emissions, but many developing nations lack the means and institutions to combat illegal logging and regulate forest industries. The Department for International Development (DfID) will shortly lay out plans to help countries strengthen the rule of law, support the trade in responsible forestry and provide on-the-ground assistance to stamp out illegal logging.  “The illegal timber trade robs the earth of trees, which not only help stop climate change, they also play a critically important role in maintaining the world’s threatened biodiversity,” said Zac Goldsmith, a minister for international development. “This is a huge success story for the UK and for the world, and sets the scene for what we hope will be a successful year of international cooperation in the run-up to COP 26.” The UK will need to form a global coalition of developing countries to put pressure on leading economies to act swiftly on carbon, if this year’s UN climate talks are to succeed. The UK will host the COP 26 talks in Glasgow in November, but the government has faced a troubled start to its presidency, with the abrupt sacking of the intended president, the former energy minister Claire O’Neill, and delays in setting out a clear plan. All countries are expected to come forward with tougher plans to reduce global emissions as part of COP 26, and experts have said this will only happen if the UK takes the lead in forming a coalition of small and big developing countries, including forested African nations and Indonesia, as well as major economies such as the US, China, India and the EU. Offering assistance to developing countries, in the form of finance and technical expertise, will be vital to that effort. Lord Goldsmith, who is rated by Ladbrokes as favourite to take over the reins of COP 26 in Boris Johnson’s expected reshuffle, pointed to some notable victories against deforestation so far. He said targeted intervention by the UK had recently led to an “extremely important” prosecution of a major illegal trading operator in Indonesia, and had encouraged China to strengthen its legal commitments to ending the trade. “These are vital steps towards making sure there is no safe harbour for illegal timber anywhere in the world,” he said. “The UK will continue to work with China, Indonesia and our other international partners to protect the world’s forests for future generations.” The new project, still in the planning stages, will build on the government’s forests governance, markets and climate programme, the focus of which includes strengthening the rule of law in affected countries in the developing world, influencing international partners to increase their efforts, supporting responsible trade and helping stakeholders on the ground to act. The DfID also helps developing countries to access new technology in the fight against deforestation, including electronic wood tracking, which marks trees with digital barcodes that officials can scan at each stage of the supply chain, and a GPS-enabled smartphone app, to enable local communities to monitor and report illegal logging in real time. In 2005, only about a fifth of Indonesia’s timber trade was legal. But today, after interventions by the UK and other partners, 100% of exports are sourced from independently audited factories and forests. In Liberia, the programme helped forest communities to negotiate fair contracts with logging companies to stop illegal deforestation, and a new land rights act was signed into law which for the first time recognised women’s rights to land."
"I am always repeating the mantra that we should “work hard and play hard”. But is having fun professionally productive?  As someone who studies animal behaviour I sometimes look to my experimental subjects for an answer. There are many things that give both animals and humans pleasure, such as eating tasty food, but these are not all necessarily fun.  When I look at my children I see that fun for them involves playing and scientists who investigate the biology of fun also focus on play.   Evolutionary ecologist Gordon Burghardt, an expert in the field, defines play as:  … repeated, seemingly non-functional behaviour differing from more adaptive versions structurally, contextually, or developmentally, and initiated when the animal is in a relaxed, unstimulating, or low stress setting.  Thus, play is something animals repeatedly do that appears to be without function, such as a cat chasing a ball of string.  It can be related to other behaviour (training to hunt, for instance), but tends to be expressed in a more exaggerated manner and only under relaxed conditions. It is arguable whether or [not all animal species play](http://www.cell.com/current-biology/fulltext/S0960-9822(14), and the debate goes back to our definition. When we think of animals playing, our minds fill with images of dogs, monkeys and dolphins not those of birds or octopuses.  Thus, we think of play as an activity undertaken by species we regard as sociable – and intelligent.   And yet octopuses are some of the smartest creatures in the sea, while parrots and crows can outperform primates in animal cognition tests. But scientists have seen these animals at play too. Does this mean that other, less intelligent animals have fun-free lives? The problem with the word “fun” is it induces anthropomorphism.  A bird “skiing” down a snow covered roof looks like fun to us humans. We assume that birds must also do it for fun.   But what looks like fun to us may in fact be the opposite, even in highly intelligent social species.  Those wonderful, apparently, [coordinated leaps](http://www.cell.com/current-biology/fulltext/S0960-9822(14) by wild dolphins are often aerial fights.  Wave surfing by black swans off the coast of Australia looks like enormous fun, but may just be the most efficient way to get to shore.  Or the animals may simply be being dragged along by the laws of physics. One solution to this problem would be to look at animal brains to determine if they have the same features which permit humans to feel like they are having fun.  Unfortunately, we simply do not have this information for the vast majority of species.  Thus, we do not know whether fun is restricted to a select group of animals.  Although adults of certain species such as bonobos, domestic dogs or ourselves are known to play, playing is still predominately done by juveniles. This makes sense, as playing often helps develop and hone the skills necessary for later life.  A kitten chases a ball of string to improve its predatory skills, a male baboon play-fights to develop the combat skills needed to climb up in the adult hierarchy, and chimpanzees play with objects to refine the hand-eye coordination skills necessary to use tools and process food. In humans, there is a very interesting link between [playfulness and creativity](http://www.cell.com/current-biology/fulltext/S0960-9822(14) – something that has not been looked at in animals. Playful people are known for being very innovative.  This link is perhaps most well known in the world of art where great composers such as Mozart or innovative painters such as Picasso were known for their playfulness.  In science as well there are many cases of this link, for example, the physicist Richard Feynman who said he played at physics and therefore his work was fun. These people generally have the ability to bring together apparently unconnected bits of information to create a new solution to a problem. In the animal kingdom, within a species, there is a variation in how much individuals play.  It would be interesting to see if the more playful ones are the innovators.  For example, was “Imo” the first wild Japanese macaque observed washing her food more playful than her peers?  And can inducing play increase creativity in animals? The mantra of my research group will now change slightly to “play hard and work hard” – not only will this make our lives more fun, but, hopefully, we will reap the benefits of increased creativity in our research."
"

Four years ago President Clinton announced a 50‐​point plan to curb so‐​called greenhouse gases, principally carbon dioxide. This week talks continue in Bonn, Germany, on an agreement to cut future emissions below 1990 levels. “The only thing we know for absolute certain is that voluntary programs won’t work,” contends Jessica Tuchman Mathews, president of the Carnegie Endowment for International Peace.



Actually, we also know that activists are misusing science in demanding draconian restrictions to avert global warming. In fact, there is no consensus among climatologists that uncontrolled, human‐​induced warming threatens the planet. Or that the kinds of measures being discussed in Bonn would avert such a disaster.



The climate has long been a favorite of apocalyptics. Two decades ago there were fears of– a new ice age. Publications like National Geographic reported shorter growing seasons, summer frosts, and advancing glaciers. Time magazine observed that “the atmosphere has been growing gradually cooler for the past three decades. The trend shows no indication of reversing.” There were also books, including “The Weather Conspiracy: The Coming of the New Ice Age and Ice: The Ultimate Human Catastrophe.”



The latter, written by Fred Hoyle and published in 1981, proclaimed: “It is 12,500 years since the last ice age ended, which means the next one is long overdue. When the ice comes, most of Northern America Britain, and Northern Europe will disappear under the glaciers.” Mr. Hoyle advocated warming the oceans.



There should be no controls without genuine consensus both that disaster threatens and that new regulations would avert disaster.



But, happily, that crisis passed. So we moved on to global warming. The basic theory is that pollutants– so‐​called greenhouse gases– are accumulating in the atmosphere, holding in the heat, and causing the world’s temperature to rise. It remains just a theory, however, since climate change is a complex business.



Indeed, there is no one right temperature. After all, there was once an ice age. If we could choose, we should choose a warmer climate. Fewer people die in the cold, less money is spent on energy, growing seasons are longer. The real issue, then is whether the Earth faces an uncontrolled catastrophic increase in average temperatures.



Unfortunately, the debate has become highly political. Stephen Schneider, who once warned of a new ice age, has complained that “it is journalistically irresponsible to present both sides.” Despite being a scientist, he admitted: “I don’t set very much store by looking at the direct evidence.” Why not? “To avert the risk we need to get some broad‐​based support, to capture public imagination. So we have to offer up some scary scenarios make some simplified dramatic statements and little mention of any doubts one might have.” So much for genuine scientific discourse. Explained Mr. Schneider: “Each of us has to decide what the right balance is between being effective and being honest.”



He’s not interested in direct evidence, presumably because, as even the Sierra Club’s Bruce Hamilton acknowledges, “If you look at the science, it’s all over the map.” Past polls have found that most climatologists do not believe human‐​induced warming has occurred. Activists cite the latest report of the UN.-sponsored Intergovernmental Panel on Climate Change, but lead author Benjamin Sanger complains that “it’s unfortunate that many people read the media hype before they read the chapter.” He cites the report’s many caveats: “We say quite clearly that few scientists would say the attribution issue was a done deal.”



Disputes begin over data collection and temperature trends. The best evidence suggests far less warming so far this century than predicted by the models. Moreover, 90 percent of the warming occurred before 1940, when emissions of supposed greenhouse gases began to climb dramatically. In fact, as John Shanahan of the Alexis de Tocqueville Institute points out, “the government’s own satellite data and balloon measurements over the last 18 years show a very slight cooling,” the opposite of “what the climate models predict should have occurred.”



Thus, there is good reason to avoid burdensome treaty commitments. Since the United States is already one of the globe’s most efficient energy consumers, massive emission cutbacks would mean fewer jobs, less production, and a lower standard of living. A Heritage Foundation study estimates the cost of proposed controls from 2001 through 2020 to be $3.3 trillion, or about $30,000 per household. It obviously matters whether environmental activists are choosing effectiveness or honesty when making their claims. It is not enough to delay the agreement’s compliance date as proposed by Mr. Clinton. There should be no controls without genuine consensus both that disaster threatens and that new regulations would avert disaster.



And that requires facts, not rhetoric. The burden of proof falls on those demanding the power to levy new taxes and impose new regulations. Unless such evidence appears, Americans should reject the Chicken Littles who cry that the sky is falling.
"
"


A guest post by: Russ Steele from NCWatch

We can only hope the most people in the US are shopping on Black Friday and not watching the Oprah Winfrey Show today.  Al Gore has brought his global warming propaganda machine to share with Oprah.  You can find the details on Oprah’s web page.  Here are some of the topics that Gore is pushing:
Classic Gore:
“Some of the leading scientists are now saying we may have as little as 10 years before we cross a kind of point-of-no-return, beyond which it’s much more difficult to save the habitability of the planet in the future,” Gore says.
Yes, but Al you have been saying that for over ten years and we are still here. And in the last ten years the global temperatures stopped rising and are now in decline.

Click for a larger image
Really Al, show me where the temperatures are beyond natural fluctuations:
Gore agrees that the planet’s temperature has indeed experienced up and down cycles, but he says the current up cycle is too extreme. “It’s way off the charts compared to what those natural fluctuations are,” he says.
Here is look at long term temperatures
 
One word of caution, these are USHCN numbers, which [have been] adjusted. Past temperatures are going down and the more recent going up.
Going, going Gored:
No place is immune to global warming, Gore says. “Of the thousand largest glaciers on every continent, 997 of them are receding,” he says. “And it’s not seasonal.”

Glaciers have been retreating long before CO2 was problem. (Graphic from Climate Skeptic) Now we learn that the glaciers have stopped retreating and are expanding:
After years of decline, glaciers in Norway are again growing, reports the Norwegian Water Resources and Energy Directorate, as reported in Daily Tech.
DailyTech has previously reported on the growth in Alaskan glaciers, reversing a 250-year trend of loss. Some glaciers in Canada, California, and New Zealand are also growing, as the result of both colder temperatures and increased snowfall.
Al needs to take a second look at the North Pole:
“The North Pole is melting.”

Here is comparison of the ice in November 1980 and 2008. Do you see some major differences, like the “North Pole is melting.” (Note: Earlier photos do not show snow coverage) Details at Cryosphere  Today
Katrina again:
“Temperature increases are taking place all over the world, including in the oceans. Gore warns that when the oceans get warmer, storms get stronger. In August 2005, millions of Americans were left homeless by Hurricane Katrina, one of the most powerful hurricanes in recent history. Gore says people should expect more Category 4 and 5 hurricanes if the ocean waters continue to warm.”
 
Looks like a decline in cyclone energy to me, not an increase.
Please let Oprah know that you expected more from someone of her intelligence and veracity here.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9aa0e681',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

We are in the process of putting the final touches on our Public Comment concerning the Department of Energy’s use of the social cost of carbon in its recent rulemaking regulating the energy efficiency of microwave ovens. (We’ll make our Comment available as soon as we file it.) The social cost of carbon (SCC) is the government’s idea of how much future damage will be caused by each ton of carbon dioxide emitted today.   
  
Our DOE Comment focuses entirely on the new science concerning the equilibrium climate sensitivity, that is, how much the earth’s average surface temperature will increase from a doubling of the atmospheric carbon dioxide content. We argue that had the new science indicating a lower equilibrium climate sensitivity been properly incorporated into the determination of the SCC used by the DOE, it would have had a significant impact on the cost/benefit analysis used to justify the new regulation. The “benefits” of requiring lower energy consumption from microwave ovens would have been reduced.   
  
But, as we have discussed previously, the new, lower estimate of the equilibrium climate sensitivity is just one of several key variables to which the SCC is very sensitive.   
  
Another is whether or not the social cost of carbon used in U.S. government cost/benefit analyses of proposed rules and regulations should reflect an estimate of global or domestic costs. Currently, in a departure from its own guidelines, the government uses the global SCC in determining the benefits accrued by reducing domestic carbon dioxide reductions. Unsurprisingly, the global SCC is many times higher than the domestic SCC.   
  
And another variable is the “discount rate” used in the SCC calculation. The discount rate reflects how much we are willing to pay now to reduce the costs of projected carbon-related damages in the future. The lower the discount rate, the more we must pay now and thus the higher the current SCC seems to be. In another departure from its own guidelines, the government’s calculation uses an especially low discount rate, resulting in a high SCC and thus more “benefits” from regulations reducing carbon emissions.   




There was an insightful column in the _New York Times_ earlier this week by Eduardo Porter that is one of the clearest explanations we have read on the effects and rationale of the choice of the discount rate when determining the social cost of carbon. He draws a distinction between what he terms a “moralist” approach (which prefers a lower discount rate) and a “business” approach (which requires a higher discount rate):   




The discrepancy between the estimates of the value of climate damage stem from radically different views on how much weight the people of the present should give to damages caused by the climate in the distant future.   
  
The estimate of [the SCC of] $65 a ton is inspired by a moral stance: if warming will impose a cost of 1 percent of the world’s income in the future, we should spend about 1 percent of our income to prevent it—or perhaps somewhat less to account for the trend that people 100 years from now are likely to be much richer than people today.   
  
By contrast, $13.50 a ton comes from the business world. Essentially, it requires that spending to prevent climate change should yield at least the same rate of return, in terms of reduced damages from warming, as any other capital investment.   
  
The two outlooks lead to entirely different decisions. The government’s rendition of the moral approach implies that it is worth making every investment to reduce carbon emissions that has a rate of return of at least 2.5 percent, in terms of avoided damages. Business logic suggests that no investment should be made if the return—after taxes—is less than 5 percent.



Ultimately, Porter thinks that the business logic will win the day. He astutely points out:   




Multiple challenges compete for the world’s resources, from economic development and ending poverty to eradicating AIDS and malaria. The climate is not the world’s only priority. Even if we were to agree that improving the well-being of future generations is worth an enormous investment, there might be better things to invest in than reducing greenhouse gas emissions.



We are happy to see more critical attention being paid to the social cost of carbon, as it represents a huge, but little-known back door that is silently swinging open for costly government regulation of carbon emissions. Economics and science argue that door should be barred and locked.


"
"

 _The Current Wisdom is a series of monthly articles in which Patrick J. Michaels, director of the Center for the Study of Science, reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press. Occasionally — as in this edition — we examine recent global warming perceptions that are at odds with reality._   
  
“The habitability of this planet for human beings really is at risk.”   
\--Al Gore, July 18, 2007   
  
The notion that people just can’t adapt to change (and therefore that governments must regulate change) is known as “Dumb People Syndrome” (DPS). Given the fact that the planet is “habitable” (meaning that there large numbers of people) over a mean annual temperature range of approximately 40°C , Gore’s statement—which is about a few degrees C, at best—is quintessential DPS.   
  
DPS has its subtypes, such as “Dumb Farmer Syndrome”, in which there’s agricultural Armageddon as the world’s farmers fail to adapt to warming conditions. It’s not only preposterous, it’s inconsistent with history.   
  
Farmers aren’t dumb, and there are incentives for their supply chain—breeders, chemical manufacturers, equipment companies, etc.—to produce adaptive technologies. Corn is already much more water-use efficient than it was, thanks to changes in genetics, tillage practices, and farm equipment. The history of U.S. crop yield bears strong witness (Figure 1).   






Figure 1. U.S. national corn and wheat yields, 1900-2012 (source: USDA National Agricultural Statistics Service).   
  
A look at the horrible crop year of 2012 is instructive. Corn yield drops about 38 bushels per acre from what’s known as the “technological trend line.” Because the “expected” yield—thanks to technology—with good weather is so high (around 160 bushels/acre), that’s a drop of about 24%, which is simply unremarkable when compared to the other lousy weather years of 1901 (36%), 1947 (21%), 1983 (29%) and 1988 (30%). Did we mention that the direct fertilization effect of atmospheric CO2 has resulted in a corn yield increase of approximately seven per cent?   
  
Most assessments of the impacts of climate change give some credence to DPS. Below is one of the “Key Findings” from the report _Global Climate Change Impacts in the United States_ produced by the U.S. Climate Change Global Change Research Program (USGCRP), which was used as a major support for the U.S. Environmental Protections Agency’s “Endangerment Finding” that human carbon dioxide emissions are a threat to health and welfare. According to the USGCRP:   




**Crop and livestock production will be increasingly challenged**.   
  
Many crops show positive responses to elevated carbon dioxide and low levels of warming, but higher levels of warming often negatively affect growth and yields. Increased pests, water stress, diseases, and weather extremes will pose adaptation challenges for crop and livestock production.



Now compare that to the corresponding “Key Finding” from our report _Addendum: Global Climate Change Impacts in the United States_ which is an independent (from the USGCRP) assessment of the scientific literature relating to environmental changes and how they may impact U.S. agriculture:   
  
  




**Crop and livestock production will adapt to climate change.**   
  
There is a large body of evidence that demonstrates substantial untapped adaptability of U.S. agriculture to climate change, including crop-switching that can change the species used for livestock feed. In addition, carbon dioxide itself is likely increasing crop yields and will continue to do so in increasing increments in the future.



Another example of the DPS relates to projections of the effects of more or stronger heat waves on human mortality. Everyone has heard—especially after last summer—how human use of fossil fuels to produce energy will increase the frequency and severity of killer heat waves.   
  
Here is how the USGCRP sees it, according to the “Key Messages” from the “Human Health” chapter of their report:   




Increases in the risk of illness and death related to extreme heat and heat waves are very likely.



History shows that things don’t work this way.   
  
Why? Because people are not dumb. Instead of dying in increasing numbers as temperatures rise, people take better precautions to protect themselves from the heat.   
  
Numerous examples of this abound, including some pioneering work that we did on the subject about 10 years ago. We clearly demonstrated that across the U.S., people were becoming less sensitive to high temperatures, _despite the fact that high temperatures were increasing_. In other words, adaptation was taking place in the face of (or, perhaps even because of) rising temperatures. Adaptations include expanding use of air conditioning, increasing public awareness, and more widespread community action programs.   
  
What was interesting about our work is we didn’t even need global warming to drive increasing heat waves. All we needed was economic activity that concentrates in cities. As they grow, buildings and pavement retain the heat of the day and impede the flow of ventilating winds. In recent years, the elevation of night temperatures here in Washington (where your tax dollars virtually guarantee economic growth), compared to the countryside, has become truly remarkable. But you won’t find an increase in heat-related mortality. Instead, there’s been a decrease.   
Our research was limited to major cities across the United States. But similar findings have since been reported for other regions of the world, the most recent being the from the Czech Republic.   
  
Czech researchers Jan Kyselý and Eva Plavcová recently published the results of their investigation of changes in heat-related impacts there from 1986 through 2009. What they found sure wasn’t surprising to us, but surely must come as quite a shock to the fans of DPS.   




Declining trends in the mortality impacts are found in spite of rising temperature trends. The finding remains unchanged if possible confounding effects of within-season acclimatization to heat and the mortality displacement effect are taken into account. Recent positive socioeconomic development, following the collapse of communism in Central and Eastern Europe in 1989, and better public awareness of heat-related risks are likely the primary causes of the declining vulnerability. The results suggest that climate change may have relatively little influence on heat-related deaths, since changes in other factors that affect vulnerability of the population are dominant instead of temperature trends. It is essential to better understand the observed nonstationarity of the temperature-mortality relationship and the role of adaptation and its limits, both physiological and technological, and to address associated uncertainties in studies dealing with climate change projections of temperature-related mortality.



Findings like these, along with our own work, caused us to conclude in our _Addendum_ report that:   
  
“In U.S. cities, heat-related mortality declines as heat waves become stronger and/or more frequent.”   
  
Evidence is much more compelling in support of a “smart people” diagnosis than its opposite. In fact, if humankind was really as dumb as the fans of DPS would have us believe, we wouldn’t be around today to hear their doomsaying, because _Homo sapiens_ would have been wiped out during vastly larger environmental swings (in and out of ice ages, for example) in our past, than those expected as a consequence of the burning of fossil fuels to produce the energy that powers our world—a world in which the human life expectancy, perhaps the best measure of our level of “dumbness” or “smartness”—has more than doubled over the last century and continues to grow ever longer.   
  
Simply put, we are not “dumb” when it comes to our survival and our ability to adapt to changing environmental conditions, but “scientific” assessments that assume otherwise most certainly are.   
  
**References:**   
  
Davis, R.E., Knappenberger, P.C., Novicoff, W.M., Michaels, P.J., 2002. Decadal changes in heat-related human mortality in the Eastern US. _Climate Research_ , **22** , 175–184.   
  
Davis, R.E., Knappenberger, P.C., Novicoff, W.M., Michaels, P.J.,2003a. Decadal changes in summer mortality in U.S. cities. _International Journal of Biometeorology_ , **47** , 166–175.   
  
Davis, R.E., Knappenberger, P.C., Michaels, P.J., Novicoff, W.M., 2003b. Changing heat-related mortality in the United States. _Environmental Health Perspectives_ , **111** , 1712–1718.   
  
Davis, R.E., Knappenberger, P.C., Michaels, P.J., Novicoff, W.M., 2004. Seasonality of climate-human mortality relationships in US cities and impacts of climate change. _Climate Research_ , **26** , 61–76.   
  
Jan Kyselý, j., and E. Plavcová, 2012.Declining impacts of hot spells on mortality in the Czech Republic, 1986–2009: adaptation to climate change? _Climatic Change_ , **113** , 437-453.


"
"
Share this...FacebookTwitter""I see lots of cold and snow in the warm times ahead!"" Britons stop believing. Welcome back to reality. (Photo source Wikipedia)
Britons are finally waking up from that global warming spell cast upon them years earlier.
Hat Tip Dirk H
What happens when the predictions of mountebanks (my favorite word lately) don’t come true, and the opposite happens?
People stop believing. That’s what’s happening now in Great Britain.
The online Daily Mail reports today on how the number of sceptic Britons has doubled in the last 4 years.
Obviously they don’t believe the junk science that insists global warming leads to more extreme cold and snow. Sorry, but many people aren’t that stupid to buy it. So take that crap alarmist science and use it for the next time you have to go.
This new scepticism in Britain is based on real-life observation, with people comparing the horoscopes of snowless winters heard earlier to the reality that is observed in real life today.
And for those who still believe in AGW, many now don’t think that it’s a real problem. The Daily Mail writes:
Fewer than half those polled – 46 per cent – are ready to use their cars less, and only 47 per cent are prepared to take public transport more often. Fewer than a quarter – 23 per cent – are willing to fly less.”
Share this...FacebookTwitter "
"

From the blog of Roger Pielke Sr. http://climatesci.org/
Erroneous News Article In The Times
Filed under: Climate Science Reporting — Roger Pielke Sr. @ 7:00 am

Thanks to Andrew  Forster of Local Transport Today in the UK for alerting us to the erroneous news article from the Times on  December 27 2008 titled
The  war on carbon – Arguments of 2009: Can Copenhagen save the planet?
An excerpt reads,
“The stakes at Copenhagen could not be much higher. Global surface  temperatures have risen by a tolerable three quarters of a degree celsius over  the past century, but the rate of increase is accelerating. The Kyoto Protocol  has had negligible impact on greenhouse gas emissions, and projections for the  mean global temperature rise in the next century range from 1.1 to 6.4 degrees.  Whether fast or very fast, the Earth is heating up.
There will be continued argument about the science of climate change over  the next 12 months, but not, except on the conspiratorial fringe, about the  threat. Climate change is real and worsening, and there is an overwhelming  likelihood that much of it is man-made.”
This is a erroneous report on the climate system! The rate of increase is  NOT accelerating. There is absolutely no question that global warming has  stopped for at least 4 years (using upper ocean data) ; e.g see
Pielke Sr., R.A., 2008: A broader view of the role of humans in the climate  system. Physics Today, 61, Vol. 11, 54-55.
http://www.climatesci.org/publications/pdf/R-334.pdf
and over 7 years using lower tropospheric data; e.g. see
Figure 7 TLT in http://www.ssmi.com/msu/msu_data_description.html.
With respect to the surface temperature trends [which have a warm bias in any  case, as we have documented in our peer review papers; e.g. see], a good set  of analyses on this subject has been posted over the last few years at http://rankexploits.com/musings/ [you should scroll back over the last several months to view; it is an excellent  comparison with model predictions]. As discussed on that website, even with the  warm biased global average surface temperature trends, the models have  over-predicted warming. The GISS data itself even shows recent cooling in the  ocean sea surface temperatures [see their figure for Monthly-Mean Global Sea  Surface Temperature; http://data.giss.nasa.gov/gistemp/2008/ where it has cooled since 2002.
The writers of the Time article, and other journalists who write  similar misinformation, damage the liklihood of responsible environmental  actions as a result of their overstatement and erroneous communication to the  public and policymakers of climate science.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99f86964',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterIn my last post I summarized the results of Ed Caryl’s analyses of stations far up in the Arctic, where temperature trends appear to follow the 60-year AMO cycle, and do not correlate at all with CO2.
Now I’ve been made aware that temperatures following the AMO are not exclusive to the Arctic. Blogsite digging in the clay has plotted temperatures from other cities located about the globe and came up with the same AMO sine wave trend, see below:

Source: http://diggingintheclay.wordpress.com/2010/09/01/in-search-of-cooling-trends/
And what follows are more plots from Iceland, Norway and Russia on the left, and from USA on the right (Sorry about the poor quality. Better quality graphics can be seen at diggingintheclay here). Again the AMO wave appears there as well. Moreover, the 1930s and 40s in USA even look a bit warmer than today’s temps.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




So poor little trace-gas CO2 just doesn’t get no respect, no matter where it goes. Every corner of the globe is ignoring this pip-squeak, climate-driver wannabe. CO2 is fading from the picture.
If the temperature has followed the AMO sine trend over the last 150 years, then why do we keep seeing hockey blades (nowadays without the stick-part) showing temperatures straight-lining up over the last 100 years or so?
Probably because GISS and others have changed historical data to make them fit their ideologized models – as unbelievable as it may sound. Steve Goddard’s site here explains how GISS has precisely done this with US temperatures by going back and flattening the graph.

The above comparator is from: Steve Goddard’s site. His post is short, but a very worthwhile read.
Before 2000, GISS showed a warmer 1930s and cooler current period. The AMO wave is clear to see.  But today, after having fiddled with the data, GISS now shows a cooler 1930s, a warmer present day and a somewhat straighter line that tries to cloak the AMO effect.
 Now you really know why they call it “man-made global warming”.
Share this...FacebookTwitter "
"It’s no surprise that dirty air kills. In fact, air pollution was recently placed in the top ten health risks faced by human beings globally. In the UK high concentrations of polluting particles cost health services around around £20 billion per year. The country’s Environmental Audit Committee recently described air pollution as a “public health crisis”. Despite this very real, very widespread risk, the quality of air we breathe doesn’t seem to attract the same level of concern that health crises such as obesity or smoking do. But this is a universal risk that impacts us in every breath we take. When such a fundamental human resource is at risk, we cannot afford to be ambivalent. We should invest in information technology that will enable smarter decisions, better planning and greener cities. It’s not enough to simply try to encourage improved individual behaviour. Air pollution, monitoring and planning needs national and international investment and attention. The evidence is clear. At present, many of the world’s cities are unable to comply with air pollution standards, in many cases far exceeding them and resulting in millions of premature deaths. Technology companies, universities and start-ups across the world are working to bring futuristic smart cities to life. Whether it’s automatic transportation infrastructure or digital government systems, this concept is spawning great ideas to improve our lives through the use of data and technology. But in the race to transform our cities, we need to ensure these smart systems enable our smarter lives to be longer lives, and by putting data to use we can help combat the ill-effects of bad air. It all starts with data. Scientists of course love data, but so do the public when they can discern real benefits from its results. There are a number of air monitoring systems in place in the UK already, such as Defra’s Automatic Urban and Rural Network of over 175 monitoring sites across the UK. This is an example of a traditional fixed monitoring station.  Likewise, several air pollution monitoring stations are run in the US under the umbrella of US Environmental Protection Agency to maintain and preserve the quality of the nation’s air.   Unfortunately these kinds of stations are expensive to install and maintain, which rather defeats the purpose of supposedly sustainable smart infrastructure.  Also, such systems often only provide a snapshot of a particular area and miss some pollution hotspots within city environments. However, there are already projects that show the potential for small, low-cost air sensing networks and demonstrate how the public could tap into this data to make a real impact on individual lives.  Air Quality Egg is a community-led air quality sensing network for urban air pollution, driven by inexpensive, DIY sensors. Similarly, Airtext provides daily forecasts of air quality in London.  On a small-scale these allow the public to make informed health decisions by avoiding areas of high pollution. Our latest work has looked at the future of low-cost sensing for managing air pollution in cities and how agencies can benefit from such inexpensive monitoring alternatives. Quite simply, the expansion of data projects such as these could save lives. The UK’s Environmental Audit Committee has argued for schools, hospitals and care homes to be built away from pollution hotspots. I would agree, and take this further, giving the public open and accurate data on how their own homes are affected by air pollution, based on a smart system of low-cost monitoring. This not only gives the option for people to consider where to live, but also provides individual impetus for understanding and acting on potentially polluting behaviour. More widely this kind of open data gives governments a reason to act. Smart cities also mean smart citizens, involved and in-touch with the facts. Where the facts show our health is at risk, governments will no longer be able to hide behind smokescreens, but will have to act on the data which is at all our fingertips."
"The Liberal MP Trent Zimmerman says the Morrison government should work towards adopting a target of net zero emissions by 2050 to bring itself into line with commitments Australia made under the Paris agreement, and to align Canberra’s policy with emissions reduction targets adopted by the states. In an interview with Guardian Australia’s politics podcast, Zimmerman said it was reasonable for the prime minister to want to do due diligence on what net zero would cost, and to consider what a policy roadmap would look like, “but this is something that we should be looking very seriously at”. He said the Paris agreement required Australia to move to net carbon neutrality in the second half of the century, and to take measures to help keep temperature rises to between 1.5C and 2C. Zimmerman – who is one of the backbench moderates pushing for the government to step up the level of ambition on climate action – predicted the 2050 target would trigger more infighting within the government. But he said the best way to persuade colleagues to accept a pivot was to explain to them, and to their communities, that the transition to low emissions energy in Australia was “not going to be a disaster”. “Not only will this not be a disaster, there will be opportunity.” Zimmerman said he wasn’t sure whether those arguments would ultimately succeed, but “we have to give it a go”. He said Australia had “lost half a decade because of the debate about what we needed to do” and expressed hope that a durable solution could be reached in this parliament. “I think the challenge of climate change is so real and so serious that if there can be some kind of consensus, that would be a positive outcome.” Zimmerman confirmed he had spoken to the independent MP Zali Steggall – who took the neighbouring electorate of Warringah from the government at the last election on a climate change platform – about a bill she wants to pursue in parliament that would impose the net zero target. “I’ve spoken to Zali and I’m open to having a conversation when we come back [to Canberra] in a couple of weeks’ time. “It is watch this space on that front.” Zimmerman said Steggall wanted to bring on the bill in March, but that timetable was too compressed to allow the government to resolve its own stance on the 2050 target. “I think the government should be given the opportunity to do a due diligence process on the 2050 target before the parliament is expected to sign up to that.” He said the leadership was very unlikely to grant MPs a conscience vote, and there was an open question about whether the bill would ever come on for debate. But he wanted to have the conversation “out of respect for Zali”. “For me, the important thing is the goal. Zali’s bill may not be the only way to achieve that goal.” The MP repeated comments he made last week that governments should not be in the business of bankrolling coal projects that the private sector will not finance. While Queensland Nationals are pushing assertively for a new taxpayer-backed coal plant at Collinsville, a number of Liberals are concerned about sending a negative signal in parts of the country where their constituents are worried about a lack of climate action, and about the fiscal exposure. Despite the government’s partisan criticism of Labor’s electric vehicles policy at the last election, including hyperbole that emissions standards were a “war on the weekend”, Zimmerman argued the Coalition needed to be at the forefront of managing the transition to electric vehicles, because “the global car market is only heading in one direction”. He said the states needed to look at consumer incentives, such as concessions on registration fees and rebates on toll roads for drivers of electric cars. At the commonwealth level, Zimmerman said, “the charging network will be really crucial”. The MP said an electric vehicle strategy would work best with the public if the focus in the first phase was on carrots rather than sticks, and once the transition was under way, regulatory options could come into play, such as emissions standards to drive faster take-up as prices came down."
"
NOTE: Zip file downloads of models with data have been fixed, see end of this post – Anthony


Source: Mantua,            2000
The essay below has been part of a back and forth email exchange for about a week. Bill has done some yeoman’s work here at coaxing some new information from existing data. Both HadCRUT and GISS data was used for the comparisons to a doubling of CO2, and what I find most interesting is that both Hadley and GISS data come out higher in for a doubling of CO2 than NCDC data, implying that the adjustments to data used in GISS and HadCRUT add something that really isn’t there.
The logarithmic plots of CO2 doubling help demonstrate why CO2 won’t cause a runaway greenhouse effect due to diminished IR returns as CO2 PPM’s increase. This is something many people don’t get to see visualized.
One of the other interesting items is the essay is about the El Nino event in 1878. Bill writes:
The 1877-78 El Nino was the biggest event on record.  The anomaly peaked at +3.4C in Nov, 1877 and by Feb, 1878, global temperatures had spiked to +0.364C or nearly 0.7C above the background temperature trend of the time.
Clearly the oceans ruled the climate, and it appears they still do.
Let’s all give this a good examination, point out weaknesses, and give encouragement for Bill’s work. This is a must read. – Anthony

Adjusting Temperatures for the ENSO and the  AMO
A guest post by: Bill Illis

People have  noted for a long time that the effect of the El Nino Southern Oscillation (ENSO)  should be accounted for and adjusted for in analyzing temperature trends.  The same point has been raised for the  Atlantic Multidecadal Oscillation (AMO).   Until now, there has not been a robust method of doing so.
This post will  outline a simple least squares regression solution to adjusting monthly  temperatures for the impact of the ENSO and the AMO.  There is no smoothing of the data, no  plugging of the data; this is a simple mathematical calculation.
Some basic  points before we continue.
–         The ENSO and the AMO both affect temperatures and, hence, any  reconstruction needs to use both ocean temperature indices.  The AMO actually provides a greater impact on  temperatures than the ENSO.
–         The ENSO and the AMO impact temperatures directly and continuously on  a monthly basis.  Any smoothing of the  data or even using annual temperature data just reduces the information which  can be extracted.
–         The ENSO’s impact on temperatures is lagged by 3 months while the AMO  seems to be more immediate.  This model  uses the Nino 3.4 region anomaly since it seems to be the most indicative of the  underlying El Nino and La Nina trends.
–         When the ENSO and the AMO impacts are adjusted for, all that is left  is the global warming signal and a white noise error.
–         The ENSO and the AMO are capable of explaining almost all of the  natural variation in the climate.
–         We can finally answer the question of how much global warming has  there been to date and how much has occurred since 1979 for example.  And, yes, there has been global warming but  the amount is much less than global warming models predict and the effect even  seems to be slowing down since 1979.
–         Unfortunately, there is not currently a good forecast model for the  ENSO or AMO so this method will have to focus on current and past temperatures  versus providing forecasts for the future.
And now to the  good part, here is what the reconstruction looks like for the Hadley Centre’s  HadCRUT3 global monthly temperature series going back to 1871 – 1,652 data  points.

Click for a full sized image

I will walk you  through how this method was developed since it will help with understanding some  of its components.
Let’s first look  at the Nino 3.4 region anomaly going back to 1871 as developed by Trenberth  (actually this index is smoothed but it is the least smoothed data  available).
–         The 1877-78 El Nino was the biggest event on record.  The anomaly peaked at +3.4C in Nov, 1877 and  by Feb, 1878, global temperatures had spiked to +0.364C or nearly 0.7C above the  background temperature trend of the time.
–         The 1997-98 El Nino produced similar results and still holds the  record for the highest monthly temperature of +0.749C in Feb, 1998.
–         There is a lag of about 3 months in the impact of ENSO on  temperatures.  Sometimes it is only 2  months, sometimes 4 months and this reconstruction uses the 3 month lag.
–         Going back to 1871, there is no real trend in the Nino 3.4 anomaly  which indicates it is a natural climate cycle and is not related to global  warming in the sense that more El Ninos are occurring as a result of  warming.   This point becomes important because we need  to separate the natural variation in the climate from the global warming  influence.

Click for full sized image

The AMO anomaly  has longer cycles than the ENSO.
–         While the Nino 3.4 region can spike up to +3.4C, the AMO index rarely  gets above +0.6C anomaly.
–         The long cycles of the AMO matches the major climate shifts which  have occurred over the last 130 years.   The downswing in temperatures from 1890 to 1915, the upswing in temps  from 1915 to 1945, the decline from 1946 to 1975 and the upswing in temps from  1975 to 2005.
–         The AMO also has spikes during the major El Nino events of 1877-88  and 1997-98 and other spikes at different times.
–         It is apparent that the major increase in temperatures during the  1997-98 El Nino was also caused by the AMO anomaly.  I think this has lead some to believe the  impact of ENSO is bigger than it really is and has caused people to focus too  much on the ENSO.
–         There is some autocorrelation between the ENSO and the AMO given  these simultaneous spikes but the longer cycles of the AMO versus the short  sharp swings in the ENSO means they are relatively independent.
–         As well, the AMO appears to be a natural climate cycle unrelated to  global warming.

Click for full sized image

When these two  ocean indices are regressed against the monthly temperature record, we have a  very good match.
–         The coefficient for the Nino 3.4 region at 0.058 means it is capable  of explaining changes in temps of as much as +/- 0.2C.
–         The coefficient for the AMO index at 0.51 to 0.75 indicates it is  capable of explaining changes in temps of as much as +/- 0.3C to +/-  0.4C.
–         The F-statistic for this regression at 222.5 means it passes a 99.9%  confidence interval.
But there is a  divergence between the actual temperature record and the regression model based  solely on the Nino and the AMO.  This is  the real global warming signal.

Click for full sized image

The global  warming signal (which also includes an error, UHI, poor siting and adjustments  in the temperature record as demonstrated by Anthony Watts) can be now be modeled against the rise in CO2 over the period.
–         Warming occurs in a logarithmic relationship to CO2 and,  consequently, any model of warming should be done on the natural log of  CO2.
–         CO2 in this case is just a proxy for all the GHGs but since it is the  biggest one and nitrous oxide is rising at the same rate, it can be used as the  basis for the warming model.
This regression  produces a global warming signal which is about half of that predicted by the  global warming models.  The F statistic  at 4,308 passes a 99.9% confidence interval.

Click for full sized image

–         Using the HadCRUT3 temperature series, warming works out to only  1.85C per doubling of CO2.
–         The GISS reconstruction also produces 1.85C per doubling while the  NCDC temperature record only produces 1.6C per doubling.
–         Global warming theorists are now explaining the lack of warming to  date is due to the deep oceans absorbing some of the increase (not the surface  since this is already included in the temperature data).  This means the global warming model  prediction line should be pushed out 35 years, or 75 years or even 100s of  years.
Here is a  depiction of how logarithmic warming works.   I’ve included these log charts because it is fundamental to how to  regress for CO2 and it is a view of global warming which I believe many have not  seen before.
The formula for  the global warming models has been constructed by myself (I’m not even sure the  modelers have this perspective on the issue) but it is the only formula which  goes through the temperature figures at the start of the record (285 ppm or 280  ppm) and the 3.25C increase in temperatures for a doubling of CO2.   It is  curious that the global warming models are also based on CO2 or GHGs being  responsible for nearly all of the 33C greenhouse effect through its impact on  water vapour as well.

Click for larger image

The divergence,  however, is going to be harder to explain in just a few years since the ENSO and  AMO-adjusted warming observations are tracking farther and farther away from the  global warming model’s track.  As the RSS  satellite log warming chart will show later, temperatures have in fact moved  even farther away from the models since 1979.

Click for larger image

The global  warming models formula produces temperatures which would be +10C in geologic  time periods when CO2 was 3,000 ppm, for example, while this model’s log warming  would result in temperatures about +5C at 3,000 ppm.  This is much closer to the estimated  temperature history of the planet.
This method is  not perfect.  The overall reconstruction  produces a resulting error which is higher than one would want.  The error term is roughly +/-0.2C but the it  does appear to be strictly white noise.    It would be better if the resulting error was less than +/- 0.2C but it  appears this is unavoidable in something as complicated as the climate and in  the measurement errors which exist for temperature, the ENSO and the  AMO.
This is the  error for the reconstruction of GISS monthly data going back to 1880.

Click for larger image

There does not  appear to be a signal remaining in the errors for another natural climate  variable to impact the reconstruction.   In reviewing this model, I have also reviewed the impact of the major  volcanoes.  All of them appear to have  been caught by the ENSO and AMO indices which I imagine are influenced by  volcanoes.  There appears to be some room  to look at a solar influence but this would be quite small.  Everyone is welcome to improve on this  reconstruction method by examining other variables, other indices.
Overall, this  reconstruction produces an r^2 of 0.783 which is pretty good for a monthly  climate model based on just three simple variables.  Here is the scatterplot of the HadCRUT3  reconstruction.

Click for a larger image

This method  works for all the major monthly temperature series I have tried it  on.
Here is the  model for the RSS satellite-based temperature series.

Click for a larger image

Since 1979,  warming appears to be slowing down (after it is adjusted for the ENSO and the  AMO influence.)
The model  produces warming for the RSS data of just 0.046C per decade which would also  imply an increase in temperature of just 0.7C for a doubling of CO2 (and there  is only 0.4C more to go to that doubling level.)
Click for a full sized image

Looking at how  far off this warming trend is from the models can be seen in this zoom-in of the  log warming chart.  If you apply the same  method to GISS data since 1979, it is in the same circle as the satellite  observations so the different agencies do not produce much different  results.

Click for larger image

There may be  some explanations for this even wider divergence since 1979.
–         The regression coefficient for the AMO increases from about 0.51 in  the reconstructions starting in 1880 to about 0.75 when the reconstruction  starts in 1979.  This is not an expected  result in regression modelling.
–         Since the AMO was cycling upward since 1975, the increased  coefficient might just be catching a ride with that increasing trend.
–         I believe a regression is a regression and we should just accept this  coefficient.  The F statistic for this  model is 267 which would pass a 99.9% confidence interval.
–         On the other hand, the warming for RSS is really at the very lowest  possible end for temperatures which might be expected from increased GHGs.  I would not use a formula which is lower than  this for example.
–         The other explanation would be that the adjustments of old  temperature records by GISS and the Hadley Centre and others have artificially  increased the temperature trend prior to 1979 when the satellites became  available to keep them honest.  The  post-1979 warming formulae (not just RSS but all of them) indicate old records  might have been increased by 0.3C above where they really were.
–         I think these explanations are both partially correct.
This temperature  reconstruction method works for all of the major temperature series over any  time period chosen and for the smaller zonal components as well.  There is a really nice fit to the RSS Tropics  zone, for example, where the Nino coefficient increases to 0.21 as would be  expected.
Click for a full sized image

Unfortunately,  the method does not work for smaller regional temperature series such as the US  lower 48 and the Arctic where there is too much variation to produce a  reasonable result.
I have included  my spreadsheets which have been set up so that anyone can use them.  All of the data for HadCRUT3, GISS, UAH, RSS  and NCDC is included if you want to try out other series.  All of the base data on a monthly basis  including CO2 back to 1850, the AMO back to 1856 and the Nino 3.4 region going  back to 1871 is included in the spreadsheet.
The model for  monthly temperatures is “here” and for annual temperatures is “here” (note the  annual reconstruction is a little less accurate than the monthly reconstruction  but still works).
I have set-up a  photobucket site where anyone can review these charts and others that I have  constructed.
http://s463.photobucket.com/albums/qq360/Bill-illis/
So, we can now  adjust temperatures for the natural variation in the climate caused by the ENSO  and the AMO and this has provided a better insight into global warming.  The method is not perfect, however, as the  remaining error term is higher than one would want to see but it might be  unavoidable in something as complicated as the climate.
I encourage  everyone to try to improve on this method and/or find any errors.  I expect this will have to be taken into  account from now on in global warming research.   It is a simple regression.

UPDATED: Zip files should download OK now.

SUPPLEMENTAL INFO NOTE: Bill has made the Excel spreadsheets with data and graphs used for this essay available to me, and for those interested in replication and further investigation, I’m making them available here on my office webserver as a single ZIP file
Downloads:
Annual Temp Anomaly Model 171K Zip file
Monthly Temp Anomaly Model 1.1M Zip file
Just click the download link above, save as zip file, then unzip to your local drive work folder.
Here is the AMO data which is updated monthly a few days after month end.
http://www.cdc.noaa.gov/Correlation/amon.us.long.data
Here is the Nino 3.4 anomaly from Trenbeth from 1871 to 2007.
ftp://ftp.cgd.ucar.edu/pub/CAS/TNI_N34/Nino34.1871.2007.txt
And here is Nino 3.4 data updated from 2007 on.
http://www.cpc.ncep.noaa.gov:80/data/indices/sstoi.indices
– Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b0adf78',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.” In this edition, we cover an important story that we missed back in 2008._   
  
  
People send us stuff. As a result of our recent _Global Science Report_ on global warming ruining our bananas, one of our fans directed our attention to an important effect of climate change that we somehow missed, back in 2008, when the alarmists at the BBC wrote that it was threatening _haggis_.   
  
  
Haggis, for the uninitiated, is sheep stomach stuffed with minced lung, liver, heart, tongue, suet, onions and oats. How offal!   
  
  
While there’s no accounting for taste, it tastes as bad as it smells.   
  
  
According to the story, there has been a rise in a parasite effecting Scottish sheep that renders the lung “unfit for consumption” (something that many of you probably thought was the case already).   
  
  
And, so as not to miss the bandwagon, an official from the Scottish Agricultural College Veterinary Investigation Centre told the BCC that:   




Part of the reason will be the parasite is able to live a pretty happy life on the ground because of higher temperatures. Maybe it’s climate change.



Or maybe not.   
  
  
It turns out that another potential cause of the increase in the lung parasite is that Scottish farmers have reduced their application of parasite treatment due to declining infections of roundworm. The treatment of roundworm also killed the lung parasite.   
  
  
There was no mention made in the BCC article as to whether global warming was behind the decrease in roundworm infestations.   
  
  
Instead, the article went on the describe the events which took place in the World Haggis Eating Championship, won by Willie Robertson from Dunkeld, who managed to put away a pound of haggis in 125 seconds. For his victory, Mr. Robertson was awarded a trophy and a bottle of whiskey—no doubt a key feature in the rest of the day’s merrymaking.   
  
  
BBC’s writing in the haggis story appears similarly merry. Here are the last three paragraphs of their report, verbatim, a candidate for first place in the 2008 International Nonsequitur Competition.   




The championship was held as part of the 125th Birnam Highland Games, and attracted competitors from Australia, New Zealand and the US.   
  
  
Climate change, meanwhile, has been blamed for affecting natural habitats in Scotland and across the world.   
  
  
Most notably, scientists and conservationists say it threatens survival of polar bears.
"
"The growing Ebola virus outbreak not only highlights the tragedy enveloping the areas most affected but also offers a commentary on they way in which the political ecology in West Africa has allowed this disease to become established. The narrative goes that the virus appeared spontaneously in the forest villages of Guinea in December 2013. But this is debatable given that there is evidence of antibodies the Ebola virus in human blood from Sierra Leone up to five years previously. Previously only one case of Ebola had been reported in the region, and it was the Ivory Coast strain of the virus. The strain detected in the blood samples is of the more virulent Zaire strain of Ebola, the same strain responsible for the current epidemic. After months of very little concerted action it’s clear that the disease is now seriously in danger of spreading out of control. The global health community has declared it a crisis of international importance, which has led the host nations to implement draconian preventions strategies, tantamount in some places to martial law in terms of surveillance, quarantine, border controls and other logistical aspects of control. But this is too little, too late. There are several mechanisms through which the virus may have emerged, and it is unlikely that this latest outbreak was spontaneous. It is poverty that drives villagers to encroach further into the forest, where they become infected with the virus when hunting and butchering wildlife, or through contact with body fluids from bats  – this has been seen with Nipah, another dangerous virus associated with bats. The likelihood of infection in this manner is compounded by inadequate rural health facilities and poor village infrastructure, compounded by the disorganised urban sprawl at the fringes of cities. The virus then spreads in a wave of fear and panic, ill-conceived intervention and logistical failures – including even insufficient food or beds for the severely ill.  Take for example the global palm oil industry, where a similar trend of deep-cutting into forests for agricultural development has breached natural barriers to the evolution and spread of specific pathogens. The effects of land grabs and the focus on certain fruit crop species leads to an Allee effect, where sudden changes in one ecological element causes the mechanisms for keeping populations – bats in this case – and viruses in equilibrium to shift, increasing the probability of spill over to alternative hosts. This is not unheard of; the introduction of fruit tree crops in cleared forests and agricultural expansion in Malaysia was associated with the emergence of Nipah virus. Bats feeding on fruit trees infected pigs in pens, which provided a vector for the virus to humans. Another example is with vector-borne diseases such as the Japanese Encephalitis, a virus carried by wild birds which expanded its range due to growing rice and pig farming. Chikungunya and Dengue Fever viruses exploited deforestation for secondary epidemiological cycles, which increased at the forest edge until the virus was able to adapt to secondary hosts and expand globally. Certainly the complexity of the agro-ecological changes in West Africa warrant scrutiny. Guinea’s new agriculture is in an early stage of development, identified by the World Bank as the highest investment potential for industrial agriculture. As global markets shift – and tariffs and taxes on multinational companies are removed, farmers with small land holdings are faced with a choice: either sell off or scale up to meet the competition. Forests are one of the first casualties.  Alongside this subtle effect is the dismantling of traditional governance, violence under colonial, neo-colonial and more recent kleptocratic governments and the economic movements of people towards urbanisation. Such turbulence, poverty, the influx of refugees from neighbouring wars and crumbling health systems have all created an ecosystem in which the natural friction that prevents Ebola from gathering pathogenic momentum has been all but eroded. Any international response can do little to remedy these contributing factors. In fact the response has been little more than a recognition of the complete failure of neo-liberal development strategies to contain the virus.  The “success” of the Ebola virus is fundamentally based on the sociological factors and population biology of those it infects. But the data required to test the hypothesis – detailed records about what people eat, where they go and how they interact – is presently unavailable. Instead research has focused on virus hunting, and with little success: more than 40,000 samples have not yet conclusively determined where the natural reservoir of Ebola lies. All the while, the socio-ecological factors that are critical to the spread of any disease are ignored."
"Chemicals are all around us. They are crucial in all manner of industries, from agriculture to food to cosmetics. Most people give little thought to how these chemicals are made – and certainly very few would consider the chemical industry as a contributor to our society’s dependence on oil. But it is. Historically petroleum has been used to develop the chemicals needed for products such as pesticides, food supplements and make-up. Although many of the building blocks required to make these chemicals occur naturally, trying to take those natural materials and use them in large-scale industrial processes has proved difficult and costly. So petroleum is used instead. Until recently, oil was seen as a cheap commodity which was available in abundance, so petroleum was perfect for use in the chemical industry. However, the world has changed. We now recognise the need to reduce our reliance on oil in order to protect the environment and maintain our national security. There are also health concerns over the use of petroleum in products we eat and apply to our bodies. This is why new advanced methods for industrial biotechnology are so important; they are enabling the use of engineered bacterial cells, rather than petroleum, in developing chemicals to be used in these products. Importantly, the bacteria can be grown on a range of cheap and renewable resources, even various kinds of farmland waste. However, in order to use bacteria effectively – and in a manner which can be scaled up by industry – we need to know a lot more about bacterial cell biology. Only by investigating the machinery and processes at the heart of cells can we learn how to use them to develop organic chemicals in a manner that was previously unfeasible for industry. At Newcastle University’s Centre for Bacterial Cell Biology we’ve spent years studying Bacillus subtilis, a bacterium that lives peacefully in soil or even in the human gut. This organism and its relatives are very good at making and secreting enzymes which are catalysts for all sorts of useful processes. It means Bacilli are already used widely by industry, for example in producing the enzymes that are used in biological washing powders such as proteases (which break down blood, egg and other protein stains) or amylases (which dissolve starch). However, the range of enzymes they can secrete efficiently is much more limited than we would like. Studies on fundamental structures and processes of the bacterium are now beginning to give us the ability to engineer the cells to secrete a wider range of proteins from a diverse range of sources. This means that before long Bacillus will be used to make all kinds of enzymes, including those needed in the chemical industry to replace processes presently dependent on petroleum. This is a huge opportunity. The European industrial biotechnology industry has an estimated annual turnover of more than €60 billion, and the global industrial enzyme market is predicted to be worth US$7.1 billion by 2018. Detergent enzymes alone make up a billion dollar business. However, continued reliance on oil-based solutions will hamper growth and could have significant societal and environmental consequences. Replacing petroleum with bacteria will have a real impact on people’s lives. Suncream is a good example. One of the projects we are working on at Newcastle is to develop organic UV-absorber compounds from renewable materials to be used in sunscreens. Damage from exposure to UV radiation is a major worry, and there is increasing demand for cosmetics that block UV rays. The industry relies on oil-based technology and inorganic metal oxide particles to create materials that block UV rays for use in sunscreen. However, we know that photosynthetic bacteria called cyanobacteria that grow in the sea make their own organic sunscreen molecules. By taking the relevant genes from cyanobacteria and transplanting them into a bacterium that is already widely used in chemical production, we hope to be able to change this. If we are successful, the process could easily be scaled up so the cosmetics industry will be able to develop cheap organic sunscreen. This is just one example of the way in which bacteria could support a future without oil. Work is already in progress to explore the potential of using waste to grow bacteria or other micro-organisms that could make chemicals such as ethanol for use as “biofuel” for cars and aeroplanes, further reducing the use of oil.  There is a lot of work still to be done to make this vision a reality, but by continuing to investigate how bacterial cells work and how they could be used in chemical production we can see a future in which waste becomes energy and we can live without oil."
"

In this month’s issue of the _Economists’ Voice_ , Robert Whaples, chair of the economics department at Wake Forest, reports on a survey he recently conducted in which he sent questionnaires to 210 Ph.D. economists randomly selected from the American Economic Association. His charge: to find out how much disagreement there is within the profession and a number of high profile public policy issues.   
  
  
What did his respondents have to say about the impact that global warming will have on the economy? 



In short, the number of economists who thought global warming would improve the U.S. economy outnumbered the number of economists who thought that global warming would harm the economy to the extent feared by the _Stern Review_.   
  
  
Will those who demand that we bow down to the consensus of scientific opinion likewise demand the same regarding the consensus of economic opinion? Not bloody likely.
"
"London mayor Sadiq Khan is to spend £50m on a Green New Deal for London as part of a pledge to make the UK capital carbon neutral by 2030. The mayor, who faces the electorate in May, said the announcement highlights his commitment to tackling the climate crisis and improving London’s dangerous levels of air pollution. “I’m unapologetic at how ambitious my plans are for a Green New Deal for London because we can’t afford not to be ambitious when it comes to saving our planet,” Khan told the Guardian. In May’s election he faces a challenge from Tory candidate Shaun Bailey, former Conservative cabinet member Rory Stewart – who is standing as an independent – as well as candidates from the Green party, the Liberal Democrats and the Women’s Equality party. But announcing the new fund, which officials said comes from higher council tax and business rates, Khan insisted May’s election was a “two horse race” between him and Bailey – and made a direct pitch for green votes. “The Tory candidate and I are the only candidates who can win this election,” he said. “Which is why I’m making a direct appeal today to Londoners who have previously supported the Green party to lend me their vote on 7 May so that I can stand up for our shared values and take action on climate change.” The mayor has introduced an ultra-low emissions zone (Ulez) in central London which is due to expand to the north and south circulars in 2021. However, other cities – including Manchester and Birmingham – have announced ambitious plans to improve air quality and encourage cycling, and Khan has been criticised for not moving quickly enough. But the mayor insisted he was determined to make London a leading green city. “These issues are personal to me. I don’t want my children to grow up in a world where our very way of life is threatened by the climate crisis. And as someone who suffers from adult onset asthma, I understand the price we pay for failing to clean up our toxic air.” Green groups welcomed the new fund but said Khan must go further if he is re-elected in May. Jenny Bates, campaigner at Friends of the Earth, said the climate and nature crisis was the biggest threat the world faced, adding air pollution in London was “a health scandal”. “While the mayor has made good strides on these issues, much more needs to be done to cut emissions from all sectors. This means expanding the Ulez to the whole of London, not just to the north and south circular roads. We also need more infrastructure to make cycling and walking safer and easier for everyone, along with clear action to cut traffic levels which a London-wide pay-as-you-go driving system would help deliver.” Bates also criticised the mayor for approving the four-lane Silvertown road tunnel in east London which she said would make “already illegally bad air breathed in some areas even worse”. “There is no place for large scale road-building in London given the climate and air pollution imperatives, nor for any airport expansion at City airport or Heathrow.” Doug Parr, policy director at Greenpeace, said it was right that all candidates for London mayor prioritised the climate emergency. “We will see what all parties have to offer but this is a helpful pledge to increase the priority that climate protection receives in a city of global significance such as London,” he added. City Hall officials said the new fund had not been allocated to specific programmes yet but proposals being considered included making homes energy efficient, creating new green spaces and speeding up the installation of solar panels in the capital."
"
Share this...FacebookTwitterNot only Europe is getting walloped by winter, so are some regions in China, where the most snow has fallen in 30 years.
The early snowstorms have isolated herdsman living in remote areas, see map, and Chinese relief services dispatched assistance.
According to China.org.cn:
Snow storms has hit four towns in Xing’an Prefecture, a pasture region about 1,500 km northeast of the regional capital, Hohhot, since last Saturday.
Snow has accumulated up to 30 cm deep in most parts of the region and a meter in some areas.
The snow was 40 days earlier than its usual arrival time and was the heaviest in 30 years. At least 700 heads of livestock are believed to have died in the storm.
Winter arrives early in Northeast China
And more here: Winter in China
UPDATE: Record cold in the UK. h/t: M White
Share this...FacebookTwitter "
"

Media Contact: (202) 789‑5200





While the DC Court of Appeals has just ruled in favor of the Obama Administration in rejecting challenges to the Environmental Protection Agency’s rules concerning carbon dioxide emissions from cars and light trucks (the so‐​called “tailpipe emissions standards”), Senior Fellow Patrick J. Michaels believes the larger battle is still to come:



“On June 25, the public comment period for the EPA’s proposed regulations on coal‐​fired power plants ended,” said Michaels. “After thorough review, I found that the report from the U.S Global Change Research Program (USGCRP), which served as the source for the scientific opinions underlying the original endangerment finding in 2010, is unrepresentative of the larger body of scientific research on the topic of anthropogenic climate change and its potential impacts on the United States.”



Michaels, working with a team of experts and scientists, assembled an addendum to the USGCRP report, which they submitted as comments on June 22.



“Our review represents the most comprehensive scientific critique of the EPA Endangerment Finding on coal‐​fired plants ever written, and directly counters their claims on how climate change impacts in the United States, using a much more exhaustive survey of peer‐​reviewed science than the EPA relied upon,” said Michaels. 



Michaels also cautions against relying too much on static reports in rulemaking on climate change.



“No static report can provide long‐​term guidance as to the nature of climate change and its impacts, as this field is constantly evolving under the weight of new scientific findings. Consequently, it is imperative that the EPA reassess the current scientific understanding on at least an annual basis,” said Michaels.



The EPA is expected to finalize regulations regarding emissions from coal‐​fired plants later this year.



###
"
"Whether you’re into mining, energy or tourism, there are lots of reasons to explore space. Some “pioneers” even believe humanity’s survival depends on colonising celestial bodies such as the moon and Mars, both becoming central hubs for our further journey into the cosmos. Lunar land peddlers have started doing deals already – a one-acre plot can be yours for just £16.75.  More seriously, big corporations, rich entrepreneurs and even US politicians are eyeing up the moon and its untapped resources. Russia has plans for a manned colony by 2030 and a Japanese firm wants to build a ring of solar panels around the moon and beam energy back to Earth. We need to be clear about the legal validity of extraterrestrial real estate as the same ideas that were once used to justify colonialism are being deployed by governments and galactic entrepreneurs. Without proper regulation, the moon risks becoming an extra-planetary Wild West. To figure out whether “earthly” laws can help decide who owns what in space – or if anything can be owned at all – we must first disentangle sovereignty from property. Back in the 17th century, natural law theorists such as Hugo Grotius and John Locke argued that property rights exist by virtue of human nature but that they can only have legal force when they are recognised by a sovereign government. Within the context of space law, the big question is whether sovereignty reaches infinity – how high must you go to escape your country? When the US was confronted with this query in the early 1950s, it lobbied for the recognition of outer space as a global commons. The Soviet Union was difficult to infiltrate to gather intelligence, so open access to Soviet air space was crucial for the US during the Cold War. Perceiving outer space as a commons was also another way of preventing national sovereignty in space. But neither the USSR nor the US was keen to fight out the Cold War on yet another front. Geopolitics dictated the decision to treat outer space as being non-appropriable. This principle can be found back in Article II of the 1967 Outer Space Treaty which clearly forbids “national appropriation by claims of sovereignty, means of use or occupation by any other means”. It has been widely accepted: no one complains the various moon landings or satellites in space have infringed their sovereignty. However, legal commentators disagree over whether this prohibition is also valid for private appropriation. Some space lawyers have argued for the recognition of real property rights on the basis of jurisdiction rather than territorial sovereignty.  Historical records of the Space Treaty negotiations clearly indicate people were against private appropriations at the time, but an explicit prohibition never made it into Article II. Lessons have been learned from this omission and the ban was far more explicit in the subsequent Moon Agreement of 1979. However only 16 countries signed the agreement, none of which were involved in manned space exploration, leaving it somewhat meaningless as an international standard.  Consequently, space entrepreneurs such as Dennis Hope from the Lunar Embassy Corporation seem to think that there is a loophole in Article II which allows private citizens to claim ownership of the moon. Most space lawyers disagree however. They point out that states assume international responsibility for activities in space, whether by national companies or private adventurers, and therefore that the same prohibition extends to the private sector. So while the idea of buying some lunar real estate might be fun, in order for these plots to be recognised as property there needs to be legal recognition by a superior authority such as a nation state. As states are not allowed to claim sovereign rights in outer space, landed property on the moon and planets will in all likelihood be outlawed. Legal commentators are hopeful that states will remain loyal to the treaty and refrain from recognising or endorsing a private property claim. If there is a precedent, it lies at the bottom of the ocean. In 1974, the US government refused to recognise the exclusive mining rights of Deepsea Ventures to the seabed beyond the limits of national jurisdiction. But all of these arguments are rather theoretical. If you just simply occupy a place and no one else can access or use it, aren’t you the de facto owner? Lawyers call this corporate possession (corpus possidendi) and it represents another reason why title deeds cannot be a legal proof of lunar ownership – no one is physically there. In order to possess something, both mind and body need to be involved. Intention alone is not sufficient; possession also requires a physical act.  The difficulty of physically establishing an act of possession on the moon should protect it from private development, but it seems technology is once again outsmarting the law. Back in the late 1990s commercial firm SpaceDev intended to land robotic prospectors on an asteroid to conduct experiments and claim it as private property. The project eventually ran out of funds and was shelved, but advocates of such “telepossession” point to cases of salvage companies claiming undersea wrecks as property after exploring them with robots. After all, if an undersea probe with a TV camera was all that was required to take possession of a (previously owned, earthly) shipwreck, why shouldn’t a space probe be enough to take possession of an unowned and unclaimed patch of celestial real estate? Though legal ownership of the moon or Mars is prohibited, the appropriation of material is a whole different matter. It looks like entrepreneurs could claim something like “enterprise rights” that allows them to explore and exploit natural resources in outer space.  I get the uncomfortable feeling of a déjà vu. Was it not Locke’s property theory that justified possession over nature and vacant land and eventually led to the colonisation of the Americas?  Let’s hope that the international community and individual states come to their senses before it’s too late and get to sign and ratify the Moon Agreement which might give us a little bit of hope that we can avoid another enclosure movement. Recent conflicts over Ukraine, the South China Sea or Syria have raised talk of a “new era in geopolitics”. They may also rekindle the realisation that outer space should not become the next playground for conquest."
"
Share this...FacebookTwitterThe online leftist Die Tageszeitung (TAZ) from Berlin has an essay by Stefan Rahmstorf, who attacks Dr Fritz Vahrenholt, head of RWE Innogy, for his comments in a December essay that casts doubt on man-made climate change appearing in the somwhat more conservative Die Welt from Hamburg.Interestingly, instead of appearing in the Science section, Rahmstorf’s rant appears in the Debate section. Is that a sign?
First Rahmstorf, praises Germany for carrying out a “scientific, factual discussion of climate science”, unlike in the USA, where he says:
It’s different in the USA: There the conservative Tea Party movement has proclaimed that man-made climate change is made up, and large parts of the economy are lobbying using dubious ‘climate sceptic’ propositions.”
Rahmstorf is visibly worried that this “dubious climate scepticism” may be spreading to Germany, and goes after RWE Vahrenholt. In his Die Welt essay, Vahrenholt assigns the blame for the cold winters  – writing:
It’s the sun, stupid!”
Rahmstorf, however does not believe the sun has an impact on the earth’s climate, and thinks it’s all due to a few molecules of CO2. And so Rahmstorf attributes the recent cold winters to miscalibrated human perception:
The winter appears cold because we had gotten used to the mild winters.”
and misleadingly reframes Mojib Latif’s 2008 predictions of cooling:
No serious scientist doubts global warming, and certainly not Mojib Latif, whose quote has nothing to do with the cold winters. It’s old and stems from his model projections of a temporary cooling, which in the meantime we know failed to materialise.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Latif made his cooling projections in 2008, and so are not that old (he predicted warm-winters back in 2000). And who can say that the cold winters aren’t related to his projections of cooling? Latif’s projections look to be true, and likely will be true for the new, current decade.
Rahmstorf then quotes, defends and explains Kevin Trenberth’s infamous “it’s travesty we can’t account for the missing heat” statement, saying it was taken out of context and that Trenberth meant something else.
Rahmstorf acknowledges the solar correlation with regards to the Russian heat wave and flood in Pakistan, but claims that this correlation is very weak, and quotes Trenberth again:
‘Without global warming, these events would not have happened’.”
Now that’s the “scientific, factual discussion” that PIK scientists and the government like to praise. Of course, anyone with even a rudimentary knowledge of science knows that blaming a couple of isolated weather events on global warming is preposterous. But what is preposterous in climate science passes as “scientific, factual” at the PIK.
Rahmstorf takes offence to the harsh criticism that Vahrenholt fired at PIK science, and thinks sceptics give the sun too great of a role in climate change. He says Vahrenholt is silent about the fact…(emphasis added)
…that also during the largest solar minimum of the “Little Ice Age” during the so-called Maunder Minimum of the late 17th century the global temperature was only a few tenths of a degree cooler than before and after, and that our model reproduces the temperature back then very well, otherwise we would have not used it for our future projections.”
A few tenths of a degree Celsius? That’s the difference between having vineyards in England and a frozen Thames? PIK science is moving beyond preposterous.
Rahmstorf ends with a comment on climate debate:
Those who wish to sow doubt on the urgency of climate protection, really have to work hard to twist the facts. However, the climate crisis can be overcome only by having an honest debate.”
Share this...FacebookTwitter "
"

Guest Post by Steven Goddard



Recreational cycling during the summer of 2007

The UK Climate Impacts Programme (UKCIP) is a government funded organization with the following scientifically neutral mission statement on their home page “The UK Climate Impacts Programme (UKCIP) helps organisations to adapt to inevitable climate change. While it’s essential to reduce future greenhouse gas emissions, the effects of past emissions will continue to be felt for decades.“
On their headline messages page they have a list of global warming predictions and supporting evidence.  In this article we will examine some of their claims and evidence.
Claim: Summers will continue to get hotter and drier…

 Evidence: Total summer precipitation has decreased in most parts of the UK, typically by between 10 and 40% since 1961.

According to the UK Met Office, the summer of 2007 was the wettest summer on record.  Summer, 2008 was the wettest on record in Northern Ireland, and broke many local rainfall records in England.  The last hot day in London (30C or 86F) was on July 27, 2006.  London is normally one of the UK’s warmest locations in summer, and it has been 915 days since London has seen any “hot” weather.

Claim: Winters will continue to get milder and wetter…

 Evidence: Average winter temperature for all regions of the UK has risen by up to 0.7 °C since 1914..

The Met office reported last month: “Temperatures from the Met Office have revealed that the UK has had the coldest start to winter in over 30 years.” 
This month, the Met Office reported: “The British Isles has experienced almost a fortnight of freezing conditions. Temperatures as low as -9 °C have been fairly common throughout southern areas of the UK, with temperatures struggling to rise above freezing in some places.“
This winter has not only been unusually cold, but it has also been unusually dry in the UK.

Recreational boating during the winter of 2008-2009
Claim: Some weather extremes will become more common, others less common…

Evidence: The average duration of summer heatwaves has increased in all regions of the UK by between 4 and 16 days since 1961.
Evidence: The average duration of winter cold snaps has decreased in all regions of the UK by between 6 and 12 days since 1961.
Evidence: There has been a trend towards heavier winter precipitation for most parts of the UK since 1961.







As mentioned above, there have been no hot days in the UK for nearly three years.  The current winter has been one of the coldest and driest in recent memory.




Claim: Sea level will continue to rise…

Evidence: Global average sea level rose by between 10 and 20 cm during the twentieth century.
Evidence: The temperature of UK coastal waters has increased by between 0.2 and 0.6 °C per decade since 1985.


It is somewhat surprising that a scientific organisation would use this information in support of global warming.  Sea level has been rising nearly continuously since the end of the last ice age, 15,000 years ago.  The average sea rise rate has been about 80cm/century, 4X-8X higher than UKCIP’s reported current levels.

From: http://www.globalwarmingart.com/wiki/Image:Post-Glacial_Sea_Level_png
Additionally, there has been little change in sea level rise rates over the last 100 years.

From: http://www.globalwarmingart.com/images/thumb/0/0f/Recent_Sea_Level_Rise.png/700px-Recent_Sea_Level_Rise.png
Regarding their discussion of UK sea temperatures since 1985, there hasn’t been much glacial activity in the UK over the last 25 years and it is unlikely that UK ice sheet melt is adding much to sea level.  Their reported UK SST changes are more likely due to ocean circulation patterns like the AMO.  Current SST anomaly maps show ocean temperatures around the UK near or below normal.  And according to the University Of Colorado, global sea level has scarcely risen since 2005.

From: http://sealevel.colorado.edu/current/sl_noib_global_sm.jpg
One might think that taxpayer funded organisations like UKCIP would be required to keep their public statements a bit more up to date and accurate.





			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99384947',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"A cross-party committee of MPs, green campaigning groups, business leaders and climate experts is needed to advise the government on crunch UN climate talks later this year to put the UK’s hosting of the COP26 talks back on track, the Liberal Democrat acting leader, Sir Ed Davey, is expected to say. His call, which will form part of a speech on climate delivered at Birkbeck College in London on Thursday, comes after the energy minister, Kwasi Kwarteng, told a meeting of ambassadors that the UK could not afford to allow the talks to fail because of the additional pressures of Brexit.  “If you want global leadership [on the climate crisis], you have to show that global action on the climate can be politically popular,” Davey, former energy and climate secretary in the Conservative-Lib Dem coalition government, told the Guardian. “We need NGOs, politicians and business leaders to form a committee to help to reach greater dialogue on international action. The UK needs to lead on this, for COP26 and beyond.” Davey criticised the government’s handling of COP26 planning so far, which got off to a bad start nearly two weeks ago with the abrupt sacking of the intended president, former energy minister Claire O’Neill, days before the scheduled launch. O’Neill went on to make vitriolic public attacks on Johnson. “It has been shambolic,” said Davey. “Insiders are tearing their hair out. If you want to be credible hosts, you have to engage with people, with society, with NGOs far more.” Leading figures in international climate diplomacy have expressed increasing disquiet over the UK’s conduct of the talks so far, warning that ministers and officials are falling behind on the preparations needed. Mary Robinson, former president of Ireland and twice a UN climate envoy, said the perception that no one in the UK wanted to serve as president – after the post was turned down by David Cameron and former foreign minister William Hague – was damaging. Kwarteng was forced on Wednesday morning to reassure international participants that the talks would be a success and that the government was treating them as a priority. He attended a high-level meeting of previous COP presidents held by the International Energy Agency in Paris, where he was questioned by the Kenyan government and others on the UK’s commitment. “We can’t guarantee success, it’s not something we can gold-plate, but this is absolutely our No 1 priority as a government,” he said. “We really cannot afford it to be a failure, at an international level and at a national level, given where we are with Brexit and other issues.” On Wednesday, fresh controversy over the Glasgow venue for the talks added to the sense of confusion, after a government source told the Financial Times that ministers were considering booking the ExCeL conference centre in London as an alternative. The Scottish government had received no notification of any intention to move the venue, a spokesperson said. “Scotland looks forward to welcoming the UN delegates and participants from around the world. Glasgow is recognised internationally for its strong track record at hosting major international events, and we are working collaboratively with the UK government, Glasgow city council, Police Scotland and other partners to ensure the conference is a success.” UN officials visited the Glasgow venue this week and liaised with all the authorities involved. Nicola Sturgeon, Scotland’s first minister, told a conference in London on Tuesday that she hoped there could be a constructive relationship between her government and Boris Johnsons’s in the approach to COP26. Davey said changing the venue would “send the wrong signals, especially post-Brexit. We are stronger in the climate change debate internationally if we are a united kingdom.” Greenpeace welcomed Davey’s call for an independent committee that would include civil society. Rebecca Newsom, head of politics, said: “There’s certainly a need for a more transparent, inclusive and joined-up approach to the UK climate summit, and this could be one of the ways to achieve it. Cross-party committees have a strong record of holding governments to account, and given how planet-critical this summit is, the case for more scrutiny and transparency is overwhelming.” The Green party’s co-leader Jonathan Bartley said: “At the moment COP26 is heading for failure because the government is unwilling to step up and take the responsibility and action necessary to facilitate its success. So, we’re on board with the fact that scrutiny of the process is crucial. “But we would need to see what actual powers this cross-party committee would have and to what extent the government will be required to act on its recommendations. It also needs to be linked to a grassroots-led proper democratic programme of engagement that genuinely seeks to empower and strengthen communities.”"
"It was our 14th expedition to the trenches of the Pacific Ocean, where depths can exceed 10,000m. And it was due to be our last for the foreseeable future. We had been aboard the Schmidt Ocean Institute’s (SOI) vessel RV Falkor for 30 days. It was almost over. Then, it turned out to be “the big one”. For this was the expedition in which my colleagues and I discovered a snailfish living some eight kilometres below the waves, deeper than any fish we know of. My colleagues from the University of Hawaii even recovered some in their traps.  In the past six years we have made many discoveries in the depths, such as the missing order of Decapoda (shrimps) that were long thought absent from the trenches but are actually rather conspicuous.  In the Kermadec Trench off New Zealand we found the “supergiant” amphipod, a crustacean 20 times larger than its shallow-sea relatives. We also filmed large numbers of tadpole-like snailfish in multiple trenches, and as deep as 7700m in the Japan Trench. Based on these observations we predicted that when exploring the Mariana Trench – the world’s deepest – we would find the the Mariana’s own personal snailfish, probably living between 6500m and around 7500m, with more being found at the deeper end of that range. We also predicted that we would see the decapods and supergiants in the upper depths of the trench, and right enough there they were. A device used to gather samples of ocean floor had an inspection camera on it to monitor the equipment.  One night after a dive to 7900m when watching the footage coming back in, a strange ethereal little fish swam past.  That got our eyebrows raised.  It looked like a snailfish, but was extremely fragile (even for a snailfish) and had a very distinctive appearance. This prompted a case of “game on”, to find it again, and sure enough we did.  The deepest we found it was at 8145m, nearly 500m deeper than our personal record from the Japan Trench. This of course means that our predictions were slightly wrong, but also makes it very exciting: there  are still fish, and perhaps other things, down there to discover and this is what drives us to do more. Our work at the deepest place on Earth is not done yet. As much as we are excited about finds such as these, we are typically chased up by people who ask “why do we bother?”, and add rather deflating comments such as “what benefit does this have to society?” In response I explain that such exploration benefits responsible stewardship of the oceans. In the long term, conservation and maintenance of the our seas relies on us really understanding the ocean – that is, the ocean in its entirety from the surface to what lies beneath the deepest seafloor. The anthropocentric opinion of “out of sight, out of mind” simply doesn’t cut it, and is sadly still common place. The deep ocean is far deeper than a person can dive to or fish from, but that doesn’t mean that the things down there are of no consequence to society.  We must not, however, confuse curiosity-driven exploration with the search for entertainment or stockpiling consumables. We know that the deep sea is not exempt from a changing climate or man-made disturbances such as plastic pollution. The depths are intrinsically linked to processes in the upper ocean that we humans are continually meddling with. Changes that happen in the upper ocean will have an effect on the largest habitat on Earth, yet people question why we study the deep sea.  We say, how can we conserve the largest habitat on Earth if we know nothing about it?  In the quest to understand the entire ocean, people have to study the shallow bits, the deepest bits and everything in-between."
"Although only a small area of land has been offered to companies exploring the potential for fracking in the UK so far, much more is likely to come. But opposition to fracking is growing – and growing fast. More than 180 local groups are already in operation, which is somewhat inconvenient for a government wanting to go “all out for shale”. In interviews, online surveys and correspondence with anti-fracking protesters, we’ve heard personal testimony that suggests that problems with fracking are not simply environmental. More than 400 peaceful protesters have been arrested – and people in the anti-fracking movement have claimed political policing and intimidation are being used, citing the actions of the Greater Manchester Police and Sussex Police at these two locations as particular examples. Our analysis of interview and questionnaire data suggests that protesters consider their rights to freedom of peaceful assembly, freedom of expression, liberty and security of person, a fair trial and respect for a private and family life, have been threatened. Each of these rights is protected by the Human Rights Act, the European Convention on Human Rights, and the International Covenant on Civil and Political Rights – all of which the UK is legally bound to observe. A report launched on October 30 from the Bianca Jagger Human Rights Foundation details the UK’s human rights commitments and calls for a fully independent human rights impact assessment of fracking developments before exploration gets underway. Protesters in Balcombe and Barton Moss reported facing violence, forcible removal without arrest and kettling. Interview respondents told us they were “kicked and pushed and punched”, “pushed and shoved in the back”, “pushed off the road by the police”, and “shoved in the back repeatedly”. Police behaviour was often described as “brutal”, “violent”, “thuggish”, “rough” and “very, very aggressive”. References were made in several interviews to “arrest quotas”. At Barton Moss, throughout the autumn and winter of 2013, one interview respondent reported that there were five arrests every day and that officers were heard saying “we need one more arrest”. The same respondent said they believed that the use of arrest quotas was “almost certainly planned in advance” and was being used as a long-term plan that would ensure “eventually everyone would be arrested”. Greater Manchester Police has denied these claims. Our data suggests the anti-fracking movement is concerned that such patterns of arrest may effectively serve to shut down protests – and certainly if protesters are arrested at one event and then again at another for breaching their bail, the logical conclusion is that, over time, fewer protesters would attend demonstrations. Queries were also raised about the legality of some of the arrests made at Barton Moss. Protesters say they were were accused of obstructing a public highway when they were on a private road leading to a drilling site. Interviewees said protesters at both Balcombe and Barton Moss were arrested for “obstructing a police officer” if they fell over in front of them. Several interview respondents raised concerns that emails, phones and social media were being monitored by the police. Although, as one interviewee indicated, such activities are difficult to prove, some respondents were insistent in their belief that it was happening. One even reported having been visited at home by two members of the Counter Terrorism and Domestic Extremism Unit after filming at a potential drilling site. The increasing opposition to fracking suggests the government has failed to adequately respond to local and national concerns over the human rights implications of fracking or provide sufficient opportunities for public participation in decision making. Both interview and survey respondents expressed dissatisfaction with the amount of consideration being given to their concerns as this controversial energy source is explored. No wonder, then, that some said the process of introducing fracking in the UK is eroding democratic values. Respondents expressed the feeling that a particular disregard had been shown for people living in proximity to exploratory sites and a disillusionment with the official avenues of complaint on offer. Not all anti-fracking activity in the UK has been lawful. It has included the occupation of energy firm Cuadrilla’s offices in Blackpool and the blockading of roads to exploratory drilling sites. But these events indicate the extent to which people have lost faith in the government’s approach. It seems likely that acts of both civil disobedience and peaceful protest will occur with increasing frequency as local communities realise the extent of the extractive industry’s impact. In anticipation of fracking development, the government needs to commission genuinely independent human rights impact assessments for all communities before any extractive activity begins. The potential rights violations described above suggest that fracking development can no longer be considered in separation from the civil and political sphere. These examples show the extent to which the fundamental rights of UK citizens who oppose governmental policy are at risk."
"
Share this...FacebookTwitterClimate journalism is like being at a third-world bazar where the media behave like merchants all shouting, pitching their catastrophe stories.Die Welt’s recent piece From Proud Jordan River, To A Smelly Trickle (roughly translated) features the crisis of water consumption and the injustice of water’s uneven distribution. Now water needs to be redistributed, along with wealth and misery.
Although the article is mainly a rant against Israeli water policy, its other objective is to admonish western societies for their profligate use of water.
This is a theme that’s steadily gaining traction on the environmental front here in Europe – along with biodiversity, ocean acidification, manmade microscopic aerosols and climate change. It’s the latest hot-seller catastrophe joining the enviro-bazar.
Die Welt doesn’t hold back citing environmental and activist groups for its reliable, “unbiased” and shocking information. At first the story focusses on Israeli water management and how it’s unfair to neighboring countries.
Die Welt writes:
According to Amnesty International, the average Israeli consumes 300 liters of water daily, while a Palestinian consumes only 70 liters. In poor regions a mere 20 liters is available daily for each person.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




At the end of the story, Die Welt admonishes western lifestyles and its excessive use of water.
The story concludes with a photo gallery that informs readers how much water consumption is needed to manufacture some basic daily products we enjoy in our daily lives. Examples:
1 hamburger: 2400 litres
1 hardboiled egg for breakfast: 135 litres
1 slice of bread: 40 litres
10 grams of cheese: 50 litres
1 cup of coffee: 140 liters
1 German breakfast: 365 liters
200 grams of potato chips: 185 liters
2-gram computer chip: 32 liters
1 sheet of paper: 10 liters
1 cotton T-shirt: 4100 liters!
1 pair of cowhide shoes: 8000 liters
1 new car: 450,000 liters
The idea is to tell us consumers that we are simply consuming too much water and that it’s having catastrophic impacts on the environment and poor people. It’s unfair and it has to be regulated. We need to feel guilty about it.
Wikipedia lists the potential manifestations of excessive water consumption:
There are several principal manifestations of the water crisis.
– Inadequate access to water for sanitation and waste disposal for 2.5 billion people
– Groundwater over drafting (excessive use) leading to diminished agricultural yields
– Overuse and pollution of water resources harming biodiversity
– Regional conflicts over scarce water resources sometimes resulting in warfare.
Expect the water crisis to get worse (not in reality, but in the media and political world). Get ready to hear a lot more about this in the future. Water-saving devices will be joining energy-saving devices soon in the government’s force-the-people-to-buy-list.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterScience turns malignant
Next week on August 3, I’ll be releasing my latest list of climate scandals, a gate-update (see Current list of climate scandals). This is a month earlier than originally planned.
The list that’s posted now is visited on average about 100 times a day. Clearly it has become some sort of resource.
Unsurprisingly, the new list coming out has grown, and it will continue to grow. This is assured because of the way climate science is operated, funded, rewarded and politicised.
The disease is hopelessly chronic and there is no treatment in sight. The system is designed, built and programmed to keep producing many more scandals. Already I see dozens of new gates in incubation.
“Climate science” even has its own immune system that works to keep out the deadly virus called “truth”.  The reality of that immune system became clear with the Muir Russell, Oxburgh and Penn State enquiries.
Soon I will have to break the climate-gate list into Volumes 1 and 2.  I have no doubts about this.
The real threat, unfortunately, is that “climate science” risks becoming a malignant cancer that will threaten to spread to other fields of science and to our public institutions. In some cases it already has. Because of “climate science”, the public is losing trust in science and the civic institutions that are supposed to police it. As mistrust of climate science reaches ever higher levels, so will the public mistrust in other fields.
Scientists in these other fields need to take notice.
Share this...FacebookTwitter "
"
Recently we’ve been discussing products for the AIRS satellite instrument (Atmospheric InfraRed Sounder) onboard the Aqua satellite. For example we’ve been looking at the only global image we can find of CO2 from its data made in 2003, wondering where the remainder of them are.
In my digging I discovered that the Apache webserver had open directory listings for folders, and this allowed me to explore a bit to see what I could find. in the \images folder I found a few images that I did not see published on the AIRS website. I’ve saved them to my server should they go offline, but have provided links to the original source URL.
One for Sea Surface Temperature at the tropics seems interesting, though the data period is too short to be meaningful. Note that to eliminate cloud issues, the soundings are done when the satellite has a lookdown to “clear sky”.
Original source image: http://airs.jpl.nasa.gov/images/Aumann_SST_graph_543x409.jpg
I find it interesting that there is a slight global cooling of the oceans during this period of September 2002 to August 2004. The question is: where is the rest of the data and why has the AIRS group not been presenting it on their website? It is after all a publicly funded NASA program.
It is also interesting that this goes against one of the “signatures” of an AGW driven warming. Dr. David Evans writes in this essay:

“The signature of an increased greenhouse effect  is a hotspot about 10 km up in the atmosphere over the tropics.”

“The signature of an increase in well-mixed greenhouse gases (such as due to carbon emissions). Warming would be concentrated in a distinct “hot spot” about 8 – 12 km up over the tropics, less warming further away, turning to cooling above 18 km.”


What I’d REALLY like to see is the January version of this map:

Unfortunately, the January version of this image is unavailable. It would be interesting to see if the concentrations in the northern hemisphere maintain which would point to industrialization sources. Or, if the pattern flips, and we see concentrations decrease in the NH and increase in the SH, that would point to seasonal variation and thus likely be driven by biomass.
I’ve put in a request to the AIRS group for the January 2003 image, and others, we’ll see what happens.
UPDATE: 7/31/08 I got a response, see this new posting


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d9da21d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter**  BREAKING NEWS!   **  BREAKING NEWS! **  BREAKING NEWS! **
Stunning!
Another huge slab of the Climate-Berlin-Wall has fallen. It’s a climate skeptic jail-break! I imagine the Climate-Politburo members must be quivering and trembling in their bunkers in Potsdam by now.
Big hat-tip to NTZ reader Ike.
A leading German news magazine has decided to depart from the dogma of angst and catastrophe and bring up climate science issues that, up to now, have been strictly taboo here in the Vaterland. Tomorrow FOCUS magazine will come out with its newest issue titled:
Prima Klima! Umdenken:Wieso die globale Erwärmung gut für uns ist. (Best Climate! Change of Thinking: Why global warming is good for us.)
Change of Thinking – yes! And the timing couldn’t be better.
Folks, this is the first time in a long time that a major German news magazine has decided to do a little investigative reporting, instead of relying on the press releases from the Palaces Of Panic like the Potsdam Institute of Climate Impact Research, NOAA, Alfred Wegener Institute, etc., and seriously look into this controversial global issue. Game over comrades!
When the global warming hoax collapses in Germany, then Europe follows right behind it – and then, of course, the rest of the world. Germany is that one domino. This represents a major setback for the warministas. Indeed it would be interesting to know what went on in the FOCUS editorial offices.
Perhaps the normally über-alarmist FOCUS has already gotten tired of the winter and longs for the warmer days. I can’t explain why they are coming out with such an issue – especially during Cancun. Whoa! That’s all I can say.
Here’s what tomorrow’s issue will feature:
78   Warm times are good times. Harvest yields increase, Forests grow, deserts shrink
86   Which impacts of climate change are proven?  Which are not?
90   The “Who is who“ in climate science


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Promo video (in German, see English text below):
http://www.focus.de/magazin/videos/focus-titel-prima-klima_vid_21483.html
The video begins with:
This week in the coming FOCUS: Best Climate. Change in thinking – global warming is good for us. FOCUS editor Dr. Christian Pandler (sp?) researched the current topic and reports on it in the new issue.
Editor Dr. Pandler (a bit paraphrased):
In the new FOCUS issue, we take a look at the question of climate change. This week the world climate conference is taking place in Cancun, where world leaders are going to debate over how to combat warming. Our question: Is global warming actually bad? Does it entail only disadvantages and only catastrophic consequences? Up to now, people have only focused on what will be bad. The question is what could be good? No one has really looked at this. It’s taboo in Germany.
We know from history that warm periods were good periods for us. Cold periods were bad periods. We know that 20,000 years ago Europe was a frozen wasteland where nobody lived. That was a real climate catastrophe. For example we had a warming 10,000 years ago, which led to a greening of the Sahara. Then there was cooling which led it to be parched again. Now it’s warming, and there are lots of signs that show it is greening up again. For the people in Africa, it is absolutely a positive development. If it continues that way, it could once again become green with a variety of wildlife, rivers and lakes and so on. This is a consequence that hardly has been discussed.
We’ve spoken to scientists who are there on site. One researcher in particular has gone there every year for 30 years and photographed how the Sahara is gradually getting greener.”
Go out and reward FOCUS by buying this issue – get an additional copy for a friend too. Thank the editor for having the guts to do this.
In the meantime, my advice to that brave editor at FOCUS: Put on your bullet-proof political vest and find the deepest possible bunker. The greenshirts are sending over the B-52s! Achtung!
This is going to be something to relish.
Ironically this comes out precisely when the science is showing signs that cooling is coming instead, and so FOCUS may be only getting false hopes up. Lol! You just can’t make this stuff up. It makes my day.
Share this...FacebookTwitter "
"

“Shut down the World Bank!” reads one of the many placards being carried through Washington D.C. As the international institution holds its annual spring meeting, thousands of activists are venting their anger about everything from world poverty to environmental degradation. While they are correct to point an accusatory finger at the World Bank for making those problems worse, their well‐​publicized efforts will do little good. The bank has become adept at co‐​opting key elements of its opposition in order to keep billions in U.S. government money flowing in its direction. 



Since first being stung by environmentalist criticism in the early 1980s, the World Bank has repeatedly promised to reform itself. Despite those promises, the bank has neither fundamentally reformed its lending practices nor radically changed the kinds of projects that receive its funding. Bank projects around the world remain environmentally destructive and socially disruptive. By its own account, 2.6 million victims of World Bank lending in poor countries are having their property confiscated, their homes destroyed, and their livelihoods ruined. 



When the bank hears public criticism of its destructive environmental record, it typically issues a press release announcing renewed commitment to old promises made in prior years’ press releases. After two decades of this behavior, there is little evidence of tangible improvement. In a recent review, the bank judged that 25 percent of its projects in 1997–98 had an unsatisfactory outcome, even by its own rather generous standards. It also concedes that only 54 percent of its projects completed could be judged “sustainable,” even though sustainable development is now supposed to be a main purpose of the bank. 



Environmental groups also have a difficult time identifying reform successes. A 1999 environmentalist report labeled the World Bank’s reform program a “failure,” noting that it had not produced more environmentally sound projects or a greater level of bank accountability to the public. According to the report, endorsed by the Environmental Defense Fund, Friends of the Earth, Greenpeace and the Sierra Club, “an evaluation of the World Bank Group’s portfolio shows that it does not promote environmental protection in its operations and loans.” 



The bank’s most noticeable improvement in recent years has been in its ability to weather criticism, exposure and political opposition. It has actively courted the support of environmental advocacy groups and has rhetorically embraced their “sustainable development” creed. The bank cooked its books to make it appear that environmental spending had gone up and provided generous grant funding to nongovernmental organizations (NGOs) in order to make them more dependent on its continued existence. It has invited green organizations to its lavish headquarters to solicit their advice and to enter into partnerships that improve the bank’s image. Despite its documentation of egregious abuses at the World Bank, the leadership of the anti‐​bank movement does not advocate — as it sometimes did — the only real solution to an institution that has demonstrated zero capacity for genuine reform: elimination. 



Why has the bank’s opposition gone soft? The World Bank, recognizing that environmentalist opposition in the early 1990s threatened its very existence, made a concerted effort to meet this challenge. An internal World Bank report identifies the key to its survival continuing “to build a public constituency for the Bank’s policies and programs, and for development assistance in general.” Central to its strategy was a systematic effort to convert the environmental movement to an accommodationist stance toward the bank. The greens must advocate incremental changes to the institution, not its elimination. 



Beginning in the early 1990s, the bank contacted its most influential environmental critics and invited them to “participate” in the bank’s work. According to the bank, this activity has caused “a reduction in NGO criticism” and a recognition on the part of NGOs that they and the bank share common interests. The bank claims that a chief benefit of appeasing the environmental movement is to “improve the overall climate of opinion around the Bank’s work.” Most important, environmental groups tempered their criticism of the bank during key moments when its budget was being debated in Congress. 



The environmental groups are now committed to working with and improving the World Bank. Implicitly, they believe that World Bank planners, who once ignored environmental considerations with tragic consequences, can now be trusted to implement major development decisions in the Third World. They suggest that the bank can become an instrument of “sustainable development.” 



The World Bank’s latest reform effort is eerily reminiscent of earlier efforts that environmental groups denounced as inadequate and diversionary. But this strategy has been effective enough to secure funding prospects on Capitol Hill. If the anti‐​bank protestors really want to “Break the Bank,” as their signs say, they will work harder to ensure that their own organizations do not compromise on this objective.
"
"

This was an eventful week for two government institutions, the Supreme Court and Senate. More than a year after Justice Antonin Scalia’s death, the high court will on Monday finally return to a full complement of nine justices. But the confirmation of the newest justice, Neil Gorsuch, happened only after the Senate decided, on a party‐​line vote, to exercise the “nuclear option” and remove filibusters for Supreme Court nominations.



These developments sound like a really big deal, but they were easily predictable given our toxic political climate and won’t actually change the operation of either institution. But here are five takeaways for our post‐​nuclear‐​option world:





By filibustering the milquetoast Gorsuch despite the high probability and repeatedly expressed intention of the Republicans to go nuclear, the Democrats have destroyed any leverage they had over the next nominee.



 **1.** The Supreme Court. The court effectively returns to the status quo before Scalia’s death. No two justices are the same, but Gorsuch could have been expected to vote the same as Scalia on all the hot‐​button cases that broke down 5–4, and also on the cases (especially in criminal procedure) that joined the court’s left and right against the middle. As it turns out, Scalia’s absence only changed the result in a handful of cases and the court has largely succeeded in avoiding 4–4 splits. Adding a ninth justice will, however, make it marginally easier to get the four votes needed to “grant cert” (have a case accepted for review), especially on potentially controversial issues.



 **2.** The Senate. The exercise of the “nuclear option” returns Senate procedures to what they were 15 years ago. The filibuster was simply not employed for partisan purposes against a nominee who had majority support before Harry Reid started filibustering George W. Bush’s lower‐​court nominees in 2003. (Infamously, the Senate denied Miguel Estrada an up‐​or‐​down vote seven times to prevent Bush from later having the opportunity to elevate the first Hispanic justice.) Reid used the “nuclear option” to eliminate that sort of filibuster a decade later, so perhaps this week’s action should be called “thermonuclear.” A Senate majority will still be able to stall a nomination made by a president of the opposing party—we could see more Merrick Garlands—but a Senate minority will lack that power.



 **3.** The next nominee. By filibustering the milquetoast Gorsuch despite the high probability and repeatedly expressed intention of the Republicans to go nuclear, the Democrats have destroyed any leverage they had over the next nominee. Should there be another vacancy under President Trump while the GOP controls the Senate, there will be zero incentive for the President to moderate his choice. It’s not at all clear that Republican senators such as Susan Collins of Maine, Lisa Murkowski of Alaska, Lindsey Graham of South Carolina and other “institutionalists” would’ve gone along with a “nuclear option” to replace Justice Ruth Bader Ginsburg with a nominee more controversial than Gorsuch. But now they won’t face that dilemma.



 **4.** Our political culture. Given the highly charged battle we’ve seen — only three Democrats, from states Trump won bigly (Indiana, North Dakota, West Virginia), voted for Gorsuch, and just one more, fellow Coloradan Michael Bennet, voted against a filibuster — too many people will now think of the justices in partisan terms. That’s too bad, but not a surprise when contrasting methods of constitutional and statutory interpretation largely track party politics. Relatedly, confirmation hearings will continue to be Kabuki theater, educational to some about various legal doctrines but not illuminating anything of the nominee’s judicial philosophy. On the other hand, perhaps nominees will occasionally feel free to express themselves, knowing that they don’t need any of the minority party’s votes.



 **5.** Neil Gorsuch. You may not agree with him on every case, but his opinions will be well‐​reasoned and clearly written. Gorsuch’s mentor, Justice Byron White, liked to say that each new justice makes for a new court, and I look forward to the breath of fresh air, intellectual rigor, collegiality, and constitutional seriousness that Justice Gorsuch will bring. Neil Gorsuch will serve with distinction.
"
"
Share this...FacebookTwitterClimate nonsense will lead to this trend in electricity prices.

German journalist Günter Ederer has a piece at the online Fuldaer Zeitung called The Electricity Bill Is Going Up And Up. Hat-tip Detmar Doering.
Politicians in all parties in Germany, from the communists to the conservatives, and everything in between, are all racing to be the first to jump off the let’s-save-the-climate cliff.
All have made rescuing the climate a target that absolutely has to be achieved – no matter the cost. Politicians of every stripe have pledged to cut Germany’s CO2 emissions 80% by 2050. Minister of the Environment Norbert Röttgen, of the conservative (in name only) party, even sets it up as a life and death matter, as absurd as it sounds:
That rescues our climate.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




No kidding. Everything else takes a back seat to this imperative – consumers be damned. Taking the little guy to the cleaners in Germany knows no limits. And so, expensive green energy sources like wind and solar are being subsidized with total abandon, and consumers are getting the big-time shaft through skyrocketing electric bills- The government profits in the end. Ederer writes:
Renewable energy must be fed into the power grid at a mandated fixed price, which for the consumer will mean a price increase of 6 cents per kwh for 2011 alone.
What benefit will all this pain render consumers? It’s a fact that CO2 emissions globally are going to continue increasing, as most countries outside of Europe could give a rat’s rear about foregoing prosperity in order to play the make believe game of rescuing the planet.
If Germany succeeded in reducing its CO2 emissions 80% by 2050, what theoretical impact would it have on the environment? Ederer tells us:
If Germany reduced its share of CO2 by 80%, or even 100%, then it help to warm the planet 0.0072 °C less. As I said earlier, that’s if all figures and calculations of the IPCC are correct.
Meanwhile, the numbers are in from the German Weather Service. October 2010 was 0.9°C colder than average. If Germany’s greedy politicians get their way. This may someday be corrected to 0.9072°C cooler.
Share this...FacebookTwitter "
nan
"
I don’t know what it is with weather stations at some universities. Of course we have the station at University of Arizona Tucson in the parking lot, and this one isn’t too far from that arrangement. It has a long and uninterrupted history, but what is it really measuring?

Click for a larger image
More pictures here
Thanks to surfacestations.org surveyor Craig Limesand we get to see the official USHCN climate station of record at Mount Mary College in Milwaukee, Wisconsin. You can see that the Stevenson Screen is just a few feet from parked cars.
This aerial view shows it better:

Click for a larger live interactive image
Note that in addition to being surrounded by asphalt and parked cars, the station is also about 35 yards from the college power plant.
According to NCDC MMS database, the station has been in this location since at least 1948, unmoved and using mercury max-min thermometers even today.
But without doing a historic evaluation to look at what transpired around the station during that history, how would we know how much is this signal is “climate change”, “UHI from Milwaukee”, or “increased parking capacity” or all of the above?
From NASA GISS, click for original source plot
 As much as I like weather stations, it is becoming clearer to me that looking for a clean climate change signal in surface data is a complex excercise in uncertainty.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9dd7a201',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
 
Guest post by Frank Lansner, civil engineer, biotechnology.
(Note from Anthony – English is not Frank’s primary language, I have made some small adjustments for readability, however they may be a few  passages that need clarification. Frank will be happy to clarify in comments)
It is generally accepted that CO2 is lagging temperature in Antarctic graphs. To dig further into this subject therefore might seem a waste of time. But the reality is, that these graphs are still widely used as an argument for the global warming hypothesis. But can the CO2-hypothesis be supported in any way using the data of Antarctic ice cores?
At first glance, the CO2 lagging temperature would mean that it’s the temperature that controls CO2 and not vice versa.

Click for larger image Fig 1. Source: http://www.brighton73.freeserve.co.uk/gw/paleo/400000yrfig.htm
But this is the climate debate, so massive rescue missions have been launched to save the CO2-hypothesis. So explanation for the unfortunate CO2 data is as follows:
First a solar or orbital change induces some minor warming/cooling and then CO2 raises/drops. After this, it’s the CO2 that drives the temperature up/down. Hansen has argued that: The big differences in temperature between ice ages and warm periods is not possible to explain without a CO2 driver.
Very unlike solar theory and all other theories, when it comes to CO2-theory one has to PROVE that it is wrong. So let’s do some digging. The 4-5 major temperature peaks seen on Fig 1. have common properties: First a big rapid temperature increase, and then an almost just as big, but a less rapid temperature fall. To avoid too much noise in data, I summed up all these major temperature peaks into one graph:

Fig 2. This graph of actual data from all major temperature peaks of the Antarctic vostokdata confirms the pattern we saw in fig 1, and now we have a very clear signal as random noise is reduced.
The well known Temperature-CO2 relation with temperature as a driver of CO2 is easily shown:

Fig 3.
Below is a graph where I aim to illustrate CO2 as the driver of temperature:

Fig 4. Except for the well known fact that temperature changes precede CO2 changes, the supposed CO2-driven raise of temperatures works ok before temperature reaches max peak. No, the real problems for the CO2-rescue hypothesis appears when temperature drops again. During almost the entire temperature fall, CO2 only drops slightly. In fact, CO2 stays in the area of maximum CO2 warming effect. So we have temperatures falling all the way down even though CO2 concentrations in these concentrations where supposed to be a very strong upwards driver of temperature.
I write “the area of maximum CO2 warming effect “…
The whole point with CO2 as the important main temperature driver was, that already at small levels of CO2 rise, this should efficiently force temperatures up, see for example around -6 thousand years before present. Already at 215-230 ppm, the CO2 should cause the warming. If no such CO2 effect already at 215-230 ppm, the CO2 cannot be considered the cause of these temperature rises.
So when CO2 concentration is in the area of 250-280 ppm, this should certainly be considered “the area of maximum CO2 warming effect”.
The problems can also be illustrated by comparing situations of equal CO2 concentrations:

Fig 5.
So, for the exact same levels of CO2, it seems we have very different level and trend of temperatures:

Fig 6.
How come a CO2 level of 253 ppm in the B-situation does not lead to rise in temperatures? Even from very low levels? When 253 ppm in the A situation manages to raise temperatures very fast even from a much higher level?
One thing is for sure:
“Other factors than CO2 easily overrules any forcing from CO2. Only this way can the B-situations with high CO2 lead to falling temperatures.”
 
This is essential, because, the whole idea of placing CO2 in a central role for driving temperatures was: “We cannot explain the big changes in temperature with anything else than CO2”.
 
But simple fact is: “No matter what rules temperature, CO2 is easily overruled by other effects, and this CO2-argument falls”. So we are left with graphs showing that CO2 follows temperatures, and no arguments that CO2 even so could be the main driver of temperatures.
– Another thing: When examining the graph fig 1, I have not found a single situation where a significant raise of CO2 is accompanied by significant temperature rise- WHEN NOT PRECEDED BY TEMPERATURE RISE. If the CO2 had any effect, I should certainly also work without a preceding temperature rise?!  (To check out the graph on fig 1. it is very helpful to magnify)
Does this prove that CO2 does not have any temperature effect at all?
No. For some reason the temperature falls are not as fast as the temperature rises. So although CO2 certainly does not dominate temperature trends then: Could it be that the higher CO2 concentrations actually is lowering the pace of the temperature falls?
This is of course rather hypothetical as many factors have not been considered.

Fig 7.
Well, if CO2 should be reason to such “temperature-fall-slowing-effect”, how big could this effect be? The temperatures falls 1 K / 1000 years slower than they rise. 
However, this CO2 explanation of slow falling temperature seems is not supported by the differences in cooling periods, see fig 8.
When CO2 does not cause these big temperature changes, then what is then the reason for the  big temperature changes seen in Vostok data? Or: “What is the mechanism behind ice ages???”
 
This is a question many alarmists asks, and if you can’t answer, then CO2 is the main temperature driver. End of discussion. There are obviously many factors not yet known, so I will just illustrate one hypothetical solution to the mechanism of ice ages among many:
 
First of all: When a few decades of low sunspot number is accompanied by Dalton minimum and 50 years of missing sunspots is accompanied by the Maunder minimum, what can for example thousands of years of missing sunspots accomplish? We don’t know.
 
What we saw in the Maunder minimum is NOT all that missing solar activity can achieve, even though some might think so. In a few decades of solar cooling, only the upper layers of the oceans will be affected. But if the cooling goes on for thousands of years, then the whole oceans will become colder and colder. It takes around 1000-1500 years to “mix” and cool the oceans. So for each 1000-1500 years the cooling will take place from a generally colder ocean. Therefore, what we saw in a few decades of maunder minimum is in no way representing the possible extend of ten thousands of years of solar low activity.
It seems that a longer warming period of the earth would result in a slower cooling period afterward due to accumulated heat in ocean and more:

Fig 8.
Again, this fits very well with Vostok data: Longer periods of warmth seems to be accompanied by longer time needed for cooling of earth. The differences in cooling periods does not support that it is CO2 that slows cooling phases. The dive after 230.000 ybp peak shows, that cooling CAN be rapid, and the overall picture is that the cooling rates are governed by the accumulated heat in oceans and more.
Note: In this writing I have used Vostok data as valid data. I believe that Vostok data can be used for qualitative studies of CO2 rising and falling. However, the levels and variability of CO2 in the Vostok data I find to be faulty as explained here:
http://wattsupwiththat.com/2008/12/17/the-co2-temperature-link/

Sponsored IT training links:
Pass PMI-001 exam fast using self study 70-290 guide and 350-029 tutorial.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e98b94e93',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The Victorian Liberal Katie Allen has declared the world is approaching an “iPhone moment” when it comes to new technology lowering greenhouse gas emissions, and Australia needs to be part of the revolution, rather than being a technology “taker”. Allen has joined fellow Liberal Trent Zimmerman in noting the Coalition’s decision to sign the Paris agreement means Australia has already committed to achieving carbon neutrality by mid-century. But she said the government should not nominate a specific date to hit the milestone until it had developed a policy roadmap.  She told Guardian Australia the smart phone had provided the requisite technological step change in communications, but there was unlikely to be a single technology in energy to drive the transformation to low emissions. Allen said there was “a bit of an arms race” in low emissions technology, and she nominated batteries, green hydrogen and “new nuclear” – modular reactors – as, potentially, part of the transformation. “I believe we need to be open-minded because it is hard for governments to pick commercialised and scalable winners,” she said. Asked whether she supported adopting a target of net zero by 2050, Allen said: “I want to see the roadmap, and how we would get there.” She noted the states had already made commitments to carbon neutrality, and that would be an important part of the picture, and said she was “very interested in an economically sound approach”. Allen’s comments join Zimmerman’s call for the government to look “very seriously” at adopting a net zero target. On Sunday night, fellow moderate Jason Falinski also told Sky News the government needed to set a roadmap for the transformation, and he would like Australia to hit net zero before 2050 “if we can”. On Tuesday The Australian reported that Scott Morrison may adopt a technology investment target as part of an Australian government push to avoid setting a more explicit net zero emissions target at the next UN climate summit in Glasgow in November. The Business Council of Australia estimates at least $22bn of investment in new technology every year and a doubling of current renewable energy generation capacity within the next 20 years will be needed to meet a net zero emissions target by 2050. Moderate Liberals have returned to the new political year attempting to build traction internally for the government to increase ambition in climate action. But the push has triggered resistance from Nationals. At the weekend, Matt Canavan, who quit his cabinet position to support Barnaby Joyce’s failed bid to return to the Nationals leadership, declared a net zero target “fantastical”. “I haven’t looked at the modelling or costs and benefits of net zero emissions closely because it just seems so fantastical to me,” Canavan told Sky News. “It seems like the kind of things that governments say, because they’re not doing much today but they’d like to try and hoodwink people that they might do something in 30 years’ time.” The Nationals leader, Michael McCormack – who remains under pressure in his role – told the ABC on Sunday net zero was a bad idea. “I think if you go down that path, what you’re going to do is send factories and industries offshore, send manufacturing jobs offshore,” McCormack said. “That’s not the Australian way. Regional Australia is more than doing its fair share, its fair share as far as making sure that we have lower emissions.” Asked if he accepted warnings from the Intergovernmental Panel on Climate Change that the emissions target was needed to limit global warming, McCormack said the “IPCC is not governing Australia”. “The Liberals and Nationals are – we took our climate policies, we took our emissions reduction policies to the election last May and we were re-elected. “The Australian people have spoken – we’re not run by international organisations, we’re run by Australians, we’re run by Scott Morrison and we’re run by myself. And we’re run by the Liberals and Nationals.” On Monday Morrison stepped around the divisions within his own ranks by repeating the same formulation he has given on net zero since Liberals and Nationals began debating the proposition in public. The prime minister told reporters: “I don’t sign up to anything when I can’t look Australians in the eye and tell them what it costs. “How many jobs it’s going to cost them. What it means for their industries. What it means for rural and regional parts of the country. Whether it means they’d have to pay higher taxes. “And none of that information is before me that would enable me to give any such commitment – and I haven’t.”"
"Watching starling murmurations as the birds swoop, dive and wheel through the sky is one of the great pleasures of a dusky winter’s evening. From Naples to Newcastle these flocks of agile birds are all doing the same incredible acrobatic display, moving in perfect synchrony. But how do they do it? Why don’t they crash? And what is the point?  Back in the 1930s one leading scientist suggested that birds must have psychic powers to operate together in a flock. Fortunately, modern science is starting to find some better answers. To understand what the starlings are doing, we begin back in 1987 when the pioneering computer scientist Craig Reynolds created a simulation of a flock of birds. These “boids”, as Reynolds called his computer-generated creatures, followed only three simple rules to create their different patterns of movement: nearby birds would move further apart, birds would align their direction and speed, and more distant birds would move closer.  Some of these patterns were then used to create realistic looking animal groups in films, starting with Batman Returns in 1992 and its swarms of bats and “army” of penguins. Crucially this model did not require any long-range guidance, or supernatural powers – only local interactions. Reynolds’s model proved a complex flock was indeed possible through individuals following basic rules, and the resulting groups certainly “looked” like those in nature. Scientifically-accurate bat behaviour in Batman Returns. From this starting point an entire field of animal movement modelling emerged. Matching these models to reality was spectacularly achieved in 2008 by a group in Italy who were able to film starling murmurations around the rail station in Rome, reconstruct their positions in 3D, and show the rules that were being used. What they found was that starlings sought to match the direction and speed of the nearest seven or so neighbours, rather than responding to the movements of all of the nearby birds around them. Simple rules, complex patterns When we watch a murmuration pulsate in waves and swirl into arrays of shapes it often appears as if there are areas where birds slow, and become thickly packed in, or where they speed up and spread wider apart. In fact this is largely thanks to an optical illusion created by the 3D flock being projected onto our 2D view of the world, and scientific models suggest that the birds fly at a steady speed. Thanks to the efforts of computer scientists, theoretical physicists and behavioural biologists we now know how these murmurations are generated. The next question is why do they happen at all – what caused starlings to evolve this behaviour?  One simple explanation is the need for warmth at night during the winter: the birds need to gather together at warmer sites and roost in close proximity just to stay alive. Starlings can pack themselves into a roosting site – reed beds, dense hedges, human structures like scaffolds – at more than 500 birds per cubic metre, sometimes in flocks of several million birds. Such high concentrations of birds would be a tempting target for predators. No bird wants to be the one that a predator picks off, so safety in numbers is the name of the game, and swirling masses create a confusion effect preventing a single individual being targeted. However, starlings often commute to roosts from many tens of kilometres away, and they burn up more energy on these flights than could be saved by roosting in marginally warmer places. Therefore the motivation for these colossal roosts must be more than temperature alone.  Safety in numbers could drive the pattern, but an intriguing idea suggests that flocks may form so that individuals can share information about foraging. This, the “information centre hypothesis”, suggests that when food is patchy and hard to find the best long-term solution requires mutual sharing of information among large numbers of individuals. Just as honeybees share the location of flower patches, birds that find food one day and share information overnight will benefit from similar information another day. Although larger numbers of birds join roosts when food is at its scarcest, which seems to provide some limited support for the idea, it has thus far proven extremely difficult to properly test the overall hypothesis. Our understanding of moving animal groups has expanded enormously over the past few decades. The next challenge is to understand the evolutionary and adaptive pressures that have created this behaviour, and what it might mean for conservation as those pressures change. Possibly we can adapt our understanding and use it to improve the autonomous control of robotic systems. Perhaps the rush-hour behaviour of the automated cars of the future will be based on starlings, and their murmurations."
"BP’s got a new boss, Bernard Looney. He doesn’t wear a tie, he’s on Instagram and he’s going to shrink its carbon footprint to “net zero” by 2050. Is this for real? It’s a sign the tide is turning. Maybe not enough to save us from catastrophic sea-level rises, but a turn nonetheless. The oil industry is incredibly savvy when it comes to public opinion, and can see the steady erosion of its “social licence to operate” (a company’s ability to go about its business without too much challenge). It has been struggling to recruit young people for years, well before the school climate strikes started. The Royal Shakespeare Company and National Galleries Scotland have both turned their back on BP sponsorship in recent months, and last weekend more than a thousand people turned up at the British Museum to protest at the firm’s involvement there. BP is not the first oil company to give itself a lick of green paint to appear more acceptable in this era of increasing climate concern. We’ve seen Statoil dropping the word “oil” with the refreshed identity of Equinor, and Dong (Danish Oil and Natural Gas) relaunched as the renewable energy company Ørsted. It’s hard to see Equinor as anything more than greenwash while it’s still drilling the Arctic, but Ørsted may yet become the world’s first green energy supermajor, betting on the power of offshore wind to eventually see off natural gas. Shell is also keen to show off its green credentials, as anyone who has seen its multimillion pound advertising campaign can tell. It’s scared, or it wouldn’t bother. BP has been here before, with a £100m “Beyond Petroleum” rebrand in the early noughties. Alongside a new sunflower logo, this emphasised the company’s commitment to wind, solar and biofuels alongside oil and gas, but renewables remained a small part of its portfolio. Clean energy is good for press photos, but not really central to BP’s core business model. Environmentalist Jonathon Porritt originally tried to engage in its initiatives, but turned away in disgust, declaring it was impossible for today’s oil majors to change at the radical speed required. If BP is serious about rising to the challenge of the climate crisis, it will have to go the extra mile to convince us. It’s one thing for a computer company to make bold claims on climate action (as Microsoft did last month, announcing it would be “carbon negative” by 2030), but it’s another for an oil major. And yet, so far, Looney’s new vision is light on specifics. Apparently we have to wait until September for the details, but it does seem clear that BP will still be selling oil and gas. BP seems to be banking on the “net” in net zero doing a lot of heavy lifting. Which leaves the rather basic question: how does it plan to balance out carbon emissions produced by burning fossil fuels? Trees are great for helping us mop up some of the damage we’ve already caused, and various other technological options might turn out to be useful on a small scale. Capturing carbon from the air and using it to make plastic; adding iron to the ocean to speed up its ability to absorb carbon; or genetically modifying trees so they have larger, carbon-sucking roots, for example. But a lot of this is still a bit sci-fi, and none of it is ever likely to soak up the quantities of oil and gas BP plans to keep selling. Looney says he wants to help the world to get to net zero, not just BP. It is, apparently, shutting down the “Possibilities Everywhere” ad campaign that showcased its (relatively small-scale) work on clean energy, instead funnelling resources into campaigns that foster “the right kind of change”. This is the part of the project we should perhaps be most wary of. It’s worth remembering that with the Beyond Petroleum push in the noughties, BP popularised the idea of a “carbon footprint”. The approach seemed to mean well but was effective at individualising the causes of climate crisis, pulling attention away from the sorts of large-scale change that would really challenge the fossil fuel companies. Beware oil execs in environmentalists’ clothing. They may simply wish to seize the growing energy for change and steer it towards their own ends: the continued burning of fossil fuels. The most offensively disingenuous idea at the heart of this flashy new strategy is that BP finally “gets” the climate crisis. Oil runs off many things, and one of them is strong science. Fossil fuel companies have long known their business was dangerous and yet they chose to keep profiting off it. Indeed, new data shows quite how much they profited ($332bn in the last three decades in the case of BP). BP hasn’t suddenly got a dose of the climate heebie-jeebies, it’s just worried that, finally, you have. The Instagramming, tie-free Looney might reflect a new phase in the oil business, but is it still business as usual underneath? It’s hard not to see the rhetorical use of the “net” in net zero as a bit of smoke and mirrors, a way of making it sound as if it is taking the crisis seriously while avoiding the simple truth: we need to stop burning fossil fuels. If BP really wants us to believe that a company that made its name in oil can be part of the solution, it needs to stop drilling. It’s as simple as that. Hold retirement parties for refineries and retrain workers for a zero-carbon future, and do it fast. That might actually be something worth Instagramming about. • Alice Bell is co-director at the climate change charity Possible"
nan
"

From this page (h/t Dave Hagen)
The U.S. Environmental Protection Agency (EPA) is inviting comment from all interested parties on options and questions to be considered for possible greenhouse gas regulations under the Clean Air Act. EPA is issuing an advance notice of proposed rulemaking (ANPR) to gather information and determine how to proceed.
The Advance Notice
The ANPR is one of the steps EPA has taken in response to the U.S. Supreme Court’s decision in Massachusetts v. EPA. The Court found that the Clean Air Act authorizes EPA to regulate tailpipe greenhouse gas emissions if EPA determines they cause or contribute to air pollution that may reasonably be anticipated to endanger public health or welfare. The ANPR reflects the complexity and magnitude of the question of whether and how greenhouse gases could be effectively controlled under the Clean Air Act.
The document summarizes much of EPA’s work and lays out concerns raised by other federal agencies during their review of this work. EPA is publishing this notice at this time because it is impossible to simultaneously address all the agencies’ issues and respond to the agency’s legal obligations in a timely manner.
Key Issues for Discussion and Comment in the ANPR:

Descriptions of key provisions and programs in the CAA, and advantages and disadvantages of regulating GHGs under those provisions;
How a decision to regulate GHG emissions under one section of the CAA could or would lead to regulation of GHG emissions under other sections of the Act, including sections establishing permitting requirements for major stationary sources of air pollutants;
Issues relevant for Congress to consider for possible future climate legislation and the potential for overlap between future legislation and regulation under the existing CAA; and,
Scientific information relevant to, and the issues raised by, an endangerment analysis.

EPA will accept public comment on the ANPR for 120 days following its publication in the Federal Register.
Background
In April 2007, the Supreme Court concluded that GHGs meet the CAA definition of an air pollutant.  Therefore, EPA has authority under the CAA to regulate GHGs subject to the endangerment test for new motor vehicles – an Agency determination that GHG emissions from new motor vehicles cause or contribute to air pollution that may reasonably be anticipated to endanger public health or welfare.
A decision to regulate GHG emissions for motor vehicles impacts whether other sources of GHG emissions would need to be regulated as well, including establishing permitting requirements for stationary sources of air pollutants.
How to Comment

Comments should be identified by the following Docket ID Number: EPA-HQ-OAR-2008-0318
Comments should be submitted by one of the following method

www.regulations.gov: Follow the on-line instructions for submitting comments.
Email: a-and-r-Docket@epa.gov
Fax: 202-566-9744
Mail: Air and Radiation Docket and Information Center, Environmental Protection Agency, Mailcode: 2822T, 1200 Pennsylvania Ave., NW., Washington, DC 20460. In addition, please mail a copy of your comments on the information collection provisions to the Office of Information and Regulatory Affairs, Office of Management and Budget (OMB), Attn: Desk Officer for EPA, 725 17th St. NW., Washington, DC 20503.
Hand Delivery: EPA Docket Center, EPA West Building, Room 3334, 1301 Constitution Ave., NW, Washington DC, 20004. Such deliveries are only accepted during the Docket’s normal hours of operation, and special arrangements should be made for deliveries of boxed information.





			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9dc4ba27',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Striking students have joined Valentine’s Day rallies across the world as the protest movement attempts to ratchet up pressure on governments and companies before crunch UN climate talks in Glasgow later this year. In London, the young demonstrators held banners proclaiming “Roses are red, violets are blue, our Earth is burning and soon we will too” and “Climate change is worse than homework” as they marched through Parliament Square on Friday to mark the first anniversary of nationwide climate strikes in the UK. Students in Durham, Glasgow, Brighton and dozens of other cities also braved often wet and cold condition to march through the streets chanting, “What do we want? Climate justice. When do we want it? Now.” Greta Thunberg, who initiated the movement as a solitary striker in Stockholm in August 2018, said climate strikes were planned in 2,000 cities across the world on Friday, and that bigger actions were planned for the coming months. In many countries, the protests have expanded to include local environmental concerns, new strategies and stronger emphasis on global climate justice. In India on Friday, strikers turned their focus on government plans to deforest swathes of the Aravallis mountain range, which is a conservation area that provides freshwater and oxygen for Delhi and other cities. Some carried banners in English reading: “I love Aravallis”, “Our green lungs” and “Protectors are turning destroyers”. In Sydney, climate strikers demonstrated with banners that depicted the devastating bushfires and blamed the government of Scott Morrison for the “climate chaos” that has hit Australia. In the Philippines, climate strikers organised an educational storytelling campaign to raise public awareness. In Scotland, Holly Gillibrand, who was one of the first strikers in the UK when she started a vigil outside Lochaber high school in Fort William in the Highlands, said the growth of the movement had been incredible. “When I began striking over a year ago, Greta Thunberg and Fridays for Future [campaign] were not well known at all and I was one of very few strikers in the UK, but since then, everything has changed. The movement has gone from one person to 7.5 million. “Even if we still aren’t getting the radical action we need from governments, politicians are feeling the pressure to act and we just need to keep pushing, keep shouting, keep rebelling until they do.” Holly continued her strike on Friday, with a hot chocolate to help get her through the wet weather. Among those striking for the first time on Friday was a group in Rwanda, where protesters tweeted Timages of themselves holding signs that said: “Rwanda stand for climate.” Friday’s action was not intended as a mega-strike like those in September, when more than 6 million people took part, but it showed how the campaign has evolved. Maryam Grassly, 17, who was among the hundreds of strikers in central London, said: “The most enthusiastic and passionate people have stayed on, and more people have got involved and been inspired by it, but at the same time we’ve lost the people who didn’t really care.” But in her mind there was no question about the relevance of the protest. “When the election happened and Boris [Johnson] got voted in I just kind of gave up on the world, I thought that’s it,” she said. “We’ve got the climate emergency and net zero by 2050 but nothing is happening as fast as it should be.” Similarly she was “excited but worried” about the UK hosting Cop26 later this year, saying the previous conference of the parties in Madrid did not go well. But the protesters were still hopeful about the difference they have made over the past few months. Maude Brown, 17, said: “I’ve been very concerned about the issue for almost a decade, I was taught about it in year 4, and no one really cared then so I’m happy that everyone is concerned about it now. I think as more politicians from our generation come in we can make a lot of change in the future.” Most of the strikers were teenagers, but there were some younger children too. Claire Bullivant, from Essex, has been bringing her children Imogen, 12, Max, 9 and Theodore, 5, to the strikes for a few months. “I just think it’s so important to get them involved and to support them because they’re so aware of what’s happening in the world, and they see it in the news, and we don’t want to sit back and do nothing,” Bullivant said. While her daughter’s school has been supportive of her decision to take time off for the strikes, her sons’ school has been less so, but Bullivant said the time off was “totally justified”. “It’s educational, it’s empowering to be part of a community and show that they’re trying to make a difference to the world, I just don’t think you can underestimate experiences like this,” she said. Asked why she was so keen to get involved, Imogen said: “If we don’t save the Earth there’s not going to be anywhere else for us to live.” A year ago, the size of the protests in the UK took police by surprise, as thousands defied their teachers to skip school and join the still nascent movement. The students are now backed by longer established environmental organisations, including Global Justice Now, Greenpeace and the Green party. Among those at Friday’s march in London was a trade union climate bloc. Friends of the Earth are backing the school climate strikers, who it credits for shifting public opinion. There is still a long way to go, but with technology developments and strong policies, the group said there was cause for hope. “Huge change is possible. In 2019, the UK went coal-free for 19 days. That’s the longest break since the 1880s, and something that would have been unthinkable a few decades ago,” it said."
nan
"Demon vs killer shrimp sounds like the latest CGI movie to come out of Hollywood. But in fact these are two particularly pernicious crustaceans that have been making their way westward across Europe from countries surrounding the Black Sea, eradicating native freshwater rivals en route. Unlike the meatier species we might be more familiar with from our dining plates, the killer shrimp (Dikerogammarus villosus) and the demon (D. Haemobaphes) are technically amphipods, a group of small, flattened, shrimp-like creatures. They’re barely bigger than a finger nail, yet they possess the most striking and dramatic of common names.  The titles are somewhat justified. These shrimp not only out-compete the slightly smaller native (Gammarus pulex) for space and food, as often happens with invasive species, they go much further – killer and demon shrimp directly prey on the hapless locals. The equivalent would be grey squirrels not only out-competing and spreading disease among the arguably cuter native red squirrel but also actively hunting and eating them. The killer shrimp reached the UK in 2010 and within a year was denounced by the Environment Agency as the nation’s single most damaging invasive species. The demon was discovered in Britain in 2012 but has spread substantially further throughout the country’s river systems; in shrimp terms, it seems demons are even worse than killers. Yet it is the evocative names given to their worst enemies that may yet save the native shrimp and their ecosystem. When a charismatic animal faces eradication through habitat loss, pollution or competition from invaders, the public are likely to take notice. Great news if you’re an elephant, say, or a red squirrel. Very often though, the tiniest animals go ignored despite their hugely important role in sustaining food chains – there’s a reason the WWF uses a panda for its emblem rather than a shrimp. Whoever coined the phrases “killer” and “demon” shrimp, therefore, did so in a moment of genius. The names have captured the public imagination and the idea of an evil crustacean has generated considerable interest. Invasive species are said to cost the UK economy a staggering £1.7 billion through eradication programmes and the economic and ecological damage to fisheries and tourism.  Given the expense and upheaval involved in fighting the invaders, perhaps we should just leave things alone – after all, does it really matter to the ecosystem if one tiny crustacean replaces another? It’s an important question. Invasive species don’t always slot neatly into food-webs where the last species disappeared. Instead, as in the case of killer shrimp and potentially the demon, these omniverous creatures can occupy multiple parts of a food chain. Consequently, this has the potential to change the food transfer from plants right through to certain fish, bird and mammal species. The native shrimp primarily feeds on fallen leaf litter and vegetation while the invaders also eat fish eggs/larvae, insects and “rival” shrimp. The new arrivals might also breed at different times, grow at different rates, have different numbers of young and carry different diseases and parasites. The various species of micro-shrimp may look fairly similar to us humans, but any switch has the potential to deeply upset the natural balance of an ecosystem. Invasive species often bring with them diseases and harmful parasites and it seems these shrimp are no different. Our research at the University of Portsmouth found that demon shrimp have carried multiple parasites with them across Europe.  We don’t yet know what these parasites will do in newly invaded areas but we do know they are playing havoc with sexuality. Our research found at least 50% of the invasive male shrimp show sexual abnormalities. These sexually abnormal (termed intersex) shrimp display female characteristics caused by feminising parasites either caught in the UK or most likely carried inside the invaders. Interestingly, these parasites don’t appear to feminise as well in British waters.  Such gender issues however haven’t hampered the demon’s ability to wipe out its British cousin. Therefore we also have to consider the parasites who live “within” the native species which can become collateral damage in the shrimp wars.  These parasites are important too as they often pass through snails or crustaceans and finally into fish or birds. Gammarus shrimp carry worm-like parasites called Acanthocephala (spiny-headed worms), for instance. These parasites can “take over” the shrimp, flooding it with serotonin and altering its behaviour so that the shrimp swims nearer the surface and is more likely to eaten by birds such as ducks. Other species of Acanthocephala have fish such as trout as their final hosts. The extinction of an intermediate host, such as the native shrimp, therefore breaks the link in the parasite life cycle, with impact felt elsewhere in the food web. Who cares, one might ask, parasites are bad right? Not necessarily. Studies have shown that ecosystems with fewer parasites are less diverse than those with many, therefore the replacement of one shrimp species for another might not necessary replace the parasites which strengthen the food web. It’s difficult to know how to stop the spread of these invasive shrimp. Public awareness campaigns are reminding people to check, clean and dry themselves and their equipment to hinder the movement of these critters but only time will tell whether we are witnessing the extinction of the native Gammarus shrimp in British waterways. Britons may soon have to get used to killers lurking in their lakes, and demons in their rivers."
"BP, Shell, Chevron and Exxon have made almost $2tn in profits in the past three decades as their exploitation of oil, gas and coal reserves has driven the planet to the brink of climate breakdown, according to analysis for the Guardian. The scale of their profits is revealed as experts say the fossil fuel boom is coming to an end, with big oil entering a “death knell” phase, according to one prominent Wall St commentator. Analysis for the Guardian by Taxpayers for Common Sense in the US reveals that since 1990 – at which point the impact of fossil fuel extraction on the climate had been well known to industry leaders and politicians for years, experts say – the big four companies have accumulated $1.991tn in profits. Critics say the findings highlight how a few corporations have generated extraordinary wealth by pursuing policies that were known to be driving the climate crisis. The climate scientist Michael Mann said he was witnessing first-hand in Australia the environmental impact of fossil fuel extraction. “Here in Sydney, where we’ve seen record drought, heat, bushfires and floods all in the short two months I’ve been here, this latest report provides a sobering reminder that we’re all paying the price – in the form of a planetary climate crisis – so that a few mega-corporations can continue to make record profits,” he said. The analysis shows that Exxon was the most profitable of the big four over the past three decades, making a total of $775bn. Shell was second with $524bn, followed by Chevron on $360bn and BP on $332bn. Autumn Hanna, of Taxpayers for Common Sense, said: “For decades, oil and gas companies have been pocketing trillion-dollar profits and padding their bottom line with tens of billions of dollars in taxpayer subsidies. All while passing the buck on climate change.” Mel Evans, a senior climate campaigner at Greenpeace UK, said the big oil companies knew the danger that their products posed to the climate well before it became common knowledge but pursued profits above the wider interests of the planet. “Why did they continue to promote those products and dispute science they knew to be correct? Why are they still spending hundreds of billions of dollars on making the problem worse, drilling for new oil and gas we can’t possibly afford to burn?” Evans said. “These figures provide the answer. Money is like rocket fuel: burn through enough of it and you can escape the pull of the Earth. But there’s nowhere else to go.” Experts say the decades-long boom is coming to an end as clean energy replaces fossil fuels. Last week Jim Cramer, the influential host of the US investment show Mad Money, said he was “done with fossil fuels” because they had entered a “death knell phase”. Oil companies have been the worst-performing investments in US stock markets over the past decade, after leading major stock market indices in previous decades. The market value of oil and gas companies now makes up only 4% of the S&P500 index, compared with about 28% in the 1980s. “If you go back to 1990 you can really see that oil and gas companies once outperformed the market,” said Kathy Hipple, a financial analyst at the Institute for Energy Economics and Financial Analysis. “But the market looks to the future and there has been a gradual awareness that fossil fuels are not going to be as big a part in the world economy. This doesn’t mean that [oil companies] will go away tomorrow, but the market won’t reward them for the profits of the past.” Last year a Guardian investigation revealed that 20 fossil fuel giants including BP, Shell, Chevron and Exxon were directly linked to more than a third of all greenhouse gas emissions in the modern era. The big four investor-owned corporations were found to be behind more than 10% of all carbon emissions since 1965. The polluters project highlighted how many of the leading fossil fuel corporations had spent billions of pounds on lobbying governments and portraying themselves as environmentally responsible. A study published last year found that the largest five market-listed oil and gas companies – the big four plus Total – spent nearly $200m each year lobbying to delay, control or block policies to tackle climate change. The profit figures were calculated in today’s money by using the annual net income attributable to shareholders for each company while taking inflation rates into account. In absolute terms, without inflation, the combined profits since 1990 were a still considerable $1.6tn, according to Taxpayers for Common Sense. The profits of the world’s most profitable listed oil companies were dwarfed last year by the financial reports of the Saudi state-owned oil firm Aramco. It listed on the Saudi stock exchange after racking up profits of $111.1bn (£84.7bn) in 2018, which was more than double the profits of Apple and five times those reported by Shell. Experts say the environmental impact of fossil fuels was known by industry leaders and politicians, particularly in the US, as far back as the mid-60s. Certainly by 1990 the facts were well known: two years earlier the Nasa scientist James Hansen had raised the alarm about the impact of fossil fuels during a landmark hearing at the US Congress. In 1992, world leaders came together at a summit in Rio to recognise the role carbon emissions were playing and to pledge coordinated action. BP did not respond to a request for comment on the findings. A spokesperson for Shell said: “Just as reliable, affordable energy has benefited us all, all of society has a role to play in tackling climate change. We’re working hard to develop lower-carbon energy options and meet demand for more and cleaner energy.” ExxonMobil said it was helping address “the dual challenge of the world’s growing demand for energy and reducing emissions”. Chevon said it was taking action to address climate change by “lowering the company’s carbon intensity, increasing the use of renewable energy and investing in breakthrough technologies.”"
"
So far, SC24 solar magnetic activity has been in a relative funk. See my post on this very issue from last month.
Leif Svalgaard points out this new paper in AGU from Keating, and kindly placed a copy on his own website for us to examine: Link to Keating-Bz.pdf
The crux of the paper is a forecast, which extends significantly into SC24, even though there is just a small number of observed data points:
Fig. 1. Actual boxcar averages for measured Bz(m) magnitude and the forecast results of applying the McNish- Lincoln technique. Actual data are represented by solid squares, while the calculated results are shown as a curve. The correlation between the two is due to the fact that the McNish-Lincoln method uses actual data when available. The calculated forecast is performed only for the time period after the end of the actual data. This plot shows that Bz(m) reached its minimum average magnitude in mid-2007 and has begun to increase in magnitude. The forecast is that it will continue to increase slowly through the first part of 2008, but will then begin to rapidly increase in magnitude beginning in the latter part of this year, reaching its first peak in late 2009.
There seem to be two schools of thought on the activity level of SC24, those who think it will be very low, and those that think it will be higher than normal.
Dr. Svalgaard goes on record here on this blog in saying: 
I’ve been predicting that SC24 would be the smallest cycle in a century, so it is no surprise that it starts out weak and anemic.
While I’m certainly no solar expert, based on what I’ve seen thus far, I’m inclined to agree. I think that Keating’s prediction will not be realized.
This graph of Ap magnetic index will be updated in a few days, with the uptick this month in SC24 spots, perhaps we’ll also see a corresponding uptick in the Ap Index.
From the data provided by NOAA’s Space Weather Prediction Center (SWPC) you can see just how little magnetic field activity there has been. I’ve graphed it below with the latest available data from October 6th, 2008:

click for a larger image


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b561ad0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterDer Spiegel reports on a paper in Nature written by Hans von Storch and Matthias Zahn claiming that elevated greenhouse gas concentration will lead to fewer North Atlantic storms by the year 2100. Der Spiegel writes:
Instead of 50 to 60, there will only be about half as many Arctic hurricanes, the scientists say.
After every big winter storm, e.g. like Kyrill, we get here in northern Germany, we always hear the media crow about how it is due to global warming. Now the opposite is claimed. We’re a long way from settled science, aren’t we?
The mechanism leading to the Nature paper’s claim is described in the abstract as follows:
This change can be related to changes in the North Atlantic sea surface temperature and mid-troposphere temperature; the latter is found to rise faster than the former so that the resulting stability is increased, hindering the formation or intensification of polar lows.
I can certainly buy that. But then the authors apply bold science.
In the Nature abstract it is written:
Now, in projections for the end of the twenty-first century, we found a significantly lower number of polar lows and a northward shift of their mean genesis region in response to elevated atmospheric greenhouse gas concentration.
The elevated greenhouse gas/lower number of polar effect is quite a hypothesis. The authors are assuming that more CO2 will lead to higher atmospheric temps and thus fewer storms. That’s awfully bold science.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




But wait, it gets even bolder, Der Spiegel writes about the scientists:
Using computer models, that also used the climate prognoses of the United Nations, the scientists have played out the development of the northern seas up to the year 2100.
Making a projection for the year 2100 with that methodology? Now that’s really bold. I’m doing all I can to rein in the sarcasm here. The authors then add:
Our results provide a rare example of a climate change effect in which a type of extreme weather is likely to decrease, rather than increase.
I haven’t read the full Nature paper, as it is behind a pay-wall. I just wonder where the authors come up with: “a rare example”. This is probably a case of: whose bread one eats, whose words one speaks, which one has to submit to when dealing with Nature and government funding.
There are actually many examples that warm climates are more beneficial than not. After all, who the hell wants to go back to a little ice age, let alone a big one?
Why not just leave the crap out? I’d have no problem with a paper presenting a couple of if-then hypotheses, like:
1) If temperatures rise, our dynamic models show that there will be fewer storms.
2) If the temperature drops, then there will be more storms.
Leave the global warming faith and religion out of it.
Share this...FacebookTwitter "
"Christiana Figueres is a founder of the Global Optimism group and was head of the UN climate change convention when the Paris agreement was achieved in 2015. Your new book is called The Future We Choose. But isn’t it too late to stop the climate crisis?We are definitely running late. We have delayed appallingly for decades. But science tells us we are still in the nick of time. You say this decade is the most consequential in human history…This is the decade in which, contrary to everything humanity has experienced before, we have everything in our power. We have the capital, the technology, the policies. And we have the scientific knowledge to understand that we have to half our emissions by 2030. So we are facing the most consequential fork in the road. If we continue as now, we are going to be irreparably going down a course of constant destruction, with much human pain and biodiversity loss. Or we can choose to go in the other direction, a path of reconstruction and regeneration, and at least diminish the negative impacts of climate change to something that is manageable. But we can only choose it this decade. Our parents did not have this choice, because they didn’t have the capital, technologies and understanding. And for our children, it will be too late. So this is the decade and we are the generation. Only 11 pages or so of the book describe the terrible consequences of unchecked climate change, while the rest talks about the possibility of a much better world. Why?It’s important for everyone to face the negative consequences that we’re sleepwalking ourselves toward, which is why those 11 pages are there. But equally as important is to spark the imagination and the creativity that comes with understanding that we do have this incredible agency to create something completely different. We wanted to offer both universes to those who, understandably, are paralysed by despair and grief at the loss that is already under way, as well as those who are paralysed by their comfort and lack of understanding of the moment that we’re in. A lot of the book is about the need for a shift in people’s consciousness. Isn’t this rather grandiose or, on the other hand, too vague to make a difference in the real world?Whatever we hold as being possible, and whatever values and principles we live by, determine the actions that we take. Whatever we hold to be near and dear to us is what we’re willing to work toward. And so to shift from doom and gloom to a positive, optimistic, constructive attitude is very important because it is what gets us up in the morning and says “yes, we can do this, we’re going to work together on that”, rather than pulling the blanket over our head and saying “it’s all too difficult”. So that change in attitude inside ourselves is critical. We also have to understand that we can no longer live in a world based on limitless extraction and waste. Rather, we have to change our consciousness to one of regeneration. How can people’s consciousness change in that way?The first thing we have to understand is the consequence of not changing our attitudes. There are very serious existential consequences. Then hopefully we can make a serious, mature decision whether we want to choose something different. One of the 10 actions recommended in the book is to be a citizen and not a consumer. Can you explain that and why it is important?The very concept of being a consumer already points us in the direction of consuming irresponsibly. We have to be able at some point, particularly in developed countries, to get to the point where we say “enough is enough”. Before you make a purchase, or an investment, or any kind of decision that impacts on the planet and on other people, the question should be: “Do I really need this and is this actually conducive to furthering the quality of life on this planet?” Another of the actions you chose is building gender equality. Why?Educating young women and empowering women to come to decision-making tables is the strongest thing that we can do for the climate. When there are more women in boardrooms and in high-level positions in institutions, you get decisions that are wiser and longer term. Of course there are many men that also do this. But there is a tendency for women to be more collaborative, which is the basis of what we need to do, and they tend to think much more long term. [Women] have the first duty of care of our newborn children and hence, biologically, we’re geared towards that stewardship. But it is just plain stupid, frankly, not to use 50% of human potential. We are in such an emergency that we need to deploy 100% of our potential. Tackling the climate emergency requires global cooperation and action by governments and businesses. But what can individuals do?We tend to forget that it is humans who have caused climate change, and we tend to export responsibility to large corporations or governments. The fact is, we all contributed to it. If we all reduce our emissions, collectively we give a signal to the market. Obviously, corporations have their own responsibilities but it’s helpful to have a strong demand from the public. Once you get governments, corporations and the public moving in the same direction towards low carbon, it can grow exponentially [such as with renewable energy and electric cars]. People reducing their emissions – by flying less, eating less meat and using clean energy, for example – is important. But is demanding that politicians tackle the climate emergency and voting accordingly perhaps the most important thing individuals can do?I would say both. If we had 50 years to fiddle with this, then you could choose one or the other. But we are in such an emergency that we can no longer do things sequentially or exclusively. We have to be an “and/also” world. • The Future We Choose: Surviving the Climate Crisis by Christiana Figueres and Tom Rivett-Carnac is published by Manilla Press (£12.99). To order a copy go to guardianbookshop.com. Free UK p&p over £15 • Christiana Figueres and Tom Rivett-Carnac will be in conversation at a Guardian Live event at the Royal Geographical Society, London SW7, on Tuesday 3 March, 7pm"
"Climate change will cause all sorts of problems for humans in the future. It could cause mass migration and conflict as people flee flooded homes or arid farmland, and fight over ever more scarce resources. It’ll mean economic slowdown as industries are hit and societies cough up the money required to adapt to the new world. Climate change will even affect your health. Indeed, some scholars have warned that climate change is the greatest risk to global health. While this is likely to be true beyond the next 30 years if CO2 emissions continue unabated, this kind of argument risks reinforcing the reductionist idea that climate is some sort of universal predictor of health.   The time lag between greenhouse gas emissions and their impact on the environment means that, until at least 2045, poverty will be a much more important health factor than climate change. My commentary, recently published in the Geographical Journal, argues that this is for two reasons.  First, climate change should not be considered a direct cause of poor health in the way specific diseases can be. Even if it were a direct cause, there would be more important threats such as heart and lung disease or cancers. These non-communicable diseases already comprise the leading causes of death globally, and their burden is likely to increase as people age and adopt Western lifestyles. Second, the threat of climate change to health is one of modification: a changed climate will exacerbate and moderate existing health problems by making people more vulnerable. If vulnerability to climate change is addressed through adaptation, the threat will be much reduced, though not averted. Climate change is only one factor among many contributing to population vulnerability, of which poverty is the most important. This can be illustrated by taking a look at two major risks to global public health.  Vector-borne diseases are transmitted between humans or between animals and humans by another living organism, or vector. Vectors include mosquitoes, sandflies, ticks and aquatic snails, and the diseases they transmit include malaria and dengue fever. Even small changes in climate can cause shifts in where these diseases are most prevalent. People living in areas where the vectors may be able to breed in future due to climate change may be under-prepared for this shift if public health systems are ill-equipped and natural immunity is lower in these areas. For example, some research suggests that malaria is likely to penetrate higher altitudes in the East African highlands due to increased temperatures. Transmission of the disease from vectors to humans is reliant on both natural and human influences. For many of these man-made factors, including nutrition and access to education or health care, poverty is the key determinant. The importance of poverty is shown by the eradication of many vector-borne diseases in richer countries with similar climates to other nations where the same diseases are still prevalent. Malaria existed in Europe less than 90 years ago and has now all but disappeared. Dengue fever was eradicated in the US due to investment in public education, mosquito control programmes, piped water and screened windows. Neither Europe nor the US needed changes in climate to rid themselves of these diseases. In the case of infectious diseases the message is clear enough: improving health will be best achieved through reducing poverty. Climate change is likely to affect crop yields due to drought, flooding and rising seas creeping onto farmland. Seafood yields will be hit by warmer water temperatures and ocean acidification. Overall, food is likely to get more expensive. Already more than 1 billion people are estimated to be suffering from a lack of sufficient dietary energy, with one of the most important reasons being that people simply cannot afford to buy food. Purchasing power is very important in determining malnutrition, and food prices have doubled in the past ten years and experienced very high volatility. There was a 60% rise in basic food prices globally in 2008/9 and another 40% increase in 2011/12. These peaks were the combined effect of financial speculation in food and increased energy prices.  In light of this, it is difficult to argue that climate change is the greatest threat to global health based on food security, as poverty is a more important determinant of access to food. Poverty is one of the things that makes people most vulnerable to climate change and it is also key contributor to many health problems. It is imperative that climate change mitigation continues, not least because in the next 30 years it is likely to become a critical health factor, but for now the focus should be on reducing poverty. Not only does this align more closely with conventional public health practices, it will also confer immediate health benefits as well as ensuring people are more resilient to climate change impacts in the future. It is vital we emphasise poverty for the next 30 years. At the current rate of development, global GDP would have to increase 15 times in order for the world’s population to all earn at least $1.25 per day; a process taking more than 100 years. Poverty kills, and just focusing on climate change could mean this message is ignored."
"Antibiotic resistance in bacteria is spreading rapidly worldwide and has even been called a global threat to humans as serious as climate change. Excessive and imprudent use of antibiotics is usually blamed, but the rate and extent of the spread can’t be explained by overuse alone. Antibiotic resistance has now been found in remote parts of the world where humans and antibiotics are scarce or absent. Recently we published a study in Environment International that reported antibiotic resistant genes in Svalbard in the High Arctic. Antibiotic resistance itself is natural, but continued human exposure to antibiotics has selected for progressively stronger bacteria. These bacteria often acquire antibiotic resistance genes that allow them to make powerful defence proteins. As more antibiotics are used new forms of resistance evolve, including multiple drug resistance in pathogens, creating a health crisis.       For our research, we extracted bacterial DNA from 40 soil cores at eight locations along Kongsfjorden on the west coast of Spitsbergen on the Arctic Ocean, 300km from the North Pole. This location is fairly unusual in the Arctic for its vibrant wildlife populations and neighbouring fjord which does not freeze. We screened the Arctic soil DNA and quantified 131 antibiotic resistance genes associated with nine major antibiotic classes. These include aminoglycosides, carbapenem, macrolides and β-lactams – many of which are used to treat human and veterinary infections.  Some of the detected genes, some of which are almost certainly not “local” to the Arctic, can confer resistance to multiple drugs. These “foreign” genes were found in highest concentrations near fresh water sources – areas where wildlife tend to congregate. One antibiotic resistance gene we found is called “blaNDM-1”. This gene confers resistance in bacteria to carbapenem antibiotics, one of our last resort remedies to infectious disease. We found this gene in soils near a small Arctic lake in 2013, but it was first detected in a hospital patient in India in 2007, and was later found in surface waters in urban India in 2010. Although not explicitly from India, BlaNDM-1 was first identified on one side of the world and migrated to the other in less than three years, ending up in a place where few humans reside and where antibiotics are scarce. How did it get there? Places with inadequate sanitation often have higher levels of resistance genes and bacteria in the surrounding water and soil. Local overpopulation, poorly controlled antibiotic use and incomplete wastewater treatment means resistance is more readily selected in these places. Local water and food supplies become contaminated with faecal matter, providing a pathway for resistance genes and their bacterial hosts to be ingested by humans and wildlife – hitching a ride from one region of the world to another. It’s unclear whether it’s a single migration via a bird or a human, or a chain reaction of exposures. By whatever pathway, resistance genes are moving fast and to places where antibiotics are not present. As antibiotic resistance migrates, it also changes as it passes through animals and the wider environment. This complicates tracing these genes from their origins to where they end up. In the case of blaNDM-1, it was initially detected as a single gene in a few places, but there are now numerous variants which have been found in tens of countries, now including the Arctic. If resistance genes migrate via humans or wildlife to other locations, especially places with inadequate local sanitation, such genes and bacterial hosts might be selected in subsequent human and wildlife populations. This is what we think is happening worldwide. These genes move around the world with people and other animals, seeding new places with resistance potential. Reducing antibiotic use is critical to tackling bacterial resistance. In the UK, this is outlined in the five-year action plan announced by the health secretary, Matt Hancock. However, reducing local antibiotic use may have a limited effect on global resistance unless environmental pathways are not given greater consideration. Improving sanitation and water quality worldwide must be part of the fight against antibiotic resistance. There is one positive note from our High Arctic study, however. Although we detected genes like blaNDM-1, we also found other interesting and unexpected antibiotic resistance genes. We found one gene that codes for multiple drug resistance in Tuberculosis bacteria in almost all of our soil cores. This gene was not explicitly related to faecal matter, which means it is almost certainly natural to this remote environment. 


      Read more:
      It’s the age of the antibiotic revolution, not apocalypse


 This might sound like bad news, but it suggests where this specific resistance gene might have originated. Understanding more about the origins of this gene might help us better understand resistance in tuberculosis. That said, focusing efforts on developing new arsenals of antibiotic drugs may not be enough. It additionally would be wise for wealthier countries to help poorer ones improve water quality and sanitation, even if it is only providing toilets to reduce open defecation. Smarter use of antibiotics in agriculture and medicine is a necessary step for tackling resistance, but only a comprehensive approach will reduce the evolution and spread of antibiotic resistance."
"

Last December the United States agreed at a United Nations meeting in Kyoto, Japan, to reduce its emissions of greenhouse gases by 7 percent below 1990 levels. That reduction, to be achieved mainly by cutting the combustion of fossil fuels, will lower emission levels 41 percent below where they will likely be in the year 2010 if the trend observed since 1990 continues. 



The Kyoto agreement–if fully complied with–would likely reduce the gross domestic product of the United States by 2.3 percent per year. However, according to a climate model of the National Center for Atmospheric Research recently featured in _Science_ , the Kyoto emission‐​control commitments would reduce mean planetary warming by a mere 0.19 degree Celsius over the next 50 years. If the costs of preventing additional warming were to remain constant, the Kyoto Protocol would cost a remarkable 12 percent of GDP per degree of warming prevented annually over a 50‐​year period. 



The Kyoto Protocol will have no discernible effect on global climate–in fact, it is doubtful that the current network of surface thermometers could distinguish a change on the order of .19 degree from normal year‐​to‐​year variations. The Kyoto Protocol will result in no demonstrable climate change but easily demonstrable economic damage. 
"
"Life in the sea isn’t easy. Talk to most people about the ocean and they are likely to imagine a tropical scene with a stretch of golden sand and warm, clear water. The reality is often quite different – the marine environment can be a surprisingly cold place. Water conducts heat far more effectively than air, which means that submerged animals quickly lose their body heat. It’s also harder to warm up again than on dry land, where animals often have the option of basking in the sun or on hot rocks. Finally, many aquatic animals use gills to get oxygen – great for breathing, but essentially another source of heat loss due to all the of water flowing across them and sucking away warmth.  All this contributes to making it much harder for aquatic animals to regulate their body temperature. So surely it would make sense to find more marine animals in warmer waters rather than in colder ones? Not necessarily. A new study published in Science by a team of US researchers led by John Grady reports higher levels of marine biodiversity in polar waters than tropical ones – but only for some types of animals. This goes against the longstanding idea that species richness is always highest in the tropics. The theory was that tropical waters provide a less thermally-extreme environment than polar ones – compare, for example, the Caribbean with Antarctica. Tropical areas often provide greater stability and productivity, giving benefits such as a predictable environment and plenty of food. As a result, scientists typically report high levels of biodiversity in warmer waters, with a multitude of species making the most of these favourable areas.  However, in this latest research, Grady and colleagues argue that rather than grouping all marine species together, we should instead consider them separately based on how they regulate their body temperature. There are two main types of “thermal strategy”. Some animals are not able to generate enough heat to warm their own body tissues, so their internal temperature is determined by conditions outside their bodies. These animals are known as “ectotherms”, and typically include reptiles, amphibians, fish (including most sharks), and invertebrates. In comparison “endotherms”, including humans, other mammals and birds, warm their body tissues by having high metabolisms to burn off food, which generates heat inside their bodies. These categories are similar to “cold blooded” and “warm blooded”, but more scientifically accurate. An ectothermic lizard might be considered cold blooded on a cool evening, but (since their body temperature matches the environment) it rapidly becomes warm on a hot, sunny day. To avoid this kind of confusion, scientists prefer to refer to the thermal strategy, rather than actual blood temperature. When marine species are separated into these categories of “ectotherm” and “endotherm”, Grady and colleagues show a pattern of marine biodiversity that both supports and contradicts the previous theory. The reptiles, fish and invertebrates rely on their environment for heat, so follow the established prediction of being generally found in warmer climates with particularly high levels of species diversity in the tropics.  But the heightened metabolism and body temperature of the endotherms opens up new possibilities. Unrestrained by the need for a warm environment, birds and mammals can successfully exploit colder habitats. They are also able to move more quickly in cold water than their ectothermic prey, which can become sluggish at cooler temperatures.  Consequently, endotherms seem to buck the expected trend and instead show higher species richness in polar waters. As the new study points out, seals and their relatives are “virtually absent from tropical waters” yet are common nearer the poles, while of all 89 species of cetacean (whales, dolphins and porpoises) only dolphins have “truly diversified in the warm tropics”. This seems to make sense. But, of course, there is more to it than thermal strategy. Apart from a considerable difference in temperature, the tropics and poles also differ in other ways. For example, differences in levels of dissolved oxygen, nutrients, algae growth, light, and so on. As a result, some areas and species are the “exception to the rule”. Regardless, categorisation by thermal strategy does show some interesting patterns of species diversity, and accounts for animals like whales and seals largely avoiding the warmest waters. But this advantage of environmental independence comes at a price. To maintain their heat-generating metabolism, endotherms have a continuous need for lots of high-quality food. For instance an ectothermic lemon shark has to eat about 2% of its body weight each day, whereas an endothermic common dolphin might have to eat 12% or more. To be fair, there are a lot of factors to consider beyond thermal strategy, but that’s still quite a difference.  In a changing climate, the energy requirements of endotherms could spell trouble. With the world warming fastest towards the poles, many scientists predict that polar species will face the biggest struggle as their habitats are dramatically altered, and they cannot retreat any further north or south to seek cooler areas. As the oceans warm, perhaps the high metabolisms and body temperatures of marine mammals will become a burden rather than a benefit."
"

America’s security commitment to Taiwan faces a significant test. China’s growing power presents a challenge to U.S. military superiority, while Taiwan’s investment in its own defense has languished. Adding to the challenge of keeping peace in the Taiwan Strait is the shifting political situation in Taiwan, exemplified by the January 2016 elections in which voters rejected the cross‐​strait rapprochement policies of the Kuomintang (KMT) and turned over control of the presidency and legislature to the Democratic Progressive Party (DPP). The China‐​Taiwan relationship has remained relatively calm, but changes in the U.S.-China balance of power could make the Taiwan Strait a dangerous place once more if the implicit U.S. defense commitment to Taiwan loses credibility.



This paper outlines three broad policy options for the United States: shoring up the defense commitment by restoring military superiority over China; sustaining a minimum level of military advantage over China; or stepping down from the commitment to use military force to maintain Taiwan’s de facto independence. It concludes that the United States should step down from the defense commitment eventually, ideally through an incremental and reciprocal process with China that would draw concessions from Beijing. In the long term, the U.S. security commitment to Taiwan is neither beneficial nor advantageous for the United States. Taiwan will have to take responsibility for its own defense.



Stepping down from the implicit commitment to come to Taiwan’s rescue with military force carries risks, but other options leave the United States worse off in the long term. The likely damage to U.S.-Chinese relations caused by pushing for military superiority in the region outweighs the benefits. Sustaining a minimum level of military advantage is possible, but absent a long‐​term economic slowdown and/​or political changes in China—both of which are beyond U.S. control—maintaining such an advantage in perpetuity will be difficult. Stepping down from the commitment through a long‐​term process would give Taiwan the time it needs to make necessary changes in its defense technology and military strategy. Peace in the Taiwan Strait is an important American interest, but it must be weighed against the difficulty of maintaining credibility and the growing costs of deterrence failure.



The U.S. defense relationship with Taiwan is a risky and costly commitment that has become increasingly difficult to sustain. Barry Posen of the Massachusetts Institute of Technology put it best when he wrote, “The U.S. commitment to Taiwan is simultaneously the most perilous and least strategically necessary commitment that the United States has today.”1 The United States can and should strive for a peaceful resolution of the Taiwan dispute, but through means other than an implicit commitment to use military force to defend the island.



Washington’s approach to keeping the peace in the Taiwan Strait during the latter years of Taiwan’s Lee Teng‐​hui (1988–2000) and most of the Chen Shui‐​bian (2000—2008) administrations was known as “dual deterrence.” Under dual deterrence the United States issued a combination of warnings and reassurances to both China and Taiwan to prevent either from unilaterally changing the status quo.2 America’s overwhelming military advantage over the People’s Liberation Army (PLA) deterred China from using military force, while Taiwan moderated its behavior lest U.S. forces not come to its rescue.3 However, the dual deterrence concept is ill‐​suited to the current military environment in the Taiwan Strait.



Dual deterrence is no longer viable because the modernization of the PLA has improved Beijing’s ability to inflict high costs on U.S. military forces that would come to Taiwan’s aid in the event of a Chinese invasion attempt.4 The deployment of two U.S. Navy aircraft carriers to the waters around Taiwan during the 1995–1996 Taiwan Strait Crisis was a major embarrassment for the PLA, and it has played an important role in driving China’s military modernization.5 Improvements in China’s anti‐​access/​area denial (A2/AD) capabilities have significantly complicated the ability of the United States to defend Taiwan by making it difficult for the U.S. Navy and Air Force to operate in and around the Taiwan Strait.6 According to a recent RAND Corporation study, “a Taiwan [conflict] scenario will be extremely competitive by 2017, with China able to challenge U.S. capabilities in a wide range of areas.”7 This shifting balance of power strains the credibility of the U.S. defense commitment to Taiwan by increasing the costs the United States would have to pay in an armed conflict.



Two additional developments will challenge the cross‐​strait peace. First, the period of rapprochement that has characterized cross‐​strait relations since 2008 has ended. The former Taiwanese president, Ma Ying‐​jeou (2008–2016), championed cross‐​strait cooperation and economic linkages that brought a welcome sense of calm after the tumultuous administrations of Lee and Chen.8 However, the January 2016 landslide victory of the DPP in both presidential and legislative elections revealed popular dissatisfaction with Ma’s policies and a weakening economy.9 President Tsai Ing‐​wen pledged to maintain peace. But her unwillingness to declare support for the “1992 Consensus” (simply stated as “one China, different interpretations”) caused Beijing to suspend communication between the Taiwan Affairs Office and Taipei’s equivalent, the Mainland Affairs Council.10 It is too early to tell how Tsai’s administration and a DPP‐​controlled legislature will affect cross‐​strait relations, but the relatively high level of cooperation the Ma administration promoted is likely over.11



Second, China’s slowing economy adds uncertainty to cross‐​strait relations. China’s GDP growth rate was 6.9 percent during the first nine months of 2015, well below the double‐​digit GDP growth rates of the last couple of decades.12 Sliding growth and the resulting social instability could encourage China’s leaders to behave more aggressively toward Taiwan to bolster domestic legitimacy and ensure regime survival.13 However, a slowing economy could also restrict military spending and encourage Chinese policymakers to avoid big conflicts as they focus on shoring up the economy. At the very least, China’s economic situation is a source of uncertainty that was not present when the United States relied on dual deterrence.



What approach should the United States take in this shifting environment? Generally speaking, there are three options for the United States: it could do more to shore up the defense relationship with Taiwan and restore its military superiority over China; sustain a minimum level of military advantage over China; or step down from the implicit commitment to use military force in defense of Taiwan. This paper explores each of these and concludes that stepping down from the commitment is the best of the three options. The success of dual deterrence should be praised, but American policymakers must begin adjusting to a new state of affairs in the Taiwan Strait.



The U.S. security commitment to Taiwan consists of two pillars established in the Taiwan Relations Act (TRA) of 1979: arms sales and an implicit promise to defend Taiwan with military force should it be attacked. Both are set forth in Section 3 of the TRA, which states, in part, that the United States is permitted to sell Taiwan “defense articles and defense services in such quantity as may be necessary to enable Taiwan to maintain a sufficient self‐​defense capability.”14 Comparatively, the implicit commitment to use force to defend Taiwan is less clear. Section 3, part 3, authorizes the president and Congress to “determine, in accordance with constitutional processes, appropriate action by the United States” in response to “any threat to the security or the social or economic system of the people on Taiwan and any danger to the interests of the United States arising therefrom.”15 Military force is not explicitly mentioned, but it falls within the category of appropriate action that the United States could take.



The imprecise wording of the TRA has served the United States well by creating “strategic ambiguity,” the underpinning of dual deterrence.16 Strategic ambiguity, the open question of whether or not the U.S. military would intervene in a cross‐​strait conflict, had two important effects. First, it gave the United States greater freedom of action in trilateral relations. By not binding itself to one particular position, the United States could better adapt to unpredictable events. Second, strategic ambiguity restricted China and Taiwan’s freedom of action. Upsetting the status quo carried high costs for both sides. The United States could warn Taiwan that no cavalry would come to the rescue if Taiwan provoked China by making moves toward de jure independence.17 Likewise, the high costs that would be inflicted on the PLA by a U.S. intervention prevented Beijing from initiating a conflict.



China’s growing military power has diminished the value of strategic ambiguity by improving Beijing’s ability to inflict high costs on an intervening American force. The mere possibility of American intervention may no longer be enough to deter China if the PLA is better prepared to mitigate the effects.



Further complicating the U.S.-Taiwan defense relationship is the slow but steady erosion of U.S. credibility over the last two decades. This analysis uses the “Current Calculus” theory set forth by Dartmouth professor Daryl G. Press as the basis for assessing U.S. credibility. Press states, “Decisionmakers assess the credibility of their adversaries’ threats by evaluating the balance of power and interests … Future commitments will be credible if—and only if—they are backed up by sufficient strength and connected to weighty interests.”18 From Beijing’s perspective, the U.S. commitment to defend Taiwan is credible if American military power can pose a threat to Chinese forces and the United States has a strong interest in defending Taiwan.



On the subject of interests, Taiwan carries much more importance for China than it does for the United States. Charles Glaser of George Washington University writes “China considers Taiwan a core interest–an essential part of its homeland that it is determined to bring under full sovereign control.”19 Beijing does not appear eager to reunite Taiwan with the mainland by force in the near future, but China’s president Xi Jinping has warned that “political disagreements that exist between the two sides … cannot be passed on from generation to generation.”20 Maintaining Taiwan’s de facto independence may be important for the U.S. position in East Asia, but it does not carry the same significance that China places on reunification.21



Since China enjoys an advantage in the balance of interests, the credibility of the U.S. commitment rests on American military power. According to Press’s model, if the United States can carry out its threat to intervene with relatively low costs, then the threat is credible.22 When the TRA was passed in 1979, the United States enjoyed a clear advantage over a militarily weak China. That is no longer the case. Several recently published assessments of a U.S.-China conflict over Taiwan have sobering conclusions: America’s lead is shrinking, victory is less certain, and the damage inflicted on the U.S. military would be substantial. In China’s Military Power, Roger Cliff of the Atlantic Council writes, “Although China’s leadership could not be confident that an invasion of Taiwan in 2020 would succeed, it is nonetheless possible that it could succeed.… Even a failed attempt, moreover, would likely be extremely costly to the United States and Taiwan.”23 The RAND Corporation reached a similar conclusion: “At a minimum, the U.S. military would have to mount a substantial effort—certainly much more so than in 1996—if it hoped to prevail, and losses to U.S. forces would likely be heavy.”24 It is impossible to determine exactly how many American ships, aircraft, and lives would be lost to defend Taiwan from a PLA attack. But given the improved quality of PLA weapons systems and training exercises, it is safe to assume that the U.S. military would have to cope with losses that it has not experienced in decades.



Of course, it is important to note that high costs do not flow one way. In a war, the United States and Taiwan would make an invasion very costly for China, which reduces the credibility of Beijing’s threats to use force. However, U.S. military superiority in a Taiwan Strait conflict was nearly absolute until very recently. This superiority made victory relatively cheap, which enhanced the credibility of the American commitment.25 Improvements to already formidable Chinese weapons systems, combined with recent reforms that enhance command and control for fighting modern war, continue to ratchet up the costs the United States would have to absorb.26



If the PLA continues to improve at the rate it has done over the last 20 years, the United States could be in the unpleasant position of fighting a very costly conflict over a piece of territory that China has a much stronger interest in controlling than the United States has in keeping independent. Close economic ties between the United States and China (bilateral trade in goods was valued at $598 billion in 2015 in nominal dollars) would likely suffer as well.27 The high costs the United States would face in a conflict over Taiwan undermine U.S. credibility. China’s stronger interests and ability to inflict high costs on the United States could encourage Beijing to take risks that until recently would have been considered unacceptable.



Broadly speaking, the United States has three options for dealing with the diminishing credibility of its implicit commitment to defend Taiwan. In this section I explain what kinds of policies would most likely accompany each option and present favorable arguments for each.



The most straightforward way to bolster American credibility would be to increase the U.S. military presence close to Taiwan and clearly demonstrate the political will to honor the defense commitment. The combination of increased military presence and unequivocal political support would be a clear break from dual deterrence. Instead of directing warnings and reassurances toward both Taiwan and China, the United States would only warn China and only reassure Taiwan.28 The United States would welcome a stronger Taiwan, but U.S. support would not be preconditioned on Taiwan’s willingness to develop its defenses.



The ultimate goal of this policy option would be the establishment of a decisive and durable U.S. military advantage over the PLA. The clearest indicator of the U.S. commitment is military resources. Increasing the survivability of American air power in the area around Taiwan would send a clear signal of support. The American forces currently deployed in Japan would be the first to respond in a Taiwan conflict. Increasing the number of hardened aircraft shelters at U.S. bases in Japan, especially at Kadena Air Base on Okinawa, would protect aircraft from ballistic missile attacks.29 Additionally, the United States would revive the annual arms‐​sale talks with Taiwan that occurred from 1983 until 2001. Advocates for returning to annual talks argue that moving away from scheduled talks resulted in arms sales becoming less frequent.30 Future arms sales would include more advanced equipment that Washington is currently unwilling to sell to Taiwan, such as the F-35 Joint Strike Fighter aircraft and diesel attack submarines.31



Politically, American policymakers would clarify that U.S. military intervention in a Taiwan conflict is guaranteed. They would interpret the TRA as a serious commitment to Taiwan’s security, and, according to Walter Lohman of the Heritage Foundation, “[make] abundantly clear to Beijing the consequences that will ensue from the use of force.”32 The TRA would not be modified in any way that reduces the scope of America’s commitment. Supporters in Congress would regularly issue resolutions that reaffirm support for the TRA, especially the parts related to the defense of Taiwan.33 Strict interpretation of the TRA would be a clear demonstration of American willpower to take a hard line against China.



Public statements by American officials about U.S. intervention would not carry any preconditions or caveats. Such statements would be similar to the one made by President George W. Bush in April 2001 that the United States would do “whatever it takes” to defend Taiwan.34 Bush eventually walked back this statement, but successful implementation of the restore‐​superiority option would require similarly categorical shows of support. Removing preconditions from the commitment would bolster credibility by removing an off ramp the United States could take to avoid intervention. Additionally, Taiwan would not be expected to spend a certain percentage of its GDP on defense to secure U.S. arms sales or intervention.



Finally, the U.S. government would actively support de jure Taiwanese independence. As Weekly Standard editor William Kristol warns, “Opposing independence … might give Beijing reason to believe that the U.S. might not resist China’s use of force against Taiwan or coercive measures designed to bring about a capitulation of sovereignty.”35 However, supporting Taiwanese independence would be risky. In 2005, China passed the Anti‐​Secession Law (ASL) in response to the growing political power of the pro‐​independence movement in Taiwan.36 Article 8 of the ASL states that “non‐​peaceful means and other necessary measures” will be employed if “secessionist forces … cause the fact of Taiwan’s secession from China.”37 The increased American military presence resulting from the restore‐​superiority option would have to be strong enough to prevent China from invoking the ASL.



Advocates of the U.S. military commitment to Taiwan argue that the island’s success as a liberal democracy is linked to the regional security interests of the United States. For example, during his failed campaign for president, Sen. Marco Rubio (R-FL) said that “Taiwan’s continued existence as a vibrant, prosperous democracy in the heart of Asia is crucial to American security interests there and to the continued expansion of liberty and free enterprise in the region.”38 In the U.S. Congress the ideologically driven, “pro‐​democracy” camp of Taiwan supporters is large and influential.39 Proponents of a strong U.S. commitment to Taiwan also argue that Taiwan’s political system is evidence that Chinese culture is compatible with democracy. According to John Lee of the Hudson Institute, “Taiwan terrifies China because the small island represents a magnificent vision of what the mainland could be and what the [Chinese] Communist Party is not. This should be a reason to reaffirm that defending democracy in Taiwan is important to America and the region.”40 Supporters of a strong U.S. defense commitment to Taiwan through restoring America’s military superiority want to send a clear message to Beijing that the security commitment has not been shaken by China’s growing military power.



The second option, sustaining a minimum advantage, would maintain the current U.S. military commitment with some slight modifications. This option is much less resource‐​intensive than the restore‐​superiority option. The United States would maintain its implicit military commitment, but with preconditions that encourage Taiwan to invest more in its own defense. Importantly, the United States would reserve the right not to intervene if Taiwan provoked an armed conflict with China. The overarching themes of this option are balance and moderation. It has taken the United States years of effort to create what appears to be a relatively stable status quo, so, its supporters ask, why risk destabilizing it by significantly altering the U.S.-Taiwan relationship without very good reason?41



Under this option, the United States would improve the military assets for defending Taiwan, but at a much smaller scale than with the restore‐​superiority option. The PLA’s steadily improving capabilities diminish the credibility of the U.S. commitment to Taiwan by raising the costs of conflict. Maintaining a qualitative advantage over the PLA as it continues to develop will enhance the credibility of the U.S. commitment to Taiwan by keeping the costs of war high for the PLA. However, such improvements would be tempered to mitigate the chance of overreaction by Beijing and possible damage to U.S.-China relations.



American arms sales to Taiwan would continue under this policy option. Arms sales create tension in the U.S.-China relationship, but three benefits of arms sales mitigate the costs they create.42 First, arms sales complicate PLA planning and raise the costs of conflict for China. Second, damage done to U.S.-China relations as a result of the arms sales is relatively small. A joint report from the Project 2049 Institute and the U.S.-Taiwan Business Council on China’s reactions to arms sales concludes, “Past behavior indicates that the PRC is unlikely to challenge any fundamental U.S. interests in response to future releases of significant military articles and services to Taiwan.”43 Finally, arms sales demonstrate the commitment to Taiwan’s defense, especially in times of political transition.



Arms sales to Taiwan would also be adjusted to counteract the PLA’s quantitative advantage and operational strengths. Expensive items such as AV-8B Harriers, F-16 fighters, and Perry‐​class frigates would no longer be sold because they are highly vulnerable to Chinese weapons systems.44Instead, arms sales would prioritize cheaper, more numerous precision‐​guided weapons and advanced surveillance assets that would prevent Chinese forces from achieving a quick victory and buy time for the United States to come to Taiwan’s rescue.45 Such weapons systems are, generally speaking, much cheaper and easier to maintain than aircraft and ships. A report from the Center for Strategic and Budgetary Assessments argues that by “forego[ing] further acquisitions of costly, high‐​end air and naval surface combat platforms” Taiwanese policymakers can focus their economic resources on more “cost‐​effective platforms” better suited to Taiwan’s defense.46



The United States would expect Taiwan to make serious defense investments by increasing military spending and developing indigenous weapons systems. Taiwan’s military spending has increased in nominal terms after a precipitous drop in the late 1990s and early 2000s, but since 1999 defense spending has not risen above 3 percent of GDP.47 Taipei’s unwillingness to spend more on defense has upset some officials in Washington. In a November 2015 letter to President Obama calling for a new arms sale to Taiwan, Sen. John McCain (R-AZ) and Sen. Benjamin L. Cardin (D-MD) wrote, “We are increasingly concerned that, absent a change in defense spending, Taiwan’s military will continue to be under‐​resourced and unable to make the investments necessary to maintain a credible deterrent across the strait.”48 Thankfully, Tsai Ing‐​wen and the DPP have made increased defense spending a major policy goal.



The development of Taiwan’s defense industry would provide an additional source of high‐​quality military equipment for the island’s defense. Taiwan has experience designing and manufacturing sea and air defense weapons. James Holmes of the U.S. Naval War College notes, “[In 2010] Taiwanese defense manufacturers secretly designed and started building a dozen stealthy, 500‐​ton fast patrol craft [Tuo Chiang–class] armed with indigenously built, supersonic anti‐​ship missiles.”49 Indigenously produced air defense systems include the Tien Kung (TK) family of missiles, the Indigenous Defense Fighter, and anti‐​aircraft guns.50 Importantly, “Made in Taiwan” is not a byword for poor quality. According to Ian Easton of the Project 2049 Institute, the TK surface‐​to‐​air (SAM) missiles are “comparable to [U.S.-made] Patriot systems in terms of capability,” and the Hsiung Feng III anti‐​ship missile “is more capable than any comparable system fielded by the U.S. Navy in terms of range and speed.”51



Sustaining a minimum advantage would be the easiest of the three policy options for the United States to implement. Inertia is a powerful force. The United States has invested a considerable amount of resources and effort to reach a stable status quo in the Taiwan Strait, creating an “if it isn’t broken, don’t fix it” mentality. Advocates of maintaining the status quo, such as the Center for Strategic and International Studies, argue that it is “critically important to U.S. interests” to deter Chinese coercion of Taiwan, lest instability spread in East Asia.52 In prepared testimony before the House Foreign Affairs Committee, Deputy Assistant Secretary of State Susan Thornton said, “The United States has an abiding interest in cross‐​Strait peace and stability.”53 Congress, historically a strong bastion of support for Taiwan, shows no indication of changing America’s Taiwan policy anytime soon.



Buttressing support for this policy option is the belief that America’s commitment to Taiwan is a bellwether for the U.S. position in East Asia. According to John J. Mearsheimer of the University of Chicago, “America’s commitment to Taiwan is inextricably bound up with U.S. credibility in the region … If the United States were to sever military ties with Taiwan or fail to defend it in a crisis with China, that would surely send a strong signal to America’s other allies in the region that they cannot rely on the United States for protection.”54 Advocates of maintaining the U.S. commitment argue that East Asia would become more dangerous if other allies lose faith in the United States and start building up military capabilities of their own.55 Supporters of the U.S. commitment also contend that backing down on Taiwan would embolden Chinese aggression in other territorial disputes.



The final policy option would do away with America’s commitment to Taiwan’s defense on the grounds that military intervention to preserve the island’s de facto independence has become too costly and dangerous for the United States. Stepping down from the commitment to come to Taiwan’s rescue would be a major change in U.S. policy. However, other factors unrelated to the U.S. commitment would still make the use of force unattractive for Beijing. Taiwan would therefore not be defenseless or subject to imminent Chinese attack if the United States chose this policy option.



Without a U.S. commitment, Taiwan would have to improve its self‐​defense capability to deter an attack by China and fight off the PLA if deterrence failed. Taiwan does face an unfavorable balance of power vis‐​à‐​vis China, but this does not doom Taiwan to military defeat. In fact, research by Ivan Arreguín‐​Toft of Boston University indicates that large, powerful actors (such as China) have lost wars against weaker actors “with increasing frequency over time.”56 However, in order to have the greatest chance of success, the weaker side must have the right military strategy. A head‐​on, symmetric fight with the PLA would likely end in disaster for Taiwan, but Taiwan could successfully deny the PLA from achieving its strategic objectives through the same kind of asymmetric strategy that China uses to make it difficult for the United States to defend Taiwan.57 A military strategy emphasizing mobility, concealment, and area denial would both raise the costs of war for China and be sustainable, given Taiwan’s limited means.



Changing Taiwan’s defense strategy would not be a quick or easy task. The most immediate roadblocks to change are the equipment and mindset of Taiwan’s military. The upper echelons of the military have resisted implementing changes that could improve their ability to fight a war against the modern PLA. For example, James Holmes points out that Taiwan’s navy “[sees] itself as a U.S. Navy in miniature, a force destined to win decisive sea fights and rule the waves.”58 This is a dangerous mindset given the PLA Navy’s dominance in fleet size, strength, and advanced equipment. The Taiwan Marine Corps (TMC) is also ill‐​suited to meeting the threat posed by China. Instead of being a light, agile force, the TMC is “heavy, mechanized, and not particularly mobile,” reflecting “a glaring failure by Taiwan’s defense establishment to recognize the TMC’s essential role in national defense.”59 Overcoming the forces of bureaucratic inertia will be very difficult, but doing so is necessary if Taiwan can no longer count on the United States.



Stepping down from the U.S. defense commitment would likely involve reductions in U.S. arms sales. Reductions in the size, quantity, and frequency of arms sales would likely precede any reductions to the defense commitment because arms sales are a measurable signal of American support for Taiwan. Lyle J. Goldstein of the U.S. Naval War College points out, “Arms sales have for some time taken on a purely symbolic meaning.”60 This implies that the negative effects of reducing arms sales would be relatively small, since China’s extant military advantages are not being offset by U.S. weaponry. Additionally, stopping the arms sales would not have to be instantaneous. The United States could reduce arms sales incrementally to give Taiwan time to improve its self‐​defense capabilities.



One common argument made by opponents of stepping down from the commitment is that it is the only thing preventing China from attacking Taiwan. This argument ignores several important factors that make the use of force unattractive for Beijing. First, China’s reputation and standing in East Asia would be seriously damaged. Other countries in East Asia would harshly criticize China’s use of force, and would likely take steps to defend themselves. For example, countries involved in territorial disputes with Beijing in the South China Sea have responded to Chinese aggressiveness by improving their military power and pushing back politically and diplomatically.61 China’s reputational costs for attacking Taiwan would be very high. Additionally, any military operation against Taiwan would tie up a great deal of resources. Other states could take advantage of a Taiwan‐​focused Beijing to push back against other Chinese territorial claims.



Second, the PLA has problems with both “hardware” (equipment) and “software” (experience) that would restrict its options for using military force against Taiwan.62 The modern PLA has no experience conducting large‐​scale amphibious landings, which are complicated operations that would be very costly to execute against a dug‐​in defender.63 On the hardware side, the PLA still lacks the amphibious‐​lift capabilities and replenishment ships necessary to mount a successful invasion attempt.64 China has made big strides shifting the relative balance of power in the Taiwan Strait, but it still faces significant challenges that will take time to overcome.65 Presently, the PLA is more prepared to push back against American intervention than to initiate an invasion of Taiwan.



How the United States goes about stepping down from its commitment is important. Suddenly abrogating the TRA would be practically impossible given the entrenched support for Taiwan within Congress. The most realistic, feasible approach requires incremental reductions in U.S. support for Taiwan. Examples of such reductions could include setting a cap on the value and/​or quality of military equipment that can be sold to Taiwan, changing the TRA to more narrowly define what constitutes a threat to Taiwan, or requiring Taiwan to spend a certain percentage of its GDP on defense in order to receive U.S. military support.



Incremental reduction would be easier to sell to U.S. policymakers because it buys time for Taiwan to improve its defenses, thus increasing the credibility of the island’s military deterrent. As discussed earlier, Taiwan’s defense industries have proven they can make high‐​quality military equipment that meets the island’s defense needs. Taiwan has the ability to develop a robust and effective military deterrent, but it needs time to overcome existing challenges and address unforeseen obstacles. If the United States were to reduce its commitment incrementally, Taiwan’s political and military leadership would have the time to address such challenges.



Incremental implementation of this policy option would also provide the United States with opportunities to learn about Chinese intentions, based on Beijing’s reaction.66 Stepping down from the defense commitment to Taiwan would be a major accommodation on a core Chinese security interest. American policymakers should demand some sort of reciprocal actions from Beijing that reduce the military threat the PLA poses to Taiwan. In Meeting China Halfway, Lyle J. Goldstein explains how “cooperation spirals” in the U.S.-China relationship can build “trust and confidence … over time through incremental and reciprocal steps that gradually lead to larger and more significant compromises.”67 However, if Washington takes accommodating policy positions and Beijing responds with obstinacy or increased aggression, then American policymakers would likely want to adjust their approach.



Stepping down from the U.S. defense commitment to Taiwan, regardless of how it is implemented, is a controversial policy option that would face significant opposition. However, there is a strong case to be made for the benefits of such a policy. Taiwan’s fate carries much more significance for China than the United States, and American military superiority over China is eroding. Although Taiwan faces serious challenges, it would be capable of maintaining a military deterrent without American support, especially given the other factors that rein in Chinese aggression. A self‐​defense strategy emphasizing asymmetric warfare could raise the costs of military conflict for China to unacceptably high levels. Most important, the risk of armed conflict between the United States and China would be significantly reduced.



Each of the three policy options has problems and shortcomings that would make their implementation difficult and limit their effectiveness. In this section I will discuss the most important flaws of each policy option.



Restoring U.S. military superiority would shore up the credibility of the American commitment to Taiwan at the cost of severe damage to the U.S.-China relationship. China might be deterred from attacking Taiwan, but it would have ample reason to strongly oppose the United States across other issue areas, including the South China Sea, trade issues, and reining in North Korea. Additionally, unequivocal American support would reduce incentives for Taiwan to improve its defenses.



The most important negative consequence of restoring U.S. military superiority is the severe damage that would be done to U.S.-China relations. China and the United States do not see eye‐​to‐​eye on many issues, but this does not make China an outright adversary.68 Chinese cyber espionage against American companies, the rise of alternative development institutions led by Beijing, and island‐​building in the South China Sea are of great concern to policymakers in Washington.69 However, U.S.-Chinese cooperation on other pressing issues, especially environmental concerns and punishing North Korea after its recent nuclear tests, has supported U.S. goals.70 China is certainly not a friend or ally of the United States, but treating it as an enemy that needs to be contained is unwise.71 Restoring U.S. military superiority would set back much of the progress made in U.S.-China relations.



Restoring U.S. military superiority might be a boon to America’s credibility in the short term, but superiority may be fleeting. The growing U.S. military presence in East Asia, a result of the Obama administration’s “pivot” or “rebalance” to the region, has exacerbated the Chinese perception of the United States as a threat.72 Restoring U.S. military superiority will likely support this perception and provide a strong incentive for China to invest even more resources in its military. Additionally, falling behind in the conventional balance of power could prompt China to increase the quantity and quality of its nuclear weapon arsenal.73 If Beijing quickly offsets the advantages of stronger U.S. military support for Taiwan, the United States could end up in a similar position to the one it’s in now, but with a stronger China to deter.



Increasing American support for Taiwan without any preconditions regarding Taiwan’s role in its own defense would be detrimental in the long run. Taiwan and the United States’ other East Asian allies are willing to cheap‐​ride on American security guarantees.74 Taiwan is not disinterested in self‐​defense, but if someone else is shouldering the burden there is less urgency to do more, especially if increasing military spending means reducing social spending. China could exacerbate Taiwan’s “guns vs. butter” dilemma if it restricted economic exchanges (trade, investment, and tourism) with Taiwan as a result of a stronger U.S. posture.



Increasing the American commitment to Taiwan carries significant risks and costs for a benefit that would likely be fleeting. The likely negative consequences of restoring U.S. military superiority would not be worth the benefits. American policymakers should not go down this path.



The biggest weakness of sustaining a minimum U.S. military advantage is that it does not resolve any of the underlying issues in the cross‐​strait dispute, most important of which is the fact that Taiwan matters more to China than it does to the United States. Since the United States cannot equalize the imbalance of stakes vis‐​à‐​vis China, credible deterrence will require the United States to maintain military superiority over a steadily improving PLA. The United States is capable of absorbing these costs in the short run, but the recent history of the U.S.-China military balance suggests that China will be able to narrow the gap eventually.



Maintaining stability in the Taiwan Strait will become more complicated as a result of two trends in cross‐​strait relations and one higher‐​level trend. First, a distinct identity is taking hold in Taiwan; the people living there see themselves as Taiwanese instead of Chinese. Surveys conducted in 2014 showed that “fewer than 4 percent of respondents [in Taiwan] self‐​identified as solely Chinese, with a clear majority (60 percent) self‐​identifying solely as Taiwanese.”75 A unique Taiwanese identity is dangerous to Beijing because it makes China’s ultimate goal of reunification more difficult, especially if the identity issue leads to greater political support for independence. Thankfully, the Taiwanese people have been very pragmatic and have not yet made a significant push for de jure independence.76



Second, if China’s economy continues to slow down Beijing could become more aggressive toward Taiwan. A parade of doom and gloom headlines reveal the weaknesses of China’s economic miracle. The Chinese stock market experienced downturns in August 2015 and January 2016 that affected global financial markets.77 China Labor Bulletin, a Hong Kong‐​based workers’ rights group, recorded more than 2,700 strikes and worker protests throughout China in 2015—more than double the 1,300 recorded the year before.78 In February 2016, Reuters reported that 1.8 million workers in China’s state‐​owned coal and steel companies will be laid off in the coming years.79 This is not to say that China’s economy is in imminent danger of a catastrophic collapse. However, the political instability resulting from economic troubles could create an incentive for Beijing to act aggressively to burnish the Chinese Communist Party’s image at home.80 Exacerbating this risk is the rise of nationalist forces within Chinese society that could push the government into a more aggressive cross‐​strait policy. Such forces played an important role in the government’s heavy‐​handed response to 2014’s Occupy Central protests in Hong Kong.81 Economic problems coupled with aggressive ideology could prompt China to back away from any rapprochement with Taiwan. This could make the task of deterring a Chinese attack harder for the United States.



Third, America’s other security commitments could draw attention and resources away from Taiwan. Keeping pace with the PLA in the Taiwan Strait will require investments in military power that will become more difficult to sustain, barring either a reduction in global commitments or a significant decrease in China’s own economic and military power. The fight against ISIS in the Middle East and North Africa, the Russian threat to Eastern Europe, and Chinese island‐​building in the South China Sea are all vying for the attention of the U.S. military. The military has been able to cope with these contingencies, but there are signs of strain on the force.82 Given America’s current global security posture, it will be difficult for the United States to sustain a minimum advantage over the PLA in perpetuity.



Sustaining a minimum U.S. military advantage is growing more difficult and costly over time as these above trends develop. Fortunately, the costs are likely to increase slowly and could be mitigated by advances in U.S. military technology. However, ultimately the United States will be stuck in the unenviable position of trying to defend Taiwan from a China that has growing military power and a strong interest in prevailing in any dispute.



The two most important potential negative consequences of stepping down from the defense commitment to Taiwan are the reputational and credibility costs to the United States and the worsening of America’s military position in the region. Advocates of maintaining the U.S. commitment also contend that Chinese control over Taiwan would lead to a substantial PLA presence, which would pose a serious threat to American and allied interests. The military dominance that the United States has enjoyed since the end of World War II would be called into question. Advocates of U.S. primacy in East Asia consider such an outcome dangerous and unacceptable.83



Opponents of stepping down from the commitment argue that both China and the United States’ Asian allies will view such a change as a sign of American weakness and unwillingness to live up to other commitments.84 If the United States does not show strong resolve as China grows more powerful, Beijing would take advantage of American weakness to more forcefully pursue objectives that are detrimental to U.S. allies and partners.85 The Brookings Institution’s Richard Bush argues that “[the United States] cannot withdraw from the cross‐​Strait contest altogether because U.S. allies and partners would likely read withdrawal as a sign that the U.S. security commitments to them are no longer dependable.”86 Stepping down from the commitment to Taiwan would have two mutually reinforcing harmful effects: China would grow bolder in threatening U.S. allies and the allies would presume that the United States would not fulfill its commitments as the threat from China grows.



Fears over these negative consequences stem from a popular misconception of credibility in which the past actions of a state are considered indicative of how the state will behave in the future. As noted earlier, academic research indicates that states take other factors into account when making judgements of credibility, but the dogmatic adherence to this misconception among the American policymaking elite makes stepping down from the commitment an uphill battle.87 Formal treaty commitments to states like Japan and South Korea carry more weight than America’s vague commitment to Taiwan, but fears of abandonment will likely weigh heavily on the minds of policymakers in Seoul, Tokyo, and Washington.88 Overturning the assumptions that credibility is bound up in upholding past promises will take a great deal of time and effort.



Ending the U.S. defense commitment to Taiwan could be detrimental to the U.S. military’s broader goals in East Asia. Taiwan lies in the middle of an island chain that runs from Japan to the South China Sea. Control of Taiwan has important strategic implications because of this location. The PLA could use Taiwan as a staging area to more easily project power into the South China Sea, the East China Sea, and the western Pacific.89 Keeping this island chain free of Chinese military bases and friendly to the United States is therefore seen as essential for America’s position in the region. Indeed, Taiwan has loomed large in American military strategy in the region for decades. In 1950 General Douglas MacArthur described Taiwan as “an unsinkable aircraft carrier and submarine tender ideally located to accomplish offensive strategy and at the same time checkmate defensive or counter‐​offensive operations” from the surrounding area.90 If Taiwan becomes the PLA’s ‘unsinkable aircraft carrier,’ it would make U.S. military actions in support of other regional interests more difficult.



Fears over China’s improved military position that would follow seizing control over Taiwan are valid, but there are roadblocks to this outcome that exist independent of the U.S. defense commitment. As mentioned earlier in this analysis, China would face numerous hurdles and negative consequences if it tried to invade Taiwan, given the difficulty of conducting amphibious invasions, the high likelihood of regional backlash, and the materiel and training limitations of the PLA.91 Taiwan could also do more to raise the costs of conflict for China through changes in military technology and warfighting doctrine.92 For example, Taiwan’s fleet of fighter aircraft is costly to maintain and outclassed by PLA fighters and surface‐​to‐​air missile capabilities.93 Reducing the size of Taiwan’s fighter fleet and redirecting funds to build up mobile missile forces that could support ground units fighting against a PLA invasion attempt would improve Taiwan’s ability to resist the PLA and inflict heavy losses on Chinese forces.94 If President Tsai and the DPP can deliver on their promises to increase defense spending and develop Taiwan’s defense industries, Taiwan could be capable of mounting an effective self defense without American intervention in the coming decades.



The United States should step down from the implicit commitment to use military force to preserve Taiwan’s de facto independence. American credibility is slowly eroding as China becomes more powerful, and the commitment will be more costly to maintain for a relatively minor benefit. Broadly speaking, the United States has two options for how it could implement this policy option: it could try to draw concessions from China to get something in return for stepping down from the commitment, or it could unilaterally drop the commitment. In either scenario, Taiwan would have to take on sole responsibility for deterring Chinese military action.



A policy that wins concessions from China would be the more desirable of the two options. Concessions could include resolution of other territorial disputes involving China and American allies or dropping the Chinese threat to use force against Taiwan. This would be characteristic of what Charles Glaser calls a grand bargain, “an agreement in which two actors make concessions across multiple issues to create a fair deal … that would have been impossible in an agreement that dealt with a single issue.”95 Making the end of the U.S. commitment to Taiwan contingent upon Chinese concessions to resolve its other territorial disputes peacefully would benefit both the United States and China.96 The United States would free itself of an increasingly costly and risky commitment to Taiwan’s defense, but only if China compromises in ways that align with U.S. allies’ interests in the South and East China Seas. China would have to limit its objectives in the South and East China Seas, but in return would earn a major policy concession from the United States on a core national interest that has much more importance than the other territorial disputes.



If China proves unwilling to make concessions across multiple issue areas, the United States could still push for concessions on China’s military posture toward Taiwan. Instead of demanding a concession on the South China Sea dispute, U.S. policymakers could press China to take actions that reduce the military threat it poses to Taiwan via an incremental, reciprocal process of concessions.97 Refusing to sell Taiwan any new military equipment would be a good way to initiate a cooperation spiral.



Stopping the sale of new equipment would not significantly reduce the Taiwanese military’s ability to defend itself for three reasons. First, most equipment sold to Taiwan by the United States does not represent the latest in U.S. military technology and is not necessarily superior to new capabilities fielded by the PLA.98 Second, Taiwan’s domestic defense industry is capable of producing new equipment that is well‐​suited to asymmetric defense, although it will take time for Taiwan’s relatively small and underdeveloped defense industry to reach its full potential.99 Finally, stopping the sale of new weapons still gives the United States the latitude to sell spare parts and ammunition for weapons systems that have already been sold. Halting the sale of new types of weapons systems will signal a reduced U.S. commitment to Taiwan’s security that would not be overly disruptive to Taiwan’s self‐​defense.



One of several ways that Beijing might respond to this U.S. concession on arms sales would be to reduce the number of short‐​range ballistic missiles (SRBMs) within firing range of Taiwan. Currently there are more than 1,000 conventionally armed SRBMs (with a maximum range of approximately 500 miles) in the PLA arsenal that could strike Taiwan.100 Improvements in guidance technology have transformed these missiles from inaccurate “terror weapons” that would likely target cities to precision munitions better suited for strikes against military airfields and ports.101Stationing the SRBMs out of range of Taiwan would be a low‐​cost, but symbolically important, action. The missiles are fired from mobile launchers that could be moved back into range of Taiwan. However, the act of moving the missiles out of range would, according to Lyle J. Goldstein, “show goodwill and increasing confidence across the Strait and also between Washington and Beijing.”102 If China agrees to America’s demand to relocate its ballistic missiles, then additional steps could be taken to further reduce the threat China poses to Taiwan.



If China proved unwilling to make any concessions, either in other territorial disputes or in cross‐​strait relations, the United States could still unilaterally withdraw from its military commitment to Taiwan. No demands or conditions would be placed on Chinese behavior. American policymakers are unlikely to accept such a course of action given recent shows of Chinese assertiveness. Charles Glaser explains, “China appears too likely to misinterpret [unilaterally ending the U.S. commitment to defend Taiwan], which could fuel Chinese overconfidence and intensify challenges to U.S. interests.”103 Unilateral withdrawal would reduce the likelihood of U.S.-Chinese armed conflict, but the dearth of other benefits would make the policy difficult for policymakers to implement. Extracting some kind of concession from China, either in cross‐​strait relations or in other territorial disputes, should be a priority.



Finally, stepping down from the commitment to defend Taiwan with military force does not remove America’s interest in keeping the Taiwan Strait free of armed conflict. The United States would retain the ability to punish China in other ways should it attack Taiwan. Diplomatic isolation and economic sanctions may not inflict the same kinds of costs on Beijing as military force, but they are additional costs that would have to be absorbed.104 Additionally, U.S. arms sales are separate from the implicit commitment to defend Taiwan and could continue, albeit in some reduced or modified form.105 Continuing to sell arms to Taiwan while stepping down from the implicit commitment to use military force to defend the island allows the United States to demonstrate support for Taiwan’s defense without taking on the risks associated with direct intervention.106



The United States should no longer provide the military backstop for Taiwan’s de facto independence. The security commitment to Taiwan outlined in the TRA is a product of a different time, when the United States enjoyed clear military advantages over China, and Taiwan could be defended on the cheap. China’s growing military power strains the credibility of the American commitment. Policymakers in Washington could respond to this changing environment by restoring American military superiority, sustaining a minimum military advantage, or stepping down from the commitment. All of these options carry risks and negative consequences, but it is in the best long‐​term interest of the United States to step down from the commitment to Taiwan.



American policymakers must come to terms with the idea that the balance of power has become much more favorable for Beijing since the TRA was adopted in 1979. Defending Taiwan is more difficult now than ever before, and this trend will be very hard to reverse. The most realistic way to reorient U.S. policy is to reach out to China to take incremental, reciprocal steps that slowly bring about the end of America’s commitment. This policy will be very difficult for the United States to implement, but the advantages to U.S.-China relations could be substantial. Changing the U.S.-Taiwan security relationship would greatly reduce the likelihood of armed conflict between the United States and China and could create opportunities for U.S.-China cooperation that are currently beyond reach.



1\. Barry R. Posen, Restraint: A New Foundation for U.S. Grand Strategy (Ithaca, NY: Cornell University Press, 2014), p. 102.



2\. Richard Bush, “Taiwan’s January 2016 Elections and Their Implications for Relations with China and the United States,” Asia Working Group Paper no. 1, Brookings Institution, December 2015, p. 5.



3\. Andrew Scobell, “China and Taiwan: Balance of Rivalry with Weapons of Mass Democratization,” in China’s Great Leap Outward: Hard and Soft Dimensions of a Rising Power, ed. Andrew Scobell and Marylena Mantas (New York: The Academy of Political Science, 2014), pp. 130–31.



4\. Invasion is not the only military option available to China. The PLA could also conduct a blockade of Taiwan or conduct decapitation strikes to eliminate Taiwan’s political leadership. This analysis focuses on a Chinese invasion attempt because it is the most severe military option in terms of costs for all sides involved, and it carries the best chance for Beijing to accomplish its ultimate goal of reunifying Taiwan with the mainland via direct military and political control.



5\. For an excellent overview of the 1995–1996 crisis, see: Ted Galen Carpenter, America’s Coming War with China: A Collision Course over Taiwan (New York: Palgrave Macmillan, 2005), pp. 66–70; Robert S. Ross, “The 1995–96 Taiwan Strait Confrontation: Coercion, Credibility, and the Use of Force,” International Security 25, no. 2 (Fall 2000): 87–123. On the role of the crisis on China’s military modernization, see: Michael S. Chase et al., China’s Incomplete Military Transformation: Assessing the Weaknesses of the People’s Liberation Army (PLA) (Santa Monica, CA: RAND Corporation, 2015), p. 14; Andrew J. Nathan and Andrew Scobell, China’s Search for Security (New York: Columbia University Press, 2012), pp. 303–08.



6\. Dean Cheng, “Countering China’s A2/AD Challenge,” The National Interest, September 20, 2013, http://nationalinterest.org/commentary/countering-china%E2%80%99s-a2-ad-challenge-9099?page=show; Henry J. Hendrix, At What Cost a Carrier? Disruptive Defense Papers (Washington: Center for a New American Security, 2013); Ronald O’Rourke, China’s Naval Modernization: Implications for U.S. Navy Capabilities—Background and Issues for Congress (Washington: Congressional Research Service, 2015).



7\. Eric Heginbotham et al., The U.S.-China Military Scorecard: Forces, Geography, and the Evolving Balance of Power 1996–2017 (Santa Monica, CA: RAND Corporation, 2015), p. 330.



8\. Lyle J. Goldstein, Meeting China Halfway: How to Defuse the Emerging U.S.-China Rivalry (Washington: Georgetown University Press, 2015), pp. 52–53.



9\. Tom Phillips, “Taiwan Elects First Female President,” Guardian (London), January 16, 2016, http://​www​.the​guardian​.com/​w​o​r​l​d​/​2​0​1​6​/​j​a​n​/​1​6​/​t​a​i​w​a​n​-​e​l​e​c​t​s​-​f​i​r​s​t​-​f​e​m​a​l​e​-​p​r​e​s​ident.



10\. Javier C. Hernandez, “China Suspends Diplomatic Contact With Taiwan,” New York Times, June 25, 2016, http://​www​.nytimes​.com/​2​0​1​6​/​0​6​/​2​6​/​w​o​r​l​d​/​a​s​i​a​/​c​h​i​n​a​-​s​u​s​p​e​n​d​s​-​d​i​p​l​o​m​a​t​i​c​-​c​o​n​t​a​c​t​-​w​i​t​h​-​t​a​i​w​a​n​.html.



11\. Bush, “Taiwan’s January 2016 Elections and Their Implications for Relations with China and the United States,” pp. 15–19.



12\. Shannon Tiezzi, “China’s ‘New Normal’ Economy and Social Stability,” The Diplomat, November 24, 2015, http://​thediplo​mat​.com/​2​0​1​5​/​1​1​/​c​h​i​n​a​s​-​n​e​w​-​n​o​r​m​a​l​-​e​c​o​n​o​m​y​-​a​n​d​-​s​o​c​i​a​l​-​s​t​a​b​i​lity/.



13\. Ted Galen Carpenter, “Could China’s Economic Troubles Spark a War?” The National Interest, September 6, 2015, http://​nation​al​in​ter​est​.org/​f​e​a​t​u​r​e​/​c​o​u​l​d​-​c​h​i​n​a​s​-​e​c​o​n​o​m​i​c​-​t​r​o​u​b​l​e​s​-​s​p​a​r​k​-​w​a​r​-​13784.



14\. The full text of the Taiwan Relations Act can be found at American Institute in Taiwan, “Taiwan Relations Act,” January 1, 1979, http://​www​.ait​.org​.tw/​e​n​/​t​a​i​w​a​n​-​r​e​l​a​t​i​o​n​s​-​a​c​t​.html.



15\. Ibid.



16\. Brett V. Benson and Emerson M. S. Niou, “Comprehending Strategic Ambiguity: U.S. Security Commitment to Taiwan,” November 12, 2001, http://people.duke.edu/~niou/teaching/strategic%20ambiguity.pdf; Carpenter, America’s Coming War with China, pp. 7–8; J. Michael Cole, “Time to End U.S. ‘Ambiguity’ on Taiwan,” The Diplomat, July 6, 2012, http://​thediplo​mat​.com/​2​0​1​2​/​0​7​/​t​i​m​e​-​t​o​-​e​n​d​-​u​-​s​-​a​m​b​i​g​u​i​t​y​-​o​n​-​t​a​iwan/; and Michal Thim, “Time for an Improved Taiwan-U.S. Security Relationship,” American Citizens for Taiwan, February 21, 2016,http://​www​.amer​i​canci​t​i​zens​for​t​ai​wan​.org/​t​i​m​e​_​f​o​r​_​a​n​_​i​m​p​r​o​v​e​d​_​t​a​i​w​a​n​_​u​_​s​_​s​e​c​u​r​i​t​y​_​r​e​l​a​t​i​o​n​ship/.



17\. Carpenter, America’s Coming War with China, pp. 84–85; Scobell, “China and Taiwan: Balance of Rivalry with Weapons of Mass Democratization,” pp. 135–36.



18\. Daryl G. Press, Calculating Credibility: How Leaders Assess Military Threats (Ithaca, NY: Cornell University Press, 2005), p. 3.



19\. Charles L. Glaser, “A U.S.-China Grand Bargain? The Hard Choice between Military Competition and Accommodation,” International Security 39, no. 4 (Spring 2015): 61.



20\. “China’s Xi says Political Solution for Taiwan Can’t Wait Forever,” Reuters, October 6, 2013, http://​www​.reuters​.com/​a​r​t​i​c​l​e​/​u​s​-​a​s​i​a​-​a​p​e​c​-​c​h​i​n​a​-​t​a​i​w​a​n​-​i​d​U​S​B​R​E​9​9​5​0​3​Q​2​0​1​31006.



21\. On the asymmetry of interests between China and the United States, see: Charles Glaser, “Will China’s Rise Lead to War? Why Realism Does Not Mean Pessimism,” Foreign Affairs 90, no. 2 (March/​April 2011): 86; Glaser, “A U.S.-China Grand Bargain?” p. 50; Goldstein, Meeting China Halfway, p. 65; John J. Mearsheimer, “Say Goodbye to Taiwan,” The National Interest no. 130 (March/​April 2014): 103; Posen, Restraint, p. 103; and Ross, “The 1995–96 Taiwan Strait Confrontation,” p. 123.



22\. Press, Calculating Credibility, p. 3.



23\. Roger Cliff, China’s Military Power: Assessing Current and Future Capabilities (New York: Cambridge University Press, 2015), p. 221. Emphasis in original quote.



24\. Heginbotham et al., The U.S.-China Military Scorecard, p. 331.



25\. Press, Calculating Credibility, p. 21.



26\. Philip C. Saunders and Joel Wuthnow, China’s Goldwater‐​Nichols? Assessing PLA Organizational Reforms (Washington: National Defense University, April 2016).



27\. The trade value figure from the U.S. Census Bureau represents the sum of U.S. exports to ($116.2 billion) and imports from ($481.9 billion) China. See United States Census, “Trade in Goods with China,” https://​www​.cen​sus​.gov/​f​o​r​e​i​g​n​-​t​r​a​d​e​/​b​a​l​a​n​c​e​/​c​5​7​0​0​.html.



28\. Richard C. Bush, Untying the Knot: Making Peace in the Taiwan Strait (Washington: Brookings Institution Press, 2005), p. 258.



29\. Cliff, China’s Military Power, p. 197.



30\. Michael Mazza, “Taiwanese Hard Power: Between a ROC and a Hard Place,” in A Hard Look at Hard Power: Assessing the Defense Capabilities of Key U.S. Allies and Security Partners, ed. Gary J. Schmitt (Carlisle Barracks, PA: U.S. Army War College Press, 2015), p. 221.



31\. Kent Wang, “Why the U.S. Should Sell Advanced Fighters to Taiwan,” The Diplomat, January 10, 2014, http://​thediplo​mat​.com/​2​0​1​4​/​0​1​/​w​h​y​-​t​h​e​-​u​s​-​s​h​o​u​l​d​-​s​e​l​l​-​a​d​v​a​n​c​e​d​-​f​i​g​h​t​e​r​s​-​t​o​-​t​a​iwan/.



32\. Walter Lohman, “What the United States Owes to Taiwan and Its Interests in Asia,” War on the Rocks, January 27, 2016, http://​waron​the​rocks​.com/​2​0​1​6​/​0​1​/​w​h​a​t​-​t​h​e​-​u​n​i​t​e​d​-​s​t​a​t​e​s​-​o​w​e​s​-​t​o​-​t​a​i​w​a​n​-​a​n​d​-​i​t​s​-​i​n​t​e​r​e​s​t​s​-​i​n​-​asia/.



33\. Riley Walters, “Affirming the Taiwan Relations Act,” The Daily Signal, March 27, 2014, http://​dai​lysig​nal​.com/​2​0​1​4​/​0​3​/​2​7​/​a​f​f​i​r​m​i​n​g​-​t​a​i​w​a​n​-​r​e​l​a​t​i​o​n​s​-act/.



34\. Quoted in Scobell, “China and Taiwan: Balance of Rivalry with Weapons of Mass Democratization,” p. 131. Also see David E. Sanger, “U.S. Would Defend Taiwan, Bush Says,” _New York Times_, April 26, 2001, http://​www​.nytimes​.com/​2​0​0​1​/​0​4​/​2​6​/​w​o​r​l​d​/​u​s​-​w​o​u​l​d​-​d​e​f​e​n​d​-​t​a​i​w​a​n​-​b​u​s​h​-​s​a​y​s​.​h​t​m​l​?​p​a​g​e​w​a​n​t​e​d=all.



35\. William Kristol, “The Taiwan Relations Act: The Next 25 Years,” in Rethinking “One China,” ed. John J. Tkacik, Jr. (Washington: The Heritage Foundation, 2004), p. 17.



36\. Zhidong Hao, “After the Anti‐​Secession Law: Cross‐​Strait and U.S.-China Relations,” in Challenges to Chinese Foreign Policy: Diplomacy, Globalization, and the Next World Power, ed. Yufan Hao, George Wei, and Lowell Dittmer (Lexington, KY: The University Press of Kentucky, 2009), p. 201.



37\. Full text of the Anti‐​Secession Law can be found at Embassy of the People’s Republic of China in the United States of America, “Anti‐​Secession Law” (full text), March 15, 2005, http://​www​.chi​na​-embassy​.org/​e​n​g​/​z​t​/​9​9​9​9​9​9​9​9​9​/​t​1​8​7​4​0​6.htm. See also, Chunjuan Nancy Wei, “China’s Anti‐​Secession Law and Hu Jintao’s Taiwan Policy,” Yale Journal of International Affairs 5, no. 1 (Winter 2010): 112–27.



38\. “Following Historic China‐​Taiwan Meeting, Rubio Calls for Strengthening U.S.-Taiwan Relations,” press release, Marco Rubio’s official website, November 7, 2015, http://www.rubio.senate.gov/public/index.cfm/press-releases?ID=2ae3bcfd-82f8-4ffe-9580–6b988123c1d0.



39\. Eric Gomez, discussion with staffer for a senior Member of the House Armed Services Committee, October 6, 2015.



40\. John Lee, “Why Does China Fear Taiwan?” The American Interest, November 6, 2015, http://​www​.the​-amer​i​can​-inter​est​.com/​2​0​1​5​/​1​1​/​0​6​/​w​h​y​-​d​o​e​s​-​c​h​i​n​a​-​f​e​a​r​-​t​a​iwan/.



41\. Eric Gomez, conversation with Andrew Scobell, Senior Political Scientist, RAND Corporation, November 23, 2015.



42\. Michael Forsythe, “China Protests Sale of U.S. Arms to Taiwan,” New York Times, December 17, 2015, http://​www​.nytimes​.com/​2​0​1​5​/​1​2​/​1​8​/​w​o​r​l​d​/​a​s​i​a​/​t​a​i​w​a​n​-​a​r​m​s​-​s​a​l​e​s​-​u​s​-​c​h​i​n​a​.html.



43\. US‐​Taiwan Business Council and Project 2049 Institute, Chinese Reactions to Taiwan Arms Sales, ed. Lotta Danielsson (Arlington: US‐​Taiwan Business Council and Project 2049 Institute, 2012), p. 36.



44\. On the subject of AV-8 aircraft, see Wendell Minnick, “Despite Pressures from China, Taiwan Might Procure Harriers,” Defense News, January 16, 2016, http://​www​.defense​news​.com/​s​t​o​r​y​/​d​e​f​e​n​s​e​/​a​i​r​-​s​p​a​c​e​/​s​t​r​i​k​e​/​2​0​1​6​/​0​1​/​1​6​/​d​e​s​p​i​t​e​-​p​r​e​s​s​u​r​e​s​-​c​h​i​n​a​-​t​a​i​w​a​n​-​m​i​g​h​t​-​p​r​o​c​u​r​e​-​h​a​r​r​i​e​r​s​/​7​8​7​3​3284/. On Perry‐​class frigates, see Ankit Panda, “US Finalizes Sale of Perry‐​class Frigates to Taiwan,” The Diplomat, December 20, 2014, http://​thediplo​mat​.com/​2​0​1​4​/​1​2​/​u​s​-​f​i​n​a​l​i​z​e​s​-​s​a​l​e​-​o​f​-​p​e​r​r​y​-​c​l​a​s​s​-​f​r​i​g​a​t​e​s​-​t​o​-​t​a​iwan/. On F-16 aircraft, see: Van Jackson, “Forget F‐​16s for Taiwan: It’s All About A2/AD,” The Diplomat, April 8, 2015, http://​thediplo​mat​.com/​2​0​1​5​/​0​4​/​f​o​r​g​e​t​-​f​-​1​6​s​-​f​o​r​-​t​a​i​w​a​n​-​i​t​s​-​a​l​l​-​a​b​o​u​t​-​a2ad/.



45\. William S. Murray, “Revisiting Taiwan’s Defense Strategy,” Naval War College Review 61, no. 3 (Summer 2008): 13–38; and Jim Thomas et al., Hard ROC 2.0 Taiwan and Deterrence through Protraction (Washington: Center for Strategic and Budgetary Assessments, 2014).



46\. Thomas et al., Hard ROC 2.0, p. 4.



47\. Bonnie Glaser and Anastasia Mark, “Taiwan’s Defense Spending: The Security Consequences of Choosing Butter over Guns,” Asia Maritime Transparency Initiative, March 18, 2015, http://​amti​.csis​.org/​t​a​i​w​a​n​s​-​d​e​f​e​n​s​e​-​s​p​e​n​d​i​n​g​-​t​h​e​-​s​e​c​u​r​i​t​y​-​c​o​n​s​e​q​u​e​n​c​e​s​-​o​f​-​c​h​o​o​s​i​n​g​-​b​u​t​t​e​r​-​o​v​e​r​-​guns/. Also see: Justin Logan and Ted Galen Carpenter, “Taiwan’s Defense Budget: How Taipei’s Free Riding Risks War,” Cato Institute Policy Analysis no. 600, September 13, 2007.



48\. For the full text of the letter, see Taiwan Defense and National Security, “Benjamin L. Cardin and John McCain Letter to President Obama Regarding Arms Sales to Taiwan,” November 19, 2015, http://www.ustaiwandefense.com/benjamin-l-cardin-john-mccain-letter-to-president-obama-regarding-arms-sales-to-taiwan-november-19–2015/.



49\. James Holmes, “Securing Taiwan Starts with Overhauling Its Navy,” The National Interest, February 5, 2016, http://​nation​al​in​ter​est​.org/​f​e​a​t​u​r​e​/​s​e​c​u​r​i​n​g​-​t​a​i​w​a​n​-​s​t​a​r​t​s​-​o​v​e​r​h​a​u​l​i​n​g​-​t​h​e​-​n​a​v​y​-​15122.



50\. Ian Easton, Able Archers: Taiwan Defense Strategy in an Age of Precision Strike (Arlington: Project 2049 Institute, September 2014), pp. 35–37.



51\. Ibid., p. 36 (TK surface‐​to‐​air missile), and p. 64 (HF-3 anti‐​ship missile). It should be noted that the quote comes from a report written in late 2014. In February 2016, the U.S. Navy announced that the Standard Missile‐​6 (SM-6), which is capable of greater range and speed than the HF-3, will be modified for use as an anti‐​ship missile. Sam LaGrone, “SECDEF Carter Confirms Navy Developing Supersonic Anti‐​Ship Missile for Cruisers, Destroyers,” USNI News, February 4, 2016, http://​news​.usni​.org/​2​0​1​6​/​0​2​/​0​4​/​s​e​c​d​e​f​-​c​a​r​t​e​r​-​c​o​n​f​i​r​m​s​-​n​a​v​y​-​d​e​v​e​l​o​p​i​n​g​-​s​u​p​e​r​s​o​n​i​c​-​a​n​t​i​-​s​h​i​p​-​m​i​s​s​i​l​e​-​f​o​r​-​c​r​u​i​s​e​r​s​-​d​e​s​t​r​oyers.



52\. Michael Green et al., Asia‐​Pacific Rebalance 2025: Capabilities, Presence and Partnerships (Washington: Center for Strategic and International Studies, January 2016), p. 94.



53\. Susan Thornton, “Testimony of Susan Thornton,” House Foreign Affairs Committee, Subcommittee on Asia and the Pacific (Washington: House Foreign Affairs Committee, February 11, 2016), p. 4, http://​docs​.house​.gov/​m​e​e​t​i​n​g​s​/​F​A​/​F​A​0​5​/​2​0​1​6​0​2​1​1​/​1​0​4​4​5​7​/​H​H​R​G​-​1​1​4​-​F​A​0​5​-​W​s​t​a​t​e​-​T​h​o​r​n​t​o​n​S​-​2​0​1​6​0​2​1​1.pdf.



54\. Mearsheimer, “Say Goodbye to Taiwan,” p. 35.



55\. Shelley Rigger, “Why Giving Up Taiwan Will Not Help Us with China,” American Enterprise Institute (November 2011), p. 3.



56\. Ivan Arreguín‐​Toft, “How the Weak Win Wars: A Theory of Asymmetric Conflict,” International Security 26, no. 1 (Summer 2001): 96.



57\. Michael J. Lostumbo et al., Air Defense Options for Taiwan: An Assessment of Relative Costs and Operational Benefits (Santa Monica, CA: RAND Corporation, 2016); Murray, “Revisiting Taiwan’s Defense Strategy,” p. 13.



58\. Holmes, “Securing Taiwan Starts with Overhauling Its Navy.”



59\. Grant Newsham and Kerry Gershaneck, “Saving Taiwan’s Marine Corps,” The Diplomat, November 16, 2015, http://​thediplo​mat​.com/​2​0​1​5​/​1​1​/​s​a​v​i​n​g​-​t​h​e​-​t​a​i​w​a​n​-​m​a​r​i​n​e​-​c​orps/.



60\. Goldstein, Meeting China Halfway, p. 65.



61\. Dan De Luce et al., “Why China’s Land Grab Is Backfiring on Beijing,” Foreign Policy, December 7, 2015, http://​for​eign​pol​i​cy​.com/​2​0​1​5​/​1​2​/​0​7​/​w​h​y​-​c​h​i​n​a​s​-​l​a​n​d​-​g​r​a​b​-​i​s​-​b​a​c​k​f​i​r​i​n​g​-​o​n​-​b​e​i​jing/; Prashanth Parameswaran, “Indonesia Plays Up New South China Sea ‘Base’ after China Spat,” The Diplomat, March 28, 2016, http://​thediplo​mat​.com/​2​0​1​6​/​0​3​/​i​n​d​o​n​e​s​i​a​-​p​l​a​y​s​-​u​p​-​n​e​w​-​s​o​u​t​h​-​c​h​i​n​a​-​s​e​a​-​b​a​s​e​-​a​f​t​e​r​-​c​h​i​n​a​-​spat/; and Richard Sisk, “Japan Sends ‘Destroyer’ to South China Sea in Message to China,” Mil​i​tary​.com, April 6, 2016, http://​www​.mil​i​tary​.com/​d​a​i​l​y​-​n​e​w​s​/​2​0​1​6​/​0​4​/​0​8​/​j​a​p​a​n​-​s​e​n​d​s​-​d​e​s​t​r​o​y​e​r​-​t​o​-​s​o​u​t​h​-​c​h​i​n​a​-​s​e​a​-​i​n​-​m​e​s​s​a​g​e​-​t​o​-​c​h​i​n​a​.​h​t​m​l​#​d​i​s​q​u​s​_​t​hread.



62\. Chase et al., China’s Incomplete Military Transformation; and Scott L. Kastner, “Is the Taiwan Strait Still a Flash Point? Rethinking the Prospects for Armed Conflict between China and Taiwan,” International Security 40, no. 3 (Winter 2015/16): 71–74.



63\. David A. Shlapak et al., A Question of Balance: Political Context and Military Aspects of the China‐​Taiwan Dispute (Santa Monica, CA: RAND Corporation, 2009), p. 118.



64\. Chase et al., China’s Incomplete Military Transformation, p. 100.



65\. Recent military reforms could speed up the pace of solving these challenges. See “Xi’s New Model Army,” The Economist, January 16, 2016, http://​www​.econ​o​mist​.com/​n​e​w​s​/​c​h​i​n​a​/​2​1​6​8​8​4​2​4​-​x​i​-​j​i​n​p​i​n​g​-​r​e​f​o​r​m​s​-​c​h​i​n​a​s​-​a​r​m​e​d​-​f​o​r​c​e​s​t​o​-​h​i​s​-​o​w​n​-​a​d​v​a​n​t​a​g​e​-​x​i​s​-​n​e​w​-​m​o​d​e​l​-army; Kor Kian Beng, “A Different PLA with China’s Military Reforms,” Straits Times (Singapore), January 5, 2016, http://​www​.strait​stimes​.com/​a​s​i​a​/​a​-​d​i​f​f​e​r​e​n​t​-​p​l​a​-​w​i​t​h​-​c​h​i​n​a​s​-​m​i​l​i​t​a​r​y​-​r​e​forms; and Mu Chunshan, “The Logic Behind China’s Military Reforms,” The Diplomat, December 5, 2015,http://​thediplo​mat​.com/​2​0​1​5​/​1​2​/​t​h​e​-​l​o​g​i​c​-​b​e​h​i​n​d​-​c​h​i​n​a​s​-​m​i​l​i​t​a​r​y​-​r​e​f​orms/.



66\. Glaser, “A U.S.-China Grand Bargain?” p. 51.



67\. Goldstein, Meeting China Halfway, p. 12.



68\. In a recent Pew survey, 23 percent of U.S. respondents considered China to be an adversary of the United States, while 44 percent considered China to be a “serious problem, but not an adversary.” Pew Research Center, “International Threats, Defense Spending,” May 5, 2016,http://​www​.peo​ple​-press​.org/​2​0​1​6​/​0​5​/​0​5​/​3​-​i​n​t​e​r​n​a​t​i​o​n​a​l​-​t​h​r​e​a​t​s​-​d​e​f​e​n​s​e​-​s​p​e​n​ding/.



69\. On cyber espionage, see: Jon R. Lindsay, “The Impact of China on Cybersecurity: Fiction and Friction,” _International Security_ 39, no. 3 (Winter 2014/15): 7–47; and Ellen Nakashima, “Chinese Breach Data of 4 Million Federal Workers,” _Washington Post_ , June 4, 2015,https://​www​.wash​ing​ton​post​.com/​w​o​r​l​d​/​n​a​t​i​o​n​a​l​-​s​e​c​u​r​i​t​y​/​c​h​i​n​e​s​e​-​h​a​c​k​e​r​s​-​b​r​e​a​c​h​-​f​e​d​e​r​a​l​-​g​o​v​e​r​n​m​e​n​t​s​-​p​e​r​s​o​n​n​e​l​-​o​f​f​i​c​e​/​2​0​1​5​/​0​6​/​0​4​/​8​8​9​c​0​e​5​2​-​0​a​f​7​-​1​1​e​5​-​9​5​f​d​-​d​5​8​0​f​1​c​5​d​4​4​e​_​s​t​o​r​y​.html. On alternative institutions, see: Jane Perlez, “China Creates a World Bank of Its Own, and the U.S. Balks,” _New York Times_ , December 4, 2015, http://​www​.nytimes​.com/​2​0​1​5​/​1​2​/​0​5​/​b​u​s​i​n​e​s​s​/​i​n​t​e​r​n​a​t​i​o​n​a​l​/​c​h​i​n​a​-​c​r​e​a​t​e​s​-​a​n​-​a​s​i​a​n​-​b​a​n​k​-​a​s​-​t​h​e​-​u​s​-​s​t​a​n​d​s​-​a​l​o​o​f​.​h​t​m​l​?_r=0. On the South China Sea, see: Melissa Sim, “U.S., China Cross Swords over South China Sea,” Straits Times (Singapore), February 25, 2016, http://​www​.strait​stimes​.com/​w​o​r​l​d​/​u​n​i​t​e​d​-​s​t​a​t​e​s​/​u​s​-​c​h​i​n​a​-​c​r​o​s​s​-​s​w​o​r​d​s​-​o​v​e​r​-​s​o​u​t​h​-​c​h​i​n​a-sea.



70\. Joshua P. Meltzer, “U.S.-China Joint Presidential Statement on Climate Change: The Road to Paris and Beyond,” Brookings Institution, September 29, 2015, http://​www​.brook​ings​.edu/​b​l​o​g​s​/​p​l​a​n​e​t​p​o​l​i​c​y​/​p​o​s​t​s​/​2​0​1​5​/​0​9​/​2​9​-​u​s​-​c​h​i​n​a​-​s​t​a​t​e​m​e​n​t​-​c​l​i​m​a​t​e​-​c​h​a​n​g​e​-​m​e​ltzer; and Somini Sengupta, “U.S. and China Agree on Proposal for Tougher North Korea Sanctions,” New York Times, February 25, 2016, http://​www​.nytimes​.com/​2​0​1​6​/​0​2​/​2​6​/​w​o​r​l​d​/​a​s​i​a​/​n​o​r​t​h​-​k​o​r​e​a​-​s​a​n​c​t​i​o​n​s​.html.



71\. Posen, Restraint, pp. 93–96.



72\. Exacerbating tensions: Andrew J. Nathan and Andrew Scobell, “How China Sees America: The Sum of Beijing’s Fears,” Foreign Affairs 91, no. 5 (September/​October 2012): 32–47; and Robert S. Ross, “The Problem with the Pivot,” Foreign Affairs 91, no. 6 (November/​December 2012): 70–82. On the subject of Taiwan’s role in the pivot, see: Green et al., Asia‐​Pacific Rebalance 2025, pp. 87–94.



73\. Fiona S. Cunningham and M. Taylor Fravel, “Assuring Assured Retaliation: China’s Nuclear Posture and U.S.-China Strategic Stability,” International Security 40, no.2 (Fall 2015): 16–19; and Gregory Kulacki, China’s Military Calls for Putting Its Nuclear Forces on Alert (Cambridge, MA: Union of Concerned Scientists, January 2016).



74\. Jennifer Lind, “Japan’s Security Evolution,” Cato Institute Policy Analysis no. 788, February 25, 2016; Logan and Carpenter, “Taiwan’s Defense Budget.”



75\. Kastner, “Is the Taiwan Strait Still a Flash Point?” p. 76.



76\. Ibid.



77\. “The Causes and Consequences of China’s Market Crash,” The Economist, August 24, 2015, http://​www​.econ​o​mist​.com/​n​e​w​s​/​b​u​s​i​n​e​s​s​-​a​n​d​-​f​i​n​a​n​c​e​/​2​1​6​6​2​0​9​2​-​c​h​i​n​a​-​s​n​e​e​z​i​n​g​-​r​e​s​t​-​w​o​r​l​d​-​r​i​g​h​t​l​y​-​n​e​r​v​o​u​s​-​c​a​u​s​e​s​-​a​n​d​-​c​o​n​s​e​q​u​e​n​c​e​s​-​c​hinas.



78\. James Griffiths, “China on Strike,” CNN, March 29, 2016, http://​www​.cnn​.com/​2​0​1​6​/​0​3​/​2​8​/​a​s​i​a​/​c​h​i​n​a​-​s​t​r​i​k​e​-​w​o​r​k​e​r​-​p​r​o​t​e​s​t​-​t​r​a​d​e​-​u​n​i​o​n​/​i​n​d​e​x​.html.



79\. Kevin Yao and Meng Meng, “China Expects to Lay Off 1.8 Million Workers in Coal, Steel Sectors,” Reuters, February 29, 2016, http://​www​.reuters​.com/​a​r​t​i​c​l​e​/​u​s​-​c​h​i​n​a​-​e​c​o​n​o​m​y​-​e​m​p​l​o​y​m​e​n​t​-​i​d​U​S​K​C​N​0​W205X.



80\. Carpenter, “Could China’s Economic Troubles Spark a War?”



81\. Taisu Zhang, “China’s Coming Ideological Wars,” Foreign Policy, March 1, 2016, http://​for​eign​pol​i​cy​.com/​2​0​1​6​/​0​3​/​0​1​/​c​h​i​n​a​s​-​c​o​m​i​n​g​-​i​d​e​o​l​o​g​i​c​a​l​-​w​a​r​s​-​n​e​w​-​l​e​f​t​-​c​o​n​f​u​c​i​u​s​-​m​a​o-xi/.



82\. David Larter, “Carrier Scramble: CENTCOM, PACOM Face Flattop Gaps This Spring Amid Tensions,” Navy Times, January 7, 2016, http://​www​.navy​times​.com/​s​t​o​r​y​/​m​i​l​i​t​a​r​y​/​2​0​1​6​/​0​1​/​0​7​/​c​a​r​r​i​e​r​-​s​c​r​a​m​b​l​e​-​c​e​n​t​c​o​m​-​p​a​c​o​m​-​f​a​c​e​-​f​l​a​t​t​o​p​-​g​a​p​s​-​s​p​r​i​n​g​-​a​m​i​d​-​t​e​n​s​i​o​n​s​/​7​8​4​2​6140/; and Andrea Shalal, “U.S. Arms Makers Strain to Meet Demand as Mideast Conflicts Rage,” Reuters, December 4, 2015, http://​www​.reuters​.com/​a​r​t​i​c​l​e​/​u​s​-​m​i​d​e​a​s​t​-​c​r​i​s​i​s​-​u​s​a​-​a​r​m​s​-​i​n​s​i​g​h​t​-​i​d​U​S​K​B​N​0​T​N​2​D​A​2​0​1​51204.



83\. Dana R. Dillon and John J. Tkacik, Jr., “China and ASEAN: Endangered American Primacy in Southeast Asia,” Heritage Foundation Backgrounder no. 1886, October 19, 2005, http://​www​.her​itage​.org/​r​e​s​e​a​r​c​h​/​r​e​p​o​r​t​s​/​2​0​0​5​/​1​0​/​c​h​i​n​a​-​a​n​d​-​a​s​e​a​n​-​e​n​d​a​n​g​e​r​e​d​-​a​m​e​r​i​c​a​n​-​p​r​i​m​a​c​y​-​i​n​-​s​o​u​t​h​e​a​s​t​-asia.



84\. Ross, “The 1995–96 Taiwan Strait Confrontation,” p. 109.



85\. Nancy Bernkopf Tucker and Bonnie Glaser, “Should the United States Abandon Taiwan?” The Washington Quarterly 34, no. 4 (Fall 2011): 32–33; Mearsheimer, “Say Goodbye to Taiwan”; Peter Navarro, “Is It Time for America to ‘Surrender’ Taiwan?” The National Interest, January 18, 2016, http://www.nationalinterest.org/blog/the-buzz/it-time-america-%E2%80%98surrender%E2%80%99-taiwan-14955; and Daniel Twining, “(Why) Should America Abandon Taiwan?” Foreign Policy, January 10, 2012, http://​for​eign​pol​i​cy​.com/​2​0​1​2​/​0​1​/​1​0​/​w​h​y​-​s​h​o​u​l​d​-​a​m​e​r​i​c​a​-​a​b​a​n​d​o​n​-​t​a​iwan/.



86\. Bush, “Taiwan’s January 2016 Elections and Their Implications for Relations with China and the United States,” p. 21.



87\. Max Fisher, “The Credibility Trap,” Vox, April 29, 2016, http://​www​.vox​.com/​2​0​1​6​/​4​/​2​9​/​1​1​4​3​1​8​0​8​/​c​r​e​d​i​b​i​l​i​t​y​-​f​o​r​e​i​g​n​-​p​o​l​i​c​y-war; Paul Huth and Bruce Russett, “What Makes Deterrence Work? Cases from 1900 to 1980,” World Politics 36, no. 4 (July 1984): 496–526; Jonathan Mercer Reputation and International Politics (Ithaca, NY: Cornell University Press, 1996), pp. 22–25; and Press, Calculating Credibility, pp. 20–24.



88\. Glaser, “A U.S.-China Grand Bargain?” pp. 77–78.



89\. Bosco, “Taiwan and Strategic Security.”



90\. Quoted in Andrew S. Erickson and Joel Wuthnow, “Why Islands Still Matter in Asia,” The National Interest, February 5, 2016, http://​nation​al​in​ter​est​.org/​f​e​a​t​u​r​e​/​w​h​y​-​i​s​l​a​n​d​s​-​s​t​i​l​l​-​m​a​t​t​e​r​-​a​s​i​a​-​1​5​1​2​1​?​p​a​g​e​=show.



91\. Nathan and Scobell, China’s Search for Security, pp. 307–08.



92\. J. Michael Cole, “How A2/AD Can Defeat China,” The Diplomat, November 12, 2013, http://​thediplo​mat​.com/​2​0​1​3​/​1​1​/​h​o​w​-​a​2​a​d​-​c​a​n​-​d​e​f​e​a​t​-​c​hina/; Eric Gomez, “Taiwan’s Best Option for Deterring China? Anti‐​Access/​Area Denial,” Cato at Liberty (blog), April 7, 2016,http://​www​.cato​.org/​b​l​o​g​/​t​a​i​w​a​n​s​-​b​e​s​t​-​o​p​t​i​o​n​-​d​e​t​e​r​r​i​n​g​-​c​h​i​n​a​-​a​n​t​i​-​a​c​c​e​s​s​a​r​e​a​-​d​enial; Holmes, “Securing Taiwan Starts with Overhauling Its Navy”; and Thomas et al., Hard ROC 2.0.



93\. Lostumbo et al., Air Defense Options for Taiwan, pp. 2–11.



94\. Ibid., pp. 73–89.



95\. Glaser, “A U.S.-China Grand Bargain?” p. 79.



96\. Ibid., pp. 78–83.



97\. Goldstein, _Meeting China Halfway,_ p. 12.



98\. Sam LaGrone, “UPDATED: U.S. Plans Modest $1.83B Taiwan Arms Deal; Little Offensive Power in Proposed Package,” _USNI News,_ December 16, 2015, https://news.usni.org/2015/12/16/breaking-u-s-plans-modest-1–83b-taiwan-arms-deal-little-offensive-power-in-proposed-package.



99\. For Taiwan’s indigenously produced equipment, see Easton, _Able Archers._



100\. Kastner, “Is the Taiwan Strait Still a Flash Point?” p. 70; and Mazza, “Taiwanese Hard Power: Between a ROC and a Hard Place,” p. 202.



101\. Murray, “Revisiting Taiwan’s Defense Strategy,” pp. 17–19; and Thomas et al., Hard ROC 2.0, pp. 12–14.



102\. Goldstein, _Meeting China Halfway,_ p. 63.



103\. Glaser, “A U.S.-China Grand Bargain?” p. 85.



104\. Carpenter, _America’s Coming War with China,_ p. 177.



105\. Eric Gomez, “The U.S.-Taiwan Relationship Needs a Change,” _Cato at Liberty_ (blog), November 30, 2015, http://​www​.cato​.org/​b​l​o​g​/​u​s​-​t​a​i​w​a​n​-​r​e​l​a​t​i​o​n​s​h​i​p​-​n​e​e​d​s​-​c​hange>.



106\. Carpenter, _America’s Coming War with China,_ p. 176.
"
"
In comments, Jonn-X wondered:
Dead pixels or new sunspecks (pore-ettes) ?
At first I was pretty sure I was looking at nothing, then I saw the official NOAA bulletin
http://www.swpc.noaa.gov/forecast.html
and the usual phrase, “The visible disk was spotless,” was omitted – typical practice when there’s something there, but too small to be “officially noticed.”
Anybody else see anything?
I do. I know where the dead pixels are, and have labeled them below in the SOHO MDI image. Note that there are two very small sunspecks, possibly soon to be sunspots, emerging on both sides of the equator.

Click for a full sized image
For those that don’t know. The SOHO spacecraft sensor does have some stuck pixels, and these can sometimes be cured in a “bake off” where they heat up the sensor for a few hours.
Our resident official solar physicist, Dr. Leif Svalgarrd will confirm or refute my suspicions on the categorizations of SC23 and SC24 I’m sure. For comparisons, you can also see the SOHO magnetogram.
I’ve included it also below:
UPDATE: The specks are fading, so far no observation agency has assigned a region or counted them that I know of, see the updated SOHO MDI.
SOHO Magnetogram- click for larger image
UPDATED SOHO MDI:

Click for larger image


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c58d268',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Art courtesy Dave Stephens
Foreword by Anthony Watts: This article, written by the two Jeffs (Jeff C and Jeff Id) is one of the more technically complex essays ever presented on WUWT. It has been several days in the making. One of the goals I have with WUWT is to make sometimes difficult to understand science understandable to a wider audience. In this case the statistical analysis is rather difficult for the layman to comprehend, but I asked for (and got) an essay that was explained in terms I think many can grasp and understand. That being said, it is a long article, and you may have to read it more than once to fully grasp what has been presented here. Steve McIntyre of Climate Audit laid much of the ground work for this essay, and from his work as well as this essay, it is becoming clearer that Steig et al (see “Warming of the Antarctic ice-sheet surface since the 1957 International Geophysical Year”, Nature, Jan 22, 2009) isn’t holding up well to rigorous tests as demonstrated by McIntyre as well as in the essay below. Unfortunately, Steig’s office has so far deferred (several requests) to provide the complete data sets needed to replicate and test his paper, and has left on a trip to Antarctica and the remaining data is not “expected” to be available until his return.
To help layman readers understand the terminology used, here is a mini-glossary in advance:
RegEM – Regularized Expectation Maximization
PCA – Principal Components  Analysis
PC – Principal Components
AWS – Automatic Weather  Stations
One of the more difficult concepts is RegEM, an algorithm developed by Tapio Schneider in 2001.   It’s a form of expectation maximization (EM) which is a  common and well understood method for infilling missing data. As we’ve previously noted on WUWT, many of the weather stations used in the Steig et al study had issues with being buried by snow, causing significant data gaps in the Antarctic record and in some burial cases stations have been accidentally lost or confused with others at different lat/lons. Then of course there is the problem of coming up with trends for the entire Antarctic continent when most of the weather station data is from the periphery and the penisula, with very little data from the interior.
Expectation Maximization is a method which uses  a normal distribution to compute the best probability of fit to a missing piece  of data.  Regularization is required when so much data is missing that the EM  method won’t solve.  That makes it a statistically dangerous  technique to use and as Kevin Trenberth, climate analysis chief at the National Center for Atmospheric Research, said in an e-mail: “It is hard to make data where none exist.” (Source: MSNBC article) It is also valuable to note that one of the co-authors of Steig et al, Dr. Michael Mann, dabbles quite a bit in RegEm in this preparatory paper to Mann et al 2008 “Return of the Hockey Stick”.
For those that prefer to print and read, I’ve made a PDF file of this article available here.
Introduction
This article is an attempt to describe some of the early results from the Antarctic reconstruction recently published on the cover of Nature which demonstrated a warming trend in the Antarctic since 1956.   Actual surface temperatures in the Antarctic are hard to come by with only about 30 stations prior to 1980 recorded through tedious and difficult efforts by scientists in the region.  In the 80’s more stations were added including some automatic weather stations (AWS) which sit in remote areas and report the temperature information automatically.  Unfortunately due to the harsh conditions in the region many of these stations have gaps in their records or very short reporting times (a few years in some cases).  Very few stations are located in the interior of the Antarctic, leaving the trend for the central portion of the continent relatively unknown.  The location of the stations is shown on the map below.

In addition to the stations there are satellite data from an infrared surface temperature measurement which records the temperature of the actual emission from the surface of the ice/ground in the Antarctic.  This is different from the microwave absorption measurements as made from UAH/RSS data which measure temperatures in a thickness of the atmosphere.  This dataset didn’t start until 1982.
Steig 09 is an attempt to reconstruct the continent-wide temperatures using a combination of measurements from the surface stations shown above and the post-1982 satellite data.  The complex math behind the paper is an attempt to ‘paste’ the 30ish pre-1982 real surface station measurements onto 5509 individual gridcells from the satellite data.  An engineer or vision system designer could use several straightforward methods which would insure reasonable distribution of the trends across the grid based on a huge variety of area weighting algorithms, the accuracy of any of the methods would depend on the amount of data available.  These well understood methods were ignored in Steig09 in favor of RegEM.
The use of Principal Component Analysis in the reconstruction
Steig 09 presents the satellite reconstructions as the trend and also provides an AWS reconstruction as verification of the satellite data rather than a separate stand alone result presumably due to the sparseness of the actual data.  An algorithm called RegEM was used for infilling the missing data. Missing data includes pre 1982 for satellites and all years for the very sparse AWS data.  While Dr. Steig has provided the reconstructions to the public, he has declined to provide any of the satellite, station or AWS temperature measurements used as inputs to the RegEM algorithm.  Since the station and AWS measurements were available through other sources, this paper focuses on the AWS reconstruction.
Without getting into the detail of PCA analysis, the algorithm uses covariance to assign weighting of a pattern in the data and does not have any input whatsoever for actual station location.  In other words, the algorithm has no knowledge of the distance between stations and must infill missing data based solely on the correlation with other data sets.  This means there is a possibility that with improper or incomplete checks, a trend from the peninsula on the west coast could be applied all the way to the east.  The only control is the correlation of one temperature measurement to another.
If you were an engineer concerned with the quality of your result, you would recognize the possibility of accidental mismatch and do a reasonable amount of checking to insure that the stations were properly assigned after infilling.  Steig et. al. described no attempts to check this basic potential problem with RegEM analysis.  This paper will describe a simple method we used to determine that the AWS reconstruction is rife with spurious (i.e. appear real but really aren’t) correlations attributed to the methods used by Dr. Steig.  These spurious correlations can take a localized climactic pattern and “smear” it over a large region that lacks adequate data of its own.
Now is where it becomes a little tricky.  RegEM uses a reduced information dataset to infill the missing values.  The dataset is reduced by Principal Component Analysis (PCA) replacing each trend with a similar looking one which is used for covariance analysis.  Think of it like a data compression algorithm for a picture which uses less computer memory than the actual but results in a fuzzier image for higher compression levels.

While the second image is still visible, the actual data used to represent the image is reduced considerably.  This will work fine for pictures with reasonable compression, but the data from some pixels has blended into others.  Steig 09 uses 3 trends to represent all of the data in the Antarctic.  In it’s full complexity using 3 PC’s is analogous to representing not just a picture but actually a movie of the Antarctic with three color ‘trends’ where the color of each pixel changes according to different weights of the same red, green and blue color trends (PC’s).  With enough PC’s the movie could be replicated perfectly with no loss.  Here’s an important quote from the paper.
“We therefore used the RegEM algorithm with a cut-off parameter K=3. A disadvantage of excluding higher-order terms (k>3) is that this fails to fully capture the variance in the Antarctic Peninsula region.  We accept this tradeoff because the Peninsula is already the best-observed region of the Antarctic.”

Above: a graph from Steve McIntyre of ClimateAudit where he demonstrates how “K=3 was in fact a fortuitous choice, as this proved to yield the maximum AWS trend, something that will, I’m sure, astonish most CA readers.”
K=3 means only 3 trends were used, the ‘lack of captured variance’ is an acknowledgement and acceptance of the fuzziness of the image.  It’s easy to imagine that it would be difficult to represent a complex movie image of Antarctic with any sharpness from 1957 to 2006 temperature with the same 3 color trends reweighted for every pixel.  In the satellite version of the Antarctic movie the three trends look like this.

Note that the sudden step in the 3rd trend would cause a jump in the ‘temperature’ of the entire movie.  This represents the temperature change between the pre 1982 recreated data and the after 1982 real data in the satellite reconstruction.  This is a strong yet overlooked hint that something may not be right with the result.
In the case of the AWS reconstruction we have only 63 AWS stations to make the movie screen, by which the trends of 42 surface station points are used to infill the remaining data.  If the data from one surface station is copied to the wrong AWS stations the average will overweight and underweight some trends. So the question becomes, is the compression level too high?
The problems that arise when using too few principal components
Fortunately, we’re here to help in this matter.  Steve McIntyre again provided the answer with a simple plot of the actual surface station data correlation with distance.  This correlation plot compares the similarities ‘correlation’ of each temperature station with all of the 41 other manual surface stations against the distance between them.  A correlation of 1 means the data from one station is exactly equal to the other.  Because A -> B correlation isn’t a perfect match for B->A there are 42*42 separate points in the graph.  This first scatter plot is from measured temperature data prior to any infilling of missing measurements.  Station to station distance is shown on the X axis.  The correlation coefficient is shown on the Y axis.

Since this plot above represents the only real data we have existing back to 1957, it demonstrates the expected ‘natural’ spatial relationship from any properly controlled RegEM analysis.  The correlation drops with distance which we would expect because temps from stations thousands of miles away should be less related than those next to each other.  (Note that there are a few stations that show a positive correlation beyond 6000 km.  These are entirely from non-continental northern islands inexplicably used by Steig in the reconstruction.  No continental stations exhibit positive correlations at these distances.)  If RegEM works, the reconstructed RegEM imputed (infilled) data correlation vs. distance should have a very similar pattern to the real data.  Here’s a graph of the AWS reconstruction with infilled temperature values.
Compare this plot with the previous plot from actual measured temperatures.  Now contrast that with the AWS plot above.  The infilled AWS reconstruction has no clearly evident pattern of decay over distance.  In fact, many of the stations show a correlation of close to 1 for stations at 3000 km distant!  The measured station data is our best indicator of true Antarctic trends and it shows no sign that these long distance correlations occur.  Of course, common sense should also make one suspicious of these long distance correlations as they would be comparable to data that indicated Los Angeles and Chicago had closely correlated climate.
It was earlier mentioned that the use of 3 PCs was analogous to the loss of detail that occurs in data compressions.   Since the AWS input data is available, it is possible to regenerate the AWS reconstruction using a higher number of PCs.  It stood to reason that spurious correlations could be reduced by retaining the spatial detail lost in the 3 PC reconstruction.  Using RegEM, we generated a new AWS reconstruction using the same input data but with 7 PCs.  The distance correlations are shown in the plot below.

Note the dramatic improvement over that shown in the previous plot.  The correlation decay with distance so clearly seen in the measured station temperature data has returned.  While the cone of the RegEM data is slightly wider than the ‘real’ surface station data, the counterintuitive long distance correlations seen in the Steig reconstruction have completely disappeared.  It seems clear that limiting the reconstruction to 3 PCs resulted in numerous spurious correlations when infilling missing station data.


Using only 3 principal components distorts temperature trends
If Antarctica had uniform temperature trends across the continent, the spurious correlations might not have a large impact in the overall reconstruction.  Individual sites may have some errors, but the overall trend would be reasonably close.  However, Antarctica is anything but uniform.  The spurious correlations can allow unique climactic trends from a localized region to be spread over a larger area, particularly if an area lacks detailed climate records of its own.  It is our conclusion is that is exactly what is happening with the Steig AWS reconstruction.
Consider the case of the Antarctic Peninsula:

The      peninsula is geographically isolated from the rest of the continent
The      peninsula is less than 5% of the total continental land mass
The      peninsula is known to be warming at a rate much higher than anywhere else      in Antarctica
The      peninsula is bordered by a vast area known as West Antarctica that has      extremely limited temperature records of its own
15 of      the 42 temperature surface stations (35%) used in the reconstruction are      located on the peninsula

If the Steig AWS reconstruction was properly correlating the peninsula stations temperature measurements to the AWS sites, you would expect to see the highest rates of warming at the peninsula extremes.  This is the pattern seen in the measured station data.  The plot below shows the temperature trends for the reconstructed AWS sites for the period of 1980 to 2006.  This time frame has been selected as this is the period when AWS data exists.  Prior to 1980, 100% of AWS reconstructed data is artificial (i.e. infilled by RegEM).

Note how warming extends beyond the peninsula extremes down toward West Antarctica and the South Pole.  Also note the relatively moderate cooling in the vicinity of the Ross Ice Shelf (bottom of the plot).  The warming once thought to be limited to the peninsula appears to have spread.  This “smearing” of the peninsula warming has also moderated the cooling of the Ross Ice Shelf AWS measurements.  These are both artifacts of limiting the reconstruction to 3 PCs.
Now compare the above plot to the new AWS reconstruction using 7 PCs.

The difference is striking.  The peninsula has become warmer and warming is largely limited to its confines.  West Antarctica and the Ross Ice Shelf area have become noticeably cooler.  This agrees with the commonly-held belief prior to Steig’s paper that the peninsula is warming, the rest of Antarctica is not.
Temperature trends using more traditional methods
In providing a continental trend for Antarctica warming, Steig used a simple average of the 63 AWS reconstructed time series.  As can be seen in the plots above, the AWS stations are heavily weighted toward the peninsula and the Ross Ice Shelf area.  Steig’s simple average is shown below.  The linear trend for 1957 through 2006 is +0.14 deg C/decade.  It is worth noting that if the time frame is limited to 1980 to 2006 (the period of actual AWS measurements), the trend changes to cooling, -0.06 deg C/decade.

We used a gridding methodology to weight the AWS reconstructions in proportion to the area they represent.  Using the Steig’s method, 3 stations on the peninsula over 5% of the continent’s area would have the same weighting as three interior stations spread over 30% of the continent area.  The gridding method we used is comparable to that utilized in other temperature constructions such as James Hansen’s GISStemp.  The gridcell map used for the weighted 7 PC reconstruction is shown here.

 
Cells with a single letter contain one or more AWS temperature stations.  If more than one AWS falls within a gridcell, the results were averaged and assigned to that cell.  Cells with multiple letters had no AWS within them, but had three or more contiguous cells containing AWS stations.  Imputed temperature time series were assigned to these cells based on the average of the neighboring cells.  Temperature trends were calculated both with and without the imputed cells.  The reconstruction trend using 7 PCs and a weighted station average follow.

The trend has decreased to 0.08 deg C/decade.  Although it is not readily apparent in this plot, from 1980 to 2006 the temperature profile has a pronounced negative trend.
Temporal smearing problems caused by too few PCs?
The temperature trends using the various reconstruction methods are shown in the table below.  We have broken the trends down into three time periods; 1957 to 2006, 1957 to 1979, and 1980 to 2006.  The time frames are not arbitrarily chosen, but mark an important distinction in the AWS reconstructions.  There is no AWS data prior to 1980.  In the 1957 to 1980 time frame, every single temperature point is a product of the RegEM algorithm.   In the 1980 to 2006 time frame, AWS data exists (albeit quite spotty at times) and RegEM leaves the existing data intact while infilling the missing data.
We highlight this distinction as limiting the reconstruction to 3 PCs has an additional pernicious effect beyond spatial smearing of the peninsula warming.   In the table below, note the balance between the trends of the 1957 to 1979 era vs. that of the 1980 to 2006 era. In Steig’s 3 PC reconstruction, moderate warming that happened prior to 1980 is more balanced with slight cooling that happened post 1980.  In the new 7 PC reconstruction, the early era had dramatic warming, the later era had strong cooling.  It is believed that the 7 PC reconstruction more accurately reflects the true trends for the reasons stated earlier in this paper.  However, the mechanism for this temporal smearing of trends is not fully understood and is under investigation.  It does appear to be clear that limiting the selection to three principal components causes warming that is largely constrained to a pre-1980 time frame to appear more continuous and evenly distributed over the entire temperature record.



Reconstruction

1957 to 2006 trend


1957 to 1979 trend (pre-AWS)


1980 to 2006 trend (AWS era)



Steig 3 PC 

+0.14 deg C./decade


+0.17 deg C./decade


-0.06 deg C./decade



New 7 PC 

+0.11 deg C./decade


+0.25 deg C./decade


-0.20 deg C./decade



New 7 PC weighted

+0.09 deg C./decade


+0.22 deg C./decade


-0.20 deg C./decade



New 7 PC wgtd   imputed cells

+0.08 deg C./decade


+0.22 deg C./decade


-0.21 deg C./decade




Conclusion
The AWS trends which this incredibly long post was created from were used only as verification of the satellite data.  The statistics used for verification are another subject entirely.  Where Steig09 falls short in the verification is that RegEM was inappropriately applying area weighting to individual temperature stations.  The trends from the AWS reconstruction clearly have blended into distant stations creating an artificially high warming result.  The RegEM methodology also appears to have blended warming that occurred decades ago into more recent years to present a misleading picture of continuous warming.  It should also be noted that every attempt made to restore detail to the reconstruction or weight station data resulted in reduced warming and increased cooling in recent years.  None of these methods resulted in more warming than that shown by Steig.
We don’t yet have the satellite data (Steig has not provided it) so the argument will be:
“Silly Jeff’s you haven’t shown anything, the AWS wasn’t the conclusion it was the confirmation.”
To that we reply with an interesting distance correlation graph of the satellite reconstruction (also from only 3 PCs).  The conclusion has the exact same problem as the confirmation.  Stay tuned.

(Graph originally calculated by Steve McIntyre)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e97a0fe00',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The unprecedented, devastating bushfires that engulfed Australia – from even before our summer began – have forever disrupted our usually laconic and relaxed memories of Christmas and New Year. Those memories are instead marked by anguish, anxiety, grief and consternation about our future.  We turned to fire and emergency chiefs, tireless local volunteers, our national broadcaster and the spirit of community to find leadership in this crisis. And very late, but at last, we turned to scientists; fire, flood and climate experts; and vitally, Indigenous knowledge, to start to rebuild a sense of a path forward for our nation. Consistent with the rapidly changing expectations of global investors, we’ve also turned to business for leadership. It’s no coincidence the Edelman trust barometer data for 2020 reveals that around the world, trust in government leaders has continued to collapse, while our highest trust is reserved for scientists and people in our local community.  Significant increases have been observed in our expectations of CEOs: 77% of people agree that the CEO of the company where they work should speak out on climate change and 74% believe that CEOs should take the lead on change rather than waiting for the government to impose it. As a company director returning to work in preparation for the half-year results period, the change in sentiment about action on climate change among many colleagues and employees has been palpable. This is both at a personal level, but also after the increasing calls from corporate regulators in relation to additional fiduciary duties of directors and executives regarding the complex and interrelated climate risks now faced by our organisations. Pleasingly, a considerable number of Australian CEOs have announced commitments to net zero carbon by 2050, and we’ve seen global organisations such as Microsoft pushing harder to account for their carbon impact in the past. I am cautiously optimistic about the impact of good businesses stepping up on climate change action. But it is tinged with the frustration of knowing that Australia could have been a global leader in decarbonising the economy had we supported business leaders calling for a legal and policy framework for reducing our carbon emissions, while protecting our economy and its most significant industries. Fifteen years ago a group of Australian business leaders from insurance, banking and energy made the case for early climate action supported by science, economics and stewardship for our country. The Australian Business Roundtable on Climate Change released The Business Case For Early Action in 2006 showing significant reductions in greenhouse gas emissions could be achieved at an affordable cost to the Australian economy. Many of us have continued the advocacy since. Particularly the insurance and finance sectors which are at the front line of exposure, as departing governor of the Bank of England, Mark Carney, continues to point out. Australia has always been a resource nation. We pay tribute to those key vital industries that have in the past shaped us as a country. But at our core, we are a resourceful nation. We have the know-how, the science and the business acumen to thrive in a low-carbon economy – and as Professor Ross Garnaut points out, be a superpower at that. It’s time to realise this vision. If we delay further we cede that opportunity to other nations. This is why the climate change bill being introduced by independent MP Zali Steggall and the crossbench comes at a crucial time. It provides an internationally proven framework that will ensure certainty so we can manage risk, as well as actively pursue new areas of opportunity in a low-carbon economy. This model has been proven and enacted in the United Kingdom, Germany, France and New Zealand. It was introduced in the UK in 2008 and adopted under a conservative government. It is sensible. It is pragmatic. It is responsible. The 2020 bill proposes: 1. An independent climate change commission to oversee the transition.2. An economy-wide net-zero target by 2050.3. Economy-wide emissions budgets and national plans for reducing greenhouse gas emissions.4. A national climate risk assessment and plans for adapting to changing climate. 5. Transparent monitoring, reporting on progress and plans. This is our moment in history to come together and demonstrate to Canberra that as a nation, we need an end to the partisan politics. We need a goal we can all work towards together. When business first called for action almost 20 years ago, time was somewhat on our side. We now have no time to waste. Beyond politics and ideology, and in collaboration with others, business leaders have the opportunity to show strong stewardship and care for a dynamic, sustainable future for our country. As business leaders – whether CEOs or nonexecutive directors – supporting a non-partisan, sensible piece of legislation is not only what our community, employees and stakeholders expect, it’s also good for business. Let’s stand up and help parliament take the next step we need for a prosperous economy to 2050.  • Sam Mostyn is a non-executive director on a number of ASX-listed boards, is a sustainability adviser and has long engagement working with civil society on climate, gender and Indigenous issues."
"
Share this...FacebookTwitterPada saluran yang indah ini kita akan sedikit berbagi menurut anda seperti apa buku petunjuk bermain judi bandarq yang benar dan aman khususnya untuk anda yang sedang pemula, bermain judi bandarq itu tidak semudah yang anda bayangkan hanya dengan menyembunyikan taruhan maka anda bakal menang begitu saja, terdapat beberapa hal penting yang harus anda pelajari beserta baik agar bisa bermain dengan profesional. Berikut merupakan panduan sederhana bermain pertaruhan bandarq. Simak baik-baik ulasannya di bawah ini.
# Langkah pendaftaran
Langkah pertama yang harus anda lakukan adalah dengan mencari distributor judi online terlebih lewat yaitu agen judi domino online yang ada pada internet, gunakan cara yang sudah kami tuliskan pada website ini untuk memperoleh agen judi domino online yang terpercaya. Setelah kamu menemukan agen judi yang bsia dipercaya silahkan lakukan pendaftaran terlebih dahulu, pati semua form dengan betul dan valid terlebih untuk isian pada nomor perkiraan hingga kontak yang dikau miliki, kemudian setelah tersebut silahkan login dengan account yang sudah anda buat tadi, lakukan transfer deposit sesuai dengan jumlah minimal maupun maksimal yang dikasih oleh situs. Setelah tersebut silahkan lanjut pada tahap berikutnya.
# Tahap pengenalan permainan judi bandarq
Kemudian tahap kedua adalah tahap pengenalan judi domino online, pertama-tama anda pahami dulu apa itu tiket judi domino online, kartu judi ini biasa disebut dengan permainan judi gaple atau dadu bernomor kalau di daerah anda. Di setiap permainan judi kartu domino dilakukan 6-8 orang di satu meja dengan nilai taruhan yang beragam, mulai dari dari 2000 rupiah terlintas ratusan bahkan jutaan yen. Permainan ini menggunakan relevansi 52 kartu acak yang masing-masing player akan jadi secara acak, tujuan dari permainan ini adalah player yang mendapatkan kombinasi poin paling besar maka dialah yang akan menjadi pemenangnya.
# Tahap pengenalan sebutan dalam permainan judi bandarq
Kemudian tahap berikutnya member akan sedikit mengulas kurang lebih pengertian dari istilah yang digunakan dalam permainan betting domino, pertama adalah pengenalan di meja sendiri. Ada istilah check, raise, fold, call, hingga all tersebut. pengertian check sendiri menjajal kartu artinya tidak berbuat taruhan, Raise menaikkan taruhan lebih tinggi, fold bukan mengikuti permainan dan mempersembahkan kartu ke bandar, all in melakukan taruhan semata dana yang kita punya dan untuk call swasembada adalah memanggil taruhan pantas dengan jumlah taruhan sebelumnya. Kemudian pembahasan mengenai tiket spesial, dalam permainan tersebut ada 4 kartu spesial yang bisa anda miliki, jika anda bisa memperoleh salah satu dari 4 kartu spesial ini bisa dipastikan anda menang. Slip spesial mulai dari slip 99/qiu, kartu murni kecil/besar, kartu kembar/balak dan slip 6 dewa.
# Tara bermain judi domino online
Tahap pertama anda bakal duduk berjumlah 8 orang-orang, pembagian kartu seperti suksesi jam, pada pembagian pertama anda hanya akan nampi 3 kartu, analisa tiket anda apakah nantinya dapat berkombinasi bagus atau bukan, jangan terkecoh dengan slip bagus sebelum kartu ke empat muncul. Lakukan judi bola sesuai dengan keyakinan serta keputusan anda, setelah tersebut kartu ke empat bakal muncul, jika kartu kamu bagus misalnya 99/qq tidak sungkan untuk all in.
Nah itulah cara permainan judi domino yang simple dan mudah untuk para pemain pemula
Share this...FacebookTwitter "
"

I am rather concerned about my elderly father‐​in‐​law, who lives in northern Virginia. I just visited him, as Washington’s temperatures bubbled into the high 90s. On his television, the summer’s first heat‐​related fatalities were being reported. I noticed that his house seemed unusually warm, and I went over to the window to turn on his air conditioner. “Don’t bother,” he told me, “it’s not working.” Without air conditioning, my father‐​in‐​law and millions of elderly citizens just like him are at grave risk in this weather. After I write this column, I’m calling Sears and having a new machine delivered pronto. 



“This weather” has been in the news for over a week now. The last week in July is, statistically speaking, normally the hottest all year in the eastern United States. Although the news may be all atwitter about the temperatures, they’re actually pretty much what you’d expect on sunny days at the end of July. 



The average July high temperature along most of the East Coast from New York on down is around 90 degrees, with the southern portion a bit above and the northern a bit below. Sometimes a sea breeze gets into the Big Apple, but Philly, Baltimore, Washington, and the like are far enough inland that they simply bake. 



Given that the last week of July is the warmest in the month, temperatures in the lower 90s should be the rule, not the exception. But these “normal” values are composed of 30‐​year averages. Some days that form that average were sunny, some were cloudy, some had rain and some were in‐​between. It stands to reason that a bright sunny day is going to be warmer than the average–so that 95 degrees is pretty “normal” as long as it doesn’t cloud up. 



This is the threshold temperature at which elderly deaths begin to take off. And, true to form, we’ve seen the usual spate of stories trying to conflate this mortality, this summer’s temperatures and global warming caused by pernicious economic activity. 



Let’s get one thing straight. There is no warming trend in U.S. summer temperatures over the last 80 years. It did warm a bit from 1900 to 1930, but that change surely wasn’t because of a greenhouse effect; we hadn’t put much new carbon dioxide in the air by then. Further, current planetary temperatures measured by satellites and weather balloons are considerably below their average for the last two decades. 



In addition, heat‐​related mortality is going down. In 1995, Chicago saw several hundred deaths in a July heat wave. But there were 885 heat‐​related deaths in the Second City in 1955. Want to see true carnage? Go back to 1900, when 10,000 Americans perished in the heat. (The globe was one degree cooler then!) 



What’s the difference here? Two words: air conditioning. 



Air conditioners use more electricity than any other home appliance. On a hot day, they create such demand for electricity that, sometimes, the power fails. After this, the county coroner isn’t far around the corner. In fact, it was a power failure that magnified the 1995 Chicago tragedy. Normally in a heat wave, the poorer South Side experiences more deaths than the North Side. But a power outage in the affluent side of town resulted in a pretty equal distribution of fatalities across income classes. 



In this summer’s heat, Mayor Richard Daley has been exhorting citizens who feel they cannot afford to run their air conditioners to take advantage of a federal program designed to subsidize payments in just that eventuality. Somehow I do not believe that every 80 year old has gotten this message and fear that some will die today. 



Which brings us back to global warming. It should be self‐​evident that the very technology that enhances the greenhouse effect–the production of electricity–is what saves our lives in the heat of a normal summer. Thousands more would die, as did in 1900, without air conditioning in a world where the enhanced greenhouse effect and dreaded global warming did not exist. 



The risk of power failure can be averted by installing new generation capacity. But every time a new power plant is proposed, someone squawks “global warming.” When lack of power causes an outage on a hot day, that well‐​intended protest becomes a lethal weapon. 



Therefore, it is somewhat ironic that all proposals to fight global warming drastically raise the price of energy and power. The Kyoto Protocol on climate change requires us to reduce our emissions of greenhouse gases (read: use of energy) by 30 to 45 percent by 2008 compared with where we would be if we just went on as we are. If the price of electricity more than doubles (a likely scenario according to most experts), how many more of our elderly will hesitate to turn on the air conditioner until it is too late? The Kyoto Protocol is a killer.
"
"
Share this...FacebookTwitterRussian Prime Minister Vladimir Putin arrived Monday on Samoilovsky Island in the western Siberian Arctic to visit a joint Russian-German scientific expedition, Lena-2010.
Lena 2010 is conducting studies on the Russian Arctic permafrost, which is 1.5 km thick at the Samoilovsky Island location and estimated to be 40,000 years old.
The German page of Ria Novosti has a video up (sorry – only in German) which reveals an interesting comment, one not reported in the western media. The video shows Putin visiting the study site, and even helps the scientists bore into the permafrost.
Update: English video click here



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




At the 1:15 mark of the video, Putin is reported to have asked each scientist the same question:
Is climate change caused by man, or not?
To which the video gives the reply at the 1:24 mark, the reporter says (translated from German):

The scientists, however, could not give an exact answer. The heat wave in Russia resulted from a combination of factors.
Well that’s a long way from meaning “yes”. So much for consensus and the science being done.
When asked about plans in the Arctic, Putin explains at the 1:09 mark:
We are planning to extract natural resources from the ground in Western Siberia. We have to understand the entire eco-system and how it will respond.  
The Russians obviously are getting ready to mine and drill in the region.
Reuters presents an interesting politically correct view of Putin’s visit, and completely ignore the answer the scientists gave to Putin’s question. Read here: http://alertnet.org/thenews/newsdesk/LDE67M11O.htm
Share this...FacebookTwitter "
nan
"
Share this...FacebookTwitterSome of you will recall this pretty face we saw a couple of months ago.
We got to know her here, here and here. The IPCC based solar impact on climate on the “consensus of a single astronomer, who agreed with herself”.
In March, 2008, Dr. Lean made a presentation on global warming, see the following video link:
http://www.youtube.com/watch?v=OOMsQcEN1Bg
It kind of surprises me (and yet it doesn’t) that she doesn’t even mention water vapour as a greenhouse gas. She neglects to mention the PDO as well, looking only at the small timescale El Ninos and La Ninas.
She also completely ignores clouds as a factor effecting the temperature of the earth.
I guess the models are getting simpler and simpler (I mean the climate models).
Okay, it appears this video is not complete, and so I’ll hold back judgement.
Share this...FacebookTwitter "
"Most of Poland’s wild boars are being culled to stop the spread of African swine fever, a deadly viral disease that endangers farm pigs.  The Polish Hunting Association was mandated to organise large-scale hunts to kill more than 200,000 wild boars by the end of January, reducing its population by almost 90%. The government had been planning to erect a 1200 km-long fence along the eastern border of Poland to stop infected wild boars from moving westward, but the costly project was abandoned.  The war against African swine fever lifts some of the hunting restrictions, allowing licensed hunters to kill pregnant sows and organise hunts in national parks and nature reservations. These drastic measures have sparked protests. Even some of the hunters have refused to take part in the slaughter of wild boars.  Wildlife biologists also warn that mass hunts are not only pointless, but can even spread the deadly virus through human-mediated contact with infected blood. A letter from over 1000 scientists urges Polish government to seek out other solutions, highlighting the importance of following biosecurity guidelines.  So: why are wild boars scapegoated?  While the disease takes its toll on the wild boar population across Eastern Europe, the authorities are trying to prevent an epidemic in another species: domestic pigs. Due to high mortality rates, African swine fever poses high risk to the European pork industry. Every outbreak in pig farms is followed by gassing all livestock, a quarantine period, and harsh trade restrictions. It is the potential economic loss that drives the political war against this deadly disease, for which there is no vaccine. The current European epidemic started in 2007, when a ship containing infected pork meat entered the harbour of Poti in Georgia. From there, African swine fever spread in the Caucasus, to Chechnya and Russia. When the epidemic reached the eastern borders of the EU in 2014, with cases of infected wild boars in Lithuania, Poland, Latvia and Estonia, wild boars became an international concern. The virus initially occurred in Sub-Saharan Africa, where it affected wild pigs, bushpigs, and warthogs. It was first described in 1921, recording an  earlier diagnosis from 1910, when domestic pigs brought by European settlers became infected. African swine fever started to spread across British East Africa, mostly in what is now Kenya, with the colonial traffic in commodities.  The first outbreak in Europe was reported in 1957 in Lisbon, Portugal. By the 1990s, the Iberian Peninsula had suffered major economic losses caused by the disease decimating its swine population. Pork prices dropping drastically. Eventually, Portugal and Spain took radical steps towards eliminating the virus and achieved it by culling nearly all of their farm pigs and wild boars. This seems to be the model of containing the most recent outbreaks that the Polish authorities are following. But it’s not the appropriate response. The viral cycle in Africa and Iberia involves a soft tick that transmits the virus between wild and domestic pigs, but this particular type of parasite is absent Eastern Europe. Biologists are still searching for an insect mechanical vector of the disease in this geographical context.  But the virus can be also transmitted between swines through direct contact, bodily fluids, and ingestion of infected meat. That is why highly mobile wild boars are believed to carry the virus, and figure as the main culprits of the epidemic. The disease can be transmitted by infected boars, but also through their carcasses and contaminated meat, putting hunting practices and meat consumption under suspicion. The two incursions of the virus to Europe occurred through improper disposal of food waste. But even before the introduction of African swine fever to Europe, wild boars had a bad reputation. In rural areas, wild boars are notoriously blamed for crop damage. They have also become a more frequent sight in European cities, often causing human-wildlife conflicts as they raid garbage, and pose a risk of attacks or road collisions.  Since the 1990s, the wild boar population has been rising rapidly across Europe due to mild winters and industrial agriculture privileging energy-rich corn and soy production (the same crops that enjoy EU subsidies). These favourable conditions have even altered the reproductive patterns of the species, allowing for more frequent pregnancies and larger litters. Human-induced conditions, such as the elimination of natural predators, climate change and agricultural mono-cultures aided wild boars to thrive.  Meanwhile, scientists have linked the most recent cases of African swine fever (reported in the Czech Republic, Hungary, Belgium and China) to human rather than wildlife mobility. Such long-distance jumps in the distribution of the virus suggest that humans play a significant role in spreading African swine fever.  As the first large-scale hunts in Poland began in mid-January, anti-hunting activists claimed that hunters were carrying traces of blood on their shoes and cars, rendering the whole operation counterproductive and dangerous. Even though wild boars are typically considered invasive pests, many Poles have started to defend the swines against such drastic eradication plans. Activists are not only disrupting hunts, but also taking to the streets, where the wild boar is becoming a symbol of political resistance. Following earlier protests against logging in the Białowieża Primeval Forest, the environmental policy of the Polish government is increasingly coming under public scrutiny."
"

The U.S. Senate will almost surely approve permanent normal trade relations (PNTR) this summer, paving the way for President Clinton to cast a favorable U.S. vote for China’s entry to the World Trade Organization before the end of the year.



Both China and the United States will benefit from more Open trade. New wealth will be created as firms, investors and workers profit from their comparative advantages and as consumers are better served. Along with the increased volume of trade will be an increased demand for financial and other services.



Western banks and non‐​bank financial institutions stand to benefit handsomely from their specialized knowledge and technological advantages. And, since every market exchange is two‐​sided and requires consent, gains for Western firms imply gains for China. PNTR and WTO accession, therefore, are good for China and for its trading partners. 



If the PRC meets investors’ expectations about greater market access and more secure property rights, foreign investment in China will continue to expand, and the flow of new capital will create opportunities for millions of Chinese to increase their standard of living. The Chinese people will demand banking, investment, insurance and other financial services‐​from both foreign and domestic firms‐​and they will become increasingly intolerant of government policies that limit their investment choices and “cannibalize” their savings. 



The dismal condition of China’s state‐​owned banks, which Are burdened with the nonperforming loans made to state‐​owned enterprises (SOEs), cannot be improved without the discipline of foreign competition. Just as non‐​state industrial firms have been allowed to grow spontaneously, private banks and financial institutions must have the freedom to develop if China is to improve its allocation of scarce capital and achieve healthy long‐​term growth. 



What China needs is not “socialist capital markets” but Real capital markets with private owners who can specialize in Risk taking, who can freely capitalize expected future profits Into their present values by selling shares on organized stock exchanges and who are held fully responsible for losses. That means prices, including interest rates, must be determined by the free market, not by the Chinese Communist Party (CCP). 



If the mainland is to become a major player in the global economy, it must stand by its commitment to liberalize its financial sector upon accession to the WTO. That means, according to the Accession Agreement, that, within five years after China accedes to the WTO, U.S. and foreign banks should have full access to the local currency market: They should be able to deal directly with Chinese citizens and business firms in renminbi, they should be able to establish branches throughout China, and they should receive national treatment (i.e., the same treatment as domestic banks). 



Any backsliding from those commitments will send a signal to the global financial community that China is not to be trusted. Capital will leave the mainland and flow to places where it is protected by the rule of law. 



Granting China PNTR and bringing it into the WTO will encourage Beijing to conform to international norms and move closer to the rule of law. But no one should be under the illusion that the adjustment process will be easy or fast. 



Hong Kong and Taiwan can show China the way toward the free market and a more open society, and the Internet and international trade can show the Chinese people that freedom is valuable in its own right, in addition to being a means to greater wealth. But, in the end, the Chinese people themselves must determine the course of their nation by the choices they make. 



China’s financial future will depend on the steps that are taken in the next several years to restructure state‐​owned banks and enterprises. Allowing greater foreign competition will play an important role in transforming the financial landscape, but ultimately the only way to efficiently allocate capital and de‐​politicize investment decisions is to privatize banks and SOEs. 



The growing enthusiasm of Chinese President Jiang Zemin For equity markets should not be misconstrued. He and his supporters have not suddenly become capitalists. They simply recognize that tax revenues are not sufficient to bail out insolvent banks and firms. Oddly, they see equity markets as a socialist tool for raising capital to “revitalize” state‐​owned banks and SOEs. 



The plan is to allow some islands of private ownership in A sea of state ownership. The state would retain majority ownership and control of the key financial and industrial firms it puts on the market. Those who invest in such firms will have attenuated private property rights and trade in pseudo, not real, financial markets.



Nevertheless, getting China’s leaders to start thinking in terms of equity markets, talking about the need for more flexible interest rates and allowing some private ownership are steps in the right direction. 



Once West meets East in the global capital markets, things will start to change‐​perhaps faster than anyone can imagine. Stock exchanges in Shanghai and Shenzhen will be expanded to include index funds, Chinese citizens will eventually be allowed to move their savings into a portfolio of international investments and Chinese capitalists will be trading over the Internet 24 hours a day. The pace of change, of course, will depend on the extent of competition that the CCP allows and, hence, on the political climate. If the global market proves more powerful than the party, change will accelerate.



The reality is that China’s current financial markets are strictly limited, the renminbi is not convertible on the capital account, SOEs are crowding out the non‐​state sector in the quest for investment funds, and foreign banks and non‐​bank financial firms are still waiting patiently to enter the Middle Kingdom.



Those constraints on capital freedom have led to rampant corruption. Below‐​market pricing of loans by state‐​owned banks has created opportunities for side payments as a means of deciding who gets the scarce state funds; exchange and capital controls have led to attempts to circumvent the law by bribery and favors. The success of those attempts is revealed by the fact that, from 1991 through 1998, more than US$100 billion illegally left the mainland for safe havens in Hong Kong and elsewhere. That figure shows up in the errors and omissions component of China’s balance of payments, according to Dong Fu, an economist at the Federal Reserve Bank of Dallas, and amounted to nearly 40 percent of total foreign direct investment in China during that period.



If China wants capital to freely come and stay, it must be free to leave. More importantly, Beijing must establish sound constitutional protection for private investors. That is the challenge for the next decade. By granting China PNTR and by allowing it to enter the WTO, the U.S. Congress will help China meet that challenge and help create real, not pseudo, financial markets for the future.
"
"
Map from the University of Alabama-Huntsville. Each contour represents 0.2 degree C per decade warming or cooling between Dec. 1979 and Nov. 2008
From the USA Today Weather Blog
This has been in my inbox for a couple of weeks, so on a  fairly quiet day for weather, I thought I’d put this out there. John Christy of the  University of Alabama-Huntsville reported earlier this month that the  Earth’s climate change over the past 30 years has been rather uneven: It’s  gotten much warmer in the Arctic and, at the same time, cooler in the  Antarctic.
Christy and his colleague Roy Spencer, who are known in some quarters as global  warming skeptics, use data from satellites to measure the temperature of the  Earth. The more well-known NASA GISS and  National Climatic Data  Center data sets primarily measure surface temperatures.
Overall, Christy found that Earth’s atmosphere warmed an average of about  about 0.72 degree F in the past 30 years, according to NOAA and NASA satellites.  More than 80 percent of the globe warmed by some amount. However, while parts of  the Arctic have warmed by as much as 4.6 degrees F in 30 years, Christy says  that much of the Antarctic has cooled, with parts of the continent cooling as  much as the Arctic has warmed (see map, above; click to enlarge).
“If you look at the 30-year graph of month-to-month temperature anomalies,  the most obvious feature is the series of warmer-than-normal months that  followed the major El  Nino Pacific Ocean warming event of 1997-1998,” says Christy. “Right now we  are coming out of one La Nina Pacific Ocean cooling event and we might be  heading into another. It should be interesting over the next several years to  see whether the post La Nina climate ‘re-sets’ to the cooler seasonal norms we  saw before 1997 or the warmer levels seen since then,” he says.
He adds that most of the warming found in the satellite data has taken place  since the beginning of the 1997-98 El Nino, and that Earth’s average temperature  showed no detectable warming from December 1978 until the 1997 El Nino.
Meanwhile, the Washington  Post reported yesterday that the USA “faces the possibility of much more  rapid climate change by the end of the century than previous studies have  suggested, according to a report led by the U.S. Geological Survey.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a221b04',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Since it was completed 100 years ago, the Panama Canal has been the only shipping route through the land mass of the Americas. Controlled by the US for most of its history, it allows ships to navigate between Pacific and Atlantic oceans without having to sail all the way to the tip of South America, through the infamous Magellan Strait. This makes it one of the world’s most important economic arteries.  But on December 22, work began on a new canal route in the region, in what is one of the most ambitious infrastructure projects of our time. The Interoceanic Grand Canal of Nicaragua will take five years to build over some 278 miles, traversing several major rivers and also Lake Nicaragua, the largest freshwater reserve in Central America. Having been discussed for some 200 years, it is expected to cost $50bn (£32bn), nearly five times the country’s annual GDP.  The project resulted from the Nicaraguan national assembly agreeing a 50-year concession with the Chinese company Hong Kong Nicaragua Development (HKND) – with the potential for a 50-year renewal thereafter. The canal will allow the passage of the world’s largest ships, some of which will be too big for the Panama Canal even after its current expansion project has completed.   The project is supported by Daniel Ortega, the Nicaraguan president. Ortega is the former commander of the Sandinista revolution and head of state in the 1980s, who returned to the helm after the 2006 election and has had a history of difficult relations with the US. His party, the Sandinista National Liberation Front (FSLN), has become an authoritarian political machine that is particularly effective at controlling the population. It is difficult to get a job in government without belonging to the FSLN, and hard to have any power without being close to the Ortega clan.  The president’s son, Laureano Ortega, handled the negotiations with Wang Jing, the chairman/CEO of HKND, on the conditions of the concession. The project is managed directly by the president. The canal budget does not appear in the draft government budget, and the finance minister himself has complained about the lack of information around project costs.  There are also a number of important questions about the project that have never been answered. We know that HKND is a sprawling Beijing-based operation with 15 subsidiaries, but despite strenuous efforts, the Nicaraguan media has so far failed to find out where its leader’s personal fortune comes from. Wang Jing has announced that he has gathered enough investors for the project, but despite several public presentations and insistent questions, the details about project finance remain unknown.  There has been speculation about the links between Wang Jing and the Chinese military. It is seen by some as the main provider of canal project funds, even though the project is officially private and China has made no official statement. The nation’s growing commercial interests in Latin America certainly make it easy to imagine that it might like to have an alternative to the Panama Canal, now run by a Panamanian government agency.  The US has not made any statement about the project either. Different specialists who I have interviewed have variously argued that the US is waiting to see if the project is serious; is too busy elsewhere, notably in the Middle East; and has not taken the measure of the project and its possible consequences. Whatever the case, the silence is surprising.  The environmental lobby has been more forthcoming. One of the most active has been the Centro Alexander von Humboldt, which has released the only environmental impact report to date. It has said that the impact of construction through the lake would be irreversible, affecting the nearly one million people who depend on it for drinking water. Dredging the lake at a depth of more than 30m, displacing millions of tons of sediment, could also radically alter and potentially destroy the biodiversity of the lake. Also controversial is the work that is currently starting on two deep-water ports on either side of the country and a highway to transport equipment and concrete-manufacturing plants. The environmental impact study for these parts of the project, by the English company ERM, is not due to complete until April 2015. The Grand Canal Act meanwhile authorises HKND to expropriate any land in Nicaragua for the needs of the project. It will displace thousands of peasants and indigenous peoples. Yet unlike the Panama project, there was no referendum on whether it should go ahead. In the face of the government’s promises about taking the country “out of poverty” and providing thousands of jobs, farmers’ property rights seem to weigh little. The farmers likely to be affected by land expropriation have been holding demonstrations in numerous localities since last September. They have been told nothing about how and under what conditions this will happen. In El Tule in the western part of the country, farmers have been blocking the Managua-San Carlos highway for several days to prevent the entry of Chinese workers and vehicles of the Nicaraguan army. With the Chinese beginning to install their first settlements, protesters have been complaining about the increasing militarisation that is accompanying the project.  Neither has anything been said about how the country will manage the future economic impact of the canal. I have heard concerns among Nicaraguan economists that the country could become little more than an enclave economy with little in common with the rest of the region. In short, it is long overdue that the spotlight be shone properly on this huge project. The environmental, social and geopolitical implications could cast a shadow over the region for decades to come."
"
Share this...FacebookTwitterThe Science Skeptical Blog here has released some fascinating results from Hans von Storch’s Klimazwiebel blogsite’s survey here. Although the survey is not representative, 578 valid participants from 28 countries took part and interesting results have been produced. Here are the most interesting results:
1. Climate science skeptics have been involved in the topic for a long time. More than 50% of the participants have been following the topic 5 years or more. And skeptics are not born as skeptics. Skeptical Science writes:
More than 75% of the participants started off as neutral or even alarmist. That shows the longer one looks at the climate issue, the more skeptical one becomes with regards to alarmism.
2. Skepticism is deepening. Skeptical Science blog writes:
The last two years have been predominated by the debates on the 2007 IPCC Report, failure in climate negotiations at the international level, and Climategate. These have served to deepen skepticism.
3. Skeptics are shown to be competent. More than 2/3 of the participants have a scientific or technical education.
4. Skeptics like to quote highly competent experts, by a wide margin they like citing  Steven McIntyre and Richard Lindzen.
5. Skeptics characterised three blogs (Realclimate, Skeptical Science, Klimalounge) as alarmist. Meanwhile Klimazwiebel and Lucias Blackboard were viewed as moderate or neutral.
6. Skeptics do read the alarmist blogs mentioned in no.5, and thus are informed of both sides of the issue. Both “neutral” blogs were viewed very positively.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




7. WUWT and CA were the sites skeptics preferred to visit.
8. German skeptics liked visiting EIKE, WUWT and Klimazwiebel the most. [Next time they ought to put NoTricksZone on the ballot, I’d clean all their clocks out. :)].
Now the following are the summary results from the Klimazwiebel website, in case you’re interested:
Q1: The reason for being a skeptic.
2/3 are skeptic because they find that knowledge about the earth’s climate system would be insufficient for legitimating mitigation measures.
Q2: How long engaged/interested in climatic issues?
25% of the respondents became interested after the hot news issue of IPCC 2007. Most layman are no longer than 10 years, and the skeptical scientists are generally engaged for a longer time.
Q.3: Initial opinion upon first contact with climatic issues?
There is a clear warmist (38%)/”neutral” tendency.
Q.4 Which experience had respondents upon having asked their first critical questions?
Two of the six possible answers were clearly on top of the votes:
– The answer was an attempt to promote a political point of view (35%)
– The answer showed limited competence of the other side.
Q.5 How did attendants get to skepticism?
Internet resources was the most ticked choice in this multiple-options question (63%). The hockey stick discussion also represents a major factor. Both of these are clearly less a factor for skeptical climate scientists (Internet 27%); for these scientific publications are an important factor (up to 69%).
Q.6 What is the tendency, related to the past two years?
A vast majority (74%) tends clearly towards skepticism in this time scale. Attendants from web pages as eike-klima-energie.eu and nelson.blogspot as well as oekowatch.org are ticked around 83% (or even 100%).
—————————————————————————————————
Copyright reminder: It is not allowed to reproduce this post without first obtaining permission from No Tricks Zone. Other sites and media may cut and paste max. 25% of the content, and then followed by a link to this site. Let’s all be fair and let credit go where credit is due. Thanks!
Share this...FacebookTwitter "
"
More anecdotal weather news of a colder and more brutal winter.

From the Juneau Empire, Juneau Alaska
Bitter cold moves in to Interior – Temperatures could drop to 50 below zero in parts  of Alaska
Meanwhile, in other news: Roofs collapsing due to record snows in Spokane, WA

FAIRBANKS – Bitterly cold weather slid over from Canada and settled into  Interior Alaska with forecasters saying temperatures could continue to slide to  nearly 50 degrees below zero in coming days.
Over the weekend, the mercury at Fairbanks International Airport dropped to  39 degrees below zero. Areas in the Interior outside the city were even colder;  46 below on the Yukon Flats, 41 below in Fort Yukon and 44 below in Central,  according to the weather service.
Rick Thoman, lead forecaster at the National Weather Service office in  Fairbanks, said temperatures rose a few degrees on Sunday, but that was it.
“The temperature will probably continue to go up and down randomly,” he said.  “With no clouds and no wind on the valley floor, temperatures are pretty much  probably going to be stuck.”
Fairbanks had experienced a relatively mild winter prior to Christmas. It had  only dropped to 30 below once, in early December.
The howling winds and frigid weather were too much for several mushers,  including four-time Iditarod winner Jeff King and his dog team, who pulled out  of the Gin Gin 200, a 200-mile race along the Denali Highway.
For the men, Brent Sass came in first, ahead of more well-known mushers such  as four-time Yukon Quest and two-time Iditarod champion Lance Mackey who was  fourth.
Mackey, resting Monday at the lodge in Paxson, said it was blowing so hard  and the teams were getting so turned around by the wind that it almost made him  laugh.
“It was almost comical. Your sled was going sideways down the road,” he said.
Further down the trail, when temperatures dipped to 50 below, it wasn’t so  much fun, he said.
“There were a lot of people not wanting to put their teams through that,”  Mackey said. “It is all about the dogs in a situation like this… They get  hardened by this stuff. That is why we do it.”
For the women, defending champion Jodi Bailey of Chatanika came in first.
Several mushers pulled out of the race from Paxson to the MacLaren River  Lodge.
“It was a real challenge this year,” Bailey wrote on a friend’s Facebook  page. “Winds like a banshee, and killer cold, wow am I glad to be back in  Paxson!!!”
According to the race Web site, temperatures at the MacLaren River Lodge were  between 35 and 40 below. It was reportedly 10 to 15 degrees colder on the lower  portions of the trail during the second portion of the race.
In Southeast Alaska, at least 20 inches of snow fell in Ketchikan, forcing  the shut down of the Ketchikan International Airport for a few hours. The  airport shut down at about 1:30 p.m. Sunday due to the heavy snow.
“We’ll stay here all night and dig out,” airport manager Mike Carney said.
The airport reopened Monday and normal operations resumed.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99e844af',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"A French ski resort has angered ecologists by using a helicopter to move snow from higher up the mountains after exceptionally mild weather left its slopes bare. Officials at Luchon-Superbagnères in the Pyrenees authorised the “exceptional” emergency operation overnight on Friday.  The helicopter spent two hours transporting 50 tonnes of snow to drop on the lower slopes used by beginners and ski schools. Hervé Pounau, the director of the local department council, said the cost of the operation would be recouped many times over by the business that would have been lost to a lack of snow. “It will cost us between €5,000 and €6,000, in the knowledge that over the long term we will get at least 10 times’ return on that investment,” Pounau said in a statement. Keeping the station open safeguarded 50 to 80 jobs, including lift operators, ski school teachers, childminders, ski equipment rental shop staff and restaurant owners, he added. “We’re not going to cover the entire ski station in snow, but without it we would have had to close a huge part of the ski domain, and it’s during the holidays that we have the most activity for beginners and the ski schools,” Pounau said. He admitted it was not “very ecological”, but added: “It’s really exceptional and we won’t be doing it again. This time we didn’t have a choice.” The operation has angered French ecologists. Bastien Ho, the secretary of Europe Écologie Les Verts party, said the snow transfer operation was evidence of an “upside-down world”. “Instead of adapting to global warming we’re going to end up with a double problem: something that costs a lot of energy, that contributes heavily to global warming and that in addition is only for an elite group of people who can afford it. It is the world upside down,” he told French television. The February-March half term holidays in France – known as the “winter sport holidays” – are staggered over four weeks across different regions and are the busiest time of the year for the country’s mountain resorts. Luchon-Superbagnères depends on this period for 60% of its income, but exceptionally mild weather has forced the resort to close all but six of its 28 slopes. It is the first time helicopters have been used to transport snow from higher altitudes to lower resorts in the French Pyrenees, though similar operations have been carried out in the Alps. Local officials said the snow would guarantee that beginners could enjoy the lower slopes and ski instructors could continue with classes for the next two weeks. • This article was amended on 19 February 2020. An earlier version said that the February-March half term holidays in France are staggered over six weeks. This should have said four weeks; while it is correct that three different areas of France take two weeks of holiday each, the weeks overlap. "
"
Share this...FacebookTwitterThe German Freie Welt has a short piece by Fabian Heintel called Lost in Translation. It takes a look at how the sceptical International Climate Science Coalition gets treated by Wikipedia.
The English Wikipedia describes the organisation as:
 …an international association of scientists, economists and energy and policy experts
That sounds respectable and authoritative enough, but too much so for the German language Wikipedia.
In climate dissent-intolerant Germany, sceptics are to be viewed as flake amateurs who have little scientific authority. Here’s how the German Wikipedia describes the International Climate Science Coalition .
…Zusammenschluss von Ökonomen, Politikern und anderen Personen
Translated in English that’s:
association of economists, politicians and other persons. 
Sounds kind of ragtag, doesn’t it?
Here’s a list of the members of the international organisation. You make the call if the German Wikipedia description fits.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterEd Caryl likes to research, and a short time ago wrote an essay about phytoplankton, see here. Some readers pointed out a flaw, and so Ed has insisted on posting a correction – as is appropriate in science. Happens to the best of us. (We’re not Penn State or CRU here).
===========================================================================
The Phytoplankton are not Starving
By Ed Caryl
In a comment to the original article, The Phytoplankton are Starving, R. de Hann made the following comment:
 So I have big trouble accepting the loss of plankton for fact.
In regard to the claim of over-fishing and fishing methods (by other reports), which is a serious problem in several places, we see the gap of lost volume filled up with other species very quickly.
We know for example that tuna eat jellyfish and in those waters where the tuna numbers have been reduced the numbers of jellyfish have exploded, compensating for the “loss of mass”.
So one species is quickly replaced by another.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Willis Eschenbach made this comment:
Humans catch and remove about 70 million tonnes of mostly big fish from the ocean annually. But the amount of mass at each trophic level reduces by about 90%. Tuna are four trophic levels up from phytoplankton. This means that for every kilo (pound) of fish that we eat, there are about 10,000 kilos (pounds) of phytoplankton that are supporting that fish.
That means that we are removing something on the order of one ten-thousandth (0.01%) of the nutrients that the phytoplankton that fed those tuna depend on …
However, that’s not all. Much of the phytoplankton goes to feed things that are generally not eaten by humans. As a result, the reduction in phytoplankton nutrients is even smaller than 0.01%. Much of it is never seen by humans in any form.
As a result, your hypothesis (reduction in plankton results from human usage of the required nutrients) fails by a number of orders of magnitude. We simply don’t remove enough nutrients to make a difference.
These comments forced me to re-think my position. I went back to the literature.
The total autotrophic biomass production in the oceans is about 48 billion tons carbon per year. We are currently harvesting about 80 million tons of fish and shellfish per year. The harvest has averaged over 70 million tons per year for the last 30 years, and over 40 million tons per year for the 30 years before that. Over the last 60 years we have harvested about 3.5 billion tons. The total biomass production in the ocean is circulating production. The fish we pull out removes that mass from circulation. So the reduction in phytoplankton nutrients is in fact small. Willis is correct; this doesn’t explain the 40% reduction in phytoplankton. As Willis correctly states, the food web in the oceans has several trophic levels, each about 10% of the level below it.
So thanks to R. de Hann and Willis Eschenbach for their comments.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterLast Saturday polar bear poster child Knut died unexpectedly, much to the horror of viewers and fans internationally. Knut was only 4 years old. Polar bears normally live to be more than 30.
Of course, everyone is asking why. What was the cause of death?
Media reports say Knut had some sort of brain ailment. Activists are blaming it all on the Berlin Zoo.
German tabloid BILD claims that offcial cause of death was drowning. But Wolfgang Röhl at German blog achgut.com doesn’t buy it. He writes:
Please – since when do polar bears drown? No, there has got to be something else behind all of this. Professor Stefan Rahmstorf of Potsdam Institute for Climate Impact Research, you’re on!”
Yes, the “experts” can clear up all of this – with an authoritive peer-reviewed paper, of course.  The consensus is there after all; the science is done. Everybody today knows what every (bad) thing is caused by.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterA Swiss report appearing at www.20min.ch claims that the extraction of groundwater for communities and agriculture is adding massively to global sea level rise.
According to a yet to be published paper by the America Geophysical Union, 2000 cubic km of freshwater are consumed yearly – with 1500 cubic km coming from lakes, ponds, rivers etc., and the remaining 500 cubic kilometers of freshwater being extracted from the ground. According to 20min.ch:
Scientists now have calculated how much of this extracted water returns from where it came: only 3%. The remaining 97% flows into the oceans via evaporation and precipitation.
Of course, the scientists say, this bodes extremely ill for groundwater levels and pose a serious threat to arid regions.
Adding to sea level rise
Moreover the AGU paper claims that the extraction of freshwater is contributing to an annual 0.8 mm rise in sea level, 0r more than 25% of the overall 3.1 mm/yr. It goes without saying that there are uncertainties involved here, as dams act to slow the entry of freshwater into the oceans. But 20min.ch writes:

Scientists are convinced that the relation between groundwater reduction and sea level rise will gain importance. Simply because global water consumption will continue to increase and for that reason groundwater reserves will continue to sink.

Uncertainty always allows alarmists to be convinced of anything they claim. Sceptic Edgar L Gärtner at eigentümlich freicomments in a piece called: Climate Lies: How geo-scientists are abandoning their expertise. Leaky faucets are causing the oceans to overflow, writes:
It can be certain that pumping out groundwater has no measureable impacts on sea levels. But perhaps US scientists are tying it to climate change in order to bring attention to the risks of exhausting groundwater reserves.
Drawing attention to potential groundwater depletion problems is legitimate. But claiming it is leading to sea level rise sounds absurd.
Share this...FacebookTwitter "
nan
"The government could ban the sale of petrol and diesel cars in 2032, three years earlier than previously suggested, the transport secretary has said. A consultation launched last week suggested all cars with internal combustion engines could be banned from 2035 but Grant Shapps told BBC radio on Wednesday the ban could come within 12 years. The ban would happen by 2035 – or even 2032, subject to consultation, he said. The comments will add to the concerns of the car industry, which has criticised the government for “moving the goalposts” with its earlier announcement that the limits could come by 2035, from an initial expectation of a 2040 ban. Mike Hawes, the chief executive of the Society of Motor Manufacturers and Traders (SMMT), said it was a “date without a plan”. Previous proposals also excluded some hybrid cars, which combine internal combustion engines with battery electric power. Hybrids are seen by carmakers as a key way of reducing the carbon footprint of their vehicles, and multiple companies such as Toyota and the Mercedes-Benz owner, Daimler, have bet heavily on hybrids to avoid steep EU fines. Shapps and Andrea Leadsom, the business secretary, will meet the SMMT on Thursday, with the lobby group hoping to persuade the government that the industry will need more help to achieve the targets. Among the main concerns for carmakers are incentives such as grants for electric cars as well as building national infrastructure capable of charging millions of vehicles. During the third quarter of 2019, the Department for Transport recorded 22,596 registrations of ultra-low emission vehicles, which includes electric cars – 39% more than the same period in the previous year. However, that still represented only 3.1% of new car registrations during the period. European manufacturers are launching a wave of electric-only models to try to reduce the emissions of the cars they sell and meet the EU targets. Volkswagen, the world’s largest carmaker by volume, plans to sell 1m electric cars by the end of 2023."
"Nine years ago, Britain generated nearly 75% of its electricity using natural gas and coal. In 2018, this dropped to under 45% – a remarkable transition away from fossil fuels in under a decade.  As energy efficiency improved, demand fell, and the UK generated less electricity than at any point since 1994. Our own analysis below looks at the past year, using similar data for Great Britain (as Northern Ireland has a separate power system), and we include net imports from France, the Netherlands and Ireland as an overall part of electrical generation. Here are a few things we found:  In 2018, Britain was coal-free for a record 1,898 hours – that’s up from just 200 hours in 2016. Coal generation fell for the sixth year in a row, and the country now has substantial periods without coal power (the longest stretch was just over three days straight). For comparison, the 5% of electricity generated from coal was a broadly similar level to the combined total of solar and hydro (see table at end of the article). Wind increased its output to 17% of the total, and combined with solar these two renewables generated more electricity than nuclear – another significant milestone. However, low levels of coal generation averaged across the year mask its importance at times when the electrical demand is particularly high. For example, over the week of the Beast from the East cold snap in February 2018, the gas system experienced significant stress and coal stepped in to provide nearly a quarter of Britain’s electricity. As coal generation is set to be phased out by 2025, the electrical system needs to continue to find alternative power sources to cope during extreme weather events.  Our analysis shows that annual renewable generation has increased by 27 terawatt hours (TWh) over the three years since 2015. This is particularly impressive considering the Hinkley Point C nuclear plant will produce a similar annual amount of electricity but will take three times as long to build (from contract signing). But what about the decade ahead? Could Britain repeat its success since 2010 and reduce its coal and natural gas generation by a further 30 percentage points? Under this scenario, the country would then generate just a sixth of its electricity from fossil fuels.  It’s definitely possible, but the next decade will be more challenging for two main reasons: the demand for electricity is expected to rise rather than fall, and incorporating ever greater levels of variable renewable generation will need additional flexibility. To achieve this, new renewable generation – new solar panels, new turbines, new hydro, tidal, marine and biomass generation – will have to replace an estimated 100 TWh per year (about four Hinkley Point Cs) from fossil fuels. That would require a build programme that was broadly 50% greater than the previous nine years.  Given the continued development of offshore wind in particular, this seems challenging but achievable. Solar and wind prices keep falling, which will help. Indeed, the UK’s business and energy secretary Greg Clarke recently said that “it is looking likely that by the mid 2020s, green power will be the cheapest power. It can be zero subsidy”. However, at some point over the next decade, electrical demand will stop falling as electric vehicles gain market share from fossil fuel vehicles, and electrical heating for homes becomes more popular. As an indication of the scale of the transport demand, in 2017 UK cars and taxis travelled 254 billion miles. If all those journeys were taken in electric vehicles about as efficient as the latest Hyundai or Tesla then total electrical demand would increase by a quarter (over 80 TWh).  These vehicles would need the equivalent of three Hinkley Point Cs to charge them over the year. This is also a similar level to current generation from renewables. The UK also needs to consider how to fill the gap that would be lost from fuel duty, which is forecast to raise around £28 billion this financial year. If charging these vehicles adds to electrical demand at peak times, there would be substantial new infrastructure costs (more pylons, stronger electrical sub-stations). If Britain adopts a smarter system, fleets of electric vehicles could provide network support by changing their times of charging or even providing electricity back to the grid. This could provide a massive new form of flexibility that is needed to accommodate greater levels of weather dependent renewable generation. This is not an easy task, though, and needs better communication between vehicle, owner and power companies. Overall, 2018 saw steady progress for low carbon generation, including record months for wind, biomass and, mid-heatwave, solar: Looking to 2019, with more renewable capacity being installed, it is possible that solar could overtake coal, and renewables could generate more than nuclear for every single month. They could also generate more than coal and gas combined over a month for the first ever time. If any of these do happen, it will be yet another indication of the speed at which Britain’s electricity system is changing.  The electrical generation data is from Elexon and National Grid. Data from other analyses (such as BEIS or DUKES) will differ due to methodologies and additional data, particularly by including combined heat and power, and other on-site generation which is not monitored by Elexon and National Grid.
Renewables in this analysis = wind + solar + hydro + biomass. "
"Despite overwhelming public opposition and a longstanding ban, fox hunting shows no signs of abating in the UK. The 2018 hunt season alone saw 550 reports of illegal hunting, though these figures only represent known incidents.  In 2014 it was found that 250,000 fox hunters attended Boxing Day hunts across the UK. In 2019, so far, at least 21 foxes have been killed by the hunt and 151 incidents of illegal hunting have been reported since the season began on November 1. The Hunting Act, which prohibited hunting foxes and wild mammals with dogs, was approved by the UK’s parliament in 2003 with 362 MPs in favour and 156 against. The following year it became law. In 2017, the British people were surveyed on whether they continue to support the ban on fox hunting and the result was resounding – the highest margin ever recorded on the matter - 85% thought fox hunting should remain prohibited. So if the ban is entering its 15th year, why is fox hunting still happening? This question is answered in the Hunting Act itself, particularly the manner in which it “outlaws” fox hunting. Article 1 states that a “person commits an offence if he hunts a wild mammal with a dog”. But the provision continues: “Unless his hunting is exempt.” Herein lies the deceit of the Hunting Act, for it lists a total of nine reasons a hunt may flout the general ban. One of the more commonly invoked exemptions maintains that it is legal to hunt foxes if they pose a danger to livestock, game, crops or fisheries. As such, fox hunting advocates would have us believe that Roald Dahl’s tale of Fantastic Mr Fox and his endeavours to outwit farmers is all too common a curse in rural communities.  This remains nothing more than a smokescreen to defy the ban. Research has shown that foxes naturally control rabbit populations that if left unchecked, would cause significant economic harm to farmers. The UK government’s Department for Environment, Food and Rural Affairs (DEFRA) also advises against controlling foxes, and instead favours strengthening protection around livestock to guard against natural predation. Another commonly used exemption exploits a loophole around flushing foxes out to help birds of prey hunt. This has seen fox hunters disguising their true intentions by taking birds of prey along with them without ever letting them loose.  There is also the dubious practise of “manufactured” trail hunting in which hounds are supposed to follow an artificial scent trail with no animal chased or killed. In reality, hunt organisers use actual fox scent and lay routes deliberately close to where foxes are known to live, meaning they quickly become the subject of a hunt. Trail hunting is again an attempt to hide the true intentions of those that wish to continue fox hunting. Monitoring and gathering accurate information on all this to help prosecute offenders is a dangerous task, with members of the public often exposed to insults, intimidation and threats from hunters. The inadequate Hunting Act and the nefarious practises of hunt organisers mean fox hunting endures in England and Wales. Scotland too, offers no refuge for foxes and the Protection of Wild Mammals Act 2002 provides similar loopholes that allow hunting to continue. Setting aside the cruelty of fox hunting, evidence from the Breeding Bird Survey suggests red fox numbers have declined by 41% since 1995. Introducing a complete hunting ban is more essential than ever to protect the UK’s foxes. The Hunting Act has humans as its focus by specifying how people can bend the law’s provisions to their circumstances. Despite its prevalence in much of environmental law, this human-centric idea is entirely the wrong approach. Any future legislative efforts need to place foxes, and other mammals, at the centre of legislation. Foxes must be protected for their own right, and a blanket ban on hunting, absent any exemptions, is the only way to safeguard populations. Severe penalties must also be included, to ensure that those already willing to flout the law will rethink their actions. The likelihood of such a move materialising during this parliament is slim, however. Prime Minister Theresa May offered a free vote to repeal the Hunting Act during the 2017 election but withdrew the pledge after her disastrous election result. It’s essential that campaigns for stronger anti-hunting laws highlight how widespread resistance to diluting the ban is. The failures of the existing ban endanger foxes and betray the wishes of a majority of the public. Any update to the Hunting Act must crack down on those who think they are above the law."
nan
"Amid the many hellish images and stories that have emerged from Australia’s apocalyptic wildfire season, the fate of the country’s wine business will have come some way down the list of most observers’ concerns. That’s only as it should be. When we are talking about the loss of human and animal life and habitats – when we are seemingly witnessing extreme predictions for a distant post-climate emergency future happening right now – fretting about the viability of a branch of the booze business can seem trivial.  As the country counts the costs of the conflagration, however, concerns about the long-term viability of what has become one of its most significant exports have come into focus. According to Wine Australia, about 1% (or 1,500 hectares) of vines were in the heart of the fire-affected zones, with the worst-hit region being the Adelaide Hills. That’s the site of some of Australia’s most exciting cooler-climate wines, and it lost as much as 30% of its vineyards, including Henschke’s renowned Lenswood vineyard, and land and buildings belonging to the cult star producer, Vinteloper. Similar soul-searching has been taking place in California, where October’s Kincade fire burned through Sonoma County, the latest in a series of increasingly devastating wildfires to engulf California wine country. In the past two years, we’ve also seen prime vineyard land in Chile, South Africa, the south of France, Portugal, Spain and Greece go up in flames with more intensity and regularity than before. It’s not just the expensive loss of vines and infrastructure. There are after-effects stored in any grapes that might be salvaged at vintage time, even on vines that were some distance from the frontline. Once they’ve been through the veraison process (when the grapes change colour), the closer the grapes are to being ready for harvest the more they are prone to “smoke taint”, with the absorption of smoke particles leading to flavours akin to a chemical fire in the finished wine. This problem has been the subject of considerable research in Australia and California – although so far, it seems, the only action available is to ameliorate rather than eliminate the aromas. By allowing less contact between juice and skins, by fining and filtering, and by adding distracting flavours such as oak and tannins, you can produce a wine where the effect of the smoke taint is dialled down. None of those options are attractive for winemakers, and many simply take the expensive decision to dump the crop. No wonder they are weighing up whether wildfires have gone beyond occupational hazard to become a threat to their viability. For now, smoke-tainted wines are relatively rare. And, as with many symptoms of the climate emergency, there’s still just about time to act rather than merely accepting that burnt has to be the taste of the new normal. Henschke Giles Pinot Noir Adelaide Hills, Australia 2017 (from £31.80 vinvm.co.uk; ozwines.co.uk)The Henschkes are best known for superlative shiraz from the Eden Valley, but, before it was badly damaged in the 2019 bushfires, their Lenswood vineyard had earned a reputation for luxuriously silky, bright red berry-fruited pinot noir. Vinteloper If Life Gives You Lagrein Langhorne Creek, Australia 2017 (from £14.80, fieldandfawcett.co.uk; thewhiskyexchange.com)Vinteloper’s David Bowley had his entire 30-hectare Adelaide Hills property destroyed by bushfire. Buying a bottle of this thirst-quenchingly tangy red from a plot of the north Italian variety lagrein in Langhorne Creek is a great way to help him stay in business. Casa de Mouraz Branco Dão, Portugal 2017 (£17, highburyvintners.co.uk)The team at Casa de Mouraz, one of the best producers in central Portugal’s Dão region, had only just harvested the grapes for this typically luminous, textured, pithy dry white before wildfire swept through its vineyards and winery in October 2017. Mayacamas Chardonnay Mt Veeder, Napa Valley, California, USA 2017 (£55, robersonwine.com)California has had to get used to wildfires: last year, the Kincade fire in Sonoma; in 2017, fires swept through Napa destroying buildings at Mayacamas Vineyards, but leaving vines, including those that produce one of the world’s great chardonnays, intact. Château de Caraguilhes Corbières, France 2017 (£10.98, Waitrose)In 50 years’ time, will winemakers still be able to produce such spicy, robust, brambly fruited reds as this from southern France’s Corbières, a region that, having already endured fire damage in 2016, lost 300 hectares of vineyard in wildfires in July last year? Vergelegen Premium Chardonnay Stellenbosch, South Africa 2017 (from £10.89, rannochscott.co.uk; ampswinemerchants.co.uk; robertsandspeight.co.uk)Another wildfire survivor: as much as 40% of the 300-year-old Cape estate Vergelegen was damaged in South Africa’s last big conflagration in 2017. The chardonnay from that vintage was not affected: it’s as rich yet balanced as ever. 
This article contains affiliate links, which means we may earn a small commission if a reader clicks through and
makes a purchase. All our journalism is independent and is in no way influenced by any advertiser or commercial initiative.
By clicking on an affiliate link, you accept that third-party cookies will be set.
More information.
"
"
Share this...FacebookTwitterlist Game Indonesia buat Permainan Judi Online! waktu kamu mencari trik judi yg lebih baru. Permainan judi Indonesia pantas dipertimbangkan buat kamu. Berikut yaitu sekian banyak trick kamu sanggup menikmati perjudian di negeri ini bersama sekian banyak trick inovatif.
bersama perkembangan tehnologi ketika ini permainan game online jadi ternama. lantaran dgn main dengan cara online menciptakan kamu lebih nyaman & enteng utk memainkannya. Terutama dalam permainan judi online, permainan game online paling baik yg mampu menghibur kamu. Bahkan permainan judi online serta sanggup berikan kamu pendapatan penambahan.
Dalam permainan judi online ketika ini amat sangat beraneka ragam yg dapat kamu mainkan. Berikut ini ada sekian banyak permainan game judi online yg dapat kamu mainkan :
Bandarq
Ini ialah permainan judi Indonesia yg terkenal & serta dinamakan bersama permainan judi card. dalam permainan bandarq dibutuhkan enam card domino ganda dalam set dua puluh delapan. Dalam biasanya kasus, pemain memakai card mungil namun dikala mereka aus, mereka dibuang. Pemain mesti membayar jumlah yg terus sebelum mulai sejak main-main. Mereka mendapati tiga card domino buat menolong mereka disaat mereka dalam perbaikan.
kamu bebas bertaruh, lipat, naik atau dingin sesudah card kamu dievaluasi. bila tak ada petaruh pada awal mulanya, kamu bakal diizinkan utk bertaruh. tidak cuma itu, apabila permainan cuma melibatkan kamu adalah( cuma satu petaruh), itu berhenti sekaligus. kamu memperoleh pot dalam kasus itu tidak dengan mesti menunjukkan card kamu. Ada dua putaran yg dikenakan batas dalam permainan. Babak ke-2 mempunyai batas yg lebih tinggi.
tidak cuma itu, card ditempatkan berpasangan di mana seluruhnya pasangan ditambahkan. Pasangan paling atas buat tiap-tiap kontes merupakan 9 sedangkan pasangan kedua dianggap sbg yg terakhir. Pemain mengaplikasikan beraneka cara juga matematika buat menang. satu orang pemain diyakini mempunyai qiu sesudah tiga ganda awal.
Poker Online di Indonesia
Poker online kurang dibatasi di Indonesia di bandingkan dgn permainan tradisional. Pemain bisa membawa pertolongan dari web website poker internasional agung utk mencapai maksud mereka. terkecuali itu, mereka sedia di negara-negara dgn yurisdiksi pemerintah Indonesia yg lebih sedikit.
Poker internasional dgn pemain Indonesia tak diizinkan. namun para pemain bebas utk menikmati beragam situasi seperti permainan duit nyata & pertandingan. dgn begitu bermacam permainan dimainkan dengan cara online yg terjangkau.
Dominoqq
cocok pengamatan, perjudian online di negeri ini konsisten meningkat sejauh itu jadi sumber pendapatan bagi tidak sedikit orang. Dominoqq terdiri dari sekian banyak permainan termasuk juga Judi Bola (perjudian bola), permainan tradisional, permainan video poker & sebagainya.
Apa yg mesti kamu tonton dalam Permainan Judi Online?
https://semogaqq.net/
Mengamankan Uang
seluruhnya pemain sesudah membuahkan duit paling tidak sedikit dalam masa terpendek. telah terang bahwa transaksi aman mempermudah pemain buat menjaga duit mereka konsisten aman. pula masalah yg terkait dgn setoran & penarikan dihindari dgn ini.
tiap-tiap pemain mendapati akun pribadi di mana dirinya mampu mengecek penghasilannya. pula disediakan bermacam akun deposito maka para pemain sanggup bertransaksi duit dari akun yg sah mereka.
tapi dalam permainan judi online seperti bandarq, dominoqq & poker kamu butuh mengetahui & pilih web judi online terpercaya buat menopang jalannya permainan yg kamu mainkan diwaktu ini.
trik utk meraih website judi terpercaya kamu mesti menonton profil web tersebut. Atau kamu pun sanggup menyaksikan anggota yg bergabung di dalam website tersebut.
Nah, itulah rekomendasi dari artikel kami berkenaan list game judi online indonesia & trick buat pilih website. mudah-mudahan dgn adanya info ini bisa menopang kamu dalam permainan nantinya. Selamat main-main & mudah-mudahan berhasil!
Share this...FacebookTwitter "
"
The hits just keep on coming. 1,672,437 page views this month, up from 1,478,801 page views in March.

After posting last months stats, there was some discontent by some angry and somewhat incredulous bloggers that it might be an April Fools joke of some sorts. Sorry, no such luck.
But what was humorous, was one particular blogger (Joe Romm) who said:
It is absurd to publish one’s page views to 7 significant digits without caveats — even 2 is stretching it. 
I got a huge chuckle out of that. So just to show that the stats are indeed real, and accurate down to that 7th digit (since they come from the WordPress internal traffic counter), I’m expanding this month’s report.
Joe, this report’s for you. Here is a screencap showing my WordPress internal report page, with all 7 digits, sans caveats. No caveats are needed since the WordPress numbers are actual, not estimated, and not drawn from router statistics.
click for a larger image
Maybe Joe will reciprocate and post a screencap his own internal stats page.
The graph on this page above differs a bit from the headline graph (which is graphically edited to move the title inwards and downwards to fit nicely on the page) because it was snapped after 00GMT (5PM PST) and it shows the new month of May stats also, which are of course quite low by comparison.
So the numbers are real, from WordPress.com (where my blog is hosted) and exact to the 7th digit, despite angry accusations otherwise.
In that blog post, Joe Romm also showed an Alexa graph to prove his point, saying:
Interestingly, there is one independent source that suggests Watts’ page views and mine are in fact the same (and hence possibly around 1.4 million).  If you go to the Web traffic ranking and comparison site Alexa, go to page views, and type in wattsupwiththat.com, you’ll get this graph:

So at best I am just negating the disinformation Watts is spreading.  Sigh.  And lest there be any doubt, WattsUpWithThat is in fact an extremist anti-scientific denialist website, as his recent posts make clear.
Gosh, all that angry prose from a traffic report? I don’t much care whether WUWT beats Climate Progress in traffic or vice versa, but I do care when the proprietor suggests that by printing my own internal stats page I’m spreading disinformation. Of course, I don’t expect an apology, but I wanted WUWT readers to know.
OK, following Joe’s lead, lets look at Alexa this month.
Pageviews: we are still about the same in April according to Alexa.

Though over the longer 6 month term, WUWT is slow and steady while Climate Progress runs hot and cold.

Joe didn’t show the other interesting comparative statistics from Alexa though, so I thought WUWT readers might enjoy seeing them.
Here is Traffic Rank over 6 months:

And here is Reach:

But having been labeled a “spreader of disinformation”, please, don’t take my word for it, visit Alexa and see for yourselves. Here is the link.
Some people might wonder why I post my web stats each month. Some might say I’m tooting my own horn, well maybe a bit, I’m admittedly proud of WUWT.
But there’s a bigger reason. WUWT has thousands of visitors, many of whom are regulars who see this blog as something they relate to and grow with. Many visit daily or even more often. There’s a sense of pride and ownership with all of you, too. Without its readers, content contributors, and those tireless volunteer moderators that keep the snark under control, WUWT would be nothing. I try not to lose sight of that. This is my monthly reminder to myself that this is a team effort.
As this blog expands its reach, it seems only fair that my readers also get to share in seeing the growth. I listen to my readers, who are often more insightful than I am, and they offer wonderful suggestions and topics.
I thank you all for making WUWT one of top science blogs on the Internet today.
And thanks to you to Joe, for bringing up this topic last month.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e963b5251',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Now that Newt Gingrich has vaulted into the lead in the Republican presidential stakes, he’s going to be seeing a lot more of this:





Calling it the “dumbest thing” he has done in recent years isn’t enough. This is not going away.



But, Newt can turn his pas‐​de‐​deux on the loveseat with Nancy into an opportunity to promote a consistent and electable political message.



Because he will be asked about this at every turn, Newt will have the air time and the opportunity to tell the truth on climate change — based upon real science — and to interface it with a limited‐​government philosophy. The two are easy to do.



Start with the obvious. Instead of waffling on the subject, he could just point out that earth’s surface temperature is about 1°C warmer than it was a century ago. There were two periods of warming, the first from about 1910 through 1945, and the second from the mid‐​1970s to the late‐​1990s. They were both roughly the same magnitude. Because the first one was long before we put the majority of fossil carbon into the atmosphere, Newt can say — with scientific authority — that the magnitude of ‘natural’ climate change is likely to be at least as large as what humans have done.



Then Newt should proceed to the future. It’s not the heat, it’s the sensitivity. How much it will warm in the future is a function of how sensitive the atmosphere is to doubling atmospheric carbon dioxide. There are a number of independent arguments now coming together showing that this value may have been overestimated. The reference for the most recent is a November issue of _Science_.



Next, a little refreshing honesty: in 2009, the House of Representatives passed a cap‐​and‐​trade bill (which the Senate did not) commanding that the average american — 38 years from now — be allowed the carbon dioxide emissions of the average citizen in 1867. Prior to its passage, Newt was undeniably for such a system. Newt could call that that the “second dumbest thing” he has done and be finished with it.



After setting the record straight, how about interjecting his own political philosophy? When asked what to do about global warming, he should give the honest answer: if the atmosphere indeed is not as sensitive to carbon dioxide changes as previously thought, the correct response is _nothing._



That’s because “nothing” really means something. Newt should channel his historian. When markets are free, capital supports innovation more efficiently than when they aren’t. Think about the remarkable changes in energy and technology in the last 100 years. Isn’t it rather obvious that the same will occur in the next century, if only we don’t hinder capital development? Newt might even use the catchy saw (first sloganed by Northern Illinois Gas in 1972) _the future belongs to the efficient_ , and that there are impressive market forces that advantage those who produce things efficiently or produce efficient things.



When the doomsayers say Newt is believing in a _Deus ex machine_ to dramatically cut carbon dioxide emissions, he might point out that those were the same folks who, only a very few years ago, told us we were running out of natural gas. Innovation and capital revolutionized drilling and fracturing shale, and we now know we have literally hundreds of years of it under our feet. There are a lot more voters and delegates around the country who will benefit from the shale revolution than there are in ethanol‐​addicted Iowa.



Thus comes Newt’s opportunity to back down his support of corn‐​based ethanol. It’s a loser. When it powers a car, its total life cycle results in more carbon dioxide emissions than simply sticking with gasoline.



Yes, if he ran away from ethanol before the Iowa primary, Romney might win. But Newt would be called courageous and the voters in the rest of the nation will notice.



Finally, Newt could say that he’s sick of the government subsidizing any form of energy or transportation. Sell GM and don’t look back. Stop subsidizing the Volt, solar energy, wind power, gas, coal, all things energy, and let people keep their money and invest. Stop tilting the federal purse at windmills.



Newt can do these things and win. Otherwise, he’s could spend fall of 2012 on that darned couch.
"
"
Share this...FacebookTwitterAs usual, there have been more horror reports in the media today about the Fukushima crisis in the German media. German Zettels Raum blog brings us back to reality today.
Leading the way with the horror stories was Der Spiegel, which had the headline here at its online site:
Increased Radioactivity – Nuclear Power Plant Fukushima completely evacuated”.
So terrible has the media been with reporting on the nuclear crisis in Japan that Zettels Raum was compelled to write:
You always have to remember that when dealing with the media, journalistic research is something comparable to a chimpanzee dealing with the question of how one can prove the existence of anti-matter.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




It’s that bad. While media outlets like Der Spiegel propagated information distorted into horror stories, here’s what Japanese NHK said of the situation, 7:13 pm Japan time (emphasis added).
TEPCO: Black smoke rises from No.3 reactor
The Tokyo Electric Power Company, or TEPCO, says black smoke was seen rising from the No.3 reactor building at the quake-damaged Fukushima Daiichi nuclear plant at around 4:20 PM on Wednesday. TEPCO told reporters that it received a report 1 hour later that the smoke had gradually cleared.
The company said that the level of radiation near the main gate of the plant, 1 kilometer west of the No.3 reactor, was 265.1-microsieverts-per-hour at 5 PM. They added there had been no major change in the levels after the smoke was observed. On Monday afternoon, gray smoke was seen rising from the same reactor building. TEPCO said that the plumes turned white before disappearing.
The power company evacuated workers from the control room of the No. 3 reactor, as well as firefighters from Tokyo and Yokohama preparing for a water-spraying operation. The firefighters had to abandon their planned water spraying operation for the day.
The evacuation took place at only one of the control rooms of the 6 reactors. Click here to see a graphic on radiation levels at the plant. Here you can see that radiation levels are trending down.
Share this...FacebookTwitter "
"

Since its inception in 1908, the Federal Bureau of Investigation has kept itself busy surveilling those who hold controversial political views — from Christian pacifists in World War I, to Martin Luther King Jr. in the 1950s, to Arab/​Muslim Americans in the 1990s. “American Big Brother,” a new project from the Cato Institute, features an interactive online timeline of these surveillance projects over the past 100 years. “The theme that emerges clearly from the timeline’s episodes is that in many of these cases, federal surveillance and political repression were directed most forcefully at individuals and organizations that challenged the prevailing political paradigm on the issue at hand,” wrote Cato policy analyst Patrick G. Eddington. And in many cases, the individuals and organizations subjected to this warrantless surveillance suffered irreparable damage to their personal and professional lives. The timeline, found at www​.cato​.org/​a​m​e​r​i​c​a​n​-​b​i​g​-​b​r​other, already features dozens of stories of surveillance, and is an ongoing project which will be updated regularly with archival research and new developments in the news.



 **The Battle for Free Expression, Continued**  
 _The Tyranny of Silence_ , the story of how one cartoon ignited a global debate over free speech, was first published two years ago. Since then, the battle for free speech has raged on—from the tragic _Charlie Hebdo_ attacks in Paris, to the fight for free speech on public campuses. Now available for the first time in paperback, Flemming Rose’s book, which The Economist dubbed one of the best books of 2014, recounts his experience publishing cartoons of the prophet Muhammad in Danish newspaper _Jyllands‐​Posten_ in 2005, which quickly exploded into a global controversy known as the “Cartoon Crisis.” Rose bravely defended the decision to print the 12 drawings, even as Muslims around the world protested, Danish embassies came under attack, and newspaper and magazine editors were arrested. Rose tells his gripping personal story of the events that unfolded. “What do you do when suddenly the entire world is on your back?” Rose recalls. The paperback edition includes a new afterword, in which Rose reflects on the _Charlie Hebdo_ attack and the state of free speech in both Europe and America. The United States, he writes, is “afflicted with identity politics and grievance fundamentalism,” while in Europe, “it looks like freedom of speech will be sacrificed on the altar of cultural, religious, and ethnic diversity.”



 **The Economics of Environmentalism**  
More than 10 years after its original publication, the second edition of Richard Stroup’s invaluable _Eco‐​nomics: What Everyone Should Know about Economics and the Environment_ provides a thoroughly updated guide to environmental problems from a free market perspective. As in the first edition, Stroup offers a concise primer of how economic principles shed light on environmental issues, and why so many environmental laws fail. But Stroup also adds new chapters, including a brief overview of the history of environmentalism in the United States, the “constantly changing view of our environment and how to protect it,” and an examination of the most controversial environmental issue of today—climate change. “Although the book is a small one, I have attempted to identify in it the core tenets of free‐​market approaches to environmental protection and to make clear why these approaches are worth serious consideration,” writes Stroup. “The weight of opinion tends to push toward a greater role for government, even though that role is often misused and sometimes has unfortunate consequences. Economics shows us the wisdom of considering a greater role for market solutions.”
"
"
I thought about writing a year end recap, but then I saw my traffic count for the month at 00 GMT (4PM PST), and thought that would do just as well at telling the story for this year. After a slight dip in October and November, WUWT has reached a new high at nearly 900,000 page views this month.

Click for a larger image
Not bad for a 12 month growth. My hit counter, as of this writing, stands at:
6,840,995 hits 
In December 2007, I hadn’t even broken 500,000.

Thanks to each and every one of you for visiting, contributing, and commenting. Thanks especially to the moderating team who keeps the temperature of this blog down whilst I think up new topics.
Here were the top 7 most popular posts in 2008, in case you missed them:
Top Posts
January 2008 – 4 sources say “globally cooler” in the past 12 months 140,090 views
A look at temperature anomalies for all 4 global metrics: Part 1 64,508 views
Where have all the sunspots gone? 59,144 views
Sudan hit by Apollo Asteroid 36,543 views
UAH: Global Temperature Dives in May 35,521 views
Solar Cycle 24 has officially started 34,877 views
Arctic sea ice back to its previous level, bears safe; film at 11 27,091 views


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9992e40d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Nothing has been more embarrassing to the administration’s global warming apocalyptics than a few satellites, first launched in 1978, that resolutely refuse to find any warming of the lower atmosphere, which all the administration’s modelers and all the administration’s men predict should be heating with reckless abandon. 



When the satellite data were first published eight years ago by Roy Spencer and John Christy, of NASA and the University of Alabama respectively, the record was only 10 years long; the global warmers chanted in unison that the record was too short and therefore only a blip. Steve Schneider, who was the federal guru of climate change (he’s now with population apocalyptic Paul Ehrlich at Stanford), told _Science_ that “the next ten years will tell the story.”



They have, but the administration hasn’t listened. The story is that there’s a slight (but statistically significant) _cooling_ __ trend in the satellite data. It’s not surprising — but it _is_ scientifically dismaying — that the administration wants that trend to stop. So when the White House held its big global warming show last fall, up popped the leader of one of the nation’s most prominent environmental organizations (hint: it’s the one that destroyed the nuclear power business, thereby causing the greenhouse increase to begin with!) to declaim, “We have got to _do something_ about the satellite.” 



Climate watchers have been wondering how long it would take the $2.1 billion the federal government spends each year on global change research to “do something.” It did on February 23, when a California rocket scientist sent a manuscript to _Nature_ magazine, claiming he had found the error in the satellite data and that the atmosphere was actually warming up after all. (Sorry, can’t mention the author. _Nature_ has a hard‐​and‐​fast rule about blabbing to the press, and they’d have to pull the paper.)



The scientist calculated how much the solar wind — a stream of high‐​energy particles that exerts a slight force on everything in the solar system — would slow the satellites. In slowing down, the satellites fall a bit toward the earth’s surface, and they “see” a smaller area from which to take their measurements. To the satellites, a reduced area would appear colder than it really is. 



The satellite data could show a spurious cooling trend that exactly matched the balloon temperatures only if there were an exactly similar bias in the balloon readings. That’s simply not the case.



Adjusting for that induces a compensating warming in the satellite data of about 0.12 degrees (C) per decade. When that warming is coupled with the currently observed cooling in the record (0.04 degrees per decade), the result is a slight warming trend of 0.08 degrees every 10 years. That is still way below where it’s supposed to be (the computer models that served as the basis for the Kyoto global warming treaty predicted about 0.35 degrees per decade by now), but at least it’s a warming. 



The rocket scientist then sent his results not only to _Nature_ but to everyone else of scientific note who carries the administration’s precipitation. If you don’t believe there’s a cabal in the science community cheering for apocalyptic global warming, you ought to see the e‐​mail list. The only ones on it who weren’t cheerleaders happen to be Messrs. Spencer and Christy, whose satellite was gored. 



Included on the list were the Big Agency science administrators, who dutifully made sure Al Gore got a copy. He was reportedly ecstatic. And _Nature_ itself, whose editorial stance increasingly resembles the vice president’s view, helped things along ASAP. Send a paper to _Nature_ showing that global warming may not be such a big deal, and they’ll take six months to review it before, in all probability, sending a terse rejection letter. But this one got turned around in 10 days! 



Balance need not apply. Spencer and Christy asked the rocket scientist if they could publish a companion response but were told no, it would take too long. What’s the rush?



The rush is that the apocalyptics want to drop this news on the public like they did the “Ozone Hole over Kennebunkport” in 1992. That’s when Gore, still in the Senate, trumpeted some NASA chemical data indicating that a Northern Hemisphere ozone hole was imminent (the late winter ozone depletions are largely confined to Antarctica for good physical reasons). By stampeding the Senate into panic, Gore rammed through a 99‐​to‐​1 vote for an accelerated ban on certain industrial refrigerants. Only two weeks later, NASA had data showing that their initial pronouncement was a gross exaggeration, and the ozone hole never appeared. NASA wasn’t forthcoming until after the Senate acted.



If Spencer and Christy are allowed to respond concurrently, they will blow the paper to kingdom come. That’s because the satellite temperatures match up perfectly with a totally independent measure, taken twice a day by weather balloons as they ascend through the lower atmosphere. The satellite data could show a spurious cooling trend that exactly matched the balloon temperatures only if there were an exactly similar bias in the balloon readings.



That’s simply not the case. Or did thousands of folks launching millions of weather balloons over the last 20 years somehow decide to fudge the numbers so they would match up perfectly with those of satellites that didn’t show global warming? Talk about a massive conspiracy of right‐​wing airheads!
"
nan
"

Seven years ago, on March 13, 1993, the eastern third of the United States was pasted by a big low‐​pressure system that was (erroneously) called the “Storm of the Century.” It dumped two inches of snow on the Gulf Coast beaches of Florida and two feet of snow in a wide swath from southwestern Virginia through New York and was followed by record cold. The next day, Kevin Trenberth, a federal climatologist with the U.S. National Center for Atmospheric Research, went on _Meet the Press_ to say that global warming, El Niño, and the 1991 Mount Pinatubo volcano were all involved. Never mind that Pinatubo caused a well‐​known global cooling. 



A look out today’s window reveals a remarkably different picture of blue skies, daffodils and softball practice. 



Given the choice, which Ides of March do you prefer? 



The fact is that we have just completed the warmest winter in the 105‐​year record for the continental United States, and few are complaining. But because it is good news, the same federal climatologists who glibly pulled the greenhouse effect trigger on the 1993 snowstorm can’t bring themselves to tell why we are so happy: global warming. Instead, they subsequently trotted out a scare story about how the warm winter is causing a drought. 



Even the most rudimentary statistical analysis of winter temperatures reveals that our coldness is determined by the number and severity of incursions of miserable, deadly, frigid air, minted in northwestern Canada, Alaska, and Siberia and shipped south. Winters in which there is little of this activity, such as the totally delightful ones of 1931–32 (87 degrees Fahrenheit in Roanoke, Virginia, on February 11, 1932) or 1999–2000, tend to record above‐​average temperatures. 



Federal climatologists could have looked it up in the January 24 issue of _Climate Research,_ which proved that the greenhouse warming in the last half of the 20th century is largely confined to those same cold air masses. That is predicted by greenhouse effect theory but rarely mentioned in the rush to gloom. In the Northern Hemisphere winter half‐​year, which itself contains over two‐​thirds of the surface temperature warming, these frigid‐​air killers of the homeless are warming at a rate 10 times greater than the average warming everywhere else! The paper proved the greenhouse effect was the finger on this trigger by showing that the more cold air there is, the more it warms up. 



Further proof, ironically, is given by satellite and weather balloon data that show no warming (after allowing for the now‐​departed 1998 El Niño) since their records became concurrent in 1979. These instruments are best at measuring the atmospheric slice from roughly 5,000 to 30,000 feet, but the cold Siberian and North American air is usually more shallow than that. 



Because people saving the planet do not brook embarrassment with decorum, please don’t ask them whether this disparity between the surface and lower‐​atmospheric temperature was forecast by the climate models cited as evidence for a global‐​warming disaster. 



Droughts are caused by deficits in water balance as evaporation exceeds precipitation. In the winter, over most of the nation, temperatures and the sun’s angle are so low that there’s very little evaporation, which is why, even in a dry January, most of the soils in the eastern half of the nation are saturated. The recent warm winter was only three degrees above the long‐​term average, or the average temperature difference between, say, March 5 and March 15. Evaporation rates remained low because the mean temperature is still way below the summer peak. So warm winter or not, winter temperatures do not cause droughts. 



Federal behavior is not hallmarked with consistency. If global warming is caused by burning of fossil fuels (it probably is), and if it is a terrible problem (I’ll bet not), then the only way it can be stopped is by raising energy prices through the roof. How high? Today’s $2.00 per gallon gasoline hasn’t dented consumption enough to cool the mean temperature of the planet 1/1,000 of a degree, even if spread over a year. In other words, a two‐​term Gore administration at that price would produce a change in temperature of less than 1/100 of a degree. But with the softballs flying by my window on this fine March day, who on earth would want to do such a thing anyway?
"
"

Mayan ruins in Guatemala.

This is an email I recently received from statistician Dr. Richard Mackey who writes:
The following appeared on Gore’s blog of Nov 19, 2008:
Looking Back to Look Forward
Looking  Back to Look Forward November 19, 2008 : 3:04 PM

A new study suggests the  Mayan civilization might have collapsed due to environmental  disasters:
These models suggest that as ecosystems were destroyed  by mismanagement or were transformed by global climatic shifts,  the depletion of agricultural and wild foods eventually contributed to  the failure of the Maya sociopolitical system,’ writes  environmental archaeologist Kitty Emery of the Florida Museum of Natural  History in the current Human Ecology journal. 
As we move  towards solving the climate crisis, we need to remember the consequences to  civilizations that refused to take environmental concerns  seriously.
If you haven’t read already read it, take a look at  Jared Diamond’s book, Collapse.”
This is a most curious  reference.
It means that Gore is advocating the abandonment of the IPCC  doctrine and barracking for the study and understanding of climate  dynamics that ignores totally the IPCC/AWG doctrine and focuses on all  the other variables, especially how climate dynamics are driven  by atmospheric/oceanic oscillations, the natural internal dynamics of  the climate system and the role of the Sun in climate dynamics.
Brian  Fagan in Floods, Famines and Emperors  El Nino and the fate of civilisations   Basic Books 1999, shows that the Maya collapse, whilst having complex  political, sociological, technological and ecological factors, was largely  driven by the natural atmospheric/oceanic oscillations of ENSO and NAO.  The  book is one of three by Brian Fagan, Prof of Anthropology UC Santa Barbara,  that documents how natural climate variations, ultimately driven by solar  activity, have given rise to the catastrophic collapse of civilisations.  The  book has a chapter on the Mayan civilisation which collapsed around 800  to 900 AD.
Here are some quotes from his book:
“The “Classic  Maya collapse” is one of the great controversies of
archaeology, but there is  little doubt that droughts, fuelled in part
by El Nino, played an important  role.”
“The droughts that afflicted the Maya in the eighth and  ninth
centuries resulted from complex, still little understood  atmosphere-
ocean interactions, including El Nino events and major decadal  shifts
in the North Atlantic Oscillation, as well as two or three  decade-long
variations in rainfall over many centuries.”
“Why did the  Maya civilisation suddenly come apart?  Everyone who
studies the Classic Maya  collapse agrees that it was brought on by a
combination of ecological,  political, and sociological factors.”
“When the great droughts of the  eighth and ninth centuries came, Maya
civilisation everywhere was under  increasing stress.”
“The drought was the final straw.”
“The  collapse did not come without turmoil and war.”
Brian Fagan describes  how the ruling class (the kings had divine powers, they were also shamans and  there was a vast aristocracy and their fellow-travellers that the tightly  regulated workers toiled to maintain) encouraged population growth beyond  what the land could carry; how the rulers enforced rigid farming practices  which were supposed to increase food production and the ruler’s incomes but  had the effect of undermining farm productivity and diminishing  the quality of the poor soils of the area.  When there were heavy  rains the soil was washed away.  In times of drought the soil blew  away.
More quotes from Brian Fagan:
“The Maya collapse is a  cautionary tale in the dangers of using
technology and people power to expand  the carrying capacity of
tropical environments.”
“Atmospheric  circulation changes far from the Maya homeland delivered
the coup de grace to  rulers no longer able to control their own
destinies because they had  exhausted their environmental options in an
endless quest for power and  prestige.”
Gore says that we should use our understanding of the Maya  collapse help us solve the climate crisis, noting that “we need to remember  the consequences to civilizations that refused to take  environmental
concerns seriously”.
Given what we know of the Maya  collapse, what is Gore really saying?
He is saying that we should take  all the IPCC/AWG publications and related papers to the tip and bury them  there and put all our efforts into the study and understanding of the reasons  for climate dynamics that address every theory except that of IPCC/AWG  doctrine.
Specifically, we should understand as well as we can how  climate dynamics are driven by atmospheric/oceanic oscillations, the  natural internal dynamics of the climate system and the role of the Sun  in climate dynamics.
In an overview of his work Brian Fagan  concluded:  “The whole course of civilisation … may be seen as a process of  trading up on the scale of vulnerability”.  (Fagan (2004, page  xv)).
We are now, as a global community, very high up on that  scale.
Allow me to quote a little from my Rhodes Fairbridge paper  because of its relevance to Brian Fagan’s work and what Gore is really trying  to say, but can’t quite find the right words.
(My paper is here: http://www.griffith.edu.au/conference/ics2007/pdf/ICS176.pdf ).
“In his many publications (for example, NORTH (2005)), Douglass  North stresses that if the issues with which we are concerned, such  as global warming and the global commons, belong in a world of  continuous change (that is, a non-ergodic world), then we face a set of  problems that become exceedingly complex.  North stresses that our capacity  to deal effectively with uncertainty is essential to our succeeding in  a
non-ergodic world.  History shows that regional effects of  climate change are highly variable and that the pattern of change is  highly variable.  An extremely cold (or hot) year can be followed  by extremely hot (or cold) year.  Warming and cooling will be  beneficial for some regions and catastrophic for others.  Brian Fagan  has documented in detail relationships between the large-scale  and
generally periodic changes in climate and the rise and fall  of civilisations, cultures and societies since the dawn of history.   The thesis to which Rhodes Fairbridge devoted much of his life is that  the
sun, through its relationships with the solar system, is  largely responsible for these changes and that we are now on the cusp of  one of the major changes that feature in the planet’s history.   As
Douglass North showed, the main responsibility of governments  in managing the impact of the potentially catastrophic events that  arise in a non-ergodic world is to mange society’s response to them so as  to
enable the society to adapt as efficiently as possible to them.
Amongst  other things, this would mean being better able to anticipate and manage our  response to climate change, to minimise suffering and maximise benefits and  the efficiency of our adaptation to a climate that is ever-changing –  sometimes catastrophically – but generally predictable within bounds of  uncertainty that statisticians can estimate.  At the very least, this  requires that the scientific community acts on the wise counsel of Rhodes W  Fairbridge and presents governments with advice that has regard to the entire  field of planetary-lunar-solar dynamics, including gravitational  dynamics.
This field has to be understood so that the dynamics of  terrestrial climate can be understood.
References:
North, D. C.,  2005. Understanding the Process of Economic Change
Princeton University  Press.
Fagan, B., 2004.  The Long Summer.  How Climate Changed  Civilization.
Basic Books.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a885ba4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Boris Johnson must put the climate crisis at the top of his government’s agenda if crunch UN talks this year are to be a success, leading international figures have told the Guardian. Alok Sharma was appointed on Thursday as the business secretary and president of Cop26, the UN talks on the climate crisis to be held this November in Glasgow. Some climate experts are concerned that he won’t be able to stand up to governments reluctant to make strong commitments to cut greenhouse gases, while at the same time supporting British businesses struggling in the turmoil of Brexit.  Lord Stern, former World Bank chief economist, said he had confidence in Sharma, having worked with him closely on international development issues. “[Sharma] has worked with developing countries, and has shown he can build up the relationships he needs,” he said. “He has the knowledge and the ability to form the relationships that are needed with developing countries that will be essential to the success of Cop26.” The appointment of Sharma has encouraged civil servants who were concerned that Cop26 had slipped down the government’s agenda. Insiders said the appointment of Sharma – a well-regarded secretary of state for international development – would kickstart the process of international diplomacy needed to make the Cop26 climate conference a success. Fatih Birol, executive director of the International Energy Agency, said: “This is an excellent choice and we at the IEA are looking forward to working together with minister Sharma and his team to support the Cop26 process in order to reach a successful outcome in Glasgow for our international climate goals. We will work together to make sure 2019 is remembered as the year of peak emissions so that the 2020s become the decade that global emissions decline quickly.” Campaigners are concerned, however, that the dual role may get in the way of a successful outcome. John Sauven, executive director of Greenpeace UK, said: “The new business secretary has a herculean task on his hands. Not only has he got to get the UK delivering on its carbon budgets, but he’s also got to get the world behind more ambitious climate action. “At home, he needs to unblock the expansion of on- and offshore wind currently sandwiched between various government departments and backbench MPs, as well as tackling carbon emission from 30m draughty buildings. Globally, he’s got to deal with leaving the EU and signing international trade deals while also cajoling countries to get behind solving the climate emergency. This will only work if Boris Johnson steps in and puts the full weight of No 10 behind it.” Aaron Kiely, climate campaigner at Friends of the Earth, said: “It’s hard to be optimistic about the UK’s presidency of the UN climate conference, when the man given the job is expected to do it as a side project. Leading international talks on the climate crisis is easily the most important role in government right now, yet Alok Sharma is expected to take it on at the same time as being business secretary. If the presidency is being treated as a part time gig then what next? Perhaps the conference venue will be expected to share space with a wedding fair.” He added: “The government really needs to start taking this seriously. At the very least this means giving someone the organisation role as a full-time job, and proper cooperation with the Scottish government.”"
"

ASOS station in Greenville, Alabama, Mac Crenshaw Memorial Airport
Photo by: Howard Wiseman
You’d think either the FAA or NOAA would get this cleaned up before surfcaestations.org volunteers have the opportunity to get such pictures. So, in the spirit of the “FAIL blog” I thought I’d offer this.
To be fair, this looks like a junker placed there rather than a aviation accident. I’ve seen many such aircraft at small airports that become forgotten derelicts like this.
Here is a view sans aircraft.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ab7e661',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Despite rising federal deficits, Congress is set to pass another budget‐​busting spending bill. This time it is a $19 billion package of disaster‐​related subsidies.   
  
  
The _Washington Post_ reports “taxpayer spending on U.S. disaster fund explodes.” It documents increases in disaster spending by the Federal Emergency Management Agency (FEMA). In a typical recent year, “spending on the federal disaster relief fund is almost 10 times higher than it was three decades ago, even after adjusting for inflation.”   
  
  
The story identifies two causes of the spending increases: climate change and population growth in disaster‐​prone areas. But it ignored perhaps the most important cause: increased federal intervention in the sorts of emergencies that used to be handled by the states, as I discuss here.   
  
  
The _Post_ is correct that more Americans are moving into disaster‐​prone areas:   




Many more Americans have moved into harm’s way, with growth exploding in the Gulf Coast region and along the Continental Divide, where tornadoes frequently occur, according to a study on the “expanding bull’s eye effect” by Stephen M. Strader of Villanova University and Walker S. Ashley of Northern Illinois University.   
  
  
Since 1970, 35 million more people and their homes have moved to coastal shoreline “in the direct path of potentially devastating storm surges,” the researchers found, a 40 percent increase.   
  
  
“We’ve put more stuff in the wrong place the wrong way,” said W. Craig Fugate, a former FEMA administrator under President Barack Obama. “We’ve got a lot more stuff — bigger houses, multiple cars, more people — in high‐​hazard areas.”



More people are also living in fire‐​prone areas of California.   
  
  
The _Post_ does not explore an important reason why Americans are moving into these areas: government subsidies. Federal subsidies for flood insurance, flood control structures, beach replenishment, and disaster rebuilding have encouraged development in coastal areas, as I discuss here. Meanwhile, state policies have contributed to building in California’s fire‐​prone areas.   
  
  
American governments are not alone in pursuing policies that increase disaster hazards. A World Bank / United Nations study identified such policies in numerous countries and discussed market‐​based reforms to mitigate risks.   
  
  
In the United States, federalism is supposed to undergird our system of handling disasters, particularly natural disasters. Under the 1988 Stafford Act, the federal government is supposed to get involved in disasters only if they are of “such severity and magnitude that effective response is beyond the capabilities of the state and the affected local governments.”   
  
  
However, presidents and congresses have increasingly ignored this limit. The number of presidential disaster declarations has soared and the costs of disaster bills have increased as politicians shoe‐​horn subsidies unrelated to immediate emergency response into bills.   
  
  
Growing federal intervention is undermining the role of the states and private institutions in handling disasters. This intervention stems from politics not practical benefits. State and local governments and the private sector are better positioned to handle most disaster response. Also, states, cities, and private utilities aid each other during disasters.   
  
  
Rising FEMA spending is not a good metric for measuring the severity of natural disasters striking the United States. Rather, it reflects growing populations living in risky areas and growing disregard for federalism in disaster‐​related response and rebuilding.   



"
"
Newly discovered evidence that polar bears, CO2, climate change, and the sun are intimately connected in ways never envisioned.
No wonder the sun seems to be slowing down.

With apologies to the French, and everybody else for that matter.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9bfbf49f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
NASA’s updated data appears to suggest the annual rate of global polar ice loss  has actually decreased
 
Greenland’s Riviera – their green southwest. Will another Maunder minimum
grip the region in cages of ice again, or will bells ring in the portside squares,
as they did in the 1300’s before that cooling came, and ships sailed the fiords?
(Source: NASA)
Excerpt:
Washington Post correspondant Juliet Eilperin, in her 12-26-08 report entitled “New climate change estimates more pessimistic,” dutifully surveys the latest bleak findings of the climate change community. Her primary source is a recently released survey comissioned by the U.S. Climate Change Science Program – expanding on the findings of the 2007 4th IPPC Report on Climate Change. Apparently this “new assessment suggests that earlier projections may have underestimated the climatic shifts that could take place by 2100.” One of Eilperin’s primary examples of alarming new data is reported as follows:
“In one of the reports most worrisome findings, the agency estimates that in light of recent ice sheet melting, global sea level rise could be as much as 4 feet by 2100. The IPCC had projected a sea level rise of no more than 1.5 feet by that time, but satellite data over the past two years show the world’s major ice sheets are melting much more rapidly than previously thought. The Antarctic and Greenland ice sheets are now losing an average of 48 cubic miles of ice a year, equivalent to twice the amount of ice that exists in the Alps.”
Three years ago what NASA quantified as an alarming loss of  annual ice loss from Greenland was easily demonstrated at that time to be an  insignificant loss, and today NASA’s updated data appears to suggest the annual  rate of global polar ice loss has actually decreased since then.
http://ecoworld.com/blog/2008/12/26/pessimistic-reporting-optimistic-data/


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a4b0434',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Remember the big, somewhat bulky Mercedes Benz cars of the early 1990s? State-of-the-art at that time, they left their competitors behind on Germany’s autobahn. Mercedes is still here today, but the autobahns are not what they were: Germans fear that two decades of neglect after reunification might have seriously damaged the country’s once superb infrastructure.  Similarly, Germany once possessed one of the world’s most well engineered power systems. Yet in 2014 the country urgently needs new power grids – or “stromautobahnen” (electricity highways) –- to keep supply stable in the face of growing proportions of volatile renewable energy. There are concerns over whether it can keep pace with the phase-out of nuclear power, a decision taken in the wake of the Fukushima disaster three years ago. Nuclear helps to stabilise demand on the grid but eight out of 17 reactors have now been shut down, with the rest due to close by 2022.  Electricity generation from renewables has risen lately to new unprecedented highs, overtaking lignite coal to become the number one single source in the nine months to September 2014. This is part of the government’s policy of “energiewende” – the rapid transformation of the power system towards green energy. The stated target is for renewables to provide 55% to 60% of German electricity by 2035 – among the most ambitious in the world.  The changing German energy mix But there are doubts as to whether it is realistic, and whether it might turn out to be harmful for Germany’s industry. One indicator is that electricity tariffs have almost doubled since 2000, for example. Meanwhile CO2 emissions have not decreased, but actually increased over the last few years because coal-fired power has been on the rise while nuclear wanes.  It is also no secret that Germany’s energy companies are not doing well, as pointed out by the chief executive of French rival EDF recently in a general attack on the state of the German energy sector. Both RWE and E.ON have had to accept huge write-downs in the value of their domestic nuclear and fossil-fuel-powered assets. Is Germany thus in the process of sacrificing the once famous reliability of its power sector for little more than problems? And if so, what might be the way forward? Back to nuclear energy?  The disastrous state of Germany’s repositories for radioactive waste such as Brunsbüttel and Asse in the north are not encouraging. And public support for both renewables and nuclear phase-out is still great. But further afield the German decision to live without nuclear has perplexed many observers. To the great amusement of the participants at a business conference in Berlin in 2010, Vladimir Putin offered the Germans firewood from Siberia to heat the country once its nuclear power stations had been switched off. Clearly the energiewende has not exactly impressed the Russians.  How about others? Japan, which has more reason to back the energiewende than almost anyone, is relaunching its nuclear programme. The United States is happy with its shale gas, and at home in Europe, not many countries have joined the Germans either. On top of this, Günther Oettinger, Germany’s accomplished top official in the European Commission, is no longer responsible for energy. It would seem then that Germany’s energy policy has not many influential supporters left.  There remains the idea to sell German-engineered green tech to the world. Siemens is a world leader in renewable technology, for example. Yet the technical level of difficulty is not necessarily high enough to serve as the basis of a high-tech industry in which the Germans can have a competitive advantage in the long term – it is too easy for other players to fabricate equivalent structures. This was illustrated by the Desertec project to build huge solar parks in the North African desert. It collapsed, where Siemens had already walked away a few months earlier. It wasn’t seen as a big deal for the Germans, since many components for the project may have come from China anyway. If the same thing happened with the production of wind turbines, however, it would be a serious setback to the idea of Germany relying on green tech as a base for future industrial production because it would be undercut by other countries who could make the same products more cheaply.   But most things in life have two sides. The great hope is that German engineering will once more surprise the world with technical solutions for difficult problems in difficult times. The central issue is the power grid. One of the key challenges is to find an electricity storage mechanism that can allow a quick and flexible response to changes in the amount of wind and solar power – currently the availability drops when the wind stops blowing or the sun stops shining.  Another big challenge is persuading consumers to react more flexibly to these ups and downs, most likely by altering the price of power accordingly. Solve these problems and the fact that renewables have no fuel costs would make them a serious alternative to other energy sources.  The power grid and its components, not Putin’s firewood, could become the “wunderwaffe” (“wonder weapon”) able to solve Germany’s energy problems. But these new techniques need to be affordable, otherwise the effect will be negligible and only perpetuate the existing problems.  To end on a positive note, there is one much less debated issue around the country’s nuclear phase-out programme. If Germany sticks with its chosen course, the development of third-generation nuclear plants will be in the hands of countries such as France, Britain and Poland. But German industry is famous for its ability to occupy profitable technological niches. Given that there are many nuclear power stations to be dismantled, and a lot of nuclear waste already rotting in the country, the Germans will have to find solutions to deal with it. If they still are the engineers they used to be, they will. This could become a very lucrative thing to then sell to the rest of the world."
"Your ‘order’ is built on sand. Tomorrow the revolution will ‘rise up again, clashing its weapons,’ and to your horror it will proclaim with trumpets blazing: I was, I am, I shall be! The final written words of Polish revolutionary Rosa Luxemburg still resonate 100 years since her death. Murdered by right-wing paramilitaries on January 15, 1919, her fate in Berlin foreshadowed the brutality of the following two decades. The German Revolution she fought for was stamped out in the chaotic aftermath of World War I. But did Luxemburg’s legacy die with it?  Luxemburg’s colossal influence on the left is still celebrated today in Germany and around the world. Her conviction that democratic socialism could flourish was rooted in her meticulous analysis of how capitalism worked and she was convinced that brutality was an inevitable feature of capitalism. Through its need for new resources and territory, Luxemburg believed capitalism would end in collapse and misery.  As her final words suggest, she saw the uncertainty of her day as an opportunity to create a fairer world. With the rise of right-wing strongmen in Donald Trump and Jair Bolsonaro and the ongoing crisis of climate change, we should heed her words today. Luxemburg was born on March 5, 1871 in Zamość, a city in Russian-occupied Poland. Her parents knew from her earliest years that her congenital hip dysplasia would not stop her from pursuing a passion for justice.  She studied philosophy and economics and in 1898 was awarded a PhD in law from the University of Zurich in Switzerland, where she was also exiled and in hiding from the Russian police. Her thesis charted industrial development in Poland from the Napoleonic Wars to the latter years of the Russian Empire. Luxemburg wrote constantly, including passionate letters to close friends and fellow feminists Clara Zetkin and Luisa Kautsky. With their correspondence she fought the loneliness of her several imprisonments between 1904 and 1906 and during World War I.  She questioned everything – challenging Karl Marx, some of his theories and her male comrades who prevaricated on war, monarchy, bureaucracy and imperialism – all of which she vehemently opposed. Luxemburg preferred organising within the labour movement to party politics and she stood against the political class who voted for war in 1914, seeing it as a betrayal of internationalism and the common interest of workers around the world. What is Luxemburg’s contribution to our understanding of the world today? Threatened by environmental destruction, violence against women, gross inequality, insecure and exploitative work and the rise of the far-right, her diagnosis of global capitalism is perhaps more relevant than ever. 


      Read more:
      Beyond Rosa Luxemburg: five more women of the German revolution you need to know about


 For her, economic expansion and the resulting devastation of the environment was not a defect of global capitalism, but an inherent feature of a destructive system. In The Accumulation of Capital she explained that by definition, capital needed to conquer, absorb and destroy non-capitalist economies and territories to survive. This is evident in the recent decision of Brazil’s new far-right president, Bolsonaro, to “integrate the Amazon region into the Brazilian economy”. This would expand the authority and reach of powerful agribusiness corporations into the Amazon Rainforest – threatening the rights and livelihoods of indigenous people and the ecosystems their lives are entwined with. Luxemburg criticised Marx for not having paid enough attention to these external contradictions in economic growth. A socialist revolution was, for Luxemburg, the only way to stop the engulfing of non-capitalist life into capitalism.  She taught that war, colonialism and unsustainable extraction from nature are products of global capitalism. The result is the loss of irreplaceable natural wealth and people struggling for food, water and shelter in the developing world.  


      Read more:
      Capitalism is killing the world's wildlife populations, not 'humanity'


 Luxemburg also criticised economic growth based on financial speculation and profit making in global stock markets. She argued that such a model is prone to crisis, as the 2008 crash demonstrated, which creates unemployment and job precarity that cannot be easily solved. The economy loses the capacity to give employment to every adult with the capacity to work. Many of Luxemburg’s contemporaries, such as Eduard Bernstein, trusted that credit would alleviate capitalism’s tendencies towards crises. However, in Reform or Revolution Luxemburg argued that credit could only postpone and even intensify crisis.  Economic crises that result in this way allow the far right to stoke division within communities by turning widespread economic insecurity into a stick to beat refugees and immigrants with. So can we save Earth from the global expansion of capital and the fascism that emerges from it? The hundredth anniversary of Luxemburg’s assassination should make us reflect on her foresight. As Luxemburg herself saw it, the choice is “socialism or barbarism”."
"

Given his very bad temper, folks have been wondering when Al Gore and his environmental soulmates at the White House were going to get nasty with people who don’t share their view of global warming. Well, the time is now, and it looks like another Scorched Earther. 



Judging by Mr. Gore’s heated rhetoric lately, he sees people who disagree with him as demonic beings who’ll be doing the scorching. _U.S. News & World Report_ quotes him as saying, “I really can’t think of a clearer demonstration of the contrast between Democratic policies and Republican policies than what happened under Scar compared to what happened under Simba.” 



For the few of you who have not seen Disney’s “Lion King,” Scar is the evil leader who takes over the pride. A terrible drought ensues. The women are enslaved. The “Circle of Life” (Elton John’s catchy theme song) is destroyed. For Gore, those are Republican values. When the good Simba returns after a few years away (could this be analogous to Gore at Harvard?), the rains return and balance is restored. Democrats, you see, can change the climate. 



I note parenthetically that drought in central Africa is often related to El Niño, that Gore has been trying to blame El Niño on global warming (scientifically, the exact opposite may be true) and … , well, you get the way his mind works. His global warming world has always been a struggle between good and evil, between New Age and the free market. More than 10 years ago, he wrote this about global warming: ” ‘Evil’ and ‘Good’ are not terms used frequently by politicians [pleeze, Al!]. Yet I do not see how this problem can be solved without reference to spiritual values.” 



Al’s world view is enthusiastically shared by Dirk Forrister, a rock‐​hard Gore man who heads the White House Office of Global Climate Change. Recently he told a Washington, D.C., meeting of the prestigious Energy Institute that critics who disagree with the official view of global warming are “clowns.”  




Science is “frivolous,” to be dismissed quite casually when it turns out to be inconvenient. 



Half an hour later, Forrister blew up when confronted with the most recent scientific findings, which provide compelling and conclusive evidence that folks who have beaten the apocalyptic global warming drum for the last decade have been just plain wrong. 



In 1990 the United Nations Intergovernmental Panel on Climate Change (IPCC) offered a “best estimate” prediction of warming over the next century of 3.2 degrees Celsius. By 1995, thanks in part to incessant attacks by so‐​called skeptics, the warming estimate was lowered to 2.0°C. 



Three months ago Department of Commerce researcher Ed Dlugokenky published a paper in _Nature_ demonstrating that atmospheric methane — an important man‐​created greenhouse gas — is likely to show very little change in the next century. That forces the warming estimate down to about 1.75°C. Forrister called this observation “frivolous.” 



At the same time, Norwegian researcher Gunnar Myhre discovered that the **direct heating effect of carbon dioxide has been overestimated** , something the “skeptics” had maintained had to be true because the planet has warmed so little. His work was published in _Geophysical Research Letters_. That drops the warming estimate to 1.5°C. Forrister called this “frivolous.” 



A popular climate model from 10 years ago that served as much of the basis for the infamous U.N. Climate Treaty and the subsequent Kyoto Protocol (currently 0 for 95 in the Senate) said that, over the past decade, the globe should have warmed about 0.45°C. The observed temperature as measured at the surface, inflated by an urban (warm) bias, shows warming of just 0.11°C. Weather balloon thermistors and barometers, two independent instruments, showed **cooling** , as do the satellites, even after correction for recently discovered orbital drift. Forrister shouted “frivolous.” 



NASA scientist James Hansen has recently argued that the reason dramatic warming didn’t show up as he had forecast was because the soil and vegetation are taking up carbon dioxide at an increasing rate. That makes the planet greener, not browner (sorry, Carol!). Accounting for Hansen’s work published in the _Proceedings of the National Academy of Sciences_ , lowers 21st‐​century warming to about 1.25°C. Forrister called this “frivolous.” 



Tom Wigley of the National Science Foundation has just published a paper in _Geophysical Research Letters_ showing that if every nation met its commitments under the Kyoto Protocol, planetary cooling would be an undetectable 0.07°C by 2050, compared to what the temperature would be if we did nothing. My own research, recently published in _Climate Research_ , shows that the largest warmings occur in the coldest winter air masses rather than in the summer. “These are all frivolous arguments,” Forrister said. 



En coda, Forrister was asked if there would even be a Kyoto Protocol if the climate modelers had told us 10 years ago that it would only warm 1.0–1.5°C over the next century. After a long pause, he said (as best as I can recall), “I don’t know. Maybe yes, maybe no.” 



Having thus opened Pandora’s floodgates, Forrister was asked if the new findings might not make it appropriate for the Senate to pass a resolution forcing the president to withdraw the United States from the U.N. treaty, which allows such an option. “Frivolous!” he shouted. “You just can’t go making frivolous arguments like that!” 



Thus the new White House policy: those who do not agree with their (now thoroughly discredited) view of global warming are evil and will scorch the earth. Science is “frivolous,” to be dismissed quite casually when it turns out to be inconvenient. Stay tuned. Things can only get worse.
"
"
Many people have pointed me to this story, I wanted to read about it a bit before posting it.  Almost two years ago, when this blog was in its very first month, I posted this story on the puzzling leveling off of global methane concentrations. FYI Methane has a “global warming potential” (GWP) 23-25 times that of CO2.
CDIAC has an interesting set of graphs on methane, the first of which shows that indeed global concentrations of CH4 through 2004 have leveled off:

This one on latitude -vs- concentration would surely seem to point to anthropogenic sources of CH4:

So here is yet another addition to the puzzle, which seems to point in the opposite direction:
MIT scientists baffled by global warming theory, contradicts scientific data 
From: TG Daily By Rick C. Hodgin
Boston (MA) – Scientists at MIT have recorded a nearly simultaneous world-wide increase in methane levels. This is the first increase in ten years, and what baffles science is that this data contradicts theories stating man is the primary source of increase for this greenhouse gas. It takes about one full year for gases generated in the highly industrial northern hemisphere to cycle through and reach the southern hemisphere. However, since all worldwide levels rose simultaneously throughout the same year, it is now believed this may be part of a natural cycle in mother nature – and not the direct result of man’s contributions.
Methane – powerful greenhouse gas
The two lead authors of a paper published in this week’s Geophysical Review Letters, Matthew Rigby and Ronald Prinn, the TEPCO Professor of Atmospheric Chemistry in MIT’s Department of Earth, Atmospheric and Planetary Science, state that as a result of the increase, several million tons of new methane is present in the atmosphere.

Methane accounts for roughly one-fifth of greenhouse gases in the atmosphere, though its effect is 25x greater than that of carbon dioxide. Its impact on global warming comes from the reflection of the sun’s light back to the Earth (like a greenhouse). Methane is typically broken down in the atmosphere by the free radical hydroxyl (OH), a naturally occuring process. This atmospheric cleanser has been shown to adjust itself up and down periodically, and is believed to account for the lack of increases in methane levels in Earth’s atmosphere over the past ten years despite notable simultaneous increases by man.
More study
Prinn has said, “The next step will be to study [these changes] using a very high-resolution atmospheric circulation model and additional measurements from other networks. The key thing is to better determine the relative roles of increased methane emission versus [an increase] in the rate of removal. Apparently we have a mix of the two, but we want to know how much of each [is responsible for the overall increase].”
The primary concern now is that 2007 is long over. While the collected data from that time period reflects a simultaneous world-wide increase in emissions, observing atmospheric trends now is like observing the healthy horse running through the paddock a year after it overcame some mystery illness. Where does one even begin? And how relevant are any of the data findings at this late date? Looking back over 2007 data as it was captured may prove as ineffective if the data does not support the high resolution details such a study requires.
One thing does seem very clear, however; science is only beginning to get a handle on the big picture of global warming. Findings like these tell us it’s too early to know for sure if man’s impact is affecting things at the political cry of “alarming rates.” We may simply be going through another natural cycle of warmer and colder times – one that’s been observed through a scientific analysis of the Earth to be naturally occuring for hundreds of thousands of years.
Project funding
Rigby and Prinn carried out this study with help from researchers at Commonwealth Scientific and Industrial Research Organization (CSIRO), Georgia Institute of Technology, University of Bristol and Scripps Institution of Oceanography. Methane gas measurements came from the Advanced Global Atmospheric Gases Experiment (AGAGE), which is supported by the National Aeronautics and Space Administration (NASA), and the Australian CSIRO network.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ba818fa',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter 
Is paranoia contagious? It seems so in certain circles. Like fear, it begins with one, and then spreads like wildfire.
Yesterday I wrote about Professor Stefan Rahmstorf’s hissing and fitting reaction to a piece written by EIKE, the humble sceptic organisation that had the audacity to bring up one of Rahmstorf’s old, yet embarrassing papers here, which I wrote about here.
Something is really getting to the poor fella. Science is supposed to be for the calm, and not the irrational. Rahmstorf has to learn to get a grip.
It seems whenever someone expresses dissent, which is normal in science, Rahmstorf flies off the handle and lashes out, almost irrationally.
Yesterday, for example, Roger Pielke Jr. wrote a piece here with details on an e-mail exchange between Rahmstorf’s sidekick Michael Mann and journalist Daniel Greenberg. These guys really seem to think the whole world is out to get them.
And recall the paranoia that pervaded throughout the Climategate e-mails involving Jones, Mann and the rest of the cast.
In this post I’m only going to focus primarily on Rahmstorf, as he is the issue here in Germany at the moment. But it fundamentally applies to the rest of his team as well.
Firstly, Rahmstorf is one of the very few “climate scientists” who truly believes that sea levels are about to rise at alarming rates over the next few decades, even refusing to accept the much more moderate projections of the IPCC, for which he is a lead author. Even though there is no data out there to support it, Rahmstorf is sure the atmosphere and seas are out to get us.
Calmer minds have attempted on numerous occasions to alleviate his anxiety by pointing out that scientific data do not support his horror visions, and that there is no need to go sleepless about it. For example sea levels over the last 100 years have risen at their normal rate of about 20 cm per century. Over the last years sea level rise has even slowed down, Read more here: Going Down.
Sea level rise is slowing. TOPEX U. of Colorado


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




And when someones tries to explain to him that the Greenland ice cores provide a reasonable temperature reconstruction of the past for the globe, he refuses to believe that too, insisting the fluctuations were local and not global. And if someone shows him that multiple proxies from all over the globe also show that temperatures fluctuated naturally, corresponding with the Greenland ice core, Rahmstorf still refuses to believe it and insists the climate is coming for us.
Proxies from all over the world confirm fluctuations on a global scale.
Whether it’s sea ice data, accumulated cyclone energy, the last two brutal German winters, ocean cycles, satellite data, etc., Rahmstorf refuses to believe the data no matter what. The climate’s after us.
So why does Rahmstorf refuse to believe data and the fellow scientists who deliver them? Here he also believes that these scientists are paid hacks of the big carbon industries, tobacco, and ultra right-wing think tanks.
The lefty Süddeutsche Zeitung newspaper, for example, even reminded Rahmstorf awhile back that EIKE is just small, mailbox sceptic organisation. Are corporations pouring millions and millions and only getting a mailbox operation for their big bucks?
Gravediggers of science
Rahmstorf is acting too paranoid, and this behaviour is now running rampant through much of the warmist side. He even thinks that European corporations are buying up US denier senators. He writes at his piece Headlines From Absurdistan, citing treehugger blog:
Also European companies don’t pinch their pennies when it comes to buying up candidates for the US Senate who deny anthropogenic climate change.
How are regular people reacting to this? Daniel Greenberg wrote to Pielke:
My sympathy to you and anyone else who has to deal with them. They’re gravediggers of science.
It’s called self-destruction.
Relax – the climate is not going to tip and destroy the planet. The data does not show this.
Share this...FacebookTwitter "
"Taking up almost the entire southern tip of India, Tamil Nadu is the country’s second-largest economy. Its delta region is considered to be the “rice bowl” of the state, also producing coconuts, bananas, nuts, spices and sugar cane. On 16 November 2018, Cyclone Gaja struck Tamil Nadu’s coastal areas, devastating local agriculture and infrastructure, and destroying thousands of homes.  Despite the region being prone to extreme weather and residents receiving some advance warning, locals reported that emergency responders only managed to reach many of the remote villages a week later. Perhaps few in the West are aware of the extent of the damage and distress this violent cyclone caused. When it comes to major storms and environmental disasters, the world’s media tend to focus more keenly when developed countries are affected. The devastation caused by extreme weather in Tamil Nadu remains a damning indictment on India’s ability to effectively address such emergencies, whether that be taking preventative measures or actually coping with the aftermath with a proper disaster management plan. Cyclone Gaja saw 45 deaths reported, crops destroyed and livestock killed. One farmer committed suicide after his small coconut plantation – his main source of income – was destroyed. People were traumatised in coastal districts that were unprepared for winds of speed of 160km/h – a category 2 tropical cyclone according to the Saffir-Simpson hurricane wind scale. Millions of trees were uprooted, agricultural land devastated, transportation blocked by debris, communications downed and there were power outages for eight weeks. Three months on and there is still only a limited power supply to some remote areas. It was this very same southern state which faced Cyclone Ockhi in 2017, Cyclone Vardah in 2016, man-made flooding in Chennai in 2015, and of course, the Boxing Day Indian Ocean tsunami in 2004.
Around 300 Tamil Nadu fishermen missing after Cyclone Ockhi have never been found. India is part of the Sendai Framework, an organisation that helps participating countries to adopt disaster risk reduction as a key goal in achieving a sustainable society. The country only released its first national disaster management plan in 2016, despite all of the states on the Bay of Bengal having a history of extreme weather events. Developing countries often faces barriers to creating a joined-up response between national, regional and local emergency plans. Tamil Nadu’s coastal areas are highly populated with people who depend on the sea for their living. Many live in small huts and makeshift houses that are easily destroyed. Often they are ill-informed about the hazards of living in places that are subject to volatile weather, but have nowhere else to go. On top of this, poor emergency planning and communications, insufficient coastal defence investment and failure to learn from previous cyclones have all led to a kind of paralysis in creating effective disaster response strategies. Cyclone Gaja stands as a warning to international disaster organisations; they must prepare the countries they work with more thoroughly for future weather disasters. They need to make clear that taking measures to limit the extent of disaster cannot be voluntary, but mandatory. It will take months to clear the debris and repair the infrastructure, and years to rehabilitate entire villages across Tamil Nadu. It is time to establish a proper framework that helps developing countries to facilitate an effective response to an emergency, crucially with the help of other, more developed nations.  It’s also time to think about alternative options such as natural coastal defences and wetland adaption, such as creating salt marshes and growing mangrove trees and sea grasses which can diffuse the energy of coastal flooding caused by storm surges or flash floods. It’s 14 years since the 2004 tsunami struck this part of the country where 10,000 people lost their lives, and there are still serious gaps in the region’s disaster response methods. Developed countries should understand the need for developing countries to be economically and technologically equipped for extreme events. For many, the issue is lack of funding for investment in coastal defences, but the Indian government needs to make this a key priority. Such extreme weather phenomena are likely to intensify as the effects of climate change – global warming and rising sea levels for example – escalate."
"
Guest post by Steven Goddard




The BBC ran an article this week  titled “Acid oceans  ‘need urgent action‘” based on the premise: 

The  world’s marine ecosystems risk being severely damaged by ocean acidification  unless there are dramatic cuts in CO2 emissions, warn  scientists.

This sounds very alarming, so being diligent  researchers we should of course check the facts.  The ocean currently has a pH  of 8.1, which is alkaline not acid.  In order to become acid, it  would have to drop below 7.0.  According to Wikipedia “Between 1751 and 1994 surface ocean pH is estimated to have decreased from  approximately 8.179 to 8.104.”  At that rate, it will take another 3,500  years for the ocean to become even slightly acid.  One also has to wonder how  they measured the pH of the ocean to 4 decimal places in 1751, since the idea of  pH wasn’t introduced until 1909.

The BBC article then asserts: 

The  researchers warn that ocean acidification, which they refer to as “the other CO2  problem”, could make most regions of the ocean inhospitable to coral reefs by  2050, if atmospheric CO2 levels continue to increase.

This does indeed sound alarming, until you consider that  corals became common in the oceans during the Ordovician Era – nearly 500 million years ago – when atmospheric CO2 levels were about 10X  greater than they are today. (One might also note in the graph below that there  was an ice age during the late Ordovician and early Silurian with CO2 levels 10X  higher than current levels, and the correlation between CO2 and temperature is  essentially nil throughout the Phanerozoic.)





http://ff.org/centers/csspp/library/co2weekly/2005-08-18/dioxide_files/image002.gif
Perhaps  corals are not so tough as they used to be?  In 1954, the US detonated the  world’s largest nuclear weapon at Bikini Island in the South Pacific.  The bomb  was equivalent to 30 billion pounds of TNT, vapourised three islands, and raised  water temperatures to 55,000 degrees.  Yet half a century of rising CO2 later,  the corals at  Bikini are thriving.  Another drop in pH of 0.075 will likely have less  impact on the corals than a thermonuclear blast.  The corals might even survive  a rise in ocean temperatures of half a degree, since they flourished at times  when the earth’s temperature was 10C higher than the present.

There seems to be no shortage of theories about how rising CO2 levels will  destroy the planet, yet the geological record shows that life flourished for  hundreds of millions of years with much higher CO2 levels and temperatures.   This is a primary reason why there are so many skeptics in the geological  community.  At some point the theorists will have to start paying attention to  empirical data.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e989122c8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
And the hits just keep on coming…

646,024 for the Month of July, up from 582,079 in June.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d548764',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Thank you Mr. Chairman and Members of the Committee for inviting me to testify before you today. By way of introduction, I am an Associate Professor of Law at Capital University Law School in Columbus, Ohio, where I teach, among other subjects, Election Law. Though I appear today on my own behalf, I am also an Adjunct Scholar of the Cato Institute. I have researched and written extensively, in both academic and popular journals, on the subject of campaign finance.



First, let me say that I appreciate the decision of this Committee to hold hearings on campaign finance reform. Before Congress now are bills to reform campaign finance by placing new limits on speech — the so called “McCain‐​Feingold” bill in the Senate (S25) and “Shays‐​Meehan” bill in the House (H493). Numerous lending constitutional scholars and campaign finance experts have called these bills unconstitutional. Yet one of the name sponsors of the bill on the Senate side just this month derided these concerns on national radio, saying “When [my opponent] starts relying on those Constitutional arguments, I know he doesn’t have much else in his arsenal.” On another occasion, on national television, this senator stated cavalierly that his opponents “may be right that that particular provision [of the bill] is unconstitutional. And that’s why we have a back‐​up provision.”



Meanwhile, the minority leader here in the House was quoted in a national news magazine earlier this month, saying that “freedom of speech” and “a healthy democracy” are “in direct conflict.”



When a member of Congress so casually treats his oath to uphold the constitution; and when the House minority leader suggests that the First Amendment must itself be amended because free speech “is in direct conflict” with democracy, it is both timely and appropriate for this committee to hold hearings.



Before congress attempts to solve the problems of campaign finance with more regulations burdening free speech rights, we should take stock of the fact that the current regulatory system is responsible for many of the evils we see in campaign finance. We do not need to plug “loopholes” in the system. Rather, we should scrap most all of the present system of campaign finance regulation, remembering the admonition of the First Amendment to the Constitution, that Congress shall make no law abridging the rights of free speech.



Before discussing the details of campaign finance, I think it is important to briefly remind ourselves that, for most of this country’s history, the funding of political campaigns has been totally or largely unregulated. During our nation’s first century, the era which produced as presidents Abraham Lincoln, George Washington, Thomas Jefferson, Grover Cleveland, James Knox Polk, and Andrew Jackson, and which saw giants such as Daniel Webster, Henry Clay, John Quincy Adams, and John C. Calhoun serve in Congress, there were literally no laws regulating campaign finance. And today, we often look back on that century as a golden age of politics — one in which memorable debates over such monumental issues as slavery and western expansion were discussed in serious campaigns, one in which people talked and debated these issues, one in which politics was marked by mass rallies and torchlight parades, and one in which voter turnout was considerably higher than it is today.



The federal government did not become involved in campaign finance until this century. If we look back, we find that the arguments made in favor of regulation a century ago were the same that we hear today: that the American people believed Congress to be made up of the “instrumentalities and agents of corporations;” that “corruption” was the norm; that new advertising techniques and technologies‐​in those days mass newspapers, recordings, train travel‐​had created an insatiable demand for political spending that could only be curbed by spending limits; and that we faced a “crisis” of democracy. In response to such complaints, the federal government passed its first campaign finance law in 1907, banning direct corporate contributions to candidates. In 1943, this ban was extended to labor unions. Additionally, congress passed greater disclosure requirements in 1925. However, these disclosure measures were so toothless as to be meaningless. For example, from its enactment in 1925 until its repeal in 1971, there was not a single prosecution under the Federal Corrupt Practices Act. Yet democracy survived, and this period of minimal regulation gave us Presidents Theodore and Franklin Roosevelt, Calvin Coolidge, Harry Truman, and Dwight Eisenhower, Congressional leaders such as Robert Taft, Hubert Humphrey, and Everett Dirksen, and serious debates over such issues as civil rights. For nearly two centuries, our democracy flourished despite, or perhaps even because of, the absence of any meaningful campaign finance regulation.



Not until the 1974 Amendments to the Federal Elections Campaign Act (FECA) did the federal government pass a campaign finance law with any serious enforcement mechanism. And it was also this law which, for the first time, gave us both contribution limits and, as a necessary accessory to those limits, the strange doctrines of independent expenditures and express advocacy. The 1974 Amendments threw of web of regulation, with an accompanying enforcement bureaucracy, the FEC, over American politics.



The stated goals of the 1974 FECA Amendments were to lower the cost of campaigning, reduce the influence of so‐​called “special interests,” open up the political system to change, and “restore confidence in government.” So what has actually happened in the twenty years since the 1974 Amendments took effect? Well, campaign spending has increased by more than 350 percent; PAC contributions have increased by more than 800 percent; House incumbents, who had previously outspent challengers by approximately 1.5 to 1, now outspend challengers by nearly 4 to 1; incumbent reelection rates have risen to record high levels, spurring the demand for term limits; and public confidence in government has fallen to record lows. Clearly, the 1974 FECA Amendments have been a dismal failure. Yet the response of the reformers — notably Common Cause, the interest group most responsible for the 1974 Amendments, and today the number one cheerleader behind the Shays‐​Meehan bill — is to argue that we need more regulation, more limits, and more bureaucracy. Indeed, some now claim that because earlier regulation has failed, we not only need still more regulation, but we need to amend the First Amendment to allow government to regulate political speech and activity in ways the Founders never dreamed of. I would suggest, however, that when an approach has failed so clearly, so dismally, with such negative consequences, over a period of twenty years, it is time to consider a whole new approach. It is time not for more regulation, nor for more efforts at “loophole” plugging, which is the approach taken by Shays‐​Meehan. Rather, it is time to deregulate American politics.



In my opinion large parts of Shays‐​Meehan and its Senate counterpart, McCain‐​Feingold, are unconstitutional. The so‐​called “voluntary” spending limits of the bills are in fact punitive and coercive, and amount to an unconstitutional condition leveled on the Constitutional right to free speech. That portion of McCain‐​Feingold abolishing PACs is unconstitutional, as even its supporters seem to recognize. If one person can spend $1000 on bumper stickers, it is inconceivable that two people cannot join together to spend $1,000 on bumper stickers.



The Senate bill’s limitations on out‐​of‐​district contributions are probably unconstitutional. There is no reason why an out‐​of‐​district contribution is more “corrupting” than an in‐​district contribution. Thus, there is no compelling government interest to justify the ban on speech.



Overall, Shays‐​Meehan and McCain‐​Feingold mark the most serious legislative assault on free speech in over two decades‐​since the 1974 Amendments to FECA. Many of those 1974 reforms were eventually held unconstitutional. The others have stifled free speech and contributed to the current problems. We should not go down that road again.



But where Shays‐​Meehan and McCain‐​Feingold are most at odds with the Constitution and sound policy is in their efforts to silence political groups engaged in issue advocacy. Indeed, this question of “issue advocacy” versus “express advocacy,” which has aroused the ire of those who would regulate political speech, is a prime example of the danger of the FECA’s attempt to regulate politics to produce a desired result. Congressional Quarterly has noted that in recent years, litigation has become a major campaign tactic. Thus we have Republicans filing complaints against the AFL-CIO, and the Democrats filing complaints against the Christian Coalition, U.S. Term Limits, Americans for Tax Reform, and the Christian Action Network, to name just a few recent complaints. In each case, the complaints amount to a blatant effort to silence political advocacy by these groups. In each of these incidents, the groups involved were not engaged in any nefarious activities such as vote fraud or bribery. Rather, their alleged infractions amounted to what might be called the crime of “committing politics.” That is to say, the groups involved were trying to persuade the American people that either their positions were right, or someone else’s were wrong. It is true that incumbent officeholders do not like being attacked for their stands on issues; particularly when they view those attacks as shameless demogogery. However, the robust discussion of issues is wholly in line with both the First Amendment and the American tradition of political participation. That opposing political interests can invoke the powers of a government bureaucracy in an effort to silence these voices is, I suggest, a much more serious blight on our system than the alleged effects of campaign contributions.



The reasons that these complaints are even treated seriously is because of the doctrine of “express advocacy.” This doctrine itself a bizarre outgrowth of efforts by supporters of campaign finance “reform” to limit campaign contributions. These “reformers” seek to prevent individuals and groups from participating in politics through contributions of money. However, under the Supreme Court’s ruling in _Buckley v. Valeo_ , Congress may not, constitutionally, restrict individual or group expenditures that do not “include explicit words of advocacy of election or defeat of a candidate.…” Thus, political speech is free from FEC regulation if it does not expressly advocate the defeat or election of a clearly identified candidate, but is subject to FEC regulation if it crosses that line.



The response of Shays‐​Meehan to this type of political activity, is to define “express advocacy” more broadly, by looking at such factors as timing and context. In fact, however, it is hard to see how these factors make any serious difference. For example, would last year’s AFL-CIO ads have been any more or any less “express advocacy” if aired three months or fifteen months before an election? Would it really matter if the group sponsoring the ads had public positions on some of the issues addressed? Clearly not. The ads would be no more nor less aimed at shaping public opinion, regardless of the added factors that Shays‐​Meehan seeks to consider. Thus, the long and short of an expanded definition of “express advocacy” would be a sharp reduction in political speech, which is precisely what the Supreme Court’s decision in _Buckley_ , protecting issue advocacy, is intended to guard against.



Of course, the FEC’s concern over “express advocacy” and independent expenditures is all part of a larger effort to plug “loopholes” in the disastrous system of contribution and spending limits enacted in 1974. The only reason anyone cares about “express advocacy” is the fear that, absent such a regulation, groups will spend money to try to affect federal elections, and in doing so will exceed the contribution limits of the FECA. I have written and commented at length on the undemocratic and deleterious effects that these limits have had on American politics. _See e.g._ Bradley A. Smith, _Faulty Assumptions and Undemocratic Consequences of Campaign Finance Reform_ , 105 Yale Law Journal 1049 (1996); Bradley A. Smith, Testimony before Committee on Rules and Administration, United States Senate, February 1, 1996; Bradley A. Smith, _Campaign Finance — Deformed_ , Wall Street Journal, Oct. 6, 1995 (copy attached). In short, these limits have entrenched incumbents; burdened grassroots political activity; limited the number and type of candidates; had the perverse effect of increasing the incentives both for campaign contributors to seek influence, rather than electoral success, and for office holders to reward financial patrons; and increased the power of unelected elites, most notably the media. The efforts to drive money from politics have grotesquely distorted our political system. The reason is simple and obvious: efforts to limit political participation not only run afoul of the Constitution, but they are like efforts to stop the flow of a river — one way or another, the water will pass, diverting course as necessary to do so. So long as the federal government spends over $1 trillion each year and regulates virtually every phase of the economy, not to mention many non‐​economic activities, the American people will seek to persuade Americans to elect their favored candidates. In modern society, this political communication and participation requires the expenditure of money.



Fortunately, past efforts to limit political discourse have consistently been struck down by the courts as unconstitutional. Nevertheless, these attempts to stop this legitimate political advocacy by placing a heavy bureaucracy over political campaigns have had, as I mentioned, a variety of negative consequences. Not the least of these is the way in which such regulation stifles true grassroots democracy.



For example, a 1991 study by the Los Angeles Times found that among the most common violators of FECA were “elderly persons… with little grasp of the federal campaign laws.” Even well‐​funded and well‐​organized groups can find their efforts at grassroots advocacy smothered. In one ill‐​founded effort to prevent political advocacy in violation of campaign finance laws, the FEC passed a rule that would have prevented the United States Chamber of Commerce from communicating political endorsements to more than 220,000 of its dues paying members, mainly small businesses whose owners and managers have little time to follow politics and rely on the Chamber of Commerce precisely for such information. Similarly, the regulation in question would have made more than two‐​thirds of the National Rifle Association’s members ineligible to receive the group’s endorsements, as well as over 44,000 dues paying members of the American Medical Association. This regulation was, fortunately, found unconstitutional by the U.S. Court of Appeals for the D.C. Circuit, but only after these groups had had their speech chilled in the 1994 election. Chamber of Commerce v. FEC, 1995 U.S. App. LEXIS 31925 (D.C. Cir. Nov. 14, 1995).



Another recent FEC rule attempted to prevent corporations and other groups from actively engaging in issue‐​oriented advertising during a campaign, on the theory that such advertising would implicate federal elections. Again, this effort to limit the flow of political information had to be struck down by a federal court. Maine Right to Life Committee, Inc. v. FEC (D. Me., Feb. 13, 1996). Simply putting similar measures into a statute will not make them constitutional.



Limitations on “express advocacy” call for precisely the type of judgments that benefit large organizations with the ability to hire a battery of lawyers to advise them through the regulatory process. Efforts to broaden the concept to include advocacy beyond such express words as “elect” or “defeat” would truly burden free speech, especially for smaller, local groups. Political participation, by definition, seeks to influence voter preferences on both issues and candidacies. Any broadening of the term would lead to a murky standard that would significantly burden most all political speech.



Nevertheless, in addition to the unconstitutional provisions of Shays‐​Meehan, we now find offered up a Constitutional Amendment which would authorize Congress to adopt “reasonable regulations,” so long as they do not “interfere with the right of the people to fully debate issues.” This is classic double speak. Our Founding Fathers recognized that government could not be trusted to make such distinctions: The incentives to crush the opposition would be too great. Thus they wisely passed the First Amendment.



Historically, debates on the First Amendment have concerned the extent to which it covers pornography, or hate speech, or commercial speech, or “fighting words,” or treasonous speech. What has always been accepted, across the political spectrum, is that it covers political speech. So let’s be honest about it: what the Amenders really seek is a clause reading “the First Amendment to this Constitution is hereby repealed.”



Efforts to limit “express advocacy,” like, indeed, the rest of the FECA regulatory scheme, are based on the belief that Americans ought not participate in politics. However, it is not a bad thing for Americans to participate in politics — it is a good thing. It is constitutionally protected. And the fact of the matter is that, more than ever in American society, communicating in the political realm requires the expenditure of money. Money is not an evil in politics — it is a source of information to voters. Efforts to regulate the flow of money in politics over the past 20 years have done much more than money ever did to distort the political system and create a public distrust of government. It is now time to try a new approach — that is, it is time to deregulate politics. There is simply no _a priori_ method to say what is fair or not fair — how much groups should be able to spend, or what kind of advocacy they can spend it on. The bureaucracy that has been established to regulate politics is stifling grassroots advocacy and political communication.



After twenty years of campaign finance regulation, it should by now be clear that independent electoral advocacy by citizen groups lies at the core of the First Amendment, and that such advocacy ought to be beyond the permissible scope of government regulation. Political battles should be fought out in forums of public persuasion. It is poor policy to divert such debates to federal courtrooms, with each side attempting to silence its opponents through such arcane concepts as “express advocacy” and “coordinated” or “uncoordinated” expenditures.



Deregulation of campaign finance, not added regulation, is the proper course of action. The FECA $1000 limit on individual campaign contributions should be abolished entirely, or at least raised to a realistic figure, in order to reduce the need for candidates to rely on independent expenditures. (The $1000 limit, in existence since 1974, has never been adjusted for inflation. Had it been, it would be approximately $3500 today. This is the minimum to which the contribution limit should be raised: $5000, $10,000, or complete removal of the cap would be preferable.) All caps on political party giving should be removed. Donations from a party to its own candidates are not “corrupting.” Moreover, since last year’s Supreme Court decision in Colorado Republican Federal Campaign Committee v. Federal Election Commission, 1996 WL 345766 (U.S. 1996), parties may spend unlimited amounts in support of their candidates, but only independently of the candidate’s campaign. Driving a wedge between parties and candidates is poor public policy. Disclosure of political expenditures meets any public need to know the source of financing. However, even here I must counsel caution. Disclosure rules can have a chilling effect on speech and may be constitutionally limited. McIntyre v. Ohio Board of Elections, 1995 WL 227810 (U.S.)(1995). Disclosure rules governing independent expenditures should be limited, therefore, to groups which engage in substantial activity, spending over $50,000 in an election cycle. Electronic filing and mandatory FEC posting of reports on the Internet would help to insure an informed public. These are the type of sensible, constitutional reforms congress should consider‐​not the unconstitutional Shays‐​Meehan bill or the foolish drive to repeal the First Amendment.



In recent years, it has become increasingly difficult to discuss meaningful campaign finance reform. This is because both public and congressional opinion has become trapped in a box. This box is the conscious creation of groups such as Common Cause, which for 25 years have worked tirelessly to convince the American public that the members of this Committee, and indeed all of Congress, are corrupt bribe‐​takers, and that the public itself consists of innocent dupes incapable of making intelligent voting decisions based on the information presented to them. By constantly drawing simplistic correlations to financial support and voting records, and through the conflation of the issue of campaign finance reform with other issues of voter concern, such as lobby reform, negative campaigning, and legislative gridlock, these groups have purposely attempted to create a climate of public opinion in which certain core assumptions are not to be challenged. These core assumptions are that political advocacy must be heavily regulated; political contributions and, ultimately, political spending limited; and all possible “loopholes” plugged. However, the heavy regulatory regime which these “reformers” have placed over campaign activity is, in fact, a major contributing factor to the very problems that have created such public disgust with the campaign finance system and, indeed, Congress in general.



Now is the time to get out of the box. We must not plunge ahead, sacrificing our First Amendment Freedoms. Congress must realize that Shays‐​Meehan style “reforms,” based, as they are, on the erroneous assumption that Americans should not spend money on political affairs, cut off grassroots involvement and decrease the flow of information to voters. The regulatory approach enacted in 1974 has had unintended, negative consequences that have only increased voter cynicism. The House should reject simplistic proposals such as Shays‐​Meehan, or efforts to amend the Constitution to destroy the right to free political speech, and move generally to deregulate political speech. It ought not be a crime to “commit politics” in America.



Thank you.
"
"

On St. Patrick’s Day, we wear green and celebrate the culture of Ireland. I’ll be down at the pub tomorrow, but I’ll be toasting Ireland’s success at attracting greenbacks — all that investment flowing into the Emerald Isle and the resulting prosperity.



Ireland has boomed in recent years, and it now boasts the fourth highest gross domestic product per capita in the world. In the mid‐​1980s, Ireland was a backwater with an average income level 30 percent below that of the European Union. Today, Irish incomes are 40 percent above the EU average. 



Was this dramatic change the luck of the Irish? Not at all. It resulted from a series of hard‐​headed decisions that shifted Ireland from big government stagnation to free market growth. After years of high inflation, double‐​digit unemployment rates, and soaring government debt that topped 100 percent of GDP, Irish policymakers began to cut spending in the late 1980s in a desperate bid to recover financial stability.



Irish government spending fell from more than 50 percent of GDP in the 1980s to 34 percent by 2005. For Europe that is a triumph of restraint, given that the average size of government across 25 EU countries today is 47 percent of GDP. 



And Ireland has steadily reduced its tax rates. The top individual income tax rate was cut from 65 percent in 1985 to 42 percent today. The capital gains tax rate was cut from 40 to 20 percent in 1999. 



However, the key to Ireland’s success has been its excellent tax climate for business. In 1980, Ireland established a corporate tax rate for manufacturing of just 10 percent. That low rate was subsequently extended to high‐​technology, financial services, and other industries. More recently, Ireland established a flat 12.5 percent tax rate on all corporations — one of the lowest rates in the world, and just one‐​third of the U.S. rate.



Low business tax rates have helped Ireland attract huge inflows of foreign investment. Given the country’s modest size, it boosts a high‐​tech industry second to none. Intel, Dell, and Microsoft are among the island’s biggest exporters. Ireland also hosts booming insurance, banking, money management, and pharmaceutical industries.



The Irish model of rock‐​bottom business taxation has been hugely influential. In recent years, corporate tax rates have been slashed across Europe. According to KPMG, the average rate in the EU has fallen from 38 percent in 1996 to 26 percent in 2006. 



Inspired by the Celtic Tiger, many Eastern European nations have gone one step further and installed both low corporate taxes and simple, flat‐​rate taxes on individuals. According to my colleague Dan Mitchell, there are now 13 nations in the “flat tax club,” including Estonia, Russia, and Slovakia. 



The average corporate and individual rates in the flat tax nations are 19 and 18 percent, respectively, and these countries are growing strongly. Ireland and some of the newer “tiger” economies are putting to rest the notion that luck, natural resources, or other uncontrolled factors are the source of growth.



It’s become fashionable to argue that increased government spending on education is the key to success for countries like Ireland. I’m skeptical. For one thing, booming economies today can attract high‐​skill workers from global labor markets. In Ireland, brain drain has been replaced by brain gain as smart people from across Europe are drawn into the country’s growing industries.



Economic growth is spurred by attracting entrepreneurs and investment capital. Countries do that by establishing the rule of law, stable money, open borders, and low taxes. Let’s call these the “rainbow” factors, since Irish legend says that there is a pot of gold at the end of the rainbow.



Consider Hong Kong, which was once a barren outpost with seemingly few natural advantages. It followed the rainbow and found a pot of gold in just a few short decades.



You may recall that the Irish leprechaun is a sneaky character who tries to hide the pot of gold. Those are the politicians who spend their time trying to undermine the free market. Leprechauns, such as Venezuela’s Hugo Chavez, may be buoyed for short periods by high oil prices or other unique factors. But natural resources are usually pots of fool’s gold because of the bad governance they encourage.



The good news is that with the competition spurred by globalization, the leprechauns are on the defensive As more countries follow the path of the trailbrazing Irish, the relationships between the rainbow factors and growth become ever more clear. 



Now if only we could chase the leprechauns out of this country and cut our corporate tax rate, we’d be enjoying Irish‐​level growth rates by next St. Paddy’s Day.
"
"

Last December representatives of 160 nations gathered in Kyoto, Japan, to hammer out an agreement on emissions of greenhouse gases. But before the U.S. Senate ratifies the Kyoto treaty, it’s important to point out that much discussion of the “greenhouse effect” is little more than hot air. 



The Kyoto agreement rests on forecasts of future greenhouse gas emissions. But energy use, which requires the burning of fossil fuels, depends on economic growth and prosperity. Economists aren’t soothsayers: they often over‐ or underestimate growth, and they can’t be expected to discern how technological advances will change fossil fuel consumption.



Indeed, many climate models — which remain unreliable — predict that most of the climate change will occur many decades from now — the forecasted increase of 4.5 degrees Fahrenheit won’t happen for a century. It’s impossible to have any confidence in forecasts of the technology, population or energy sources of 2098. We can predict, however, that future generations will have better technology at their disposal, that they will be wealthier, and that they will live longer. They will certainly be in a better position to deal with adverse climate changes than we are today.



The Clinton administration had difficulty deciding what it could accept at Kyoto. Its quandary was magnified by the projected failure of the United States to reduce emissions to 1990 levels by the year 2000. Rather than cutting them, a booming economy appears likely to boost emissions of carbon dioxide by at least 15 percent in this decade. Cutting emissions enough to prevent climate change, which might require slashing emissions by some 60 percent, seems out of reach. Avoiding a warmer world would require a radical curbing of emissions by all countries, which in turn would lead to a worldwide slowdown in growth, perhaps even a depression that might make the 1930s look like Disneyland on a good day.



The Kyoto agreement is futile. Even Bert Bolin, the former chairman of the United Nations’ body of experts on global warming, says that the present plan would, if fully implemented, cut warming a quarter century from now “by less than 0.1 degree C, which would not be detectable.” We are plunging into a treaty that creates gigantic obligations without examining its costs and benefits. Congress has demanded that the Clinton administration provide estimates of the costs, but none have been forthcoming.



For most of the world, warming over the next century would cost only a little or would be an actual benefit. The few regions that actually would be harmed by warming should have help.



There is no need to rush into a treaty that would have little benefit but great cost. If climate change becomes a real problem, many steps can be taken that wouldn’t cripple our economy. Ocean scientists have shown, for example, that if the seas were “fertilized” with iron filings, phytoplankton (algae) would bloom and absorb vast quantities of carbon dioxide. The minuscule plants are nutritionally starved for iron and, when provided with that metal, multiply rapidly, absorbing large amounts of carbon. Some experts estimate that iron supplements might offset 15 to 20 percent of man‐​made carbon dioxide over the next few decades.



In addition, harvesting and replanting timber could sequester much carbon. Forest researchers have concluded that an active program of cropping and replanting fast‐​growing forests, then turning the lumber into housing and other long‐​term products, together with reforestation, could offset 12 to 15 percent of human greenhouse gas emissions. Seeding the ocean with iron and properly managing forests could by themselves do as much to slow climate change as capping greenhouse gas emissions at 1990 levels. Furthermore, scientists may develop other strategies in the future that don’t require shrinking our economy. 



The administration is under tremendous pressure to act immediately. To retain credibility with environmentalists, politicians, other countries and the mass media, it must take steps to reduce carbon dioxide emissions even if the limitations would have no benefit and would potentially impose high costs. To succeed in this high‐​wire act, President Clinton will probably propose new regulatory steps and mandates, such as higher fuel efficiency standards for new cars, more stringent restrictions on appliances, strict insulation levels for new buildings and more spending on mass transportation. Most of those regulations would be phased in slowly — that is, after President Clinton leaves the White House and many current members of Congress retire. The actual legislation required to meet the goal might even await a future Congress. Whatever difficulties present themselves, the administration will negotiate a formula that will allow it to claim that the entire world is cutting greenhouse emissions.



In short, this unnecessary measure would devastate our economy. For most of the world, warming over the next century would cost only a little or would be an actual benefit. The few regions that actually would be harmed by warming should have help. Delaying action by 20 to 30 years is the only prudent, “no regrets” policy. Technology will advance. Incomes in Third World countries will grow. The world will be more capable of coping with change. Except for measures that make sense with or without global warming — like ending subsidies for energy and energy use — Congress should resist any attempts to limit greenhouse gas emissions.
"
"
Share this...FacebookTwitterA mountebank fleecing incredulous gamblers (Hieronymus Bosch - Wikipedia)
How many times has the demise of the planet or humanity been predicted? How many charlatans have passed through the revolving doors of history? You’d need the resources of the Census Bureau to tally that up.
A couple of days ago I presented yet another such prediction from a scientist at the Potsdam Institute For Climate Impact Research (PIK).
The planet is going to hell in a hand-basket they keep telling us.  But when you boil it down, they all have one thing in common: They’ve been wrong every time. 
The charlatans of the past have been replaced by a new generation of charlatans – government-funded climate scientists who are paid to get the masses to stampede in panic into the arms of government programs, all aided and abetted by the chatterbox media.
It turns out that life as a whole on the planet has never been better.
One media outlet steps up
In Germany we’re finding out that not all the media accept what is written down on the press releases. There are a few journalists who actually do good research and the hard work of digging. German news magazine FOCUS has an uplifting and positive article about how things, by almost every measure, are far better today than anyone expected.
 The FOCUS report written by veteran journalist Michael Miersch takes a look at some of the charlatans of the past and a bit into the psychology of why end-of-the-world scenarios find such mass appeal among the gullible, who later wonder what happened when things turn out differently.
This is not to say all the world’s problems have been solved. But those problems are under much better control today then they were say 50, 100 or 200 years ago. Here are just a few of the observations made by FOCUS.
Human prosperity
The world’s population has grown 6-fold since 1800, and at the same time life expectancy has doubled. Between 1955 and 2005 inflation-adjusted average personal income has tripled for the average person on the globe.”
Many of the poor indeed have gotten much richer.
Agriculture
How often do we hear about the threats of industrial agriculture devouring land to feed the exploding masses of people? Guess what? Modern agriculture protects wildlife and forests. FOCUS:
With the crop yields of 1961, farmers would have needed 32 million square km of cropland to have fed the 6 billion persons on the globe in 2000. Instead they have been able to harvest the necessary amount of crops on just 15 million acres. That means an area almost the size of South America was spared the plow. Forests and savannahs were thus saved.
All thanks to modern agricultural technology, which today continues to develop nicely. Yet, today, many greens are busily bemoaning the very agriculture that has rescued millions of sq km of forests and wildlife from primitive manaul agricultural practices. Worse, they also want us to fuel our cars with bio-diesel, which would require the extra deforestation of millions of sq km.
And so how do today’s results compare to the projections made by “intellectuals” like Paul Ehrlich? Clearly he has earned a top spot in that elite Club of Rome Crackpots, along with Al Gore, James Hansen and others. And another thing:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Inflation-adjusted prices for food have fallen 75% since 1950.
In fact, food has become so bountiful that bureaucrats are now whining about the social problems of obesity.
Today there are two things that could reverse the tremendous progress that agriculture has made – the Green Movement (think bio-diesel and organic farming) and global cooling. Guess which side (skeptic or alarmist) wants to see both?
Good news are never welcome by alarmist malcontents, who are psychologically sustained by daily doses of misery, pessimism and promises of catastrophe. This keeps Institutes like the PIK running.
On the other hand, optimists who maintain a positive, stay-the-course outlook for the future are viewed by the malcontents as simple-minded, naive and ignorant. Funny how the optimists always turn out to be right.
Miersch writes:
Completely contrary to what we hear day in and day out from the newspapers and TV, whether it’s war, hunger, illiteracy, political suppression, or environmental pollution, all of the world’s evils are shrinking.
That’s especially obvious with air pollution, which has reduced to such an extent in western countries that control freaks and tree huggers are now forced to call the non-pollutant CO2 a pollutant just to have something to do.
On poverty, peace and prosperity
First let’s recall, that all of Eastern Europe and much of South America were governed by dictators less than 50 years ago. Thankfully, they’ve fallen and things have gotten much better with free markets in control. FOCUS writes:
The United nations has determined that poverty receded more in the second half of the 20th century than in the 500 years before it.
Miersch then drives it home, using Germany as an example:
Germans who are now retiring belong to the first generation that has gotten to know peace, freedom, and prosperity as permanent fixtures in their lives. This has never happened in their history.
Isn’t apocalypse tough? But looking at the German media today, you’d think times couldn’t be worse – the planet is threatened by an imminent climate catastrophe that is “hidden in fog – but it’s there!”  
The future
So what lies in the future now that we have seen that every apocalyptic warning heard earlier in history has ended up being just fly-crap in the wind? The business of apocalypse is a big industry and involves lots of money – so don’t expect the end-of-world-charlatans to go away. There’s more money in it today than ever.
Being wrong every time isn’t going to deter today’s modern charlatans. They have a whole new line-up of catastrophes in their bag of tricks: climate change, biodiversity, ocean acidification, species extinction, to name some. And, there are plenty of malcontents out there who want to hear more, more, more.
But I suspect, like the earlier scares of the past, we’ll soon be able to put those on the list of seriously endangered species as well. Here today, extinct tomorrow.
Share this...FacebookTwitter "
"Mining giant Glencore has predicted its carbon footprint will shrink by almost a third by 2035, but will not set climate targets for the company. The company expects its total carbon emissions to fall by 30% in the next 15 years as it gradually produces less coal due to the “natural depletion” of its coal reserves. Glencore promised investors last year it would cap its coal production, and said on Tuesday that production was expected to fall as its Colombian mines close. It may produce less coal from mines in Australia and South Africa, too. The Switzerland-based miner has also vowed to disclose its contribution to the climate crisis and show investors how it plans to align its business with the Paris climate goals from April this year. Glencore has yet to set a target to reduce its full climate impact because it does not include the so-called scope 3 emissions, which would take account of the emissions produced from using the coal it sells. Ivan Glasenberg, the chief executive of Glencore, described climate targets like BP’s aim to be a “net zero” company by 2050 as “wishy-washy” because the goals are “a long way out”. “As we rebalance our portfolio towards commodities supporting the transition to a low-carbon economy, we expect the intensity of our scope 3 emissions to decrease,” a statement from Glencore said. “Starting in 2020, we will start disclosing our longer-term projections for the intensity reduction of scope 3 emissions, including mitigation efforts.” The mining company’s existing targets cover its direct greenhouse gas emissions, and the emissions produced from its operations. Glencore has cut these emissions by 10% between 2016 and 2020 after promising to reduce them by 5%. The company pledged to set out new longer-term targets that “support the Paris goals” later this year. Glencore is the first big mining company to spell out how emissions from the coalmining sector are likely to fall over the next decade as big economies begin to choose gas over coal-fired electricity, and miners shift their attention to commodities that can help to support a low-carbon future. Glencore said it weighted its spending last year towards “energy transition materials” such as copper, cobalt and nickel, which are used to make batteries and electric vehicles. Demand for these materials is expected to grow rapidly as global economies use more renewable energy to power homes and cars. BloombergNEF estimates that coal’s role in the global power mix will fall from 37% today to 12% by 2050. Glasenberg said the company was also “spending a lot of money on carbon capture” technology that could help to reduce the climate impact of coal by trapping the emissions before they can contribute to global heating."
"Our children will enjoy in their homes electrical energy too cheap to meter.    – Lewis L Strauss, chair of the US Atomic Energy Commission, 1954. When Strauss first coined the phrase above he was thinking of hydrogen fusion, a technology that always seems to be a tantalising 30 years away. However even without futuristic new technology the British electricity system may actually approach this mythical situation of energy that is “too cheap to meter”. In the future, the industry’s costs will be determined by the number and size of power plants and turbines that will be needed, rather than the fuel burned in them. This won’t mean an end to electricity bills – but it will mean some major changes in how they are calculated. When the UK’s electricity industry was privatised in the 1990s its power plants had the capacity to generate more than was needed. This was despite the fact many of these plants had already been in service for decades. The national transmission network was also well established. This meant the cost of electricity was largely determined by fuel prices – mainly coal and gas. The hardware used by the industry today has changed little since privatisation – in fact much of it has been in service since the 1960s. On a typical day in November 2014, 37% of the UK’s electricity was produced by 40-year old coal-fired power stations, 31% came from gas power stations, many built during Margaret Thatcher’s “dash for gas”, and 14% from nuclear power stations, all but one of which are scheduled to close in the next ten years. The rest came from renewables and imports from the continent. Over the coming 15 years this will change. To meet national and European targets for CO2 and industrial pollution, existing coal plants will be phased out and gas will be reserved for periods of peak demand. Most electricity will be provided by nuclear and renewables, supplemented during the winter by coal and gas.  This means a big shift from fuel to infrastructure costs – constructing the power stations, turbines, solar panels and associated infrastructure will cost many billions of pounds. Nuclear power’s refuelling and operating costs are relatively low. For wind and solar, the operating costs are effectively zero.  To get anywhere near its emissions targets the UK will have to see a drastic reduction in the amount of gas used for home heating. While scrapping gas boilers in favour of electric heat pumps may reduce emissions, it will also mean the load on the electricity grid will be much greater in winter than in summer.  The problem here is that renewable energy doesn’t operate on the same cycle as us humans – solar panels are much less productive on a gloomy day in December, when electricity demand is high, than on a sunny day in June, when it is low.  The financial incentives for electricity generators are changing. The US fracking boom has reduced natural gas prices in America (but not in Europe as the networks are not connected) and many US electricity generators have therefore switched from coal to gas, leaving a surplus of coal. This has in turn reduced world coal prices at a time when the situation in Ukraine has left gas supplies uncertain. Therefore, if they have the choice, UK generators are burning coal rather than gas.  At periods of low electricity demand, the availability of almost free wind energy is depressing the demand for fossil-fuel generation so it is not difficult to see why the UK’s gas plants are increasingly under-used.   In the UK, generators (the firms who own the power plants) sell their electricity to retailers (the firm named at the top of your bill) through an auction every half hour. The market works on energy prices (£/kWh). This was logical when the investment in power stations had been paid-off years earlier and fuel inputs such as coal or gas were the biggest cost. However, in the approaching situation where marginal energy costs could be almost zero and capacity charges – based on peak usage – are very high, it is difficult to see how an electricity market auction is possible. Bid prices would no longer be related to operating costs and there must be a risk that it would be closer to a game of poker, where the bid is based on an assessment of the competition, rather than on cost. A free market for electricity would be likely to produce extremely high prices in winter, particularly at periods of peak demand, but very low prices at times when the demand can be met entirely by renewable energy. If these energy costs are passed on to the customer, we could see the cost of using an electric kettle to make a cup of tea at 18:30 in January being many pounds, but electricity costing almost nothing during long periods in the summer.   Headlines about a pensioner paying £20 to boil a kettle would be a big challenge for whichever government is in power. However, if peak-time prices are not allowed to reflect shortages there would be no incentive for continued investment in backup fossil fuel generation to be used during these periods. Such generation would be used only rarely, but without it the UK would risk blackouts – even more politically challenging. Reforms to the electricity market are supposed to resolve this problem through introducing regular capacity auctions, where generators can bid to hold otherwise unused plants in readiness over a particular time period. Taken with an agreed “strike price” for renewable and nuclear generators which guarantees a minimum payment per kWh this has the effect of almost eliminating a competitive market for low-carbon electricity generators. As leading energy economist Dieter Helm wrote at the time of electricity privatisation, “the idea that governments could simply retreat from the scene and leave it to competitive markets is an illusion – energy is just too important to the economy and society”.  Perhaps consumers should be charged on their contribution to national peak load, rather than on their energy consumption. Then energy really would become too cheap to meter."
nan
"
Share this...FacebookTwitterLast week it was reported almost everywhere by the German media that North Atlantic currents were the warmest in 2000 years and melting the Arctic at an unprecedented rate.
Labrador and North Atlantic currents (Chart from US Coast Guard)
These media reports were based on a study published in Science. The authors write in the abstract (emphasis added):
Here, we present a multidecadal-scale record of ocean temperature variations during the past 2000 years, derived from marine sediments off Western Svalbard (79°N). We find that early–21st-century temperatures of Atlantic Water entering the Arctic Ocean are unprecedented over the past 2000 years and are presumably linked to the Arctic amplification of global warming.”
I’m not sure what they mean by “early 21st century”. Perhaps the years 2000 to 2007? The study says the waters are about 2°C warmer. Here it has to be noted that they are presuming, i.e. postulating. The CO2 link here is a bit of wild speculation.
Here’s how Der Spiegel puts it in a report titled Atlantic Current Is Heating Up The Arctic:
The experts suspect that the accelerated reduction in sea ice and the measured warming of ocean and atmosphere in the Arctic over the last decades, among other factors, was the result of an enhanced transfer of warmth from the Atlantic. The Fram Straits is even about 1.4° C warmer than during the Medieval Climate Optimum, a time when temperatures in Europe were significantly increased.”
And Der Spiegel writes:
‘Cold sea water is decisive in the formation of sea ice, which in turn cools because it reflects sunlight,’ says Thomas Marchitto of the University of Colorado in Boulder. The melting is accelerating by itself.”
Media reports like the one in Der Spiegel of course emphasized the supposed vicious circle of the melting Arctic ice dynamic: more melting leads to more warming, which then accelerates the process – all unleashed of course by man-made CO2.
If anything they are, perhaps unwittingly, admitting that the Arctic sea ice reduction of the 2000s can be traced back to ocean currents.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




And as things stand right now, just the opposite is occurring. The Arctic is NOT melting, it’s freezing up again – recovering from its low in 2007, and quite impressively.
The Global Rumblings website here reports that 70 trillion cubic feet of ice have been added to the Arctic core since January 2009. That translates to 2000 cubic km – enough to cover Manhattan with 20 miles of ice (or 32,000 Manhattans with 1 meter of ice).
The US Navy PIPS 2.0 graphic shows ice thickness. The following comparator shows how it’s the Arctic that has gone green.

Source: Global Rumblings
Some will say that PIPS is not a reliable indicator of Arctic ice thickness, and so cannot be used reliably. But you can put that rumour to rest, see PIPS WUWT.
So why is the Arctic thickening and regrowing, and no longer melting at an unprecedented rate as claimed by the media?
This could have to do with the Labrador Current, which flows southward between Greenland and Labrador. Reports say it is slowing down. That means cold water is not getting transported out of the Arctic. A Der Spiegel article just 2 weeks ago titled “Feared Atlantic current is now weakening” suggests that this current is at its weakest level in 1800 years. What is it caused by? According to scientists, Der Spiegel says:
As a cause for the change, scientists suspect climate change. The coincidence that this has happened during the warming of the last decades allows this to be the conclusion, they believe. But the knowledge about ocean currents still has many holes says Wallace Broecker of Columbia University in USA – a pioneer in ocean research.”
Changes in the atmosphere controls the ocean currents? Right. And as usual, they’re sure – yet admit there are many holes in the knowledge and so they are not sure.
Meanwhile, the ice keeps growing.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterJapan has decided to back out of carbon trading, likely from intense pressure from industry. This will no doubt disappoint many in organized crime, read here and here.
According to the online Environmental Leader:
A September survey of Japan’s largest business lobby group, found that 95% of companies surveyed oppose carbon trading, citing competition from countries like India and China that are not bound by similar pollution limits, reported Bloomberg.”
Japan’s government also said it won’t support an extension of  the Kyoto Protocol after its greenhouse-gas emissions targets expire in 2012, calling the treaty “outdated” because it only regulates 27 percent of global emissions, and doesn’t include the U.S. and China, reported Bloomberg.”
Meanwhile the Financial Times Deutschland here reports the same in a piece called: CO2 Trade No Longer In Vogue. The FTD blamed pressure from industry for leading to Japan’s decision to back out of the scheme, and added:
The decision is a major setback for the once praised system that was touted as a way to achieve environmental protection using  market economy means.”
Turns out that the Japanese have determined that it’s not very market economy-friendly after all.
The USA, under its newly elected Congress, have also signalled it will reject any trading scheme. Japan meanwhile is planning hefty taxes on carbon fuels and is considering energy feed-in traiffs like those used in Germany for supporting renewable energy.
Share this...FacebookTwitter "
"Is your morning coffee an espresso or a skinny latte? Is it from a darkly roasted French or Italian blend? If it’s a high quality brew, it’s almost certainly made with beans from the Arabica species (Coffea arabica), which is known for its finer flavours. Examples would be Javan coffees, Ethiopian sidamo, and the expensive Jamaican blue mountain. If you’ve stirred together an instant blend, it’s probably from a different species, Robusta (Coffea canephora), known for its harsher taste. But there are more than 100 species of coffee in the wild. All produce similar beans that you could make a recognisable coffee drink from. Robusta is sometimes openly mixed with Arabica in commercial products – and is often secretly used to adulterate “100% Arabica” products, too. A third species, Coffea liberica, native to west and central Africa, is widely grown for local use in tropical countries, but is not globally traded because of its more bitter taste.   A fourth species Coffea eugenoides bred with Robusta to give rise to Arabica, a crossbreed. Another 38 closely related species are known or assumed to have fertile pollen transfer with commercial coffees. There are a further 82 species which are more distantly related to the commercial breeds, but scientists could interbreed them with commercial coffees in a lab. All these coffee relatives can help enhance the genetic diversity of commercial coffee species, making them more adaptable to changes in their environment. Climate change is threatening global coffee yields as changing temperatures and rainfall patterns affect plant growth. The changing climate may also be leaving plants more vulnerable to disease. All major commercial coffee growing countries have been badly affected by the fungal disease “coffee leaf rust”, which spread across Africa and into Asia during the early 20th century, then to South America, becoming entrenched globally by the turn of the millennium. The Central American coffee rust outbreak that began in the 2011-2012 harvest season affected 70% of farms in the region, resulting in over 1.7m lost jobs and US$3.2 billion in damage and lost income. 


      Read more:
      Climate change is causing havoc for global coffee yields


 Robusta varieties used for the instant blends have been key to developing resistance to coffee leaf rust in Arabica varieties through cross breeding. As climate change and disease risks escalate, wild coffee species offer a crucial resource for maintaining the world’s coffee supply. Arabica has tightly limited geographic ranges in which it grows well and Robusta, while resistant to leaf rust, is vulnerable to other diseases. A recent study led by the UK’s Kew Royal Botanic Gardens set the value of this variety in context: over 60% of coffee species are threatened with extinction. The authors explained that wild relatives of coffee are already used as local substitutes for globally traded commercial crops. They offer different climatic tolerance ranges and disease resistance traits that can help ensure global coffee production continues to meet demand.  But coffee species are particularly vulnerable to extinction because they occur in a small numbers of small geographic ranges – such as pockets of wild Arabica populations between certain altitude ranges in the Ethiopian highlands. Wild coffee species – and wild varieties of the commercial species – are almost all in decline due to competition for land use and overharvesting of the coffee plant for timber or firewood. A number of wild coffee relatives haven’t been spotted for many decades and may be extinct.  One species, the cafe marron, from the remote island of Rodrigues in the Indian Ocean, was known from only one sighting in 1877. A century later, a schoolboy drew an “unusual” tree while exploring and showed it to a teacher. They recognised it as a surviving cafe marron. The sole surviving specimen of that wild coffee has inspired wider forest conservation on Rodrigues. It is also being cultured in lab collections at Kew. Sadly, there may be less hope for other species. Coffee seeds don’t store well, unlike wild relatives of other crops such as wheat or maize. So we can’t rely on storage in seed banks to conserve coffee diversity and resilience. Freezing plant matter in labs or growing samples in test tubes might be an alternative, but not one that has been explored beyond existing commercial strains. Preserving different coffee varieties in botanic gardens isn’t really viable for protecting genetic diversity either. Coffee species readily fertilise each other, “contaminating” the resource you’re trying to conserve. While some experts suggest we preserve coffee diversity in collections, the Kew Gardens study argues that the sustainability of coffee depends on conservation of these species where they grow, in protected areas and working with communities throughout their native distribution in Africa and Asia. Conserving genetic diversity should be included in existing approaches for sustainable coffee production, such as Fair Trade and Rainforest Alliance certifications. Ensuring the continuity of the coffee trade means protecting the ecosystems coffee comes from and the livelihoods of people across the bean to coffee cup economy. We can also expect new flavours and even coffees with naturally low or zero caffeine content. Naturally caffeine-free Indian Ocean island cafe marron anyone?"
"
Two Stories for you, one about the snow itself, and the other about climate law being debated and passed in the middle of the unusual snow.- Anthony
London has first October snow in over 70 years
From the Guardian
Cold snap causes flight cancellations while a motorway accident kills one driver and causes severe disruption

Parts of south-east England had more than an inch of snow last night while London experienced its first October snowfall in more than 70 years as winter conditions arrived early.
Snow settled on the ground in parts of the capital last night as temperatures dipped below zero. A Met Office spokeswoman said it was London’s first October snow since 1934.
For greater south-east of England it was the first October snow since 1974. High Wycombe in Buckinghamshire had 3cm (1.2 inches). One of the coldest temperatures recorded was -4.1C in Benson, Oxfordshire.
“It is unusual to have snow this early,” the Met spokeswoman said. “In October 2003 sleet and snow was recorded in Northern Ireland, Wales, south-west, north-west and north-east England and the Midlands, but it was mainly over higher ground.”
read the entire story here
How Parliament passed the Climate Bill (in spite of the weather)
By Andrew Orlowski The Register
Posted in Government, 29th October 2008 12:35 GMT

Excerpt: Snow fell as the House of Commons debated Global Warming yesterday – the first October fall in the metropolis since 1922. The Mother of Parliaments was discussing the Mother of All Bills for the last time, in a marathon six hour session.
In order to combat a projected two degree centigrade rise in global temperature, the Climate Change Bill pledges the UK to reduce its carbon dioxide emissions by 80 per cent by 2050. The bill was receiving a third reading, which means both the last chance for both democratic scrutiny and consent.
The bill creates an enormous bureaucratic apparatus for monitoring and reporting, which was expanded at the last minute. Amendments by the Government threw emissions from shipping and aviation into the monitoring program, and also included a revision of the Companies Act (c. 46) “requiring the directors’ report of a company to contain such information as may be specified in the regulations about emissions of greenhouse gases from activities for which the company is responsible” by 2012.
Recently the American media has begun to notice the odd incongruity of saturation media coverage here which insists that global warming is both man-made and urgent, and a British public which increasingly doubts either to be true. 60 per cent of the British population now doubt the influence of humans on climate change, and more people than not think Global Warming won’t be as bad “as people say”.
Read the rest of the story at the Register, here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9bd25ee1',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

One of the few things that BOTH sides of the Carbon Dioxide and AGW debate seem to be able to agree on is the belief that CO2, as a trace gas, is “well-mixed” in the atmosphere. Keeling’s measurements at Mauna Loa and other locations worldwide rely on this being true, so that “hotspots” aren’t being inadvertently measured.
As support for this, if you do some Google searches for these phrases, you’ll get hundreds of results of the usage together:
CO2 + “well mixed”
“carbon dioxide” + “well mixed”
You’ll find complete opposites using the same “well mixed” phrase, for example:

Gavin Schmidt of Real Climate writes in comment # 162 of this thread on Realclimate.org
“A full doubling of CO2 is 3.7 W/m2, and so by looking at all well-mixed GHGs you get about 70% of the way to a doubling.”
Roger Pielke Sr. writes in April 2008:
“…and thus are not providing quantitatively realistic estimates of how the climate system responds to the increase in atmospheric well mixed greenhouse gases in terms of the water vapor feedback.”
You’ll also find the phrase in use in titles of scientific papers, for example this one published in the AGU:
New Estimates of Radiative Forcing Due to Well Mixed Greenhouse Gases
And you’ll find the phrase used in popular media, such as this article from the BBC:
Carbon dioxide continues its rise
In describing the emasurements of CO2 at Mauna Loa Observatory: “The thin Pacific air is ideal for this research since it is “well-mixed”, meaning that there is no obvious nearby source of pollution, such as a heavy industry, or a natural “sink”, such as forest which would absorb CO2.”
Hmm, “no obvious nearby source of pollution” I suppose the volcanic outgassing nearby doesn’t count as “pollution” since it is natural in origin.
So it seems clear that there is a broad agreement on the use of the term. I suppose you’d call that “scientific consensus”.
So it was with some surprise that I viewed this image from NASA JPL, a global CO2 distribution as measured by satellite:

Note the variations throughout the globe, ranging from highs of 382 PPM to lows around 365 PPM. There is a whole range of data and imagery like this above available here
My question is: how does this global variance translate into the phrase “well-mixed” when used to describe global CO2 distribution? It would seem that if it were truly “well-mixed”, we’d see only minor variances on the order of a couple of PPM. Yet clearly we have significant regional and hemispheric variance.
NASA JPL provides this caption to help understand it:
Although originally designed to measure atmospheric water vapor and temperature profiles for weather forecasting, data from the Atmospheric Infrared Sounder (AIRS) instrument on NASA’s Aqua spacecraft are now also being used by scientists to observe atmospheric carbon dioxide. Scientists from NASA; the National Oceanic and Atmospheric Administration; the European Center for Medium-Range Weather Forecasts; the University of Maryland, Baltimore County; Princeton University, Princeton, New Jersey; and the California Institute of Technology (Caltech), Pasadena, Calif., are using several different methods to measure the concentration of carbon dioxide in the mid-troposphere (about eight kilometers, or five miles, above the surface). The global map of mid-troposphere carbon dioxide above, produced by AIRS Team Leader Dr. Moustafa Chahine at JPL, shows that despite the high degree of mixing that occurs with carbon dioxide, the regional patterns of atmospheric sources and sinks are still apparent in mid-troposphere carbon dioxide concentrations. “This pattern of high carbon dioxide in the Northern Hemisphere (North America, Atlantic Ocean, and Central Asia) is consistent with model predictions,” said Chahine. Climate modelers, such as Dr. Qinbin Li at JPL, and Dr. Yuk Yung at Caltech, are currently using the AIRS data to study the global distribution and transport of carbon dioxide and to improve their models. 
As we’ve found with surface based temperature measurement, it seems the more we look at satellite data, the more we learn that our earth bound assumptions based on surface measurement don’t always hold true.
When measuring the planet, looking at the whole planet at one time seems a better idea than trying to measure thousands of data points at the surface, sorting out noise, doing adjustments to “fix” what is perceived as bias, and assuming the result is accurately representatiive of the globe.
UPDATE: 7/31/08 I got a response from the AIRS team on satellite CO2 measuremenst, see this new posting
We won’t have to rely on ground based CO2 measurements much longer.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9db144bd',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterGuest writer Ed Caryl recently looked at 9 “rural” stations scattered over the Arctic: from Alaska, to Canada, to Northern Europe western Russia and Siberia, and found Arctic temperatures follow the AMO, and not CO2. Read here A Light In Siberia. It’s important to note that the 9 stations were selected because they appeared to be NOT influenced by man-made heat sources.
First, here’s the AMO going back more than 150+ years. The cycles are clear to see.

Atlantic Multidecadal Oscillation (AMO). Source: http://www.appinsys.com/globalwarming/SixtyYearCycle.htm
North Pole, 17 March 1959. Image from NAVSOURCE
The AMO shows warm periods centering at about 1880, 1940 and 2005, i.e., 60 year cycle. We recall seeing photos of submarines surfacing at the North Pole back in the 1950s, see photo left, meaning it was relatively balmy back then too, as the AMO chart suggests.
Well how do the temperature curves of Ed’s 9 untainted Arctic stations match up with the AMO? The following are the GISS graphs of these 9 stations, each shown individually. Take a look at each of them:





What happens from 1940 – 1980, a time when CO2 was increasing? What happens after 1980? How do these charts match up with the above AMO chart? Fit pretty well? It seems so.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Some of the temperature records shown above are shorter and some are longer. But they all show that temperatures between 1940 and 1980 were dropping. Remember that the Arctic is called the canary in the coal mine. When the globe cools or warms, you really see it in the Arctic, so they say.
Next Ed Caryl plotted each of the above graphs on a single chart. Ed calls these “rural” stations isolated because they are not impacted by man-made heat sources like asphalt, light bulbs, etc:
Plot of the 9 ""rural"" (isolated) stations.
And then he normalized the plots and generated an average. He explains how here, scroll down to “The averaging of station data”.  The resulting plot with a linear trend line is shown as follows:
Average of the ""rural"" stations
Sure some hot-shot statisticians out there are going to say you can’t do this, or that, or whatever blah blah blah…but that’s just nitpicking. Attention to tiny detail is a later thing.
Ed’s method suffices for now to generate a good general picture. If the math hotshots out there want to do it with micrometers, no one is stopping them. I doubt the general picture is going to change that much, though.
If you look at the 9 individual plots above and imagine how a composite of all 9 would look like, it would look like Ed’s chart – common sense.
Doesn’t the shape of above curve look eerily similar to the shape of the AMO from 1920 to today? Ed thought so too, and so he superimposed the average of the 9 isolated stations and the AMO:

Gee, do you think Arctic temperatures correlate better with CO2? 
Of course this is only a preliminary analysis that examined only 9 isolated stations scattered over the entire Arctic perimeter. But I suspect that if all stations were thrown in, except the crappy ones equipped with light bulbs and of that sort, you’d end up with similar results.
Could the AMO possibly drive climate? Well, the latest paper authored by Phil Jones and others seem to be hinting at this. Read my post from yesterday https://notrickszone.com/2010/09/24/der-spiegel-the-oceans-influence-greater-than-thought/.
Obvious conclusion: Trace gas Co2 drives the Arctic climate about as much as a sea breeze drives a loaded freight train.
Share this...FacebookTwitter "
"

“No matter how much you pay with a carbon levy, virtually nothing is received climatically… No matter the level of domestic action that we take, it will pale in comparison to the rapid expansion of carbon dioxide emissions in other parts of the world.”



How much global warming will result from U.S. emissions over the course of this century, and how much of that could be prevented by a carbon tax? These two questions have the same simple answer— _virtually none_. One or two tenths of a degree a century out with–and without–a carbon tax makes the whole climate debate a peculiar exercise.



The Intergovernmental Panel on Climate Change (IPCC) estimates that the earth’s average temperature will increase somewhere between 1.1°C and 6.4°C over the 21st century, depending on the assumed pathway of anthropogenic emissions (both greenhouse gases and aerosols) and the actual (but unknown) climate sensitivity.



A temperature rise towards the low end of this range is not worth worrying too much about (the ‘lukewarming’ position), while a rise near the higher end of the range is potentially much more problematic (the alarmist position). And while lukewarmers and alarmists stray apart when it comes to the amount of climate change they are expecting, _they are bound together by the fact that there is practically nothing that can be done to change the situation, either way_. Why? They use the same math.



But you won’t hear many alarmists admitting to that fact—if they did, you would never have heard of the terms like “cap‐​and‐​trade” or “carbon tax.” Instead, you’d be much more familiar with words like “planning” and “adaptation.”



 **How Much U.S.-Side Global Warming?**



Lest alarmists protest, let’s work through the numbers to see just how much “global warming” is being caused by U.S. economic activity.



In other words, how much of the IPCC’s projected 1.1°C to 6.4°C of warming will the U.S. be responsible for in the next century? The answer is about 0.08°C of the low end estimate and about 0.35°C of the high end estimate (according to an IPCC‐​like analysis*). Using the IPCC’s mid‐​range scenario, carbon dioxide emissions from the U.S. contribute about 0.19°C of the total 2.96°C global temperature rise.



Yep, that is it. For all the incessant talk as to how the highly consumptive U.S. lifestyle—from SUVs, to air conditioners, to big screen TVs and huge portion sizes—is leading climate catastrophe, the sum total of our contribution to “global warming” this century will amount to the neighborhood of _about 0.2°C_. Not five degrees. Not two degrees. But about _two‐​tenths of a degree Celsius_. And even this number may be on the high side if the climate sensitivity is lower than about 3°C (see here for more on recent findings concerning the climate sensitivity).



So all the U.S. carbon dioxide emissions restriction tactics—EPA regulations, cap and trade schemes, carbon taxes, efficiency programs, guilt‐​inducing ad campaigns, etc.—are aimed at chipping away at this already tiny 0.2°C. Big deal.



When considering any of these options, you have to ask yourself (or your representatives in Congress) how much are you willing to pay—in dollars or inconvenience, or both—to avert some portion of this 0.2°C of global temperature increase and its accompanying inconsequential and impossible to measure climate change?



 **Avertable Climate Change**



No matter how much you pay with a carbon levy, virtually nothing is received climatically.



Consider the effect of the Waxman‐​Markey Climate Bill that was passed by the U.S. House of Representatives back in the summer of 2009. That cap‐​and‐​trade scheme was designed to step down U.S. carbon dioxide emissions ultimately by 83% by the year 2050. This would have taken a monumental effort that was sure to be disruptive in any number of ways.



The net climate result? Instead of 0.19°C of warming coming from the U.S. by the year 2100 (assuming the IPCC mid‐​range scenario), our contribution would have been reduced to 0.08°C—for a net “savings” of about 0.11°C of “global warming”. (See my analysis here.) This amount is of virtually no environmental consequence and was repeatedly cited as one of the reasons that this legislation died in the Senate.



Much the same holds true for the present day fad for a carbon tax. The talk of a carbon tax—or more rightly a carbon _dioxide_ tax—was bolstered recently by superstorm Sandy and its aftermath (widely, but wrongly, blamed on anthropogenic climate change). A tax on carbon dioxide emissions would be felt from the gas station to the grocery store and everywhere in between as virtually every aspect of our modern life benefits from cheap carbon dioxide emitting, fossil‐​fuel produced energy.



A carbon tax has become so trendy that even “no new tax pledge” champion Grover Norquist briefly flirted with it before quickly reconsidering.



And for good reason. For about the only thing that a carbon (dioxide) tax in the U.S. will _not_ do, is produce a detectable mitigation of anthropogenic global warming and any associated effects.



The U.S. Energy Information Agency recently projected the impacts on carbon dioxide emissions in the U.S. out to the year 2035 resulting from a carbon dioxide tax of $15/​per ton emitted (beginning in 2013 and increasing by 5% per year out to 2035) and for a tax of $25/​ton of CO2 (beginning in 2013 and increasing by 5% per year out to 2035). The EIA projections are shown in Figure 1. I have continued the same emissions reductions pathway out from 2035 until the year 2100—admittedly, this is a shot in the dark, but at least is gives us something to work with.









Figure 1. Energy Information Agency estimates for the future course of U.S. carbon dioxide emissions resulting from a $15/​ton tax on carbon dioxide emissions (solid blue line) and a $25/​ton tax on carbon dioxide emissions (solid red line), 2010–2035. I have extended these projections to the year 2100 (dotted lines).



When I substitute these carbon tax pathways for U.S. carbon dioxide emissions for the one already included in the IPCC mid‐​range scenario, I calculate that the amount of “global warming” contributed by the U.S. drops from 0.19°C by the year 2100 to 0.13°C and 0.08°C for the $15/​ton and $25/​ton carbon tax respectively (Figure 2).









Figure 2. Amount of total global warming (red bars) and the U.S. contribution to the total global warming (blue bars) over the 21stcentury under three different scenarios. BAU= business‐​as‐​usual as portrayed by the IPCC A1B mid‐​range emissions scenario; $15/​ton CO2=$15/ton tax on U.S. carbon dioxide emissions as prescribed by the EIA to the year 2035 and extended to 2100; $15/​ton CO2=$15/ton tax on U.S. carbon dioxide emissions as prescribed by the EIA to the year 2035 and extended to 2100.



A global warming “savings” of 0.06°C to 0.11°C across this century is of no scientific consequence, while a tax of carbon dioxide emissions of $15 to $25 per ton is sure to be of significant personal consequence (and, my guess, only very temporary, i.e., until the next election cycle).



 **Conclusion**



Any tax on carbon dioxide is clearly a case of not getting what you pay for. You will pay a lot, and receive nothing in return, or at least nothing that you will ever realize, or that could be proven.



What is working against any form of a carbon tax is that the U.S. plays only a minor role in the future course of global warming driven by anthropogenic activities. The rest of the world—primarily the developing countries like China and India—is where the rubber meets the road for climate change. No matter the level of domestic action that we take, it will pale in comparison to the rapid expansion of carbon dioxide emissions in other parts of the world.



Instead of trying to make an expensive repair a very small and inconsequential leak, I would think that our attention ought to be directed at determining just how big the coming flood may be, and make our plans accordingly.



 **Appendix: Methodological Note**



I have used the Model for the Assessment of Greenhouse‐​gas Induced Climate Change (MAGICC) for my analysis of the effect of U.S. emissions on projected global temperature rise. MAGICC is sort of a climate model simulator that you can run from your desktop (available here). It was developed by scientists at the U.S. National Center for Atmospheric Research.



There are many parameters that can be altered when running MAGICC, including the climate sensitivity (how much warming the model produces from a doubling of CO2 concentration) and the size of the effect produced by aerosols. In all cases, I’ve chosen to use the MAGICC default settings, which represent the middle‐​of‐​the‐​road estimates for these parameter values (e.g., climate sensitivity equals 3.0°C).



I’ve had to make some assumptions about the U.S. emissions pathways as prescribed by the original IPCC scenarios in order to obtain the baseline U.S. emissions (unique to each scenario) to which I could apply the various emissions reduction schedules. The most common IPCC definition of its scenarios describes the future emissions, not from individual countries, but from country groupings. Therefore, I needed to back out the U.S. emissions.



To do so, I identified which country group the U.S. belonged to (the OECD90 group) and then determined the current percentage of the total group emissions that are being contributed by the United States—which turned out to be about 50%. I then assumed that this percentage remained constant over time. In other words, that the U.S. contributed 50% of the OECD90 emissions in 2000 as well as in every year between 2000 and 2100.



Thus, I am able to develop the future emissions pathway of the U.S. from the group pathway defined by the IPCC for each scenario (in this case, the B1, the A1B and the A1FI scenarios). The Waxman‐​Markey and carbon tax reductions were then applied to the projected U.S. emissions pathways, and the new U.S. emissions were then recombined into the OECD90 pathway and into the global emissions total over time.



It is the total global emissions that are entered into MAGICC in order to produce global temperature projections. My results are largely insensitive to minor changes in these assumptions.
"
"While Europe was in the early days of the Renaissance, there were empires in the Americas sustaining more than 60m people. But the first European contact in 1492 brought diseases to the Americas which devastated the native population and the resultant collapse of farming in the Americas was so significant that it may have even cooled the global climate.  The number of people living in North, Central and South America when Columbus arrived is a question that researchers have been trying to answer for decades. Unlike in Europe and China, no records on the size of indigenous societies in the Americas before 1492 are preserved. To reconstruct population numbers, researchers rely on the first accounts from European eyewitnesses and, in records from after colonial rule was established, tribute payments known as “encomiendas”. This taxation system was only established after European epidemics had ravaged the Americas, so it tells us nothing about the size of pre-colonial populations. Early accounts by European colonists are likely to have overestimated settlement sizes and population to advertise the riches of their newly discovered lands to their feudal sponsors in Europe. But by rejecting these claims and focusing on colonial records instead, extremely low population estimates were published in the early 20th century which counted the population after disease had ravaged it. On the other hand, liberal assumptions on, for example, the proportion of the indigenous population that was required to pay tributes or the rates at which people had died led to extraordinarily high estimates. Our new study clarifies the size of pre-Columbian populations and their impact on their environment. By combining all published estimates from populations throughout the Americas, we find a probable indigenous population of 60m in 1492. For comparison, Europe’s population at the time was 70-88m spread over less than half the area. The large pre-Columbian population sustained itself through farming – there is extensive archaeological evidence for slash-and-burn agriculture, terraced fields, large earthen mounds and home gardens.  By knowing how much agricultural land is required to sustain one person, population numbers can be translated from the area known to be under human land use. We found that 62m hectares of land, or about 10% of the landmass of the Americas, had been farmed or under another human use when Columbus arrived. For comparison, in Europe 23% and in China 20% of land had been used by humans at the time. This changed in the decades after Europeans first set foot on the island of Hispaniola in 1492 – now Haiti and the Dominican Republic – and the mainland in 1517. Europeans brought measles, smallpox, influenza and the bubonic plague across the Atlantic, with devastating consequences for the indigenous populations. Our new data-driven best estimate is a death toll of 56m by the beginning of the 1600s – 90% of the pre-Columbian indigenous population and around 10% of the global population at the time. This makes the “Great Dying” the largest human mortality event in proportion to the global population, putting it second in absolute terms only to World War II, in which 80m people died – 3% of the world’s population at the time. A figure of 90% mortality in post-contact America is extraordinary and exceeds similar epidemics, including the Black Death in Europe – which resulted in a 30% population loss in Europe. One explanation is that multiple waves of epidemics hit indigenous immune systems that had evolved in isolation from Eurasian and African populations for 13,000 years. Native Americas at that time had never been in contact with the pathogens the colonists brought, creating so-called “virgin soil” epidemics. People who didn’t die from smallpox, died from the following wave of influenza. Those who survived that succumbed to measles. Warfare, famine and colonial atrocities did the rest in the Great Dying. This human tragedy meant that there was simply not enough workers left to manage the fields and forests. Without human intervention, previously managed landscapes returned to their natural states, thereby absorbing carbon from the atmosphere. The extent of this regrowth of the natural habitat was so vast that it removed enough CO₂ to cool the planet. The lower temperatures prompted feedbacks in the carbon cycle which eliminated even more CO₂ from the atmosphere – such as less CO₂ being released from the soil. This explains the drop in CO₂ at 1610 seen in Antarctic ice cores, solving an enigma of why the whole planet cooled briefly in the 1600s. During this period, severe winters and cold summers caused famines and rebellions from Europe to Japan.  The modern world began with a catastrophe of near-unimaginable proportions. Yet it is the first time the Americas were linked to the rest of the world, marking the beginning of a new era. We now know more about the scale of pre-European American populations and the Great Dying that erased so many of them. Human actions at that time caused a drop in atmospheric CO₂ that cooled the planet long before human civilisation was concerned with the idea of climate change.  Such a dramatic event would not contribute much to easing the rate of modern global warming, however. The unprecedented reforestation event in the Americas led to a reduction of 5 parts per million CO₂ from the atmosphere – only about three years’ worth of fossil fuel emissions today."
"When Luxembourg announced recently that all public transport in the country will be free from next year, this radical move was received with astonishment. After all, most nations would surely shy away from putting such strain on public finances and from antagonising those taxpayers who don’t use public transport.  But supporting public transport is almost always good for the environment. So, if the finances add up, does this mean that the case for free public transport is a no-brainer? Economists like me view subsidies (or taxes) on specific goods as ways to better align people’s decisions with what is best for society as a whole. The key question is whether free public transport is a good way of achieving this. When thinking about whether to buy any item such as a book or an apple, we usually compare how much we enjoy using this item with what we must pay for it. In most cases, if the item is supplied within a competitive market, the price that we pay for something largely reflects society’s cost of producing it, such as the use of natural resources or labour.  This is not the case for driving a car, however. In addition to our own private costs for petrol and wear and tear, every car ride imposes costs on other people by polluting the air and congesting the roads. Few of us would want to fully account for these social costs when deciding whether to use the car to do the school run or the groceries. Therefore, people will often find that the benefit of another car ride exceeds the private cost, even when social costs – that pollution and congestion – exceed any social benefit. In other words, people will use their cars too much from society’s point of view. The same reasoning applies for a person’s choice between private and public transport. If I think about whether to take the car to get to work, I will compare the benefits and costs to me with the next best alternative, which may be to take the bus or train.  But my use of public transport affects other people much less than if I travelled by car: per user, public transport causes much less additional road congestion and air pollution than a car. Yes, if too many people take the bus it may get overcrowded, but once a specific service is consistently over capacity, the bus operator can add more services. But as most people base their decisions on their own cost on benefits rather than those they impose on other people, the decision between public and private transport will typically be biased against public transport. The economic idea of subsidising public transport is to level the playing field between these options. If the subsidy is equal to the difference in other people’s cost of me driving the car versus taking the bus, my decision on the mode of transport will be aligned with society’s best interest. So, are the environmentalists right after all? Let’s have a look at Luxembourg. Public transport in the small, wealthy country is already dirt cheap – a two-hour ticket with unlimited journeys is just €2 – but road congestion is still among the worst worldwide. It seems Luxembourgish commuters are still choosing to spend hours on a congested road, even though they could easily afford the train.  Partly this is because, in general, individual traffic is more convenient than public transport, as car drivers can travel independently of timetables, train lines or bus routes. Therefore, a denser network or more frequent timetable may be a more effective way of getting people out of their cars than an even higher subsidy. Furthermore, when cheap public transport induces commuters to leave their cars at home, roads get less congested. However, this may make driving into the city more attractive for people who otherwise would have stayed at home, or more people may choose to live on the outskirts rather than in the city centre if commuting gets more convenient or cheaper. This demonstrates a fundamental dilemma of transport policy: as soon as traffic problems are relieved, even more people will want to travel. Therefore, those who are sceptical of entirely free public transport do have a point. An alternative way of levelling the playing field between car driving and public transport without inducing even more people to travel is to increase the petrol tax. Indeed, petrol prices in Luxembourg are markedly lower than in neighbouring Germany, Belgium and France, which may well contribute to Luxembourgers’ reliance on cars. In times of ever more alarming news about global warming, every car that won’t be driven as a result of free public transport is an achievement. However, an optimal policy needs to carefully balance subsidies for public transport use with petrol taxes and investments in the public transport network. 


      Read more:
      Luxembourg's free public transport sounds great, but it won't help people get from A to B


"
"The flow of the Colorado River is dwindling due to the impacts of global heating, risking “severe water shortages” for the millions of people who rely upon one of America’s most storied waterways, researchers have found. Increasing periods of drought and rising temperatures have been shrinking the flow of the Colorado in recent years and scientists have now developed a model to better understand how the climate crisis is fundamentally changing the 1,450-mile waterway.  The loss of snow in the Colorado River basin due to human-induced global heating has resulted in the river absorbing more of sun’s energy, thereby increasing the amount of water lost in evaporation, the US Geological Survey scientists found. This is because snow and ice reflect sunlight back away from the Earth’s surface, a phenomenon known as the albedo effect. The loss of albedo as snow and ice melt away is reducing the flow of the Colorado by 9.5% for each 1C of warming, according to the research published in Science. The world has heated up by about 1C since the pre-industrial era and is on course for an increase of more than 3C by the end of the century unless planet-warming emissions are drastically cut. For the Colorado this scenario means an “increasing risk of severe water shortages”, the study states, with any increase in rainfall not likely to offset the loss in reflective snow. The magnitude of the Colorado’s decline as outlined in the Science paper is “eye popping”, according to Brad Udall, a senior scientist at Colorado State University and an expert on water supplies in the west who was not involved in the research. “This has important implications for water users and managers alike,” Udall said. “More broadly, these results tell us that we need to reduce greenhouse gas emissions as soon as we possible can. “We’ve wasted nearly 30 years bickering over the science. The science is crystal clear – we must reduce greenhouse gas emissions immediately.” The Colorado rises in the Rocky Mountains and slices through ranch lands and canyons, including the Grand Canyon, as it winds through the American west. It previously emptied into the Gulf of California in Mexico but now ends several miles shy of this due to the amount of water extraction for US agriculture and cities ranging from Denver to Tijuana. The river’s upper basin supplies water to about 40 million people and supports 16m jobs. It feeds the two largest water reserves in the US, Lake Powell and Lake Mead, with the latter supplying Las Vegas with almost all of its water. Snowpacks that last into late spring have historically fed streams that have nourished the Colorado River, as well as reducing the likelihood of major fires. As the climate heats up, the river is evaporating away and the risk of damaging wildfires is increasing. The climate crisis is compounding existing threats to the river, which include intensive water pumping for agriculture, water use by urban areas and the threat of pollution from uranium mining. Lake Mead, the vast reservoir formed by the Hoover dam, has dropped to levels not seen since the 1960s. A 19-year drought that racked stretches of the river almost provoked the US government to impose mandatory cuts in water use from the river last year, only for seven western states to agree to voluntary reductions. The problems are set to become more severe, however, as the climate becomes hotter and drier at a time when demand for water from expanding cities in the American west increases."
"
Al Gore Leaves The Light On For Ya
From Nashvillepost.com
By Kleinheider
The “312” is his address – 312 Lynnwood Blvd. Nashville
Even during Earth Hour. President of the Tennessee Center For Policy Research Drew Johnson takes a Saturday drive by Al Gore’s during the time most environmentalists went dark:
I pulled up to Al’s house, located in the posh Belle Meade section of Nashville, at 8:48pm – right in the middle of Earth Hour. I found that the main spotlights that usually illuminate his 9,000 square foot mansion were dark, but several of the lights inside the house were on.
In fact, most of the windows were lit by the familiar blue-ish hue indicating that floor lamps and ceiling fixtures were off, but TV screens and computer monitors were hard at work. (In other words, his house looked the way most houses look about 1:45am when their inhabitants are distractedly watching “Cheaters” or “Chelsea Lately” reruns.)
The kicker, though, were the dozen or so floodlights grandly highlighting several trees and illuminating the driveway entrance of Gore’s mansion.
I [kid] you not, my friends, the savior of the environment couldn’t be bothered to turn off the gaudy lights that show off his goofy trees.
More here
Here’s a look at Al Gores Nashville mansion:
Gore's Mansion in Nashville 

Vice President Al Gore has purchased this home, in Nashville’s exclusive Belle Meade section, for a reported USD2.3 million. The deed for the Colonial-style home, which sits on 2.09 acres of some of the city’s most expensive land, was signed on June 17, 2002. Gore and his wife, Tipper, will keep other homes in Tennessee and Virginia. It was published February 28, 2007 that research group in Tennessee, where the former vice president lives, claims that Mr Gore’s 20-room, eight-bathroom home in Nashville consumes more electricity in a month than the average American household uses in a year.
Photo and description Source: Daylife
You can see it here on Google Maps
From an aerial view looking south you can see what could be a handful of solar panels, though the orientation is puzzling if that is what they are. Update: in comments it it pointed out that they may also be skylights, which seems more probable. So it appears there are no solar panels on Mr. Gore’s home. Note the SUV fleet.
From Microsoft Live Earth - click image for an interactive view
Here is a view looking east:
From Microsoft Live Earth - click image for an interactive view
UPDATE: The photos above don’t show solar panels, however an alert commenter found this photo showing the placement on the one flat section of roofing shown in the aerial views above:
Solar panels are seen on the roof of the home of former Vice President Al Gore in Nashville, Tenn. , Thursday, June 7, 2007. Gore, the environmental activist stung by criticism over his house's energy efficiency, said Friday that renovations are nearly complete to make it a model ""green"" home. Earlier this year, a conservative group criticized Gore, citing electric bills that were far more than the typical Nashville home. Utility records showed the Gore family paid an average monthly electric bill of about $1,200 last year for its 10,000-square-foot home. Source: AP
The 34 panels look to be between 200 and 250 watts each, for a total capacity at full sun of 6.8 to 8.5 kilowatts for the system.They will provide an offset, but will not fully replace energy consumption there. Given the 10,000 sq foot size and the pool, this is an undersized installation for the home. Some ground based panels would have helped.
– Anthony

Sponsored IT training links:
We offer guaranteed success in OG0-093 exam using latest 1z0-007 dumps and 70-272 sample tests



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e977d2c01',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterBy Ed Caryl
In attempting to keep A Light in Siberia as short as possible, the how and why of some points were not included. This led to some comments calling into doubt some of the results. I would like to clarify some of those points.
The choice of baseline period for the surface temperature anomaly map
The 1933 to 1963 baseline for the surface temperature anomaly map was chosen for two major reasons. First, that period includes the peak of the last warm period before the present one. Second, that period was before most of the UHI warming took place for the arctic stations studied, making them show up on the map as red or orange grid-squares. The Arctic and Antarctic stations are highlighted. If you choose the modern warm period, 1979 to 2009, the baseline period includes much of the UHI warming, and the anomalies are much less pronounced.
GHN GISS 1933 - 1963
GISS 1979 - 2009
 
The satellite temperature map shows the arctic warming
Yes, it sure does. The reason is that the bottom of the AMO cycle was just prior to the beginning of the satellite measurements (1979). The arctic has been warming since then. If the satellites had been first launched in 1940 it would be a different picture. In 2050 it will be a different picture. These cycles are 70 years long, the biblical “three-score and ten”. Our main problems with studying climate are that we don’t live long enough to remember more than part of one cycle, and the satellite era has only been 31 years.


The selection criteria for “Urban” versus “Isolated”
“Urban” – Next to or in a town or research station with growth, or change in population over time, or change in heat generated over time.
“Isolated” – A location with no significant population or heat generator changes over time, a stable unchanging environment. An isolated location is one that never had more than one or two buildings, and with always the same size staff, and no adjacent town.
Remember, in the Arctic, in the winter, the environment around all these locations is very cold, bleak, desolate, and unpopulated. A steam-heated town or research station will stand out in the infrared like a bonfire in a desert.
 The averaging of station data
The stations discussed all have data over different time periods, and have average temperatures that are different. Some have gaps in the data. How can these be averaged without distortion?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The data was downloaded from GISS as text files. These were dropped into Excel spreadsheets, and converted into column-delimited files. The monthly data was discarded, as the annual average has already been computed by GISS and is the rightmost column. Data gaps in GISS files are marked by the entries 999.9. These cells were cleared.
After putting all the stations into one spreadsheet with the total year span in the leftmost column, and each station with its own column, aligned with the correct years, each station column was averaged using the SUM of the column divided by the COUNT of the cells in each column with data. Then the average of all the columns was computed. This number is then the average of all the temperatures in all the stations over the whole time period. Call that the “table” average.
The next step was to “normalize” the data for each station by subtracting the “table” average from each column average. This results in a normalization factor for each column. That normalization factor was then subtracted from each value in that column. The normalization factor will be different for each station.

 The rest is the data for all the stations now plot right over each other, in a narrow range, and now can be averaged across the rows in the same manner as the columns were averaged, using SUM divided by the COUNT for each row. Those adjacent years with many stations reporting data get a somewhat smoother plot than those years that have only one or two stations with data, but there is no distortion in the average from some stations being much warmer or cooler than the others.

The R2 question
R-squared value definition:
The R-squared value, also known as the coefficient of determination, is an indicator that ranges in value from 0 to 1 and reveals how closely the estimated values for the trendline correspond to your actual data. A trendline is most reliable when its R-squared value is at or near 1.
The above definition is cut and pasted unedited from the Excel Help files. What it means is that if the data and the trend-line coincide, as above, (if the data plotted is a straight line and the trend is coincident) then the R2 value would be 1.

Our very noisy data, with the AMO sine-wave-like curve superimposed, has a trend line with an extremely low R2 value, because it has a very low correspondence to the data. The trend has very little statistical significance, thus little or no warming is indicated.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterSweden’s meteorological agency reports that ice coverage on the Baltic Sea is greater than it’s been in nearly a quarter century. Read more here.
About 250,000 square kilometres of the Baltic Sea are now covered in ice according to the Swedish Meteorological and Hydrological Institute (SMHI).
The last time so much of the Baltic was frozen was the winter of 1986-87, when ice covered nearly 400,000 square kilometres of the sea’s surface.
SMHI warns that ice coverage on the Baltic could expand further in the coming days, possibly setting a new record.
The area of interest is in the circle.
Most Baltic Ice in 25 years.
Share this...FacebookTwitter "
"Mike Bloomberg has donated hundreds of millions of dollars to environmental advocacy causes, but his campaign is coming under fire for a climate plan that lags far behind the other Democratic candidates for president. In the latest televised Democratic presidential debate on Wednesday night, Bloomberg said he wouldn’t “go to war with China” over its highest-in-the-world carbon emissions. He called fracked gas a “transition fuel”, and said that “we want to go to all renewables, but that’s still many years from now”. But, he added: “The world is coming apart faster than any scientific study had predicted. We’ve just got to do something now.”  The former New York City mayor has committed to rejoining the international climate agreement that Donald Trump plans to exit. He says he would cut carbon pollution in half by 2030 and pursue a 100% clean energy standard, both on par with what scientists say is needed. But he has offered few details on how he would achieve those goals compared with his competitors. He has broadly pledged to financially support new energy technology, nix fossil fuel tax subsidies and issue a new federal rule for power plant emissions. One organization, the Center for Biological Diversity’s Action Fund, ranks Bloomberg’s plan last out of six contenders – tied with the Minnesota senator Amy Klobuchar. By comparison, the group gives the Vermont senator Bernie Sanders a perfect score. Sanders would declare a national climate emergency, end fossil fuel production and exports, ban fracking for fossil gas, prosecute oil companies for their role in the crisis and give the public – rather than private companies – control of the power system. Brett Hartl, chief political strategist for the Center for Biological Diversity’s Action Fund, said Bloomberg, who came to the race late and is massively outspending his peers on advertising, is trying to mask his inadequate plan with “snazzy climate ads”. “There’s no discussion about how he would actually accomplish any of this and most of it is frankly sort of vague things that actually Congress would have to do,” Hartl said. “It’s just not very ambitious, there aren’t a lot of details and when you look at all of the other candidates it’s probably the least fleshed-out plan of any of them.” Bloomberg spokeswoman Daphne Wang said: “When it comes to comparing the candidates’ plans, most have similarly aggressive targets for decarbonizing the US economy, but we think that Mike has a more detailed and concrete, practical plan for how to actually meet those goals.” She added: “His plans look at every possible lever for climate action – using executive authority, budget and tax bills, and appropriations bills, not just legislation – so that he can maximize climate progress regardless of whether the Senate is in Democratic control.”  Wang said some rankings are focused on shutting down fossil fuels, ceasing production and exports, but “the difference with these groups’ priorities is a difference of strategy, not goals”. “These rankings assume that the most effective way of reducing fossil fuel usage is to limit the supply; Mike thinks the way for the world to move beyond coal, oil and gas is to replace them with something cleaner,” she said. She added that “unlike most of his primary rivals, Mike has actually meaningfully reduced emissions”, including by helping to retire more than half the US coal fleet with contributions to the Sierra Club. While the climate crisis is increasingly important to voters, it falls behind other concerns, particularly about the state of healthcare in the US, according to a new poll from Politico and Harvard’s TH Chan School of Public Health. A little more than half of respondents, and 68% of Democrats, said that “making major increases in federal spending and regulation to reduce climate change” is an “extremely” or “very important” priority. But some advocates worry that without further review voters will assume Bloomberg’s climate plan is sufficient because of his background. Bloomberg is the only candidate who hasn’t committed to an overall climate spending level, which his campaign said is because his various policy plans are related and can only be assessed once they are all rolled out. Sanders says his blueprint would cost $16tn. At the lower end of the range candidates have pledged, Klobuchar would spend $1tn on energy infrastructure. Bloomberg also doesn’t specify a date by which he would phase out fossil fuels, although he says his first priority would be to issues a series of executive actions to reach 80% clean electricity within eight years. A high-profile philanthropist, Bloomberg is hugely influential in environmental advocacy, having spent big on many groups, including the Sierra Club’s Beyond Coal campaign which funded lawyers to bring legal battles against coal plants at the state level. Last summer Bloomberg pledged $500m to a new program called Beyond Carbon, aimed at closing every coal plant in the country and halting the growth of gas-powered electricity. As one of the richest people in the United States, Bloomberg has also built a large political and support network for local leaders, and he has won endorsements from high-profile mayors, including London Breed in San Francisco. By entering the race late, Bloomberg escaped scrutiny of his major policy proposals. He also dodged pressure from activists such as the youth-run Sunrise Movement, which argues that despite Bloomberg’s record of climate advocacy he’s not aligned with the “vision and values” of a Green New Deal – a broader reimagining of American society that would address both inequity and the climate crisis – which Sanders outlined and co-sponsored with the New York congresswoman Alexandria Ocasio-Cortez. The Sunrise Movement – which has endorsed Sanders – has criticized Bloomberg, a former Republican, for fighting labor unions and backing the re-election of Michigan’s former Republican governor Rick Snyder following the water crisis in Flint. The group says Bloomberg’s history of supporting racial profiling through New York’s stop-and-frisk programs, which he has since renounced, is at odds with a Green New Deal. And the organization has highlighted how he has backed gas as a replacement to coal. Coal has a higher carbon output than gas, but gas is increasingly a major threat to climate progress. Bloomberg would not ban fracking, a method of extraction that has made a huge amount of cheap gas available to drillers. Bloomberg’s campaign pushed back on the criticism, arguing he supports the right to form unions and has a “progressive agenda for criminal justice reform”. “While gas played a role in the early stages of retiring coal plants because it was less polluting and cheaper for consumers, once renewable energy became even cheaper than gas, Mike announced his investment in Beyond Carbon – and his goal to stop the expansion of gas plants,” his campaign said. Bloomberg also supported the Keystone XL pipeline which would move petroleum from oil sands in Canada. Julian Brave NoiseCat, director of Green New Deal strategy for the thinktank Data for Progress, said Bloomberg’s late entry has allowed “for a very easy narrative for him, which is that he’s a big philanthropist that has put his money where his mouth is and done a lot for funding of climate [work]. “We need a more substantive conversation about what the difference is between what looks like the two frontrunners,” NoiseCat said. He said he “wouldn’t be surprised if more and more activists show up to his rallies and take him to task”. On the other hand, he added, “having someone who could spend basically unlimited money on paid media and could leverage his massive philanthropy and political network to just mobilize the gears on climate and make something happen is worth sitting with for a minute”."
"
Share this...FacebookTwitterI found this comment from a reader who calls him(her)self “wordsmith”, posted under my “Contact” sidebar. Because it’s Christmas Eve and things are a little hectic, I thought maybe some readers could give me a hand and help out this poor warmist. Here’s his(her) comment:
It’s amazing to me when I hear Climate Change skeptics talk about the misinformation from those who believe it is happening, that it’s primarily caused by man, and that the consequences will be significant. What would be the motivation for so many scientists, the vast majority of scientists, far more educated and intelligent than either of us, to spread misinformation? Do you believe they’re just stupid? More so than you? Is there some economic benefit for them? Do you discount what all scientists say? Also, give me a break–an engineering degree does not in any way qualify you to be a climate expert. I work with a lot of engineers and you don’t have to be that bright to get an engineering degree, especially from some no-name university. MIT, maybe, but not the ones you attended.
He seems annoyed that I’m expressing myself freely at this blog.
Share this...FacebookTwitter "
"

I’m willing to wager two things. First, I’ll bet that anyone who said global warming is an overblown bunch of hooey had a terrible time at this year’s holiday cocktail parties. Second, I’ll take even money that the 10 years ending on December 31, 2007, will show a statistically significant global cooling trend in temperatures measured by satellite. 



Those two bets are related. Our greener friends were in high attack mode on the party circuit last month as TV newscasts presented a relentless nightly stream of reports on how 1998 was the warmest year since gosh knows when. NBC got British scientist Phil Jones to say that it was the warmest year of the millennium, even though the thermometer was invented only about three‐​quarters of the way through it. The figures he used were estimates based mainly on tree ring measurements prior to 1900 (that’s 900 of the last 1000 years), and those estimates are known to miss an enormous amount of true temperature variation. 



As far as 1998 was concerned, the wire stories correctly noted (for a change) that all three true temperature histories — from ground‐​based thermometers, weather balloons in the lower atmosphere and satellite soundings of the lower layers — showed the same thing: record temperatures. The satellite history is 20 years long, the weather balloon history is 42, and reliable global ground‐​based temperature data really extend back only about 55 years. But they all were higher than kites. 



The many hours spent explaining to folks that 1998 blew the top off the record because heat from the big El Niño burped its way out to space could easily have been saved if I’d just had a vest‐​pocket temperature history handy. To save you a similar investment of time, there’s one included with this article. It’s intended to wrap around your business card (or if you’re not working, a credit card) for easy access at the next party. 



Brandish the card while you’re being jabbed into the corner by a sharpened carrot stick (or, God forbid, a Ranch‐​sauced bit of broccoli) by some punk wearing a Phish shirt. What he’ll see is absolutely no warming trend whatsoever from when the satellite measurements began (January 1979) through the end of 1997, followed by one warm year in 1998.  




Our greener friends were in high attack mode on the party circuit last month as TV newscasts presented a relentless nightly stream of reports on how 1998 was the warmest year since gosh knows when. 



The stunning blip in 1998 is just that — a blip. Close examination (data shown here run through November 1998) shows that temperatures have dropped back down to the levels typical of 1979–97. 



Those who follow global warming know that two California scientists recently found a problem with the satellite temperatures. The data shown on the chart have been corrected by NASA scientist Roy Spencer. 



Last year was so warm that it induces a statistically significant warming trend in the satellite data. Thus the second bet: Starting with 1998, there will almost certainly be a statistically significant cooling trend in the decade ending in 2007. 



In part, rapid cooling in late 1998 explains the record level of hysteria accompanying the global warming stories in the last half of December. Anyone pushing for climate legislation knows that it’s now or never, because the warming is over. 



If the Kyoto protocol doesn’t pass in the heat of this particular moment, it never will. So maybe readers will want to make copies of the little wallet card and send a few to their friends here in Washington. Or perhaps if you come to town for a little social gathering, you can pull it out and show it around. And, while we’re at it, any takers on my wagers?
"
" When Josep Borrell, the EU’s newly appointed foreign policy chief, recently caused outrage by dismissing young climate activists as flaky sufferers of “Greta syndrome”, he made not just a serious error of judgment but a serious mistake in macroeconomics. It was a mistake that is symptomatic of the dire state of European economic debate after a decade of austerity and schwarze Null (balanced budget) ideas.  “The idea that young people are seriously committed to fighting climate change – we could call it the ‘Greta syndrome’ – allows me to doubt that,” Borrell said, before going on to question their naivety about the cost of tackling the climate crisis. “I would like to know if young people demonstrating in Berlin … are willing to lower their living standards to offer compensation to Polish miners, because if we fight against climate change for real they will lose their jobs and will have to be subsidised.” For Borell, greening the European economy is a zero-sum game, in which paying for the move to a low-carbon economy must come out of the pockets of German taxpayers. Borrell’s views, however unpalatably delivered, are perfectly aligned with the flawed macroeconomics of the much-vaunted European Green Deal. The clue is in the name. The European Green Deal is the European commission’s proposed €1tn plan to finance the transition away from fossil fuels to decarbonising Europe’s economy. But the commission quietly dropped the word “new” from original US plans for a green new deal, which of course echo Franklin D Roosevelt’s Depression-era economic New Deal. Losing that “new” is a signal that the commission does not seek system change through ambitious green macroeconomics and tough regulation of carbon financiers. Rather, it takes a politics as usual, third-way approach that seeks to nudge the market towards decarbonisation. The macroeconomics of the European Green Deal remains trapped in the black zero logic of austerity. Instead of ambitious green fiscal activism, it mostly reshuffles existing European funds through a logic of seed funding to mobilise private sector money. Public money will be used to take risk out of private business activities and finance a “just transition” mechanism that promises to protect groups like Polish miners after their coal mines close through retraining and reskilling programmes. But there is little guarantee that European taxpayer money will reach Polish miners. It will probably go into the pockets of decarbonisation “barons”: clever local elites who will funnel transition money to their businesses, just as land barons siphoned most of the subsidies originally intended for small farmers under the common agricultural policy. Take Romania. Mining unions there complain that measures intended to “reskill’ miners, tested in the Valea Jiului region in Transylvania for the past 15 years, solely benefitted decarbonisation firms. Their connections to Romania’s political elites allowed them to capture the “market” for reskilling services, but private investment and jobs in new economic sectors never actually materialised. In dismissing green macroeconomics, the European commission puts its hopes on private finance. The logic is that the state won’t have to pay if the private sector will, provided there is nudging from public funds to “derisk” green investments. Here, the commission seems to have powerful allies, such as institutional investors with trillions ready to be greened. Larry Fink, the head of BlackRock, one of the world’s largest asset managers, recently noted that “we are on the edge of fundamentally reshaping finance” by taking decarbonisation seriously. The turn to green finance is a welcome step given that BlackRock and other global investors have so far behaved more like greenwashing carbon financiers than responsible climate investors: talking green while consistently blocking climate shareholder resolutions. But the danger is that the public money the commission plans to put into greening the European economy will instead merely subsidise greenwashing. Think of it as a two-step strategy through which carbon financiers can turn climate into a profitable business. The first step involves shaping the rules of the game, such as the “green list” of assets (or “green taxonomy”) currently being negotiated by the EU institutions. The EU taxonomy of sustainable activities has important advantages over the private environmental ratings (known as ESG ratings) currently used by private finance to identify green assets. Drawing on a broader range of views, including climate experts, the EU taxonomy sets a public standard of green that makes it more difficult for carbon financiers to purchase green ratings privately. Done properly, it could become a global standard for measuring and regulating the environmental performance of global finance. But the EU list risks being watered down in the ongoing political negotiations over the exact details of what constitutes “green” activities. Already, furious lobbying has led to the inclusion of a category of “enabling” activities under the auspices of “pathways to green”. These could easily become loopholes for activities that are more brown than green. The incentive for carbon financiers is to stick the label green everywhere they can in preparation for the second step: persuading European regulators to promote (de-risk) green assets. Meanwhile the commission refuses to talk about – let alone regulate – “brown finance”. Yet the strict regulation of brown finance could be a powerful tool for financing the European Green Deal. The commission could impose penalties on brown assets, either through taxation (a green FTT) or regulation, thus accelerating the switch to green assets. Those outraged by Borrell’s dismissive remarks about Greta Thunberg’s generation should note that the real political battle is to ensure that the European Green Deal does not morph into the first greenwashed social pact between regulators and carbon financiers, between Brussels and local elites, exporting greenwashed finance standards to the rest of the world. Climate activists should be pushing for a complete green economic agenda that recognises the critical role of green fiscal activism in organising the transition to low-carbon. It also means protecting public finances from carbon financiers, ensuring instead that private finance becomes the first lever in the climate fight. • Daniela Gabor is professor of economics and macrofinance at UWE Bristol "
"

As the list of Democratic presidential candidates grows, so do their promises. So far, the candidates have largely embraced the same policy focus: expanded entitlement spending to guarantee new welfare benefits.



Massachusetts Sen. Elizabeth Warren recently endorsed a universal federal provision of child care. Vermont Sen. Bernie Sanders has long supported “Medicare for All,” and businessman Andrew Yang is perhaps best‐​known for his advocacy of a universal basic income. Meanwhile senators Cory Booker, Kamala Harris, Kirsten Gillibrand and others have all backed the Green New Deal, which promises to address climate change and inequality by providing universal health care and creating millions of jobs.



While reasonable people can disagree on some aspects of these proposals, one fact is uncontroversial: the United States cannot afford them.





Given that the US cannot afford its existing entitlement programs, adopting the massively expensive policies proposed by Democratic candidates would be foolish.



Congressional Budget Office projections of the federal debt make this point compellingly. According to CBO projections, federal debt held by the public, currently at 78% of America’s gross domestic product, or GDP, will approach 100% in the next decade and reach 152% by 2048. Cutting the debt‐​GDP ratio to its 1957–2007 average within 25 years would require policymakers to permanently reduce spending or increase taxes by 3.8% of GDP, which amounts to about $800 billion, or 24% of federal revenue.



Distinguished economists have recently argued that debt may be less costly in the future because of low interest rates. This assumes we can forecast future rates. In reality, estimates of long‐​run interest rates differ widely and are highly uncertain. Rates have stayed low over the last decade due to a combination of factors, such as monetary policy, weak foreign demand and deleveraging in the wake of the 2008 recession. These forces are unlikely to play as important a role in the future.



More importantly, while low interest rates might permit running a long‐​term deficit that is stable relative to GDP, standard forecasts, such as those from CBO, project rising deficits.



Some Democrats have also claimed that federal debt is not a constraint by relying on “Modern Monetary Theory” (MMT), which argues that central banks can issue enough money to fund federal expenditures with little threat of inflation. But the stable inflation of recent years is precisely due to central banks’ intentional pursuit of price stability as the primary objective of monetary policy. If monetary policy were to focus on funding government spending instead, the threat of inflation would increase rapidly.



Several Democrats have also advocated increasing taxes, but the revenue generated would not come close to funding their proposals. Warren has campaigned for two new wealth taxes that are estimated to increase federal revenue by about $210 billion annually. New York Rep. Alexandria Ocasio‐​Cortez supports a 70% marginal tax rate on income over $10 million, which is estimated to generate an additional $20 billion to $70 billion per year, assuming that wealthy Americans are not discouraged from working. Even if the US adopted all three of these new taxes, annual federal revenue would increase by at most $280 billion.



The additional revenue would hardly make a dent in the cost of Democrats’ policy proposals. For example, Yang’s plan for a universal basic income is estimated to cost $3.8 trillion annually, and the Green New Deal would likely cost upwards of $6.6 trillion per year. Revenue from all three tax increases would fund less than 10% of either of these programs, let alone help pay down existing debt.



Supporters claim that although Medicare for All would massively increase government spending, it would be offset by decreases in private spending that leave total health care expenditures unchanged. While this is plausible in the short run, little evidence suggests that public health care spending would grow more slowly than private spending in the future. Public health care spending in the US has grown faster than private spending over the past 30 years, and other countries with publicly‐​funded health care systems have continued to see increases in expenditures as a share of GDP. An aging US population is likely to increase health care costs in the future, regardless of whether those costs are paid by government.



Given that the US cannot afford its existing entitlement programs, adopting the massively expensive policies proposed by Democratic candidates would be foolish. Instead, the federal government should cut entitlement spending and look for cheaper ways to address problems like climate change.



Instead of the Green New Deal, the federal government could adopt a revenue‐​neutral carbon tax to decrease emissions without exacerbating the fiscal imbalance. Economists from across the political spectrum supportcarbon taxation as the most cost‐​effective way to address climate change. And a carbon tax would be most effective if uniformly adopted by other countries, too.



At a minimum, the US should slow the growth of entitlement spending to no more than the average growth of GDP. Reasonable approaches include increasing the age of eligibility for Social Security and Medicare, modifying the indexation of Social Security benefits or tightening eligibility requirements for disability insurance. These reforms would keep entitlement spending affordable, unlike current policy.



Reasonable people can disagree on the benefits of federal welfare programs, the appropriate level of redistribution or optimal cost‐​cutting reforms. But everyone should agree that restoring fiscal sanity in the United States requires significant cuts to federal entitlement spending. The policies advocated by Democratic candidates will only make things worse.
"
"
Share this...FacebookTwitterEuropean particle physics research center CERN located near Geneva, Switzerland has gotten its budget cut by 6% over the next five years. Europe no longer has the money to fund it. 
It was just a question of time. With so much money being pissed away on bogus climate research and save-the-world projects, eventually you run out of funds to pay for real science and research.
How many billions have been poured into researching the non-problem of climate change? The IPCC? Hansen’s GISS surface station folly? Green subsidies? The list is endless.
What benefit have we gotten? I’d say we’ve gotten a lot more damage than benefit, especially if you consider what could have been done with the money instead. Lost opportunities.
Swiss radio has a report (German): CERN budget cut and writes:
The money worries of the European countries is now adversely impacting the European CERN research facility in Geneva. Over the next 5 years 343 million Swiss Francs have to be saved – about 6 percent of the budget.
While the management of CERN say the financial measure is painful, it will be bearable. But the research laboratory’s association of employees has a different opinion: The future of CERN is being put at risk.
Here’s my advice to CERN employees who fear losing their jobs: Call your elected officials and tell them stop wasting so much on bogus climate science.
Share this...FacebookTwitter "
"
More harbinger of the Northern Hemisphere winter to come?







A bulldozer cleans snow on the Sichuan-Tibet  road in Nyingchi, southwest China’s Tibet Autonomous Region Oct. 30, 2008.  (Xinhua Photo)





LHASA, Oct. 30 (Xinhua) — The death toll has risen to seven, and one person  remains missing, as a result of the worst snowstorm on record in Tibet, local  authorities said Thursday. 
 The seven people killed either frozen to death or were crushed by  collapsing buildings. About 144,400 heads of livestock died in the storm, which  also knocked out telecommunications and traffic in parts of Shannan prefecture. 
In Lhunze County, 1,348 people stranded by damaged buildings or blocked roads  had been rescued, the county government said. Rescue operation for the remaining  289 trapped was still underway. 
 The worst-hit county had 36 consecutive hours of snowfall from Sunday,  with an average snow coverage of 1.5 meters. Four people died and one remained  missing in the snowstorm. 
 The rescued people have been moved to other villages, sleeping in schools  or government buildings. 
 A road linking Lhunze to Cuona County reopened on Thursday after 63 hours  of snow clearing efforts of armed policemen and transportation staff. 
 Cuona had been isolated from the outside for three days due to the road  blockage. 
 The Tibet regional civil affairs department has allocated relief  materials such as clothes and tents to the affected areas.
h/t to Dr. Roger Pielke


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b21998f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The consistent message from those who would seek to exploit shale gas is that it has three distinct advantages over existing forms of fossil fuel energy: it is cheap, it has a lower influence on global warming, and it reduces the reliance in foreign imports. In the UK the ability of shale gas to replace substantial amounts of other energy sources is unproven. The International Energy Agency (IEA) says even high levels of shale gas extraction by the middle of the century would still only leave shale on an equal footing with gas, coal and oil. Claims of low energy prices have been rejected by many commentators including the industry itself. Lord Browne, former head of BP and current chair of the leading shale gas company Cuadrilla, contradicted both the prime minister and the chancellor last year by saying that in the UK fracking would not reduce gas prices. This leaves the mitigation of climate change as the central plank of the shale gas argument. Replacing coal and oil with gas would, it is argued, emit lower carbon dioxide volumes per unit of energy which is a major contributor to global warming.  However, a new report published in the journal Nature shows that this is unlikely to be true either. Haewon McJeon from the US Pacific Northwest National Laboratory along with teams from Australia, Italy, Germany and Austria ran five different models to see what would happen if abundant gas was made available on the market. They concluded it would indeed change things, but it would not mitigate climate change. Such cheap, available gas would affect CO2 emissions by between -2% and +11%. This in turn would lead to a slight increase in climate forcing (imposed disturbances in the earth’s energy balance) according to most models, of between -0.3% and +7%. This means that large-scale shale gas extraction will actually mean more CO2 emitted into the atmosphere, and more human-generated climate change. It’s certainly some way off the minimum 80% carbon reduction climate scientists say we need to achieve from our energy by 2050. The researchers explain that an abundance of gas will lead to higher levels of consumption, even when growing populations and wealth are accounted for. If gas is cheap, people will use more of it. This higher consumption assumes that prices of gas will come down, contradicting the industry view in Europe. However, if prices stay the same as oil and coal, the expectation is that impact in global warming will be considerably worse. The team ran a further unlikely scenario where coal would be effectively outlawed to see the effect of new gas reserves on climate change. This produced CO2 emission reductions of up to -6% – still a long way from the required -80%. Unfortunately, this is a best case scenario. One of the many concerns about fracking for shale gas is that it produces high levels of fugitive methane, another greenhouse gas which has over 20 times the warming properties of carbon dioxide. When the team modelled this, the upper influence of climate forcing goes from +7% to +12% by 2050. The unequivocal conclusion of the team is that: Abundant gas does not discernibly reduce climate forcing … and, under high fugitive emission assumptions, three models reported increased climate forcing of more than 5%. This work confirms what the IEA has known for some time. In May 2012 the agency issued a special report on the exploitation of shale gas called Golden Rules for a Golden Age of Gas. Committed readers needed to go past the executive summary and soldier on until the 90th page before reading that if shale gas was fully exploited it would mean global warming will increase to at least 3.5C, well past the comparative safety and current target of 2C. Politicians have thus far been able to resist the tide of academic works that have shown why exploiting a new source of fossil fuel may not be a particularly great idea. This latest report is particularly stark and has a better chance than most to divert evidence-based policy towards a more sustainable direction."
"
Share this...FacebookTwitterAlex Bojanowski at Germany’s online Der Spiegel reports here on a new paper appearing in Nature that shows climate change in the 1970s was caused by ocean cooling. Climate simulation models once indicated that the cooling in the 1970s was due to sun-reflecting sulfur particles, emitted by industry. But now evidence points to the oceans.

I don’t know why this is news for the authors of the paper. Ocean cycles are well-known to all other scientists. The following graphic shows the AMO 60-year cycle, which is now about to head south.
Atlantic Multidecadal Oscillation (AMO). Source: http://www.appinsys.com/globalwarming/SixtyYearCycle.htm
Computer models simulating future climate once predicted that it would soon get warm because of increasing GHG emissions, but, writes Der Spiegel, citing Nature:
Now it turns out that the theory is incomplete. A sudden cooling of the oceans in the northern hemisphere played the decisive role in the drop of air temperatures.
The paper was authored by David W. J. Thompson, John M. Wallace, John J. Kennedy, and Phil D. Jones. The scientists discovered that ocean temperatures in the northern hemisphere dropped an enormous 0.3°C between 1968 and 1972. Der Spiegel writes:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A huge amount of energy was taken out of the oceans. The scientists said that it was surprising that the cooling was so fast.
 This shows, again, that the climate simulation models used for predicting the future are inadequate. It’s not sure what caused the oceans to cool. But scientists are sure that aerosols were not the cause. Der Spiegel describes a possible scenario how the oceans may have cooled: 
Huge amounts of melt water from Greenland’s glaciers poured into the Atlantic at the end of the 1960s, and formed a cover over the ocean. The melt water cooled the ocean for one thing, and acted to brake the Gulf Stream, which transports warm water from the tropics and delivers it to the north. The result: the air also cools down.
But, as Spiegel reports, that hardly explains why there was also cooling in the north Pacific. Der Spiegel:

 The scientists will have to refine their climate simulations. The new study shows one thing: The influence of the oceans is greater than previously thought.

I’d say that’s a very polite way of saying: Your models have been crap, and it’s back to the drawing board. This time don’t forget to properly take the oceans and every thing else into account. Yes, there’s a quite a bit more to climate than a single trace gas in the atmosphere. Hooray – the warmists are finally beginning to realize it! (Maybe)
Share this...FacebookTwitter "
"

NEW 4/10/09: There is an update to this post, see below the “read the rest of this entry” – Anthony
Guest Post by Richard Lindzen, PhD.
Alfred P. Sloan Professor of Meteorology, Department of Earth, Atmospheric and Planetary Science, MIT 

This essay is from an email list that I subscribe to. Dr. Lindzen has sent this along as an addendum to his address made at ICCC 2009 in New York City. I present it here for consideration. – Anthony
Simplified Greenhouse Theory
The wavelength of visible light corresponds to the temperature of the sun’s surface (ca 6000oK). The wavelength of the heat radiation corresponds to the temperature of the earth’s atmosphere at the level from which the radiation is emitted (ca 255oK). When the earth is in equilibrium with the sun, the absorbed visible light is balanced by the emitted heat radiation.
The basic idea is that the atmosphere is roughly transparent to visible light, but, due to the presence of greenhouse substances like water vapor, clouds, and (to a much lesser extent) CO2 (which all absorb heat radiation, and hence inhibit the cooling emission), the earth is warmer than it would be in the absence of such gases.

The Perturbed Greenhouse
If one adds greenhouse gases to the atmosphere, one is adding to the ‘blanket’ that is inhibiting the emission of heat radiation (also commonly referred to as infrared radiation or long wave radiation). This causes the temperature of the earth to increase until equilibrium with the sun is reestablished.
For example, if one simply doubles the amount of CO2 in the atmosphere, the temperature increase is about 1°C.

If, however, water vapor and clouds respond to the increase in temperature in such a manner as to further enhance the ‘blanketing,’ then we have what is called a positive feedback, and the temperature needed to reestablish equilibrium will be increased. In the climate GCMs (General Circulation Models) referred to by the IPCC (the UN’s Intergovernmental Panel on Climate Change), this new temperature ranges from roughly 1.5°C to 5°C.
The equilibrium response to a doubling of CO2 (including the effects of feedbacks) is commonly referred to as the climate sensitivity.
Two Important Points
1. Equilibration takes time.
2. The feedbacks are responses to temperature – not to CO2 increases per se.
The time it takes depends primarily on the climate sensitivity, and the rapidity with which heat is transported down into the ocean. Both higher sensitivity and more rapid mixing lead to longer times. For the models referred to by the IPCC, this time is on the order of decades.
This all leads to a crucial observational test of feedbacks!


The Test: Preliminaries
Note that, in addition to any long term trends that may be present, temperature fluctuates on shorter time scales ranging from years to decades.

Such fluctuations are associated with the internal dynamics of the ocean- atmosphere system. Examples include the El Nino – Southern Oscillation, the Pacific Decadal Oscillation, etc.
These fluctuations must excite the feedback mechanisms that we have just described.
The Test
1. Run the models with the observed sea surface temperatures as boundary conditions.
2. Use the models to calculate the heat radiation emitted to space.
3. Use satellites to measure the heat radiation actually emitted by the earth.
When temperature fluctuations lead to warmer temperatures, emitted heat radiation should increase, but positive feedbacks should inhibit these emissions by virtue of the enhanced ‘blanketing.’ Given the model climate sensitivities, this ‘blanketing’ should typically reduce the emissions by a factor of about 2 or 3 from what one would see in the absence of feedbacks. If the satellite data confirms the calculated emissions, then this would constitute solid evidence that the model feedbacks are correct.
The Results of an Inadvertent Test
From Wielicki, B.A., T. Wong, et al, 2002: Evidence for large decadal variability in the tropical mean radiative energy budget. Science, 295, 841-844.
Above graph:
Comparison of the observed broadband LW and SW flux anomalies for the tropics with climate model simulations using observed SST records. The models are not given volcanic aerosols, so the should not expected to show the Mt. Pinatubo eruption effects in mid-1991 through mid-1993. The dashed line shows the mean of all five models, and the gray band shows the total rnage of model anomalies (maximum to minimum).
It is the topmost panel for long wave (LW) emission that we want.
Let us examine the top figure a bit more closely.

From 1985 until 1989 the models and observations are more or less the same – they have, in fact, been tuned to be so. However, with the warming after 1989, the observations characteristically exceed 7 times the model values. Recall that if the observations were only 2-3 times what the models produce, it would correspond to no feedback. What we see is much more than this – implying strong negative feedback. Note that the ups and downs of both the observations and the model (forced by observed sea surface temperature) follow the ups and downs of temperature (not shown).
Note that these results were sufficiently surprising that they were confirmed by at least 4 other groups:
Chen, J., B.E. Carlson, and A.D. Del Genio, 2002: Evidence for strengthening of the tropical general circulation in the 1990s. Science, 295, 838-841.
Cess, R.D. and P.M. Udelhofen, 2003: Climate change during 1985–1999: Cloud interactions determined from satellite measurements. Geophys. Res. Ltrs., 30, No. 1, 1019, doi:10.1029/2002GL016128.
Hatzidimitriou, D., I. Vardavas, K. G. Pavlakis, N. Hatzianastassiou, C. Matsoukas, and E. Drakakis (2004) On the decadal increase in the tropical mean outgoing longwave radiation for the period 1984–2000. Atmos. Chem. Phys., 4, 1419–1425.
Clement, A.C. and B. Soden (2005) The sensitivity of the tropical-mean radiation budget. J. Clim., 18, 3189-3203.
The preceding authors did not dwell on the profound implications of these results – they had not intended a test of model feedbacks! Rather, they mostly emphasized that the differences had to arise from cloud behavior (a well acknowledged weakness of current models). However, as noted by Chou and Lindzen (2005, Comments on “Examination of the Decadal Tropical Mean ERBS Nonscanner Radiation Data for the Iris Hypothesis”, J. Climate, 18, 2123-2127), the results imply a strong negative feedback regardless of what one attributes this to.
The Bottom Line
The earth’s climate (in contrast to the climate in current climate GCMs) is dominated by a strong net negative feedback. Climate sensitivity is on the order of 0.3°C, and such warming as may arise from increasing greenhouse gases will be indistinguishable from the fluctuations in climate that occur naturally from processes internal to the climate system itself.
An aside on Feedbacks
Here is an easily appreciated example of positive and negative feedback. In your car, the gas and brake pedals act as negative feedbacks to reduce speed when you are going too fast and increase it when you are going too slow. If someone were to reverse the position of the pedals without informing you, then they would act as positive feedbacks: increasing your speed when you are going too fast, and slowing you down when you are going too slow.

Alarming climate predictions depend critically on the fact that models have large positive feedbacks. The crucial question is whether nature actually behaves this way? The answer, as we have just seen, is unambiguously no.
UPDATE: There are some suggestions (in comments) that the graph has issues of orbital decay affecting the nonscanner instrument’s field of view. I’ve sent a request off to Dr. Lindzen for clarification. – Anthony
UPDATE2: While I have not yet heard from Dr. Lindzen (it has only been 3 hours as of this writing) commenter “wmanny” found this below,  apparently written by Lindzen to address the issue:
“Recently, Wong et al (Wong, Wielicki et al, 2006, Reexamination of the Observed Decadal Variability of the Earth Radiation Budget Using Altitude-Corrected ERBE/ERBS Nonscanner WFOV Data, J. Clim., 19, 4028-4040) have reassessed their data to reduce the magnitude of the anomaly, but the remaining anomaly still represents a substantial negative feedback, and there is reason to question the new adjustments.”
I found the text above to match “wmanny’s” comment in a presentation given by Lindzen to Colgate University on 7/11/2008 which you can see here as a PDF:
http://portaldata.colgate.edu/imagegallerywww/3503/ImageGallery/LindzenLectureBeyondModels.pdf
– Anthony
UPDATE3: I received this email today  (4/10) from Dr. Lindzen. My sincere thanks for his response.
Dear Anthony,
The paper was sent out for comments, and the comments (even  those from “realclimate”) are appreciated.  In fact, the reduction of the  difference in OLR between the 80’s and 90’s due to orbital decay seems to me to  be largely correct.  However, the reduction in Wong, Wielicki et al (2006) of  the difference in the spikes of OLR between observations and models cannot be  attributed to orbital decay, and seem to me to be questionable.  Nevertheless,  the differences that remain still imply negative feedbacks.  We are proceeding  to redo the analysis of satellite data in order to better understand what went  into these analyses.  The matter of net differences between the 80’s and 90’s is  an interesting question.  Given enough time, the radiative balance is  reestablished and the anomalies can be wiped out.  The time it takes for this to  happen depends on climate sensitivity with adjustments occurring more rapidly  when sensitivity is less.  However, for the spikes, the time scales are short  enough to preclude adjustment except for very low sensitivity.
That said,  it has become standard in climate science that data in contradiction to alarmism  is inevitably ‘corrected’ to bring it closer to alarming models.  None of us  would argue that this data is perfect, and the corrections are often plausible.   What is implausible is that the ‘corrections’ should always bring the data  closer to models.
Best wishes,
Dick

Sponsored IT training links:
Best quality 70-448 prep material is available for download. Pass the real exam using JN0-350 guide and E20-361 lab tutorial.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e973bbaff',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"Seagrass is one of the most important coastal habitats where young ocean-going fish such as Atlantic cod can grow and develop before setting out on the journey of life. But these critically important habitats, revealed in new research, are being damaged the world over and its not just threatening biodiversity but our food security. Some 30,000 km2 of seagrass (Zostera marina) has disappeared over the past two decades, about 18% of the global area. This is incredibly important. One hectare of seagrass absorbs 1.2 kilogrammes of nutrients each year, equivalent to the treated effluent of 200 people. It can produce 100,000 litres of oxygen per day, can support 80,000 fish and 100m invertebrates – and absorb ten times as much CO2 as a pristine area of Amazon rainforest. Providing shallow-water habitats where young ocean-going fish can grow and develop is one of the key ecosystem services that our coastal seas provide, but unfortunately we largely don’t recognise the value of them in supporting the fishery resources of vast ocean basins. We continue to allow the loss of this coastal habitat to occur throughout the world – in spite of regulations in many nations to protect key habitats and biodiversity. The diminished status of fisheries for species such as the Atlantic cod (Gadus morhua) means we need to move beyond purely managing adult fish stocks and consider the interaction of individual high-value species within the whole seascape and the critical habitats within that.  This is critical as our declining fishery stocks do not align with the needs of the region or the planet as a whole. To meet the needs of the predicted human population of 2050, an additional 75m tons of protein from fish and aquatic invertebrates will be required – this is a 50% increase in current supply.  The Atlantic cod is a species of significant economic and historic importance but is now better known for its catastrophic decline. Apart from overfishing, the causes of this decline and its subsequent lack of recovery remain largely unresolved. The degree to which specific coastal and shallow-water habitats are important for this species still remains unquantified at the scale of the North Atlantic.   There is extensive evidence of the presence of juvenile Atlantic cod in seagrass throughout the North Atlantic. Juvenile cod have been recorded in such high density in seagrass that they average 246 individuals per hectare. This density of juvenile Atlantic cod is higher in seagrass meadows compared to alternative habitats. This includes an incredible dataset from Norway where researchers have sampled juvenile cod in seagrass annually since 1919 and other recent studies observing juvenile cod in seagrass in North Wales by our team at Swansea University using stereo Baited Remote Underwater Video systems and seine nets.  Juvenile Atlantic cod have greater long-term viability after having spent time in seagrass, which improves their chances of reaching maturity. Our new analysis, published in the open access journal, Global Ecology and Conservation, illustrates how juvenile Atlantic cod grow faster in seagrass than in surrounding alternative habitat types and have higher survival rates from predation. Although juvenile Atlantic cod do not always need to use seagrass meadows as juvenile habitats, it appears that they may intentionally select seagrass as a nursery habitat. This data comes from studies throughout the Atlantic including Newfoundland in the west and Sweden in the east. The study, conducted in collaboration with Richard Lilley at Cardiff University, was an extensive meta-analysis of research on the life history of the Atlantic cod resulting in a review and synthesis of its nursery habitat usage. It includes data sources from throughout the region, ranging from Newfoundland, to Norway, to Scotland and to Sweden.  Our work provides strong evidence that seagrass meadows are of significant importance to contributing to Atlantic cod stocks, and our review presents extensive quantitative evidence of the role of seagrass as valuable nursery habitat for Atlantic cod. These findings are of major significance given the continued threats to these systems.  Seagrass meadows are globally important resources that are being threatened by a whole series of issues ranging from climate change and major weather events to boating activity, poor water quality and coastal development. This work clearly illustrates how key habitats in our coastal seas such as seagrass need protecting, not just for biodiversity but for the continued food security provided by our oceans."
"
Share this...FacebookTwitterThe Portuguese sceptic site Ecotretas has a piece today called Not so fine Mr Gore which features a video clip of Al Gore making a speech before a crowd, predominantly businessmen. Gore seems to think that Portugal’s economy is on the right track and is coming out of recession. Far from it. In fact, Portugal’s economic woes are worsened by Gore’s vision for the planet.
So Ecotretas reminds Gore:
Someone should tell Al Gore that Portugal is not coming out of the Great Recession. In fact, Portugal’s economy is getting worse, as can be seen by the high sovereign risk, usually one of the highest 10 in the world in the last months. This is so because major bad economic decisions have been made in the last years, namely in alternative energy.
The total cost for Portuguese consumers and taxpayers is estimated at 700 million euros this year. But costs are growing exponentially.
Just like Spain, we’re going down, while unemployment keeps going up, and the promised green jobs are one of Europe’s lowest. So, Mr. Gore: If some Portuguese told you that “I feel fine”, I can bet he felt like the farmer in your nasty story!
Gore delivers a humorous story to make a point. Probably a lot of scientists feel like the poor farmer when asked: “Is your data, which show man is causing climate change, fine?”
Share this...FacebookTwitter "
"From the 1950s until recently, we thought we had a clear idea of how continents form. Most people will have heard of plate tectonics: moving pieces on the surface of the planet that collide, pull away or slide past one another over millions of years to shape our world.  There are two types of crust that sit on top of these plates: oceanic crust (that beneath our oceans) and continental crust (that beneath our feet). These move across the surface of the Earth at rates of up to 10cm per year. Many are in a state of constant collision with one another.  Continental crust is thicker than oceanic crust. When continents collide, they buckle upwards and sideways to form mountain ranges: the Himalayas, for example. When continental and oceanic regions collide, the oceanic crust slides beneath the continent and gets consumed back into the Earth in a process that geologists call subduction.  In these circumstances, the plate on top is subjected to compressing and stretching forces that can create mountain belts such as the Andes in South America. The sinking ocean plate meanwhile melts and can produce volcanoes at the surface. All of this adds new material to the continent. As the plate beneath pushes its way under the one above, large earthquakes can also be generated, like the one that struck Sumatra in 2004 and caused the Boxing Day tsunami.  For 60 years the orthodoxy has been that these processes gradually form supercontinents, such as Gondwana or Laurasia, where a vast land mass is brought together before slowly breaking up and drifting away in pieces again. This has happened a number of times in cycles since the Earth was formed, collecting and then separating land over and over again. Now we have new information that suggests that the process is more complex than we had thought. When supercontinents break apart, small pieces of so-called “exotic continental crust” sometimes splinter off and get set adrift in newly formed oceanic crust (which is generated in places where continents break up).  When the oceanic crust containing the remnant fragment of continental material collides with another continent, the exotic piece of crust is too thick and buoyant to take part in the usual process of subduction. Instead of sliding beneath, it gets stuck at the margin of the continent.  When the surrounding zones of tectonic collision recede as the large piece of continental crust increases in size, the newly formed crust is forced to wrap itself around the exotic continental fragment. This creates a dramatic bent mountain belt called an orocline. This theory was first published by a group of Australian academics earlier this year, based on predictions from their 3D computer model. But the field evidence to support their findings was limited, so the race was on to demonstrate that this really does happen.  To confuse things further, not all oroclines are necessarily formed in this way: sometimes mountain ranges can bend for other reasons. So the likes of the Texas Orocline in eastern Australia or the Cantabrian Orocline in Iberia would be good places to look for evidence of the new theory. But their existence doesn’t tell us anything by itself.  This is where my team came in. I have spent the best part of 12 years driving around the outback in eastern Australia, digging holes to bury small seismic sensors. These record earthquakes from places like Indonesia, Fiji and Japan, which through a process called seismic tomography has enabled us over time to build up a 3D image of the Earth’s crust in Australia. It is similar to the X-ray-based computerised tomography (CT-scan) that doctors use to construct internal images of parts of the human body. Over the years I planted about 700 of these sensors. The sensors have now enabled us to prove that the theory is correct. Ironically we found what we were looking for, not in any of the world’s known bent mountain ranges but in one of the flattest places on Earth: the Hay plains in western New South Wales, a dry dusty expanse over hundreds of miles.  Hay is the site of an old sea that formed and receded due to variations in sea level, during which sediments were deposited on the eroded bedrock below. Our imaging shows that buried underneath it are the remains of exactly the sort of orocline the theory predicted.  What does this mean for geology? It shows us that continents form in more complex ways than we thought. Scientists will now probably start testing other parts of the Earth’s crust to try and find examples elsewhere, including the oroclines that we can already see. It is very hard to say how widespread these features will turn out to be. Most likely the old version of plate tectonics will still be true in the majority of cases. The discovery may give us new insights into how minerals are formed. I wouldn’t go as far as to say it will help us to find more minerals, but it should add extra sophistication to our predictive framework for saying where and how minerals form.   It will also make us think more about what happens when supercontinents break apart, especially smaller pieces the size of Tasmania or the UK. It could mean that a lot of them end up forming new continents through this sort of process. Previously scientists hadn’t given this much thought. Wherever the new findings take us, it may be the beginning of a new chapter in how the world fits together."
"
A snowmobiler negotiates the streets of Crosby, North Dakota. Photograph courtesy of the Crosby Journal.

Guest Post by Harold Ambler
Snow, wind, and cold have assaulted North Dakota yet again in the past 24 hours. In Bismarck Friday morning the temperature was 12 below zero with a new inch or two of snow expected following Thursday’s more significant storm.
According to USA Today, snow in the southern part of the state was bad enough Thursday that snowplow operators were pulling off the road, blinded by the whiteout conditions. A foot of snow was common in the heaviest band.
The National Weather Service predicts a high temperature of 3 degrees Fahrenheit Friday in Bismarck, as well as additional snow. As of Thursday, three-quarters of the state’s roads were still snow-covered, in whole or in part, from the storm that just ended the day before.

Howling winds and copious snow have combined to leave austere scenes like this in Cavalier County, North Dakota. Photograph courtesy of the ND Department of Emergency Services.

More than once during the winter, the Department of Transportation has issued a no-travel advisory, most recently on February 10.
Cecily Fong, spokeswoman for the state’s Department of Emergency Services, said that the winter got off to a bad start on November 4. “That first storm was definitely a blizzard with blowing and drifting snow,” she said.  Since then, according to Fong, several counties have seen more than 400 percent of normal snowfall.
December was a record breaker for Bismarck, as it was at many other locations around the state. In Bismarck, the total for the month was 33.3 inches, the greatest amount ever received in a single month.
Those were early days, it turned out. Frequent storms, followed by howling northwest winds and record-breaking cold, have made it a winter to remember. On January 15, the morning low at the Bismarck airport was 44 below zero, the coldest ever for the date, and one degree shy of the all-time coldest reading for a state known to be less than balmy.
By the end of January, many counties had more than 400 percent of normal snow totals on the ground, and Governor John Hoeven had declared a state of emergency. 
“There has been a repeated pattern,” said Fong,  ”where the county will come and plow a road and then two days later, without any additional snow, the road becomes impassable again.” Relatively speaking, the people in Bismarck have gotten off light. Divide County, in the state’s northwest corner, has received 500 percent of normal snowfall.
Steve Andrist, who has lived most of his life in Divide county and is the publisher of the weekly Crosby Journal, commended the street department. “There has never been more than a day or a day and a half where the roads were

Roads that were cleared once, and twice, have needed to be cleared a third time in various locations throughout the state. Photograph courtesy of the ND Department of Emergency Services.

impassable,” he said.
After a lifetime living so near the Canadian border, did the last few months really amount to anything? “This winter got my attention,” he said. “The thing that’s different about this one is the volume of snow. It’s so much more than we anticipated. As far as snow and moving it, and moving it again, and having to move it again a third time, this has been very unusual.”
On February 19, the governor asked the federal government to provide emergency assistance for snow removal. “We’ve got roads that aren’t being plowed,” Fong said, “just because the funds aren’t available to do it.”
Although the spring melt is weeks away, Fong said that flooding is already a concern. “We don’t know where, and we don’t know when, but we’re keeping our eyes on it.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e97fec37c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"


A glacial region in Norway (Source:  NRK)
Reposted from the DailyTech
By: Mike Asher


Scandinavian nation reverses trend, mirrors  results in Alaska, elsewhere.
After years of decline, glaciers in Norway are again growing, reports the  Norwegian Water Resources and Energy Directorate (NVE). The actual magnitude of  the growth, which appears to have begun over the last two years, has not yet  been quantified, says NVE Senior Engineer Hallgeir Elvehøy.
The flow rate of many glaciers has also declined. Glacier flow ultimately  acts to reduce accumulation, as the ice moves to lower, warmer  elevations.
The original trend had been fairly rapid decline since the  year 2000.  
The developments were originally reported by the Norwegian  Broadcasting Corporation (NRK).
DailyTech has previously reported on the  growth in Alaskan glaciers, reversing a 250-year trend of loss. Some  glaciers in Canada, California, and New Zealand are also growing, as the result  of both colder temperatures and increased snowfall.
Ed Josberger, a  glaciologist with the U.S. Geological Survey, says the growth is “a bit of an  anomaly”, but not to be unexpected.
Despite the recent growth, most glaciers in the nation are still smaller than  they were in 1982. However, Elvehøy says that the glaciers were even smaller  during the ‘Medieval Warm Period’ of the Viking Era, prior to around the year  1350.
Not all Norwegian glaciers appear to be affected, most notably those in the  Jotenheimen region of Southern Norway.





			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ac52297',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Plastic pollution has become pervasive, pernicious and persistent. In 2016, global plastic production was around 335m tonnes and it continues to be released into our rivers and oceans on a catastrophic scale, despite recently becoming one of the world’s most publicised and well documented environmental issues. The so called “Blue Planet” effect, which emerged after David Attenborough’s TV show highlighted the issue of plastic pollution in the UK, has seen the introduction of small but significant changes in behaviour and legislation. Awareness campaigns and the growth of numerous grassroots organisations, particularly on a local scale, are making significant contributions to resolving some of the issues. But much of this activity is based around macroplastics, larger pieces of pollution that we know more about and that more easily attract public action through activities such as beach cleaning. A major part of the plastic problem is microplastics, tiny pieces less than 5mm in width whose impact on the environment is much less well understood and that are harder for the public to do something about. With that in mind, I’ve launched a citizen science project that aims to harness the public’s concerns about microplastics in order to gather the data that scientists need. There have been piecemeal attempts to resolve some of the microplastic issues, such as the bans on microbeads in cosmetics introduced in a number of countries. But the key challenges highlighted by a 2015 report on microplastics from the GESAMP group of international scientists – increasing scientific knowledge, adapting our behaviour and changing public perception – still exist. There are probably two reasons for this. First, the problem is vast and complex, and there are only a small, albeit growing, number of scientists looking into it. Second is the fact that ocean microplastics are harder to see than other forms of pollution and so the problem is more easily ignored. Getting people to stop using plastic drinking straws is one thing. But getting them to think or do something about the potential impact of tiny pieces of plastic that hide in the sand on beaches around the world is much more difficult. If we want to meet the objectives of the GESAMP report, we can only do it with with public engagement and participation. Citizen science potentially provides a win-win opportunity to meet those challenges. In early 2018, my colleagues and I at the University of Portsmouth along with the conservation charity I founded, Just One Ocean, developed a citizen scientist microplastic research programme. The aim was to gather data to help evaluate the environmental impacts of visible microplastics (between 1mm and 5mm in width) on the coastal environment. For each location studied, we wanted to know how many microplastics there were, how they were spread and what other characteristics they had.  


      Read more:
      Explainer: what is citizen science?


 Citizen science is growing but still viewed with some scepticism by many professional scientists. So citizen projects need to be carefully planned and carried out to ensure data is reliably accurate and consistent, and isn’t prejudiced by the biases of participants. Projects also need to be able to attract volunteers and consider issues of time, effort, cost and accessibility for volunteers. With these in mind we made the task simple, short and easy. Volunteers are provided with clear written information backed up by video guidance, while images of the data they collect provides a verification process. In trials undertaken on the south coast of the UK with nearly 100 volunteers, this method captured substantial amounts of scientific data that was consistent and accurate enough to prove citizen science could be used to study microplastics. The pilot also improved the knowledge and skills of the participants and encouraged the local community to take ownership of the problem.  Though we are still improving the programme, the success of the trials was enough for us to launch “The Big Microplastic Survey” as a global research project – and since July 2018 we’ve signed up participants from 42 countries. As well as individuals, this has included universities in India, Australia and South Africa, environmental agencies in remote locations such as St Helena, the Falklands and Ascension Island, and small scientific communities in Malaysia and Indonesia. Volunteers are able to undertake a simple microplastic survey in around 30 minutes using the sort of equipment you will find in most households. They take samples of sand from a specified area and use a density separation process with sea water to remove the microplastics. They then record the data, take a photograph of the microplastics and send us the results. We haven’t set an end date for the project and, in the longer term, our ambition is to enable volunteers to upload data in real time onto a geographic database that will be made freely available for researchers. We hope it will not only increase our scientific understanding of the problem, but also engage with hundreds of people. The challenges we face from plastic pollution are significant, but so are the potential contributions that ordinary people all over the world can make – a situation that scientists should embrace. There is a lot of interest in microplastics and people want to be involved, but up until now they did not know how to be. This project provides them with the opportunity they might have been waiting for."
"Over the past 30 years China has become the world’s factory. For the past few weeks, the production line has been shut down by plant closures deemed necessary to halt the spread of coronavirus. Beijing fears there will be both short- and long-term damage from the outbreak. The country is on course for its first quarter of negative growth in decades, while earlier this week China’s ambassador to the World Trade Organization (WTO) called on other countries not to use coronavirus as an excuse to put up trade barriers. The fact that Zhang Xiangchen felt the need to make this appeal speaks volumes. China suspects there will be backdoor protectionism – and it is almost certainly right, because for years countries around the world have needed little encouragement to resort to protectionism. What’s more, the restrictions are not just on the movement of goods. Earlier this month, the US treasury announced curbs on foreign investment to protect critical technology, data and infrastructure from foreign sabotage. Donald Trump’s plans for a wall along the US border with Mexico are emblematic of a toughening up of controls on migration. An era of open markets and open borders – where trade and transnational capital flows rose rapidly as a share of global output – has run its course. The instruments of deglobalisation are being weaponised. Companies are realising that lengthy global supply chains have costs as well as benefits. Coronavirus has brought that home with a vengeance Today, governments are less interested in breaking down barriers and more concerned about safeguarding jobs, preventing intellectual property theft and the risk of cybercrime. Companies are realising that lengthy global supply chains designed to take advantage of low wages in the developing world have costs as well as benefits. Coronavirus has brought that home with a vengeance, and is likely to further encourage the repatriation of production that was offshored in the 1990s and 2000s. Deglobalisation has happened before, notably between 1914 and 1945. It is happening now as a result of geopolitics, economic torpor, rising inequality, the failure to develop new political structures to manage globalisation, and the response to new threats. Every wave of globalisation has required a champion, a hegemonic power confident enough to spread the gospel of free trade and open markets. That role fell to Britain in the late 19th century and the US in the second half of the 20th century. But America’s self-confidence has been punctured by the rise of China as a strategic threat, and the power struggle between the world’s two biggest economies is heating up. Stock markets were jubilant when Washington and Beijing signed a trade deal but – rather like the Molotov-Ribbentrop pact of August 1939 – it is merely a truce of convenience. If Trump wins re-election in November, hostilities will resume. Countries also get defensive when times are tough, as they have been ever since the financial crisis of 2008. The most recent wave of openness occurred in the decade after the collapse of communism, culminating in China becoming a member of the WTO in 2001. This was a time when growth was strong, partly due to a globalisation feedback loop in which cheaper imports pushed down inflation rates and allowed central banks in the west to keep interest rates low and asset prices high. But the financial crisis exposed the weaknesses of a system that was able to operate globally without adequate controls and effective supervision. The resulting slump was deep and the recovery has been long, painful and incomplete. Inevitably, countries have become more cautious. That trend has been amplified because globalisation’s fruits have been enjoyed primarily – though not exclusively – by owners of capital and the better off. Consumers have gained from lower prices, but inequality has risen in every part of the world. In democracies, there is a limit to how long people will put up with the rich getting richer while their living standards are stagnating or barely growing. The hope – always somewhat sketchy – was that an international polity would be developed to match the internationalisation of economics. If capital could organise on a global level, the argument went, then democratic mechanisms could and would be developed too. This simply has not happened. The one multilateral institution created in the past three decades to manage globalisation – the WTO – is in a parlous state. The WTO has two main functions: as a forum where comprehensive trade deals are negotiated, and to provide a court where trade disputes between countries can be settled. It is currently doing neither. The failure to develop a transnational political response to globalisation has meant voters have demanded a response at a level where they have a voice: the nation state. Taking back control is proving to be a compelling rallying cry for the populists of the right such as Trump and the populists of the left such as Bernie Sanders. Neither man has much time for the WTO. Global heating looks certain to add to the deglobalisation pressure. The existential threat posed by the climate emergency is forcing governments, businesses and consumers to ask some questions about the way the global economy works. Is it sensible to ship car parts backwards and forwards across national borders or invest in fossil fuel companies? Is it sustainable to fly in fruit and veg from the other side of the world rather than grow it locally? Most fundamentally of all, are there more important things than economic growth? Globalisation was sold as a way of boosting prosperity for all by making markets bigger and more efficient. For a while the model worked, but when it blew up and caused extensive collateral damage, a backlash was inevitable. Deglobalisation is the result. • Larry Elliott is the Guardian’s economics editor"
"Mass melting of the West Antarctic ice sheet, driven by warmer ocean temperatures, was a major cause of extreme sea level rise more than 100,000 years ago, according to new research. A research team, led by scientists at the University of New South Wales, examined the cause of high sea levels during a period known as the last interglacial, which occurred 129,000-116,000 years ago.  Their study finds that melting of the West Antarctic ice sheet caused a sea-level rise of more than three metres and it took less than 2C of ocean warming for that to occur. The authors say their findings could have “major implications” for the future given the ocean warming and ice melt currently occurring in Antarctica. The study’s lead author Chris Turney is a climate change and earth scientist at UNSW. He said the West Antarctic was particularly vulnerable to ocean warming because it sits mostly on the sea bed, rather than on land. “This has been a big concern and is what the concern is in the present day,” Turney said. “So the question is how much could fall into the ocean and this is where the last interglacial [period] is so important.” The paper says ocean temperatures during the last interglacial were likely up to 2C warmer than they are today and global sea levels were 6-9 metres higher. To trace Antarctica’s potential contribution to this sea-level rise, the scientists travelled to West Antarctica to the Patriot Hills Blue Ice Area, which is on the periphery of the West Antarctic ice sheet. Blue ice areas are created by katabatic winds. When these winds blow over mountains, they remove snow and ice, allowing ancient ice to come to the surface. A lot of Antarctic research involves deep ice core drilling to study years of climate history. In this study, the researchers used what they called “horizontal ice core” analysis, which involved simply walking across the valley towards the mountain. “As you walk towards the mountain, you walk over increasingly older ice,” Turney said. They used some shallow drilling to take ice samples from the surface. Through isotope measurements, they found a gap in the ice sheet record immediately prior to the last interglacial. Turney said this gap coincided with an extreme rise in sea level and suggested a period in which there was no ice accumulating in that valley. “It means that a large part of the west Antarctic almost certainly disappeared in the last interglacial. It melted. It flowed rapidly into the ocean,” he said. He said the research also suggested this mass melting happened quite early during the ocean warming “somewhere between zero and 2C”. Countries have signed on to the Paris agreement which aims to keep global heating below 2C. Turney said the current summer in Australia alone had shown the dangers of a warming world just at 1C. He said the team’s research could be used to focus on which sections of West Antarctica are most vulnerable to the current climate crisis. “What these results suggest, or show, is that when people talk about a 2C warmer world as a good thing, actually what it shows is we don’t want to get close to 2C,” he said."
"

This Sunday, Al Gore will probably win an Academy Award for his global‐​warming documentary _An Inconvenient Truth_ , a riveting work of science fiction.



The main point of the movie is that, unless we do something very serious, very soon about carbon dioxide emissions, much of Greenland’s 630,000 cubic miles of ice is going to fall into the ocean, raising sea levels over twenty feet by the year 2100.



Where’s the scientific support for this claim? Certainly not in the recent Policymaker’s Summary from the United Nations’ much anticipated compendium on climate change. Under the U.N. Intergovernmental Panel on Climate Change’s medium‐​range emission scenario for greenhouse gases, a rise in sea level of between 8 and 17 inches is predicted by 2100. Gore’s film exaggerates the rise by about 2,000 percent.



Even 17 inches is likely to be high, because it assumes that the concentration of methane, an important greenhouse gas, is growing rapidly. Atmospheric methane concentration hasn’t changed appreciably for seven years, and Nobel Laureate Sherwood Rowland recently pronounced the IPCC’s methane emissions scenarios as “quite unlikely.”



Nonetheless, the top end of the U.N.‘s new projection is about 30‐​percent lower than it was in its last report in 2001. “The projections include a contribution due to increased ice flow from Greenland and Antarctica for the rates observed since 1993,” according to the IPCC, “but these flow rates could increase or decrease in the future.”



According to satellite data published in _Science_ in November 2005, Greenland was losing about 25 cubic miles of ice per year. Dividing that by 630,000 yields the annual percentage of ice loss, which, when multiplied by 100, shows that Greenland was shedding ice at 0.4 percent per century.



“Was” is the operative word. In early February, _Science_ published another paper showing that the recent acceleration of Greenland’s ice loss from its huge glaciers has suddenly reversed.



Nowhere in the traditionally refereed scientific literature do we find any support for Gore’s hypothesis. Instead, there’s an unrefereed editorial by NASA climate firebrand James E. Hansen, in the journal _Climate Change_ — edited by Steven Schneider, of Stanford University, who said in 1989 that scientists had to choose “the right balance between being effective and honest” about global warming — and a paper in the Proceedings of the National Academy of Sciences that was only reviewed by one person, chosen by the author, again Dr. Hansen.



These are the sources for the notion that we have only ten years to “do” something immediately to prevent an institutionalized tsunami. And given that Gore only conceived of his movie about two years ago, the real clock must be down to eight years!



It would be nice if my colleagues would actually level with politicians about various “solutions” for climate change. The Kyoto Protocol, if fulfilled by every signatory, would reduce global warming by 0.07 degrees Celsius per half‐​century. That’s too small to measure, because the earth’s temperature varies by more than that from year to year.



The Bingaman‐​Domenici bill in the Senate does less than Kyoto — i.e., less than nothing — for decades, before mandating larger cuts, which themselves will have only a minor effect out past somewhere around 2075. (Imagine, as a thought experiment, if the Senate of 1925 were to dictate our energy policy for today).



Mendacity on global warming is bipartisan. President Bush proposes that we replace 20 percent of our current gasoline consumption with ethanol over the next decade. But it’s well‐​known that even if we turned every kernel of American corn into ethanol, it would displace only 12 percent of our annual gasoline consumption. The effect on global warming, like Kyoto, would be too small to measure, though the U.S. would become the first nation in history to burn up its food supply to please a political mob.



And even if we figured out how to process cellulose into ethanol efficiently, only one‐​third of our greenhouse gas emissions come from transportation. Even the Pollyannish 20‐​percent displacement of gasoline would only reduce our total emissions by 7‐​percent below present levels — resulting in emissions about 20‐​percent higher than Kyoto allows.



And there’s other legislation out there, mandating, variously, emissions reductions of 50, 66, and 80 percent by 2050. How do we get there if we can’t even do Kyoto?



When it comes to global warming, apparently the truth is inconvenient. And it’s not just Gore’s movie that’s fiction. It’s the rhetoric of the Congress and the chief executive, too.
"
"
Share this...FacebookTwitterIt appears that major media outlets took reports based on old observations and passed it on to the public as breaking news last week.
H/T: Reader Dirk H.
Last week a number of major media outlets reported on how a large plume of oil had been spotted in the Gulf of Mexico, insisting that the BP oil spill was not naturally disappearing, as claimed by BP and US officials.
Well, it turns out that last week’s reports were based on oil plume observations made way back in June and junk science. Here are some of the headlines we saw last week in the land of angst, Germany:
Sueddeutsche Zeitung on August 18:
Scientists: 80% Of The Oil Is Still There
Der Spiegel on August 18;
Scientists Attack US Government’s Announcement That Worst Is Over
WDR5 German Radio on August 20:
Large Oil Cloud Beneath The Surface
Die Welt on August 21:
Platform Operator Accuses BP Of Cover-Up  and on August 20: 35 Kilometer Oil Slick
All these reports claimed that BP and officials were premature in calling off the emergency, and that the oil slick was far greater than the public was led to believe. There’s much more oil out there and the problem is still an environmental catastrophe, they all insisted last week.
What did these enviro-bedwetting journalists base their stories on?
They were based on a study by Woods Hole Oceanographic Institute describing an undersea oil cloud observed June 19 to 28. Here’s the Wood Hole press release. These media outlets were too lazy to check out the source.
So is there still lots of oil out there? No.
The Washington Post wrote yesterday on a Study: Petroleum-eating microbes significantly reduced gulf oil plume that according to the newest findings by a team of scientists led by Terry C. Hazen of the Lawrence Berkeley National Laboratory in California, the oil plumes are today practically gone, and on how microbes and bacteria have…



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




…reduced the amount of oil amounts in the undersea “plume” by half about every three days, according to research published online Tuesday by the journal Science.
The Washington Post also mentions how the Woods Hole study was based on observations from June.  The WaPo writes:
In the Woods Hole study, scientists described finding an undersea oil cloud June 23 to 27 similar to the one Hazen and his colleagues found between May 25 and June 2 – which was similar to one found soon after by people from the Montereyâ Bay Aquarium Research Institute.
And so the mystery of the missing oil is explained:
The findings, by a team of scientists led by Terry C. Hazen of the Lawrence Berkeley National Laboratory in California, help explain one of the biggest mysteries a mystery of the disaster: Where has all the oil gone?
‘What we know about the degradation rates fits with what we are seeing in the last three weeks,’ Hazen said. “We’ve gone out to the sites, and we don’t find any oil, but we do find the bacteria.”
The oil is gone. The WaPo report also shows that the feared oxygen depletion catastrophe is also all hype:
One thing that many scientists feared – severe depletion of oxygen as microbes consumed the oil – apparently hasn’t happened. The Woods Hole study published last week found no decrease in oxygen in the oil plume, and the new study found only a slight one.
No plume could be found in the past three weeks, however. The oil that remains appears to be too diluted to be detected.
There it is folks. Another bout of incontinence by the German media. Listening to them last week, you’d think the Gulf of Mexico was sloshing with crude. Bad reporting is a great way to wrongly scare away tourists at a time the Gulf coast needs them the most.
Share this...FacebookTwitter "
"Ponds are taken for granted. Perhaps it’s because most of us have seen them – and on occasion, fallen into them – and think they’re only good for goldfish. Ponds may be the number one habitat for children’s “minibeast” hunts, but we are supposed to grow out of them in adulthood.  As James Clegg, a 20th-century British naturalist wrote, ponds are  a field particularly suited to the activities of the amateur, whose humble pond-hunting, if carried out systematically and carefully, may well result in valuable contributions to science. But all-too often, ponds are missed out of conservation strategies which are instead fixated on larger lakes and rivers. This is a serious omission – ponds are the most common and widespread habitat for all plants and animals across the continents and islands of Earth, from Antarctica to the tropics. Perched on the surface of Alpine glaciers or waiting out desert droughts to refill with the rains, deep in equatorial forest or amid the city sprawl. They could well be found on Mars.  The past 20 years have seen a blossoming of research into ponds, led in the UK by the Freshwater Habitats Trust and, internationally, the European Pond Conservation Network. These organisations bring together researchers and practitioners to help conserve pond biodiversity. Their work has revealed that ponds are biodiversity hotspots in the landscape, disproportionately rich in species when compared to rivers, streams and lakes and home to many rare specialists, such as fairy and tadpole shrimps.   Ponds benefit humans by slowing down water run-off that can cause flooding and mopping up excess nutrients – a great example of what are now recognised as “small water bodies” that enrich and enliven a landscape. But, globally, ponds may also be important in influencing atmospheric carbon by storing and releasing it, given the intensity of geochemical processes and the sheer number of ponds around the world. However, just how fast ponds can bury carbon is poorly understood. Measuring the rate at which ponds can store carbon is tricky, primarily because the age of many ponds is unknown. To get precise measurements of carbon burial rates we exploited an unusual opportunity using some small, lowland pools whose age is known to the exact day. The ponds were dug out in 1994, at Hauxley Nature Reserve in north-east England. Their original purpose was to follow the colonisation of plants and invertebrates. Two decades later they had accumulated a layer of sediment, dark and rich in organic debris, distinctly different to the underlying clay. We used sediment cores and dug out all of the sediment from some ponds, to measure the organic carbon that had accumulated. The amount of carbon in the cores was scaled up to the amount dug up from other ponds to reflect the total volume of sediment. The ponds’ burial rates for organic carbon ranged from 79 to 247g per square metre per year, with a mean of 142g. These rates are high – much higher than the rates of 2-5g attributed to surrounding habitats such as woodland or grassland. Small ponds occupy a tiny proportion of the UK’s land area – scarcely 0.0006% – compared to grassland at 36% or 2.3% for ancient woodland. But the rate of carbon burial we found would result in ponds burying half as much as the vastly greater expanse of grassland. However, the role ponds play in the carbon cycle is complicated. Some ponds may be significant sources of greenhouse gases, such as permafrost thaw ponds in the Arctic which release even more carbon as the tundras they’re found in warm. Our Hauxley ponds can switch back and forth from being a net sink to a net source of carbon as they dry out or re-flood. Nevertheless, our ponds have accumulated plenty of carbon over their 20 years and provided a home to a wealth of animals and plants. Nothing was done to engineer carbon burial in our ponds – there was no artificial enhancement of productivity to maximise carbon capture. They are small, shallow, lowland ponds among the intensively farmed landscapes typical of much of the temperate climes. Similar ponds and tiny wetlands are dotted throughout the local landscape, primarily scraped out for wildlife conservation.  These lowland ponds are easy to create, even in a back garden. They can be small and temporary – clean water is the key – and the value of their wildlife is now firmly understood. No longer overlooked, the importance of ponds in the carbon cycle and in fighting climate change is becoming apparent."
"
Share this...FacebookTwitterThe July Arctic sea ice outlook for September is out. Click here.
Here’s a graphic of the prediction made by 16 different institutes this month, now that they are all 30 days wiser.
Now compare this to the projections made 30 days ago, late June.
Then again, some are incapable of learning anything. Anyway, at least five now concede that we may not even reach last year’s low. Strangely, after July’s slow melt, some have grown even more pessimistic.
Then again, optimism has never been a trait one finds in climate “science”. Who do you think will grab the headlines?
Share this...FacebookTwitter "
"

Kalamazoo State Hospital
In an effort to add to the surfacestation.org survey coverage, I’ve been looking at a number of stations from the aerial vantage points available in Google Earth and Microsoft Live Maps. I particularly look for the stations that have Stevenson Screens, as those are the most visible and easy to spot from these online resources.
I was disappointed to learn though that the USHCN station at Kalamazoo State Hospital (a state psychiatric hospital) has been closed. It was probably due to recent construction of new wards as seen here:
Click for a live interactive aerial view
While I was scouring online image databases looking for a surviving photo that would show the placement of the Stevenson Screen, I stumbled across this photo on Flickr taken from afar and this strange comment about it:

Flickr caption reads: I took this photo from well away from the grounds of the State Hospital, so as to avoid violating state law.
“Apparently, it is illegal to take photographs on the grounds of the state hospital as a protection to those who are patients there.  I received a lengthy explanation of this law from the Michigan State Police officer who was on patrol at the hospital the morning I visited.  He advised me that the hospital office has quite a collection of confiscated cameras.”
I did visit another state mental hospital in Napa, CA and found this placement of the MMTS:

You can see a full set of pictures, at the surfacestations.org image database.
So WUWT readers, here is the challenge against very unlikely odds:
Find a surviving photo of the USHCN weather station at Kalamazoo State Hospital. 
Good hunting.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a64e2ca',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Former environment minister Owen Paterson has called for the UK to scrap its climate change targets. In a speech to the Global Warming Policy Foundation, he cited “considerable uncertainty” over the impact of carbon emissions on global warming, a line that was displayed prominently in coverage by the Telegraph and the Daily Mail. Paterson is far from alone: climate change debate has been suffused with appeals to “uncertainty” to delay policy action. Who hasn’t heard politicians or media personalities use uncertainty associated with some aspects of climate change to claim that the science is “not settled”? Over in the US, this sort of thinking pops up quite often in the opinion pages of The Wall Street Journal. Its most recent article, by Professor Judith Curry, concludes that the ostensibly slowed rate of recent warming gives us “more time to find ways to decarbonise the economy affordably.” At first glance, avoiding interference with the global economy may seem advisable when there is uncertainty about the future rate of warming or the severity of its consequences. But delaying action because the facts are presumed to be unreliable reflects a misunderstanding of the science of uncertainty. Simply because a crucial parameter such as the climate system’s sensitivity to greenhouse gas emissions is expressed as a range – for example, that under some emissions scenarios we will experience 2.6°C to 4.8ºC of global warming or 0.3 to 1.7 m of sea level rise by 2100 – does not mean that the underlying science is poorly understood.  We are very confident that temperatures and sea levels will rise by a considerable amount. Perhaps more importantly, just because some aspects of climate change are difficult to predict (will your county experience more intense floods in a warmer world, or will the floods occur down the road?) does not negate our wider understanding of the climate. We can’t yet predict the floods of the future but we do know that precipitation will be more intense because more water will be stored in the atmosphere on a warmer planet. This idea of uncertainty might be embedded deeply within science but is no one’s friend and it should be minimised to the greatest extent possible. It is an impetus to mitigative action rather than a reason for complacency. There are three key aspects of scientific uncertainty surrounding climate change projections that exacerbate rather than ameliorate the risks to our future. First, uncertainty has an asymmetrical effect on many climatic quantities. For example, a quantity known as Earth system sensitivity, which tells us how much the planet warms for each doubling of atmospheric carbon dioxide concentration, has been estimated to be between 1.5°C to 4.5ºC. However, it is highly unlikely, given the well-established understanding of how carbon dioxide absorbs long-wave radiation, that this value can be below 1ºC.  There is a possibility, however, that sensitivity could be higher than 4.5ºC.  For fundamental mathematical reasons, the uncertainty favours greater, rather than smaller, climate impacts than a simple range suggests. Second, the uncertainty in our projections makes adaptation to climate change more expensive and challenging.  Suppose we need to build flood defences for a coastal English town. If we could forecast a 1m sea level rise by 2100 without any uncertainty, the town could confidently build flood barriers 1m higher than they are today.  However, although sea levels are most likely to rise by about 1m, we’re really looking at a range between 0.3m and 1.7m. Therefore, flood defences must be at least 1.7m higher than today – 70cm higher than they could be in the absence of uncertainty. And as uncertainty increases, so does the required height of flood defences for non-negotiable mathematical reasons.  And the problem doesn’t end there, as there is further uncertainty in forecasts of rainfall occurrence, intensity and storm surges.  This could ultimately mandate a 2 to 3m-high flood defence to stay on the safe side, even if the most likely prediction is for only a 1m sea-level rise. Even then, as most uncertainty ranges are for 95% confidence, there is a 5% chance that those walls would still be too low. Maybe a town is willing to accept a 5% chance of a breach, but a nuclear power station cannot to take such risks. Finally, some global warming consequences are associated with deep, so-called systemic uncertainty. For example, the combined impact on coral reefs of warmer oceans, more acidic waters and coastal run-off that becomes more silt-choked from more intense rainfalls is very difficult to predict. But we do know, from decades of study of complex systems, that those deep uncertainties may camouflage particularly grave risks. This is particularly concerning given that more than 2.6 billion people depend on the oceans as their primary source of protein. Similarly, warming of Arctic permafrost could promote the growth of CO2-sequestering plants, the release of warming-accelerating methane, or both. Warm worlds with very high levels of carbon dioxide did exist in the very distant past and these earlier worlds provide some insight into the response of the Earth system; however, we are accelerating into this new world at a rate that is unprecedented in Earth history, creating additional layers of complexity and uncertainty. Increasingly, arguments against climate mitigation are phrased as “I accept that humans are increasing CO2 levels and that this will cause some warming but climate is so complicated we cannot understand what the impacts of that warming will be.” This argument is incorrect – uncertainty does not imply ignorance. Indeed, whatever we don’t know mandates caution. No parent would argue “I accept that if my child kicks lions, this will irritate them, but a range of factors will dictate how the lions respond; therefore I will not stop my child from kicking lions.”  The deeper the uncertainty, the more greenhouse gas emissions should be perceived as a wild and poorly understood gamble. By extension, the only unequivocal tool for minimising climate change uncertainty is to decrease our greenhouse gas emissions."
"Climate change is already affecting many aspects of our everyday lives, including what we eat, where we live and the buildings we inhabit. And dangerous industries that make our way of life possible, such as agriculture, construction and fishing, are becoming riskier than ever as a result of changing weather. As extreme natural events become more common, it is increasingly important these industries adapt for the future.  Stronger winds, more frequent storms and increased flooding obviously make life more difficult for anyone who works outdoors. Workers also might potentially face increased levels of heat and pollution exposure, which could be harmful to their health. But climate change won’t just potentially make existing workplace hazards worse – it will create new ones. In the construction sector, new building materials and practices may come with unanticipated health and safety consequences. The industry also faces the challenge of designing and constructing buildings and infrastructure that can withstand more stress from a changing climate. Parts of the world most affected will need infrastructure built to higher standards. In areas like California, where problems with drought and wildfires have grown dramatically in recent decades, resilient construction guidelines have emerged.  In the Netherlands and Japan, for example, many car parks and spaces beneath buildings can store floodwater. Residential buildings have also been adapted to float on stilts if floodwaters rise. Buildings are also constructed to be more sustainable, with green roofs, living walls, and better energy efficiency to lessen environmental impact. Developing other innovative design strategies and techniques will help ensure building safety, but it’s important that worker safety is kept in mind. Finding novel solutions to more extreme weather will be crucial to increase safety in the fishing industry. Improved GPS, tracking, and better storm forecasting will help fishermen navigate more precisely, and better identify dangerous waters and conditions.  Extreme weather also means more days’ work will be lost because of poor and dangerous conditions for fishermen globally. But if more ships stay in port because of extreme weather, this will have economic consequences on anticipated revenue and incomes. To limit this, the industry needs to increase training and preparedness to operate in bad weather, as well as investing in improved technological support, such as satellite-based geographic information systems and long-range weather-forecasting.  The industry is already responding with efforts to improve scientific advice and data collection to predict weather systems and extreme events, and to better understand trends in fish stock populations and distributions. There are also attempts to improve vessel safety, port resilience, and reduce the vulnerability of freight while at sea. Warmer seas may also cause some fish species such as cod to move from their traditional habitats, often further from key fishing ports. This creates problems for smaller fishing vessels that aren’t designed to navigate deeper and stormier waters. Better sat-nav systems are therefore essential to help these vessels operate safely. Increased levels of fish farming, meanwhile, will need to be rolled out in a way that minimises pollution caused by intensive rearing.  


      Read more:
      Climate change threatens global fish stocks


 The farming sector is at growing risk from greater flooding, not only because of extreme rainfall but because of growing soil erosion, which reduces the ground’s ability to absorb water, worsening flood risk. Lowland and coastal areas in particular might be more susceptible or even completely lost to saltwater flooding.  Agricultural workers also face the threat of more heat-related illnesses, as well as greater exposure to pesticides and contaminated and polluted air. But the extent of this impact will be hard to forecast until we know more about the extreme weather patterns that may become the norm. It’s unlikely many of these problems can be halted, but it may be possible to develop new crops more suited to a changing climate, or pest and disease resistance. Researchers are already developing new strains of staples (such as maize) to adapt to these changes. Other new technologies, such as satellite imagery and drones can identify drought in crops, and target irrigation where it is most needed. Farmers are also being asked to change the way they use the land in order to reduce the environmental impact of agriculture. Extreme weather and climate change is becoming the norm, and these events are part of a long-term shift that will continue. The solutions to our situation will be in technical and engineering innovations, and in changing the ways we influence nature. We know these problems will only grow in their magnitude and effect, so it is essential we adapt for things we can no longer avoid."
"In a move to protect its ski slopes and growing economy, Utah – one of the reddest states in the nation – has just created a long-term plan to address the climate crisis.  And in a surprising turnaround, some of the state’s conservative leaders are welcoming it. “If we don’t think about Utah’s long-term future, who will?” Republican state house speaker Brad Wilson said at a recent focus group to discuss the proposals. At the request of the Republican-dominated state legislature, a University of Utah economic thinktank produced the plan to reduce emissions affecting both the local air quality and the global climate. Project lead Thomas Holst, an energy analyst, never expected to be at the helm of an effort like this. A few years ago, the Utah legislature passed a resolution urging the EPA to “cease its carbon dioxide reduction policies, programs, and regulations until climate data and global warming science are substantiated”. But now the perspectives of some state lawmakers – and of Holst, who spent most of his career in the oil and gas industry – have shifted. “The economist Adam Smith talked about an invisible hand that guides the economy, and in this particular case, the cost of renewable energy, whether it’s wind or solar, has gone down so rapidly and made itself so market efficient versus fossil fuels, that there is a change, and the change can’t be ignored,” Holst said. “So now is the opportunity for a state like Utah which is rich in both renewables as well as fossil fuels to embrace that change and get out ahead of it.” Other red states and municipalities are slowly starting to address global heating. After banning the words “climate change” from state environmental agencies, Florida now has a chief resilience officer tasked with preparing for sea level rise. After a year of disastrous flooding, Nebraska lawmakers advanced a bill to develop a climate change plan for a full legislative debate.  Utah prides itself on being business friendly – and it has a rapidly growing tech sector concerned about environmental issues, as well as booming tourist economy that revolves around the ski industry and public lands. The Utah plan, known as the Utah Roadmap, began, like a number of recent environmental initiatives, with young people clamoring for action. High school students drafted a resolution that recognized the impacts of the climate crisis and encouraged emissions reductions, and persuaded two Republican lawmakers to sponsor it. Environmental advocates say it was the first measure of its kind to pass in a red state. The legislature followed up with state money for experts to provide policy recommendations. Another factor that has primed Utah leaders to address the climate crisis is the state’s unique air quality issues. The majority of the population lives in mountain valleys where in winter, temperature inversions can trap air pollutants, often reaching levels that impact health, particularly among children and the elderly. “It cuts across political lines. [Clean air] is not a partisan issue in our state,” said Utah speaker Wilson. He said there is not the same kind of consensus on climate change in the legislature, but “there is absolutely overlap between air quality concerns we have and reducing greenhouse gas emissions”. Natalie Gochnour, the head of the economic policy institute that drafted the Utah Roadmap, said its proponents managed to turn a hyper-partisan issue into a shared priority by emphasizing the local impacts of the climate crisis. Research suggests that framing policy around economic benefits and sustainability allows local leaders to respond to climate change without getting caught up in political divisions. “That tends to pull some of the politics out of it – not for everybody – but for many. I think enough to create momentum on Capitol Hill,” Gochnour said. Clean air concerns are also the reason officials are pushing Utah gas refineries to produce cleaner gasoline, and when the Trump administration announced plans to roll back clean car standards, Utah’s bipartisan clean air caucus held a press conference urging Congress to resist the move. Holst, the roadmap project lead, acknowledged that blue coastal states have taken the initiative on ameliorating climate change, but he sees potential for Utah. “Is there an opportunity for a red state to take a leadership role? We believe that there is. And by composing a road map, by encouraging our legislative leaders to embrace this, we believe that there can be a change, and that Utah will be willing to take a leadership role,” he said. Utah’s per capita carbon emissions are higher than most states, in part because it’s nearly twice as reliant on coal, but utilities serving Utah customers plan to close many of their coal power plants by 2030, converting to wind, solar, natural gas, and possibly hydrogen. Republican state lawmaker Melissa Garff Ballard has an ambitious plan to make Utah a source of hydrogen power serving the western US. Among the Utah Roadmap’s top priorities is to reduce CO2 emissions by half over the next decade – a challenge for a state with a growing population. The plan suggests focusing on energy-efficient buildings and clean transportation options. It recommends expanding Utah’s network of charging stations, incentivizing the purchase of electric vehicles, and involving auto dealers in strategies to increase the zero-emissions vehicle supply. Business leaders have told Holst they are drafting a document that would pledge to move forward with the Utah Roadmap’s recommendations. “What I’m interested in is a viable future for the state of Utah,” Republican state representative Stephen Handy said. “There are still a number of Utah legislators who don’t want to look at the science that’s very obvious on climate change, but we’ve come a long way.” This article was amended on 19 February 2020 to clarify Brad Wilson’s comments on clean air."
"
Recently we’ve been discussing products from the AIRS satellite instrument (Atmospheric InfraRed Sounder) onboard the Aqua satellite. There has been quite a bit of interest in this because unlike the satellite temperature record that goes back to 1979, until now we have not had a complementary satellite derived CO2 record. We are about to have one, and much more.

Click image to see a slide show with this graphic in it (PDF)
I wrote to the AIRS team to inquire about when the satellite data on CO2, and other relevant products might be made public. All that has been released so far are occasional snippets of data and imagery, such as the short slide show above.
Here is the response I got from them:
Thank you for your interest in the AIRS CO2 data product.
We are still in the validation phase in developing this new product.
It will be part of the Version 6 data release, but for now those of us
working on it are intensively validating our results using in situ
measurements by aircraft and upward looking fourier transform IR
spectrometers (TCCON network and others).
The AIRS CO2 product is for the mid-troposphere. For quite some time
it was accepted theory that CO2 in the free troposphere is
“well-mixed”, i.e., the difference that might be seen at that altitude
would be a fraction of a part per million (ppmv). Models, which
ingest surface fluxes from known sources, have long predicted a smooth
(small)variation with latitude, with steadily diminishing CO2 as you
move farther South. We have a “two-planet” planet – land in the
Northern Hemisphere and ocean in the Southern Hemisphere. Synoptic
weather in the NH can be seen to control the distribution of CO2 in
the free troposphere. The SH large-scale action is mostly zonal.
Since our results are at variance with what is commonly accepted by he
scientific community, we must work especially hard to validate them.
We have just had a paper accepted by Geophysical Research Letters that
will be published in 6-8 weeks, and are preparing a validation paper.
We have global CO2 retrievals (day and night, over ocean and land, for
clear and cloudy scenes) spanning the time period from Sept 2002 to
the present. Those data will be released as we satisfactorily
validate them.
I suggest you Google “Carbon Tracker” for some interesting maps
generated using model atmospheres and data for CO2 sources. It shows
the CO2 weather in the lowest part of the atmosphere.
The big picture is that CO2 sources and sinks are in the planetary
boundary layer. Global circulation of CO2 occurs in the free
troposphere. Thus, PBL is local whereas free troposphere is
international.
———-
AIRS Team
With the suggestion of using the Google “Carbon tracker”, some readers might look at this response as a “dodge”. I don’t see it that way at all. Why? Because they are actively engaged in proving the instrument by doing a series of aircraft based measurements to validate the data the instrument on the spacecraft is seeing.
For example, read this paper from them:
First Satellite Remote Sounding of the Global Mid-Tropospheric CO2
These graphics show how hard they are working to validate the data from in situ measurements using airborne flask samples sent to a lab spectrometer:

…and the results of the flask sample measurements:

Read more about this here in this paper (PDF)
Also if you read between the lines in their response to me, particularly this paragraph:
Since our results are at variance with what is commonly accepted by he
scientific community, we must work especially hard to validate them.
We have just had a paper accepted by Geophysical Research Letters that
will be published in 6-8 weeks, and are preparing a validation paper.
I’d say that waiting that 6-8 weeks for the paper and supporting data will be well worth it.  The working title of the upcoming paper is: “Satellite Remote Sounding of Mid-Tropospheric  CO2” and the lead author is Moustafa T. Chahine.
Good things come to those who wait.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d78f2bb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
From NASA Science News h/t to John-X
Spotless Sun: 2008 is the Blankest Year of the Space Age
Sept. 30, 2008: Astronomers who count sunspots have announced that 2008 is now the “blankest year” of the Space Age.
As                      of Sept. 27, 2008, the sun had been blank, i.e.,                      had no visible sunspots, on 200 days of the year. To find                      a year with more blank suns, you have to go back to 1954,                      three years before the launch of Sputnik, when the sun was                      blank 241 times.
“Sunspot counts are at a 50-year low,” says solar physicist David Hathaway of the NASA Marshall Space Flight Center. “We’re experiencing a deep minimum of the solar cycle.”

Above: A histogram showing the blankest years of the last half-century. The vertical axis is a count of spotless days in each year. The bar for 2008, which was updated on Sept. 27th, is still growing. [Larger images: 50                      years, 100 years]
A spotless day                      looks like this:

The image, taken by the Solar and Heliospheric Observatory (SOHO) on Sept. 27, 2008, shows a solar disk completely unmarked by sunspots. For comparison, a SOHO image taken seven years earlier on Sept. 27, 2001, is peppered with colossal sunspots, all crackling with solar flares: image. The difference is the phase of the 11-year solar cycle. 2001 was a year of solar maximum, with lots of sunspots, solar flares and geomagnetic storms. 2008 is at the cycle’s opposite extreme, solar minimum, a quiet time on the sun.
And                      it is a very quiet time. If solar activity continues as low as it has been, 2008 could rack up a whopping 290 spotless days by the end of December, making it a century-level year in terms of spotlessness.
Hathaway cautions that this development may sound more exciting than it actually is: “While the solar minimum of 2008 is shaping up to be the deepest of the Space Age, it is still unremarkable compared to the long and deep solar minima of the late 19th and early 20th centuries.” Those earlier minima routinely racked up 200 to 300 spotless days per year.
Some                      solar physicists are welcoming the lull.
“This gives us a chance to study the sun without the complications of sunspots,” says Dean Pesnell of the Goddard Space Flight Center. “Right now we have the best instrumentation in history looking at the sun. There is a whole fleet of spacecraft devoted to solar physics–SOHO, Hinode, ACE, STEREO and others. We’re bound to learn new things during this long solar minimum.”
As an example he offers helioseismology: “By monitoring the sun’s vibrating surface, helioseismologists can probe the stellar interior in much the same way geologists use earthquakes to probe inside Earth. With sunspots out of the way, we gain a better view of the sun’s subsurface winds and inner magnetic dynamo.””There is also the matter of solar irradiance,” adds Pesnell. “Researchers are now seeing the dimmest sun in their records. The change is small, just a fraction of a percent, but significant. Questions about effects on climate are natural if the sun continues to dim.”
Pesnell is NASA’s project scientist for the Solar Dynamics Observatory (SDO), a new spacecraft equipped to study both solar irradiance and helioseismic waves. Construction of SDO is complete, he says, and it has passed pre-launch vibration and thermal testing. “We are ready to launch! Solar minimum is a great time to go.”
Coinciding with the string of blank suns is a 50-year record low in solar wind pressure, a recent discovery of the Ulysses spacecraft. (See the Science@NASA story Solar                      Wind Loses Pressure.) The pressure drop began years before the current minimum, so it is unclear how the two phenomena are connected, if at all. This is another mystery for SDO and the others.
Who                      knew the blank sun could be so interesting?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c0c0072',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Despite some positive climate action, new fossil fuel infrastructure is still being built and deployed. Dozens of new coal power plants are currently planned or under construction, for instance, while petrol car sales will nearly hit 100m in 2019. But what if all that ceased tomorrow? It turns out that if we built no more fossil fuel infrastructure and instead replaced existing infrastructure at the end of its productive life with a zero carbon alternative we could limit peak temperature rise to 1.5°C – as long as we start now. Colleagues and I recently analysed what would happen to global emissions if fossil fuel power plants, cars, ships, aircraft and industrial infrastructure were all phased out. Our results are published in Nature Communications. This may be a hypothetical scenario, given a full phase out is unlikely to happen any time soon, but calculating what would happen in such a scenario gives us a better idea of the challenge ahead.   In our optimistic scenario, the process of replacing all the fossil fuel infrastructure with zero carbon alternatives (or not replacing it at all) began from the end of 2018. Doing this we found that the chance of keeping global temperature rise to below 1.5°C was 64%. Delaying a fossil phase out until 2030 would make this a lot less likely, even if the phase out rate was sped up. A coal power plant is typically operated and emits carbon dioxide (CO₂) for about 40 years. So, every new coal plant built in the recent past or today carries a climate change commitment. For instance, Drax, the UK’s largest power plant,  used to burn coal exclusively and has probably warmed the planet by a few ten-thousandths of a degree over its lifetime. That isn’t much by itself, but such warming all adds up. We call this the “committed warming” from fossil fuel infrastructure. Drax now predominately burns wood pellets, and is one example of how fossil fuel infrastructure could be replaced. Our analysis produces a scenario that reduces CO₂ emissions to nearly zero over 40 years. This compares with the recent IPCC special report on 1.5°C, which concluded that reducing emissions to net zero over 35 years was required to get even a 50% chance of limiting global warming to 1.5°C. The additional, narrow window of five years to get to net zero can be explained by different approaches. Some of this difference is accounted for by the timing of emissions phase out. As every year of procrastination brings the date by which we’ll have to reach net zero emissions forward by two years, even delaying until after 2020 rather than 2018 means that a 40 year phase out would have to happen in 36 years to achieve the same outcome. We also used historical temperature observations to guide the future climate response to emissions. Our projection of additional warming for each additional tonne of CO₂ is a little lower than the IPCC’s best estimate, although well within the “likely” range.  Alongside power stations, cars, ships and planes, we also applied the “asset lifetime” assumption to meat cattle. Cows produce a lot of methane, so if we ate them all over the next three years without breeding any more, we could certainly reduce our greenhouse gas emissions considerably while having a gluttonous time doing so. Non-livestock emissions are more tricky and harder to mitigate – we still need to eat, and grow crops using fertilisers – but we assumed that we get more savvy at doing so, eventually reaching zero emissions by 2100.  Our study also comes with a number of important caveats. For instance, we did not delve into whether coal power will be replaced with solar panels or wind turbines, and we weren’t concerned with the exact sort of electric vehicle that will replace the petrol car. What matters to us is that these replacements are zero carbon, or that fossil fuel infrastructure is not replaced at all.  In many cases, zero carbon alternatives do not exist or might be difficult to scale up (for example in aviation). We also assumed that expected lifetime of fossil fuel infrastructure would not increase and that the surviving infrastructure would not be used more intensively (no rebound effects). Lifecycle effects are not included either: with our current high-carbon industry, manufacturing a wind turbine costs energy and creates emissions, even if the energy produced from it does not. In the fossil phase out scenario, we project temperatures would peak at anywhere between 0.1℃ and 0.8℃ above today – this shows the level of uncertainty in how the climate will respond, mostly related to how much warming is masked by air pollution in the present day. Because air pollution reflects sunlight and can brighten clouds, it reduces some of the warming we would otherwise experience. If fossil fuels are phased out, this air pollution will be removed, unmasking the warming. Importantly however, the CO₂ and fossil methane emissions would also reduce at the same time, meaning there would not be a sudden spike in warming from phasing out fossil fuels.  The good news is that the optimistic scenario we looked at in our research found that current global fossil fuel infrastructure is not yet at the point where it will definitely take us over 1.5°C. However, as every year’s delay makes staying below 1.5°C less and less likely, urgent action is still needed.  Indeed, building more of this infrastructure carries an economic cost. If more fossil fuels plants “lock in” more warming, then remedial action would involve premature retirement of these “stranded assets”, or costly negative emissions technologies to take carbon out of the air. We must do our best to avoid fossil fuel lock in and bring forward alternative technologies as quickly as possible."
"

Human life expectancy is rocketing upwards, crops are growing ever bigger, and nutrition is improving dramatically. As we stand witness to the greatest democratization of wealth in human history, how many times do we have to read that environmental disaster is upon us? This time the story is that the North Polar summer icecap is melting, and that computer models prove it is a result of all that pernicious industrial prosperity.



That’s what the casual reader must take from the December 3 issue of Science, and the same day’sWashington Post, citing the work of University of Maryland’s Konstantin Ya Vinnikov. Who claims that the well‐​known (but slight) recession of Arctic sea ice that has occurred in the last five decades is caused by human‐​induced climate change. The reason, Vinnikov says, is that computer models using greenhouse gas warming and sulfate aerosol cooling melt only about as much summer arctic sea ice as has already been lost. When run without human influence, the computer models melt this much ice only 2 percent of the time. We don’t worry about winter ice because everything is frozen up there in that season.



To be fair to the Post, science writer Curt Suplee went out of his way to point out the highly controversial nature of the finding, and that there’s a reasonable argument that Vinnikov’s finding isn’t as solid as it is cracked up to be. But Suplee didn’t have the space to go into the gory details. The problem lies in the “logic” of the study, in which a computer model is used to “prove” something. That requires that the model be correct. But every global warming model has gotten the behavior of 80 percent of the lower atmosphere wrong over the last quarter century, predicting warming where there was none. Because the atmosphere is a stirred fluid, correctly predicting the warming that occurred in the remaining 20 percent can only mean that the right answer was arrived at for the wrong reason. 



Welcome to 21st century science! Perhaps, instead of comparing two computer models, it might be better to see if warming is human induced by looking at the actual temperature history.



Let’s stipulate that the decline in Arctic sea‐​ice is real. But is it a consequence of the largely unknown behavior of the world’s largest natural highball glass? The Arctic Ocean contains the largest mass of floating ice on earth. Lest folks worry that melting this ice will inundate Miami, they might pour a scotch and soda and watch the ice melt. The “sea level” remains the same, even as the drink goes stale. I’ll drink to the notion that a lot of the observed melting is simply a continuation of a long‐​term process that initially had nothing to do with people.



We have a pretty decent history of Arctic temperatures, thanks to the Cold War. That history begins in 1958–about the time we realized we had to know a lot about the Arctic atmosphere, owing to a long‐​standing dispute with our nuclear‐​armed adversaries across the pole. The temperature history has been summarized and is continually updated by Department of Commerce scientist James Angell. What it shows certainly complicates Vinnikov’s computer‐​based analysis.



As is plain and clear, there is no warming at all in the first three decades of these measurements. Then a trend sets in, beginning in 1988, a mere decade ago. How can one melt the ice from 1950 through 1988 when there is no regional warming? In fact, it looks like the ice was in the process of melting long before the initiation of putative human greenhouse warming. Even worse, the period from 1950 to 1975 was one of cooling in the Northern Hemisphere, and still the ice melted.



A more logical explanation than what appears in Science is that the ice has been responding to a rapid and dramatic 2 degree centigrade warming of the Arctic that took place from 1920 to 1940. In other words, it takes many decades for the sea and the ice to adjust to past thermal imbalance. Is this Arctic warming really such a terrible thing? Scientists are pretty sure that the earth was about 1.5ºC warmer 4000 to 7000 years ago than it is now (although there is uncertainty as to why). That means that the summer Arctic ice had to have receded considerably beyond its current position. That era, known as the “hypsithermal period,” was also called the “climatic optimum” in previous generations of textbooks (written before the current “hysterical period”), because it accompanied the rise of agriculture and civilization. 



Maybe it’s not an accident after all that crops are growing better than ever and people are getting rich.
"
"
Share this...FacebookTwitterWorld leaders don't discuss climate? Is it a non-issue?
I’ve been keeping an eye on the Wikileaks disclosure of classified documents, which has deeply embarrassed a wide collection world political leaders, and particularly the US government in general.
I was hoping that among all of these confidential documents, some would be protocols, messages or reports on discussions on climate policy. So I did some trolling in the Internet, hoping to find more on this, but came up with nothing. Admittedly my search was not the most thorough of searches.
Seems very odd that this “vital” issue, one we are told our very survival hinges on, is nowhere mentioned in any of these hot documents. Do you mean to tell me world leaders never talk about it behind the scenes? I have a truly difficult time believing that. Call it the deafening silence.
Either:
a) World leaders, in reality, couldn’t care less about the topic, and thus all the talk about rescuing the climate is just a facade issue in front cameras and mikes to distract the public, meaning the leaders never really talk about it behind the scenes, and so no climate-related protocols exist. This seems highly unlikely to me.
or
b) The leaked documents were selectively disclosed to simply to target and cause personal embarrassment to some world leaders, while others were kept under wraps, meaning climate-related documents were sifted away by Wikileaks or by the media outlets with whom they were entrusted (e.g. the Guardian and Der Spiegel). Selective release with the intent to produce a designed reaction would be immoral, irresponsible, arrogant and a complete abuse of journalistic power.
The danger and the probability here is that Wikileaks, or the media outlets with whom the documents have been entrusted, have carefully cherry-picked the documents in order to engineer and produce a desired political reaction. This is what makes the release of the documents so disturbing, dangerous, and untrustworthy. We probably are getting only the picture that the disclosers want us to see.
I’m not into conspiracy theories, but I do smell a rat. We’ve seen this pattern of behavior time and again in climate science journalism.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




If anyone knows of the existence of any climate-related documents that are already out in the public domain, we would all like to see them. All the data is needed to make an accurate picture of the situation.
================================================
UPDATE 1: h/t Ron de Haan: Wikileaks co-founder on Climategate E-mails:

At 4-min mark:
UK Intelligence tried to frame us as a conduit for the FSB because they didn’t like the truth of what was in those e-mails.”
They’re both hiding things.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterOr to call it what it is: theological propaganda.
Google is also responding to the growing skepticism in climate science by starting “an effort to foster a more open, transparent and accessible scientific dialogue aimed at inspiring pioneering use of technology, new media and computational thinking in the communication of science to diverse audiences.” They’ll start by focusing on communicating the science on climate change.
Read here
That means they are going to help ramp up climate propaganda. According to their press release at Google blog, they’ve started the effort by selecting 21 mid-career Ph.D. scientists who have “the strongest potential to become excellent communicators.” A list of the 21 selected members can viewed at the website.
The selected fellows will participate in a workshop at Google headquarters in California. Google writes:
Following the workshop, fellows will be given the opportunity to apply for grants to put their ideas into practice. Those with the most impactful projects will be given the opportunity to join a Lindblad Expeditions & National Geographic trip to the Arctic, the Galapagos or Antarctica as a science communicator.
Expect to see more calving ice footage – dubbed with dire messages of planetary destruction. Folks, they’ve been crowing this 20 years.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




This is not the Internet giant’s first effort to try to sway public opinion concerning climate change. Last year it launched Google Earth map “which shows how the world would be affected by a global average temperature increase of 4C in a bid to rebuild public trust in climate science,” the UK Telegraph writes here.
Thankfully, trust is real hard to rebuild once lost. And it doesn’t get rebuilt by returning (again) to the old scare-mongering and serial exaggerating. If they truly wish to win back the trust of the public, then they have to start being honest. But they can’t afford that because that would mean the end of their Utopian pipe dream and scam.
And check out this jewel of propaganda made with massaging, über-alarmist Al Gore.

Share this...FacebookTwitter "
"
It never ceases to amaze me how people think when it comes to the Arctic. Somehow there is this pervasive belief that “if we just go there and document it, we’ll be able to demonstrate how climate change is affecting the arctic”.  This is the second team with such dubious aspirations this year, the first being failed kayaker Lewis Gordon Pugh who spun his dismal and embarrassing failure into an “accomplishment”, and then would not even take valid questions about his false claim of being the person who “kayaked furthest north”.
I have no sympathy for these people. Nature is teaching them hard lessons, let us hope they retain the material. – Anthony

STUCK IN THE ARCTIC FOR THREE WEEKS…AND COUNTING
Posted: 	Friday, September 26, 2008 8:20 AM by Jen Brown
From Peter Alexander, TODAY correspondent
So, here we are. In the Arctic. Day 23. Good times!
Producer Paul Manson and I, along with cameraman Callan Griffiths and soundman Ben Adam, were sent here on assignment to report on climate change and the Arctic for an upcoming broadcast. The primary news peg — and one reason for our visit — is that for only the second time in recorded history the Northwest Passage is ice free, effectively clearing this shortcut between Europe and Asia.
Our intention was to stay on board for 10 days, shooting video and interviews.  Mother Nature, apparently, had other plans. Inclement weather, along with an emergency search and rescue mission, has spoiled all five of our attempts to leave the ship.  Getting stuck in the Arctic is not uncommon; getting stuck five times is like punishment.
Joining the team
We left NYC Sept. 3, joining up with a team of scientists from ArcticNet on board the Canadian Coast Guard icebreaker, Amundsen. (In Canada, the Coast Guard is civilian, not military. It is part of the country’s Department of Oceans and Fisheries.) This particular Coast Guard ship has been dedicated to scientific research and outfitted with all the necessary tools. In a unique partnership, the scientists work side-by-side with the Coast Guard crew. For example, the scientists are testing water samples and sediment samples (from the ocean floor) as well as mapping uncharted territories in this remote part of the world. There are 40 scientists, 40 Coast Guard members and the four of us. By now we’re part of the team, learning to help on deck, in the lab and at dinner.
We boarded the Amundsen Thursday, Sept. 4, in Resolute Bay, a small Inuit village, along the Northwest Passage. The plan was to fly off by helicopter at the northern most civilian community in North America, Grise Fjord, and then begin our long journey home. Freezing rain and harsh weather kept our chopper grounded both Monday and Tuesday. The ship kept going and our chance to get off passed. We continued North with the expedition along the coasts of the Canadian Arctic and Greenland, coming within 900 miles of the North Pole.
Over the next couple weeks, we would make three more attempts to fly to land. Each one failed due to weather. Unbelievably, on Thursday our absolute best chance to get off the ship failed, too. The ship was diverted back north to assist a search and rescue mission, something the crew says has only happened once or twice in the last couple years.  From the beginning, we were warned that the ships primary mission was science. The cost of operating this icebreaker and moving the expedition forward is $50,000 a day. While we’ve been welcomed guests on board, we knew the ship wouldn’t be stopping for us. 
Close quarters
Paul and I have been sharing what would normally be the infirmary on this overloaded ship. To our eye, it’s roughly, 10 by 12 feet. A thin curtain is the only thing separating us — and our dignity. Callan and Ben share a bunk bed in a slighter larger room downstairs.






Soundman Ben Adam, producer Paul Manson, cameraman Callan Griffiths and correspondent Peter Alexander



In our 23 days on the ship we have covered more than 2,500 miles. The ship rocks incessantly and a sonar machine used for ocean floor mapping ticks loudly all day and night. It’s akin to being audibly poked day in and day out. (Callan has lovingly promised to buy each of us a metronome when we get home so that will be able to sleep as comfortably in NYC.)
Since we were done shooting two weeks ago, we’ve been left with a lot of time to fill. Meals have become a priority. It’s often the only way we can keep track of what time and day it is. Thursday is a favorite — breakfast crepes. Speaking of crepes, we’ll remind you this is a French-Canadian ship, and so we’ve been more than well fed. In fact, we’re convinced Fabien, the ships pastry chef — yes, I said pastry chef — is trying to kill us slowly with desserts.
Meals are always heavy and large. (Now, so are some of us.) But fear not, there is a fitness club on board. Let us describe it for you: it’s half the size of our bedroom (read: infirmary), and consists of a treadmill, two bikes and a bench that’s hidden beneath a four-foot ceiling. (Running on a treadmill when the ship is rocking could easily pass as its own Olympic sport.) Not to worry, we’ve now collectively run or biked the length of Greenland six times over. The other hours have been spent staring at the ocean, staring in the abyss and staring at each other — followed by routine games of Scrabble, “what’s for dinner?” and “if you could be any kind of animal, what would you be?”
A once-in-a-lifetime experience
Let’s be clear, although we’ve been mentally ready to leave for a long time now, we have seen and done some extraordinary things, including meeting some inspiring scientists whose dedication to their field reminds us daily why we’re here. We’ve seen polar bears, beluga whales and icebergs the size of floating hotels. Each sighting reminds us how far away we are from home. In addition, we’ve seen sea creatures from far below the ocean’s surface that would rival the characters at the Star Wars bar.
The scenery is both breathtaking and intimidating. We’ve been awed by sights that most people will never see and appreciate that this is a once-in-a-lifetime experience. (Hopefully.)
VIDEO: Peter Alexander and Paul Manson phone home to describe the (mis)adventures


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c3cfdff',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterWhen you have an institution that can longer function because of incompetence, corruption, or whatever human element, it is impossible to repair it without first replacing the bad personnel behind the problem. Anything else is like trying to cure someone with a bad liver by treating everything else except the liver itself.That’s the case at the Met Office. If you truly want to reform it, then it has to be purged of its rotten apples. Piers Corbyn reports on the UK parliament transport select committee, click here, into the December cold & snow, and concludes:
THE MET OFFICE’s submission is, I would say: a Mubarak-style, bunkerish, self-serving, denial of reality.”
He then lists the points why, among them is a comment on the usefulness of seasonal forecasts, which, as we know, their own have been completely wrong. Piers writes (emphasis added):
They say ‘accurate regional forecasts on a monthly scale have proved to be useful. Perhaps they are talking about someone else’s forecasts (eg WeatherAction’s). The observed FACT is Met Office seasonal forecasts have demonstrably negative skill. They have consistently – with zero success in all the last 6 unusual (extreme) seasons – misled the public, emergency services and Councils and led to deaths on unsalted roads consequent on ill-advised Council’s believing MetOffice warmist winter forecasts.”
Piers then writes on the Met Office’s commitment in developing forecasting science, calling it “as delusional as Colonel Gaddafi”. To top it all off, the Met Office then has the temerity to expect the British taxpayers to cough up many more millions for super-computers to “improve their forecasts”.
Everybody knows that no matter how good your computers are, if you feed them with garbage, then you get garbage out. I’d rely on Piers and his laptop long before I’d call the clowns at the Met Office.
In its submission, the Met Office does promise to take the steps needed for generating accurate monthly forecasts for the public, adding:
The extent and speed of this development is, of course, dependent on the availability of resources – particularly in supercomputing power to enable modelling to incorporate new science and understanding…..”.
Never let a crisis go to waste.
Indeed the Met Office needs a good house-cleaning. They are attempting to screw the public yet again. And unless the house does indeed get cleaned, more Bitish lives will be put at risk.
Share this...FacebookTwitter "
"

Congratulations to the first winners of the Cato on Campus Op‐​Ed Contest: Mytheos Holt and Šimon Franěk! Their op‐​eds, both dealing with environmental policy, tied for the December 2008 Cato on Campus Op‐​Ed Contest.   
  
  
Mytheos is a junior at Wesleyan University. Citing Cato Senior Fellow in Environmental Studies Patrick Michaels regarding the dangers of excessive environmentalism, Mytheos’s op‐​ed, “Burning the Greenbacks to Save the Greenhouse,” was published by the California Independent Voter Project.   
  
  
Šimon is an 18‐​year‐​old student at Gymnasium Kladno in the Czech Republic. Citing Patrick Michaels to refocus the global climate debate, Šimon’s op‐​ed, “Climate Change vs. Liberty in Europe,” calls attention to the politics behind global warming and the need to consider the ramifications of policy decisions based on global warming.   
  
  
Mytheos and Simon were each sent autographed copies of Patrick Michaels’ books _Meltdown: The Predictable Distortion of Global warming by Scientists, Politicians and the Media _and _Climate of Extremes: Global Warming Science they Don’t Want You to Know_ (just released in January). Their op‐​eds will also be considered for the Cato on Campus Op‐​Ed of the Year, with a chance of receiving a full scholarship to Cato University.   
  
  
Submit your work to one of three Cato student contests.
"
nan
nan
nan
"
This sums the banking issue well.
“Is anyone even paying attention to these Wing Nut AGW people? With 1/2 of America worried about having to eat cat food during their retirement, global warming is the last thing on their mind.”
From “Jeff” in comments


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c2e4a48',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"France is to restrict access to Mont Blanc in the Alps in an effort to halt reckless summit attempts and protect the biodiversity of the mountain and its surroundings. Emmanuel Macron announced the new rules during a visit to Chamonix on Thursday when he visited the famous Mer de Glace (Sea of Ice) glacier, which has shrunk dramatically over the last 20 years.  “What we see with this glacier melting is irrefutable evidence of global warming,” Macron said after visiting the 4.7-mile (7.5km) glacier. He said the fight for biodiversity was urgent and a “fight for our own survival”. “It’s the battle of the century,” Macron said. He announced a new agency, the French Office of Biodiversity (OFB), to monitor and protect the environment. The French government is to establish a protected zone around Mont Blanc and limit the number of people who can access the summit, which at just over 4,800 metres is the highest in western Europe. Climbers will also be asked for proof they have planned their ascent, that they have reserved places at reservations en route and are carrying specified equipment. Macron’s visit came after Jean-Marc Peillex, mayor of Saint Gervais – near Contamines-Montjoie, where a coronavirus outbreak was reported last week – wrote to the Elysée claiming the free-for-all on the mountain was risking lives and damaging the environment. “It is all well and good to worry about the Amazon rainforest, but to ignore what is happening on Mont Blanc and to allow this disrespect to continue is intolerable,” Peillex wrote last September. He said “oddballs” were polluting the mountain, citing a recent attempt by a former British Royal Marine to summit Mont Blanc for charity while carrying a full-sized rowing machine, which he abandoned on the path down. As many as 30,000 people attempt to climb Mont Blanc every year – around 200-300 a day. Some ignore weather and safety warnings and many leave rubbish on the mountain. Peillex continued: “The difficulty is that if we start listing everyone’s madcap schemes then we’ll finish up with an endless list. There will always be someone willing to do something new or original that new rules haven’t imagined. The idea is to be reasonable. What’s this site for? Is it a place for opera or to advertise desserts? Isn’t it better to give it back its original vocation for mountaineering and skiing? If the president supports this kind of thinking we will have won. “Today, 99% of people are shocked by what happens on the summit of Mont Blanc … Let us allow people to go on Mont Blanc, but let us set the rules. When you go to someone’s house, you’re not the one who sets the rules. You don’t put your feet on their table. It’s the same here.” The Mer de Glace has lost more than 65 metres in depth and 300 metres in length since 1996."
"
Share this...FacebookTwitterDi kesempatan kali ini , Agen Poker online Indonesia akan mengemukakan beberapa info berkaitan bagaimanakah cara untuk Tingkatkan Kesempatan Menang Dalam Bermain Poker itu. Info kesempatan ini bisa kalian aplikasikan baik bagi kalian yang masih pemula dalam bermain poker atau kalian yang sudah pakar dalam bermain poker bentuk online itu. Mengenai info itu bisa kalian lihat serta kalian baca seperti berikut ini :
Mengerti basic dari permainan. Permainan apa pun yang kalian mainkan semua tentu mempunyai yang namanya ketentuan serta basic dari permainan tersebut tidak kecuali permainan poker itu. Permainan poker itu biasanya sama juga dengan permainan poker berbentuk online. Karena itu, jika kalian sudah kuasai permainan poker berbentuk konvesional itu tentu saja kuasai permainan poker berbentuk online itu.
Cara Jitu Dalam Bermain Poker Online Indonesia
Cobalah mengingat rutinitas musuh. Ini ialah langkah paling akhir yang dapat kalian kerjakan dalam bermain poker online, yakni dengan memperhatikan gerak musuh kalian seperti rutinitas musuh kalian. Ini bermanfaat untuk kalian dalam memastikan langkah apa yang perlu kalian kerjakan waktu musuh tengah lakukan kesukaannya itu. Satu diantara contoh dari hal itu bisa kalian lihat dari kartu yang sedang digengam oleh pemain itu. Jika kartu yang dipegangnnya itu ialah kartu yang lumayan bagus, tentu raut muka atau ekspresi yang diperlihatkan itu ialah raut muka suka dan lain-lain.
Dana taruhan. Jika kalian sudah lakukan 2 hal seperti yang kami berikan di atas itu langkah paling akhir yang perlu kalian kerjakan ialah mulai mempersiapkan dana yang cukup waktu bermain poker online itu pada bandar yang kalian percayai. Coba untuk mempersiapkan dana yang cukup di luar dari dana keperluan inti atau dana genting sebab ke-2 dana itu sebenarnya penting.
Hindari Berekspresi Berlebihan Dalam Bermain Poker Online Indonesia
Jauhi untuk berekspresi terlalu berlebih. Ini hal yang paling penting dalam bermain Poker Online Indonesia sebab beberapa pemain tentu bisa memeperkirakan kartu apa yang ada ditangan kalian hanya dengan melihat ekpresi kalian. Tentu saja kalian akan terlihat suka waktu tengah memperoleh kartu yang bagus serta begitupun sebaliknya. Untuk menghindari hal yang tidak dapat ditebak, lebih baik kalian menghindari berekspresi terlalu berlebihan saat bermain hingga musuh kalian tidak bisa memprediksi siapa serta kartu apa yang ada ditangan kalian sekarang.
Demikian beberapa info yang dapat kami berikan berkaitan bagaimanakah cara untuk tingkatkan kesempatan dalam bermain poker di situs poker online sah itu. Mudah-mudahan lewat info yang kami berikan itu bisa menolong kalian dalam bermain poker itu hingga potensi kalian dalam bermain poker itu makin baik daripada awalnya. Serta mudah-mudahan lewat info itu dapat juga tingkatkan kesempatan kalian dalam bermain poker itu hingga prosentase kemenangan kalian dalam bermain makin tinggi dibanding awalnya.
Share this...FacebookTwitter "
nan
"
Share this...FacebookTwitterJohn Kerry is a co-sponsor of the latest Cap & Trade bill, which thank God has been put off indefinitely. The bill, should it become law, God forbid, would force all working Americans to pay more for energy and to live more humbly.
But living humbly and paying more taxes to the government applies only to the little guys. For the rich, elite and privileged, like Kerry, they’d continue to fly around in private jets, be chauffeured in limousines and frolic the seas in big yachts. Watch Kerry’s reaction when confronted about skirting Massachusetts taxes by birthing his new 76 foot $7 million yacht in Rhode Island:
[SEE VIDEO]
By berthing the yacht in Rhode Island, Kerry skirts paying nearly $450,000 in sales tax and a yearly $70,000 excise tax bill to his own home state. Look at how pissy he gets when confronted by the media.
Worse, Kerry, who claims to be fighting for American jobs, had the yacht built in New Zealand. The same yacht could have been built at a yard in his own state of Massachusetts. So much for American jobs. And what about environmental friendliness?
What does a 76-foot yacht include?
According to the Boston Herald, Kerry’s yacht has two cabins, a pilot house fitted with a wet bar and cold wine storage.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




I wasn’t able to get further specific information on Kerry’s yacht, but looking at other typical 76-foot yachts on the market, we find that they are far from spartan. Take this 76-foot Monte Fino for only $2,450,000 – a real bargain when compared to Kerry’s $7 million cruiser. The Monte Fino includes a 10,000 liter diesel fuel tank, 1500 hp twin engines and is filled with high-tech electronic doo-dads.
God knows what Kerry got for his $7 million.
Kerry spokesman David Wade said Friday the boat is being kept at Newport Shipyard not to evade taxes, but “for long-term maintenance, upkeep and charter purposes.”
John Kerry is married to Theresa Heinz, who is millionaire heiress to the Heinz ketchup fortune, and is a philanthropist and environmentalist.
When you’re rich, you can do things like this. It’s okay. But these rich people should not be making it much harder for the rest of us to make ends meet.
When confronted by the media in the above clip, Kerry defends berthing the floating palace in tax-haven Rhode island, claiming he is paying his taxes. Then he scuttles away – not on a bicycle or in a hybrid car – but in a chauffeured SUV.
“Can I get outa here please!”, he orders his chauffeur.
And let’s not forget Sen. Jeff Greene and 145 ft yacht dragging anchor through coral reef: http://www.miamiherald.com/2010/07/23/1743175/greene-denies-his-anchor-damaged.html
Share this...FacebookTwitter "
"
Share this...FacebookTwitterToday I’m coming out a day early and declaring July 2010 as the slowest melting July since the AMSR-E satellite record has been kept. The once ballyhooed “death spiral” is dead.
Reminds me of that line in Tarantino’s cult film Pulp Fiction:
“Who’s Zed?”
“Zed? Zed is dead.”
At the end of June I recall seeing lots of headlines in the newspapers about a record Arctic sea ice melt occurring. Words like “alarming” and “unprecedented” were used liberally. The reports were splashed with pictures of polar bears for added effect.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




One month later the media are completely silent. As the following graphic shows, this July’s Arctic sea ice melt was the slowest since this dataset has been kept.  Click Here.
Here are the numbers for the amount of July-melt in million square kilometers:
Year      6/30 to 7/30
2003           2.25
2004           2.08
2005           2.52
2006           2.11
2007           3.00
2008           2.45
2009           2.81
2010          1.85
It was the first time that July failed to reach 2 million sq. km. Now 2010 is on track to reach last year’s low. So far the Arctic has been cold this summer, one of the coldest summers north of 80°N on record, Click Here.
What’s the forecast?
Meteorologist Joe Bastardi projects a significant Arctic sea ice recovery in the couple of years ahead, flying in the face of predictions made by climate “scientists”. Bastardi’s claim is in line with the latest NOAA seasonal forecasts.
NWS/NCEP forecasts a cold Arctic in the months ahead.
La Nina is strengthening and global temps, dare I say, are beginning a death-spiral of their own.
Share this...FacebookTwitter "
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   




The role that anthropogenic “global warming” from the emissions of greenhouse gases from the combustion of fossil fuels plays is debatable—both in timing and magnitude. Almost certainly its influence is present and detectable in the U.S. annual average temperature record, but beyond that simple statement, not a whole lot more can be added with scientific certainty.



We now stand nearly a year later with more evidence of proof and point.   
  
  
Through November of this year, the U.S. average temperature is only 0.53°F above the 20th century mean temperature (the default baseline used by NCDC). Last year the annual temperature was 3.24°F above it.   






Figure 1. Average January‐​November temperature in the contiguous United States from 1895–2013 as compiled by the National Climatic Data Center (source: NCDC, Climate at a Glance).   
  
  
With the cold start to December across the country, the annual temperature for 2013 has an increasingly good shot at coming in below the 20th century average. For this to happen, the U.S. temperature for December would have to average about 27.6°F. For the first 12 days of the month, the average has been 28.4°F, and the forecast is for continued cold, so getting to the needed temperature is not out of the question.   
  
  
If 2013 does come in below the 20th century average, it would be the first year since 1996 to have done so, and would end a 16‐​year long run of above average annual temperature for the U.S. You can follow the chase here.   
  
  
But even if the rest of the month is not quite cold enough to push the entire year into negative territory, the 2013 annual temperate will still be markedly colder than last year’s record high, and _will be the largest year‐​over‐​year decrease in the annual temperature on record_ , underscoring the “outlier” nature of the 2012 temperatures.   
  
  
Will 2013 mark the end of the decade and a half period of abnormal warmth experience across the U.S. that was touched off by the 1998 El Niño event, and a return to conditions of the 1980s and early‐​to‐​mid 1990s? Or will 2013 turn out to just be a cold blip in the 21st century U.S. climate?   
  
  
In either case, 2013 shows that the natural variability of annual temperatures in the U.S. is high (as is decadal and multi‐​decadal variability, see Figure 1)—an important caveat to keep in mind when you face the inundation of every‐​weather‐​event‐​is‐​caused‐​by‐​human‐​global‐​warming hysteria.   
  
  
Stay tuned!   
  
  
_The Center for the Study of Science would like to thankRyan Maue of WeatherBELL Analytics for his summary of December temperatures and the expected temperatures for the rest of the year._
"
"In my 2010 book, Crisis Economics, I defined financial crises not as the “black swan” events that Nassim Nicholas Taleb described in his eponymous bestseller but as “white swans”. According to Taleb, black swans are events that emerge unpredictably, like a tornado, from a fat-tailed statistical distribution. But I argued that financial crises, at least, are more like hurricanes: they are the predictable result of builtup economic and financial vulnerabilities and policy mistakes. There are times when we should expect the system to reach a tipping point – the “Minsky Moment” – when a boom and a bubble turn into a crash and a bust. Such events are not about the “unknown unknowns” but rather the “known unknowns”. Beyond the usual economic and policy risks that most financial analysts worry about, a number of potentially seismic white swans are visible on the horizon this year. Any of them could trigger severe economic, financial, political and geopolitical disturbances unlike anything since the 2008 crisis. For starters, the US is locked in an escalating strategic rivalry with at least four implicitly aligned revisionist powers: China, Russia, Iran and North Korea. These countries all have an interest in challenging the US-led global order and 2020 could be a critical year for them, owing to the US presidential election and the potential change in US global policies that could follow. Under Donald Trump, the US is trying to contain or even trigger regime change in these four countries through economic sanctions and other means. Similarly, the four revisionists want to undercut American hard and soft power abroad by destabilising the US from within through asymmetric warfare. If the US election descends into partisan rancour, chaos, disputed vote tallies and accusations of “rigged” elections, so much the better for rivals of the US. A breakdown of the US political system would weaken American power abroad. Moreover, some countries have a particular interest in removing Trump. The acute threat that he poses to the Iranian regime gives it every reason to escalate the conflict with the US in the coming months – even if it means risking a full-scale war – on the chance that the ensuing spike in oil prices would crash the US stock market, trigger a recession, and sink Trump’s re-election prospects. Yes, the consensus view is that the targeted killing of Qassem Suleimani has deterred Iran but that argument misunderstands the regime’s perverse incentives. War between US and Iran is likely this year; the current calm is the one before the proverbial storm. As for US-China relations, the recent phase one deal is a temporary Band-Aid. The bilateral cold war over technology, data, investment, currency and finance is already escalating sharply. The Covid-19 outbreak has reinforced the position of those in the US arguing for containment and lent further momentum to the broader trend of Sino-American “decoupling”. More immediately, the epidemic is likely to be more severe than currently expected and the disruption to the Chinese economy will have spillover effects on global supply chains – including pharma inputs, of which China is a critical supplier – and business confidence, all of which will likely be more severe than financial markets’ current complacency suggests. Although the Sino-American cold war is by definition a low-intensity conflict, a sharp escalation is likely this year. To some Chinese leaders, it cannot be a coincidence that their country is simultaneously experiencing a massive swine flu outbreak, severe bird flu, a coronavirus outbreak, political unrest in Hong Kong, the re-election of Taiwan’s pro-independence president, and stepped-up US naval operations in the East and South China Seas. Regardless of whether China has only itself to blame for some of these crises, the view in Beijing is veering toward the conspiratorial. But open aggression is not really an option at this point, given the asymmetry of conventional power. China’s immediate response to US containment efforts will likely take the form of cyberwarfare. There are several obvious targets. Chinese hackers (and their Russian, North Korean, and Iranian counterparts) could interfere in the US election by flooding Americans with misinformation and deep fakes. With the US electorate already so polarised, it is not difficult to imagine armed partisans taking to the streets to challenge the results, leading to serious violence and chaos. Revisionist powers could also attack the US and western financial systems – including the Society for Worldwide Interbank Financial Telecommunication (Swift) platform. Already, the European Central Bank president, Christine Lagarde, has warned that a cyber-attack on European financial markets could cost $645bn (£496.2bn). And security officials have expressed similar concerns about the US, where an even wider range of telecommunication infrastructure is potentially vulnerable. By next year, the US-China conflict could have escalated from a cold war to a near hot one. A Chinese regime and economy severely damaged by the Covid-19 crisis and facing restless masses will need an external scapegoat, and will likely set its sights on Taiwan, Hong Kong, Vietnam and US naval positions in the East and South China Seas; confrontation could creep into escalating military accidents. It could also pursue the financial “nuclear option” of dumping its holdings of US Treasury bonds if escalation does take place. Because US assets comprise such a large share of China’s (and, to a lesser extent, Russia’s) foreign reserves, the Chinese are increasingly worried that such assets could be frozen through US sanctions (like those already used against Iran and North Korea). Of course, dumping US Treasuries would impede China’s economic growth if dollar assets were sold and converted back into renminbi (which would appreciate). But China could diversify its reserves by converting them into another liquid asset that is less vulnerable to US primary or secondary sanctions, namely gold. Indeed, China and Russia have been stockpiling gold reserves (overtly and covertly), which explains the 30% spike in gold prices since early 2019. In a sell-off scenario, the capital gains on gold would compensate for any loss incurred from dumping US Treasuries, whose yields would spike as their market price and value fell. So far, China and Russia’s shift into gold has occurred slowly, leaving Treasury yields unaffected. But if this diversification strategy accelerates, as is likely, it could trigger a shock in the US Treasuries market, possibly leading to a sharp economic slowdown in the US. The US, of course, will not sit idly by while coming under asymmetric attack. It has already been increasing the pressure on these countries with sanctions and other forms of trade and financial warfare, not to mention its own world-beating cyberwarfare capabilities. US cyber-attacks against the four rivals will continue to intensify this year, raising the risk of the first-ever cyber world war and massive economic, financial and political disorder. Looking beyond the risk of severe geopolitical escalations in 2020, there are additional medium-term risks associated with climate change, which could trigger costly environmental disasters. Climate change is not just a lumbering giant that will cause economic and financial havoc decades from now. It is a threat in the here and now, as demonstrated by the growing frequency and severity of extreme weather events. In addition to climate change, there is evidence that separate, deeper seismic events are under way, leading to rapid global movements in magnetic polarity and accelerating ocean currents. Any one of these developments could augur an environmental white swan event, as could climatic “tipping points” such as the collapse of major ice sheets in Antarctica or Greenland in the next few years. We already know that underwater volcanic activity is increasing; what if that trend translates into rapid marine acidification and the depletion of global fish stocks upon which billions of people rely? As of early 2020, this is where we stand: the US and Iran have already had a military confrontation that will likely soon escalate; China is in the grip of a viral outbreak that could become a global pandemic; cyberwarfare is ongoing; major holders of US Treasuries are pursuing diversification strategies; the Democratic presidential primary is exposing rifts in the opposition to Trump and already casting doubt on vote-counting processes; rivalries between the US and four revisionist powers are escalating; and the real-world costs of climate change and other environmental trends are mounting. This list is hardly exhaustive but it points to what one can reasonably expect for 2020. Financial markets, meanwhile, remain blissfully in denial of the risks, convinced that a calm if not happy year awaits major economies and global markets. • Nouriel Roubini is a professor at NYU’s Stern School of Business and was senior economist for international affairs in the Clinton White House’s Council of Economic Advisers. He has worked for the IMF, the US Federal Reserve and the World Bank. © Project Syndicate"
"Delta announced an ambitious plan on Friday to become the first US airline to go carbon neutral, committing $1bn over the next 10 years to mitigate all emissions from its global business. The move by Delta will put pressure on other airlines to follow suit at a time when the UN is warning that airplane emissions of carbon dioxide will triple by 2050.  Carbon dioxide emitted by airlines increased by 32% from 2013 to 2018, according to a study by the International Council on Clean Transportation. A Guardian analysis found long-haul flights generate more carbon emissions than the average person in dozens of countries around the world produces in a year. “There’s no challenge we face that is in greater need of innovation than environmental sustainability, and we know there is no single solution. We are digging deep into the issues, examining every corner of our business, engaging experts, building coalitions, fostering partnerships and driving innovation,” said Ed Bastian, Delta’s chief executive.  Delta outlined a series of efforts it will use to reach its goal: The company will seek to reduce its carbon footprint by decreasing the use of jet fuel and increasing efficiency. Delta will seek to offset its carbon emissions by investing in carbon removal programs in forestry, wetland restoration, grassland conservation, marine and soil capture and other negative emissions technologies. The company will seek coalitions with its employees, suppliers, global partners, customers, industry colleagues, investors and other stakeholders to reduce their carbon footprint. Delta is the latest big company in a heavily polluting industry to announce plans to go carbon neutral. This week energy company BP announced plans to reduce its carbon footprint to net zero by 2050, which has been met with skepticism by climate activists. Peter Miller, carbon offsets expert at the Natural Resources Defense Council, said he was encouraged to see Delta spending money to reduce its carbon footprint but said that there were at present few low-carbon options available at scale for airline companies. “It will be critical that the offsets they purchase are credible and real,” he said. “And that is certainly possible. It’s a whole lot better than doing nothing.”"
"

Mr. Chairman, distinguished members of the subcommittee:



My name is Roger Pilon. I am a senior fellow at the Cato Institute and the director of Cato’s Center for Constitutional Studies.



I want to thank Chairman Hyde of the committee and Chairman Coble of the subcommittee for their invitations to me to testify on the important issue of “Judicial Misconduct and Discipline.” These hearings have been called, I understand, because of a concern that a number of people have expressed about “judicial activism”–the practice by judges of applying to cases before them not the law but principles or values that are no part of the law. Because such a practice is thought by many to constitute judicial misconduct, some in Congress are searching for ways to discipline it.



 **I. Summary**



At the outset, let me summarize my thoughts on this subject, then discuss it in somewhat more detail. There can be no question that judicial activism, as just described, has been a problem in our legal system for some time. The power of the judiciary under our Constitution to declare the law and decide cases under that law is awesome; when abused, that power is too often beyond reach. At the same time, I believe that many of those who have complained most often about judicial activism have overstated and misstated the problem, thus distracting us from the real issue–legislative activism on the part of Congress, which leads to judicial activism.



Overstating the problem. Many of the examples of “judicial activism” that are cited turn out, when examined more closely, not to be cases in which the judge failed to apply the law but applied the law differently, or applied different law, to reach a result different than the result thought correct by the person charging activism. To be sure, there is no bright line between failing to apply the law and wrongly applying the law or applying the wrong law, but when that distinction is drawn, it turns out that there are fewer cases of true judicial activism than at first may appear.



Misstating the problem. More importantly, the problem of “judicial activism” is seriously misstated when it is cast, as it often is, as involving judges overruling the will of the people. In our legal system, judicial review often requires a judge to do just that. In such a case, were the judge to defer to the political will, exercising “judicial restraint” when the law requires active judicial intercession, that restraint would itself be a kind of activism, for it would amount to an “active” failure to apply the law in deference to democratic or majoritarian values. The judge in such circumstances would be shirking his judicial responsibilities every bit as much as if he overrode a legitimate exercise of political will in the name of other values.



Thus, as terms of art, judicial “activism” and “restraint” can be quite confusing and even misleading. What is more, they are often used in ways that camouflage the real issues. What we all want, I assume, is judges who are neither “active” nor “restrained” but “responsible”–responsible to the law. But when the law is unclear or inconsistent, judicial responsibility may be difficult to achieve–and “activism” inevitable. In the end, therefore, our substantive law may be the ultimate source of the problem before us today. That, in fact, is what I will argue shortly. Let me begin, however, with a brief overview of the complaints. [1]



 **II. The Critics of Judicial Activism**



Complaints about “judicial activism,” however formulated, can be found from our inception as a nation. In their modern form, however, they have come largely since the advent of the Warren Court and most often from political conservatives. My fellow panelist today, Professor Lino Graglia, with whom I have debated the issue more than once, has put the complaint starkly:



… the thing to know to fully understand contemporary constitutional law is that, almost without exception, the effect of rulings of unconstitutionality over the past four decades has been to enact the policy preferences of the cultural elite on the far left of the American political spectrum. [2]



“That is exactly right,” comments Judge Robert Bork in his recent best‐​seller, _Slouching Towards Gomorrah_ , “and the question is what, if anything, can be done about it.” [3] I gather that these hearings are a partial answer to that question.



The bitter confirmation battle that followed Judge Bork’s Supreme Court nomination a decade ago had a way of concentrating the issue for many, of course. Still, the issue has been in the air since the 1950s, covering subjects as various as civil rights, apportionment, federalism, speech, religion, abortion, education, criminal law and procedure, and much else. And in each case, the complaints from conservatives have been essentially the same.



Speaking before the Federalist Society’s 10th anniversary lawyers convention last November, for example, Senator Orrin Hatch, chairman of the Senate Judiciary Committee, summarized the issue from his perspective:



What is at stake … is nothing less than our right to democratic self‐​government as opposed to … “Government by Judiciary.” For when we commission judicial activists who distort the Constitution to impose their own values, policy preferences, or visions of what is just or right, we are in effect sacrificing our ability to govern ourselves through the democratic political processes to the whims and preferences of unelected, life‐​tenured platonic guardians. [4]



Judges “must _interpret_ the law, not legislate from the bench,” Senator Hatch continued. “A judicial activist, on the left _or_ the right, is not, in my view, qualified to sit on the federal bench.” [5]



In a similar vein, little more than two months ago Senator John Ashcroft, chairman of the Constitution Subcommittee of the Senate Judiciary Committee, told the Conservative Political Action Conference at its annual meeting that it was time “to take a broader, comprehensive look at the alarming increase in activism on the court.” [6] Asking what we can do to put an end to “judicial tyranny,” Senator Ashcroft called for rejecting “judges who are willing to place private preferences above the people’s will.” [7]



Not to be outdone by the Senate, on March 11 House Majority Whip Tom DeLay told editors and reporters at the _Washington Times_ that “as part of our conservative efforts against judicial activism, we are going after judges” and are “right now” writing articles of impeachment. [8] Those sentiments were echoed two days later by Congressman Bob Barr of this subcommittee when he appeared on CNN’s “Crossfire.” Clearly, perhaps as never before, the issue of judicial activism is on the nation’s agenda. [9]



 **III. Overstating the Problem**



It is not entirely clear just what has brought the judiciary and its methods to the nation’s attention at this point in time. Cynics point to the need for something–some issue–in a drifting Republican Party: “The revolution is in the doldrums. Nobody’s got a plan; nobody’s got a direction.” [10] Others, however, have noted a rising frustration among conservatives over their relative ineffectiveness on the judicial front despite having dominated the judicial selection process since the Nixon years. [11] And still others cite a series of recent cases that have seemed to crystalize complaints about judicial activism: the district judge who stayed the California Civil Rights Initiative (CCRI); [12] the New York judge who suppressed evidence in a drug case, saying the police had no reason to stop the suspects; [13] the decision by the Supreme Court that the Virginia Military Institute had to become coeducational. [14]



Looked at in broad perspective, there can be no question that the drift in American law over the past 40 years and more has been in large part to the left, as that term is ordinarily understood. And a good part of that drift has resulted from court decisions. Yet by no means can all or even most of the drift be attributed to the courts. Moreover, even that part that has resulted from court decisions does not arise entirely or even primarily from “judicial activism”–not unless that idea is stretched to include every decision that conforms to some leftist political agenda.



In fact, when we look at most such decisions closely, we rarely find that the judge or justices “legislated.” To be sure, they often reach results consistent, if not with their “whims,” at least with their “values, policy preferences, or visions of what is just or right.” But those results can usually be tied to some legal anchor, even if it takes some stretch to do so.



Take the recent CCRI decision by U.S. District Court Judge Thelton Henderson, which enjoined enforcement of the initiative shortly after it was passed by some 54 percent of California’s voters. Many critics of the judiciary immediately pointed to the decision as a blatant example of judicial activism. Judge Henderson’s opinion was a stretch, to be sure. But it was not without legal foundation, citing _Hunter v. Erickson_ , 393 U.S. 385 (1969) and _Washington v. Seattle School District No. 1,_ 458 U.S. 457 (1982). Moreover, as we know, the case has taken the normal appellate course; the decision has since been reversed by the U.S. Court of Appeals for the Ninth Circuit; [15] and plaintiffs have just filed a petition for certiorari with the Supreme Court. We are likely to learn from the Court whether the cases Judge Henderson relied upon in fact apply or are still good law. In the meantime, however, we are hard pressed to say that his decision was “lawless,” however strained it may have been.



One could review putative cases of judicial activism almost _ad infinitum_ , of course, but the fact remains that the better part of such cases do not exhibit judicial lawmaking, just better or worse judicial reasoning. It is no small irony, however, that when we do come across a genuine case of blatant judicial activism that cuts the other way, politically, many conservative critics of the judiciary are strangely silent. That was pointed out just last week, for example, by conservative constitutional scholar Bruce Fein in an op‐​ed in the _New York Times_ , citing the current controversy over the decision of an Alabama state judge to defy a long line of Supreme Court rulings on the separation of church and state “by posting a copy of the Ten Commandments in his courtroom and inviting clergy to lead juries in prayer,” [16] even after a state appellate court found the practices unconstitutional.



 **IV. Misstating the Problem**



In the end, therefore, those who are concerned about judges who seem always to be leaning to the left may be better advised to look less to the judicial role in our system–to the practice of judicial review–and more to the reasoning judges employ in performing their roles and, more importantly, to the sources they employ when doing their reasoning. Bad reasoning is just that and should be called that, not called judicial “activism.” But bad law, from which so much bad reasoning proceeds, is another matter. We should hardly be surprised that judges today are thought so often to be engaged in “judicial activism” when they are called upon so often to apply law that is inconsistent, incoherent, and fairly invites them to make all manner of value judgments. In such circumstances, they can hardly be seen to be doing anything but legislate.



We come, then, to what in fact is the crux of the matter. Under our system of law, the role of the judge should be much simpler than it has come to be. The problem, however, does not go back just 40 years, as too many conservatives believe. Rather, its institutional roots are in the New Deal. And its ideological roots are in the Progressive Era, when we stopped thinking of government as a “necessary evil,” as the Founders had conceived of it, and started thinking of government as an engine of good, an instrument for solving all manner of social and economic problems. Standing in the way of carrying out that agenda, of course, was a constitution that established a government of limited, enumerated powers–a constitution that held, more or less, until the New Deal. As we all know, however, when President Roosevelt was unable to get his programs past the Court–there being no authority for them under the Constitution–he threatened to pack the Court with six additional members. Not even Congress would go along with that. Nevertheless, the Court got the message; there was the famous switch in time that saved nine; and by 1938 the Court had essentially turned the Constitution on its head, as New Deal architect Rexford Tugwell would later tell us the administration meant for it to do. [17]



In a nutshell, a document of delegated, enumerated, and thus limited powers became in short order a document of effectively unenumerated powers, limited only by rights that would thereafter be interpreted narrowly by conservatives on the Court and episodically by liberals on the Court. Both sides, in short, would come to ignore our roots in limited government, buying instead into the idea of vast majoritarian power–the only disagreement being over what rights might limit that power and in which circumstances. Indeed, we need look no further than to Judge Bork–no liberal he–to see the new vision stated–and wrongly ascribed to James Madison. The “Madisonian dilemma” that constitutional courts face, Bork tells us, is this:



[America’s] first principle is self‐​government, which means that in wide areas of life majorities are entitled to rule, if they wish, simply because they are majorities. [It’s second principle is] that there are nonetheless _some_ things majorities must not do to minorities, _some_ areas of life in which the individual must be free of majority rule. [18]



That gets the Madisonian vision exactly backward, of course. America’s first _political_ principle may indeed have been self‐​government, but its first _moral_ principle–and the reason the people instituted government at all–was individual liberty, as the Declaration of Independence makes plain for “a candid world” to see.



Indeed, we did not throw off a king only to enable a majority to do what no king would ever dare. Rather, the Founders instituted a plan whereby in “wide areas” individuals would be entitled to be free simply because they were born so entitled, while in “some” areas majorities would be entitled to rule not because they were inherently so entitled but as a practical compromise.



That gets the order right: individual liberty first; self‐​government second, as a means toward securing that liberty–with wide berths to state governments, which were later reined in by the Civil War Amendments. That is why the Constitution enumerated the powers of Congress and the executive, to limit them. And that is why the Bill of Rights concludes with the Ninth and Tenth Amendments: to make clear that Americans begin and end with their rights, enumerated and unenumerated alike, while government proceeds only with the power it is given.



The New Deal changed all that, of course, not by amending the Constitution, the proper method, but by radically reinterpreting it: in particular, by reading the General Welfare and Commerce Clauses not as shields against power, as they were meant to be, but as swords of power; then by turning the Bill of Rights into a document of “fundamental” and “nonfundamental” rights. [19] None of that was found plainly in the Constitution–to the contrary, the entire document tends plainly the other way. Rather, it was invented virtually out of whole cloth, by the New Deal Court, to make way for the New Deal’s political agenda.



Our modern problem of overweening, inconsistent, incoherent statutory law began, then, not with an activist Court–to the contrary–but with an activist Congress and executive branch, bent on expanding government power. In time, however, the problem was abetted by an activist Court–succumbing to pressure from the political branches. But as noted earlier, the Court’s “activism” was not as we think of it today–a search for rights not apparent in the Constitution. Rather, it was activism in finding rationales for power–what conservatives today call deference to the political branches.



It needs to be said again, however, that the New Deal Court’s activism was not entirely without legal foundation. The sources for the Court’s rulings were there, in the Constitution, even if it did take a high degree of creativity, to be charitable, to draw them out, and even if doing so did fly in the face, for the most part, of a century and a half of constitutional jurisprudence that went the other way.



We come, then, to the bottom line in all of this. Law, including constitutional law, is not written in immutable stone. It is to some extent malleable, of necessity, and is given life by those charged with giving it life–the judiciary. In doing their work, however, judges do not work in a vacuum. They work instead in a larger political climate. If we who shape that climate persist in believing that it is proper for government to be addressing our every problem, no matter how trivial or personal, and persist in believing that our Constitution can legitimately be read to authorize that result, then we should not be surprised that the judiciary is dragged along to play its part in the process–today, often, to try to undue the mess that legislatures make of the effort. [20]



Yes, judges today often thwart the majoritarian will–as a vestige, perhaps, of their former principal role. Just as often, however, a judge may see himself as simply a facilitator in the grand enterprise of government. We are coming to the close of what has rightly been called the century of government–more accurately, the century of failed government planning. If we are unhappy with the role the judiciary sometimes plays in this setting, it may be that we need to look first to the material we give judges to work with–the reams of statutory material we have enacted over the course of the century.



The Founders had a simpler vision in mind when they set out to craft our legal order. They left most human affairs to private ordering, not to government planning. That gives the judiciary–and Congress–relatively little to do. Is that not what critics of judicial activism want?



A curriculum vitae is attached. Pursuant to House Rule XI, clause 2(g)(4), neither I nor the Cato Institute receives any federal funds–as a matter of principle.



[1] I have discussed the issues that follow more fully in: “Congress, the Courts, and the Constitution,” _Cato Handbook for Congress_ (105th Congress), ch. 3 (esp. pp. 36–42), (1997); “A Government of Limited Powers,” _Cato Handbook for Congress_ (104th Congress), ch. 3 (1995) (reprinted as “Restoring Constitutional Government,” _Cato’s Letter No. 9_ (1995)); “Rethinking Judicial Restraint,” _Wall Street Journal_ , Feb. 1, 1991, at A10 (op‐​ed); “Constitutional Visions,” _Reason_ , Dec. 1990, at 39–41 (review of Robert Bork’s _The Tempting of America_ ); “Legislative Activism, Judicial Activism, and the Decline of Private Sovereignty,” in _Economic Liberties and the Judiciary_ (J. Dorn & H. Manne eds., 1987); and “On the Foundations of Justice,” 17 _Intercollegiate Rev._ 3 (1981).



` `[2]Lino Graglia, “It’s Not Constitutionalism, It’s Judicial Activism,” 19 _Harvard Journal of Law & Public Policy_, 293, 298 (Winter 1996).



[3]Robert H. Bork, _Slouching Towards Gomorrah_ 114 (1996).



[4]“Remarks of Sen. Orrin Hatch Before the Federalist Society’s 10th Anniversary Lawyers Convention,” Senate Judiciary Committee News Release, Nov. 15, 1996, at 4.



[5]Id., at 5 (original emphasis).



[6]John Ashcroft, “Courting Disaster: Judicial Despotism in the Age of Russell Clark,” March 6, 1997, at 4 (MS available from the office of Senator Ashcroft).



[7]Id., at 3.



[8]Ralph Z. Hallow, “Republicans out to impeach ‘activist’ jurists,” _Washington Times_ , March 12, 1997, at 1. See also Katharine Q. Seelye, “House G.O.P. Begins Listing A Few Judges to Impeach,” _New York Times_ , Mar. 14, 1997, at A24.



[9]This very brief overview barely touches on the vast body of both scholarly and popular literature on the subject, to say nothing of political activism about judicial activism. In this last category, for example, is the Judicial Selection Monitoring Project of the conservative Free Congress Foundation’s Center for Law & Democracy, which on January 27, on behalf of 260 grassroots organizations and 35 radio and television talk show hosts, petitioned President Clinton and members of the Senate to nominate and confirm only those candidates for the federal bench who are committed to judicial restraint.



[10]Michael Kelly, “TRB from Washington: Judge Dread,” _The New Republic_ , Mar. 31, 1997, at 6. See also Laurie Kellman, “Republicans rally ’round judge‐​impeachment idea,” _Washington Times_ , Mar. 13, 1997, at A1: “The plan is aimed in part at reviving Republican morale, which has flagged this year because of Mr. Gingrich’s ethics troubles and the majority’s sparse floor schedule,” at A18.



[11]See, _e.g._ , Terry Eastland, “Deactivate the Courts,” _The American Spectator_ , Mar. 1997, at 60. For a fuller treatment of why conservative efforts to influence the courts have been so unsuccessful, see James F. Simom, _The Center Holds: The Power Struggle Inside the Rehnquist Court_ (1995). For a critique of that book, and the Court itself, see Roger Pilon, “A Court Without a Compass,” 40 _New York Law School Law Review_ 999 (1996).



[12]Coalition for Economic Equity v. Wilson, 946 F. Supp. 1480 (N.D. Cal. 1996).



[13]United States v. Bayless, 913 F. Supp. 232 (S.D.N.Y.), rev’d on rehearing, 921 F. Supp. 211 (S.D.N.Y. 1996).



[14]United States v. Virginia, 116 S. Ct. 2264 (1996).



[15]Coalition for Economic Equity v. Wilson, 1997 U.S. App. LEXIS 6512 (9th Cir.).



[16]Bruce Fein, “Judge Not,” _New York Times_ , May 8, 1997, at A39. Cf. Debbie Kaminer, “Thou Shalt Not Display the Ten Commandments in Court,” _Legal Times_ , May 5, 1997, at 27; Terrence P. Jeffrey, “Governor James at the Courthouse Door,” _Human Events_ , May 9, 1997, at 6.



[17]“To the extent that these [New Deal policies] developed, they were tortured interpretations of a document [ _i.e._ , the Constitution] intended to prevent them.” Rexford G. Tugwell, “A Center Report: Rewriting the Constitution,” _Center Magazine_ , Mar. 1968, at 18, 20.



[18]Robert H. Bork, _The Tempting of America_ 139 (1990)(emphasis added).



[19]I have discussed these issues more fully in Roger Pilon,“Freedom, Responsibility, and the Constitution: On Recovering Our Founding Principles,” 68 _Notre Dame Law Review_ 507 (1993).



[20]Thus, the Court has long been criticized by conservatives for its 1971 decision in Griggs v. Duke Power Co., 401 U.S. 424, which gave rise to the “effects test” in antidiscrimination law and to a host of affirmative action programs. But in interpreting the language of section 703 (h) of the Civil Rights Act of 1964, which authorizes “any professionally developed ability test” that is not “designed, intended, _or used_ to discriminate because of race” (at 433, emphasis by the Court), the Court simply drew upon the ambiguity of “used.” Congress could later have addressed that ambiguity, of course, but it did not. In cases like this, then, responsibility rests ultimately with Congress.
"
"
Share this...FacebookTwitterUpdate 7/30/2010: WUWT debunks this scare: here!
Reading the German online daily news this morning, today’s scare-de-jour is the “shocking” reduction of phytoplankton now underway, all due of course to manmade climate change. The “news” is based on a report just published in Nature by scientists Daniel Boyce and Marlon Lewis of the Dalhousie University (Halifax) and Boris Worm of the German Potsdam Institute for Climate Impact Research (think Schellnhuber and Rahmstorf).
When reading about such stories,  it’s a very good idea to first read the following:
Science Turns Authoritarian
Here’s a sampling of today’s headlines in Germany:
Die Welt PLANKTON REDUCTION PUTS FOOD CHAIN AT RISK
Der Spiegel: FOOD CRISIS IN WORLD’S OCEANS
Focus: GREEN FOOD SUPPLY IN OCEANS SHRINKING
Süddeuschte Zeitung: DISAPPEARING IN THE OCEANS
The cause of the phytoplankton decrease is “warming of the oceans”, Nature reports. Satellite measurements since the end of the 1970s have shown fluctuation in oceanic plankton levels, but have not delivered a clear picture.
That’s why the researchers went back and looked at data of ocean chlorophyll content. The team analysed almost 450,000 measurements from the period 1899 to 2008. The result, according to Die Welt:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In eight of ten oceanic regions, phytoplankton has decreased during the 20th century. Content dropped with increasing sea surface temperature, especially in the tropics and subtopics regions. It is suspected that as a consequence of warming, a more pronounced layering of water occurred.
Süddeutsche Zeitung writes that phytoplankton concentrations in the oceans have declined by two thirds since 1899.
What do I think?
Schellnhuber: Visionary of The Great Transformation
The study involves the Potsdam Institute For Climate Impact Research, directed by Hans Joachim Schellnhuber, visionary of The Great Transformation , and Stefan Rahmstorf, who predicts sea levels will rise 1.7 meters in the next 90 years, but refuses to even bet on a 60 cm rise. This is an institute that is well well-known for activist science and fear-mongering, with the clear agenda of reorganizing how people live. A dangerous social engineering experiment.
I’m sure the findings of this study will turn out to be more plankton-crap.
UPDATE: Beware of science authority. Above I mentioned the Science Turns Authoritarian story, which looked at how often certain authoritarian phrases are used in the media. Click on the following graphic:

Be wary, be very wary, of media claims on climate science.
Share this...FacebookTwitter "
"
From the BBC, a video report so absurd, you wonder if it is an April fools joke. The premise? Noise from excessive ice calving  and cracking due to “climate change” would affect the bear’s hearing. I wonder what agency was gullible enough to provide a grant for this load of rubbish? Like polar bears have never heard ice floes cracking and calving before? Give me a break. Plus, the polar bear they are using for a test subject isn’t in it’s natural environment, it’s at a zoo and who’s to say this bear establishes a credible baseline hearing test? This is just unbelievable stupidity in the guise of bad science. What next? Hearing aids for polar bears? A hat tip to Tony B in the UK for alerting me to this story. – Anthony

How to test a bear’s hearing

Click preview image above for link to video story
Scientists in California are testing the hearing of polar bears to try to find out whether the noises associated with melting Arctic ice could affect their ability to survive.
The BBC’s Peter Bowes goes to SeaWorld in San Diego to meet Charly, a 12-year-old polar bear taking part in the experiment – and his trainer Mike Price.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c1eea0f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe Moving Finger writes; and, having writ,
Moves on: nor all thy Piety nor Wit
Shall lure it back to cancel half a Line,
Nor all thy Tears wash out a Word of it.
– Omar Khayyám
Scratch off the Potsdam Institute For Climate Impact Research from the alarmist list. No kidding!
The European Institute For Climate and Energy has a new piece written by Raimund Leistenschneider that takes a look at two interesting papers dug up from 2003. I wonder if Rahmstorf and Schellnhuber are going to feign amnesia on this. Big hat tip to NTZ reader Ike!
Rahmstorf 2003 paper shows pronounced cooling



The paper by Prof Stefan Rahmstorf confirms that today’s temperatures are actually quite cool compared to temperatures earlier in the Holocene.
In a paper he authored: “Timing of abrupt climate change: A precise clock“, Geophys. Res. Lett.. 30, Nr. 10, 2003, S. 1510, doi:10.1029/2003 GL017115, Ramhstorf examined the Dansgaard-Oeschger (DO events).
These events are rapid climate changes occurring 23 times during the last ice age between 110,000 and 23.000 BP and were reconstructed from the GISP-2-ice cores from Greenland. The following chart is a plot in Rahmstorf 2003 paper showing the temperature over the last 50,000 years.
 The next graphic shows the temperature for the last 50,000 years and the last 9,000 up close below, also derived from the GISP-2-ice cores.

On Rahmstorf’s paper, EIKE writes:
Easy to recognize, at least using the studies done by Rahmstorf, we are living in a comparably cold time today. During the MWP 1000 years ago, when the vikings were farming Greenland, it was 1°C warmer than today. During the Roman Optimum 2000 years ago, when Hannibal crossed the Alps with his elephants in the wintertime, it was even 2°C warmer than today. And during the Holocene climate optimum 3500 years ago it was about 3°C warmer than today. Since about  3200 years ago, there has been a cooling of about 2°C.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Multiple studies confirm that the warming was not a regional phenomena.
Schellnhuber could not discern any warming back in 2003
Meanwhile Prof. Hans Joachim Schellnhuber, Rahmstorf’s boss at the Potsdam Institute, was unable to discern any warming when examining a multitude of worldwide temperature records back in 2002 and 2003.
In a paper published in 2003, using their own studies, the authors concluded there had been no global warming over the last decades. (J.F. Eichner, E. Koscielny-Bunde, A. Bunde, S. Havlin, and H.-J. Schellnhuber: Power-law persistence and trends in the atmosphere, a detailed study of long temperature records, Phys. Rev. E 68 2003),
The temperature records of 95 stations distributed over the globe were studied. In the paper’s summary discussion, Schellnhuber and his colleagues wrote:
In the vast majority of stations we did not see indications for a global warming of the atmosphere.
and
Most of the continental stations where we observed significant trends are large cities where probably the fast urban growth in the last century gave rise to temperature increases.
And la pièce de resistance!
The fact that we found it difficult to discern warming trends at many stations that are not located in rapidly developing urban areas may indicate that the actual increase in global temperature caused by anthropogenic perturbation is less pronounced than estimated in the last IPCC Intergovernmental Panel for Climate Change.
Last I checked, global temperatures have gone nowhere since 2003. So where’s the warming?
—————————————————————————————————
Copyright reminder: It is not allowed to reproduce this post without first obtaining permission from No Tricks Zone. You may cut and paste max. 25% of the content, and then followed by a link to this site. Thanks!
Share this...FacebookTwitter "
"

LONDON (Reuters) – Next year is set to be one of the top-five warmest on  record, British climate scientists said on Tuesday.
The average global temperature for 2009 is expected to be more than 0.4  degrees celsius above the long-term average, despite the continued cooling of  huge areas of the Pacific Ocean, a phenomenon known as La Nina.
That would make it the warmest year since 2005, according to researchers at  the Met Office, who say there is also a growing probability of record  temperatures after next year.
Currently the warmest year on record is 1998, which saw average temperatures  of 14.52 degrees celsius – well above the 1961-1990 long-term average of 14  degrees celsius.
Warm weather that year was strongly influenced by El Nino, an abnormal  warming of surface ocean waters in the eastern tropical Pacific.
Theories abound as to what triggers the mechanisms that cause an El Nino or  La Nina event but scientists agree that they are playing an increasingly  important role in global weather patterns.
The strength of the prevailing trade winds that blow from east to west across  the equatorial Pacific is thought to be an important factor.
“Further warming to record levels is likely once a moderate El Nino  develops,” said Professor Chris Folland at the Met Office Hadley Center.  “Phenomena such as El Nino and La Nina have a significant influence on global  surface temperature.”
Professor Phil Jones, director of the climate research unit at the University  of East Anglia, said global warming had not gone away despite the fact that  2009, like the year just gone, would not break records.
“What matters is the underlying rate of warming,” he said.
He noted the average temperature over 2001-2007 was 14.44 degrees celsius,  0.21 degrees celsius warmer than corresponding values for 1991-2000.
(Reporting by Christina Fincher; Editing by Christian Wiessner)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99d79ecd',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Guest Post by Steven Goddard
January, 1790 was a remarkable year in the northeastern US for several reasons.  It was less than one year into George Washington’s first term, and it was one of the warmest winter months on record.  Fortunately for science, a diligent Philadelphia resident named Charles Pierce kept a detailed record of the monthly weather from 1790 through 1847, and his record is archived by Google Books.  Below is his monthly report from that book.
JANUARY 1790 The average or medium temperature of this month was 44 degrees This is the mildest month of January on record. Fogs prevailed very much in the morning but a hot sun soon dispersed them and the mercury often ran up to 70 in the shade at mid day. Boys were often seen swimming in the Delaware and Schuylkill rivers. There were frequent showers as in April some of which were accompanied by thunder and lightning The uncommon mildness of the weather continued until the 7th of February.
Compare that to January, 2009 with an average temperature of 27F, 17 degrees cooler than 1790.  One month of course is not indicative of the climate, so let us look at the 30 year period from 1790-1819 and compare that to the last 10 “hot” years.
From Charles Pierce’s records, the average January temperature in Philadelphia from 1790-1819 was 31.2F.  According to USHCN records from 2000-2006 (the last year available from USHCN) and Weather Underground records from 2007-2009, the average January temperature in Philadelphia for the last ten years has been 29.8 degrees, or 1.4 degrees cooler than the period 1790-1819.  January, 2009 has been colder than any January during the presidencies of Washington, Adams, Jefferson, or Monroe.  January 2003 and 2004 were both considerably colder than any January during the terms of the first five presidents of the US.  Data can be seen here.
According to several of the most widely quoted climate scientists in the world, winters were much colder 200 years ago than now – yet the boys swimming in the Delaware in January, 1790 apparently were unaware.
Another interesting fact which can be derived from Charles Pierce’s data, is that January temperatures cooled dramatically during the period 1790-1819 – as can be seen in the graph below.  The cooling rate was 13F/century.  What could have caused this cooling?  We are told by some experts that variations in solar activity can only affect the earth’s temperature by a few tenths of a degree.  CO2 levels had been rising since the start of the industrial age.  The downward trend is fairly linear and does not show any sharp downward spikes, so it is unlikely to be due to volcanic activity.  What other “natural variability” could have caused such a dramatic drop in temperature?

Looking at the sunspot records for that period, something that clearly stands out is that solar cycle 4 was very long, and was followed by a deep minimum lasting several decades.  Perhaps a coincidence, but if not – Philadelphia may well be in for some more very cold weather in coming winters.

Source for graph:
http://www.springerlink.com/content/k37032647541h753/fulltext.pdf


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99659303',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe winter has even overwhelmed winter-hardened Sweden. So much so that the military has been called out to assist.
Temperatures as low as -36 degrees Celsius have been recorded in Sweden as snowfalls and storm winds play havoc with transport services.

Transport agency calls for caution after snow (23 Nov 10)
Sweden braces for new winter storm (22 Nov 10)
Rail operators prepared for harsh winter: report (16 Nov 10)

Sweden’s main meteorological agency, SMHI, noted that the winter continued its march south across the country as strong winds from the Baltic Sea brought heavy snowfalls in eastern areas of Svealand and Götaland.
The snow is expected to remain on the ground in many parts of southern Sweden as temperatures are set to remain well below zero.
“There has been a lot of snow overnight,” Lisa Frost at SMHI said.
Kalmar county was obliged to call for military help on Wednesday to aid in the battle against widespread flooding which had caused damage to property in the area.
Read more: http://www.thelocal.se/30396/20101124/ (The Local).
The Independent got half of it right anyway. Maybe not “rare”, but at least “exciting”.
Share this...FacebookTwitter "
"Last month was the hottest January on record over the world’s land and ocean surfaces, with average temperatures exceeding anything in the 141 years of data held by the National Oceanic and Atmospheric Administration.  The record temperatures in January follow an exceptionally warm 2019, which has been ranked as the second hottest year for the planet’s surface since reliable measurements started. The past five years and the past decade are the hottest in 150 years of record-keeping, an indication of the gathering pace of the climate crisis. According to Noaa, the average global land and ocean surface temperature last month was 2.05F (or 1.14C) above the 20th-century average. This measurement marginally surpassed the previous January record, set in 2016. A pulse of unusual warmth was felt across much of Russia, Scandinavia and eastern Canada, where temperatures were an incredible 9F (5C) above average, or higher. The Swedish town of Örebro reached 10.3C, its hottest January temperature since 1858, while Boston experienced its hottest ever January day, at 23C (74F). Meanwhile, the Antarctic has begun February with several temperature spikes. The southern polar continent broke 20C (68F) for the first time in its history on 9 February, following another previous high of 18.3C just three days previously. Scientists called the readings “incredible and abnormal”. Noaa said the four warmest Januaries on record have occurred since 2016, while the 10 warmest Januaries have taken place since 2002. The world’s governments agreed in 2015 to keep the global temperature increase to well below 2C, compared with the pre-industrial era, in order to stave off disastrous flooding, food insecurity, heatwaves and mass displacement of people. However, planet-warming emissions from human activity are not showing any sign of decline, let alone the deep cuts needed to meet the 2C goal and address the climate crisis. According to scientists, the world must halve its emissions by 2030 to stand any chance of avoiding disastrous climate breakdown. • This article was amended on 17 February 2020. A previous version said the average global land and ocean surface temperature in January was 2.5F above the 20th-century average. This has now been corrected to 2.05F."
"
What Lunar Orbiter 1 saw as it looked back at Earth on August 23, 1966. Climate studies of Earth will benefit by a look back in time thanks to decades old view from the Moon. Credit: LOIRP/NASA
From Space.com: Old Moon Images Get Modern Makeover
WOODLANDS, Texas — Think of it as a space age twist to that adage: Something old, something new…something borrowed, something blue.
Back in 1966 and 1967, NASA hurled a series of Lunar Orbiter spacecraft to the moon. Each of the five orbiters were dispatched to map the landscape in high-resolution and assist in charting where best to set down Apollo moonwalkers and open up the lunar surface to expanded human operations.
Imagery gleaned from the Lunar Orbiters over 40 years ago is now getting a 21st century makeover thanks to the Lunar Orbiter Image Recovery Project (LOIRP). 
By gathering the vintage hardware to playback the imagery, and then upgrading it to digital standards, researchers have yielded a strikingly fresh look at the old moon. Furthermore, LOIRP’s efforts may also lead to retrieving and beefing up video from the first human landing on the moon by Apollo 11 astronauts in July 1969.
Digital domain
Dennis Wingo, LOIRP’s team leader, detailed the group’s work in progress during last week’s 40th Lunar and Planetary Science Conference.
Teamed with SpaceRef.com, LOIRP’s saga is one of acquiring the last surviving Ampex FR-900 machinery that can play analog image data from the Lunar Orbiter spacecraft. Wingo noted that the work is backed by NASA’s Exploration Systems Mission Directorate, the space agency’s Innovative Partnership Program, along with private organizations, making it possible to overhaul old equipment, digitally upgrade and clean-up the imagery via software. 
LOIRP is located at NASA’s Ames Research Center at Moffett Field, Calif. There, project members are taking the analog data, converting it into digital form and reconstructing the images.
By moving them into the digital domain, Wingo said, the photos now offer a higher dynamic range and resolution than the original pictures, he added.
“We’re going to be releasing these to the whole world,” Wingo said. 
Use of the refreshed images, contrasted to what NASA’s upcoming Lunar Reconnaissance Orbiter (LRO) mission is slated to produce, has an immediate scientific benefit. That is, what is the frequency of impacts on the Moon’s already substantially crater-pocked surface?
“We’ll be able to get crater counts,” Wingo told SPACE.com. “LRO imagery of the same terrain imaged decades ago will provide a crater count over the last 40 years.” 
Frozen in time
There’s also a more down to Earth output thanks to LOIRP scientists.
They have used a Lunar Orbiter 1 image of the Earth for climate studies, basically a snapshot frozen in time that shows the edge of the Antarctic ice pack on August 23, 1966. 
The team is working with the National Snow and Ice Data Center in Boulder, Colorado to correlate their images of the Earth with old NASA Nimbus 1 and Nimbus 2 spacecraft imagery that flew at about the same time — in the mid-1960s — as the Lunar Orbiter 1. Nimbus satellites were meteorological research and development spacecraft.
Wingo said that the original Nimbus images may have been recorded on an Ampex FR-900 – so by processing the original Nimbus tapes there is a very good chance that they can provide NASA with polar ice pack data from ten years earlier.
Lessons learned
One treasure hunt outing by LOIRP may lead to finding what some term as “lost” Apollo 11 slow scan tapes, Wingo said. 
“We don’t think they are lost. People have been looking for the wrong tapes,” he said, explaining that they were recorded on Ampex FR-900 equipment — not on another type of recorder as previously thought.
Wingo said those Apollo tapes are stored at the Federal Records Center, labeled and ready for a look see.
“We think for the 40th anniversary of Apollo we may be able to get the original slow scan tapes,” Wingo said. If so, the hope is to recover them and give the public a higher-quality, never-before-seen view of human exploration of the Moon.
There is a lesson learned output from LOIRP. 
In the beginning, very few people thought this could be done…but now they have seen the results,” Wingo said.
It is not enough to have 100 year recording medium, Wingo explains. Without the retention of the specific era equipment that images are archived on, it will be impossible for future generations to recover older NASA or other satellite data, he advised.
This is a general issue, not specific to the Lunar Orbiter program. The retention of critical hardware should be a requirement for flight efforts. The original historic Apollo 11 slow scan images have been lost due to inattention to this critical detail, Wingo concluded.
(h/t to Gary Boden)
UPDATE: Dennis Wingo responded in comments, and offers this LA Times story on the real trials and tribulations of this project. 
http://www.latimes.com/news/nationworld/nation/la-na-lunar22-2009mar22,0,931431.story
We owe Mr. Wingo and his team, and especially Nancy Evans, a debt of gratitude for preserving space history against the odds. – Anthony
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96e2e2c7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterAs an American citizen living in Germany, today is just another regular work day here, but of course I still celebrate Thanksgiving, and do so by having a lavish turkey dinner on Friday evening with friends and family. For the non-Americans who visit this site,  here’s a short version of how Thanksgiving started and became a tradition.
The Pilgrims escape oppression in Europe
Giving thanks and celebrating festivals for successful harvests had existed for centuries, way before the first American Thanksgiving. Giving thanks in America started when the first Pilgrims came to Massachusetts (Plymouth Rock) from England on the Mayflower in 1620. The Pilgrims came to the New World to escape persecution and oppression, particularly from the Church of England, kind of like how climate skeptics are oppressed by the Church of Climatology today.
The Pilgrims land at Plymouth Rock and many starve to death because of climate
The Mayflower with its 102 passengers had been originally bound for Jamestown, Virginia, but Atlantic storms blew the ship north to Massachusetts (Storms back then had natural causes, and were not man made ;). The first winter there was especially harsh, and because they had arrived too late they could not grow crops and they didn’t have fresh food. Half the Pilgrims on the Mayflower died of mal-nutrition and starvation during the first winter alone. But despite the extreme hardship, these newcomers had some luck, as it was the tradition of the local Wampanoag Indians, led by Chief Massasoit, to share food with any visitors.
The Indians teach the Pilgrims adaptation, and not useless mitigation
The following spring, in 1621, the Indians taught them how to grow corn (maize) and introduced cranberries, which were new foods for the new settlers. They also showed them how to grow other crops like beans, pumpkins and squash in the strange soil. The Indians taught the Pilgrims how to hunt and fish as well. Back then, the Indians taught the Pilgrims that it was useless to mitigate climate. Now just imagine if the Pilgrims had resorted to rain dancing and forbidden tree cutting. We can be thankful they were smarter then our political leaders of today.
The fruits of adaptation
After the first harvest had been completed by the new colonists in the autumn of 1621, the colonists had an abundance of food (the wonders of adaptation!). Governor William Bradford therefore proclaimed a day of thanksgiving and prayer. And to thank the Indians for teaching them how to survive in the New World, the Pilgrims of Plymouth Rock invited their Indian friends to their first Thanksgiving. It was a three day celebration to give thanks to God and the leaders of the Wampanoag Indians.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Thanksgiving spreads to other states and declared a national holiday 
After the first Plymouth Thanksgiving, the custom spread to the other colonies. But each region chose its own date. In 1789 George Washington, the first president of the United States, declared November 26 as a day of thanksgiving, but it still was not an official holiday. Thanksgiving Day continued to be celebrated in the United States on different days in different states – until Mrs. Sarah Josepha Hale, editor of Godey’s Lady’s Book, embarked on a campaign. For more than 30 years she wrote letters to the governors and presidents asking them to make Thanksgiving Day a national holiday.
Thanksgiving becomes a national holiday
In 1863, President Lincoln called on Americans to unite “with one heart and one voice” and to celebrate Thanksgiving Day on the last Thursday of November. In 1939 President Franklin D. Roosevelt moved Thanksgiving Day a week earlier to make the Christmas shopping season longer. However, because some states used the new date and others the old one, it was changed again just 2 years later. Now Thanksgiving Day is celebrated on the fourth Thursday in November.
Freedom of Want – by Norman Rockwell (1943)
Why turkey?
The turkey tradition was pushed by Benjamin Franklin, who even wanted to make it the United States national symbol. In the end the bald eagle was selected instead of the wild turkey as the official national symbol. I think most Americans will agree it was the right choice. Finally, the turkey was made famous by Norman Rockwell’s 1943 image of the family Thanksgiving, Freedom of Want, that appeared on the cover of the Saturday Evening Post. The turkey has been the Thanksgiving Day favorite ever since.
Dinner and family
The American tradition of Thanksgiving revolves around an extravagant meal, with turkey at the center.  Thanksgiving dinner also includes corn, cranberry, potatoes, gravy and a variety of pies for dessert – like pumpkin pie or apple pie. It’s tradition to say a special prayer of thanks before the meal. In many homes, family members each mention something they are very thankful for. Thanksgiving is a time for families to come together.
I have very fond memories of Thanksgivings in New England as a boy, especially of coming in out of the cold into a warm house heated by a wood-burning Franklin stove and savouring the aroma of an 18-pound turkey that’s been baking in the oven for five hours. Thanksgiving memories last a lifetime.
Share this...FacebookTwitter "
"

After five days of contentious discussions in Bangkok, governments from nearly 200 countries last week agreed to an agenda for further talks to forge a new United Nations global warming agreement. One sticking point has been developing nations’ insistence that industrialized countries should take the first steps in reducing emissions and should help finance reductions in developing countries. But this represents a serious misreading of the underlying economic situation.



The theory behind the “developed countries should pay” model was articulated by Yvo de Boer, executive secretary of the U.N. Framework Convention on Climate Change: “The problem of climate change … is a result of rich countries’ emissions, not the result of poor countries’ emissions. The historic responsibility of this problem lies with industrial nations.”



Yet although greenhouse gas emissions can be blamed on nations based on the location of emission activities, these emissions are the effluvia of civilization and all its activities. In today’s interconnected world, economic activity in one country helps provide livelihoods and incomes for many inhabitants elsewhere, and vice versa. A substantial portion of economic growth in developing countries is attributable to trade, remittances, tourism and direct investment from industrialized countries.



For example, remittances, mainly from the United States, Britain and the oil‐​rich Gulf states, account for 13% of Bangladesh’s GDP. Absent economic activities that directly or indirectly fuel such contributions to developing countries, U.S. emissions might be lower, but so would jobs and incomes in developing countries like Bangladesh.



These linkages have had hugely positive effects. Greenhouse‐​gas‐​fueled economic activity has enabled today’s rich societies to invest in agricultural, medical and public health research that has raised crop yields and lowered hunger in developing countries; to devise effective medical interventions to address old diseases like tuberculosis, malaria, diarrhea and smallpox and new diseases like AIDS; and to provide aid in times of famine or other natural disasters.



Absent such economic activity, human capital would have been lower worldwide. Consider, for instance, the millions of non‐​Americans who have been cycled through universities in the U.S. who then returned to advance their native countries’ economic and technological development.



Some might argue that one should not take indirect effects of greenhouse‐​gas‐​producing activities into consideration: Only direct effects should be considered. But the notion of assigning responsibility or demanding compensation for climate change is itself based on indirect and inadvertent outcomes. Industrialized countries did not emit greenhouse gas emissions just for fun. There are clearly benefits.



So if the U.S. contribution to global warming, for instance, could be estimated, the next step would be to estimate the net harm caused to, say, Bangladesh. This requires estimating both direct and indirect impacts not just of climate change but all greenhouse gas‐​producing activities on Bangladesh.



This raises some serious questions, including: Had there been no greenhouse gas‐​producing activities in the U.S., what would have been Bangladesh’s GDP and level of human well‐​being? How would that affect life expectancy, which is currently 62 years but was only 35 years in 1945? Would Bangladesh’s hunger and malnutrition rates rise? How many Bangladeshis were saved in the 1960s and 1970s because of food aid from industrialized countries? How much of its increase in agricultural productivity is due to higher CO2 levels, or indirectly due to efforts enabled because the U.S. was wealthy enough to support them? If future agricultural productivity declines due to climate change, how do you subtract past and present benefits from future harms?



Clearly, it’s premature to assign “responsibility” to industrialized countries for net damages to developing countries, since we don’t know whether those damages have, in fact, been incurred. Even if one could assign responsibility for climate change, it does not follow that it would be “fairer” if industrialized nations were to expend resources now on ambitious mitigation measures, based partly on the premise that it would reduce future climate change risks for developing nations. The same resources would, in the short‐ to medium term, provide greater and faster benefits to precisely those nations by reducing existing — and generally larger — climate‐​sensitive risks and vulnerabilities such as hunger, malaria and the threat of cyclones and other extreme events.



The U.N. climatocrats owe it to the people of the developing world to consider these trade‐​offs before they charge ahead with their ambitious new agenda.
"
"

Nearly three‐​fourths (71%) of Americans believe that political correctness has done more to silence important discussions our society needs to have. A little more than a quarter (28%) instead believe that political correctness has done more to help people avoid offending others.



The consequences are personal—58% of Americans believe the political climate today prevents them from saying things they believe. Democrats are unique, however, in that a slim majority (53%) do not feel the need to self‐​censor. Conversely, strong majorities of Republicans (73%) and independents (58%) say they keep some political beliefs to themselves.





Most Americans (59%) think people should be allowed to express unpopular opinions in public, even those deeply offensive to other people. Forty percent (40%) think government should prevent hate speech in public. Nonetheless, an overwhelming majority (79%) agree that it is “morally unacceptable” to engage in hate speech against racial or religious groups. Thus, the public appears to distinguish between _allowing_ offensive speech and _endorsing_ it.





Despite this, the survey also found Americans willing to censor, regulate, or punish a wide variety of speech and expression they personally find offensive:



An overwhelming majority (82%) of Americans agree that it would be difficult to ban hate speech because people can’t agree what speech is hateful and offensive. Indeed, when presented with specific statements and ideas, Americans can’t agree on what speech is hateful, offensive, or simply a political opinion:





African Americans and Hispanics are more likely than white Americans to believe:



However, black, Hispanic, and white Americans agree that free speech ensures the truth will ultimately prevail (68%, 70%, 66%). Majorities also agree that it would be difficult to ban hate speech since people can’t agree on what hate speech is (59%, 77%, 87%).





Two‐​thirds (66%) of Americans say colleges and universities aren’t doing enough to teach young Americans today about the value of free speech. When asked which is more important, 65% say colleges should “expose students to all types of viewpoints, even if they are offensive or biased against certain groups.” About a third (34%) say colleges should “prohibit offensive speech that is biased against certain groups.”



But Americans are conflicted. Despite their desire for viewpoint diversity, a slim majority (53%) also agree that “colleges have an obligation to protect students from offensive speech and ideas that could create a difficult learning environment.” This share rises to 66% among Democrats, but 57% of Republicans disagree.





More than three‐​fourths (76%) of Americans say that recent campus protests and cancellations of controversial speakers are part of a “broader pattern” of how college students deal with offensive ideas. About a quarter (22%) think these protests and shutdowns are simply isolated incidents.



However, when asked about specific speakers, about half of Americans with college experience think a wide variety should not be allowed to speak at their college:



Excluding a speaker who would disrespect police, Democrats are about 15 to 30 points more likely than Republicans to say each of these speakers should not be allowed to speak.





Two‐​thirds (65%) say colleges need to discipline students who disrupt invited speakers and prevent them from speaking. However, the public is divided on how: 46% want to give students a warning, 31% want the incident noted on the student’s academic record, 22% want students to pay a fine, 20% want students suspended, 19% favor arresting students, and 13% want students fully expelled.  
Democrats take a softer while Republicans take a harder approach to handling disruptive college protestors. Nearly two‐​thirds (64%) of Democrats say colleges should listen to and address the students’ concerns, compared to 36% of Republicans. Conversely, Republicans are two to six times as likely as Democrats to support some sort of punishment for the students, such as suspending or expelling them (47% vs. 15%), noting the incident on the students’ records (41% vs. 22%), or having police arrest the students (32% vs. 7%).





Most people support the heckler’s veto. A majority (58%) say colleges should cancel controversial speakers if administrators believe the students will stage a violent protest otherwise. Democrats and Republicans again disagree: Democrats say universities should cancel the speaker (74%) and Republicans say they should not cancel the speaker (54%) if the students threaten violence. 



The survey finds that many microaggressions that colleges and universities advise faculty and students to avoid aren’t considered offensive by most African Americans and Latinos. The percentage who say these microaggressions are _not offensive_ are as follows:



The one microaggression that African Americans (68%) agree is offensive is telling a racial minority, “you are a credit to your race.” Latinos are evenly divided.





A majority (66%) of Americans have heard of safe spaces, but half or less are familiar with other social justice terms and phrases popular on college campuses today, including: cultural appropriation (50%), trigger warnings (49%), “check your privilege” (48%), microaggressions (43%), and “mansplaining” (41%).



In contrast, strong majorities of current college students and graduate students are familiar with all of these words and phrases: safe spaces (86%), cultural appropriation (76%), trigger warnings (75%), “check your privilege” (77%), microaggressions (66%), and “mansplaining” (69%).



Nearly two‐​thirds (65%) of the public say colleges shouldn’t advise students about offensive Halloween costumes and should instead let students work it out on their own. A third (33%) think it is the responsibility of the university to advise students not to wear costumes that stereotype racial or ethnic groups at off‐​campus parties.



A majority of African Americans (56%) believe universities should intervene and advise against offensive costumes. Conversely, a strong majority (71%) of white Americans and a majority of Latinos (56%) believe that college students should discuss offensive Halloween costumes among themselves without administrator involvement.





Only 20% of current college and graduate students believe their college or university faculty has a balanced mix of political views. A plurality (39%) say most college and university professors are liberal, 27% believe most are politically moderate, and 12% believe most are conservative.



Democratic and Republican students see their college campuses differently. A majority (59%) of Republican college students believe that most faculty members are liberal. In contrast, only 35% of Democratic college students agree most professors are liberal.





A slim majority (51%) of Americans oppose, while nearly as many (48%) support, the idea of a confidential reporting system at colleges and universities in which students could report people who make offensive comments about a person’s race, gender, sexual orientation, age, or disability status.



This “bias reporting system,” as it’s often referred to, is highly popular among current students. More than two‐​thirds (68%) of college students and graduate students support it, while less than a third oppose (30%).



Americans tend to oppose firing people for their beliefs or expression. However, Democrats and Republicans differ on what beliefs or expressive acts they believe are fireable offenses:





A majority of Republicans (63%) agree with President Trump that journalists today are an “enemy of the American people.” Conversely, most Americans (64%), as well as 89% of Democrats and 61% of independents, do not view journalists as the enemy.





Despite this, Republicans (63%) agree with most Americans (70%), including Democrats (76%) and independents (71%), that government should not have the power to stop news stories even if officials say they are biased or inaccurate.



Most Americans believe many major news outlets have a liberal bias, including the _New York Times_ (52%), CNN (50%), and MSNBC (59%).1 Fox News, on the other hand, is perceived to have a conservative bias (56%). Americans are divided about whether CBS is balanced (42%) or has a liberal bias (40%). Local news stations are a rare trusted source. A majority (54%) say their local TV station provides balanced news coverage without bias.





Majorities of Democrats believe most major news organizations are balanced in their reporting, including CBS (72%), CNN (55%), the _New York Times_ (55%), as well as their local news station (67%). A plurality (44%) also believe the _Wall Street Journal_ is balanced. The two exceptions are that a plurality (47%) believe MSNBC has a liberal tilt and a strong majority (71%) say Fox has a conservative bias.



Republicans, on the other hand, see things differently. Overwhelming majorities believe liberal bias colors reporting at the _New York Times_ (80%), CNN (81%), CBS (73%), and MSNBC (80%). A plurality also feel the _Wall Street Journal_ (48%) has a liberal bias. One exception is that a plurality (44%) believe Fox News has a conservative bias, while 41% believe it provides unbiased reporting.





The public distinguishes between a business serving people versus weddings:



Few support punishing businesses who refuse service to same‐​sex weddings. Two‐​thirds (66%) say nothing should happen to a bakery who refuses to bake a cake for a same‐​sex wedding. A fifth (20%) would boycott the bakery. Another 22% think government should sanction the bakery in some way, such as by fining the bakery (12%), requiring an apology (10%), issuing a warning (8%), taking away their business license (6%), or sending the baker to jail (1%).





Nearly two‐​thirds (61%) of Hillary Clinton’s voters agree that it’s “hard” to be friends with Donald Trump’s voters. However, only 34% of Trump’s voters feel the same way about Clinton’s. Instead, nearly two‐​thirds (64%) of Trump voters don’t think it’s difficult to be friends with Clinton voters.





Most Americans (59%) say people should be allowed to express unpopular opinions in public, even those that are deeply offensive to other people. A substantial minority (40%), however, say government should prevent people from engaging in hate speech against certain groups in public.





Racial minorities support government banning public hate speech, including 56% of African Americans and 58% of Latinos. Conversely, a majority of white Americans (66%) oppose banning hate speech. 





While solid majorities of Republicans (72%) and independents (60%) oppose government banning hate speech, Democrats stand out with a slim majority in support (52%). However, African American and Latino Democrats largely drive these numbers. A majority (55%) of white Democrats say government should allow public hate speech, but majorities of black Democrats (59%) and Hispanic Democrats (65%) say it should prevent such speech in public.



Thus, the Democratic Party is divided on matters of free speech. White Democrats are more likely to oppose government regulations on speech while black and Hispanic Democrats are more likely to support it.



Current college and graduate students diverge from Americans who have already graduated from college. About half (49%) of current students say government should ban hate speech while the same proportion (49%) say it should not. In contrast, among college _graduates,_ 64% say hate speech should be legal and a third (36%) say it should not.



Using a political typology to identify ideological groups,2 we find that Libertarians (82%) are the most opposed to hate speech laws, followed by Conservatives (75%) and a slim majority (53%) of Liberals. However, nearly two‐​thirds of Populists (64%) say government should prevent hate speech in public.



Altogether, Hispanic and black Americans, Democrats, women, Populists, and college students are most supportive of the government prohibiting public hate speech. Whites, Republicans, independents, men, and Libertarians are most opposed.



Although most Americans say government should not prevent people from engaging in public hate speech, most think hate speech is morally unacceptable. Nearly 8 in 10 (79%) say that it is “morally unacceptable” to “say things that might be offensive to racial or religious groups.”



This indicates that Americans make a distinction between allowing speech and endorsing that speech. Most think that speech that is offensive or insulting toward minority groups should be legally permitted, but that it is still wrong.



More than 8 in 10 Americans (82%) believe that it’s hard to ban hate speech “because people can’t agree what speech is hateful.” Seventeen percent (17%) disagree.



As a later section will show, Americans are sharply at odds over what speech they would personally define as hateful, offensive, or neither. For instance, a majority of Democrats (52%) believe saying that transgender people have a mental disorder is hate speech. Only 17% of Republicans agree. On the other hand, 42% of Republicans believe it’s hateful to say that the police are racist, while only 19% of Democrats agree.



Majorities across partisan groups, demographic groups, college students, and non‐​college students alike agree that hate speech is hard to define and thus may be hard to regulate.



Most Americans (75%) are aware that making racist statements in public is legal under the First Amendment. However, a substantial minority—24%—think hate speech is currently prohibited by law.



Unsurprisingly, those with less education are more likely to think that hate speech is currently illegal. About a third (32%) of those with high school degrees or less think hate speech is illegal, compared to 19% of college graduates and 13% of those with post‐​graduate degrees.



When asked if Americans might favor banning hate speech against particular groups of people, Americans still oppose such laws. There is, however, relatively more support for banning offensive and insulting speech against African Americans (46%). After that, about 4 in 10 would support banning offensive speech about Jewish Americans (41%), immigrants (40%), armed service members (40%), Hispanics (39%), Muslims (37%), the police (37%), gays, lesbians, and transgender people (36%), and Christians (35%). About a third (32%) would support banning insulting speech about white people.



Interestingly, Democrats favor hate speech protections for some groups more than others. Majorities of Democrats support making it illegal to say offensive or insulting things in public about African Americans (61%) and Jewish Americans (53%). Compared to Republicans, Democrats tend to be more supportive of hate speech laws across the board. Nearly half support hate speech laws for immigrants (49%), gays, lesbians and transgender people (48%), Latinos (46%), and Muslims (45%). About 4 in 10 support such laws for military members (42%), Christians (39%), the police (38%), and a third (33%) support such laws for white Americans.





In contrast, majorities of Republicans tend to more consistently oppose hate speech laws for all the groups included on the survey, with about 3 in 10 in support. However, Republicans are relatively more likely to support banning hate speech against military service members (36%) and the police (36%) but less likely to support such laws for Muslims (25%) and LGBT people (24%).



Racial minority groups are more likely than whites to support hate speech laws for groups across the board, but particularly members of their own racial/​ethnic group. Nevertheless, blacks and Hispanics are more supportive than white Americans of laws banning offensive speech about white Americans as well.



African Americans are most likely to favor a law that bans hate speech against African Americans (62%). Fewer support banning hate speech against Hispanic (53%) and white (41%) Americans.



Latinos are most likely to favor a law that bans hate speech against Latinos (65%). A majority (59%) also favor making offensive speech against African Americans illegal and 47% favor banning hate speech against white Americans.



Whites are comparatively less likely to support banning hate speech against particular racial/​ethnic groups. Nevertheless, whites are most likely to favor a law that bans hate speech against black Americans (39%). A little more than a quarter support banning offensive speech about Latino (28%) and white (26%) Americans.



Hispanic (51%) and black (40%) Americans are also more likely than white Americans (32%) to support making it illegal to say offensive or disrespectful things about the police. This is surprising given that surveys have long shown that African Americans and Latinos view the police more negatively.3 The data reveal that both groups tend to more consistently support laws that restrict offensive public speech about any group, not just some groups.



As one 26‐​year‐​old Hispanic female further explained, “we are all human beings, we must all respect each other equally.” Similarly, a 31‐​year‐​old black male in the survey explained that he supported hate speech laws not only for African Americans “but it should be for everybody because it will stop the hate.”



Women are more likely than men to support hate speech laws for different racial, religious, and other groups—particularly for African Americans.



A majority of women (57%) favor a law that would make it illegal to say offensive or insulting things about African Americans in public while 43% oppose. In contrast, only 36% of men would similarly favor this law while 64% would oppose it.



Majorities of women oppose similar laws for other groups. However, they are about 15 points more likely than men to favor banning hate speech against immigrants (46% vs. 33%), gays, lesbians, and transgender people (45% vs. 27%), the police (45% vs. 28%), Hispanics (45% vs. 30%), Muslims (44% vs. 30%), Jewish people (45% vs. 35%), and Christians (43% vs. 27%).



Besides slurs and biological racism, Americans are strikingly at odds over what speech and ideas constitute hate.4





First, majorities agree that calling a racial minority a racial slur (61%), saying one race is genetically superior to another (57%), or calling gays and lesbians vulgar names (56%) is not just offensive—but is hate speech. Interestingly a majority do not think calling a woman a vulgar name is hateful (43%), but most would say it’s offensive (51%). Less than half believe it’s hateful to say that all white people are racist (40%), transgender people have a mental disorder (35%), America is an evil country (34%), homosexuality is a sin (28%), the police are racist (27%), or illegal immigrants should be deported (24%). Less than a fifth believe it’s hateful to say Islam is taking over Europe (18%) or that women should not fight in military combat roles (15%).



Liberals and conservatives significantly diverge over what speech they define as hateful, offensive, or simply an opinion. (See Appendix B).



Majorities of Americans agree with liberals that slurs and biological racism are hateful. However, majorities do not agree with liberals that it’s hateful to say “transgender people have a mental disorder” (35% of all Americans vs. 59% of liberals) or to call women a vulgar name (43% vs. 54%).



Strikingly, majorities of conservatives don’t think any of these ideas are “hateful” although most consider them “offensive” or hateful. In fact, conservatives are about 40 points less likely than liberals to think that saying transgender people have a mental disorder (17% vs. 59%) or saying racial slurs (43% vs. 81%) are hateful. While strong majorities of conservatives agree these are at least _offensive_ or hateful, they are less likely to equate these phrases and ideas with hate specifically.



Liberals are also more likely than conservatives to view a variety of political opinions and speech as either _offensive_ or hateful.





Liberals are more than 40 points more likely than conservatives to think it is offensive or hateful for a person to say that homosexuality is a sin (90% vs. 47%), women shouldn’t fight in military combat roles (87% vs. 47%), illegal immigrants should be deported (80% vs. 36%), or Islam is taking over Europe (79% vs. 33%). Not even a majority of conservatives find these statements to be offensive or hateful.



Notice that two of these, women fighting in combat roles and deporting illegal immigrants, are _policy positions_ that a substantial number of Americans hold. Yet, to merely express these as political positions would also be viewed as highly offensive to a large share of the population.



Furthermore, President Trump has explicitly advocated for deporting illegal immigrants during his 2016 presidential campaign.5 Thus, a large share of Americans not only disagree with his policy position but also find it highly offensive if not hateful. 



Majorities of conservatives did not find any of the statements included on the survey hateful. However, they were _more likely_ than liberals to find several statements hateful. First, conservatives are about twice as likely as liberals to think it’s hateful to say the police are racist (39% vs. 17%). Second, conservatives are somewhat more likely to believe it’s hateful to say that America is an evil country (39% vs. 29%). Third, conservatives are somewhat more likely than liberals to think it’s hateful to say that all white people are racist (44% vs. 35%).



Most Americans (68%) do not think it’s morally acceptable to use physical violence against Nazis, while 32% say it is morally acceptable.6



However, strong liberals stand out with a slim majority (51%) who say it’s moral to punch Nazis in the face. Only 21% of strong conservatives agree. The survey found liberals were more likely to consider upsetting and controversial ideas “hateful” rather than simply “offensive.” This may help partially explain why staunch liberals are more comfortable than the average American with using violence against Nazis.



Strong liberals’ approval of Nazi‐​punching is not representative of Democrats as a whole. A majority (56%) of Democrats believe it is not morally acceptable to punch a Nazi. Thus, tolerance of violence as a response to offensive speech and ideas is found primarily on the far Left of the Democratic Party.





Approval for punching Nazis also varies with age and race. Millennials (42%) are nearly twice as likely as people over 55 (24%) to say violence is morally justified. African Americans (45%) are also 17 points more likely than whites (28%) and 10 points more likely than Latinos (35%) to say punching Nazis is morally acceptable. Nevertheless, majorities of each of these groups say physical force is not justified, even against a Nazi.



Nearly two‐​thirds (64%) of Americans oppose a law that would make it illegal to deny that the Holocaust happened. About a third (35%) would support banning Holocaust denial. These results put Americans at odds with a number of European countries that have outlawed denying the historicity of the Holocaust.7



Support for banning Holocaust denial varies with ideology. A plurality (50%) of strong liberals support such a law, followed by 43% of liberals, 33% of moderates, 30% of conservatives, and 26% of strong conservatives.



A slim majority (54%) of Americans oppose a law that would ban making sexually explicit statements in public, while 45% would oppose.



Although majorities of Democrats (52%), Republicans (55%), and independents (57%) all oppose such a law, certain demographics would support it.



Women (54%) are more than 20 points more likely than men (36%) to support banning sexually explicit public statements. Hispanics (55%) and African Americans (50%) are also somewhat more likely than white Americans (41%) to support such a ban.



Church attendance also predicts support for banning sexually explicit public statements. A slim majority (52%) of regular churchgoers support such a law, but support declines as church attendance declines. A majority (55%) of those who seldom attend church and nearly two‐​thirds (63%) of those who never attend oppose a ban.



Libertarians (67%) and Liberals (64%) are most opposed to banning sexually explicit language in public. 8 Conservatives also marginally oppose (54%). But Populists stand out, with a majority (57%) who say we should outlaw explicit statements. (See Appendix A).



Americans oppose legal restrictions on hate speech, Holocaust denial, and sexually explicit public statements. However, nearly two‐​thirds (62%) would support a law making it illegal to call for violent protests. A little more than a third (37%) would oppose this law.



Outlawing public calls to violently protest is not controversial. Solid majorities of partisans and demographic groups support prohibiting this type of public speech.



Nearly 6 in 10 liberals (59%) favor a law that would require people to refer to transgender persons by their preferred gender pronouns, not their biological sex. This is in sharp contrast to what Americans overall support. Nearly two‐​thirds (62%) oppose a law requiring people use certain pronouns for transgender people while 37% would support it. Moderates (60%) and conservatives (82%) are highly opposed to such laws, including 59% of conservatives who _strongly_ oppose.





These results are relevant to the cities and states that are moving to fine or jail businesses and landlords who refuse to use transgender people’s preferred pronouns. For instance, California enacted a new law that punishes long‐​term nursing home care staff who refuse to use a resident’s preferred name or pronouns.9 Or in New York City, new regulatory guidance subjects landlords and businesses to fines for refusing to use transgender employees’, customers’, or tenants’ preferred pronouns.10 Americans overall, however, do not support these laws.



While Democrats are more supportive of censorship when it comes to hate speech, Republicans disdain criticizing patriotic symbols like the American flag.



A majority (53%) of Republicans favor stripping a person of their U.S. citizenship if they burn the American flag, while 47% would oppose. These results fit with President Trump’s tweets soon after his presidential election victory in which he called for a “loss of citizenship” to punish flag burning.11





While aligned with Trump, Republicans are out of step with the mainstream: 61% of Americans don’t think we should strip people of their citizenship for flag burning. Thirty‐​nine percent (39%) think revoking a person’s citizenship is a reasonable response to flag burning.



A strong majority of Democrats (71%) and independents (61%) oppose such a proposal. Nevertheless, a non‐​insignificant minority of Democrats (28%) and independents (38%) support stripping citizenship from a flag burner.



Latinos align most with Republicans on this issue: 49% agree flag burners should have their citizenship revoked. Latinos are 22 points more likely than African Americans (27%) and 10 points more likely than white Americans (39%) to support such a policy.



Support for revoking citizenship steadily declines with education. While nearly half (48%) of those with high school degrees or less agree with President Trump, only 29% of college graduates and 20% of post‐​grads agree.



Although the Supreme Court has ruled that flag burning is protected speech under the First Amendment, a majority (58%) of Americans still favor a law banning it while 42% oppose.



Majorities of Republicans (72%) and independents (60%) also favor making it illegal to burn or desecrate the flag. Democrats stand out, with a slim majority (53%) who oppose a flag burning ban.



Hispanic Americans are most in favor (63%) of a ban on flag burning, followed by white Americans (58%). African Americans are divided, with 50% in favor and 49% opposed. Women are also more likely than men to support such a ban (65% vs. 50%).



Libertarians (56%) and Liberals (62%) stand out in opposition to a flag burning ban. 12 In contrast, nearly three‐​fourths of Conservatives (74%) and Populists (69%) support it. (See Appendix A).



In this section, the survey report investigates the public’s assumptions about how free speech operates. We find that Americans believe free speech has both benefits and costs. First, nearly two‐​thirds (67%) think that “freedom of speech ensures the truth will ultimately win out” and 58% say free speech does more to protect minority viewpoints. But also, most believe that speech can turn violent: 53% say hate speech _is_ an act of violence and even more say that hate speech _leads to_ violence against minority groups (70%). Ultimately, a majority (56%) think it’s possible to both ban hate speech and still protect free speech.



There are wide racial and partisan divides over how people think free speech operates. Democrats, African Americans, and Latinos are more likely than Republicans and white Americans to believe that hate speech is violent and allows majority views to crowd out minority viewpoints, that supporting a racist’s free speech right is as bad as being a racist, that people who offend others with their ideas have bad intentions, and that we can simultaneously ban hate speech and protect free speech.





Americans provide a strong endorsement of free speech with 67% who agree that “free speech ensures the truth will ultimately win out.” About a third (32%) do not believe that truth can prevail with the free exchange of ideas.



This concept is non‐​controversial with strong majorities of political partisans and demographic groups who share this belief. However, strong liberals (42%) are more likely than moderates (31%) and strong conservatives (25%) to lack this confidence in free speech.



Most Americans (58%) believe that free speech does more to protect minority viewpoints rather than those of the majority. However, African Americans stand out, with 59% who believe free speech does more to protect majority opinions, rather than views held by a minority of individuals. Nearly two‐​thirds (64%) of white Americans believe free speech primarily protects minority views. Latinos are evenly divided on this question.





Majorities of Democrats (53%), independents (57%), and Republicans (66%) agree that free speech does more to allow for and protect minority views.



However, the Democratic Party is divided. Six in 10 black Democrats believe free speech allows the majority to crowd out minority views, while 6 in 10 white Democrats believe it primarily protects minority views. Latino Democrats are divided with 51% who think free speech primarily protects majority opinions.



College protests of controversial speakers across the country have elevated an idea that deeply offensive speech is like an act of violence.13 A slim majority of Americans appear to endorse these sentiments: 53% say that “hate speech is an act of violence.” Another 46% do not believe that hate speech is violence.



Equating speech with violence is highly controversial and sharply divides Americans by political ideology, race, gender, and age.



While two‐​thirds (66%) of Democrats say hate speech is violence, 58% of Republicans say hate speech is _not_ violence. Independents are split, with 51% who disagree hate speech is tantamount to violence.





African Americans (75%) and Latinos (72%) are nearly 30 points more likely than white Americans (46%) to believe hate speech is violence. Instead, a slim majority (53%) of white Americans believe it is not.



While nearly two‐​thirds (63%) of women believe hate speech is violence, a majority (56%) of men disagree.



Americans under 30 (60%) and seniors (57%) are also more likely than middle‐​aged Americans (35–64) to believe hate speech is violence (49%).



These differences may partially explain why Democrats, students, African Americans, Latinos, and women are more supportive of hate speech laws. Equating hate speech with violence provides a greater justification for restricting it.



One reason why Americans may believe hate speech is violence is that a majority (70%) believe that “hate speech leads to violence against minority groups.” This is a view shared by a majority of partisans and racial/​ethnic groups. Nevertheless Democrats (89%), African Americans (85%), and Hispanic Americans (79%) are more likely to believe this than independents (60%), Republicans (54%), and white Americans (66%).



A variety of campus protestors and social justice activists have argued that society can prohibit hate speech while still protecting Americans’ First Amendment rights to free speech. As Scott Crow, a former Antifa organizer put it, “hate speech is not free speech.“14 Similarly, a widely circulated Wellesley College newspaper staff editorial argued that “shutting down rhetoric that undermines the existence and rights of others” is “not a violation of free speech” because such rhetoric is “hate speech.“15



The survey finds that a majority (56%) of Americans agree with the idea that “society can prohibit hate speech and still protect free speech.” Forty‐​three percent (43%) disagree that society can simultaneously prohibit hate speech and protect free speech.





The idea that society can have both hate speech bans and uphold the First Amendment divides partisans and demographic groups. A majority of Democrats (64%) and independents (54%) think it’s possible. A slim majority (52%) of Republicans think it’s not.



Strong majorities of African Americans (69%), Latinos (71%), and women (64%) believe society can both protect free speech and ban hate speech, but white Americans and men are evenly divided. Current college students and graduate students (62%) are also more likely than college graduates (47%) to believe this can be done.



Nearly two‐​thirds of African Americans (65%) and Latinos (61%) agree that “supporting someone’s right to say racist things is as bad as holding racist views yourself.” About a third (34%) of white Americans agree. This suggests that Americans of color may not believe people are reasoning in good faith when they say we should _allow_ speech even if we strongly _disagree_ with it.





This perspective was on full display at the College of William and Mary when student protestors recently prevented an invited ACLU affiliate from speaking at an event, “Students and the First Amendment.” Protestors explained this was in retaliation for the ACLU’s defense of white nationalists’ free speech rights.16 The Black Lives Matter of William and Mary student group wrote on their Facebook page, where they live streamed their shut down of the event: “We want to reaffirm our position of zero tolerance for white supremacy no matter what form it decides to masquerade in.“17 From these students’ perspective, the ACLU supporting someone’s right to say racist things was as bad as being a racist organization.



Most Democrats (53%) also believe supporting a racist’s free speech rights is as bad as holding racist views. However, the Democratic Party is divided by race. While 72% of black Democrats and 65% of Latino Democrats believe this, only 42% of white Democrats agree. Instead, a majority (57%) of white Democrats don’t believe supporting a racist’s right to free speech is the same as supporting racism. Majorities of independents (57%) and Republicans (66%) agree.



Men and women also disagree about whether supporting the right to speak is the same as endorsing its content. Nearly two‐​thirds (65%) of men don’t believe supporting free speech rights is the same as supporting the speech’s content. But a slim majority (51%) of women believe that it is.



A slim majority (51%) of current college students and graduate students believe a person doesn’t deserve the right of free speech if they don’t respect other people. In contrast, a majority (55%) of Americans overall don’t think a person should lose their free speech rights even if they don’t respect others.



There is also a wide race gap here. Six in 10 African Americans (59%) and Hispanics (62%) believe people don’t deserve the right of free speech if they don’t respect others, compared to 36% of white Americans. Instead a majority (62%) of white Americans think even disrespectful people should retain their free speech rights.



A majority (58%) of Americans believe people “usually have bad intentions when they express offensive opinions.” Forty‐​one percent (41%) disagree that people who offend others with their ideas usually have nefarious motives.



Democrats (69%) are 22 points more likely than Republicans (47%) to believe that people have bad intentions when they express offensive opinions. Instead, most Republicans (52%) think people may mean well even when they share an opinion others find offensive.



Latinos (75%) and African Americans (70%) are also about 20 points more likely than white Americans (52%) to think people usually have bad intentions when expressing offensive ideas.



Populists and Liberals are the most likely to believe (67%) that people who express offensive opinions have nefarious motives.18 Libertarians are the polar opposite, with 67% who do _not_ think offensive ideas imply hurtful intentions. Conservatives are evenly divided.



On the campaign trail, then‐​candidate Donald Trump contended: “I think the big problem this country has is being politically correct.“19 A strong majority of Americans (70%) agree with this sentiment. Even though the survey did not attribute the quote to President Trump, fully 90% of Republicans and 78% of independents agree. Democrats are evenly divided.



Why do many people believe political correctness is a problem? Why do others believe it is necessary? Nearly three‐​fourths (71%) of Americans say that political correctness has done more to silence important discussions our society needs to have. Conversely, a little more than a quarter (28%) think that political correctness does more to help people avoid offending others.



Strong majorities of white Americans (74%), African Americans (64%), and Latinos (58%) agree that political correctness has silenced necessary conversations. Overwhelming majorities of Republicans (89%) and independents (80%) also agree.



Far fewer Democrats believe political correctness has done more to silence necessary discussions (50%) than reduce offense. Liberal Democrats are driving these numbers. More than two‐​thirds (68%) of strong liberals believe political correctness primarily helps reduce offense. In stark contrast, nearly 9 in 10 strong conservatives (87%) say it primarily silences conversations society needs.



Most Americans self‐​censor their political opinions because they’re afraid they might offend someone. Nearly 6 in 10 (58%) report that the “political climate” these days prevents them from saying what they believe “because others might find them offensive.” Four in 10 don’t feel the need to censor their opinions.



The political climate appears to favor liberal Democrats, as they are among the few groups who feel they _do not_ need to censor their opinions. However most other political and demographic groups do self‐​censor.



Strong liberals are the most comfortable sharing their true beliefs (69%). Far fewer strong conservatives (24%) and moderates (41%) agree. Similarly, Democrats (53%) are more likely than Republicans (26%) and independents (39%) to feel they can express their opinions. Instead, nearly three‐​fourths (73%) of Republicans and 58% of independents are afraid to share some of their true beliefs because of the political climate.



Why are Republicans more afraid than Democrats to share their views in this “political climate” given that Republicans currently control both Congress and the White House? Perhaps political power does not solely determine the political climate. Cultural sources of power, such as media, academia, and entertainment may matter more. The survey found that Americans believe most large media outlets, like the _New York Times_ (52%) and CNN (50%), have a liberal bent. A plurality (45%) also believe college faculties are mostly liberal. These institutions may shape the political environment such that liberals feel more comfortable sharing their political views.



But perhaps, one might argue, liberals feel more comfortable sharing their political opinions because their views are less offensive. However, the survey found several instances where conservatives are more offended than liberals by political views more commonly held among liberals. For instance, conservatives are about twice as likely as liberals to say calling the police racist is hate speech (39% vs. 17%). Conservatives are also somewhat more likely to believe it’s hateful to say that America is an evil country (39% vs. 29%). Conservatives are also more offended than liberals by flag burning and NFL players refusing to stand for the national anthem.



There are certain topics that Americans feel less inclined to discuss with others in their social surroundings, such as over dinner with co‐​workers or with classmates.



In such an environment, less than half of Americans would be “very willing” to discuss gay and lesbian issues (45%), race relations (45%), women’s issues (48%), and foreign policy (48%).20 Only about half would be similarly willing to discuss issues related to immigration (51%), the police (51%), abortion (52%), or poverty (53%). Americans are somewhat more willing to discuss the environment (64%), health care (61%), education (60%), crime (58%), and gun issues (56%).



There are some issues Democrats feel more comfortable discussing than Republicans and vice versa. Compared to Republicans, Democrats are more likely to say they’d be very willing to discuss women’s issues (57% vs. 41%), gay and lesbian issues (52% vs. 37%), poverty (57% vs. 47%), race relations (50% vs. 40%), and the environment (69% vs. 62%). Conversely, Republicans feel relatively more comfortable than Democrats talking about crime (63% vs. 54%) and gun issues (60% vs. 52%). Across the board, however, Democrats are more willing than Republicans to discuss major policy issues.



The survey also asked respondents to use their own words to describe political beliefs they hold, but feel unable to share because of the political climate. Even though Democrats are more likely than Republicans to feel comfortable sharing their opinions, Americans of all political stripes have views they censor. A sampling of these opinions can be found in the box “Dangerous Ideas vs. Approved Beliefs” below.



Liberals, particularly those in conservative areas, feel they can’t express secular beliefs, their dislike of Donald Trump, support for immigration, gun control, police reform, ending the Drug War, and LGBT rights, and a belief that racism continues in America today.



Conservatives, particularly those in liberal areas, feel they can’t share their religious beliefs, support for Trump, patriotism, a belief that racial minorities receive special privileges in society, opposition to illegal immigration, affirmative action, same‐​sex marriage, and abortion, and support for the border wall, gun rights, free speech, deportation of unauthorized immigrants, and more rigorous security screening for Muslims entering the United States.



Notably, liberals also self‐​censored conventionally conservative sentiments. These included: indifference to identity politics, a belief that racial minorities receive favoritism, support for free speech, and opposition to “PC culture” and removing Confederate statues.







Even though Democrats are more likely than Republicans to feel comfortable sharing their opinions, Americans of all political stripes have political views they feel can’t be expressed. The survey asked people to use their own words to describe what views they feel can’t be shared. Location matters a lot. Liberals in conservative areas and conservatives in liberal cities both self‐​censor.21



Nearly two‐​thirds (61%) of Americans believe that “people often call others racist or sexist to avoid having to debate with them.” More than a third (37%), however, say “people usually only call someone out for racism or sexism when they deserve it.”





A slim majority (51%) of Democrats believe that calling out racism or sexism is typically justified and not an avoidance tactic. In sharp contrast, about three‐​fourths (76%) of Republicans and two‐​thirds (65%) of independents believe it’s primarily used as a tool to stifle debate.



A majority (58%) of African Americans believe that a person called out for racism or sexism usually deserves it, while 41% think that such labels are often used to avoid discussion. Whites (66%) and Latinos (55%) are 14–25 points more likely to believe these labels are primarily used to suppress debate.





Nearly two‐​thirds (61%) of Clinton voters agree that “it’s hard to be friends with people who voted for Donald Trump” while 38% disagree. Trump voters don’t feel a similar animus toward Clinton voters. Instead, a majority (64%) of Trump voters do not think that it’s hard to be friends with Clinton voters while 34% believe it is difficult.





Two‐​thirds of Americans (66%) say colleges and universities aren’t doing enough today to teach young Americans about the value of free speech. This is a view shared by 51% of current college and graduate students, while 46% think colleges are doing enough.



When asked which is more important, 65% say colleges should expose students to “all types of viewpoints even if they are offensive or biased against certain groups.” About a third (34%) say colleges should “prohibit offensive speech that is biased against certain groups.”





Strong liberals (52%), African Americans (54%), and Latinos (54%) stand out with slim majorities who believe it’s more important for colleges to prohibit offensive and biased speech on campus. Conversely, majorities of regular liberals (66%), conservatives (73%), and white Americans (73%) think colleges need to expose students to a wide variety of perspectives even if they are offensive or prejudiced.



But Americans are conflicted. While most say colleges need to prioritize viewpoint diversity, a slim majority (53%) also agree colleges have “an obligation to protect students from offensive speech and ideas that could create a difficult learning environment.” Problems arise, as evidenced earlier in the report, when students disagree about what speech is offensive and would create a difficult learning environment.



Americans are divided by race, party, gender, and education. Nearly three‐​fourths of Latinos and African Americans (74%) agree colleges need to protect students from offensive ideas that could disrupt the learning environment. Less than half (44%) of white Americans agree. While a solid majority of Democrats (66%) believe colleges have this obligation, majorities of Republicans (57%) and independents (51%) do not believe colleges should do this.





Men and women are also divided. A majority of men (56%) don’t think colleges should protect their students from offensive ideas while 64% of women think colleges should.



With more education, Americans become more averse to colleges shielding students from offensive speech even if it risks disrupting the learning environment. Six in 10 Americans (61%) with high school degrees or less think colleges should protect students from offensive ideas, compared to 44% of those with college degrees and 37% of post‐​graduates.



Although Americans say it’s more important for colleges to expose students to a variety of diverse viewpoints, even offensive ones, many are willing to shut down speech they personally find offensive. About half of Americans who have college experience don’t think a wide variety of speakers should be allowed to speak at their university.22





An overwhelming share (81%) of respondents with college experience agree that campus speakers who advocate for violent protests shouldn’t be allowed to speak at their university. Nearly two‐​thirds (65%) oppose a speaker who would reveal the names and identities of unauthorized immigrants attending the college. A solid majority (57%) would also oppose allowing any speaker who says the Holocaust did not occur. About half would oppose allowing a speaker who says all white people are racist (51%), that Muslims shouldn’t be allowed to come to the U.S. (50%), that transgender people have a mental disorder (50%), or that gays and lesbians should receive conversion therapy (50%). Nearly half would support cancelling a speaker who says all Christians are backward and brainwashed (49%), who publicly criticizes or disrespects the police (49%), who defends the police stopping African Americans at higher rates than other groups (48%), or says the average IQ of whites and Asians is higher than African Americans and Hispanics (48%), says all illegal immigrants should be deported (41%), or says men on average are better at math than women (40%). (Results are similar among Americans without college experience who were asked if the aforementioned speakers should be allowed to speak in their local community. (See Appendix C.))



The reader may notice that most of these hypothetical speakers are taken from real‐​world examples of controversial campus speakers or other public figures who could be invited to speak on a college campus. (Note that several of these campus speakers were not shut down because of controversial ideas they planned to include in their speech but for things they have said in the past.) If campus presidents agreed to cancel speakers that large numbers of their student body and faculty found offensive, these results imply they would have to prohibit a wide range of speakers including:



Major differences emerge between Democrats and Republicans in their willingness to allow controversial and offensive speakers speak on campus. Even on issues in which one might expect Republicans to be more offended, they were less likely than Democrats to support cancelling the speaker. Majorities of Democrats would not allow, while Republicans would allow, a speaker who:



There is also a wide racial gap between white Americans and black and Hispanic Americans in allowing these speakers to come to campus. Majorities of black and Hispanic Americans would not allow, while white Americans would allow, a speaker who:



Majorities of black, white, and Hispanic Americans all oppose allowing a speaker who would reveal the names of unauthorized immigrants on campus, deny the Holocaust, or call for violent protests.





Men and women are similarly divided, with majorities of men supportive of nearly all these speakers being allowed to speak on campus and women opposed. Young Americans are also more averse to allowing these speakers to speak at their college or university, compared to older Americans.



Taken together, Republicans, white Americans, men, and older people are more supportive than Democrats, African Americans, Latinos, women, and younger people of allowing these campus speakers to speak at their college or university. Why are these latter groups more supportive of censoring speech? Perhaps because they are more likely to believe that colleges have an obligation to protect students from offensive ideas.



About two‐​thirds (64%) of current college and graduate students say that if their college or university hosted a speaker who believes some races are superior to others, they would not attend the speech. Sixteen percent (16%) say they would attend the speech. Many would also take action: 43% would attend the speech and ask the speaker tough questions, 39% would hold a counter‐​event in a different location, 26% would hold a protest outside of the speech location.



Notably, few students would try to forcibly shut down the speech by shouting loudly so the speaker cannot speak (7%) or by forcibly removing the speaker from the stage (7%).33 Although most wouldn’t use shouting or physical force to stop an offensive speech, more than a third (36%) would sign a petition to get the speech cancelled beforehand.



Democratic and Republican students say they’d handle the situation differently. Democratic students are more likely than Republicans to say they’d hold a counter‐​event in a different location (50% vs. 33%), protest outside (38% vs. 15%), or sign a petition beforehand to get the speech canceled (48% vs. 22%). On the other hand, Republican students are somewhat more likely to say they’d attend the speech and ask tough questions (53% vs. 44%) or simply attend the speech (25% vs. 15%).



More than three‐​fourths (76%) of Americans say that recent student protests and cancellations of controversial speakers on college campuses are part of a “broader pattern” of how college students respond to controversial ideas. About a quarter (22%) believe these protests and cancellations are isolated incidents, not indications of a broader pattern.



This perception is not controversial. Strong majorities of current students and non‐​students alike believe recent shut downs of campus speakers tell us something broader about how students deal with offensive ideas.



Nearly two‐​thirds (65%) of Americans say that colleges and universities should discipline college students who disrupt invited campus speakers and prevent them from speaking.



Republicans are most likely to support disciplining students (83%); 67% of independents agree. Democrats on the other hand are evenly divided over whether colleges should punish students who shut down speakers (50%). White Americans (71%) are also more likely than Latinos (51%) and African Americans (49%) to support disciplining these students.



When asked how specifically colleges and universities should handle disruptive college protestors, Americans are less resolute. A plurality (50%) say that first, colleges and universities should listen and address the students’ concerns. After that, 46% want colleges to give students a warning, 31% say colleges should note the incident on the students’ records, 22% say students should pay a fine, 20% say colleges should suspend students for 30 days, 19% want the police to arrest the students, 13% want colleges to completely expel the students, 11% want to suspend students for a semester. Only 6% say colleges should do nothing.





Democrats take a softer while Republicans take a harder approach to handling disruptive college protestors. Nearly two‐​thirds (64%) of Democrats say colleges should listen to and address the students’ concerns, compared to 36% of Republicans who agree. Conversely, Republicans are two to six times as likely as Democrats to support some sort of punishment for the students, such as noting the incident on the students’ records (41% vs. 22%); supporting suspending or expelling the students (47% vs. 15%); or having police arrest the students (32% vs. 7%).



Ultimately 75% of Republicans would impose at least one of the listed punishments, compared to less than half (42%) of Democrats. Most Democrats would rather listen and address the students’ concerns or give them a warning. Given that research shows most of academia leans left of center, this might help explain why few universities have punished students who have shut down controversial campus speakers.34



Most Americans would accede to the heckler’s veto. A solid majority (58%) of Americans think college administrators should cancel controversial invited campus speakers if students threaten to stage a violent protest. Four in 10 think colleges should move forward with the invited speaker regardless.



Democrats and Republicans disagree about how to respond to threats of student violence: 74% of Democrats think colleges should cancel such controversial speakers while 54% of Republicans think colleges should not cancel the speech.



A slim majority of men (51%) believe colleges should resist student threats. Conversely, more than two‐​thirds (67%) of women think colleges should cancel speakers if students threaten violent protest.



  
  
  
  
  
  
  




A slim majority (51%) of Americans oppose while nearly as many (48%) support the idea of a confidential reporting system at colleges through which students could report people who make offensive comments about a person’s race, gender, sexual orientation, age, or disability status.



This “bias reporting system,” as it’s often described, is highly popular among current students. More than two‐​thirds (68%) of current college students and graduate students support it while less than a third oppose (30%). However, 63% of those who have already graduated from college oppose a system to allow students to report bias on campus.



A bias reporting system is highly divisive along partisan and demographic lines. Solid majorities of Democrats (60%), African Americans (67%), Latinos (59%), and women (54%) support it. Conversely, majorities of Republicans (64%), white Americans (57%), and men (58%) oppose it.



The survey finds that many microaggressions that colleges and universities advise faculty and students to avoid aren’t considered offensive by most people of color.35 The survey included a variety of statements that major universities have identified should be avoided because the colleges contend they “communicate hostile, derogatory, or negative messages to target persons based solely upon their marginalized group membership.“36 However, most African Americans and Latinos do not find most of these statements offensive.





Strong majorities of African Americans and Latinos say the following statements are _not offensive_ :



Seventy percent (70%) of Asian Americans do not think it’s offensive to ask an Asian person, “where are you from?” (The sample size for Asian Americans is small and thus their responses are not shown separately for each of these microaggressions.)37



The one microaggression that African Americans (68%) agree is offensive is telling a racial minority “you are a credit to your race.” Latinos are evenly divided on this question.



There may be other microaggressions not included on the survey that these groups find derogatory. However, African Americans and Latinos do not find most of the key microaggressions identified in academic training manuals insulting.



Two years ago at Yale, a controversy erupted over a series of emails about offensive Halloween costumes. A resident advisor and Yale lecturer pushed back against an email from college administrators advising students not to wear offensive Halloween costumes. The advisor emailed her students and expressed confidence in students’ capacity to discuss offensive Halloween costumes among themselves without administrators getting involved. Many students interpreted her email as an endorsement of offensive costumes, rather than of freedom of expression and the ability of people to discuss and resolve offense without oversight. What do Americans think?



The survey finds that nearly two‐​thirds (65%) of Americans agree that “college students should discuss offensive costumes among themselves without administrators getting involved.” A third (33%) say “college administrators have a responsibility to advise college students not to wear Halloween costumes that stereotype certain racial or ethnic groups at off‐​campus parties.”



A significant racial divide emerges about how to handle offensive Halloween costumes. A majority (56%) of African Americans feel college administrators should intervene and advise students against offensive costumes. Conversely, a strong majority (71%) of white Americans and a majority of Latinos (56%) believe that college students should discuss offensive Halloween costumes among themselves without administrator intervention.



A majority (54%) of college and graduate students agree that students should discuss offensive costumes without intervention from school authorities. However, students (45%) are 12 points more supportive than Americans overall (33%) of administrators advising about offensive costumes.



About two‐​thirds to three‐​fourths of college students and graduate students are familiar with the new language of social justice terms and phrases that have emerged on college campuses. However, most Americans overall are unfamiliar with these words and phrases. The one exception is “safe spaces,” which two‐​thirds of the general public and 86% of current students have heard something about them.





Most Americans (55%) and current college and graduate students (55%) say college newspapers should not need approval from college administrators before printing controversial news stories and editorials. However, nearly two‐​thirds of African Americans (63%) and a majority of Hispanic Americans (54%) think student papers should get approval before printing controversial stories. In contrast, 61% of white Americans don’t think student papers should need approval.



Similar majorities of Democrats (56%), independents (55%), and Republicans (54%) oppose requiring that student papers get permission before printing controversial stories. However, Democrats are divided along racial lines. More than two‐​thirds (68%) of white Democrats do not believe such permission should be necessary while 65% of black Democrats and 57% of Hispanic Democrats believe it should be.



Men and women are also divided. Nearly two‐​thirds of men (63%) do not believe controversial news stories in student papers should need approval while 51% of women think they should.



 **The Faculty** Only 20% of current college students and graduate students believe their college or university faculty has a balanced mix of political views. A plurality (39%) of current students agree that most college and university professors are liberal. Twenty‐​seven percent (27%) believe most are politically moderate, and 12% believe most are conservative.



Democratic and Republican college students see their campuses very differently. A majority (59%) of Republican college students believe that most faculty members are liberal. In contrast, Democratic college students are 25 points less likely to believe that most of the faculty is liberal (35%). Democratic students are also about twice as likely as Republican students to think their professors are moderate (32% vs. 16%) or conservative (14% vs. 9%).



 **The Students** Current students believe that most of their campus’ student body is liberal. Fifty‐​percent (50%) believe that most students at their college or university are liberal, 21% believe most are moderate, 8% believe most are conservative, and 19% believe there is a balanced mix of political views. Democratic and Republican students largely agree on the ideological composition of their campus student body. 



In sum, there is a widespread perception that most faculty and students in colleges are liberal. These results matter because if universities become political echo chambers, it could lead to the exclusion of non‐​conforming political views, self‐​censorship, and less rigorous academic inquiry. Without a free exchange of ideas, there may be less thorough checking of academic work and the quality of research may decline. By extension, the public may lose confidence in the process of academic inquiry and become skeptical of its results.



Although many Americans favor silencing offensive speakers on college campuses and in local communities, most oppose firing people for their political beliefs or expression.



Nearly two‐​thirds (61%) of Americans oppose firing NFL (National Football League) players who refuse to stand for the national anthem before football games in order to make a political statement. These results stand in contrast to President Trump’s urging NFL teams to fire players who refuse to stand for the anthem. A little over a third (38%) of Americans align with Trump and support firing these players.38





Conservative Republicans stand out with their support for firing NFL players who refuse to stand for the national anthem. Nearly two‐​thirds (65%) of Republicans say NFL players should be fired for this reason. Only 19% of Democrats and 35% of independents agree. Punishing NFL players for their political speech distinguishes political Conservatives from Libertarians. Using a political typology to identify these ideological groups, the survey finds that Conservatives (62%) are the only political group to support firing NFL players. Conversely, 60% of Libertarians, 85% of Liberals, and 62% of Populists all oppose firing players. 39



People who are older, with less education, and living in smaller towns and rural communities are most likely to support punishing players who refuse to stand for the national anthem.



A majority (57%) of Americans over 65 think such players should be fired while 71% of Americans under 30 think they should not. Those without college degrees (44%) are more likely than college graduates (32%) and those with post‐​graduate degrees (26%) to support punishing NFL players who engage in this form of political protest. Americans living in rural communities are divided equally over whether teams should fire NFL players who refuse to stand for the national anthem. Conversely, those living in large urban centers solidly oppose (69%) such firings.



Majorities across racial groups oppose firing NFL players who kneel during the national anthem before football games. However African Americans (88%) are about 30 points more likely than Hispanic Americans (60%) and white Americans (55%) to oppose. 



Not wanting to fire NFL players because of their political expression doesn’t mean that most people necessarily agree with the content of that expression. As surveys have long found, including this one, the public opposes desecrating or disrespecting patriotic symbols, like the American flag. It’s likely such views extend to the national anthem as well. Thus, many appear to make a distinction between allowing expression and endorsing its content. Americans can be tolerant of players’ refusing to stand for the national anthem, even if they don’t agree with what the players are doing.



Most Americans (55%) don’t think a business executive should be fired from their job if they burn an American flag as part of a weekend political protest. However, a majority (54%) of Republicans think an executive should be fired for flag burning on the weekend. A plurality (50%) of Hispanics agree with Republicans that such an employee should be fired. In contrast, majorities of Democrats (61%), independents (57%), white Americans (56%), and African Americans (57%) don’t believe this should be a fireable offense.



A slim majority (53%) of Americans say that business employers should not discipline their employees for posting controversial or offensive opinions on social media accounts like Facebook. Forty‐​six percent (46%) think businesses should.





Democrats stand out with 58% who say businesses _should_ discipline their employees for offensive Facebook posts. In contrast, 60% of Republicans and 62% of independents think employees shouldn’t be punished at work for what they write online.



There is also a racial divide. A majority (59%) of African Americans think employees should be subject to discipline at work for their social media posts, while 56% of whites think they should not. Latinos are evenly divided.



Majorities of Americans don’t want to fire people from their jobs because of their political beliefs. But, the public is most likely to support firing an executive who believes that African Americans are genetically inferior (46%). About a quarter to a third support firing business executives who believe that all white people are racist (35%), believe transgender people have a mental disorder (30%), believe men are better at math than women (26%), believe psychological differences help explain why there are more male than female engineers (25%), or believe homosexuality is a sin (22%).



Besides a belief in biological racism, majorities of Democrats and Republicans oppose firing business executives for these other beliefs. Nonetheless, Democrats are considerably more likely than Republicans to support doing so. Democrats are about three times more likely than Republicans to support firing an executive if they believe transgender people have a mental disorder (44% vs. 14%) or believe homosexuality is a sin (32% vs. 10%). Democrats are twice as likely as Republicans to support firing an employee if they believe psychological differences help explain why there are more male engineers (34% vs. 14%), or that men are better at math than women (35% vs. 17%). Democrats and Republicans are more similar in their support for firing executives who believe all white people are racist (40% vs. 33%).





We find that the more strongly a respondent identifies as liberal the more supportive they are of firing people for each of these beliefs. However, the more strongly a respondent identifies as conservative the more likely they are to support firing a person for burning an American flag or firing an NFL player for refusing to stand for the national anthem. Thus, Americans become more likely to support firing people for offensive beliefs and expressions the more ideological—either liberal or conservative—they become.





Some of these results are surprising given that they test the boundaries of tolerable beliefs in the workplace. For instance, one might have expected that a belief in biological racism would be grounds for firing a business executive in charge of fostering merit and talent among all employees. Nevertheless, most Americans oppose firing someone for this belief.



Furthermore, few Americans wish to fire executives for their beliefs about homosexuality or differences between men and women. These results imply that high‐​profile firings in recent years of Silicon Valley executives and employees for these reasons, such as Brendan Eich at Mozilla or James Damore at Google, do not reflect the demands of the public at large.



Early in his presidential tenure, Donald Trump tweeted that the national news media is “fake news” and that it is an enemy of the American people.40 Nearly two‐​thirds (64%) of Americans do not agree with President Trump that journalists today are an “enemy of the American people.” Thirty‐​five percent (35%) side with the president.



However, nearly two‐​thirds (63%) of Republicans agree that journalists are an enemy of the American people. Such a charge is highly polarizing: 89% of Democrats and 61% of independents disagree.





Although Republicans think that the national news media is a threat, they don’t believe government ought to regulate news stories, even if biased or inaccurate. Strong majorities of Republicans (63%), independents (71%), and Democrats (76%) agree that “government should not be able to stop a news media outlet from publishing a story that government officials say is biased or inaccurate.”



Among all Americans, 70% say government should not shut down news stories regardless of whether officials think the story is inaccurate. A little more than a quarter (29%) think government should have the authority to stifle stories authorities say are inaccurate or biased.





While Republicans stand out with their negative view of the media, Democrats have uniquely positive evaluations of it. A slim majority (52%) of Democrats say the national news media is doing a good or even excellent job “holding government accountable.” In contrast, only 24% of independents and 16% of Republicans agree.



Among all Americans, only a third (33%) agree the news media is doing its job holding government accountable. More than two‐​thirds (67%) say it is not. Even more Republicans (84%) and independents (75%) share such negative views of the media.



The more a person identifies as liberal the more likely they are to say the media is doing a good job. Among strong liberals, 59% say the national news media is doing a good or excellent job holding government accountable. In contrast, 87% of strong conservatives say it’s doing a poor or fair job.



Why do Republicans lack confidence in the national news media while Democrats view it positively? Perhaps because most Americans perceive a liberal bias among most major news organizations.41





Fifty‐​two percent (52%) of respondents say that the _New York Times_ allows a liberal bias to color its reporting. Fifty percent (50%) feel CNN also succumbs to a liberal media bias. Fifty‐​nine percent (59%) say that MSNBC also has a liberal bias. Of all the top news organizations included on the survey, only Fox News was perceived to have a conservative bias (56%).



Americans feel their local news stations and broadcast news channels do a better job than cable news in providing balanced reporting. A majority (54%) say their local news station is balanced, without a liberal or a conservative bias. A plurality (42%) also believe that CBS is balanced. Nevertheless, respondents were four times as likely to say CBS has a liberal bias than a conservative bias (40% vs. 10%), and almost twice as likely to say their local station has a liberal bias (23% vs. 14%).



Majorities of Democrats believe most major news organizations are balanced in their reporting, including CBS (72%), CNN (55%), the _New York Times_ (55%), as well as their local news station (67%). A plurality (44%) also believe the _Wall Street Journal_ is balanced. The two exceptions are that a plurality (47%) believe MSNBC has a liberal bias (37% believe it’s unbiased) and a strong majority (71%) say Fox has a conservative bias.



Republicans, on the other hand, see things differently. Overwhelming majorities believe liberal bias colors reporting at the _New York Times_ (80%), CNN (81%), CBS (73%), and MSNBC (80%). A plurality also feel the _Wall Street Journal_ (48%) has a liberal tilt. Only when evaluating their local TV news station do most Republicans, but not a majority, perceive balanced reporting (42%). Similar to Democrats’ perceptions of MSNBC, a plurality of Republicans (44%) believe Fox News has a conservative bias; 41% believe it provides unbiased reporting.





The news outlets that Republicans find most objective are their local news station (42%), Fox (41%), and the _Wall Street Journal_ (28%). The media organizations Democrats find most objective include CBS (72%), their local news station (67%), CNN (55%), and the _New York Times_ (55%).



Who cares more about protecting religious liberty in the United States? It depends on whose liberty is at stake. Republicans tend to care more about protecting the conscience of religious bakers, florists, and other wedding‐​related businesses who refuse service to same‐​sex weddings. On the other hand, Democrats care more about ensuring Muslims have the right to build mosques in their communities.



Americans make a distinction between requiring businesses with religious objections to serve gay and lesbian _people_ and providing custom services to same‐​sex _weddings._



While 50% of Americans say businesses with religious objections should be required to provide services to gays and lesbians, only 32% think a baker should be required to bake a special‐​order cake for a same‐​sex wedding. Instead 68% say a baker should not be required to bake a custom wedding cake if doing so violates their religious convictions.



Majorities of Democrats say a business should be required to provide service to both LGBT people (73%) and bake a custom cake for same‐​sex weddings (52%), even if doing so violates the business owner’s religious beliefs. Conversely, majorities of Republicans say business owners should not be required to provide services in either situation, either to LGBT people (77%) or for same‐​sex weddings (87%).



Most Americans (73%) do not view baking a special‐​order wedding cake for a same‐​sex wedding as an endorsement of same‐​sex marriage. About a quarter (26%) do view it as an endorsement. However, Republicans (41%) are 28 points more likely than Democrats (13%) to view baking the cake as an endorsement of the marriage.



Evangelical Protestants are also more likely to believe (42%) that baking a custom cake for a same‐​sex wedding would be an endorsement of that wedding. In contrast, about a quarter of Mainline Protestants (26%), Catholics (27%), or other religious groups (28%) view it as an endorsement. Only 14% of non‐​religious people agree.



These data suggest that one reason Americans may disagree about requiring businesses service same‐​sex weddings is they don’t agree on what providing those services means. For some Americans, it would require them violate their conscience, while it would not appear that way to others.



What should happen to a religious baker who refuses to bake a special‐​order cake for a same‐​sex wedding? Most Americans (66%) say nothing should happen to the baker. Alternatively, a fifth (20%) would support a boycott of the bakery and 22% would support some kind of government punishment including: issuing a fine (12%), requiring an apology (10%), issuing a warning (8%), revoking their business license (6%), or sending the baker to jail (1%). Another 6% support suing the baker for damages.





Strong liberals stand out with a majority (58%) who favor some form of government punishment for a baker who refuses to bake the cake. In contrast, 22% of moderates and only 4% of strong conservatives support some form of government sanction against the baker or bakery.



An overwhelming majority of Americans oppose requiring churches and religious organizations perform same‐​sex wedding ceremonies if doing so violates their religious beliefs. This is non‐​controversial, with strong majorities of Democrats (73%), independents (81%), Republicans (91%), evangelical Protestants (92%), and non‐​religious people (72%) in agreement.



A slim majority (52%) of Americans say that local government officials should be required to perform same‐​sex wedding ceremonies, even if doing so violates that official’s religious convictions. Nearly as many (47%) say these officials should not be required to perform these ceremonies.



Partisans are sharply divided. Nearly 7 in 10 (69%) Democrats say local officials should be required to perform same‐​sex wedding ceremonies. In contrast, 68% of Republicans say such officials should not be required to do this. Independents are divided, with a slim majority (51%) who say officials should perform the ceremonies.



Most Americans (69%) would oppose a law that would ban the building of mosques in their community while 28% would favor. Although a slim majority (51%) of Republicans also oppose such a law, they are the most likely group to support it (47%). Far fewer Democrats (14%) and independents (28%) would also support a ban on building mosques in their communities.





The question distinguishes Libertarians from Conservatives. Using a political typology to identify ideological groups,42 we find that Libertarians (76%) are 25 points more likely than Conservatives (51%) to oppose a ban on building mosques. Eighty‐​nine percent (89%) of Liberals and 67% of Populists also oppose such a law.



The Cato 2017 Free Speech and Tolerance survey asked the following three questions to identify clusters of like‐​minded respondents based on their answers to questions about the proper role of government involvement in economic affairs and in promoting traditional values.



Respondents were divided into five groups, based on whether they wanted more or less government involvement in economic affairs and promoting traditional values. Here are the five groups defined:





The Cato Institute 2017 Free Speech and Tolerance Survey was conducted by the Cato Institute in collaboration with YouGov. YouGov collected responses August 15 to 23, 2017, from 2,547 Americans 18 years of age and older who were matched down to a sample of 2,300 to produce the final dataset. The survey included oversamples of 769 current college and graduate students, 459 African Americans, and 461 Latinos. Results have been weighted to be representative of the national adult sample. The margin of error for the survey, which adjusts for the impact of weighting is +/- 3.00 percentage points at the 95% level of confidence. The margin of error for current college and graduate students is +/- 5.17; for African Americans it is +/- 6.69; for Hispanics it is +/- 6.68; for whites it is +/- 4.13. This does not include other sources of non‐​sampling error, such as selection bias in panel participation or response to a particular survey. 



Data on the moral acceptability of punching a Nazi come from a Cato Institute/​YouGov survey conducted August 21 to 22, 2017, of 1,141 respondents, with a margin of error of +/- 4.5 percentage points, which adjusts for the impact of weighting.



YouGov conducted the surveys online with its proprietary Web‐​enabled survey software, using a method called Active Sampling. Restrictions are put in place to ensure that only the people selected and contacted by YouGov are allowed to participate.



The respondents in each survey were matched to a sampling frame on gender, age, race, education, party identification, ideology, and political interest. The frame was constructed by stratified sampling from the full 2013 American Community Survey (ACS) sample with selection within strata by weighted sampling with replacements (using the person weights on the public use file). Data on voter registration status and turnout were matched to this frame using the November supplement of the Current Population Survey (CPS), as well as the National Exit Poll. Data on interest in politics and party identification were then matched to this frame from the 2007 Pew Religious Life Survey. The matched cases were weighted to the sampling frame using propensity scores. The matched cases and the frame were combined and a logistic regression was estimated for inclusion in the frame. The propensity score function included age, gender, race/​ethnicity, years of education, non‐​identification with a major political party, census region, and ideology. The propensity scores were grouped into deciles of the estimated propensity score in the frame and post‐​stratified according to these deciles. The weights were then post‐​stratified to match the election outcome of the National Exit Poll, as well as the full stratification of four‐​category age, four‐​category race, gender, and four‐​category education.
"
"This summer has seen mother nature banish us to our bedrooms. It’s been the summer of staying indoors – whether you’re in Australia sheltering from violent storms and avoiding the worst air quality in the world, or abroad, stuck in your hotel room, cruise ship cabin or apartment trying not to infect or be infected with coronavirus. It’s impossible to tell if this mass banishment indoors is just a temporary thing – a strange confluence of events (weather patterns, rapid spread of mysterious virus) – or whether this is the new normal. As the world out there feels increasingly unsafe and unpredictable, are we going to spend more and more time inside? Already as a society we’re showing a preference for the indoors life. From the comfort of our couch we order in UberEats or Deliveroo rather than go out to a restaurant, we stream movies at home rather than go to the cinema, we talk to our friends on group chat rather than meeting up at the pub, we meet people through dating apps rather than on the dancefloors or at bars, and we’ll exercise at home – following a YouTube tutorial such as the highly popular Yoga with Adrienne – rather than attend a class. In overcrowded, hot cities such as Cairo, leaving your apartment for anything from a haircut to a blood test has become such a hassle that a micro economy has sprung up in delivering goods and services. For a few dollars someone will come to your house and wax your bikini line or deliver you the paper and a coffee. One man told the New York Times: “There is no incentive to go outside. Just one hour outside is enough to ruin anyone’s day.” It’s probably a good thing we’re getting more and more habituated to spending time at home, as increasingly we are finding ourselves confined there, but not by choice. Right now tens of millions of people in China are either in self-imposed or forced quarantine, thousands are stuck in their cabins on infected cruise ships, and others have been quarantined in detention centres, air force bases and disused mining camps. In Australia, appalling air quality from the bushfires meant schools kept their students indoors at lunch and recess, and parents were not letting their kids play outside. Friends who are runners have complained they couldn’t exercise outside during the haze, and everything from music festivals to fun runs were cancelled due to fires and poor air quality. Eastern New South Wales got another taste of cabin fever when 550mm  of rain fell across parts of NSW last weekend. Sydney recorded its heaviest rain in three decades and was battered by gale-force winds. On Sunday, the worst day of the storm, I was trapped at a friend’s house in North Bondi. I had stuff to do, people to meet, food to buy!! – but leaving seemed … dangerous. Rain came in over the golf course across the road in vast, thick sheets. I saw no one on the street all day, except for a lone man with a dog. Both appeared to be walking sideways. The pair disappeared over the hill, towards the cliffs, presumably never to be heard from again. On the balcony, the gales knocked over and smashed large terracotta pots, the rain came in sideways – in both directions - colliding then being picked up by the wind and swirled in the air in a mini-tornado. All I could do was sit on the couch and watch it in horror as the ceiling leaked in the bedroom, ruining the carpet, and the wind sounded like someone badly imitating a ghost. Woooooo … woooooooooo … Ordering UberEats was out of the question. It would be cruel, possibly psychotic, to insist that some young person on a pushbike and zero hours contract deliver a burrito while dodging flying debris and falling trees. (Although restaurants will argue that they need to keep making money during bad weather.) Socialising was also cancelled. Instead I kept in touch with friends throughout the day via text (boredom was the main gripe), and passively, through watching each other’s Instagram stories of sodden carpets, scared pets and knocked-up pantry dinners. Expect more of it – these weather “events” are becoming more extreme and frequent across the world. Just this week, Storm Dennis and a few days earlier Ciara hit the UK with “snow showers combine(d) with strong winds”. Up to 10cm of snow was forecast in higher areas, hardly ideal weather to be out and about. People are coping in different ways with being trapped indoors. One man confined in China has run 50km around his living room. “I have not been outside for many days, today I cannot bear sitting down any more!” the amateur marathon runner posted last week. Meanwhile an Australian couple quarantined in their cabin on a cruise ship for 14 days “kept the party flowing by getting a drone to deliver wine (pinot noir) straight to their cabin”. Having been quarantined myself, in 2012 for whooping cough, I can tell you it’s no fun. After being sent home from the doctor and told not to leave the house for two weeks, I had to isolate myself. I couldn’t go into work (apologies Microsoft!), my flatmate had to move in with her girlfriend until the contagion had passed, and I switched my entire life – everything from socialising to grocery shopping – online. If I didn’t die of whooping cough, I thought I would die of boredom. I was going stir crazy sitting inside staring at the walls, not to mention lonely. Little did I know, I had glimpsed the future. Brigid Delaney is a Guardian Australia columnist "
"
Share this...FacebookTwittertutorial main bandarqq Online supaya Menang tidak sedikit , terhadap peluang yg indah ini kami dapat sedikit mengulas info yg berhubungan bersama perjudian online, utk artikel kali ini kita dapat mengupas tuntas seperti apa meraih kemenangan dalam main judi card domino88, nah sebelum masuk ke pembahasan ada sekian banyak perihal yg butuh kamu ketahui terkait dgn permainan judi satu ini. mula-mula merupakan buat menyangkut berapa jumlah card yg dimanfaatkan dalam permainan ini, permainan card domino sendiri ialah salah satu tipe permainan card yg telah lama ada, terkait dgn jumlah card yg dimanfaatkan, permainan ini memakai jumlahnya 52 kombinasi card bersama bulatan 1 pasang & mempunyai nilai yg berlainan.
selanjutnya buat system permainannya sendiri lumayan gampang & simple buat dipelajari, masing-masing buat satu meja mampu terdiri dari 6-8 orang pemain, buat tiap-tiap pemain dapat mendapati pembagian 4 card, pembagian card perdana dilakukan sejalan jarum jam, sebelum menuju pembagian card ke-2 rata-rata pemain bakal disuruh utk pilih call, raise, fold, check maupun all in. seterusnya baru card ke-2 bakal dibagian, nah disini system permainannya yaitu member yg mendapati card dgn nilai yg paling gede sehingga dialah yg dapat jadi pemenangnya, lumayan gampang bukan ? pasti saja pass gampang bila dimainkan oleh para pemain profesional, sedangkan utk kamu sendiri masihlah nampak susah, berikut ini merupakan pembahasan trik menang bemain judi card domino 99 online paling enteng buat pemula. utk meraih kemenangan main judi QQ online, sehingga aspek perdana yg mesti kamu melakukan merupakan percaya bersama insting yg kamu punyai, seluruhnya pemain judi tentunya mempunyai insting buat kemenangan. seandainya kamu telah pass percaya, sehingga kemenangan telah dekat ada didepan mata kamu. main-main judi QQ domino 99 bukan lagi semata-mata cuma bersama memanfaatkan unsur tebakan saja, melainkan ada unsur di mana seluruhnya pemain mesti memang mendalami & percaya dgn ketentuan mereka. buat dikarenakan itu kalau kamu mau menang main-main judi, pahami bersama baik seluruh permainan & pastikan kamu percaya bersama insting yg kamu miliki.
selanjutnya yg berikutnya seandainya mau menang main-main judi domino sehingga kamu mesti memang lah sanggup mendalami bersama baik permainan ini, apalagi buat kombinasi card sendiri. Ada tidak sedikit pemain yg senantiasa terkecoh bersama kombinasi card gede utk perdana kali muncul contohnya saja 9 + 6, & terhadap hasilnya mesti kalah. dgn kemunculan angka 9 + 0. system permainan ini memang lah sedikit mengecoh para pemainnnya, menjadi jangan sampai sempat gemar dulu diwaktu kamu memperoleh kombinasi angka 9 ataupun angka istimewa yang lain. butuh kamu pahami dgn baik bahwa judi pun mempunyai segi kelemahan adalah tiap-tiap kemunculan susah utk kamu tebak. setelah itu jikalau kamu mau menang dalam main-main judi domino 99 sehingga kamu mesti mampu main-main dengan cara sehat, tujuan dari main-main dengan cara sehat ini ialah main-main dgn pikiran yg kalem. Ada tidak sedikit pemain yg kadang main-main bersama trick yg serakah atau nafsu tinggi buat mendapati kemenangan dalam disaat yg langsung, padahal sanggup dikatakan apabila main dgn nafsu yg tinggi cuma bakal mengambil kekalahan dalam ketika yg segera. buat itu konsisten konsentrasi & main dgn trick yg baik.
silakan bergabung dgn kami & temukan kemenangan yg pantas seperti sama seperti pemain judi sejati.
Share this...FacebookTwitter "
"For the past few million years the world’s oceans have existed in a slightly alkaline state, with an average pH of 8.2. Now, with carbon emissions escalating, there is more CO₂ in the world’s atmosphere. This dissolves in the oceans, altering the chemistry of the seawater by lowering the pH and making it more acidic – up to 30% more in the past 200 years. This growing acidification of the oceans is becoming a serious problem for the production of shellfish around the world. Shellfish are creatures which produce calcium carbonate shells and skeletons, such as mussels, oysters and corals. They create their protective shell structures through a process known as biomineralisation – producing hard minerals such as calcium carbonate by filtering calcium and carbonate from the water. If the amount of carbonate available in the oceans is reduced by acidification, it limits the ability of these creatures to create shells. But now coastal acidification is happening close to land in regions where freshwater run-off can release sulphate soils and excess carbon, which also lowers pH and carbonate available for producing shells. This is being exacerbated by flooding and rises in sea levels caused by climate change. Recent studies reported these implications for the Sydney rock oyster in New South Wales, Australia. Historically, oyster production in the region has seen a decline in larger “plate-grade” oysters and an increase in smaller oysters. This can be due to a number of reasons which are physical, biological and economic, including pressures on farmers to harvest oysters early to avoid high winter mortalities in cold dry weather. But our recent study suggests that coastal acidification in Australia is damaging oysters’ ability to grow properly as well. The change in shell growth mechanisms could have implications for the future, such as producing thinner shells which are prone to fracture, causing potential risk of shell damage during culture and harvesting. The situation in New South Wales is not an isolated case. In Washington state in the US, acidification caused by deeper, colder seawater with high levels of CO2 rising to the surface has caused malformations in oyster larvae and loss of hatchery seed production. Read more: Explainer: why ocean acidification is the evil twin of climate change  A report by one shellfish hatchery detailed the impact on shell formation in oyster larvae under these detrimental conditions. Oyster farms in Washington, have put measures in place to sustain oyster shellfisheries under increasingly acidic conditions. This includes treating hatchery water to increase pH, making more carbonate available for early larvae shell formation, and growing oyster seed in different locations to ensure their survival for future production.  In Scotland, a country famous for its high-quality shellfish, acidification is less of an imminent threat. There are no sulphate soils or deeper water with high levels of CO2 rising to the surface, as can be found in Australia and America. But as coastal acidification is made worse by climate change – in particular freshwater run-off from increased rainfall and sea-level rise – this could have a serious effect on commercial shellfisheries all over the world, including Scotland. The changes in seawater chemistry associated with freshwater run-off include lowered salinity and pH, and carbonate availability. This, coupled with increasing temperatures, adds pressures to shellfish farmers producing mussels and oysters. I have previously reported on the effect of experimentally induced high CO2 acidification on mussels, where shells showed reduced growth and became more brittle. Shellfish are predicted to produce thinner shells which may also be more prone to fracture throughout harvesting, transportation or when another animal attempts to eat them. The industry needs to consider ways to reduce this risk. In Washington producers have adjusted the carbonate chemistry in oyster hatcheries to develop larvae before release into farms where acidification has the potential to reduce early shell development. In New South Wales, the Department of Primary Industries has done studies on the Sydney rock oyster to examine the potential for selective breeding to develop resilient strains that can cope better with more acidic seawater conditions. Researchers have reported on the potential for selective breeding for disease resistance and faster shell growth, which could create acidification resilience in the oyster. Our next step working with Australia’s DPI is to examine these selectively bred oysters to understand the potential for combating the acidification problem. It is important for the Scottish shellfish industry to understand the risks posed by climate change already playing out in Australia and the US. With climate change in the future threatening freshwater and CO2-induced ocean acidification in UK waters, the country could suffer the same fate."
nan
"As wind power companies venture into ever-deeper waters, the traditional windmill-style turbine may not be the most suitable solution. It’s time to look at alternatives. Wind turbines traditionally had blades that rotate around a horizontal axis, and this has remained standard in modern wind farms. Conventional horizontal-axis wind turbines (HAWTs) came to dominate the wind energy market, both on land and in shallow waters offshore. But the best wind farm sites are often found in deeper seas, far from wind obstructions, shipping lanes, nimbys and migrating birds. Support structures fixed rigidly to the seabed may not be economically viable in depths beyond 50m, so engineers are instead turning to floating foundations. These are very different conditions in which to generate electricity from wind, which has meant a re-emerging interest in alternative wind turbine configurations. As we report in Philosophical Transactions of the Royal Society adopting a vertical rotation axis is one such alternative. These turbines, which always face the wind, are known as vertical-axis wind turbines (VAWTs). Traditional horizontal-axis turbines are very top-heavy, with the blades and the equipment that generates power necessarily fixed to the top of the tower. In a large offshore turbine this part can weigh several hundred tonnes and be around 100m above sea level. In a vertical-axis turbine the generation systems can be at a much lower height, even right at the base of the tower. This means VAWTs tend to have a much lower centre of gravity. On floating supports this advantage is magnified – just think of how stable you are when seated in a canoe rather than standing on it. Lowering the generators also makes them much more accessible and easier to maintain or replace. For example, with a VAWT workers only need to climb a short distance to access machinery rather than climb a 100m tower that is constantly moving.  In general, bigger wind turbines produce cheaper electricity. This has led to turbines getting ever larger and more powerful, especially offshore where 50m blades are now routine and some are even far bigger. But such horizonally-rotating giants may soon reach their limit. If the axis of the blade is considered, when this blade is horizontal the gravitational force will be perpendicular to it, while when the blade is vertical (upward) the gravitational force will be parallel to the axis and pointing toward the root of the blade. Finally, when the blade is vertical again (downward), the gravitational force will be parallel to the blade axis, but now pointing toward the tip.
Some studies claim this oscillating gravitational load limits the size of horizontal axis turbines. Vertical-axis turbines resolve this problem because as they rotate they experience a constant gravitational force, always in the same direction. Without the stress of holding up 80m metal blades by one end, VAWTs can potentially become much larger. Research has also shown vertical turbines can be placed closer to each other in a wind farm. So for a given area, more VAWTs can be installed, and more electricity generated than if HAWTs were used, thereby reducing the cost of electricity. So now the obvious question arises: why have VAWTs not yet been used if they are so advantageous? The main reason is that it is still a new technology. Although researchers have looked at vertical axes for decades, the technology fell behind due mainly to material and bearing system limitations. Investors still see the known technology as a safe bet and even when looking at deep-sea wind they have been more inclined to support horizontal-axis turbines on floating platforms. However both governments and forward-thinking companies are now investing in the potential of VAWTs for deep sea offshore wind. The Nova project, funded by the UK government, demonstrated vertical-axis offshore was commercially viable. Sandia National Laboratories, backed by the US energy department, and various EU-backed projects  have also supported the technology. Industrial companies such as the British VertAx Wind or Norway’s Gwind are also following suit through the development of different concepts. The French company Nenuphar is pushing ahead its VAWT design, with plans (pictured at the top of this article) to install the first floating wind farm off the coast of Marseilles. With increasing demand for energy we are obliged to explore every possible solution. Vertical-axis wind turbines located in the deep sea have strong potential to be one of these solutions."
"If we don’t make a fundamental change to the way we are living, the world faces
the destruction of entire eco-systems, flooding of coastal areas, and ever more extreme weather. Such was the stark warning in  a recent Intergovernmental Panel on Climate Change (IPCC) report. The task is enormous.  One way to approach it is to look back to a time when scientific thinking did manage to initiate revolutionary changes in our outlook. In the 17th century, the philosopher Francis Bacon called for a “great fresh start” in our thinking about the natural world, and helped usher in the scientific revolution that replaced the staid thinking of the time. We could do worse than follow his example once again – this time in our social and political thinking – if we are to tackle the biggest challenge of our era. In his key work Novum Organum, Bacon identified “four idols” of the mind – false notions, or “empty ideas” – that don’t just “occupy men’s minds so that truth can hardly get in, but also when a truth is allowed in they will push back against it”. A true science, he said, should “solemnly and firmly resolve to deny and reject them all, cleansing our intellect by freeing it from them”.  Bacon’s idols – listed below – are no longer part of standard scientific thinking, but they are still in place within our moral and political thought, and provide a useful model for understanding the challenges we face and how we might respond to them. For Bacon, these “have their foundation in human nature itself … in the tribe or race of men”. Human understanding, says Bacon, “is like a false mirror, which … distorts and discolours the nature of things by mingling its own nature with it”.  Bacon was referring to our understanding of the world around us. But his point applies to our morality too. As the philosopher Dale Jamieson has argued, our natural moral understanding is too limited to grasp the moral consequences and responsibility that comes with a problem like climate change, in which diffuse groups of people cause a diffuse set of harms to another diffuse set of people, over a diffuse range of time and space. Since the “idols of the tribe” are natural and innate, they are tricky to shift. As Jamieson argued, one way to combat them is for individuals to mindfully cultivate green virtues, such as rejecting materialism, humility about your own importance, and a broad empathy with your ecosystem. “Everyone has a cave or den of his own,” Bacon wrote, “which refracts and discolours the light of nature.” The cave is the knowledge set, specific to each individual, as a result of their upbringing and learning.  This has become even more splintered in recent years, as people follow their own silos of information online. For instance, although most in the UK think that rising global temperatures are the result of man-made emissions, a sizeable minority (25%) do not. On the day of the recent IPCC report, much of the UK press ran as their main story a drunken kiss between two contestants on a reality TV show. To combat the idols of the cave we must ensure that, through education, the media and culture, the scientific consensus behind climate change is well known.  For Bacon, these arose “from consort, intercourse, commerce”. Everyday language, he argued, diminishes our understanding of the world by promoting concepts “imposed by the apprehension of the vulgar” over those of “the learned”.  The language that dominates contemporary political and economic discourse similarly diminishes our relationship with the natural world. The emphasis is on profit, consumption and continuous growth, rather than well-being and sustainability.  Consequently, our economic system is not well geared towards the environment.  “Donut Economics”, and the “post-growth” movement are useful proposals for reframing our economic systems and combating Bacon’s idols of the market. At a global political level, the UN’s 17 Sustainable Development Goals provide a basic political vocabulary for tackling climate change. These “are idols which have immigrated into men’s minds from the various dogmas of philosophies[…]representing worlds of their own creation”. They are preconceived dogmas – of a religious, political or philosophical kind – that undermine clear, evidence-based thinking about the world. In contemporary politics, preconceived dogma – often in the form of vested interests – continues to exert a hold on our response to climate change. For instance broadcasters routinely invite climate change deniers (often industry-funded) to debate points of scientific evidence, on the grounds of “balance”.  To combat the idols of the theatre, we need a recognised global hub where relevant information from expert bodies can be assessed and translated into actions. This would be the modern equivalent of the French mathematician Marin Mersenne in the 17th century, whose wide range of contacts (from Hobbes to Pascal to Descartes to Galileo), allowed to him act, as Peter Lynch puts it, like “a one-man internet hub” for the emerging scientific revolution. To tackle climate change, we urgently need a far-reaching restorative project, of similar scale and scope to the scientific revolution. Such change can sometimes seem remote and difficult to conceive. Yet, as Bacon himself put it:  By far the greatest obstacle to the progress of science – to the launching of new projects and the opening up of new fields of inquiry – is that men despair and think things impossible."
"
Share this...FacebookTwitterIt was bound to happen sooner than later. A high level German politician speaking out against dubious climate science. Marie-Luise Dött, German Parliamentarian and a central figure on Angela Merkel’s environmental committee, expressed scepticism on climate change, the Financial Times Deutschland reports here in an article titled: The Climate Revisionists.
Now she is at the receiving end of brimstone and hellfire from all sides, including the media.
Here in Germany, climate skeptics face a level of intolerance not seen here in 65 years.
Last Wednesday, she made comments at a parliamentary forum discussion on the economic  impacts of climate protection held by the FDP Free Democrats, the junior coalition partner of Angela Merkel’s CDU/FDP coalition government.  Fred Singer – “a tobacco lobbyist” – was a guest speaker.
Dött’s comments not only left environmentalists and climate protection activists speechless and gasping for air, but exposed Dött as a climate skeptic. She is reported to have called climate protection a
…replacement religion, and that anyone who dared to express doubt could be branded an outlaw, forced to confess sins, sent to purgatory, or even cast into hell, if being really bad.
Free scientific thinking is a myth here.
Well, the vicious intolerant reactions she is now reaping confirm that her views are accurate, more than ever imagined. Even colleagues from within her own CDU Party piled on:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The next days are going to be very uncomfortable for her.
The intolerance from the opposition came swiftly. Hermann Ott, a spokesman for the German Green Party, blasted Dött and her CDU Party:
The CDU and the FDP Free Democrats are moving outside of the common community when they provide a forum in the German Parliament for the blind theories of climate change deniers.
(Note: denying the Holocaust in Germany is a crime. Ott is de facto calling Dött a criminal of the worst kind).
A member of the SPD was said to be in “shock” and demanded Dött be fired. He added it all confirmed the “real intentions of the coalition government.”
I’m not even going to get into what the media snobs are saying. Noses could not be higher.
Frau Dött not only has revealed herself to be skeptical of climate science, but has exposed Germany’s return to last century’s intolerance.
You can send a message of support here: E-Mail to MdB Marie-Luise Dött. Just fill in your message in the box: “Nachricht” with your name and city (You can ignore the boxes below the “Nachricht” box – they’re optional. 
Share this...FacebookTwitter "
"

Yes, you read the title correctly.
Sometimes I feel like a strange attractor for weather station chaos. Here I am at home tonight minding my own business, in my home office and I have the TV on. JEOPARDY comes on. Alex Trebek announces the categories…and I pay little attention until the last one is announced and he says “National Weather Service”. I practically got whiplash turning to look at the TV.  In 25 years of watching this TV program, that is a category I never expected to see.
Then to explain the category, up pops one of the “clue crew” people standing in front of the NWS office in Upton, New York, in the parking lot.
I didn’t hear a single word she said, because my eyes were transfixed on what was right behind her: a Stevenson Screen and MMTS just a couple of feet from the parking lot with the brick walls of the NWS office right behind it.
WTH!? Then it was gone.
I waited out the first round of JEOPARDY hoping to see more, but the contestants avoided the NWS category. Finally with nothing left they started into it. Then there it was again, the NWS station with visitor parking privileges.
After acing the category (the final answer was supercell) I decided to see if I could find this NWS office in Upton and maybe get a picture. I found that and more.
My first simple Google Image search found it right away, a photo taken during an open house on a Skywarn page:
MMTS and Stevenson Screen, NWS Office Upton NY - Photo: Bergen Skywarn
It did show the proximity to the brick building, but it really didn’t tell the whole story of what I saw in the TV shot. What was funny was that in the JEOPARDY segment, the NWS employees had apparently done some “sprucing up” and had painted the legs of the shelter and the MMTS mount pole a blue color to match the logo color of the NWS emblem over the office door:
Upton NY NWS office looking North - Photo: NWS
I found the above picture and the one below at the NWS Upton web page where they have a “virtual tour” of the facility. Here is another angle from the web page that shows the overall NWS complex, including the NEXRAD Doppler radar tower:
Upton NY NWS office looking Northeast - Photo: NWS
Looking at the style of the automobiles, I’m guessing these photos were taken sometime in the early 90’s when this office was opened. What is interesting about these photos, besides the siting issues with proximity to parking and the building, is the fact that the Stevenson Screen door is facing SOUTH rather than the requisite north. The idea is to keep direct sunlight from hitting the thermometers when readings are taken.
I thought perhaps this station is purely a “figurehead” used for school tours, etc, but then I thought: “Why would they want to show it being done incorrectly?”. I checked the NCDC MMS meta-database to see if the station was active. Oddly I couldn’t find the right station in Upton in the database. Poking around again at the NWS Upton website I found out why: This used to be New York City’s station. It was once on top of the RCA building as I discovered from their virtual tour:



Dec. 28, 1960 to Oct. 24, 1993
RCA/GE Building
30 Rockefeller Center NY, NY
Mezzanine  Level



Your National Weather Service office was located in midtown Manhattan on the mezzanine level of this building until October 25, 1993, when we relocated to our current site. The picture depicts the top of the building where our old radar was located (ball-shaped object). NOAA Weather Radio transmitters are also pictured and still reside atop 30 Rock.  
Once knowing it was the NYC station and not “Upton”, I was able to find it in the NCDC MMS metadatabase and determine that indeed it is an active station. Fortunately it is listed as NOT being part of the climate network, and neither USHCN or GISS uses this station.
From the lat/lon posted there (40.86667 -72.86667 ) I was able to locate the station on Google Earth:
Using the ruler tool - less than 4 meters from parking - Click for larger image
It turns out that the NWS Forecasts Office happens to be on the grounds of the Brookhaven National Laboratory, and the address is at 175 Brookhaven Ave, Upton, NY.
It seems that there is ample room in the grassy area in the rear to place a weather station, rather than putting it up front in the parking lot. A Microsoft Live maps image also shows the proximity issues up front and with the building.
Upton NY NWS office looking west - Click for live interactive image
Of course looking at this photo, it would now seem that the rear of the building might not be the best choice either with that bank of 5 a/c units back there. But it could find a site further away to the rear or perhaps cleared more trees.
Even if this station isn’t in the climate network,  it really does beg the question: why does the NWS blatantly flaunt flout their own 100 foot rule? Further, since this NWS Office is located on the grounds of the Brookhaven National Laboratory, wouldn’t you think they’d want to put their absolute best scientific foot forward?
Even is this station is only used to show school kids what a weather station looks like and how it is operated, why not do it right and show proper placement away from biases, proper door alignment on the screen, and explain why these things are important for proper measurements?
Or, maybe, these things aren’t important to the NWS at all.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ae5fb3f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"After a summer of catastrophic bushfires, the most brutal evidence of the impacts of climate change, the government has managed to move the debate towards the pros and cons of setting a long-term net zero emissions target for 2050.  While #Scottyfrommarketing copped flack for his lack of empathy or response to the summer of fires, you have to admit, he’s done an amazing job of shifting the debate away from the Coalition’s failure to reduce emissions or prepare for the fires they were warned of by 24 former fire and emergency chiefs.  As Australia’s forests (and international credibility) burned, Scott Morrison took a match to common sense and got away with it. Apparently, drilling for more gas reduces emissions, and we need to debate what happens in 2050 rather than what the government must do today.  It’s an impressive achievement. To be clear, I’m not opposed to setting targets for net zero by 2050. I’m glad that the Liberal premiers of New South Wales, South Australia and Tasmania have torched the idea that committing to zero emission reductions is “virtue signalling” or, as the Business Council of Australia said in the lead up to the last election, that a similarly ambitious target was “economy wrecking”. After a decade of nonsense from so many conservatives, it’s a relief to hear the rhetorical support for big cuts to emissions from the political parties and business groups that have done so much to stifle climate action in Australia. But we must beware the siren song of ambitious climate action in the future. As former resources minister Matt Canavan helpfully reminded us this week: “It’s exactly the same as to say: look, I’m going to lose 10kg in 10 years’ time but I’m not going to do anything about it today.” I couldn’t have said it better myself. Maybe I’m being cynical, but Senator Canavan might be on to something. There is a real risk that Gladys Berejiklian, Jennifer Westacott and other conservative voices calling for a net zero target for 2050 are simply trying to avoid making hard decisions today. But luckily there’s a simple way to test their commitment; by starting the transition today. A Chinese proverb says while the best time to plant a tree was 20 years ago, the second best time is right now. It’s true we don’t know today exactly how we will make steel or travel between continents without fossil fuel in 2050. But that uncertainty should drive us to do simple things we do know. The less oil we waste moving inefficient cars around our cities, the more oil that can be burned flying people to Europe. Similarly, the less coal we waste making electricity with highly subsidised power stations, the more coal that can be burned to make steel. Neither techno-optimism nor techno-pessimism is a substitute for the rapid roll-out of changes we know must be made. It’s not hard: Stop increasing emissions. Don’t build new coal-fired power stations that will pollute for decades. Stop the expansion of industries whose business case relies on increasing fossil fuel consumption. Don’t build new coal mines, open new gas fields, or expand logging of the only viable carbon capture and storage device known today – the tree. Start investing in things that reduce emissions now. Renewable energy and storage are a good start. Energy efficiency measures are great too. (Both measures save money, so won’t be a “cost to the economy”.) Invest in science and future technology, but don’t bet the planet on it. We’ve been told that so-called carbon capture and storage (of the non-tree variety) is just “10 years away” for the last 20 years. Likewise, cheap safe nuclear and “green” hydrogen. After 30 years of hope it’s time to start doing things that work. Now. Stop the accounting tricks. Pretending that dodgy “carryover credits” are a substitute for burning less coal will do nothing to stop climate change. Which brings me to the “net” in net zero emissions. Some emissions of greenhouse gasses are inevitable. Not only do cows burp but some people do as well. As of today, making milk, producing steel, and flying planes all generate lots of emissions. And while technological solutions are possible, it’s not inevitable that those solutions will be quick enough, or cheap enough, to reduce emissions to zero by 2050. The first response to the reality that some products will generate emissions for decades to come, is to buy less of those products. The second is to get cracking with installing the simple stuff that we already know works, in the sectors where it’s easiest to do so. But it’s the third response that puts the “net” into net zero targets. It’s possible to sequester huge amounts of carbon dioxide in trees and soil. It might even be possible one day to capture C02 from the smokestack of power stations and bury it. Maybe. But after billions of dollars and decades of government-funded research, the best carbon capture and storage device known is still the tree. Installing solar, wind and batteries is far cheaper, and the technology exists now. Because some “negative emissions” are possible, we don’t have to get all of our manufacturing or farming emissions to zero. But because there’s a limit to the amount of negative emissions we can generate, we have to do everything we can to drive down emissions in the sectors where it’s easiest, such as electricity. Then, there’s international offsets. Another way for Australia to be net zero by 2050 is to continue our pollution-as-usual path and buy huge amounts of offsets from other countries. But as Canavan rightly reminds us: “… we are being told that we need all countries to hit net zero by 2050 to save the planet. Who are we going to buy carbon emissions from if all other countries have stopped emitting too? We can’t buy carbon credits from Mars.” So, with this sage counsel ringing in our ears, those of us who take the science of climate change seriously must ensure we can walk and chew gum at the same time. Long-term targets are only as valuable as the urgent actions we attach to achieve them. And short-term action won’t drive big change unless we bring powerful new voices into the conversation. The best way to hit a net zero target would have been to stop building coal-fired power stations, coalmines and gas fields 20 years ago. But the second best way is to stop building them today, to boost our renewable energy targets today, and to introduce energy efficiency standards for buildings and vehicles today. We’ve got 30 years to solve the hard problems, but no time to avoid the easy solutions. • Richard Denniss is chief economist at the Australia Institute"
"

It seems that ever since 1776, European elites have been grousing about the symbols of American culture that are “forced” upon them. First it was the ideas of our revolution, such as liberty and democracy. Today it’s Hollywood movies, EuroDisney and — horror! — McDonald’s hamburgers. So perhaps it’s no surprise that some European Union politicians seem positively gleeful at the prospect of exporting to the United States that great staple of European life: ubiquitous taxation. 



At issue is who will collect the value‐​added tax (VAT) when a European consumer downloads a digital product or service from a U.S.-based company. The VAT — which can add as much as 25 percent to the cost of a product — is usually charged at the point of entry on “tangible” products shipped from the United States. But since products such as software and music can be delivered directly over the Internet, there aren’t any packages for EU tax collectors to inspect at the border. Thus, when French President Jacques Chirac downloads a game called, say, “Jerry Lewis’ Championship Boxing,” he’s responsible for paying the VAT himself. There’s no practical way to track Chirac’s purchase, however, so the odds of his paying the tax voluntarily are essentially zero. 



Ever alert to the scourge of untaxed commerce, EU politicians have hit upon a solution to their problem: have the Americans harvest the loot! 



That beggar‐​thy‐​neighbor approach to tax collection has been endorsed by the European Commission, which proposed on June 7 that U.S. companies be required to collect a VAT on all sales of digital products to Europe. If the proposal becomes law — which could happen later this year — all U.S. business will have to register with the tax agency of at least one EU member country, ascertain the location and tax status of each and every customer, transmit tax payments electronically to the relevant tax authority, and submit to audits and “due diligence examinations” to make sure no one is cheating. In return for acting as Europe’s tax collectors, American businesses would receive half‐​price coupons for EuroDisney (just kidding, they wouldn’t get anything). 



But as the history‐​minded among us have noted, America once fought a war to escape burdensome taxes imposed from across the Atlantic. In the days before the Internet, America’s political leaders uniformly denounced “taxation without representation” as an intolerable evil. 



Even given today’s less‐​principled political climate, it’s difficult to see what Washington has to gain by going along with Europe’s scheme. Under the current rules, European consumers have an incentive to shop VAT‐​free from U.S. companies, which also makes the United States an attractive market for e‐​commerce investors. European companies rightly complain about that disparity, but too bad: let Europe solve its own tax problems. If political leaders there weren’t reflexively opposed to tax cuts, they could simply exempt digital products and services from the VAT. 



Enforcement will be difficult without Washington’s help, but Europe’s tax collectors are determined to try. U.S. businesses with a branch office in Europe, for example, could probably be forced to comply. Another option being considered is “blacking out” the Web sites companies that refuse to register for VAT collection. But consider that despite having highly centralized systems of Internet service provision, authoritarian governments (such as China) have been unable to control access to dissident Web sites. The Internet is simply too massive and decentralized to police effectively. 



That enforcement problem is why Europe is calling for increased “international collaboration” on tax collection, meaning that the IRS would monitor U.S. companies’ compliance with EU tax law. And there’s no guarantee that Congress won’t play along: the digital‐​VAT already has allies among U.S. state and local politicians who would love to see broader taxation of the Internet. After all, if American businesses can be compelled to collect taxes on their sales to foreign consumers, surely those businesses can be told to collect taxes on sales of products destined for other states? Two wrongs, the politicians hope, will in this case make a right. 



Ordinary Americans, as they pause this summer to consider the reasons our ancestors took up arms against the British 224 years ago, aren’t going to buy it, and Congress should take heed. “Taxation without representation” remains an intolerable evil that neither time nor technology has diminished.
"
"
Share this...FacebookTwitterThe Fukushima nuclear reactor crisis has provided anti-nuclear activists with an assortment of (unsubstantiated) scare stories for turning the tide of public opinion against nuclear energy. So it’s little wonder that few places have been so gripped by hysteria as Germany, home to a large number of eco-stalinists and chronic doomsayers in the media.
German warmist website klimaretter.de (climatesaviour.de in English) writes that this however is not the case in Finland. Despite Fukushima, Finland is still very much in favour of using this plentiful source of zero-emissions energy. Klimaretter writes:
Despite the bad experiences encountered building the Olkiluoto 3 reactor– which will be finally finished 4 years later than planned and at double the cost in 2013 – the government approved the new construction of two additional nuclear reactors. And approved is approved, ways Mari Kiviniemi, head of the government.”
And why not? Nuclear power has been the safest of all sources of energy so far, as I mentioned in a previous post, renewable energy sources have many more deaths per terawatt-hour of electricity produced than nuclear. The dangers of nuclear power are mostly hype, and nothing to do with reality.

Source: http://nextbigfuture.com/2011/03/lowering-deaths-per-terawatt-hour-for.html
The Finnish government reminds us that in Finland there is practically no danger of earthquakes or tsunamis. And klimaretter reminds us:
The emergency power supply in the event of a disruption of the external power source für is already with the old reactors are designed twice as safely as the Japanese damaged reactors.
One thing that the panic purveyors seem to forget is that there are concepts in engineering and design called “applying lessons learned” and “continuous improvement”. No doubt the Japan earthquake exposed the weaknesses of the older Japanese reactors and safety systems. ´But in the end, this will be a blessing in that the lessons learned can now be implemented into the design of the new generation reactors, thus making the safest source of energy out there even safer.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Silly, desperate argument: “What if a plane crashes into it!”
This is a silly argument because planes could crash into anything, and so then nothing ought to be built. What if a plane crashed into a hydro-electric dam? A chemical plant? A football stadium?
Like anything else, all we can do is reduce the risk. And the best way to reduce risk in energy supply is to use the system that has proven to be the safest. That choice is obvious.
Thanks to reader DirkH for this video of a test plane colliding into a conrete wall at 500 mph. Risk dispelled!

Share this...FacebookTwitter "
"

Of course many of you that live in this weather already know this, but there is an early start to winter this year, not only in the USA, but also in London, where it snowed in October for the first time in over 70 years.
So far, no mention of this broadly distributed U.S. record event in the mainstream media. There are a few individual mentions or record lows in Florida. See this Google News search.
Here, from NOAA’s  National Climatic Data Center (NCDC), is a list of these new or tied records for October 29th, 2008.
I find the -25 below in Alaska interesting, since it bested the old record by 4 degrees.
Here are the 115 new or tied low temperature records:
The table below has been formatting to fit the blog, Here is a direct link to the original data from NCDC



29 October 2008
Record
New (83)
Tied (32)
Previous
Record
Previous
Year
Period
of
Record


CIRCLE HOT SPRINGS, AK
-25.0°F
-21.0°F
2001
44


TONSINA, AK
-17.0°F
-16.0°F
1985
42


CAMP HILL 2 NW, AL
21.0°F
25.0°F
1968
76


HAMILTON 3 S, AL
23.0°F
24.0°F
1968
45


CENTREVILLE 6 SW, AL
26.0°F
28.0°F
2001
32


MUSCLE SHOALS AP, AL (KMSL)
27.0°F
28.0°F
1952
67


GREENVILLE, AL
28.0°F
29.0°F
2001
78


GENEVA #2, AL
29.0°F
29.0°F
2001
32


HIGHLAND HOME, AL
29.0°F
30.0°F
1976
112


HUNTSVILLE INTL AP, AL (KHSV)
30.0°F
30.0°F
2005
50


MONTGOMERY AP ASOS, AL (KMGM)
31.0°F
32.0°F
2001
60


ATMORE, AL
32.0°F
33.0°F
2001
48


MOBILE RGNL AP, AL (KMOB)
32.0°F
36.0°F
1987
60


FAIRHOPE 2 NE, AL
33.0°F
34.0°F
1952
89


CODEN, AL
34.0°F
35.0°F
1957
43


DAUPHIN IS #2, AL
47.0°F
48.0°F
2001
32


BOONEVILLE 3 SSE, AR
28.0°F
29.0°F
1993
30


MURFREESBORO 1 W, AR
29.0°F
29.0°F
1993
33


SPARKMAN, AR
29.0°F
29.0°F
2005
40


FORDYCE, AR
30.0°F
30.0°F
1993
71


ROHWER 2 NNE, AR
31.0°F
32.0°F
1997
47


WEST MEMPHIS, AR
31.0°F
33.0°F
1976
45


BLYTHEVILLE, AR
32.0°F
32.0°F
1939
79


EUDORA, AR
32.0°F
32.0°F
1997
45


PERRY, FL
29.0°F
32.0°F
1987
71


TALLAHASSEE WSO AP, FL (KTLH)
29.0°F
31.0°F
1987
63


GLEN ST MARY 1 W, FL
29.0°F
32.0°F
1957
80


MAYO, FL
30.0°F
30.0°F
1957
57


NICEVILLE, FL
31.0°F
33.0°F
2001
62


JACKSONVILLE INTL AP, FL (KJAX)
33.0°F
39.0°F
1987
60


APALACHICOLA AP, FL (KAAF)
34.0°F
41.0°F
1976
76


PENSACOLA RGNL AP, FL (KPNS)
36.0°F
38.0°F
1968
60


TAMPA WSCMO AP, FL (KTPA)
42.0°F
45.0°F
1963
75


ORLANDO INTL AP, FL (KMCO)
43.0°F
49.0°F
1952
54


DAYTONA BEACH INTL AP, FL (KDAB)
44.0°F
46.0°F
1957
60


KISSIMMEE 2, FL
44.0°F
45.0°F
1968
46


VERO BEACH INTL AP, FL (KVRB)
46.0°F
48.0°F
1943
57


FT MYERS PAGE FLD AP, FL (KFMY)
47.0°F
47.0°F
1910
109


WEST PALM BCH INTL AP, FL (KPBI)
49.0°F
51.0°F
1944
69


MIAMI INTL AP, FL (KMIA)
55.0°F
61.0°F
1968
60


FT LAUDERDALE INTL AP, FL (KFLL)
55.0°F
62.0°F
2006
35


KEY WEST INTL AP, FL (KEYW)
61.0°F
66.0°F
1957
56


NAHUNTA 6 NE, GA
28.0°F
30.0°F
1957
45


PLAINS SW GA EXP STN, GA
30.0°F
30.0°F
2001
52


BLAKELY, GA
31.0°F
34.0°F
1976
95


ALBANY CAA AP, GA
31.0°F
35.0°F
1952
33


BRUNSWICK, GA
39.0°F
40.0°F
1957
90


CASSODAY, KS
24.0°F
24.0°F
1993
46


IOLA 1 W, KS
26.0°F
26.0°F
1980
48


HOMER 3 SSW, LA
27.0°F
33.0°F
2001
55


BASTROP, LA
29.0°F
31.0°F
2005
78


ASHLAND, LA
30.0°F
32.0°F
2005
54


MONROE ULM, LA
30.0°F
32.0°F
2005
31


ALEXANDRIA AP, LA (KESF)
31.0°F
31.0°F
2005
56


MANSFIELD, LA
33.0°F
34.0°F
2005
32


JONESVILLE LOCKS, LA
33.0°F
39.0°F
2005
36


SLIDELL, LA
34.0°F
35.0°F
1957
52


BUNKIE, LA
34.0°F
34.0°F
1957
50


RED RVR RSCH STN, LA
34.0°F
35.0°F
2001
31


RESERVE, LA
35.0°F
35.0°F
1913
101


BOYCE 3 WNW, LA
39.0°F
41.0°F
2001
31


GALENA, MO
22.0°F
25.0°F
1963
43


MT VERNON M U SW CTR, MO
22.0°F
25.0°F
1980
48


BUFFALO 2 N, MO
22.0°F
23.0°F
1980
44


WASOLA, MO
25.0°F
26.0°F
1952
61


HICKORY FLAT, MS
26.0°F
27.0°F
2001
51


OAKLEY EXP STN, MS
27.0°F
28.0°F
2001
37


WINONA 5 E, MS
28.0°F
28.0°F
2001
54


GRENADA 5 NNE, MS
28.0°F
29.0°F
1957
53


MCCOMB AP, MS (KMCB)
31.0°F
34.0°F
1957
60


WIGGINS, MS
32.0°F
34.0°F
1957
52


ROLLING FORK, MS
32.0°F
35.0°F
2005
35


PASCAGOULA 3 NE, MS
33.0°F
33.0°F
1987
71


YAZOO CITY 5 NNE, MS
33.0°F
33.0°F
1963
46


GRANDFATHER MTN, NC
17.0°F
17.0°F
1968
52


SUPERIOR 4E, NE
20.0°F
21.0°F
1991
53


TUSKAHOMA, OK
24.0°F
31.0°F
1973
46


MARIETTA 5SW, OK
25.0°F
26.0°F
1952
67


LINDSAY 2 W, OK
27.0°F
31.0°F
1993
43


KEYSTONE DAM, OK
28.0°F
29.0°F
1980
41


PERRY, OK
28.0°F
28.0°F
1980
89


BROKEN BOW DAM, OK
32.0°F
32.0°F
1973
34


SANDHILL RSCH ELGIN, SC
30.0°F
30.0°F
1976
50


DICKSON, TN
23.0°F
23.0°F
1952
106


AMES PLANTATION, TN
28.0°F
29.0°F
2001
31


JOHNSON CITY, TX
28.0°F
34.0°F
1970
41


GILMER 4 WNW, TX
28.0°F
30.0°F
1952
72


MT VERNON, TX
28.0°F
35.0°F
1973
42


SMITHVILLE, TX
28.0°F
34.0°F
1957
81


WARREN 2 S, TX
29.0°F
33.0°F
1957
32


WEATHERFORD, TX
29.0°F
29.0°F
1913
103


EMORY, TX
29.0°F
35.0°F
1995
42


GREENVILLE KGVL RADIO, TX
30.0°F
30.0°F
1952
103


MADISONVILLE, TX
30.0°F
31.0°F
1955
61


CENTERVILLE, TX
30.0°F
33.0°F
1970
65


KERRVILLE 3 NNE, TX
31.0°F
36.0°F
2006
34


CENTER, TX
31.0°F
31.0°F
1952
65


FOWLERTON, TX
32.0°F
32.0°F
1970
52


HILLSBORO, TX
32.0°F
32.0°F
1913
97


HENDERSON, TX
32.0°F
36.0°F
1973
67


AUSTIN BERGSTROM INTL, TX (KAUS)
33.0°F
37.0°F
1970
35


CLEVELAND, TX
33.0°F
35.0°F
1965
44


HONDO MUNI AP, TX (KHDO)
34.0°F
40.0°F
1993
37


GRAPEVINE DAM, TX
35.0°F
35.0°F
1910
66


LONGVIEW 11 SE, TX
35.0°F
38.0°F
1993
33


LA GRANGE, TX
36.0°F
38.0°F
2005
46


TOWN BLUFF DAM, TX
36.0°F
37.0°F
2001
37


JACKSONVILLE, TX
36.0°F
36.0°F
1970
44


VICTORIA ASOS, TX (KVCT)
37.0°F
40.0°F
1980
53


STILLHOUSE HOLLOW DAM, TX
37.0°F
38.0°F
1970
40


EL CAMPO, TX
38.0°F
39.0°F
1970
36


MATAGORDA 2, TX
40.0°F
40.0°F
1952
78


ARANSAS WR, TX
40.0°F
46.0°F
1980
35


POINT COMFORT, TX
42.0°F
43.0°F
2007
48


RAYMONDVILLE, TX
45.0°F
45.0°F
1970
92




Here are 163 new or tied lowest high temperature records for October 29th, 2008
Here is a direct link to NOAA’s NCDC data for these records:



29 October 2008
Record
New (120)
Tied (48)
Previous
Record
Previous
Year
Period
of
Record


BRIDGEPORT 5 NW, AL
49.0
55.0
2001
44


SAND MT SUBSTN, AL
50.0
50.0
1952
59


MOULTON 2, AL
51.0
53.0
1973
49


TALLADEGA, AL
52.0
55.0
1973
107


CLANTON, AL
52.0
53.0
1910
110


SYLACAUGA 4 NE, AL
52.0
56.0
1997
46


BELLE MINA 2 N, AL
52.0
53.0
1952
57


VERNON, AL
54.0
55.0
1973
49


HAMILTON 3 S, AL
54.0
58.0
1968
45


GREENVILLE, AL
55.0
59.0
2001
78


JASPER, AL
55.0
55.0
1976
45


EVERGREEN, AL
55.0
57.0
1910
83


THORSBY EXP STN, AL
55.0
57.0
1997
50


BREWTON 3 SSE, AL
57.0
60.0
1958
79


CODEN, AL
59.0
59.0
1997
44


MARSHALL, AR
52.0
52.0
1969
54


FT BRAGG 5 N, CA
53.0
53.0
1953
72


FERNANDINA BEACH, FL
64.0
64.0
2001
109


ST PETERSBURG, FL (KSPG)
64.0
64.0
1952
96


GAINESVILLE RGNL AP, FL (KGNV)
64.0
64.0
2007
45


ST AUGUSTINE LH, FL
66.0
69.0
1987
34


KEY WEST INTL AP, FL (KEYW)
71.0
74.0
1987
56


FT LAUDERDALE INTL AP, FL (KFLL)
76.0
78.0
1989
35


ALPHARETTA 4 SSW, GA
49.0
53.0
1959
41


GAINESVILLE, GA
49.0
49.0
1910
103


ALLATOONA DAM 2, GA
50.0
53.0
1953
43


DALLAS 7 NE, GA
51.0
55.0
1976
50


ELBERTON 2 N, GA
51.0
51.0
1910
68


HARTWELL, GA
51.0
53.0
2001
94


TOCCOA, GA
51.0
51.0
1910
105


SILOAM 3 N, GA
56.0
56.0
2003
46


MAUNA LOA SLOPE OBS 39, HI
48.0
48.0
1976
49


NORMAL 4NE, IL
45.0
45.0
1988
31


PERU, IL
46.0
46.0
1988
45


COLUMBIA CITY, IN
39.0
41.0
1968
44


PORTLAND 1 SW, IN
41.0
43.0
1976
30


BLUFFTON 1 N, IN
42.0
44.0
1980
36


NEW CASTLE 4 SSE, IN
42.0
42.0
1968
58


BAXTER, KY
44.0
49.0
1968
56


WEST LIBERTY 3NW, KY
45.0
46.0
1973
56


MT VERNON, KY
45.0
48.0
1980
49


JAMESTOWN WWTP, KY
47.0
48.0
1976
31


MONTICELLO 3 NE, KY
47.0
47.0
1980
52


PAINTSVILLE 1 E, KY
47.0
51.0
2003
30


BRADFORDSVILLE, KY
48.0
48.0
1968
44


BARBOURVILLE, KY
48.0
50.0
1953
54


FROSTBURG 2, MD
37.0
39.0
1976
36


SAVAGE RVR DAM, MD
39.0
41.0
1976
56


EMMITSBURG 2 SE, MD
48.0
48.0
1965
50


CUMBERLAND 2, MD
50.0
50.0
2002
32


IONIA 2 SSW, MI
39.0
42.0
1988
69


LAPEER WWTP, MI
40.0
41.0
2006
56


GROSSE POINTE FARMS, MI
44.0
44.0
2006
57


SHELBINA, MO
48.0
48.0
1980
62


WELDON SPRING NWS, MO
50.0
50.0
1976
42


PORTAGEVILLE, MO
50.0
50.0
1976
41


RIPLEY, MS
50.0
54.0
1968
66


INDEPENDENCE 1 W, MS
51.0
52.0
1976
50


IUKA, MS
51.0
57.0
1997
30


PONTOTOC EXP STN, MS
51.0
54.0
1968
55


HICKORY FLAT, MS
52.0
52.0
1980
51


WINONA 5 E, MS
52.0
54.0
1997
54


HOLLY SPRINGS 4 N, MS
52.0
54.0
1976
46


EUPORA 2 E, MS
53.0
55.0
1976
76


GRENADA 5 NNE, MS
53.0
56.0
1997
53


CALHOUN CITY, MS
53.0
59.0
1980
52


BELZONI, MS
55.0
57.0
1976
76


NORTH WILKESBORO, NC
48.0
52.0
1976
53


YADKINVILLE 6 E, NC
48.0
51.0
2003
50


STATESVILLE 2 NNE, NC
50.0
52.0
2003
101


ALBEMARLE, NC
53.0
55.0
2003
96


CLAYTON WTP, NC
55.0
55.0
2001
47


LEWISTON, NC
55.0
56.0
2005
52


ELIZABETHTOWN 3 SW, NC
56.0
60.0
2005
47


CAPE HATTERAS MITCHELL, NC (KHSE)
56.0
56.0
1976
51


FLEMINGTON 5 NNW, NJ
42.0
45.0
1976
110


NEW BRUNSWICK 3 SE, NJ
43.0
44.0
1976
40


DELHI 2 SE, NY
33.0
35.0
1952
75


BINGHAMTON WSO AP, NY (KBGM)
33.0
33.0
1952
60


WARSAW 6 SW, NY
35.0
35.0
1965
53


BAINBRIDGE 2 E, NY
35.0
39.0
1939
56


NORWICH, NY
36.0
37.0
1925
99


WATERTOWN AP, NY (KART)
37.0
39.0
1962
59


ELMIRA, NY
38.0
38.0
1928
112


PORT JERVIS, NY
40.0
40.0
1952
113


YORKTOWN HTS 1 W, NY
40.0
43.0
1976
43


WEST POINT, NY
42.0
42.0
1952
108


CADIZ, OH
39.0
41.0
1910
102


COSHOCTON AG RSCH STN, OH
40.0
42.0
1980
51


STEUBENVILLE, OH
40.0
41.0
1952
66


NEWARK WTR WKS, OH
42.0
42.0
1952
73


HANNIBAL L&D, OH
42.0
43.0
1976
33


NAPOLEON, OH
42.0
46.0
1980
39


NEW LEXINGTON 2 NW, OH
43.0
43.0
1952
66


WASHINGTON COURT HOUSE, OH
44.0
45.0
1968
81


BRADFORD RGNL AP, PA (KBFD)
31.0
35.0
2002
51


PLEASANT MT 1 W, PA
33.0
35.0
1959
55


DUBOIS FAA AP, PA (KDUJ)
34.0
38.0
1968
41


FRANCIS E WALTER DAM, PA
35.0
39.0
1976
41


WELLSBORO 4 SW, PA
36.0
37.0
1980
74


HAWLEY 1 E, PA
36.0
44.0
1997
82


CHALK HILL 2 ENE, PA
37.0
43.0
1990
31


MATAMORAS, PA
37.0
45.0
1965
42


TOWANDA 1 S, PA
38.0
39.0
1925
114


CONFLUENCE 1 SW DAM, PA
39.0
40.0
1957
62


TIONESTA 2 SE LAKE, PA
40.0
40.0
2001
65


WAYNESBURG 1 E, PA
41.0
44.0
1976
47


STEVENSON DAM, PA
42.0
43.0
2001
39


HAMBURG, PA
43.0
43.0
1907
67


WEST CHESTER 2 NW, PA
44.0
44.0
1976
103


LEWISTOWN, PA
46.0
47.0
1997
66


LONG CREEK, SC
49.0
52.0
1952
54


CHESTER 1 NW, SC
51.0
52.0
1959
76


PICKENS, SC
52.0
54.0
1952
57


SUMTER, SC
54.0
58.0
2001
81


CALHOUN FALLS, SC
54.0
55.0
1925
90


MANNING, SC
56.0
58.0
2001
35


BAMBERG, SC
56.0
57.0
1959
56


ANDREWS, SC
58.0
58.0
2001
37


ALLARDT, TN
43.0
44.0
1968
78


MONTEAGLE, TN
44.0
45.0
1952
68


TAZEWELL, TN
46.0
50.0
1976
42


LIVINGSTON RADIO WLIV, TN
48.0
50.0
1973
43


NEAPOLIS EXP STN, TN
49.0
52.0
1976
31


PORTLAND SEWAGE PLT, TN
50.0
51.0
1976
52


COVINGTON 3 SW, TN
50.0
51.0
1976
109


LINDEN WTP, TN
50.0
53.0
1976
45


SMITHVILLE 2 SE, TN
51.0
54.0
1976
36


SELMER, TN
51.0
54.0
1976
50


PULASKI WWTP, TN
51.0
57.0
2001
50


LEXINGTON, TN
51.0
51.0
1968
41


RIPLEY, TN
51.0
53.0
2002
43


MARTIN U OF T BRANCH E, TN
52.0
52.0
1976
72


CHEATHAM L&D, TN
52.0
54.0
1976
35


BROWNSVILLE, TN
52.0
52.0
1973
101


ATHENS, TN
52.0
52.0
1976
46


WYTHEVILLE 1 S, VA
39.0
41.0
1893
86


ABINGDON 3 S, VA
40.0
52.0
2006
36


BLACKSBURG NWSO, VA
40.0
46.0
1976
54


PULASKI 2 E, VA
40.0
43.0
1968
53


SALTVILLE 1N, VA
40.0
50.0
1968
49


GRUNDY, VA
42.0
47.0
1968
44


STAFFORDSVILLE 3 ENE, VA
42.0
48.0
2001
37


LURAY 5 E, VA
46.0
46.0
1976
66


STERLING RCS, VA
50.0
51.0
2002
31


WEST ALLIS, WI
43.0
44.0
1954
46


SNOWSHOE, WV
24.0
29.0
2005
31


TERRA ALTA #1, WV
31.0
40.0
1967
43


BELINGTON, WV
35.0
41.0
1976
41


ROWLESBURG 1, WV
36.0
40.0
1976
66


SUMMERSVILLE LAKE, WV
37.0
43.0
1976
41


BUCKEYE, WV
37.0
42.0
1968
46


FAIRMONT, WV
39.0
43.0
1952
102


ELKINS RANDOLPH CY AP, WV (KEKN)
39.0
39.0
1952
82


WESTON, WV
39.0
39.0
1925
106


CLARKSBURG 1, WV
39.0
44.0
1934
83


UPPER TRACT, WV
39.0
39.0
1910
38


OAK HILL, WV
40.0
45.0
1976
67


MORGANTOWN L&D, WV
40.0
42.0
1980
62


WEST UNION 2, WV
41.0
45.0
1976
35


MIDDLEBOURNE 3 ESE, WV
41.0
48.0
1980
66


GASSAWAY, WV
41.0
47.0
1952
54


PINEVILLE, WV
42.0
48.0
1976
62


GRANTSVILLE 1 ESE, WV
42.0
48.0
1976
43


BLUESTONE LAKE, WV
42.0
46.0
1976
65


DUNLOW 1 SW, WV
44.0
47.0
1997
36


RIPLEY, WV
44.0
44.0
1988
61


PARKERSBURG, WV
44.0
44.0
1952
82




Here are the 63 snowfall records:
Direct link to NOAA’s NCDC data for snowfall records
HTML clipboard



29 October 2008
Record
New (63)
Tied (0)
Previous
Record
Previous
Year
Period
of
Record


ASHFIELD, MA
1.5 in
0.0 in
2007
30


EAST BRIMFIELD LAKE, MA
0.1 in
0.0 in
2007
46


MC HENRY 2 NW, MD
9.0 in
2.0 in
2006
37


FROSTBURG 2, MD
3.4 in
0.7 in
2006
36


SANDUSKY, MI
0.5 in
Trace
1925
99


MAPLE CITY 1E, MI
0.3 in
Trace
1993
49


MARSHALL, NC
1.0 in
0.2 in
1910
109


GRANDFATHER MTN, NC
0.5 in
Trace
1973
53


MT WASHINGTON, NH (KMWN)
10.1 in
9.5 in
2000
60


POTTERSVILLE 2 NNW, NJ
2.0 in
0.0 in
2007
40


NEW BRUNSWICK 3 SE, NJ
1.5 in
0.0 in
2007
40


FLEMINGTON 5 NNW, NJ
1.0 in
0.8 in
1965
110


HOOKER 12 NNW, NY
19.0 in
3.5 in
1968
97


STILLWATER RSVR, NY
13.0 in
2.0 in
1990
83


TUPPER LAKE SUNMOUNT, NY
13.0 in
2.0 in
1934
109


LOWVILLE, NY
9.0 in
3.0 in
1893
116


PISECO, NY
8.0 in
1.0 in
2006
65


HIGHMARKET, NY
5.2 in
3.0 in
1965
84


NEWCOMB, NY
4.8 in
1.0 in
1965
49


CANTON 4 SE, NY
4.5 in
1.5 in
1962
115


INDIAN LAKE 2SW, NY
3.0 in
1.5 in
2006
109


ROCK HILL 3 SW, NY
2.3 in
0.0 in
2007
45


FRIENDSHIP 7 SW, NY
2.0 in
1.3 in
2006
39


LOCKE 2 W, NY
2.0 in
0.0 in
2007
76


BINGHAMTON WSO AP, NY (KBGM)
0.6 in
0.4 in
1952
60


JAMESTOWN 4 ENE, NY
0.5 in
0.0 in
2007
48


YOUNGSTOWN WSO AP, OH (KYNG)
1.6 in
0.6 in
1952
74


CLEVELAND WSFO AP, OH (KCLE)
0.3 in
Trace
2003
60


RIDGWAY, PA
6.0 in
Trace
1987
115


MEYERSDALE 2 SSW, PA
3.0 in
Trace
2006
45


DUNLO, PA
3.0 in
0.5 in
2006
60


SOMERSET, PA
2.8 in
1.4 in
2006
59


MAHANOY CITY 2 N, PA
2.1 in
0.0 in
2007
36


EBENSBURG SEWAGE PLT, PA
2.0 in
1.0 in
1965
44


KANE 1NNE, PA
2.0 in
1.0 in
1965
114


CONFLUENCE 1 SW DAM, PA
2.0 in
Trace
1965
62


MERCER, PA
2.0 in
Trace
1990
58


GLEN HAZEL 2 NE DAM, PA
2.0 in
1.5 in
2006
66


CHALK HILL 2 ENE, PA
1.2 in
Trace
1987
31


BOSWELL, PA
1.0 in
Trace
1965
48


PORT ALLEGANY, PA
1.0 in
0.5 in
2006
60


TIONESTA 2 SE LAKE, PA
0.8 in
0.5 in
1965
87


SLIPPERY ROCK 1 SSW, PA
0.7 in
Trace
2006
59


FRANCIS E WALTER DAM, PA
0.7 in
Trace
1990
45


PITTSBURGH WSCOM 2 AP, PA (KPIT)
0.6 in
0.4 in
1952
63


BUFFALO MILLS, PA
0.3 in
Trace
1965
84


MATAMORAS, PA
0.3 in
0.0 in
2007
104


MT MANSFIELD, VT
12.0 in
4.0 in
2006
53


ROCHESTER, VT
2.5 in
1.0 in
2000
79


MORRISVILLE 4 SSW, VT
1.4 in
Trace
2007
46


ESSEX JUNCTION 1 N, VT
1.2 in
Trace
2000
36


NEWPORT, VT
1.2 in
1.1 in
2000
78


ST ALBANS RADIO, VT
1.0 in
0.3 in
1992
30


CORINTH, VT
1.0 in
0.0 in
2007
60


SNOWSHOE, WV
8.0 in
1.0 in
1995
33


BAYARD, WV
5.5 in
1.5 in
1952
106


TERRA ALTA #1, WV
5.0 in
1.5 in
2006
60


GLADY 1 N, WV
4.4 in
Trace
2005
35


VALLEY HEAD, WV
3.2 in
2.0 in
1952
70


BELINGTON, WV
1.6 in
Trace
1968
70


BARTOW 1 S, WV
0.5 in
0.1 in
2006
64


ROCK CAVE 2 NE, WV
0.5 in
0.0 in
2007
55


SUTTON LAKE, WV
0.1 in
0.0 in
2007
91





			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9bbd0a86',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"**Back to the future. Next week England will return to the tier system, where the country is carved up into different categories.**
Areas where there are higher levels of coronavirus will be under tighter restrictions. Parts of the country where it is less prevalent will have looser limits.
It is the same model that didn't do enough to slow the winter surge of the disease last time. But second time round, it's different in some important ways.
First off, the strictest regime, tier three, will overall be tougher than the last time round.
Swathes of the country are still going to stuck under a pretty restrictive system. (You can read all about exactly how the tiers will work here.)
Second, gyms and businesses that carry out personal care like hairdressers will stay open everywhere. Stricter yes, but way short of a rerun of the spring lockdown.
Third, the decision on which set of instructions a region will have to follow will be taken by the government and its scientists centrally.
For all of the warm words about the importance of local leaders, and local authorities, there is no desire in central government to repeat the very messy political process of trying to get regional buy-in when decisions are being made about moving in and out of the different categories.
That might be a more straightforward way of making the choices. But it could make it harder to get local support for measures being taken.
Although it's worth noting the chancellor has already changed his mind about extending national financial support which was such a bone of contention last time.
And in theory, areas could move more quickly in and out of the different tiers too - with possible switches every fortnight.
First time round the government struggled to communicate how the system would work cleanly, but this is going to be the way things are run until March so ministers want to get it right.
But the political test of how bumpy it could be won't come until Thursday when ministers reveal the answer to the question every MP and member of the public wants to know - what will the rules be in the place they live.
The Tory backbenches and the opposition this time are more demanding of the government in terms of explaining its decision-making.
But the extent of the potential push back will depend to a large extent on just how much of the country is asked to keep coping largely behind closed doors.
Downing Street has repeatedly made it clear that more of England will be in the toughest tier than in the first phase of this system, but won't be drawn yet beyond that.
The prime minister's wi-fi might have held up through his press conference on Monday, unlike the House of Commons earlier (although the PM protested the problems were on the Commons' end) but it's not clear yet how his arguments for the revised tier system will hold up too.
And don't forget the relative success of the new system depends in large part on all of us - how the public responds to the new system of rules and regulations, and how patient we are all willing to be.
P.s. Remember all four parts of the UK are following different approaches right now. Northern Ireland is about to tighten up further. Wales has not long left its circuit break lockdown, and Scotland has a new system of five tiers.
That patchwork is partly why it is 'possible, but not definite', according to government insiders tonight that the four parts of the country will agree how to manage Christmas on Tuesday.
The four administrations have been trying to figure out a way of allowing a bit more flexibility around families spending time with each other at Christmas.
But there are considerations around how long it should be, how to manage transport, how many households it should include, even what really constitutes a household.
There doesn't seem any doubt that an agreement will be reached. It seems (for once perhaps?) the delay is because of the complexities not a conflict.
Conversations continue, and there should be a deal of some sort before too long."
"

The busted forecast for last week’s East Coast snowstorm points to a very troubling aspect of modern life: We now believe the output of computers more than we trust our own eyeballs. 



At noon on January 25, the most sophisticated weather‐​forecasting model in human history predicted a total snowfall for Washington, D.C., of somewhat less than an inch in the succeeding 36 hours. All the human forecasters I know went along. 



The computer model, named for the Greek letter eta (pronounced “ay‐​ta”), which describes its mathematical coordinate system, forecast a storm far out to sea, sparing the I-95 megalopolis that stretches from Richmond to Boston. Instead, eta confined its significant snow to a sliver of southeastern Virginia, before shoving everything north, east and offshore. 



Fear in the forecasting community was palpable as minute‐​by‐​minute updates from our national radar network painted an army of green and yellow monsters marching north and west toward our nation’s capital. By 9 p.m. they were closer than most of the Confederates ever got, and still the forecast was for a minor dustup. It wasn’t until eta, whose major update cycle is 12 hours, was run again, that forecasters decided disaster was at hand. 



Why didn’t we believe our eyes when the model was clearly busting? The truth is, as models become more sophisticated, forecasters are increasingly reluctant to abandon them, even in the face of contrary evidence. But, “the computer eta my forecast” is an insufficient excuse. 



Would that this were the case merely for the 48‐​hour forecast. Unfortunately, it appears that the same pathology has infected the 48‐​year projection. Just like weather forecast models, our climate simulations have become increasingly sophisticated in recent years. And just like the situation with the recent snow, there has been incontrovertible and advancing evidence, this time over the course of the last two decades, that climate simulations are making a disastrous error, and it has taken forever for forecasters to acknowledge it. 



Every computer model predicts that the entire troposphere, or roughly the bottom 40,000 feet of the atmosphere, should be warming rapidly. Most models even predict that much of the warming accelerates with height. 



But, for over 20 years now, we have two independent measures of temperature — satellites and weather balloons — that show no net warming at all from 5,000 feet skyward. In 1996, we had 17 years of satellite data, and yet the “Policymakers Summary” (the only part that gets read) of the U. N. Intergovernmental Panel on Climate Change report contained not one mention of the word “satellite.” That is the report usually cited as the “consensus of scientists.” 



If that was the 1996 consensus of my profession, it was largely a consensus of ostriches, not scientists. How long could we continue to sweep under the computer the fact that somehow we got 88 percent of the atmosphere wrong? 



Not very long. Here is what the National Research Council now says about our ability to model climatic behavior: “It is clear,” they recently wrote, “that reconciling the discrepancy … is not simply a matter of deciding which [climate model] is correct.… In the long term it will require major advances in our ability to interpret and model [atmospheric behavior].” In other words, our models don’t work. Here, for once, eyeballs have triumphed over the computer. 



Most people use weather forecasts to plan the future. The Kyoto Protocol on climate change was a response to a climate forecast. This onerous document would require the United States to drastically reduce the energy use that has propelled us on our gee-doesn’t-your-401 k -look‐​great way for the last two decades. Everyone (except, maybe, President Clinton, judging from his State of the Union Address) believes it will cost us a fortune. 



When it finally became clear that Washington was going to get buried in the last snowstorm, the federal government shut itself down. Now that we have admitted that our climate forecasts for the next century aren’t worth much at all, can we please shut down the Kyoto Protocol on climate change?
"
"
Share this...FacebookTwitterWe were told that the mild winters we experienced in Europe were due to global warming. Now, suddenly, we are getting hit with yet another nasty cold winter.
Why? Guest writer Juraj Vanovcan presents his observations and interesting evidence that it has nothing to do with CO2. He presents what I think is an astonishing finding near the end.
===============================================================
Predicting The European Climate From The CET Record – Lesson Learned
By guest writer Juraj Vanovcan
This post was inspired by the article Negative AO bringing cold winters back to Europe.
Recalling the summers and winters of the early 1980s, it becomes obvious to me that it is the prevalence of air circulation that determines if a season is warm or cold. The very mild winter of 2006/2007 in Central Europe was characterized by a sustained flow of warm Atlantic air over the European continent, while the cold and snowy 2005/2006 winter received a lot of Arctic air entering the mid-latitudes from the North.
Air circulation is governed by pressure differences and basically this is what North Atlantic Oscillation is all about.
 
Fig 1 Example of positive NAO (Source: JISAO webpage).
I compared the NAO index with the European long-term climate record. Checking the Central European Record (CET) shown, one sees there is an obvious correlation between NAO and winter temperatures. The dark blue line is CET, orange/light blue is NAO.

Fig 2 North Atlantic Oscillation index compared with CET winter record, 1860-2010 (CET graph source: http://climate4you.com/ ).
As observed above, the NAO oscillates in an 80-year long sine wave cycle. The first period with mild winters happened in the 1920s, which of course we do not remember. The second positive phase began in 70s and mild winters in Central Europe become frequent since late 80s. It also seems that the current period with prevalent NAO-positive years has ended; the recent string of cold winters in North-Western Europe suggests this as well.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




On the other hand, CET summers do not show correlation with NAO in the early part of 20th century. Comparing the CET summer anomalies with AMO index (detrended North Atlantic SST) however gives reasonable correlation again. Summers in the period 1930-1950 were often as warm as the recent ones, and the extremely hot and dry summer of 1946 in Central Europe with its catastrophic impact on crops has since never been repeated. However, warm summers in this period were combined with cold winters, like those of 1939/40/41.
Fig 3 Atlantic Multidecadal oscillation index compared with CET summer record, 1860-2010
It is worth noting that while the NAO peaked circa 20 years before the warm AMO phase centered on 1940, their warm phases were much closer to each the other in the later part of the century. It means that while Europe experienced cold summers and mild winters in 1920s and warm summers and cold winters in the 1940s, the last 20 years saw both warm summers as mild winters. This is also probably the reason why 2000-2010 decade is slightly warmer than 1940-1950.
Based on the observed SST and OHC record for North Atlantic, it seems however that AMO had peaked around 2005 and it is now heading down. This time, both NAO and AMO being in their negative phases will mean miserable summers and cold winters. Such shift in temperature trend is already being observed in the whole northern extratropics record.

Fig 4 Northern hemisphere north of 30N, HadCRUT3 data
IPCC attributed the post-1975 warming phase almost solely to anthropogenic reasons, namely to increased “greenhouse effect” caused by increase of CO2 molecules from 3.5 to 4 per other 10,000 molecules in the atmosphere.
We can conclude, however, that at least for Europe, observed warming is fully explainable by natural variations, two of which – AMO and NAO – had their positive phases overlaid during the last 20 years. Neither CO2, aerosols nor greenhouse effect theory are needed.
For those still seeking the anthropogenic signature in recent warming, here is a comparison of 1890-1920 warming trend compared against 1980-2010 warming trend in the winter CET record. The running 10-year mean is strikingly similar, following even minor dips and upticks.
Fig 5 CET winter record with late century warming superimposed on early century warming 
Extrapolations into the future may be tricky, especially when pulling some 20-year trend into year 2100, which seems to be a favorite practice in modern climatology. Observing the European climate record of the early part of 20th century and understanding its causes gives us much more predictive skill when forecasting climate for the next decades. All climate indicators today point to cooling, and not only in the European region.
Juraj Vanovcan (juraj.vanovcan@gmail.com)
26 November 2010
Share this...FacebookTwitter "
"

Either global warming is the greatest crisis ever to confront humankind, or it is a lefty plot completely manufactured by scientists and politicians in pursuit of research funding and control over our lives. That’s about the way it plays out in the media, on blogs and in conversations on the Metro. Anyone out front on this issue is either an apocalyptic or a denier, virtuous or vile.



Similarly, one camp maintains that temperatures are rising dramatically with unspeakable portents, while the other thinks what has happened is entirely a result of undefined internal oscillations in the earth‐​sun climate system, and that there is virtually no human component to climate change. This group is especially fond of the lack of statistically significant surface warming since 1995. Since 1997, temperatures really flatlined.



There’s a third way, which suffers from the problem that it is subtle, neither black nor white, and doesn’t do well in sound bites. It’s a “lukewarm” synthesis, arguing indeed that humans have something to do with the rise in surface temperature measured since the mid‐​1970s, but that it is hardly the end of the world as we know it. This view claims to accommodate the seemingly odd behavior of temperature in the last 15 years.



Each of these positions — let’s call them hothead, flatline, and lukewarm — are testable against observed history and theory. To keep some interest in this occasionally boring topic, I’m going to examine them sequentially, starting with the hotheads.



The hothead argument is that we have already set the planet on the road to climate calamity, and that we must promptly reduce the atmospheric concentration of dreaded carbon dioxide‐ the main global warming emission — to levels seen decades ago.



Before we started torching carbon stored in forests and then carbon stored underground as coal and oil, the carbon dioxide concentration of our atmosphere was about 280 parts per million (ppm). It’s now around 390, and headed for a nominal doubling to 600ppm between 2070 and 2090 if the world continues its current rate of development and does not find an effective (meaning neither solar nor wind) and politically acceptable (meaning not nuclear, at least for now) alternative for hydrocarbon fuels.



The high priest of the hotheads is NASA’s James Hansen, who preaches that, unless we dial back to 350ppm, we will lose, within a hundred years’ time, the vast majority of Greenland’s ice, which will raise sea levels about 20 feet. Hansen has testified that he thinks this could happen within a hundred years.



The hothead theory is that the ice on that gigantic island is much less stable than previously thought, and that with a tad more warming, lakes will form in the summer, drain thousands of feet down to the bedrock, and lubricate the flow to the ocean. It quickly melts, submerging a lot of Florida and Manhattan. The Washington Monument becomes an island.



The reason that glaciers flow to begin with is because the bottom is liquid. It’s quite unclear that simply adding more water will have much effect. Recent studies indicate that when the lakes drain suddenly into the ice, the acceleration of flow is not sustained. But that’s today; what about in the future?



One way to project the future with confidence is to look to history, when it was warmer. Danish colonists established a series of weather stations on the Greenland coast, with reliable records that go back over 225 years. They unequivocally show that — from 1920 through 1960 — there was substantially more warming than has been observed in recent decades. If hothead theory is correct, there should have been a detectable jump in sea level during that period, but there was none.



Further, there is very strong evidence that the integrated warming — that’s temperature times time — was much greater for _millennia_ after the end of the recent ice age around 10,800 years ago. Assuming that humans will find something better to power the world with than carbon dioxide‐​emitting fossil fuels in the next one or two hundred years, that total warming back then was greater or equal to what we are likely toinflict on Greenland.



In those millennia — which are only the blink of a geologist’s eye ago — trees used to grow where there is now only barren tundra. When they died, they were preserved in the acidic bogginess, so we can tell exactly when they were alive with carbon dating. It’s very clear that the forest in Eurasia used to extend all the way to the Arctic Ocean during that warm period.



Plant ecologists know that the northern limit of the forest is determined by the mean July temperature. The dead trees tell us it was as much as 13 degrees F warmer than the 20th century average.



The author of that work, Glen MacDonald of UCLA, has noted that the only way to get that region so warm is with a massive influx of Gulf Stream water from the Atlantic. The only “gate” for that is the channel between Greenland and Scandinavia, which means that Greenland (at least the eastern half) would have been pretty balmy compared to today.



And Greenland still retained the lion’s share of its ice cap.



Even so, the Arctic Ocean was likely to have been largely ice‐​free during the summer during much this time — from 6,000 to 8,000 years ago — as noted by theUniversity of Stockholm’s Martin Jacobsson in a 2010 edition of the scientific journal _Quaternary Science Reviews_. The Geological Survey of Norway foundsomething similarin 2008. Not only did Greenland’s ice survive — so did the polar bear.



So it appears that the ice that the hotheads skate on is pretty thin. Are the flatliners doing any better? We’ll have a look in Part 2.
"
"
Share this...FacebookTwitterH/T EIKE
The folly of windfarms and solar panels knows no boundaries. For example nowhere today in the north German area where I live is it possible to drive or take my bike 10 minutes anywhere without seeing a cluster of white behemoths chopping through the landscape (and birds).
Photo source: www.greenpeace.org. Here Greenpeace is major proponent of such let’s-target-and-desecrate-the-landscape projects.
Just days ago, Germany’s leading, renown political daily the Frankfurter Allgemeine Zeitung (FAZ) had a piece on the dark side of alternative energy sources, writing:
This program for ‘rescuing the climate’ is reckless, technocratic and ugly.

The FAZ adds,
Wind and solar parks will change the landscape with no consideration of protecting nature. Soon seeing natural landscape  will be possible only by looking in story books.
In fact, disfiguring the landscape is taking on such proportions and has gotten so out of control that some of the windmills’ former proponents, real environmentalists, are now beginning to sober up and are painfully realising the disaster they’ve helped to create.
The FAZ reports that even the President of the Federal Environmental Department, Jochen Flasbarth, is having second thoughts. He warns that an attempt to supply Germany with 100% renewable energy by 2050 will lead to a:
…huge aggravation of its citzens. The unavoidable consequence of a decentralized supply of renewable energy is that it will lead to an alteration of the landscape in many regions.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Face it, windparks need lots of space and are eyesores. It takes 1000 large 5-MW turbines to replace a single 1000 megawatt coal fired power plant. And because the turbines rarely work at 100% capacity (20% is typical) you need 5 times as many! And when the wind stops blowing, you need a conventional power plant to kick in anyway (200% capacity to ensure 100% supply).
And because land is not in plentiful supply, many projects are slated for offshore. The FAZ writes:
Most people don’t have any idea as to how big the dimensions of these windparks are, and even less an ideas of the ecological damage they cause to marine wildlife during their set-up and threat they pose to shipping vessels later. Germany’s Green party and environmental groups are silent on this topic.
The sprouting of offshore windmills is becoming such a threat that even the WWF warns of the Wild Wild West of the Baltic Sea. Sweden’s The Local writes on wind energy development until 2030:
Wind energy resources are forecast to undergo significant expansion over the period, from a current 400 megawatts to 25,000 megawatts which correspond to a 60-fold increase by 2030. Much of the expansion in wind energy will occur at sea.
Ironically environmentalists always protest loudly when it comes to building a new street, fearing for the life of field mice and worms. Yet they don’t seem to care about the destruction of natural scenery that hundreds of windparks cause and the thousands of migratory birds they kill as they navigate through their beloved machanized killing fields.
Well, how can that be? It gets down to green money. According to the FAZ:
Greenpeace earns money in replacing highly efficient, space-saving coal and nuclear power with extremely expensive, vast space-consuming, ugly and dangerous-to-wildlife, mechanized eyesores that happened to have the ecological seal of approval.
I ask myself: How much longer will it be before the real environmentalists and citizens wake up to this utter folly?
Share this...FacebookTwitter "
"
Part I: Ranking global warming among present-day risks to public health.

Guest essay by Indur M. Goklany
There seems to be no limit to the hyperbole surrounding climate change – and that’s no hyperbole. Numerous politicians have informed us over the years that climate change is one of the most important problems facing mankind.  In fact, U.N. Secretary General Ban Ki-moon has called it the defining challenge of our age.”
But is it?
I answer this question in a paper just published in the refereed section of Energy & Environment.
A 2005 review article in Nature on the health impacts of climate change estimated that 166,000 deaths were “attributable” to climate change in 2000. This estimate was derived from a World Health Organization (WHO) sponsored study that even the study’s authors acknowledge may not “accord with the canons of empirical science” (see here). But I will accept this flawed estimate as gospel for the sake of argument.
In the year 2000, however, there were a total of 56 million deaths worldwide. Thus, climate change may be responsible for less than 0.3% of all deaths globally (based on data for the year 2000). This places climate change no higher than 13th among mortality risk factors related to food, nutrition and environment, as shown in the following table.
Specifically, climate change is easily outranked by threats such as hunger, malnutrition and other nutrition-related problems, lack of access to safe water and sanitation, indoor air pollution, malaria, urban air pollution. And had I included other risks to public health beyond environmental, food and nutritional factors (e.g., HIV/AIDS, TB, various cancers, etc.) then climate change would have ranked even lower than 13th.
With respect to biodiversity and ecosystems, today the greatest threat is what it always has been – the conversion of land and water habitat to human uses, i.e., agriculture, forestry, and human habitation and infrastructure. See, e.g., here.
Climate change, contrary to claims, is clearly not the most important environmental, let alone public health, problem facing the world today.
But is it possible that in the foreseeable future, the impact of climate change on public health could outweigh that of other factors?
I will address this question in subsequent blogs.



Risk factor

Ranking


Mortality   (millions)


Mortality   (%)



Blood   pressure
1
7.1
12.8


Cholesterol
2
4.4
7.9


Underweight   (hunger)
3
3.7
6.7


Low fruit   & vegetables
4
2.7
4.9


Overweight
5
2.6
4.6


Unsafe water,   poor sanitation
6
1.7
3.1


Indoor smoke
7
1.6
2.9


Malaria

1.1
2.0


Iron   deficiency
8
0.8
1.5


Urban air   pollution
9
0.8
1.4


Zinc   deficiency
10
0.8
1.4


Vitamin A   deficiency
11
0.8
1.4


Lead exposure
12
0.2
0.4


Climate   change
13
0.2
0.3


Subtotal
27.6
49.4


TOTAL from all causes
55.8
100.0



Priority ranking of food, nutritional and environmental problems, based on global mortality for 2000. Source: I.M. Goklany, Is Climate Change the “Defining Challenge of Our Age”? Energy & Environment 20(3): 279-302 (2009), based on data from the World Health Organization. Note that malaria isn’t ranked in this table because deaths due to malaria were attributed by WHO to climate change, underweight, and zinc and vitamin A deficiencies.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96d06e75',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The world’s largest financier of fossil fuels has warned clients that the climate crisis threatens the survival of humanity and that the planet is on an unsustainable trajectory, according to a leaked document. The JP Morgan report on the economic risks of human-caused global heating said climate policy had to change or else the world faced irreversible consequences.  The study implicitly condemns the US bank’s own investment strategy and highlights growing concerns among major Wall Street institutions about the financial and reputational risks of continued funding of carbon-intensive industries, such as oil and gas. JP Morgan has provided $75bn (£61bn) in financial services to the companies most aggressively expanding in sectors such as fracking and Arctic oil and gas exploration since the Paris agreement, according to analysis compiled for the Guardian last year. Its report was obtained by Rupert Read, an Extinction Rebellion spokesperson and philosophy academic at the University of East Anglia, and has been seen by the Guardian. The research by JP Morgan economists David Mackie and Jessica Murray says the climate crisis will impact the world economy, human health, water stress, migration and the survival of other species on Earth. “We cannot rule out catastrophic outcomes where human life as we know it is threatened,” notes the paper, which is dated 14 January. Drawing on extensive academic literature and forecasts by the International Monetary Fund and the UN Intergovernmental Panel on Climate Change (IPCC), the paper notes that global heating is on course to hit 3.5C above pre-industrial levels by the end of the century. It says most estimates of the likely economic and health costs are far too small because they fail to account for the loss of wealth, the discount rate and the possibility of increased natural disasters. The authors say policymakers need to change direction because a business-as-usual climate policy “would likely push the earth to a place that we haven’t seen for many millions of years”, with outcomes that might be impossible to reverse. “Although precise predictions are not possible, it is clear that the Earth is on an unsustainable trajectory. Something will have to change at some point if the human race is going to survive.” The investment bank says climate change “reflects a global market failure in the sense that producers and consumers of CO2 emissions do not pay for the climate damage that results.” To reverse this, it highlights the need for a global carbon tax but cautions that it is “not going to happen anytime soon” because of concerns about jobs and competitiveness. The authors say it is “likely the [climate] situation will continue to deteriorate, possibly more so than in any of the IPCC’s scenarios”. Without naming any organisation, the authors say changes are occurring at the micro level, involving shifts in behaviour by individuals, companies and investors, but this is unlikely to be enough without the involvement of the fiscal and financial authorities. Last year, analysis compiled for the Guardian by Rainforest Action Network, a US-based environmental organisation, found JP Morgan was one of 33 powerful financial institutions to have provided an estimated total of $1.9tn (£1.47tn) to the fossil fuel sector between 2016 and 2018. A JP Morgan spokesperson told the BBC the research team was “wholly independent from the company as a whole, and not a commentary on it”, but declined to comment further. The metadata on the pdf of the report obtained by Read said the document was created on 13 January and that the author of the file was Gabriel de Kock, an executive director at JP Morgan. The Guardian has approached the investment bank for comment. Pressure from student strikers, activist shareholders and divestment campaigners has prompted several major institutions to claim they will make the climate more of a priority. The business model of fossil fuel companies is also weakening as wind and solar become more competitive. Earlier this month, the influential merchant bank Goldman Sachs downgraded ExxonMobil from a “neutral” to a “sell” position. In January, BlackRock – the world’s biggest asset manager – said it would lower its exposure to fossil fuels ahead of a “significant reallocation of capital”. Environmental groups remain wary because huge sums are invested in petrochemical firms, but some veteran financial analysts say the tide is changing. The CNBC money pundit Jim Cramer shocked many in his field when he declared: “I’m done with fossil fuels. They’re done. They’re just done.” Describing how a new generation of pension fund managers was withdrawing support, he claimed oil and gas firms were in the death knell phase. “The world has turned on them. It’s actually happening kind of quickly. You’re seeing divestiture by a lot of different funds. It’s going to be a parade that says, ‘Look, these are tobacco. And we’re not going to own them,’” he said. “We’re in a new world.”"
"It can be easy to lose hope in the age of what some call the planet’s sixth mass extinction. It is clear that there are no easy solutions to ongoing biodiversity loss, and while there are some inspiring local successes, we have so far failed to change the trajectory of environmental destruction that has historically accompanied economic expansion. Meanwhile, the main underlying driver of biodiversity loss, our consumption of natural resources, is only projected to increase. But we are optimistic that things can improve. Though biodiversity in Europe is still declining, three major trends give us hope for our home continent.  People are increasingly recognising how their lifestyle choices impact the environment, and this is slowly driving a cultural shift towards sustainability. Among those aged 18-35 across the world, climate change and the destruction of nature is already considered the most serious global issue and, as this generation becomes increasingly influential, that mindset might cascade into multiple biodiversity successes. It is already affecting consumption choices, with “green” products and lower-impact diets on the rise to the benefit of both biodiversity and human health. This rising focus on sustainability is also delivering notable policy successes. One example is last year’s European parliament vote to ban many single-use plastics, following widespread concern about their impacts on wildlife.  Perhaps most importantly, questions are increasingly being asked about how we can deliver societal prosperity on a finite planet, hence the widespread acclaim of alternative narratives that aren’t based on growth, such as Doughnut Economics. There are even small signs that these alternatives might be growing in policy influence, such as members of the European parliament organising last year’s Postgrowth Conference in Brussels. Meat consumption is a major threat to biodiversity globally, and in Europe a sixth of all land is devoted to pasture. One exciting driver of the reduced meat consumption outlined above is the emergence of ambitious meat substitutes which could both significantly reduce the amount of land needed for farming, and reduce other pressures on biodiversity such as pollution.  A key strength of meat substitutes is that they don’t rely on people sacrificing the “experience” of meat. This means they can work alongside demand reduction efforts to limit agricultural consumption.  A study by Impossible Foods in the US found that someone replacing half of their beef consumption with one of the company’s plant-based alternatives could lead to a 12% reduction in their total agricultural footprint. On top of that, the company claims its meatless burger scored as highly as actual beef in its own blind taste tests.  As culture changes, and as tasty meat substitutes becomes more popular, demand for land-intensive red meat might start to dwindle. This in turn could free up areas of land for nature restoration and rewilding.  Around 50m hectares of land has has been abandoned across Europe in the past 40 years. That’s an area the size of Spain. Alongside improved wildlife legislation, this has stimulated the extraordinary comeback of bears, wolves and lynx across the continent.  But abandoning farms and leaving them to nature does have its downsides. Abandonment is a leading driver of the loss of certain components of biodiversity (such as some open-habitat-loving farmland birds) as well as cultures associated with extensive agriculture and traditional farming practices, and so these will of course have to continue to be managed into the future.  There is also a worry that using agricultural land more efficiently could simply cause people to consume more, and thus use more farmland anyway – a so-called rebound effect.  Nevertheless, the benefits of restoring nature to lands freed from farming are potentially vast – it’s also one of the most important pathways through which Europe can meet its international climate change and biodiversity commitments. And then there are enterprises such as the “rewilded” Knepp Estate in West Sussex, England, which are showing how landscape-scale restoration can potentially create economic opportunities through tourism and low impact harvesting. At a time when agricultural subsidies are increasingly uncertain, they offer a compelling alternative to conventional farming. All-in-all, there is little doubt that biodiversity is still under extreme threat in Europe and globally. However, we remain optimistic that emerging cultural changes, sustainable technological innovations, and an increasing recognition of the benefits of a wilder countryside might leave European biodiversity better off in future."
"

Al Gore’s histrionics are amusing, but nothing he has said compares to Sheryl Crow’s proposal to restrict how much toilet paper can be used. Perhaps there can be a new monitoring bureaucracy to search our homes. Maybe government agencies can stand guard in public restrooms. The BBC reports on the latest in cutting‐​edge environmentalism: 



Singer Sheryl Crow has said a ban on using too much toilet paper should be introduced to help the environment. … The 45-year-old…has just toured the   
US on a biodiesel‐​powered bus to raise awareness about climate change. …The pair targeted 11 university campuses to persuade students to help combat the world’s environmental problems. … “I have spent the better part of this tour trying to come up with easy ways for us all to become a part of the solution to global warming,” Crow wrote. …“I propose a limitation be put on how many squares of toilet paper can be used in any one sitting.”



Crow’s publicists managed to get the BBC to reference her biodiesel bus, but her environmental bona fides do not stand up under closer scrutiny. Thesmok​ing​gun​.com exposes the demands she makes when going on tour: 



The rock star’s performance contract includes specific day‐​to‐​day instructions on what kind of booze Sheryl needs in her dressing room (TSG has never seen such attention to detail in any other concert rider we’ve posted). … promoters are directed to purchase specific booze depending on what day of the week the concert falls, as the below rider excerpt reveals. Additionally, when the global warming warrior hits the road, her touring entourage (and equipment) travels in three tractor trailers, four buses, and six cars. Now that’s a carbon footprint!
"
"

A new piece of scientific research hit the presses last week. It reported finding more warming in one of the (several) satellite-observed temperature histories of the earth’s lower atmosphere than had been previously reported. As these satellite-measured temperatures were the recent subject of comments made by presidential candidate Ted Cruz, a lot of scrutiny and interest surrounds these new findings—findings which seemed to refute some of Cruz’s assertions.   
  
In researching his story on the new study, the Associated Press’s Seth Borenstein solicited my opinion about them and how they may alter climate change skeptics’ way of thinking about the satellite-observed temperatures—temperature datasets which had previously shown precious little warming over the past nearly two decades.   
  
I was happy to offer my thoughts, and equally happy to see some of them reflected in Seth’s AP story. Given topical and length constraints, understandably, Seth had to be selective.   
  
But I do have a bit more to say about the new research finding besides that it “shows ‘how messy the procedures are in putting the satellite data together.’”   
  
Many of my additional thoughts were included in my broader email response to Seth’s initial inquiry and, with his permission, I am reproducing our correspondence below.   
  
To Seth’s summary of my thoughts, I’d add “but even considering the new findings, the complete collection of satellite- and weather balloon-observed temperature histories of the earth’s atmosphere indicate that climate models are projecting too much warming in this important region.”   
  
Again, my thanks to Seth for reaching out to me in the first place. Here is out question and answer exchange:



Chip,   
  
Seeing that the climate doubter community has hinged so much on RSS and saying there has been no warming post 1997 _ despite NOAA heat records in 1998, 2005, 2010, 2014 and 2015 _ you’ve seen the RSS update that shows there has been warming in the last 18 years. I’m wondering what your thoughts are on it. Will you and those in your community keep using RSS, even if it shows no warming. Add to that the UAH record warming in February. Are satellites now contradicting the climate doubter community?   
  
Thanks,   
Seth



 ****   




Seth,   
  
Thanks for soliciting my opinion.   
  
I can't speak for the climate doubter community, however that is defined.   
  
Personally, my doubts are not that human-caused climate change as a result of greenhouse gas emissions is not occurring and that a temperature rise as a result is not detectable in large spatial averages, but I have doubts that the change is taking place at the rate projected by the collection of climate models and that its effects are currently detectable on most smaller scale climate/weather metrics.   
  
So with that out of the way, I’ll give some opinions as to the new RSS results and their importance to my way of thinking…   
  
First off, as I have tweeted (https://twitter.com/PCKnappenberger/status/705515578325270529), the overall 1979-2014 trend in the RSS v4 MT data is still pretty far beneath the climate model expectations…far enough to continue to indicate a sizable discrepancy that needs further scientific attention.   
  
Second, the trend in the new RSS v4 MT now makes it the mid-tropospheric (MT) dataset (including other satellite based and weather-balloon based) that has the greatest trend over the 1979-2014 period (see the same tweet mentioned above, as well as this one, https://twitter.com/PCKnappenberger/status/705472903458914305 which shows the old and new RSS data in comparison to weather-balloon compilations).   
  
Given these two things, I don’t think it helps settle any questions regarding the temperature behavior of the mid-troposphere.   
  
But what it does do is shed more light on just how messy the procedures are in putting the satellite data together. Decisions, guided by science but not specifically defined by it, occur at many points in the procedure. The new RSS paper, again highlights how sensitive the final results are to those decisions. It is good that we have many different groups involved in assembling both the satellite history and the weather-balloon history. That these different groups provide answers that are pretty close to each other helps not to lower the uncertainty in any single result, but that the general result is not indicative as to what is going on in the MT. The new RSS v4 now lies outside the old envelop of these collective findings. It’ll either prove to move the science in a bit of a different direction, or prove to be an erroneous result. Time will tell.   
  
As to the impact on the “pause,” IMO there was too much being made about the “pause"" in the first place. No serious student of climate science thought that it would last forever. The important thing about it was that it provided a challenge to climate science and prompted enhanced research into natural climate variability, climate sensitivity, and other important aspects of climate science. So that it’s now over comes as no surprise. But, once the El Nino warming subsides, I think we’ll probably see a continuation of the modest (below model mean) rate of warming.   
  
I hope this is useful. If you have any further questions, I’d be more than happy to try to answer them.   
  
-Chip



In addition to Seth’s story for the AP, more reactions about the new satellite-study can be found at Watts Up With That, Climate Etc., and at Roy Spencer’s blog, among others.


"
"Senior Morrison government ministers are publicly at odds about whether Australia will take a long-term emissions reduction target to global climate talks in November after Labor unveiled a target of net zero emissions by 2050. On Friday the finance minister Mathias Cormann confirmed the government “will be finalising a longer-term target in time for Cop26” but the emissions reduction minister would commit only to “a long-term strategy” despite repeatedly being asked about a new target.  As revealed by Guardian Australia, Anthony Albanese used a speech to a progressive thinktank on Friday to commit the ALP to adopting a net zero target by 2050 if it wins the next federal election, without the use of carryover credits from the Kyoto period. Scott Morrison is holding off from making a commitment to carbon neutrality by 2050, partly because of an internal brawl within the Coalition and partly because the prime minister says Australia should not sign up to targets in the absence of costings. Some in the government have noted publicly in recent weeks that Australia implicitly accepted the net zero pathway when the Coalition signed and ratified the Paris agreement, and Liberal moderates are now pushing to make net zero an explicit target beyond the 26-28% emissions reduction promised by 2030. Asked about the existing Paris commitment on Friday, Cormann told Sky News “we will be finalising our longer-term emissions reduction target in time for Cop26 in Glasgow later this year”, using the word “target” at least three times in his formulation of future government policy. “But what we will do as part of our process is to ensure that the agenda we determine to achieve any such target is environmentally effective and economically responsible,” he said. “And we will ensure that we can look the Australian people in the eye and say ‘this is what we’re aiming to achieve, this is what it will mean for electricity prices, this is what it will mean for jobs’.” Cormann accused Labor of committing to targets without setting out the cost, warning that Albanese is “making the same mistake” as former leader Bill Shorten. Later on Friday, Angus Taylor told Sky News the Coalition had a “very clear target for 2030” and a “very clear view that will drive emissions [reductions] beyond 2030”. Taylor repeatedly avoided the question about whether a technology roadmap to reduce emissions would result in a new longer-term emissions reduction target. “What we’ve said is we’ll have a long-term strategy and we’re not going to have any target that is uncosted, unfunded [and] unplanned,” he said. “We’re not going down that path.” “We’ve made very clear that we’re focused on a long-term strategy and technology will be the focus of it … not taxes.” “We are working towards a long-term strategy, a technology investment roadmap, we have a target for 2030, we’re going to meet and beat that target.” Despite the Coalition criticism, business rode to Labor’s defence. Australian Industry Group chief executive Innes Willox said the net zero target “is increasingly widely supported by Australian businesses, industry advocates such as Ai Group, the wider community and governments of all complexions”. “That growing consensus is important to guide and discipline the development of efficient, trade neutral and fair policies to get there,” he said. “We shouldn’t underestimate the challenge of net zero, which goes well beyond generating cleaner electricity. “Nor should we get too hung up on economic projections, which are about as reliable as trying in 1990 to estimate the cost and value of smartphones in 2020.” Every state and territory has expressed at least an aspirational objective of achieving net zero emissions by 2050, and Australia has been urged by the UK and its Pacific neighbours to sign up to that target. Albanese noted on Friday that the Business Council of Australia is calling for it as well as major corporates including AGL, Santos, BHP, Amcor, BP, Wesfarmers and Telstra. “Seventy-three countries, including the UK, Canada, France and Germany, many with conservative governments, have already adopted it as their goal,” he said. “Australia should too.” Earlier, Labor’s climate change spokesman, Mark Butler, told Radio National the opposition would set out a detailed policy about how to achieve targets and its cost “well before” the next election. Butler argued that the cost of reducing emissions should not be divorced from the cost of inaction and noted Melbourne University research had found actions to reduce emissions have a benefit cost ratio of 20 to one."
"Firms and governments must increasingly internalise the possibility – indeed, I would argue, the overwhelming probability – of an acceleration of four secular developments that influence what business and political leaders do and how they do it. Decision-makers should think of these trends as waves, which, especially if they occur simultaneously, could feel like a tsunami for those who fail to adapt their thinking and practices in a timely manner. The first and most important trend is climate change, which has evolved from a relatively distant concern, on which there is ample time to take remedial action, to an imminent and increasingly urgent threat. The mobilisation of various concerned segments of society, owing partly to unusual climatic disruptions in recent years, has greatly increased the pressure on companies to act now. BP’s recent announcement that it intends to achieve “net zero” carbon emissions by 2050 – a notable promise by an energy company that operates in several highly challenging settings – is the latest example of business responding to such calls. It is only a matter of time until this pressure also prompts governments to take further steps, not only to encourage green activities but also to tax and regulate those that cause pollution. Second, privacy concerns have grown alongside technical innovations involving artificial intelligence and big data. Society is increasingly recognising that recent technological advances allow not only for more efficient compilation of huge amounts of personal data but also for using this information to monitor and alter behaviours. Broadly speaking, data is controlled and exploited either by governments (particularly in China), big tech companies (as in the US), or more by users (as in Europe). But none of these three general operating paradigms seems to provide sufficient comfort and assurance to most people. The third secular force involves disruptions to the multi-decade process of economic and financial globalisation. The initial trigger for this was the trade-policy pivot by the US president Donald Trump’s administration – from cooperative conflict resolution to explicit confrontation, from multilateralism to bilateralism (or even unilateralism) and from rule-based to more ad hoc arrangements – aimed at creating a still-free but fairer trading system. But de-globalisation has been turbocharged by the outbreak of the deadly Covid-19 virus, which has disrupted the flow of goods and services in China and beyond. These challenges to globalisation have opened the door for governments to weaponise economic tools to meet objectives that transcend economics, such as national security. This, in turn, is calling into question conventional wisdom about cross-border supply chains, just-in-time inventory management and reliance on external demand to boost domestic growth. The final trend is demographic and concerns more than the ageing of societies in Europe and Asia and this trend’s economic and political implications. It also goes beyond the growing realisation that millennials’ starkly different expectations – regarding professional careers, personal engagement, political action and the delivery of goods and services – will persist and deepen. For starters, businesses need to be smarter about “anywhere, any place, any time” delivery. Furthermore, job loyalty and tenure are decreasing, while expectations of comprehensive job fulfilment and engagement are rising. Self-mobilisation for political and other causes, often with no visible leadership structure, has become a lot easier, yet often is less durable and raises tricky questions about what comes afterward. And all of this is taking place amid the continued migration of an ever-expanding range of interactions from physical to virtual spaces. Each of these secular forces will have an important impact on the effectiveness and success of companies and governments alike. And while being challenging overall, the four trends involve a diverse and geographically dispersed set of winners and losers. Executives and policymakers, therefore, must make timely revisions (including pre-emptive changes) not only to their business models and operational approaches but also to both their tactical and strategic mindsets. Getting this right will require cognitive diversity, openness to constructive criticism, repeated scenario analyses and multidisciplinary approaches. Moreover, because each of the secular forces involves a considerable degree of uncertainty (with lots of known unknowns, and probably more than a few unknown unknowns behind them), a combination of resilience, optionality and agility also is important. And this is even before one considers unanticipated periodic shocks such as the coronavirus outbreak. The challenges to sound decision-making and leadership in business and government are not limited to mapping each of the four secular forces and the required adaptation. Decision-makers also must consider correlations and causalities between these trends that can make their total impact multiplicative rather than merely additive. As a quick illustration, consider another aspect of demographic change: migration and the humanitarian challenges that often come with it. Climate change confronts countries with the possibility of waves of migratory human flows that they will find hard to accept and inhumane to refuse. The combination of de-globalisation and the misuse of AI and big data to infringe individual privacy is similarly troubling. This could lead to questionable behaviour by some governments and encourage malicious non-state actors to disrupt societies and economies. The world is in a period of accelerating change, the leading edge of which is the ever-growing list of developments that have gone from impossible to inevitable. Many (though by no means all) of the challenges facing business and political leaders may be broken down into four secular changes that can help anchor the timely formulation of required responses at the local, national, regional and global levels. The faster that companies and governments recognise this, the likelier they will be to alter the balance of benefits, costs and risks in their favour. • Mohamed El-Erian is chief economic adviser at Allianz. He served as chair of President Barack Obama’s Global Development Council and is a former deputy director at the IMF © Project Syndicate"
"
Share this...FacebookTwitterThat’s what one of Germany’s leading national dailies, DIE WELT,  writes here at it’s online site.Who knows! Maybe parts of the German media are beginning to see the the big block-letter writing on the wall and are now slowly taking baby-steps towards acknowledging the claims and science behind catastrophic global warming are not all what they are cracked up to be.
Maybe the far-fetched, cockamamie explanations on why all the cold is caused by warming has led the less zealous among the media to reconsider the science. Maybe all the phony predictions that keep turning out to be wrong are finally raising suspicions.
DIE WELT writes (emphasis added):
Forecasts made by many climate scientists, shortly before the year ended, prophesizing that 2010 would be the hottest on record have – once again – proven to be false.
According to CRU data, 2010 was in third place behind 1998 and 2005, and almost exactly tied with the year 2003. Certainly: The last decade was the warmest since records started being kept 130 years ago, but during the decade the warming – at least for the time being – stopped.”
 Colder winters are forecast for the future
The 4th cold winter in a row is also causing some people to wake up from their global warming trance and prompting them to ask questions. DIE WELT writes:
We are now experiencing the 4th cold winter in a row.”
And adds:
The climate scientists who are most loudly warning of global warming now say Central Europe must expect colder winters.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The reason for this, writes DIE WELT, is because of a change in the NAO, which in the 1980s and 1990s supplied Europe with mild winters with winds from the Atlantic keeping the temperature on the mild side. DIE WELT also writes that Europeans ought to get used to long cold winters because that’s pretty much what is now forecast for the years ahead.
Global warming causing cold winters – just another theory
And Die Welt seems a bit annoyed by all the changing science, and so explains that the colder winters caused by global warming, all triggered by a lack of sea ice in the Arctic, is just a theory and reminds us that:
During the especially mild winter of 2006/2007 a completely different story was told: Namely that climate change would be more noticeable in milder winter months and less so as hot summer periods and that this one particular mild season would be typical for winters in the future.”
and then adds later in their report:
The very climate scientists who are warning of global warming now say Central Europe must expect colder winters.”
Thanks, DIE WELT, for reminding readers of that. PIK; GISS and Hadley scientists would like to have everyone forget all those now embarrassing computer-model-based forecasts. It’s encouraging to see journalists start wondering about the new theories that keep popping up when things turn out differently.
Spring 2011 forecast to be cold
Finally, DIE WELT tells us that this spring in Central Europe is not going to be warm either, quoting meteorologists. Unfortunately it has been a long brutal winter for much of Europe. And with the end of February upon us, many Europeans are really getting itchy for spring and some real warming. Unfortunately, it’s going to take (quite) awhile longer, so reports DIE WELT.
But all that has changed, and this winter looks like it is about to get a long extension. Although temperatures are forecast to get above freezing by the weekend, meteorologists are forecasting that March and April will remain on the cool side.
How much cold is it going to take to make the rest of us doubt the bogus warming?
Share this...FacebookTwitter "
"The international gas market was once constrained by pipelines and long-term trade deals. Now, it is rapidly globalising. Cross-border trade and investment in gas is growing, and national gas markets are now more connected than ever. This is largely a good thing, but for some countries these changes expose their consumers to the highs and lows of global markets. Take the UK. Colleagues and I recently completed a research project on the country’s global gas challenge. As with everything from oil and wheat to laptops and mobile phones, the UK now sources most of its natural gas abroad. Import dependency – the ratio of gas imports to domestic consumption – hit an all-time high of over 50% in 2013.  This imported gas reaches the UK through pipelines from Norway’s offshore gas fields and the European gas market, and through specially designed ships carrying liquefied natural gas (LNG). The international natural gas trade – and the UK’s gas supply chain – are undergoing a profound globalisation in which LNG’s much greater geographical flexibility versus pipeline gas is playing an important role. Liquefying natural gas makes it cheaper to transport and store. The gas, which is mainly methane, is cooled to below its boiling point of -162˚C so that its volume is reduced 600-fold. A tank of LNG stores roughly as much energy as the same amount of crude oil, making it commercially possible to move gas by either road or ship beyond the limits of the pipeline network.  Though technology to turn natural gas into liquid is not new, major investments have driven a doubling of ocean-borne LNG trade so that it now accounts for a third of all internationally-traded gas. No longer constrained by pipelines, LNG is generating a more geographically complex and globally interconnected gas market. The UK’s growing import dependency has drawn it into this globalising gas trade. The country’s three active terminals – South Hook and Dragon in south Wales, and the Isle of Grain near London – are capable of importing more than two-thirds of annual gas consumption. Building these terminals has opened the UK up to gas imported from far and wide. Physical infrastructure is one thing; whether gas actually shows up is quite another. The volume of liquefied gas arriving in the UK has been a lot less than physical capacity would suggest and highly variable over time.  UK imports of LNG: Contracts concluded between LNG sellers and buyers in the UK allow cargo to be diverted to take advantage of regional differences in gas prices. Differences in price – and the strategies adopted by LNG producers in placing their gas – are important determinants of how much LNG flows to the UK and when.  LNG imports peaked at a third of UK gas consumption in 2011 as the country took delivery of liquefied gas originally intended for the US but displaced by growing shale gas production. Imports quickly fell away, however, in the second half of that year as LNG was diverted to Japan in the wake of the tsunami and the post-Fukushima nuclear shutdown. Things changed when the large price differential between UK/European and Asian gas markets sharply narrowed, in part because of falling oil prices throughout 2014. In the past six months or so, LNG cargoes have begun to arrive in the UK more frequently. The capacity to import more gas makes the UK more resilient. Yet it also introduces new uncertainties and vulnerabilities.  The waxing and waning of liquefied gas imports indicates how the UK functions as a reserve market for global LNG: its physical infrastructure and market liquidity can absorb substantial cargoes, but much of this gas will go elsewhere when more attractive opportunities are available.  The geographical flexibility of LNG has diversified the UK’s supply options but, at the same time, it has also created new dependencies, with more than 90% of the UK’s LNG imports coming from Qatar. A substantial proportion of the UK’s current LNG supply depends, therefore, on the whim of Qatar Petroleum.  “Gas security” requires understanding the UK’s changing position in a globalising gas market: increasingly that means looking beyond Russia and Ukraine to consider the implications of new developments in global LNG."
"

Global warming is increasing Western wildfires! 



At least that’s what lots of news stories said in response to a July 6 Sciencexpress paper by A.L. Westerling of the Scripps Institute of Oceanography and three co‐​authors. 



Like most scientific issues, though, this one is more complicated than the headlines suggest. 



Westerling examined wildfire data between 1970 and 2003 and found that major fires have occurred four times as frequently, on average, since 1986 than they did from 1979 through 1985. But he had one key conclusion: “Whether the changes observed in western hydro‐​climate and wildfire are the result of greenhouse gas‐​induced global warming or only an unusual natural fluctuation, is presently unclear.” 



Why so unclear? In large part, because the science isn’t straightforward, and three decades is a very short period of climate time. 



Snowmelt and temperature are thought to be the driving factors for Western wildfires. The years with high wildfire frequency tend to be those in which snow begins melting earlier than normal, which the authors of the paper said was “not surprising.” But examination of Westerling’s own data shows no significant difference in snowmelt timing between the periods of 1970 to 1985 and 1986 to 2003. 



However, spring and summer regional temperatures have gone up slightly — about 1 degree Celsius — over the same period, which serves to increase evaporation and, thus, flammability. 



Changes of this sort are certainly not unprecedented. Rather than limiting the perspective to 34 years, why not look at the last 1,200? Two years ago, Columbia University scientist Edward Cook and several colleagues reconstructed the West’s drought history back to 800 A.D. They wrote that “compared to earlier megadroughts that are reconstructed to have occurred around A.D. 936, 1034, 1150, and 1253 … the current drought does not stand out as an extreme event, because it has not yet lasted nearly as long.” 



In fact, Cook’s study shows a general decline in Western drought over the last millennium, with the recent era looking pretty much like the long‐​term average. In other words, the West is naturally accustomed to more drought than it has experienced since it was colonized by immigrants. It is also worth noting that the Western population boom began in the early 20th century, the wettest era of the last 1,200 years. 



What are perhaps more interesting are the changes in overall moisture that have accompanied the warming of the U.S. in 20th century. 



“Drought” is a combination of lack of rain and increasing evaporation. The latter is obviously dependent upon temperature, as more surface moisture evaporates into a warmer atmosphere. 



But as the country warmed, precipitation also went up. In fact, it went up far more than evaporation did. So, all else being equal, the U.S. as a whole is a wetter place than it was before the planet’s surface temperature began to rise. 



This doesn’t mitigate the fact that the West is a dry, fire‐​prone region, and will continue to be, if history is any guide. But relating human‐​induced global warming to Western drought is more difficult. 



There are many indices of drought severity, all of which attempt to balance rainfall, evaporation, streamflow and other factors. Perhaps the most often used is the Palmer Drought Severity Index. It has been around for more than half a century, and the National Climatic Data Center, in Asheville, N.C., records it for different regions of the country. 



Most scientists think humans are behind the planetary warming that began in the mid‐​1970s. (Another warming of similar magnitude occurred in the early 20th century, but was entirely natural in origin). But what is the relationship between global warming and drought in the Western U.S.? 



There isn’t any. Statistically speaking, the correlation is zero, which means that as humans have warmed the planet, they haven’t influenced Western drought. This holds whether one starts at the beginning of the Palmer record, in 1895, or the first year of Westerling’s study, 1970. 



Seeing as we have had about a hundred years of global warming, the lack of a clear relationship between earth temperature and Western drought is reassuring, because the relationship between drought and fire is real, even if it is more complicated than people are led to believe.
"
"Given the Coalition’s unconscionable track record, it is very, very hard to assume the Morrison government will approach anything in climate change policy from a position of good faith. But brace yourself, because I’m going to say something that might surprise you. I don’t think it’s dumb for Scott Morrison to be arguing that the Coalition should develop a roadmap before settling on a long-term emissions reduction target.  Before anyone chokes on their Weetbix, I think it is entirely possible the Morrison roadmap, expected sometime in the coming weeks, will be pap – either more vacuous window dressing or yet another stick to beat political opponents with. It could easily be a script for doing nothing, or not nearly enough, or a tedious apologia for outright target avoidance. It would be idiotic for me to forecast triumph in advance of the facts, so I’m not high-fiving a roadmap I haven’t seen, I’m just saying the core concept of having a roadmap leading to a target isn’t intrinsically terrible. If there is any good faith attached to the exercise, this could be a way to thread a needle. Humour me while I step you through my thinking. Once upon a time, before Tony Abbott weaponised climate change to win elections, it would have been possible for Australia to have just ploughed on with this transition without the endless handwringing and mud wrestling. There would have been a sensible policy mechanism to drive decarbonisation, with governments working around the edges to ensure the transition was as fair as it could be. No fuss, just steady progress. But the Coalition broke the debate. The Liberal and National parties deliberately injected unreason into the polity. Having rendered an entire discussion radioactive in order to re-elect sufficient numbers of Nationals in Queensland to hold on to government, the challenge now arrayed before the Coalition is how to reverse engineer their own bastardry. Given the epic scale of that bastardry – unpicking it takes some doing. Obviously, there is zero prospect of achieving any course correction if the Coalition doesn’t want to change direction. Sadly, it is entirely possible our government will pretend to care about climate change in places where it is electorally advantageous to do so while doing absolutely nothing of substance to arrest the danger. It is possible they are that cynical, even after the terrible portents of the summer. But some inside the government really do want to shift, and Morrison, for now, is leaving that option open. If we accept for a rash moment that the key people do want to pivot, if we accept there is any underlying sincerity or scintilla of national interest consideration going on here, then the case for action will need to be built in increments from the ground up. It has to be built in increments, for two reasons. The first is about internals. If the government pivots too quickly, it will blow up the Coalition. Poor Mathias Cormann made the mistake on Friday of saying the government would be finalising a longer-term target for the next UN-led meeting in Glasgow, only to find himself corrected by Angus Taylor, who declared it would be a long-term “strategy” (because the word target is equivalent to an improvised explosive device. I mean good God, spare us). The second is about external perceptions. The Coalition has been telling Australians for 10 years that ambitious emissions reduction targets are terrible, and nothing has to change. Now it has to contemplate how the opposite can also also true. There is only one way for the opposite to be true that I can see, and that’s to make the case that Australia can make a transition to lower emissions in an orderly way where people aren’t left behind, and in fact, are given new opportunities. So now we are back at the roadmap not being a dumb idea. In a strange way, the Coalition having to navigate its fraught internals, and having to workshop how it might gradually stop lying without losing all credibility, creates opportunity for politicians to tell a story that has only been told fitfully. For more than a decade, the climate debate in Australia has been either a wonkish seminar about carbon pricing, emissions trading, clean energy targets, renewable energy targets, national energy guarantees, safeguard mechanisms, abatement targets, carryover credits, and the like – concepts that are vitally important but carry absolutely no practical meaning for most people – or it has been a slasher movie replete with surround-sound alarmism, hyperbole, intrigue, betrayals and bouts of regicide. The policy is arcane, the politics has been absolutely rancid, and frankly, a lot of the reporting of this issue has been execrable too, and in many places, the reporting goes on being terrible, either deliberately as a corporate mission, or because reporters confuse balance with false balance. It is entirely understandable, given these realities, that voters now don’t know who or what to believe. When everything is a stinking mess, it makes sense to me to retreat to first principles, and build a case for action by, wait for it … telling people what action looks like. What are the new technologies? Are they job-creating or job-destroying? What is the role of government in rolling them out? What will be the consequences of the change for jobs, growth and living standards? Perhaps that story can be a reassuring one. Because what’s before us is more multidimensional than the Coalition, shambling, in fits and starts, towards reality; a government wondering, in its cups, how to market an implicit mea culpa. I think a number of progressive people who are now fully sold on the need for climate action fail to grasp something that is now pretty obvious. The truth is many voters remain unpersuaded. Australians report they are worried about climate change. Poll after poll tells us Australians are becoming more anxious about a lack of action. But a majority are not voting for climate action. They are prioritising other things. This seems a strange kind of dissonance. The election result last May tells us most voters prioritised their immediate economic security over climate action, which leads me to an obvious conclusion: climate action will not happen in Australia unless the debate shifts, and shifts compellingly, to an economic frame. Logically, that process starts with mapping out how decarbonisation intersects with future job prospects; it starts with answering a basic question – what options does this transformation give me? The climate debate in Australia has of course touched on these themes, but there has been no cut through narration by either of the major parties, largely because nobody in major party politics has the guts to say the safety of the planet means fossil fuels are on the way out, and because the whole system has been obsessed with either targets and mechanisms, or with gothic intrigue, like whether the latent communist Malcolm Turnbull has somehow snuck back in to lead the Liberal party, or whether Anthony Albanese looked sideways at a coalminer. So targets are really important. Policy mechanisms also remain critically important. But nothing lasts if voters fear the future. So here is my question: who in politics has the guts to navigate Australia out of climate change Groundhog Day by being honest about what this transition actually means?"
"Since the 1960s winters across Europe have become milder, and in the UK our winters have become warmer and wetter. Big snow dumps are less common now. Much of this change can be blamed on anthropogenic global warming, but it turns out that Europe’s efforts to clean its skies have also reduced the likelihood of harsh winter weather. Air pollution can have a significant impact on our climate. Some pollutants, like sulphate particles, scatter radiation and cause cooling. Air pollution can also change cloud properties. Since the 1970s countries across Europe and North America have significantly curbed their air pollution, resulting in more of the sun’s energy reaching Earth’s surface over these locations. Yuan Wang, from the California Institute of Technology, and colleagues have input air pollution data (gathered between 1970 and 2005) into climate models to investigate how cleaner skies may have impacted atmospheric circulation patterns and extreme weather. Their results, published in Nature Climate Change, reveal that the reduction in air pollution has altered the strength and location of high altitude winds, shifting the jet stream further to the north during winter. This change has suppressed cold extremes over northern Eurasia. Future pollution reduction over China is expected to exert a similar influence."
"

Pundits, politicians and the press have argued that global warming will bring disaster to the world, but there are good reasons to believe that, if it occurs, we will like it. Where do retirees go when they are free to move? Certainly not to Duluth. People like warmth. When weather reporters on TV say, “it will be a great day,” they usually mean that it will be warmer than normal. 



The weather can, of course, be too warm, but that is unlikely to become a major problem if the globe warms. Even though it is far from certain that the temperature will rise, the Intergovernmental Panel on Climate Change (the U.N. body that has been studying this possibility for more than a decade) has forecast that, by the end of the next century, the world’s climate will be about 3.6° Fahrenheit warmer than today and that precipitation worldwide will increase by about 7 percent. The scientists who make up this body also predict that most of the warming will occur at night and during the winter. In fact, records show that, over this century, summer highs have actually declined while winter lows have gone up. In addition, temperatures are expected to increase the most towards the poles. Thus Minneapolis should enjoy more warming than Dallas; but even the Twin Cities should find that most of their temperature increase will occur during their coldest season, making their climate more livable.



Warmer winters will produce less ice and snow to torment drivers, facilitating commuting and making snow shoveling less of a chore. Families will have less need to invest in heavy parkas, bulky jackets, earmuffs and snow boots. Department of Energy studies have shown that a warmer climate would reduce heating bills more than it would boost outlays on air conditioning. If we currently enjoyed the weather predicted for the end of the next century, expenditures for heating and cooling would be cut by about $12.2 billion annually.



Most economic activities would be unaffected by climate change. Manufacturing, banking, insurance, retailing, wholesaling, medicine, educational, mining, financial and most other services are unrelated to weather. Those activities can be carried out in cold climates with central heating or in hot climates with air conditioning. Certain weather‐​related or outdoor‐​oriented services, however, would be affected. Transportation would benefit generally from a warmer climate since road transport would suffer less from slippery or impassable highways. Airline passengers, who often endure weather‐​related delays in the winter, would gain from more reliable and on‐​time service.



The doomsayers have predicted that a warmer world would inflict tropical diseases on Americans. They neglect to mention that those diseases, such as malaria, cholera and yellow fever, were widespread in the United States in the colder 19th century. Their absence today is attributable not to a climate unsuitable to their propagation but to modern sanitation and the American lifestyle, which prevent the microbes from getting a foothold. It is actually warmer along the Gulf Coast, which is free of dengue fever, than on the Caribbean islands where the disease is endemic. My own research shows that a warmer world would be a healthier one for Americans and would cut the number of deaths in the U.S. by about 40,000 per year, roughly the number killed on the highways.



According to climatologists, the villain causing a warmer world is the unprecedented amount of carbon dioxide we keep pumping into the atmosphere. As high school biology teachers emphasize, plants absorb carbon dioxide and emit oxygen. Researchers have shown, moreover, that virtually all plants will do better in an environment enriched with carbon dioxide than in the current atmosphere, which contains only trace amounts of their basic food. In addition, warmer winters and nights would mean longer growing seasons. Combined with higher levels of CO2, plant life would become more vigorous, thus providing more food for animals and humans. Given a rising world population, longer growing seasons, greater rainfall, and an enriched atmosphere could be just the ticket to stave off famine and want.



A slowly rising sea level constitutes the only significant drawback to global warming. The best guess of the international scientists is that oceans will rise about 2 inches per decade. The cost to Americans of building dikes and constructing levees to mitigate the damage from rising seas would be less than $1 billion per year, an insignificant amount compared to the likely gain of over $100 billion for the American people as a whole. Let’s not rush into costly programs to stave off something that we may like if it occurs. Warmer is better; richer is healthier; acting now is foolish.
"
"
Share this...FacebookTwitter
Prof. Hans von Storch’s site Klimazwiebel here brings our attention to a new site called Information portal climate change posted by the Austrian ZAMG – Austria’s Central Bureau for Meteorology and Geophysics.
Normally Klimazwiebel posts in English, but because, I suppose, the links and related literature are in German, Prof. von Storch posted this one in German.
The new ZAMG site has the purpose of providing site visitors with a trustworthy resource on the subject of climate science. It is for informing the public.
One of the contributors to the site is Dr. Reinhard Böhm of the ZAMG. Translating the quote provided by Klimazwiebel, Reinhard Böhm writes, interestingly:
In more than 80 individual articles, which have about 1000 references and links and offer additional literature, we wish to create a work of ‘Public Science’ at a scientifically reasonable level. We think we have been successful in achieving this. With this, as a body of rational information, we hope to deliver a counter view – especially at this time – when the COP 16 conference in Mexico will again provide the usual international climate hype.”
Even a moderate warmist like Böhm admits what the IPCC is all about.
In my view the ZAMG Internet resource is warmist, yet rational and not alarmist. I haven’t yet closely examined the links and data the site uses to make the assertions it does on the subject of science, but to me my first impression is that it’s too warmist and does not explain why this warming is any different from previous warmings throughout history.
ZAMG is a government funded institution. One thing is sure: climate science is very politicised and depends heavily on the source of funding. One can only go out so far on a limb before it gets sawn off.
Reinhard Böhm is the author of the book Heiße Luft. Reizwort Klimawandel – Fakten, Ängste, Geschäfte (Hot Air Word of Controversy Climate Change – Facts, Fears, Business).

Share this...FacebookTwitter "
"Boris Johnson’s reshuffle rewarded a number of Brexit-supporting MPs who, on the face of it, appeared odd choices for the roles to which they were appointed. Critics say the government’s choice to lead on the climate emergency has previously seemed a somewhat half-hearted environmentalist, the new attorney general has taken a dim view of the courts and the international development secretary appears to have questioned the value of international aid.  As president of the crunch UN Cop26 climate talks to be hosted by the UK this November, Sharma has a critical role. Yet he showed little interest in the climate emergency before taking over as secretary for international development last July. He has used the term “climate” only six times in parliament, and on only two of those occasions did he have anything substantive to say about the crisis. A Guardian analysis showed he voted only twice in favour of climate protection in 13 votes on the issue. Campaigners are also concerned that his dual role as business secretary will make him beholden to powerful vested interests. However, he did urge the World Bank last October to devote more funding to the climate. The new attorney general has often seemed at odds with the courts. She supported the proroguing of parliament, which was later ruled illegal by the supreme court. In an article last year, she defended the prime minister’s controversial move. She said: “They [remainers] say that proroguing parliament is thwarting democracy … They argue that Boris Johnson is stopping MPs from holding the government to account. That’s hypocritical … What the ‘Outrage Brigade’ has failed to mention during its angry five minutes on air is that parliament was already going to be away on recess for party conferences for three weeks.” In a comment piece on the Conservative Home website last month, she accused the courts of exercising “a form of political power”. She said that while the Human Rights Act was “noble in its intentions”, “the concept of ‘fundamental’ human rights has been stretched beyond recognition”. The new international development secretary has raised questions about the value of international aid. In 2012 – three years before she became MP for Berwick-upon-Tweed – she replied to a tweet from DfID saying: “No one in Africa should go hungry,” by saying: “Nor in the UK. There r kids in NE who have no regular meals due to chaotic parents. Should they go hungry?” A year later, she tweeted about an article by the Conservative donor Michael Ashcroft calling for an end to the guaranteed aid budget of 0.7% of gross national income (GNI), saying: “Interesting article by Lord Ashcroft on the value (or otherwise) of the overseas aid budget,” ending with “#charitybeginsathome”."
"

As part of a yearly tradition, the Cato Institute and Heritage Foundation co-host a debate in which interns of both think tanks debate whether conservatism or libertarianism is a better ideology. Following this year’s debate, the Cato Institute conducted a post-debate survey of attendees to ask who they thought won the debate and what they believe about a variety of public policy and social issues.   
  
The survey finds that millennial conservative and libertarian attendees agree on matters of free speech and religious liberty, the size and scope of government, regulation, health care and what to do about climate change. However, striking differences emerge between the two groups particularly on matters of immigration, the temporary Muslim travel ban, gender pronouns and bathrooms, government’s response to opioid addiction, the death penalty, religious values in government, domestic surveillance, foreign policy, as well as evaluations of the Trump administration.   
  
_Full LvCDebate Attendee Survey results foundhere_   
  
**Priority Differences and Similarities**   
  
Examining conservative and libertarian millennial attendees’ issue priorities offers a quick overview of their similarities and differences. The survey asked attendees how concerned they are about 21 different issues:   








As the chart shows above, conservative millennials are more concerned about morality in society, abortion, terrorism, national security, drug use, and immigration. Libertarian millennials are more concerned about government domestic surveillance, the criminal justice system, and trade. Top priorities shared by both groups include the size and scope of government, free speech, government spending and debt, the economy, and taxes. Notably, libertarians and conservatives share their lowest priority: few are concerned about income inequality.   




**Voting in 2016**   
  
Although 88% of conservative millennial attendees identify as Republican (and 100% do if you include independent leaners), only 51% voted for Donald Trump in 2016. Nevertheless, this is considerably higher than the 20% of libertarian millennial attendees who voted for Trump. Among both sets of Trump voters, fully 7 in 10 said their vote was _against_ Hillary Clinton rather than a vote _for_ Trump. Thus, President Trump received few enthusiastic votes among this group of politically engaged millennial conservatives and libertarians.   
  
While a majority of conservative attendees ultimately voted for Trump, a majority (55%) of libertarian attendees voted for Libertarian candidate Gary Johnson instead. Few voted for Democratic candidate Hillary Clinton (3%). In fact, more said they did not vote (19%) than voted for Clinton.   




**Party Identification**   
  
Conservative millennials overwhelmingly (88%) identify as Republicans, while most libertarian attendees identify with the Libertarian Party (42%) or as politically independent (36%).   




However, after asking independents and libertarians if they lean toward a political party, 100% of the conservative millennial attendees leaned with the Republican Party. A majority (58%) of libertarian millennial attendees did as well, while a third said they are truly independent, and 6% identified as Democrats.   
  
**Evaluations of Trump and Key Political Figures**   
  
Although few of the libertarian and conservative millennial attendees were enthusiastic supporters of Trump during the election, 64% of conservative attendees approve of Trump’s job performance. In stark contrast, 80% of libertarian attendees disapprove. Nevertheless, conservative approval is “soft” with only 12% “strongly” approving. Libertarians are more ardently opposed, with 53% who “strongly disapprove” of President Trump.   




Major differences also emerge in evaluations of key political figures. Notably, while 8 in 10 conservative millennial attendees have a favorable opinion of Attorney General Jeff Sessions, 8 in 10 libertarian attendees have an unfavorable view of him. Libertarian aversion likely stems from disagreements about the criminal justice system, policing, and drug policy. Conservative attendees also have favorable views of Kellyanne Conway (62%), former campaign manager and now Counselor to President Trump, while libertarian attendees do not (22%). Conversely, libertarian millennial attendees have positive views of former Gov. Gary Johnson (61%), while conservatives do not (18%). Conservative attendees are also about twice as likely as libertarians to have positive views of Senators Ted Cruz (88% vs 50%) and Marco Rubio (91% vs 48%).   
  
Libertarian and conservative millennial attendees come together in their shared favorable views of Senator Rand Paul, Education Secretary Betsy Devos (likely due to their shared support of school choice), and writer George Will. They also share unfavorable views of Steve Bannon, Milo Yiannopoulos, and Ann Coulter, figures more closely associated with the “alt-right.”   




**Where Do They Get their News?**   
  
Conservative and libertarian millennial attendees share similar news consumption habits, sharing five of their top six news outlets: the Wall Street Journal (85%, 86%), the New York Times (66%, 75%), the Washington Post (68%, 72%), CNN (50%, 48%) and National Review (76%, 61%). However, conservatives are about 30 points more likely to regularly watch Fox (74% vs 45%) and 15 points more likely to read the Federalist (51% vs 36%). Conversely, libertarians are 53 points more likely to read _Reason_ (24% vs 77%).   




**Culture Wars and Transgender Issues**   
  
Conservatives and libertarian millennials are starkly at odds when it comes to what pronouns to use when referring to transgender people. When referring to a transgender person, 68% of libertarian millennial attendees choose to use the person’s preferred gender pronouns. In contrast, 67% of conservative millennial attendees say they use the pronouns corresponding with the transgender person’s biological sex.   




This difference extends to bathroom access as well. Eight in 10 conservative millennials say transgender people should be required to use the restroom corresponding with their biological sex. Conversely, 70% of libertarian millennials say transgender people should be allowed to use the restroom of the gender they identify with.   
  
Most conservatives (81%) disagree that we as a society need to do more to ensure LGBT people feel fully accepted. Meanwhile, libertarians are split, with 50% believing more should be done for LGBT people to feel accepted and 47% agreeing with conservatives that no more needs to be done.   




Despite these differences, conservatives and libertarians agree on religious liberty: 99% of conservative and 87% of libertarian respondents believe that businesses should be allowed to refuse service to same-sex weddings.   
  
**Immigration**   
  
Conservative and libertarian millennials diverge dramatically on questions of illegal _and legal_ immigration. While 79% of libertarians support increasing the number of immigrants allowed into the country each year, only 20% of conservatives agree. Instead, most conservatives would prefer to decrease the number (35%) or keep it the same (45%).   




When it comes to handling illegal immigration, a majority (52%) of conservative attendees support deporting illegal immigrants and 24% wish to bar them from citizenship. In contrast, 70% of libertarian attendees want to allow illegal immigrants to stay in the US and be able to eventually apply for citizenship.   




Finally, 69% of conservatives favor building a wall along the Mexican border, but 90% of libertarians oppose. However, conservatives are less intensely in favor of the wall than libertarians are opposed to it: only 14% of conservatives “strongly favor” while 67% of libertarians “strongly oppose” its construction.   




When it comes to passing a temporary ban on Muslims immigrating to the United States, conservative attendees are evenly divided, while 86% of libertarian attendees are opposed.   




**Opioid Epidemic**   
  
While most likely share concerns about overuse of prescription painkillers, conservative and libertarian attendees starkly disagree about what government should do about it. Eight in 10 conservatives agree “government needs to do more” to combat prescription painkiller addiction, but 8 in 10 libertarians disagree that government should take on this role.   




**Organ Donations**   
  
It is currently illegal to buy or sell human organs. However, 92% of libertarian attendees believe such a market should be legal; 8% agree with the status quo. Conservatives are ardently opposed with 72% who think such a market should remain illegal, while 28% would favor legalization.   
  
**Foreign Policy and National Security**   
  
Conservative and libertarian millennials sharply disagree on questions of foreign policy. Nearly 90% of conservative attendees support either increasing (38%) or maintaining (51%) our military presence around the world, while 84% of libertarians support decreasing this presence. Moreover, 92% of libertarian respondents support cutting defense spending to help balance the federal budget, while 72% of conservatives oppose such cuts.   




Conservative and libertarian attendees also make different trade-offs between national security and privacy. Seven in 10 conservatives say they’d be willing to give up some personal freedom and privacy for the sake of national security. In contrast, 9 in 10 libertarians say they would not be willing to give up more freedom and privacy for security.   
  
In line with such priorities, 60% of conservative attendees approve of government collection of domestic telephone and Internet data, while 92% of libertarians disapprove of this collection.   
  
**Health Care**   
  
On matters of health care, conservatives and libertarians are often aligned—particularly when it comes to repealing the Affordable Care Act/Obamacare (99%, 95%). However, similar to what’s playing out at the Congressional level, libertarians and conservatives disagree about how to improve the health care system in the country. Three-fourths (76%) of libertarians say the health care system “needs to be completely rebuilt.” However, conservatives are evenly divided with 49% agreeing that the system should be rebuilt, but 46% who think the system “needs major reform but doesn’t need to be completely rebuilt.” Less than 5% of either group think the current system works well and only needs minor changes.   




**Climate Change**   
  
Libertarian and conservative millennial attendees disagree about the causes of climate change but they agree on a solution. Conservatives are more likely to believe climate change is a natural phenomenon, with 54% believing increases in Earth’s temperature are either mostly or entirely due to natural causes. Meanwhile, 62% of libertarians think climate change comes partially, mostly, or entirely from human activity. Despite these different underlying beliefs, 99% of conservative and libertarian attendees agree that technological innovation in the free market will better solve climate change than government regulation.   




**Criminal Justice**   
  
Majorities of conservatives and libertarians reach consensus on several criminal justice issues. Both agree that police departments using military weapons and drones are not necessary for law enforcement purposes. But libertarians (92%) agree more than conservatives (61%). Both groups also favor eliminating mandatory minimum prison sentences for people convicted of selling drugs (64% conservative, 84% libertarian).   
  
However, conservative and libertarian attendees diverge on the death penalty: a majority (56%) of conservatives favor it while a majority (75%) of libertarians oppose it. Respondents also diverge in their perception of racial equality before the law, with 54% of conservatives saying that African Americans and other minorities “receive equal treatment with whites” in the criminal justice system and 73% of libertarians believing that minorities do not receive equal treatment.   




**Free Markets and the Welfare State**   
  
Despite the variety of policy differences between libertarian and conservative attendees outlined above, the two groups largely agree about economic issues, the benefits of free markets, and trade.   
  
For instance, 100% of both groups say they favor a smaller government providing fewer services and low taxes. Nearly 100% of both oppose raising taxes on wealthy households and also agree that regulation too often does more harm than good. Eight in 10 conservatives and nearly 100% of libertarians believe free trade must be allowed even if domestic industries are hurt by foreign competition.   
  
**Religion**   
  
Conservative and libertarian millennials have different ideas about the role of religion in society. An overwhelming majority (83%) of libertarian attendees say religious values should _not_ play a more important role in government. But, 62% of conservative attendees disagree and think such values should play a more important role. Conservatives also believe that it’s important for kids to be brought up with religious values. Libertarians are divided, but tend to disagree (56%).   




Much of this contrast may stem from differences in religious affiliation and habits. While 95% of conservative respondents have a religious preference, 38% of libertarians describe themselves as “non-religious.” Moreover, conservative millennial attendees are twice as likely as libertarians to attend church weekly or monthly (80% vs 41%). A majority (58%) of libertarian respondents either never or rarely attend a religious service.   




**Who Won the Intern Debate?**   
  
Who won the intern debate depends on whom you ask. Among conservative millennial attendees: 53% said the conservative team won and 44% said the libertarian team won. Among libertarian millennial attendees, 94% said the libertarians won while 5% said the conservatives won. Among the moderates, liberals and progressives in the audience, 83% felt the libertarian team won and 12% thought the conservatives won.   
  
**Implications**   
  
This survey provides a useful snapshot of young politically engaged conservatives and libertarians who are interested enough in politics and public policy to intern in Washington or attend an event for Washington interns. Thus, this data offers an idea of the direction young activists may take public policy as they age and the cleavages that may animate policy debates into the future.   
  
_Full LvCDebate Attendee Survey results foundhere_   
  
_David Kemp contributed to this report._


"
"**Gyms and non-essential shops in all parts of England will be allowed to reopen when lockdown ends next month, the prime minister has announced.**
Boris Johnson told the Commons that the three-tiered regional measures will return from 2 December, but he added that each tier will be toughened.
Spectators will be allowed to return to some sporting events, and weddings and collective worship will resume.
Regions will not find out which tier they are in until Thursday.
The allocation of tiers will be dependent on a number of factors, including each area's case numbers, the reproduction rate - or R number - and the current and projected pressure on the NHS locally.
Tier allocations will be reviewed every 14 days, and the regional approach will last until March.
The PM, who is self-isolating after meeting an MP who later tested positive for coronavirus, told MPs via video link he expected ""more regions will fall - at least temporarily - into higher levels than before"".
He said he was ""very sorry"" for the ""hardship"" that such restrictions would cause business owners.
Speaking later at a Downing Street briefing, Mr Johnson added that ""things will look and feel very different"" after Easter, with a vaccine and mass testing.
He warned the months ahead ""will be hard, they will be cold"" - but added that with a ""favourable wind"" the majority of people most in need of a vaccination might be able to get one by Easter.
Until then, the PM said, there would be a three-pronged approach of ""tough tiering, mass community testing, and [the] roll-out of vaccines"".
Describing how the tiers had become tougher, the PM said:
Where pubs and restaurants are allowed to open, last orders will now be at 10pm, with drinkers allowed a further hour to finish their drinks.
Indoor performances - such as those at the theatre - will also return in the lower two tiers, although with reduced capacity.
In terms of households mixing, in tier one a maximum of six people can meet indoors or outdoors; in tier two, there is no mixing of households indoors, and a maximum of six people can meet outdoors; and in tier three - the toughest tier - household mixing is not allowed indoors, or in most outdoor places.
In all tiers, exceptions apply for support bubbles. From 2 December, parents with babies under the age of one can form a support bubble with another household.
Mr Johnson said the tiers would now be a uniform set of rules, with no negotiations on additional measures for any particular region.
Measures in Scotland, Wales and Northern Ireland continue to be decided by the devolved administrations, but a joint approach to Christmas, involving all four nations, will be set out later in the week.
The prime minister said: ""I can't say that Christmas will be normal this year, but in a period of adversity time spent with loved ones is even more precious for people of all faiths and none.
""We all want some kind of Christmas; we need it; we certainly feel we deserve it.
""But this virus obviously is not going to grant a Christmas truceâ¦ and families will need to make a careful judgement about the risks of visiting elderly relatives.""
For the third week running we have had some positive vaccine news, but the announcement about the toughened tiers is a reminder, if we needed any, that the next few months will be tough.
Ministers and advisers have been hinting for the past week that the tiers will be toughened - and that is exactly what has happened.
Attention will now naturally turn to which areas will be in which tiers.
Deciding that is a complex equation that will take into account whether the cases are going up or down, the percentage of tests that are positive, hospital pressures and infection rates among older age groups.
To give a flavour of how complex this is places in the North West and Yorkshire have some of the highest rates but they are falling the fastest.
London and the South East have lower rates and more hospital capacity but cases are going up.
Fine judgements will have to be made. We will find out on Thursday.
Mr Johnson also announced changes to sport for both spectators and participants.
While elite sport has continued behind closed doors during the lockdown, grassroots and amateur sport has been halted since 5 November.
From 2 December, outdoor sports can resume, while spectators will be allowed to return in limited numbers. Some organised indoor sports can also resume.
In the lowest risk areas, a maximum of 50% occupancy of a stadium, or 4,000 fans - whichever is smaller - will be allowed to return. In tier two, that drops to 2,000 fans or 50% capacity, whichever is smaller.
In tier three, fans will continue to be barred from grounds.
In tiers one and two, business events can also resume inside and outside with tight capacity limits and social distancing, as can indoor performances in theatres and concert halls, the government's plan says.
Labour leader Sir Keir Starmer described the government's return to the regional system as ""risky... because the previous three-tier system didn't work"".
He added that decisions on which areas will belong to each tier must be taken without delay - ""I just can't emphasise how important it is that these decisions are taken very quickly and very clearly so everybody can plan.
""That is obviously particularly important for the millions who were in restrictions before the national lockdown, because the message to them today seems to be 'you will almost certainly be back where you were before the national lockdown - probably in even stricter restrictions'.""
Helen Dickinson, of the British Retail Consortium, said shops would be ""relieved"" at the decision to allow them to reopen.
""Sage data has always highlighted that retail is a safe environment, and firms have spent hundreds of millions on safety measures including Perspex screens, additional cleaning, and social distancing and will continue to follow all safety guidance,"" she said.
But the UK hospitality industry warned the new rules ""are killing Christmas and beyond"" and said pubs, restaurants and hotels faced going bust.
Meanwhile, a further 15,450 positive coronavirus cases were recorded across the UK on Monday. There have also been a further 206 deaths within 28 days of a positive test. Figures can be lower on a Monday, due to a lag in reporting.
Earlier, it was announced that daily coronavirus tests will be offered to close contacts of people who have tested positive in England, as a way to reduce the current 14-day quarantine period.
Mr Johnson said people will be offered tests every day for a week - and they will not need to isolate unless they test positive.
He also said rapid tests will allow every care home resident to have up to two visitors tested twice a week."
"
From The Sun
By VINCE SOODIN
Published: 26 Aug 2008
TWO British schoolgirls cheated death after being stung by a lethal Portuguese  Man O’War.
Paddling Molly Purcell, ten, suffered toxic shock and gasped for breath after  the tentacles of the sea creature — which can kill with a single sting —  wrapped around her arms and legs.
Pal Amelia Walsh, 12, was left with huge welts on her legs after brushing part  of the creature that was draped over a rock.
Medics used freezing water and seawater to flush out the toxins.
Bathers were evacuated after last Friday’s attack at Monmouth Beach, Dorset.
Molly, of Ascot, Berks, said: “I thought I was stung by a bee at first, then  suddenly it felt like my arm was on fire. It got worse and worse until I  couldn’t stop shaking.”
Last night mum Sheenagh said: “Molly is getting better but her arms are still  very swollen.”
Brits were warned last month of seven species of poisonous sea creatures  heading for our shores due to global warming.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9d1e6c9d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
From ICECAP

By Joseph D’Aleo CCM, AMS Fellow
2008 will be coming to a close with yet another spotless days according to  the latest solar image.

This will bring the total number of sunspotless days this month to 28 and for  the year to 266, clearly enough to make 2008, the second least active solar year  since 1900.

See larger image here.
The total number of spotless days this spolar minimum is now at around 510  days since the last maximum. The earliest the minimum of the sunspot cycles can  be is July 2008, which would make the cycle length 12 years 3 months, longest  since cycle 9 in 1848. If the sun stays quiet for a few more months we will  rival the early 1800s, the Dalton Minimum which fits with the 213 year cycle  which begin with the solar minimum in the late 1790s.

See larger image here.
Long cycles are cold and short ones like the ones in the 1980s and 1990s are  warm as this analysis by Friis-Christensen in 1991 showed clearly. 

See larger image here.
In reply to the arguments made that the temperatures after 1990 no longer  agreed with solar length, I point out that it was around 1990 when a major  global station dropout (many rural) began which led to an exaggeration of the  warming in the global temperature data bases. Also the length from max to max of  21 to 22 was 9.7 years and cycle 22 length min to min 9.8 years, both very short  suggesting warm temperatures in the 1990s. The interval of cycle 22 max to cycle  23 max centered in the mid 1990s began to increase at 10.7 years and the min to  min length of cycle 23 is now at least 12.3 years.
With the Wigley suggested lag of sun to temperatures of 5 years and  Landscheidt suggested 8 years, a leveling of should have been favored around  2000-2003 and cooling should be showing up now.  Looking ahead, put that  together with the flip of the PDO in the Pacific to cold and you have alarming  signals that this cooling of the last 7 years will continue and accelerate.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99a67de2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The new minister for resources, Keith Pitt, says he sees no case to increase the level of taxation on the booming gas industry, despite concerns from experts that large multinationals are avoiding paying tax, and the budget is missing out on valuable revenue. The Queensland National, who replaced Matt Canavan in the portfolio after Canavan resigned to back Barnaby Joyce’s unsuccessful tilt at the party leadership, told Guardian Australia: “I think the taxation levels are reasonable where they are, and it will be steady as she goes.”  Pitt acknowledged there were calls to overhaul the tax regime, with concerns Australians are not being properly compensated for the extraction of the country’s abundant natural resources, but he said “there are also expert views around saying we should increase the GST, or change a whole pile of other policies, but that’s not the government’s position”. The new resources minister also called for: An expansion of Australia’s controversial coal seam gas industry, including the contentious Santos project in north-west New South Wales, arguing the industry had worked to fix problems and “the technology is proven”. “I think we should open up Narrabri and others,” Pitt said. “If we can drive down domestic gas prices, everyone’s lives are made easier, particularly business.” More exploration of carbon capture and storage, even though CCS has not been commercially viable despite years of development. Pitt said there were “real opportunities” with the technology that could be combined with ultra super critical coal plants to reduce emissions. He said he was firmly technology-neutral when it came to power generation, but “coal will continue to be an important part of not only the economy, but what happens in regional areas for a long time to come”. While he was supportive of the coal industry, Pitt declined to express a view about the viability of the Collinsville coal-fired power plant proposal that has split the government. He also declined to say whether the government would indemnify the project against future carbon risk. Scott Morrison has left open the option of his government indemnifying the plant from future carbon risk, and politically connected power baron Trevor St Baker told Guardian Australia no project at that scale could proceed without an indemnity from the commonwealth. But Pitt said those were issues for his colleague, the energy minister, Angus Taylor, and he said he would not be in a position to have an informed view about whether or not Collinsville stacked up until the feasibility study was completed. “That’s why we are doing the study,” he said. “I can’t assess the feasibility of the project until that is completed.” Pitt has previously advocated consideration of nuclear power in Australia, but declined to repeat that view now he was in the resources portfolio. He said it would be up to Taylor if nuclear proceeded in Australia, and he said it would certainly not proceed unless there was bipartisan support. He said he accepted the science of climate change, but declined to express a view about whether global heating was fuelled by human activity. Pitt said there was “no doubt” the climate was changing, but the focus now should be on building resilience, and on technological solutions to lowering emissions. Pitt said he has never held the view that “we should be pouring unrestricted amounts of all sorts of bits and pieces, not just CO2 or greenhouse gases, into the atmosphere”. Pitt’s comments came as Australia’s chief scientist, Alan Finkel, told the National Press Club Australia’s energy sector was already well on the way with the transition to low emissions energy. But he said while renewable technologies were “being scaled up, we need an energy companion today that can react rapidly to changes in solar and wind output”. Finkel said Morrison had nominated gas as the transitional fuel “in the short term”. But he declared the “hero” of the transition would be hydrogen. In parliament, Taylor said the government was committed to rolling out bilateral agreements with the states to guide the energy transition, like the $2bn deal unveiled recently with New South Wales that Canberra says will boost gas supply. But he said for that deal to be replicated, the states “must do the right thing”. “When it comes to those deals, whether it’s NSW or Victoria, the principle is very simple,” Taylor said. “No gas, no cash.”"
"
Share this...FacebookTwitterSnow already!
A cold blast of polar air has moved over central Europe, pushíng temperatures far below normal. Snow is forecast in the Alps at elevations as low as 1400 meters.
The following chart shows the forecast for the next 2 weeks. It shows that summer is finished for much of Europe. In fact, as mentioned above, the lower Alps are expected to get snowfall in the couple of days ahead. Continue reading below.
According to Snowfinder, it’s time to get your skis out. Snow is in the forecast at many ski resorts. At Mürren Schilthorn in Switzerland elev. 2360 meters, snow is forecast for early this week. The same is true for Kitzbühel in Austria at 2000 meters elevation.
Heck, even below 2000 meters snow is in the forecast, for example Steinplatte Waidring in Austria , which is only 1800 meters elevation. Maybe Nature will publish a study attributing it to global warming.
===================================================
Update 1,  August 30: Rain mixed with SNOW was reported on the Brocken summit, elevation 1141 meters, In north Germany this morning on north German NDR radio.
Update 2, August 30: Snow line has dropped to 1400m (below the 5000′ line) in the Alps. Heavy snow is in the forecast. See following chart (h/t: Reader Patagon):
Source: http://www.meteoexploration.com/snow/snowmaps.html
Share this...FacebookTwitter "
"Materials essential for technology products such as electric vehicles, wind turbines or hard disks, known as rare earth elements, aren’t becoming any less rare, or any less crucial. In fact, experts at a major rare earths conference in Milan on October 16 – the European Rare Earths Competency Network (ERECON) – agreed supply shortages will continue for the time being. This isn’t just a matter for tech companies: their gloomy outlook should be of crucial importance for the future of international relations.  We need new sources of such materials, and we need our current industries to become more resilient towards supply disruptions. They will happen and we must be prepared. Rare earth elements are like the salt and pepper of numerous everyday products – the world wouldn’t end without them, but it would be an impoverished place. Some say the worst is over. The supply crunch for rare earth became apparent when prices tripled in 2011, driven by concerns over limited supply and export restrictions imposed by China, the dominant producer at the time. Following this price peak the US, Japan and the EU successfully appealed to the WTO and forced China to open up its export market. Supplies from outside of China such as the Mountain Pass mine in California have also developed and China’s global market share has accordingly gone from 95% down to around 75%, according to conference participants. And geologists around the world don’t stop reassuring us that the reserves are plentiful. So, is this much ado about nothing? Actually, the picture looks less bright than those slowly emerging trends of a more independent supply might suggest. Most countries are at least as vulnerable as in 2011. China is progressively moving “downstream” in the rare earths supply chain. Once, it merely dominated in mining the actual materials, now it wants to create jobs and a high-tech manufacturing industry dependent on these rare earths. It has put measures in place to “encourage” manufacturers to locate their production in China, to “ensure” access to rare earths according to conference participants.  This leaves European producers in a pretty uncomfortable position. They want reliable contracts with downstream industries, which operate closer to the final products. But survival in competitive times is tough, and the firms that actually make cell phones or wind turbines are now locked into investment decisions that will require more rare earth supplies for the foreseeable future. There is a less-known ethical dimension in it. Some 40% of Chinese rare earth extraction is estimated to take place in illegal mines according to an expert who works with Chinese sources. Consumers should start realising that a significant number of fancy devices and cherished renewable energies are in fact based upon a dirty, bloody and illegal extraction process. On top of all that, the gunboat diplomacy that created the momentum for the 2011 supply crunch continues. Territorial disputes in the South and East Chinese Seas have left China on bad terms with several neighbouring countries. Given that historians constantly remind us that the Great War emerged out of a similar constellation in Europe in 1914, one should clearly be worried about what potentially could escalate into a serious catastrophe. The US, France, Germany and Japan have all run large-scale projects to test new recycling options that could help reduce dependency on new rare earths. These sorts of initiatives can yield results quickly. For countries who haven’t put such projects in place (including the UK) it’s high time to catch up. Manufacturers such as Siemens or General Electric who depend on rare earths seem ready and willing to engage. After all it is in their interest to establish supply chains that can withstand a run on these materials. A platform for engagement between government and manufacturers may require policy intervention – some sort of technology strategy board or an international metal recovery covenant. Though recycling and increased efficiency is important such steps won’t be enough by themselves: new supplies will be needed – and this means mining. Sweden has some of the most promising deposits in Europe, so has Greece, among other regions. Local politicians may struggle to get backing for new mining but the public interest in green technologies that rely on a sustainable supply should help. Greenland also has rich rare earth deposits – potentially a quarter of the world’s supply. China is already getting its foot in that door, with 1,500 workers already mining there, but the race is not yet over. The rare earth supply crunch is a short-term urgency with mid-term opportunities. Nobody should blame China for acting in its own interests. Rather, others must collaborate together to reach a solution. Last but not least international efforts to establish due diligence along the value chain should help China to move away from illegal mining and establish a more sustainable and transparent rare earth supply."
"

Americans just aren’t scared enough by global warming to take it seriously. That’s the lesson from the poll published in the _Washington Post_ last month showing prospective Savior‐​in‐​Chief Al Gore down by 19 percent. If people really thought Gore’s pet cause was such a threat, wouldn’t they vote to protect their children? Can “Clinton fatigue” be that bad? 



Our friends at the United Nations understand the need to get the United States more involved in stopping global warming. They also understand Americans’ basic sense of fair play and compassion, as evinced by our sending troops to Somalia, Haiti, Kosovo, Ersatz‐​Yugoslavia and maybe Dili. If some nation can convince us it’s getting the short end of the stick, U.S. largesse is not far behind. 



To enlist our help, the United Nations is currently holding a special conference of “island states” that view themselves as threatened by global warming in general and sea level rise in particular. 



“In low‐​lying areas, the sea has claimed our burial grounds,” said Samoan UN envoy Tuiloma Neroni Slade, chairman of the Alliance of Small Island States (AOSIS), an official UN hectoring organization. Slade added that in the Maldives, about 800 miles south of Bombay, “Climate change is already taking effect in terms of some of the life support systems.” Ditto for the Marshall Islands, Vanuatu et cetera. 



Slade is banking on Americans’ being too guilt ridden to check the facts and ratifying the Kyoto Protocol on global warming pronto in order to make up for our sins. Unfortunately, facts are just a click away. 



The 1995 report of the UN’s Intergovernmental Panel on Climate Change (IPCC) contains a whole chapter on sea level rise, complete with charts. The monitoring station closest to Samoa is Sydney, Australia, where there has been a truly tiny rise in sea level of only 3.14 inches in the last 100 years. But almost all of that took place before 1950. Since then, the rise in sea level, which has “claimed their burying grounds,” has been 0.4 inches. 



In the IPCC report, Bombay is the station nearest the Maldives. As Casey Stengel used to say, “You could look it up,” and there it is: sea level has fallen an inch in Bombay in the last 50 years. 



Nice try, Mr. Slade. 



Of course, the IPCC forecasts that sea level will rise in the next 100 years. The most recent projection gives two median values: 19.3 inches from one model and 10.6 inches from another termed “equally plausible.” But global warming is proceeding at a slower pace than those models assumed, so it’s probably a good idea to cut the totals by a third or so. Could Pacific Islanders adapt to 10 inches of sea‐​level rise in the course of a century? 



Consider the Outer Banks of North Carolina, where, every few years, the sea rises about 12 feet in 10 minutes. This is a hurricane. Because of hurricanes, up until 1950 or so very few people lived there. Fearing the wind, the handful of mainlanders who came in the 1950s built little one‐​story “flattop” homes, nestled beneath the dune crest to protect them from the wind. 



When away from home, those people were able to charge $100 or so a week for a summer “beachfront” rental, which really meant a human‐​eye view of the barrier dune. The flattop owners then discovered that wind wasn’t the problem after all, as their vacation houses were washed away into the sea by the numerous hurricanes of the 1950s and 60s. 



One day they got the fine idea of elevating their homes on stilts so the sea could rush harmlessly underneath during a hurricane. Of course, that didn’t protect them from a direct hit by the northeastern eyewall of a Category 3 storm. In that case, a beachfront home is usually plumb out of luck, but the damage swaths in such storms are surprisingly narrow considering the thousands of miles of developed coastline from Brownsville, Texas, to Eastport, Maine. Somehow, damaged areas tend to appear larger on TV. 



Stilts protect the houses from most every other hurricane. And, as a side benefit of the elevation of their homes, vacationers now view sunrises over the Atlantic Ocean and sunsets over Albemarle Sound from the same house. Rent skyrocketed, to $5,000 per week in high season. 



A trip to Kwajelein in the Marshall Islands reveals that most of the homes are as close to the ground as they were in North Carolina before someone discovered how to make big bucks and survive foot after foot of extremely rapid tidal inundation. It seems probable that the AOSIS people will figure out how to adapt to 10 inches of sea‐​level rise in 100 years. They don’t need our help to raise property values fiftyfold. And if they don’t, it won’t be because they couldn’t.
"
"Is there any embarrassment in the Ocado boardroom for the part that supine non-executives played in sanctioning an £88m bonus for directors? It would seem not. Andrew Harrison, who chairs the remuneration committee, brazened it out in the annual report, urging shareholders to look at a different number – the 265% increase in the company’s share price over the past two years (and certainly not the £214m loss it delivered in 2019). Tim Steiner, the chief executive, got £54m of the pot, receiving a tidy £58.7m once his salary and other bonuses are taken into account. He’s hit the jackpot twice because the surge in the share price has boosted the value of his personal stake to nearly £300m.  The bonus, even measured by the yardstick of City excesses, is obscene. It’s one of the biggest bonus payouts made by a listed UK company, only bettered by the likes of Jeff Fairburn, the disgraced former chief executive of the housebuilder Persimmon, who pocketed £75m, and Sir Martin Sorrell, who banked £70m from WPP in 2015. Yet we are supposed to see Ocado differently these days. Think Tesla, not Tesco – well, Elon Musk’s potential $50bn pay deal provides a flattering comparison, at least. Today Ocado has successfully reinvented itself as a tech stock selling armies of grocery-picking robots to supermarkets – and says it should have Silicon Valley-style pay deals to keep its indispensable executives “motivated”. But for many people Ocado is still the company that pulls up at their front door with bags full of Waitrose groceries. That will end this autumn when the company will attempt to switch horses to Marks & Spencer – which is paying £750m for half its UK grocery business – without falling from the saddle. So you would think the company would be more concerned about its image. Once beloved of the middle classes, Ocado is facing incoming fire from parents and teachers opposed to its plan to open a distribution hub – complete with diesel fuel pumps – yards from their north London primary school. The company, which has been trialling electric vans in high-density areas, has said it will replace all the diesel vans if it can get a power upgrade at the site. Maybe the best part of the £100m that executives have pocketed would have been better spent on greening its fleet? The annual report shows that Steiner’s pay was more than 2,600 times that of the median employee, who all-in gets £22,500. As part of the preamble, the company highlights that executive pay is “more at risk than wider employee pay due to the use of variable pay” and without the mega bonus the multiple would only have been about 200 times. The icing on the cake was the decision to raise the basic salaries of the four executives who shared the £88m jackpot; this year Steiner will have to drag himself out of bed for a guaranteed £720,000 salary. Which brings us back to Waitrose, part of the employee-owned John Lewis Partnership, where staff in good years are paid the same rate of bonus as top executives. Once Ocado shifts over to M&S, shoppers will have to make a conscious decision to move away from their familiar service to Waitrose’s own website. As a shopper, all you can do is vote with your feet. The US environmentalist and author Bill McKibben famously warned that in the race to tackle the climate crisis, winning slowly is the same as losing. Last week BP only barely found its feet in the starting blocks – again. Almost 23 years ago, BP’s chief executive at the time, John Browne, became the first oil boss to publicly accept responsibility for the looming climate crisis. In a landmark address at Stanford University he promised “substantial, real and measurable” action from the UK oil giant. On the top of his list was a pledge to control BP’s own emissions. But almost a quarter of a century later, the world is still waiting. Browne was in the audience in London on 12 February at a London hotel conference room as his former protege and BP’s new chief executive, Bernard Looney, set out a new target for the oil industry’s climate ambitions. He one-upped Browne’s vow to cut carbon emissions by setting an ambition to reduce and “neutralise” enough greenhouse gases to turn BP into a carbon-neutral company. The 21st-century version of BP’s green turn may have relied more heavily on slick video production and branded graphics, but the pledges share a worrying parallel: neither set out how BP would achieve the grand goals set for the middle of this century. And so, the world has been asked to wait a little longer. Investors must wait until September to learn how Looney plans to make his green vision a reality. The net-zero revolution itself is a 2050 target, meaning another quarter of a century will pass before the company intends to meet it. Looney’s address acknowledged that people would be impatient for detail, and many others would never be satisfied that BP is going far enough, fast enough. On this point, at least, he is wholly correct. He is already on borrowed time. For anyone looking to invest their life savings, the gambling industry is hardly a tantalising prospect. Public unease about the industry, following criticism over its failings in dealing with problem gamblers, has reached boiling point. The Gambling Commission, the UK regulator, has begun flexing its muscles. It has already banned bets on credit cards, and promised to review a proposal by MPs that would see online casino stakes cut to £2, bringing web-based casino games into line with fixed-odds betting terminals. Betting shares duly tanked as investors digested the prospect of decreased revenues. Bookmakers fought tooth and nail to save FOBTs, only to emerge utterly defeated by a ragtag coalition of ex-addicts, MPs and rivals in the casino and amusement arcade business. With that defeat fresh in the mind, betting firms now fear the same treatment will be meted out to their lucrative online operations. The stakes have never been higher. Where once the industry dismissed concerns about problem gambling and sought to discredit its opponents, the newly formed Betting and Gaming Council has preferred the tactic of deflection. What about unauthorised overseas gambling sites, they ask. What about the role that banks and big tech companies can play in reducing harm? Most of all, the BGC points to the voluntary steps taken by the industry to address public problem gambling, including reining in advertising. Such olive branches, of course, were offered only after the industry found itself staring down the barrel of a gun. Now the government is reviewing the Gambling Act and has a once-in-a-generation chance to introduce precise, well-informed legislation. Cutting stakes to £2 would not be a cure-all. Limits may have a role in addressing gambling disorders but so do things like curbing the speed of play and robust affordability checks. Whatever the outcome, the odds are short on the sector emerging smaller and less profitable."
"

Part III: Where does global warming rank among future risks to environmental health?

Guest essay by Indur M. Goklany 
NOTE: Entire 3 part series is now available as a PDF here

In Part 1 of this series we saw that even if one gives credence to the oft-repeated but flawed estimates from the World Health Organization of the present-day contribution of climate change to global mortality, other factors contribute many times more to the global death toll. For example, hunger’s contribution is over twenty times larger, unsafe water’s is ten times greater, and malaria’s is six times larger. With respect to ecological factors, habitat conversion continues to be the single largest demonstrated threat to species and biodiversity. Thus climate change is not the most important problem facing today’s population.
In Part 2 we saw that even if we assume that the world follows the IPCC’s warmest (A1FI) scenario that the UK’s Hadley Center projects will increase average global temperature by 4°C between 1990 and 2085, climate change will at most contribute no more than 10% of the cumulative death toll from hunger, malaria and flooding into the foreseeable future. It would simultaneously reduce the net population at risk of water stress.
Clearly, climate change would, through the foreseeable future, be a bit-player with respect to human well-being.
Here I will examine whether climate change is likely to be the most important global ecological problem in the foreseeable future.
As in Part 2, I will rely on estimates of the global impacts of climate change from the British-government sponsored “Fast Track Assessments” (FTAs).
The following figure, which presents the FTA’s estimates of habitat converted globally to cropland as of 2100, shows that the amount of habitat lost to cropland may well be least under the richest-but-warmest scenario (A1FI), but higher under the cooler (B1 and B2) scenarios. Thus, despite a population increase, cropland could decline from 11.6% in the base year (1990) to less than half that (5.0%) in 2100 under the warmest (A1FI) scenario.  That is, climate change may well relieve today’s largest threat to species and biodiversity!
One reason for this result is that higher atmospheric concentrations of CO2 might make agriculture more efficient, and this productivity increase would not have been vitiated as of 2100 by any detrimental impacts of higher temperatures.

The next figure shows that in 2085 non-climate-change related factors will dominate the global loss of coastal wetlands between 1990 and 2085.

[In this figure, SLR = sea level rise. Note that the losses due to SLR and “other causes” are not additive, because a parcel of wetland can only be lost once. For detailed sources, see here.]
Thus we see that neither on grounds of public health nor on ecological factors is climate change likely to be the most important problem facing the globe this century.
So the next time anyone claims that climate change is the most important environmental problem facing the globe now or whenever, ask to see their proof that climate change outranks other problems.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9626ae4b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterWhat is Europe to do? The old continent wants to save the world but it seems no one wants to play along.
Before the summer, there seemed to be some hope the USA could pass a climate rescue bill with President Barack Obama and the Democrat-held Congress. But that hope has all but completely disappeared.
So writes the very green Austrian Der Standard here: Europe Should Focus On China For Climate Protection, Not USA.
With the Democrats getting voted out of office in Congress, the respective concepts for protecting the climate will wind up in the trash can. Europe should quickly search for willing partners.
All together now: Ohhhhh! How dreadful!
But hope is the last to die, and not quite everyone has lost it. Yet, Der Standard sees the writing on the wall:
While there are still a few NGOs who have hope in the good of people and Obama, there’s no one left in the USA who is ready to bet even a single cent on a US climate law could get off the ground.
Jeffrey Sachs, Director of the Earth Institute at Columbia University in New York, and occasionally a columnist for Der Standard says:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




There’s nothing left to salvage!
According to Der Standard:
In an interview conducted via Skype with participants of a Climate Workshop in Brussels, Sachs stated that people in the United States have practically zero interest in climate protection. At least for the next two years, but likely it’ll be much longer before any real kind of initiatives on the issue of climate protection can be expected.
Der Standard mopes that the oil disaster in the Gulf of Mexico had no impact on the public’s opinion, and that a wide majority of Americans favor offshore oil drilling. Now come the old conspiracies and paranoia. Der Standard:
The reason is fear of losing jobs and an unprecedented propaganda machine that was put into motion by the oil industry and its allies.
These allies include a number of evil European corporations like Eon,  BP, Bayer, Basf, Arcelor oder GDF-Suez. Now the enviro-malcontents in Europe and at Der Standard say Europe has to shift its focus to China. Der Standard writes that China has become a “high flyer” in wind and solar energy. Partnership there must be enhanced.
Europe has to accept that there is nothing more to gain from the USA, except frustration. It makes much more sense to work on mutual interests with China.
Consider this a major milestone. Cap & Trade is dead for good in the United States. Even Europe sees it.
Share this...FacebookTwitter "
"Scott Morrison has described a report he may adopt a technology investment target to avoid signing up to a commitment of zero greenhouse gas emissions by 2050 as speculation, but confirmed his government will take a “technology over taxation” approach to climate change. On Tuesday, Morrison told reporters in Melbourne the report in the Australian was “very speculative”, but said it was true that emissions reductions were achieved through technology, not “meetings”.  The Australian suggested Morrison favoured a technology investment target as a way to help Australia resist an international push for a more explicit commitment to reduce emissions to net zero by mid-century at the next major UN climate summit in Glasgow in November. The government is expected to soon release what it calls a technology investment roadmap, but has said little about what it will entail. Several moderate Liberal MPs including Katie Allen and Trent Zimmerman have noted Australia effectively committed to net zero emissions by signing the 2015 Paris agreement. Under that deal, countries agreed to keep global heating above pre-industrial levels below 2C and to pursue policies to restrict it to 1.5C. Morrison has begun the political year torn between moderate Liberals attempting to build traction internally for the government to increase climate action and resistance from the Nationals, who want more government support for coal-fired power. Cabinet discussions have explored how to reposition the government’s climate policies, with a focus on technology, as the bushfire crisis caused a spike in concern about the environment and a hit to Morrison’s popularity. At a press conference on Tuesday, Morrison reiterated that Australia would not make commitments without “having thoroughly looked at what is the impact on jobs”. Asked if an investment target would create tension with those who want a net zero emissions commitment, Morrison said “currently no one can tell me that going down that path won’t cost jobs, won’t put up your electricity prices, and won’t impact negatively on jobs in the economies of rural and regional Australia”. A major report by CSIRO last year found there was no trade-off between strong economic growth and transitioning to zero emissions. The result of two years’ work by 50 leaders across the community, the CSIRO’s Australian National Outlook report found bold action to combat rising challenges could lead to GDP growth of 2.76% to 2.8% annually, a 90% increase in real wages and net zero emissions by 2050. On Tuesday, Morrison suggested investment in technology would enable steel plants to use hydrogen power and increase uptake of renewable energy. “The smart way in dealing with this to get emissions down … is to focus on the technology and making sure that that technology is affordable and it is scalable,” he said. “You want to get global emissions down? … You need technology that can be accessed and put in place, not just here in Australia, but all around the world. Meetings won’t achieve that, technology does. And I can tell you taxes won’t achieve it either.” The industry minister, Karen Andrews, told reporters at Flinders University she was focused on “what the solutions will be rather than endlessly discussing targets or whether or not climate change is real”. Andrews said it was “clear that we do have businesses … that are already investing”, citing BAE and the Innovative Manufacturing Cooperative Research Centre. Labor’s climate change spokesman, Mark Butler, said a technology target would be a throwback to 2007, when then prime minister John Howard and US president George W. Bush rejected emissions reduction targets agreed under the Kyoto Protocol in favour of a “technology approach” that including support for nuclear power and “clean coal”. “Of course technology is the key to lowering emissions, but industry has made it clear that the take-up of new technology requires a serious energy and climate policy,” Butler said. “This is something the Morrison government will never deliver as long as the hard-right climate deniers continue to run the show on climate policy.” The leader of the Greens, Adam Bandt, said: “Instead of a technology smokescreen, we need a Green New Deal to work with affected communities and workers to phase out coal, create new industries and look after the workers in those communities who are affected”. Erwin Jackson, policy director with the Investor Group on Climate Change, said acting on technology was important but not enough. He said all policy needed to be tied to an emissions goal. Net zero emissions was the most credible long-term scenario and had widespread backing, including within the business community. “Setting a path to net zero emissions is prudent economic risk management,” Jackson said. “The only question is if it’s going to happen in a smooth way or in a disruptive way.” Martijn Wilder, a partner at climate advisory and investment firm Pollination and chairman of the Australian Renewable Energy Agency (Arena), said technology would play a critical role in reaching net zero emissions if backed by strong policies. “With the right mix of technologies and policies, Australia is easily capable of reaching net zero well before 2050 and in doing so building an incredibly strong decarbonised economy,” he said. Wilder said having strong public institutions, such as Arena and the Clean Energy Finance Corporation, and policies similar to Britain’s promised ban on new petrol, diesel and hybrid cars by 2035 could “greatly accelerate the transition”. In addition to the technology investment roadmap, the government has commissioned a review of its climate policies led by the businessman Grant King, and promised an electric vehicle policy and a long-term emissions strategy. It has also made it known it is likely to give an $11m grant to the owners of the Vales Point coal-fired power plant in the May budget."
"When MPs announced a citizens’ assembly on the climate emergency last June, two crucial things hadn’t yet happened: Boris Johnson’s takeover of the Conservative party; and the subsequent general election campaign where the main opposition parties each offered radical plans to address the climate crisis, and then lost to Johnson, who had offered no plan at all. For everyone hoping for action on climate, the election was a particularly bruising experience. First throwing open the door to a previously unthinkable possibility – immediate, concrete plans to fight the crisis, far beyond anything proposed by the inadequate Paris Agreement – and then, just as quickly, slamming that door shut. Perhaps even more tightly than before, given Johnson’s disinterest in all things climate-related. And because now the party without an apparent serious climate plan is in charge of taking the critical first steps towards Theresa May’s government’s goal of hitting net-zero by 2050, while the parties willing to commit to action are shut out of power by the enormous Tory majority.  The climate assembly – which met for the second time in Birmingham last weekend – was created to act as a kind of workaround for traditional partisan deadlock, and to chart a safe route forward for governments to act on the climate crisis. Its conceit is that it offers direct access to the real will of the people: 110 citizens – chosen to be representative of the British population – attend sessions where they are briefed by experts on the issue; they then come up with a set of policies to solve it. A citizens’ assembly in Ireland helped the government to put forward the referendum that ended that country’s abortion ban last year. The mood at the assembly so far has been hopeful. The setup assumes that our political leaders have the best intentions, but are paralysed by indecision, both because of the unfathomable number of options and the fear that the public will punish them for the wrong choice. The whole function of politics here is outsourced to the public: they’re asked to make their own tough decisions. And so they have sat attentively listening to presentations on the greatest hits of never-tried climate policy, and have been asked to weigh them up. There was mention of personal carbon allowances (be your own carbon market), a very popular idea in 2008, and more recent schemes such as creating nationwide repair networks, and forcing manufacturers to build goods to last. All the solutions presented reflect the mainstream of British climate policy over the past two decades. The experts put forward a fairly cautious mix of technocratic market incentives and regulation, with large spending programmes reserved for infrastructure projects. Still, there were signs that the assembly might take some bold decisions. The experts barely mentioned free public transport, yet were quizzed on it incessantly. And the members wanted to know who was most responsible for emissions, and how they could be made to pay. The polls suggest that public opinion has leapt ahead of the government’s climate ambition over the past year, and it’s possible that the assembly recommendations will confirm that. But even if the assembly puts the most progressive options in front of the government, there’s still the sticky question of implementation. “How can we be sure the government will follow on our recommendations,” an assembly member asked Labour MP Rachel Reeves, one of only three elected officials in attendance. She didn’t have a good answer, but she could easily have told them it’s not on her – she’s only ever been in opposition during her 10-year tenure in parliament. The assembly’s most valuable contribution may be breaking the deadlock between equally effective but controversial policies – a land-use carbon tax or a consumption tax, for example – although the Tories may well choose neither. The problem with how to tackle the climate crisis in the UK is not partisan deadlock, but lack of government interest. A citizens’ assembly can’t change that. Its decisions aren’t guaranteed to sway the government from its prearranged course. The Irish assembly is held up as a model for its recommendations on the abortion referendum. However, it also submitted proposals on the climate crisis, from huge investments in peat restoration and public transport to a tax on agriculture emissions. The Irish government ignored all of these, in favour of what the prime minister, Leo Varadkar, called “ambitious but realistic” policies to “nudge people to change behaviour”. Nudging people towards radical change is a depressing continuation of the managerial away-day optimism that has dominated thinking on the climate crisis over the past 20 years. One would have better luck trying to nudge a stream back up a mountain. Instead, the best chance for tackling the crisis comes from politicians and parties who don’t wait for incremental solutions. Party politics has become an unexpected laboratory for fusing climate policies to big, popular, social spending programmes – based on the model of a green new deal, which was first put forward by experts in 2008. Unlike a sitting government, opposition policy shops are a frictionless environment: ambition can be doubled and redoubled without pushback. And if the programmes capture the public attention, they are quickly copied by other parties. Labour proposed a green industrial revolution based on using the resources of the state to foster green industries, creating jobs and increasing living standards. This in turn pushed the Liberal Democrats to try to match the offer – remarkable for a party committed to the decentralisation of state power. In the US, the Bernie Sanders campaign is not only currently leading the polls to win the Democratic nomination with the Green New Deal as a top-line policy, it has also pushed every other candidate towards embracing the idea – something that would have seemed impossible just four years ago.  For a long time it has been assumed that public opinion is a barrier to climate action. But the climate assembly will likely confirm what the polls have been indicating for the past year: that people are now ready to move further and faster on climate action than the minimal effort shown by the government. If their advice is ignored or diluted beyond recognition, then maybe citizens’ assemblies are an imperfect mechanism for the scale of change needed to tackle the climate crisis. Only a green new deal for the UK will do. • Stephen Buranyi is a writer specialising in science and the environment"
nan
" New South Wales bureaucrats sought urgent advice from major cotton farmers about how recent rainfall might damage their water harvesting infrastructure, in an apparent effort to justify giving them the green light to retain the first rainfall in over a year, rather than letting it flow downstream. On 7 February the government announced it would restrict the harvesting of overland flows throughout the northern Murray-Darling Basin for the first time, because it was “in the public interest”. But within days, the government had lifted the ban for two valleys and part of a third.  Emails obtained by the Guardian show the NSW Department of Planning, Industry and Environment asked irrigator groups for “urgent” advice on and examples of the sort of damage that their members might incur to levees, pumps and regulators if the water was allowed to flow across their land and then down the Barwon River to the Lower Darling. But by the time the emails were sent, the NSW government had already given the large cotton growing areas a three-day exemption from the embargo. That allowed several huge cotton properties between Walgett and Wee Waa, and west of Moree, to take the first flows in years. The Guardian understands the department received numerous representations from the cotton industry after the government declared the embargo. The government rarely uses its public interest powers and so it detailed the reasons for the embargo. These included “to cope with a water shortage”, to address a “threat to public health and safety” caused by critically low town water supplies and to “manage water for environmental purposes”, noting that the river system health was declining rapidly, putting aquatic biota at risk. The cotton properties have extensive water harvesting infrastructure such as levees and pumps, which are used to divert overland flows into massive on-farm storages. There is no information at this stage about how much water was harvested. However, the proliferation of this infrastructure – only some of which is approved – has had a major impact on how much rainfall reaches the rivers in the northern Basin. Some estimates say historical flows have been reduced to 40% or lower. The decision has angered farmers and communities in the lower Darling, where extremely dry conditions have led to mass fish deaths. Less water entering the river means it may not flow downstream and will instead seep into the dry riverbeds. The email was sent at 3.46 pm on 10 February by a bureaucrat involved in flood plain management to the heads of the Gwydir Valley irrigators, the Namoi Valley irrigators and Barwon Darling Irrigators. “URGENT: requesting further details re: infrastructure risks from temporary FPH restrictions,” the subject line read. “Could you please provide some examples of properties in your valleys where landholders have reported risks to infrastructure from not being able to take floodplain harvesting? “Please supply; property name, contact details (if you have them handy) and a quick description of reported at risk infrastructure.” The email was then forwarded by the irrigator bodies to the cotton growers. This appears to have been after the exemption had been granted. A spokesman for the department told the Guardian: “Following concerns during the rain event that the restrictions were exacerbating localised flash flooding, the department decided to take a precautionary approach due to the potential safety and infrastructure risks, and temporarily lifted the embargo in a limited area.” But the independent NSW MP Justin Field said claims that the decision was taken to reduce the risk to infrastructure didn’t pass the pub test. “This infrastructure is designed to divert flood water into on-farm storages,” he said. “The minister has to explain what infrastructure was at risk, on whose request the decision was made, why such a large area was exempted from the embargo and how much water has been diverted,” he said. A spokeswoman for the minister, Melinda Pavey, said she had not played a role. The department said: “We began investigations as soon as practicable, including an aerial review as soon as it could be arranged.” The Natural Resources Access Regulator (NRAR) said its officers had flown from Tamworth to Walgett after the most recent rainfall/flood event. “Of the properties observed it appeared that most had managed the volume of water, with limited infrastructure damage,” a spokeswoman said, noting this was not part of NRAR’s normal compliance activities. “There was limited infrastructure damage to roads, channels, levee banks, and pumps,” she said.  The NSW Department of Primary Industry and previous ministers have come under scrutiny for providing preferential treatment to cotton interests in the past. In the wake of a Four Corners program in 2017 which revealed a tape of senior NSW bureaucrats offering irrigators information so they could attack the Murray-Darling Basin plan, the head of the water division resigned. An independent inquiry referred a number of matters to the NSW Independent Commission against Corruption. Despite two years of investigations there have been no findings to date. The department said it was continuing to monitor river flows. The embargo is now in place for all designated floodplains until 28 February. More rain is expected this weekend."
"

Ten years ago the Alps endured a virtually snowless winter. Environmentalists blamed global warming. A Swiss lobbying group, Alp Action, wrote in 1991 that global warming would put an end to winter sports in the Alps by 2025. 



This year the Alps have had their greatest snowfall in 40 years, according to very preliminary data. Greenpeace has blamed global warming. 



How in the world can that be? Is it possible to blame global warming for every weather anomaly, even if two consecutive events are of opposite sign? Can such a claim have “scientific” justification? 



If one regards the United Nations as an authority on such things, the answer, unfortunately, is yes. Global warmers, thanks to the good offices of the U.N. Intergovernmental Panel on Climate Change, can blame any weather event on pernicious economic prosperity and resultant greenhouse gas emissions. 



The most recent IPCC summary on climate change was published three years ago. IPCC purports to be the “consensus of scientists” but in fact is a group of individuals hand‐​picked by their respective governments. Does anyone really expect Al Gore to send me to represent the United States at one of those meetings? (Thank you, no, I have been to one and that was enough.) 



Absent my sage advice, here’s what the United Nations wrote in 1995: “Warmer temperatures will lead to … prospects for more severe droughts and/​or floods in some places and less severe droughts and/​or floods in others.” 



As a punishment for not cleaning out the cat box, you might ask your kid to diagram this sentence. Rather than strain the graphics of this word processor, we’ll simply parse it. What the IPCC is saying is that global warming will cause in “some places” and/​or “others”: 



So, according to the “consensus of scientists,” it’s OK to blame a flood, or, if you’re in the mountains, a flood of snow, on global warming. It’s also OK to blame a drought or a snowless Alp on global warming. 



It’s even OK to blame weather that is more normal than normal (“less intense wet and dry periods”) on global warming. 



The IPCC statement, which cannot be proved wrong, is a cynical attempt to allow anyone to blame anything on global warming. As Julius Wroblewski of Vancouver, Canada, wrote to me, this logic “represents a descent into the swamp of the non‐​falsifiable hypothesis. This is not a term of praise. Falsifiability is the internal logic in a theory that allows a logical test to see if it is right or wrong.” 



A non‐​falsifiable theory is one for which no test can be devised, and the U.N. statement fits the bill perfectly. There is simply no observable weather or climate that does not meet its criteria, except one: absolutely no change in the climate, meaning no change in the average weather or the variability around that average. 



Every climatologist on the planet knows that is impossible. Climate has to change because the sun is an inconstant star and the Earth is a nonuniform medium whose primary surface constituent, water, is very near its freezing point. Freezing (or unfreezing) water makes the planet whiter (or darker), which affects the degree to which it reflects the sun’s warming rays. A flicker of the sun, therefore, ensures climate change. 



A hot young climatologist named Robert Mann, writing in Geophysical Research Letters, recently provided a powerful demonstration of this phenomenon. Using long‐​term records from tree rings and ice cores, he concluded that the planet was on a 900‐​year cooling streak between 1000 and 1900. Then we warmed up almost twice as much as we had cooled, but at least half of that warming was caused by our inconsistent sun. Two NASA scientists recently demonstrated that the sun has been warming throughout the last 400 years. As a result, if the last decade weren’t among the warmest in the last millennium, something would have been wrong with the basic theory of climate: The sun warms the Earth. 



That doesn’t mean we haven’t supplied a bit of greenhouse warming, too. But greenhouse warming behaves differently than pure solar warming: It occurs largely in the coldest air masses of winter. 



That’s a far cry from the United Nation’s nonsense about “some places” and “others” experiencing more unusual, less unusual or unusually usual weather. And it has nothing to do with avalanches or snowless winters, either.
"
"

In an election that promises to be close, command and control of the agenda–from “rats” to prescription drugs–is increasingly important, and timing is paramount. Proof is that in each of President Clinton’s victories, “October surprises” nudged the polls enough to turn squeakers into decisive victories. What’s coming this year, befitting the veep’s proclivities, is the first election year “environmental surprise.”



Those with foresight can already spy it along the planet’s limb. NASA scientists are leaking that Antarctic ozone depletion, which conveniently reaches its seasonal nadir around Oct. 20, is proceeding faster than ever. This doesn’t guarantee record depletion this year (in some years depletion has commenced rapidly, only to fizzle), but it certainly increases the chances. And the fact that NASA is already promoting lurid graphics–like the one that appeared in the Washington Post on Sept. 11–means, as usual, that its political antennae are up.



This is the same agency that discovered life in Martian rocks (later shown to be wrong) right at budget time. Now NASA is betting on Gore, who through his years in the Senate rewarded the space program handsomely. In a 1992 budget hearing, Gore announced that global warming should be “NASA’s number one scientific priority,” and the agency has been cashing the checks ever since.



Along about the time ozone bottoms out, the administration is going to release its “National Assessment of Global Warming,” which will forecast hell and damnation for us in coming decades unless we dramatically reduce our use of fossil energy–coal, oil and natural gas. How hard will it be for Gore to play this for political advantage? Can you hear the rhetoric? “The most eminent scientists in the nation all predict environmental disaster unless we curtail our use of oil, and my opponent’s largest contributors are the corporate polluters who created this problem,” Gore will intone.



Right about then, NASA will announce that the ozone hole has grown so large that it threatens the people of Chile, Argentina and South Africa, including the three million residents of Cape Town.



But how could the ozone hole still be growing, given that production of the putative cause–chlorofluorocarbon refrigerants (CFCs)–was stopped almost 10 years ago by a treaty called the Montreal Protocol? NASA will note that the rules of physics require that surface warming caused by increased greenhouse gases must be accompanied by a cooling of the stratosphere, and the colder it is up there, the more that ozone is depleted. NASA is itching to marry the distant cousins of global warming and ozone depletion before election day.



So what’s wrong with a little public service that just happens to occur at the same time the Clinton administration releases its global warming national assessment?



It turns out that the assessment is based on two computer models that simply do not work. When asked to simulate how the U.S. climate should have changed in the 20th century as a result of greenhouse gases, the computer models do worse than a table of random numbers applied to the problem.



Continuing to use these models to drive a forecast of national Sturm und Drang violates the number one ethic of science: If your theory doesn’t stand the test of the facts, it must be either changed or abandoned. What about oil and global warming? Recently, James Hansen, Gore’s original guru on climate change, said that reducing fossil fuel use is an expensive proposition that will do little to reduce global warming in coming decades. Instead, he argued, we ought to concentrate on other emissions whose control is not so economically detrimental.



Imagine the cost if we had rushed to do what Gore proposed in his book Earth in the Balance. Gas prices would be as high as they are in Britain, where we have witnessed the first riots created by global‐​warming taxes and where we may also witness the fall of a government because of unpopular global‐​warming policies. But that would be the second government to collapse. Last spring, the ruling coalition in Norway went under because it was against building two power plants, on the grounds that they would contribute to global warming.



Ironically, Hansen also works for NASA; but don’t expect to see him on television in October. Nor can we expect to see NASA’s own calculations show that ozone depletion will attenuate in coming decades because of the Montreal Protocol. Instead of these truths, look for environmental gloom and doom to dominate the end of October. And just how will Bush respond, as the clock winds down?
"
"
Share this...FacebookTwitterH/T: Rudolf Kipp at Skeptical Science (see first reader comment).
That is about the same as the US annual GDP. That’s the number reported in a recent article appearing in the online Frankfurter Allgemeine Zeitung FAZ), Germany’s leading political daily.

Europe set to climax with green bloody self-flagellation.
The FAZ writes:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In the coming 40 years, the EU must invest 270 billion Euros annually if it wants to reach its long-term climate targets. That’s the result of a draft of a strategy paper that Climate Commissar Connie Hedegaard wants to introduce in early March. In order to reduce greenhouse gas emissions by 80 to 95% by 2050, large-scale investments for the expansion of renewable energies, energy grid, CCS, passive homes, electric vehicles and modern industrial plants will be needed.”
What return will European taxpayers and citizens get for this investment? Maybe a theoretical, imperceptible o.1°C of reduced warming, along with a phony belief they saved the planet. Not even the Soviets could have managed a failure of such proportions. What better way to drive an entire continent into ruin could one possibly conceive?
But the EU seems to think this is all bearable. After all, it is only 1.5% of the EU economy – about the amount paid by the EU because of the financial crisis. And it’s the way to keep Europe in the leading position as the world’s climate protector.
But it is doubtful that even the most green-eyed bureaucrats believe these targets are achievable. So they added an escape clause in the whole thing. The EU will also be allowed to count CO2 reductions achieved in third countries. I’m not clear on what that means exactly, but to me it’s an open door to accounting tricks.
Still, it is an ambitious plan, especially amid all the conference failures of the past. Indeed rather than discouraging Europe, these global failures to agree on reductions in emissions have only emboldened Europe to get even tougher. FAZ writes:
The European Parliament and enviironmental activists have repeatedly demanded that the EU unilaterally commit itself to even tougher reductions in order to give climate change negotiations more impetus.”
What do the EU bureaucrats care? After all, it’s the EU citizens who are going to have to bear all the pain. The citizens be damned. And besides, they’ll be able to enjoy the benefits of an imperceptibly cooler 0.1°C and a feeling they rescued planet. So let the self-flagellation begin.
Share this...FacebookTwitter "
"

The impact of state business taxation on employment and capital has been heavily debated in both academic and policy circles on both theoretical and empirical grounds. State‐​level business taxation could depress business activity through several channels. Businesses that might otherwise have hired or invested might simply not do so because of the difference between pre‐​tax and after‐​tax profits, or, alternatively, businesses might move their activities to another U.S. state. On the other hand, increased business taxation might not have a negative effect on business activity if businesses can change their activities to use more tax‐​favored production strategies or organizational forms, or if tax revenues are spent on public goods that improve the state’s business climate. As U.S. states face increasing fiscal pressures, the debate over the effects of state tax policy on state‐​level business activity is likely to intensify.



A long empirical literature has studied the geographic location decisions of new firms or establishments as a function of state tax and other characteristics. These studies have used aggregated panel data at the state, county, or industry level to examine the effect of state and local taxes on economic growth, employment, or capital formation. This line of work has faced two main challenges. First, tax policy is not exogenously determined, so that ascribing a causal interpretation to correlations between state tax changes and counts of businesses or employees has been problematic. The primary concern is that state governments might change tax policy in anticipation of changing economic conditions. Some work addresses this question by using county‐​level data to study how state taxation or business climates affect business activity in border counties between states that change policy and those that do not. The second challenge is that the studies have lacked comprehensive micro‐​data at the establishment level, so that the decisions of individual businesses cannot be tracked over time, leaving uncertainty as to whether firms are relocating their businesses to other regions or reducing the scale of their operations.



Our research uses comprehensive and fully disaggregated establishment‐​level data from the U.S. Census Bureau to examine the impact of state business taxation on employment and capital. We focus on firms with establishments in multiple states. To measure an effect of state tax policy on establishment counts, employment, and capital, we begin by exploiting the fact that the corporate tax code only has a direct effect on firms organized as subchapter C corporations, whereas firms organized as S corporations, partnerships, or sole proprietorships (so‐​called pass‐​through entities) are only directly affected by the individual tax code and other business taxes.



This setting allows for separate identification of the effects of the corporate tax code on the activities of C corporations and of the effects of the personal tax code on the activities of pass‐​through entities. Furthermore, the establishment‐​level micro data allow us to disentangle reallocation versus pure economic disincentives of taxation. Specifically, we examine whether firms increase their activities in a given state when taxes increase in the other states in which they are active.



We consider the complete sample of all U.S. establishments from 1977–2011 belonging to firms with at least 100 employees and having operations in at least two states. We find that a one percentage point increase (decrease) in the state corporate tax rate leads to the closing (opening) of 0.03 establishments belonging to firms organized as C corporations in the state. This corresponds to an average change in the number of establishments per C corporation of 0.4 percent. A similar analysis shows that a one percentage point change in the state personal tax rate affects the number of establishments in the state per pass‐​through entity by 0.2–0.3 percent. On the intensive margin, we find similar results. The elasticity of C corporation employment for a given establishment is 0.4 with respect to the state corporate income tax rate, and the elasticity of pass‐​through business employment is 0.2 with respect to the personal income tax rate. These effects are robust to controls for local economic conditions and heterogeneous time trends. 



Opposite effects of around half of these magnitudes are observed in response to tax changes in the other states in which firms operate, so that around half of the baseline effect is offset by reallocation of activity across states. This lends strong support to the view that tax competition across states is economically relevant and is consistent with earlier findings that emphasize the importance in the labor market of shifts in the distribution of employment opportunities across work sites.



We find no empirical correlation in the data between changes in employment at an establishment belonging to a C corporation and the personal tax rate. Similarly we find no empirical correlation between changes in employment at an establishment belonging to a pass‐​through entity and the corporate tax rate. The lack of cross‐​correlation is consistent with the identifying assumption in these regressions that there are not state‐​level trends in general business activity that follow changes in tax policy for reasons unrelated to the tax reforms. This finding also suggests that movement of activity between the corporate and noncorporate sector, while clearly important on the national level over the past several decades, plays a somewhat limited role in shaping the overall economic response to state‐​level tax changes.



Further analysis captures complexities, heterogeneity, and changes in state tax codes regarding apportionment of income in multi‐​state firms. If a company has physical presence in more than one state, the company has to apportion its profits according to each state’s apportionment factor weights for property, payroll, and sales. Furthermore, so‐​called throwback or throw‐​out rules at the state level require companies to apportion profits arising from sales into states where they have no physical presence back to states where they do. 



We show that the response of moving establishments and employees is greatest when those factors have greater apportionment weights. Even if the sales apportionment factor is large, we also find strong effects when throwback or throw‐​out rules are in effect, as these rules mitigate the tax attractiveness of firm moves to high sales‐​apportionment states.



We further examine whether there is evidence of confounding differential trends in C corporations versus pass‐​through entities in the years leading up to tax changes in a subsample of firms affected by states that changed at least one of their tax rates by at least 100 basis points. These large tax changes occurred 161 times during the sample period. Here we find similar directional results of somewhat smaller magnitude. Around half of the effects are felt in the tax year in which the tax rate changed, with the full force being felt in the following year.



Analysis on the subset of the Census data on manufacturing firms allows us to consider the impact of state taxation on capital formation and location. We find that capital shows similar directional patterns to labor in its response to taxation, but that the elasticities are 36 percent smaller. Furthermore, the detailed data on the location of manufacturing firm property allow us to consider the impact of a state policy that raises the actual tax claim on a dollar of total corporate profit by one percentage point, as opposed to increases in the statutory rate by one percentage point. Under this definition, which captures cross‐​sectional firm heterogeneity in the extent to which statutory changes affect the tax burden, we find somewhat larger elasticities of around 0.4 percent for labor and 0.3 percent for capital.



One concern about the analysis might be that the results could be affected by firms that change their organizational form in response to changes in the tax code. Earlier work finds that the share of economic activity by firms in corporate form does in fact respond to the relative taxation of personal to corporate state income. However, our sample in this paper consists only of firms with activity in more than one state, and firms must choose one organizational form that will be applicable to all entities. For these firms, federal tax policy should be far more important for the organizational form decision than the mix of state tax policies they face, a hypothesis we confirm in the data. Limiting the sample to the 92 percent of observations belonging to firms that do not change their organizational form within 5 years of tax changes leaves the results unaffected.



Overall, our findings on the effects of corporate taxation are larger than those found in work that has examined the impact of tax policy at the national level. Some of this difference may be attributable to differences in the measurement of corporate tax rates (average versus marginal), the level of analysis (state versus federal), the identification strategy and the distinction between GDP per capita and the variables we consider. That said, in our analysis, tax competition across states roughly doubles the baseline effects that would be found in the absence of firms’ ability to move across states, and for that reason we would extrapolate that the impact of state policy on state business activity should be about double the impact of federal business taxation on federal business activity.



 **Note**



This research brief is based on Xavier Giroud and Joshua Rauh, “State Taxation and the Reallocation of Business Activity: Evidence from Establishment‐​Level Data,” National Bureau of Economic Research Working Paper no. 21534, September 2015.
"
"License holders will be allowed to kill some of Britain’s most endangered bird species under temporary permits licensed by Natural England and Natural Resources Wales. The monitoring and enforcement of these permits relies on self-reporting and regulation – loopholes which could be exploited to feed the demand for illegal bird products in Europe. The birds at risk throughout England and Wales include species whose numbers are threatened in the UK, according to the RSPB (Royal Society for the Protection of Birds). Bullfinches, meadow pipits and oystercatchers are all included in the permits and are amber-listed for intermediate conservation priority. Another species, the skylark, will be subject to licensed killing despite the RSPB red-listing it as a critical conservation priority for the UK. Both Natural England and Natural Resources Wales are sponsored by central government and are responsible for “promoting nature conservation” and “protecting people and the environment” according to their websites. They cite safety concerns to justify granting the permits and claim killing birds could prevent damage to crops and reduce interference with air traffic. Although the permits strictly outline the overall number of birds that are allowed to be killed, monitoring and enforcing this will be crucial. By licensing the shooting, trapping, and killing of songbirds in the UK, the government could be offering a route for supplying dead birds to the illicit trade across Europe. The illegal bird trade within the European Union is thought to be worth at least €10m a year. This doesn’t just refer to the trade in exotic species from outside Europe, but includes the widespread trade of songbirds for human consumption – particularly in parts of France and northern Italy where songbirds are regarded as forbidden delicacies. Dishes such as ortolan bunting or polenta ucelli – polenta with roasted songbird – are synonymous with luxury and prestige. Other songbirds such as sparrows and thrushes are trapped and eaten throughout Italy and Cyprus. The trade in songbirds makes for quick profits: one gram of songbird meat is estimated to sell for the equivalent of one gram of marijuana. The trapping and consumption of songbirds is widely illegal across the European Union, but it still occurs illegally in some member states such as France and Italy. Although the 1981 UK Wildlife and Countryside Act forbids wild birds being sold for food or shot for sport, enforcing this rule may be difficult to guarantee under the permits. Recent seizures illustrate that although the European bird trade is a lucrative branch of the illicit global trade in wildlife, it was neither part of the discussions nor mentioned in the declaration released after the London Conference on the Illegal Wildlife Trade in October 2018. The omission highlights that policy makers often turn a blind eye to Europe as a thriving market for illegal wildlife products, particularly those items that are “less attractive” or of a lower profile than rhino horn or ivory.  Demand driven by European consumers is one of the key issues that policymakers  often fail to address. The UK government has tried to position itself as a world leader on this issue, but its decision to grant licences to shoot endangered bird species – even in small numbers – undermines this. Even strict conservation laws are often difficult to enforce as legal and illegal activities become intertwined. For example, licensed hunters may hold the correct permits, but might use illegal methods, such as using calling devices or decoys to attract birds in greater numbers.  While the illegal killing of birds has received much more attention in the last couple of years, we still know too little about its commercial side. What drives demand? How are illegal products trafficked? How much profit do they bring in and what groups benefit? Which bird species are trafficked for human consumption and which for the pet market? There is simply a lack of official and up-to-date data on the networks, routes and products of this trade. Due to such uncertainty, there is a failure to grasp how changes in conservation laws in the UK could export criminal activity to other regions. A good example is the flourishing hunting tourism industry in Serbia, which caters particularly to hunters from Italy and Malta who have shifted their hunting abroad to avoid strict regulations in place at home. If a door is closed to supplying this trade, another is opened elsewhere, and so the source simply moves. There is cause to worry that songbird killing and trapping opportunities in the UK could prove to be another supply route for the pan-European trade. Policing those who are issued permits may encounter the same problems that regulating the protection of birds has met elsewhere in the UK. The RSPB published a report in 2017 that found a striking 67% of crimes against birds of prey in the UK were committed by gamekeepers and that self-regulation had evidently failed.  Other studies also found a correlation between the persecution of birds of prey and grouse shooting. For example, where birds of prey pose a hindrance to grouse shooting, gamekeepers may destroy their nests. Although complex structures are in place to prosecute wildlife crime in the UK, the RSPB found that in 2017 only four out of 68 cases of crimes against birds of prey were prosecuted. Given these difficulties in enforcing existing regulations, how can we be certain that those songbirds which are legally trapped or killed in the UK do not feed the demand for illegal songbirds in European restaurants? The seizures of illegal songbird shipments from the Balkans into the EU have shown that geographic distance and national borders pose no significant obstacle. Due to these challenges in monitoring and enforcing the regulations, the government’s permission to grant licenses to kill endangered bird species in Britain – even for public health or safety reasons – not only threaten bird populations but also undermine efforts to tackle the illegal trade in bird products in Europe. This article was amended on February 4 2019 to clarify the role of the permits and the responsibilities of Natural England and Natural Resources Wales."
"
Share this...FacebookTwitterBack in January I wrote about how the Arctic had gained 2000 cubic kilometres in ice volume. This was calculated by comparing the sea ice thickness of January 2008 to that of January 2011, see the following chart.
Mid-January Arctic sea ice thickness comparison of 2008 to 2011.
Blue color shows thin ice, while green shows thicker ice. Clearly the Arctic’s ice was much thicker in Jan-2011 than it was three years ago in Jan-2008. There is about twice as much solid green in 2011 than in 2008, thus yielding the net gain of about 2000 cubic kilometers.
5000 cubic km more sea ice?
Now that we’ve just past the March-peak in Arctic sea ice area, I thought it’s a good time to take another look at the Arctic sea ice thickness again. What follows is a comparison of March 23, 2008 (the old death spiral days) and March 25, 2011.
Source of charts: http://www7320.nrlssc.navy.mil/pips2/ithi.html
Well lo and behold, we see yet another huge increase. That ice thickness increase has grown even more.
Note the huge difference in just three years. Today practically the entire Arctic cap is green…meaning an average thickness of over 3 meters. In 2008 the average thickness down to about 2m. This is all further confirmed by the current Catlin Arctic Survey (h/t: tomnelson.blogspot).
Some of the ice we crossed was really thick multi-year ice that was just too thick to drill through. Our drill goes to four and a half metres and it wasn’t breaking through. In other areas we were able to drop our measuring devices down to a depth of 200 metres below the floating ice.”
I wonder if Mark Serreze is taking note. Hopefully he’ll behave as the scientist he claims to be and make the observation.
I haven’t systematically calculated the March differences above, but you can assume that the core cap itself has an area of 5 million square km. Now multiply that times one meter of added thickness and you get a volume growth of 5000 cubic km!
Okay, that’s just rough guesstimating (perhaps) on the high side, but the ice gain is whopping no matter what. Dr. “Freeze” Serreze can make the calculation himself. Predictions of higher sea ice extents come September are well-founded. The recovery appears to be well on the way. Arctic warming has disappeared.
Share this...FacebookTwitter "
"

From ETH in Zurich, this interesting essay on the last glacial period has some interesting points to ponder. h/t to Sid Stafford – Anthony

The last glacial period was characterised by strong climatic fluctuations. Scientists have now been able to prove very frequent and rapid climate change, particularly at the end of the Younger Dryas period, around 12,000 years ago. These fluctuations were accompanied by rapid changes in circulation in the oceans and the atmosphere.

Researchers are able to determine when glaciers were stable and when they melted by studying titanium content in glacial lake sediments. (Picture: siyublog/flickr)

Sediment deposits in lakes are the climate archives of the past. An international team of researchers from Norway, Switzerland and Germany have now examined sediments originating from the Younger Dryas period from the Kråkenes Lake in northwest Norway. In the sediments, they found clues that point to a “climate flicker” at the end of the last glacial period, oscillating between colder and warmer phases until the transition to the stable climate of the Holocene, our current interglacial period. The short-term, strong fluctuations of the Younger Dryas would have dwarfed the “extreme weather phenomena” seen today, according to Gerald Haug, professor at the Department for Earth Sciences at ETH Zürich and co-author of the study, which was published online yesterday in “Nature Geoscience”.
Seasonal sediment deposits
Seasonal sediment accumulation, for example, gave scientists clues to these strong climate fluctuations. They can be read in lakes in a similar way to reading rings on trees. In warmer phases and melting glaciers, the accumulation of sediments increases. More clues on the changes in glacier growth were given by the element titanium, which is present in the sediments. Glaciers erode their bedrock, and in doing so concentrate the titanium contained in the sediments they are carrying. The sediments containing titanium are washed into the glacier’s draining lakes in the meltwater. The amount of sediment and the titanium content can therefore allow us to deduce when the glaciers were stable and when they melted. The researchers interpreted the maxims, recurring every 10 years, as phases of strong glacier activity caused by temperature fluctuations and thus as warmer times.
A seemingly self-preserving cycle
The scientists also examined a sediment core from seabed deposits of the same age in the North Atlantic. They reconstructed the original temperature and salt concentration of the water based on microfossils and the oxygen isotope ratio in the sediment. It was shown that the results from the lake sediments corresponded to those from the sea sediments. “The melting of glaciers was caused by the warm Gulf stream advancing into this region,” Gerald Haug explains. This increase in temperature caused the west winds to shift to the north and brought warm air to northern Europe. However, the meltwater draining into the Atlantic lowered the salt concentration and the density of the surface water, changing the convection in the ocean, which in turn allowed new sea ice to form. Subsequently, the Gulf Stream and the west winds were again forced out of the North Atlantic area and the region cooled down once again. These processes were repeated for around 400 years, until the current interglacial period was able to stabilise itself.
The Würm glaciation began around 100,000 years ago and lasted until around 10,000 years ago. In this period, there were strong fluctuations between warm and cold phases, particularly in the North Atlantic area. The Younger Dryas, which ushered in the current interglacial period, is one of the best-known and best-researched abrupt climate changes of that glaciation. It began around 12,900 years ago and at first caused an abrupt temperature drop in the northern hemisphere, as well as a temperature rise of up to 10°C in less than 20 years towards the end, around 11,700 years ago.
Unclear mechanisms
Up until now, there have been several studies which document the glacial conditions during the Younger Dryas period of 1,200 years. However, the mechanisms which caused it, sustained it and finally led to an interglacial period have yet to be fully understood. The researchers believe that further high-resolution studies of this type could give insights into how glacial periods are triggered and how they are brought to an end.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9812b02e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

It’s nearly impossible to read major newspapers, magazines, or online publications in recent months without encountering a plethora of articles contending that the United States is turning inward and “going alone,” “abandoning Washington’s global leadership role” or “retreating from the world.” These trends supposedly herald the arrival of a new “isolationism.” The chief villain in all of these worrisome developments is, of course, Donald Trump. There is just one problem with such arguments; they are vastly overstated bordering on utterly absurd.



President Trump is not embracing his supposed inner isolationist. The policy changes that he has adopted regarding both security and international economic issues do not reflect a desire to decrease Washington’s global hegemonic status. Instead, they point to a more unilateral and militaristic approach, but one that still envisions a hyper‐​activist U.S. role.



For instance, it’s certainly not evident that the United States is abandoning its security commitments to dozens of allies and clients. Despite the speculation that erupted in response to Trump’s negative comments about the North Atlantic Treaty Organization (NATO) and other alliances during the 2016 election campaign (and occasionally since then), the substance of U.S. policy has remained largely unchanged. Indeed, NATO has continued to expand its membership with Trump’s blessing—adding Montenegro and planning to add Macedonia.





If you look at his actions and not his words, you won’t find it.



Indeed, Trump’s principal complaint about NATO has always focused on European free‐​riding and the lack of burden‐​sharing, not about rethinking the wisdom of the security commitments to Europe that America undertook in the early days of the Cold War. In that respect, Trump’s emphasis on greater burden‐​sharing within the Alliance is simply a less diplomatic version of the message that previous generations of U.S. officials have tried sending to the allies.



Moreover, Trump’s insistence at the July NATO summit in Brussels that the European nations increase their military budgets and do more for transatlantic defense echoed the comments of President Obama’s Secretary of Defense Chuck Hagel in 2014. Hagel warned his European counterparts that they must step up their commitment to the alliance or watch it become irrelevant. Declining European defense budgets, he emphasized, are “not sustainable. Our alliance can endure only as long as we are willing to fight for it, and invest in it.” Rebalancing NATO’s “burden‐​sharing and capabilities,” Hagel stressed, “is mandatory—not elective.”



Additionally, U.S. military activities along NATO’s eastern flank certainly have not diminished during the Trump administration. Washington has sent forces to participate in a growing number of exercises (war games) along Russia’s western land border—as well as in the Black Sea—to demonstrate the U.S. determination to protect its alliance partners. Trump has even escalated America’s “leadership role” by authorizing the sale of weapons to Ukraine —a very sensitive step that President Obama carefully avoided.



Trump even seems receptive to establishing permanent U.S. military bases in Eastern Europe. During a state visit to Washington in mid‐​September, Poland’s president, Andrzej Duda, promised to provide $2 billion toward construction costs if the United States built a military base in his country. Duda even offered to name the base “Fort Trump.” Trump’s reaction was revealing. Noting that Poland “is willing to make a very major contribution to the United States to come in and have a presence in Poland,” Trump stated that the United States would take Duda’s proposal “very seriously.” _American Conservative_ columnist Daniel Larison notes that while Trump often is accused of wanting to “retreat” from the world, “his willingness to entertain this proposal shows that he doesn’t care about stationing U.S. forces abroad so long as someone else is footing most of the bill.”



U.S. military activism does not seem to have diminished outside the NATO region either. Washington persists in its futile regime‐​change campaign in Syria, and it continues the shameful policy of assisting Saudi Arabia and its Gulf allies pursue their atrocity‐​ridden war in Yemen. Both of those Obama‐​era ventures should have been prime candidates for a policy change if Trump had wished to decrease America’s military activism.



There are no such indications in Europe, the Middle East, or anywhere else. The U.S. Navy’s freedom of navigation patrols in the South China Sea have actually increased in size and frequency under Trump—much to China’s anger . Washington’s diplomatic support for Taiwan also has quietly increased over the past year or so, and National Security Advisor John Bolton is on record suggesting that the United States move some of its troops stationed on Okinawa to Taiwan. The U.S. military presence in Sub‐​Saharan Africa is increasing, both in overall size and the number of host countries.



Those are all extremely strange actions for an administration supposedly flirting with a retreat from the world to be adopting. So, too, is Trump’s push for increases in America’s already bloated military budget, which now exceeds $700 billion—with even higher spending levels on the horizon.



Accusations of a U.S. retreat from the world on non‐​military matters have only slightly greater validity. True, Trump has shown little patience for multilateral arrangements such as the Trans‐​Pacific Partnership, the Paris climate agreement, or the United Nations Human Rights Council that he concluded did not serve America’s national interests. On those issues, the president’s actions demonstrated that his invocation of “America First” was not just rhetoric. However, regarding such matters, as well as the trade disputes with China and North Atlantic Free Trade Agreement partners, the administration’s emphasis is on securing a “better deal” for the United States, not abandoning the entire diplomatic process. One might question the wisdom or effectiveness of that approach, but it is a far cry from so‐​called isolationism.
"
"It is 2050. We have been successful at halving emissions every decade since 2020. We are heading for a world that will be no more than 1.5C warmer by 2100 In most places in the world, the air is moist and fresh, even in cities. It feels a lot like walking through a forest and very likely this is exactly what you are doing. The air is cleaner than it has been since before the Industrial Revolution. We have trees to thank for that. They are everywhere.  It wasn’t the single solution we required, but the proliferation of trees bought us the time we needed to vanquish carbon emissions. When we started, it was purely practical, a tactic to combat climate crisis by relocating the carbon: the trees took carbon dioxide out of the air, released oxygen and put the carbon back where it belongs, in the soil. This, of course, helped to diminish climate crisis, but the benefits were even greater. On every sensory level, the ambient feeling of living on what has again become a green planet has been transformative, especially in cities. Reimagining and restructuring cities was crucial to solving the climate challenge puzzle. But further steps had to be taken, which meant that global rewilding efforts had to reach well beyond the cities. The forest cover worldwide is now 50% and agriculture has evolved to become more tree-based. The result is that many countries are unrecognisable, in a good way. No one seems to miss wide-open plains or monocultures. Now we have shady groves of nut and orchards, timber land interspersed with grazing, parkland areas that spread for miles, new havens for our regenerated population of pollinators. A major part of the shift to net-zero emissions was a focus on electricity; achieving the goal required not only an overhaul of existing infrastructure but also a structural shift. In some ways, breaking up grids and decentralising power proved easy. We no longer burn fossil fuels. Most of our energy now comes from renewable sources such as wind, solar, geothermal and hydro. All homes and buildings produce their own electricity – every available surface is covered with solar paint that contains millions of nanoparticles, which harvest energy from the sunlight, and every windy spot has a wind turbine. If you live on a particularly sunny or windy hill, your house might harvest more energy than it can use, in which case the energy will simply flow back to the smart grid. Because there is no combustion cost, energy is basically free. It is also more abundant and more efficiently used than ever. Homes and buildings all over the world are becoming self-sustaining far beyond their electrical needs. For example, all buildings now collect rainwater and manage their own water use. Renewable sources of electricity make possible localised desalination, which means clean drinking water can now be produced on demand anywhere in the world. We also use it to irrigate hydroponic gardens, flush toilets and shower. Petrol and diesel cars are anachronisms. Most countries banned their manufacture in 2030, but it took another 15 years to get internal combustion engines off the road completely. What’s strange is that it took us so long to realise that the electric motor is simply a better way of powering vehicles. It gives you more torque, more speed when you need it, and the ability to recapture energy when you brake and it requires dramatically less maintenance. We also share cars without thinking twice. In fact, regulating and ensuring the safety of driverless ride sharing were the biggest transportation hurdles for cities to overcome. The goal has been to eliminate private ownership of vehicles by 2050 in major metropolitan areas. We’re not quite there yet, but we’re making progress. We have also reduced land transport needs. Drones organised along aerial corridors are now delivering packages, further reducing the need for vehicles. Thus we are currently narrowing roads, eliminating parking spaces and investing in urban planning projects that make it easier to walk and bike in the city. While we may have successfully reduced carbon emissions, we’re still dealing with the aftereffects of record levels of carbon dioxide in the atmosphere. The long-living greenhouse gases have nowhere to go other than the already-loaded atmosphere, so they are still causing increasingly extreme weather, though it’s less extreme than it would have been had we continued to burn fossil fuels. Glaciers and Arctic ice are still melting and the sea is still rising. Severe droughts and desertification are occurring in the western United States, the Mediterranean and parts of China. Ongoing extreme weather and resource degradation continue to multiply existing disparities in income, public health, food security and water availability. But now governments have recognised climate crisis factors for the threat multipliers that they are. That awareness allows us to predict downstream problems and head them off before they become humanitarian crises. Everyone understands that we are all in this together. A disaster that occurs in one country is likely to occur in another in only a matter of years. It took us a while to realise that if we worked out how to save the Pacific islands from rising sea levels this year, then we might find a way to save Rotterdam in another five years. The zeitgeist has shifted profoundly. How we feel about the world has changed, deeply. And, unexpectedly, so has how we feel about one another. When the alarm bells rang in 2020, thanks in large part to the youth movement, we realised that we suffered from too much consumption, competition, and greedy self-interest. Our commitment to these values and our drive for profit and status had led us to steamroll our environment. As a species, we were out of control and the result was the near-collapse of our world. We emerged from the climate crisis as more mature members of the community of life, capable of not only restoring ecosystems but also of unfolding our dormant potentials of human strength and discernment. Humanity was only ever as doomed as it believed itself to be. Vanquishing that belief was our true legacy. • This is an edited extract from The Future We Choose: Surviving the Climate Crisis by Christiana Figueres and Tom Rivett-Carnac, published by Manilla Press (£12.99). To order a copy go to guardianbookshop.com. Free UK p&p over £15 • Christiana Figueres and Tom Rivett-Carnac will be in conversation at a Guardian Live event at the Royal Geographical Society, London SW7, on Tuesday 3 March, 7pm"
"
Share this...FacebookTwitterSweden’s English language The Local has the following headline today:

Coldest December in Sweden in 110 years
The last few days of the year look to be very cold throughout Sweden, according to a forecast by the Swedish meteorological agency SMHI. 
This means that several parts of Sweden, including the southern region Götaland and eastern Svealand, will have experienced the coldest December in at least 110 years.”
Read the complete article here.
This reality of course flies in the face of what climate models had predicted earlier. The SMHI (Sweden’s Met Office and devout warmist organisation) keeps archives, and so I thought surely there must be something there that had earlier forecast warmer winters for Sweden. I didn’t have to look very long to find it.
First there’s this report dated 16 September 2010 here: New climate projections indicate more extreme weather. Here are just a couple of excerpts (Warning – you might first want to tie your butt to yourself to keep from laughing it off!):
New climate projections for severe weather situations in 100 years also show that truly cold days will virtually disappear.”
And:
The new scenarios show the effects of global warming with more details than before, thanks to more computer power and high geographical resolution.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




And:
‘As a whole, the new ensembles are an important foundation for continued climate research. However, they can already be applied to many areas,’ says Grigory Nikulin.”
Does he mean like governments preparing for winters? And finally:
Truly cold weather, such as -10°C in Spain or -30°C in southern Sweden, is unlikely to occur in future.”
How stupid must they feel now? The assertions made above likely stem in part from an SMHI-published report 2 years ago called: Temperature and precipitation changes in Sweden; a wide range of model-based projections for the 21st century.
The report analyzed the climate change signal for Sweden in scenarios for the 21st century in a large number of coupled atmosphere-ocean general circulation models (AOGCMs), used in the AR4 by the IPCC.
At the SMHI Rossby Centre, regional climate models were run under different emission scenarios and driven by a few AOGCMs. They used the results of the runs as a basis in climate change in Sweden. What did they find? (Crap, of course, but read it for yourself):
Projected responses depend on season and geographical region. Largest signals are seen in winter and in northern Sweden, where the mean simulated temperature increase among the AOGCMs (and across the emissions scenarios B1, A1B and A2) is nearly 6°C by the end of the century, and precipitation increases by around 25%. In southern Sweden, corresponding values are around +4°C and +11%.
Okay, it’s still a long way to the end of the 21st century. But as Sweden’s 2010 December-of-the-century shows, the models and calculations seem to have forgotten a few important details. Back to the drawing board!
Share this...FacebookTwitter "
"
Previously on WUWT we discussed the media’s fascination with “melt” when it comes to ice shelves cracking off. Then there’s also this picture that keeps getting recycled.

http://www.ogleearth.com/wissm.jpg
It is clear from the photo above that we see a stress crack, not a melt. Now we have a time lapse satellite photo series of the Wilkins ice shelf that shows the process of currents and winds causing those stresses.
Mike McMillan writes:
Fox News is reporting that the Wilkins ice shelf bridge that’s been eroding  has finally collapsed.
http://www.foxnews.com/story/0,2933,518374,00.html
I  went back to the old ESA sat photos and noticed something interesting.   I downloaded the gif animation and did some highlighting.

In  the upper area, the shelf was previously fractured, then glued together  by new ice.  I highlighted a string of drift ice in green to show what  the currents were doing during the previous collapse.  The current runs down  from the top, compressing the fractured shelf and likely busting up the new  ice glue.  The current then reverses, pulling the fractured shelf ice out to  sea. The green drift ice looks almost like a fingertip crunching into the  shelf, and clearly shows the compression.
A different process works on  the lower side of the ice bridge.  A gyre pulls
off chunks of unfractured  ice.  I’ve highlighted a chunk of non-edge ice in
pink, and we can watch it  tumble out along with a companion berg.  Note the
sea immediately refreezes  in the open areas.  One of the gif frames shows the
gyre swirling the new  ice, and I’ve enlarged the frame.

http://i40.tinypic.com/erg287.jpg
UPDATE: I slowed down the original animation to 1 frame per second, with a 2 second pause at end, per requests in comments. -Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9678f743',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
WUWT readers may remember last year that we had an early outbreak of Tornado season, and media opportunist Senator John Kerry immediately jumped at the chance to blame the weather event on “global warming” as we reported here on WUWT:
Kerry appeared on MSNBC on February 6 to discuss storms that have killed at least 50 people throughout the Southeastern United States. So, of course, Kerry used the platform to advance global warming alarmism.
“[I] don’t want to sort of leap into the larger meaning of, you know, inappropriately, but on the other hand, the weather service has told us we are going to have more and more intense storms,” Kerry said. “And insurance companies are beginning to look at this issue and understand this is related to the intensity of storms that is related to the warming of the earth. And so it goes to global warming and larger issues that we’re not paying attention to. The fact is the hurricanes are more intensive, the storms are more intensive and the rainfall is more intense at certain places at certain times and the weather patterns have changed.”
See the original WUWT report here.
So, this year is a little bit different. We have a late and slow start to tornado season. Always a good thing. That being said, this report from Joe D’Aleo discusses why its been slow, and debunks a recent Weather Channel claim that the current deficit of tornadoes has something to do with “global warming”.  Seeing how global warming causes both individual tornado events and decreased tornado events, I’ve apparently terribly underestimated its omnipotent power to influence weather. 😉

Graph from NWS/NOAA. Smaller (F1) tornadoes seem to be on the increase, but not larger ones.
Even though tornado reports seem to be on the rise, the larger damaging tornados, F2-F5 don’t seem to be. There are some good reasons for this, and it might be a good primer for readers to revisit this report I made about the issue of tornado reporting:
Increasing tornadoes or better information gathering?
BTW, if anyone wants a really cool weather radar program for tracking severe storms, please see my StormPredator program here – Anthony

Tornado Season So Far Not as Bad as 2008
By Joseph D’Aleo, CCM, on Intellicast
After another La Nina season with again a lot of snow and precipitation in the north central, another relatively active tornado season was expected and so far it has delivered on that promise. However given the La Nina was not as strong and the rebound in the Pacific towards El Nino is a month earlier than last year, the number of storms so far, have been less. It looks like May will fall well short of last May’s 461 tornadoes.

Cedar Hill, Texas, Photo credit Pat Skinner, TTU
The annual summary to date can be found here. The tornadoes so far in 2009 have been in the southeast quadrant of the nation. Climatologically, that is where the season normally begins.

See larger image here.
In 2008, the tornadoes when all was said and done, were found in all but 4 of the lower 48 states.

See larger image here.
As we move into summer, expect the activity to shift north with the jet stream. The march of the season – climatology of tornadoes normally follows this depiction (source here).
We are below the 5 year average for tornadoes for the season to date, below all but 2005.

See larger image here.
The activity was as usual concentrated in what is called tornado alley in the plains, Midwest to the Gulf. You can see in 2008 that the daily events peaked in May with the biggest day on the 23rd of May before falling off in summer as El Nino like conditions developed in the eastern Pacific. That is occurring a month earlier this year and perhaps, that explains the quieter May.

See larger image here.
The activity was as usual concentrated in what is called tornado alley in the plains, Midwest to the Gulf.

The reason that this region is most vulnerable is that this is where the combination of moisture from the Gulf of Mexico, dry air in mid levels from Mexico, a strong jet with cold air aloft coming out of the Rockies and a boundary between still cool air to the north and the warm humid summer like air in the south all come together.  Read more and see some useful links here. 
By the way, last year, the alarmist media blamed the increase in tornados on global warming. Well guess what this year, stormchasers across Tornado Alley have been frustrated this spring by what seems to be a lack of tornadoes and severe weather.  Indeed, VORTEX2, the largest tornado field study ever, has been running for more than two weeks now and has not seen one twister.  Last week, Weather Channel Senior Meteorologist Stu Ostro speculated that global warming was the cause. Of course this is the normal, mother nature has a perverse sense of humor – projects to study east coast storms had no east coast storms that winter, just one passing front. You just knew that VORTEX2 would lead to a lack of storms to study. We need them to schedule a massive study of hurricanes and we will surely have a dud season.
As to Stu’s reasoning, a big ridge would lead to heat and dryness across the central states. Instead the region has been hit hard with a steady stream of progressive troughs with heavy rainfall and very little warmth (fuel for storms). These storms brought more toirnadoes in April 2009 than in April 2008.

See larger image here.
The real issue may instead be that the El Nino like conditions that came in late May last year shutting off severe weather activity in June 2008 came on a month earlier this year and severe weather activity has diminished in May.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e961a4c69',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The Intergovernmental Panel on Climate Change (IPCC) is set to publish the “synthesis report” of its fifth assessment period, drawing on three individual working group reports already published on: the physical science of climate change; climate impacts and adaptation; and mitigation, or how to reduce emissions or enhance the natural uptake of greenhouse gases (GHGs).  The latest report is the first collective assessment of climate change by governments since the 2007 report, published just as the world fell off a financial and economic cliff. It is therefore a vital input to the current round of UN climate negotiations culminating in Paris next year. IPCC reports can be over 1,000 pages long – and no one seriously expects even the most diligent of politicians to read through the whole thing. To make life easier, each report includes a “Summary for Policymakers”. However this summary is subject to line-by-line agreement by representatives of the 195 government members. There is therefore always a risk that the IPCC’s policy messages might not adequately reflect the underlying science. This isn’t just a theoretical possibility. The summary of the working group report on climate mitigation published earlier this year left out an important analysis of country emissions by income grouping, which differs from the standard binary classification – developed and developing – used in the UN climate negotiations.  No doubt some governments were concerned that this income-based analysis might imply the need for greater action on their part. In terms of the physical science, there are perhaps three key headline messages: human influence on the climate system is clear; warming of the climate system is unequivocal; limiting the risks from climate change will require substantial and sustained reductions of GHG emissions. The present pause in the rise of the global mean surface temperature does not mean we do not need to be concerned.  Of the 14 warmest years on record, 13 have occurred this century and 2014 looks likely to be another very warm year globally. Atmospheric concentrations of carbon dioxide are still growing rapidly. On current trends, by the end of the century, the average surface temperature of our planet is as likely as not to have increased by 4°C relative to pre-industrial conditions, with greater increases nearer the poles.  Worryingly, we are fundamentally changing the climate system, raising the likelihood of severe, pervasive, and irreversible impacts on society and the natural systems on which we all depend. Even the modest warming so far (0.85°C since the second half of the 19th century) has had a clear impact. Mitigating our use of fossil fuels lies right at the heart of an effective response to climate change. While a 2°C target remains technically feasible, achieving it will be extremely challenging. The IPCC’s mitigation report compared hundreds of energy modelling scenarios that strongly suggest that to achieve a 2°C target, global GHG emissions would need to be around 40-70% lower than 2010 levels by 2050 and near zero by 2100.  This would require rapid improvements in energy efficiency and at least a tripling of the share of zero- and low‐carbon energy supply by 2050. The longer action is delayed and the lower the availability of key low-carbon technologies, such as carbon capture and storage, the less feasible achieving a 2°C target becomes. Internationally agreed action on climate is difficult to achieve and we are rapidly running out of time to limit global warming to 2°C.  At some point soon it may become impossible in practical terms. The IPCC highlights the fact that climate risk increases with the cumulative level of carbon emissions over time, not just the emissions in any given year. So reversing rising emissions is a first necessary step to limit these catastrophic risks.   An effective response requires action across all major countries, not just the developed world. The international community now needs to build on – and go further than – the 2009 Copenhagen Accord, which was a paradigm shift that stimulated emissions-reduction pledges to 2020 from a wide range of countries – including China and India. National emissions-reduction contributions beyond 2020 are due to be made in early 2015. The major developed economies will need to make serious commitments on finance as well as emissions; the recent EU summit decision on its 2030 target should help. China now emits more CO2 per capita than the EU, while India’s per capita emissions are still less than 40% of the world average – what works for one won’t be necessarily be appropriate for the other, yet action by both is critical.    The 2015 Paris summit is therefore an important moment, but it will not be able – and is not intended – to provide the final answer. The summit will make more progress if it focuses on near-term operational targets, such as achieving a peak in global emissions by a certain date rather than triggering a damaging row over how the small budget of future cumulative CO2 emissions consistent with a 2°C target should be shared among nations.  Aiming to pass “peak emissions” by a certain point would provide a coherent framework for assessing individual national contributions and negotiating the conditions under which they could be improved. It could also allow continued growth of emissions in the poorer developing countries for some time to come, which is essential.  Governments are beset by many other challenges and focused on the short term. The next climate agreement must therefore also build in the flexibility to enhance our response in the light of evolving scientific and political developments."
"
Share this...FacebookTwitter!!! UPDATE: Read the Europol Press Release here !!!
– Estimated 5 billion euros in damage for European taxpayers
– Massive fraud involving criminal networks / Middle East
Hat tip Reader Dirk H.
Here’s more proof that trading of CO2 emission certificates is fraught with fraud and attracts seedy criminal organizations – all costing the consumers and taxpayers billions.
Worse yet, it has spread out of control and appears that the authorities can’t keep up.
The Austrian online Kleine Zeitung here reports that Europol have raided an elaborate CO2 emissions scam in Italy and have arrested more than 100 persons.
The Kleine Zeitung writes: “The damage runs in the billions of euros”.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




According to Europol, the Italian tax authorities, directed by the Milan Prosecutor’s Office, have raided 150 companies in Italy. The fraud involves evasion of value added tax with CO2 emission certificates. More than 100 have been arrested and are suspected of being involved in organised crime.
The Kleiner Zeitung reports that the Italian Electric Utilities trading markets had earlier halted entire trading with emissions certificates “because a high number of abnormal transactions”. The loss in tax revenue just from VAT (MTIC (Missing Trader IntraCommunity Fraud) alone is estimated to be 500 million euros, the online Kleine Zeitung writes.
The fraud is widespread
According to reports, it’s been known since June of last year that criminal organizations have been using CO2-emissions trading for defrauding governments of value added tax.
This is not the first time that police raids of this scale have taken place. It’s the latest in a series of raids that have been carried out all over Europe this year, all involving the trading of CO2 emission certificates. It seems the authorities just can’t keep up with the multitudes of swindlers out there.
Norway, Switzerland and the EU countries Belgium, Czech Republic, Denmark, Latvia, the Netherlands, Slovak Republic and Portugal are all among the countries trying to identify the network of criminals behind this massive fraud – a fraud with links to criminal networks operating outside the EU and in other continents, like the Middle East.
2500 investigators – trying to identify. That’s means they haven’t yet. That’s a lot of fraud. The fraud has spread from science to finance. Expect a meltdown – sooner than later.
UPDATE 2: Recall this Danish fraud: http://climaterealists.com/index.php?id=6790
Share this...FacebookTwitter "
"Behind the scenes, away from the Commons chamber and the TV cameras, the UK’s parliamentary committees continue to debate and scrutinise the mountain of legislation needed in order to secure an orderly exit from the EU. Among the legislation currently under consideration is the government’s proposed Draft Environment (Principles and Governance) Bill. The bill itself is an attempt to fill some of the gaps that will inevitably emerge in UK environmental law once the country leaves the EU. For instance, the European Commission currently has the ability to take member states to the EU Court of Justice, as it has threatened to do with the UK, which is not presently not complying with air pollution limits. After Brexit, the EC would have no such power. Overall, there are several encouraging aspects to what is the first general environmental bill before parliament in more than 20 years. They include, for example, the first explicit incorporation into UK law of the environmental principles that have shaped EU environmental law.  Another positive initiative is the proposal to create a new “watchdog” in the form of an office for environmental protection whose job it will be to monitor the implementation of environmental law. On closer scrutiny, though, the bill is a missed opportunity. First, the incorporation of environmental principles into UK law is unlikely to provide the safe harbour that many commentators hope they do. The bill does require the environment secretary to prepare a statement setting out his or her interpretation of the principles. But, while other government ministers must then take the statement into consideration – they do not necessarily have to obey it. The duty also applies only to central government ministers and not public authorities more widely. In any case, environmental principles are rarely able to dictate a specific eco-friendly outcome. This is because they do not “establish” formal legal rules but instead invite a decision maker to consider competing interests through procedures such as risk assessments.  For example, the UK’s National Planning Policy Framework (NPPF) contains a presumption in favour of the principle of sustainable development, which at first glance sounds important from an environmental perspective. In reality, however, the principle and the NPPF still allows planning authorities lots of scope to decide what constitutes sustainable development in their local area, whether it is housing developments or protecting the greenbelt.   Though the creation of an “office for environmental protection” represents a potentially bold step by the government, its scope is too limited. The bill proposes that the office must “monitor the implementation of environmental law”. However, this doesn’t mean much as there is no reference to any standard to measure this implementation against.  The reference to “implementation” does make sense if the UK develops environmental law in response to EU or international obligations. But the whole point of Brexit is to do away with EU law and, as it stands, the bill contains no powers for the proposed office to monitor the implementation of international environmental law. Its lack of legal enforcement powers are also disappointing. The office would be limited to issuing so-called “decision notices” where a “serious” breach of the law has taken place. But what “serious” means is not defined in the bill. A “decision notice” also has little legal power as the recipient is merely obliged to respond in writing, explaining whether they agree with it and whether they intend to take any steps in response. A public authority handed a “decision notice” by the office for environmental protection could simply answer “no and no”, and the office would have little power to enforce its findings.  Many experts had hoped the proposed office would be given powers to fine the government. The air pollution cases brought against the UK government nicely highlighted the importance of fines, when the government conceded that it hoped to be compliant with EU emissions standards by 2020. That year was chosen not by science or policy plausibility, but because 2020 was likely to be the earliest point when the European Commission would take steps to fine the UK. However, a conscious decision has been made not to give the office for environmental protection any powers to fine the government. In light of this, the failure to include more innovative methods of enforcement is a disappointment.  The bill ought to include provisions for the office to negotiate so-called enforcement undertakings with the government, for instance. These are written agreements between a regulator and offender. The offender must agree to take specific steps to remedy the illegal activities, including restoring the environment to its previous state, and compensating any third parties who have suffered harm. Enforcement undertakings are already used extensively by the UK’s Environment Agency and have proved to be cost effective and efficient. In sum, the government’s proposal for a new post-Brexit environmental regime is laudable and disappointing in equal measures. Consequently, the bill is much like the government’s attempt to deliver Brexit itself: it is difficult to please everyone."
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   




In examining the climate change output from the DICE model, we found that it projects a degree of future sea level rise that far exceeds mainstream projections and are unsupported by the best available science. The sea level rise projections from more than half of the future scenarios examined exceed even the highest end of the projected sea level rise by the year 2300 as reported in the _Fifth Assessment Report_ (AR5) of the UN’s Intergovernmental Panel on Climate Change (see Figure 1).   






_Figure 1. Projections of sea level rise from the DICE model (the arithmetic average of the 10,000 Monte Carlo runs from each scenario) for the five scenarios examined by the federal interagency working group (colored lines) compared with the range of sea level rise projections for the year 2300 given in the IPCC AR5 (represented by the vertical blue bar)._ _(DICE data provided by Kevin Dayaratna and David Kreutzer of the Heritage Foundation)_   
  
Interestingly, Nordhaus (2010b) recognizes that the DICE sea level rise projections are outside the mainstream climate view as expressed by the IPCC:   




“The RICE [DICE] model projection is in the middle of the pack of alternative specifications of the different Rahmstorf specifications. Table 1 shows the RICE, base Rahmstorf, and average Rahmstorf. _Note that in all cases, these are significantly above the IPCC projections in AR4._ ” [emphasis added]



The justification given for the high sea-level rise projections in the DICE model (Nordhaus, 2010) is that they well-match the results of a “semi-empirical” methodology employed by Rahmstorf (2007) and Vermeer and Rahmstorf (2009).   
  
However, as we have pointed out, recent science has proven the “semi-empirical” approach to projecting future sea level rise unreliable. For example, Gregory et al. (2012) examined the assumption used in the “semi-empirical” methods and found them to be unsubstantiated. Gregory et al (2012) specifically refer to the results of Rahmstorf (2007) and Vermeer and Rahmstorf (2009):   




The implication of our closure of the [global mean sea level rise, GMSLR] budget is that a relationship between global climate change and the rate of GMSLR is weak or absent in the past. The lack of a strong relationship is consistent with the evidence from the tide-gauge datasets, whose authors find acceleration of GMSLR during the 20th century to be either insignificant or small. It also calls into question the basis of the semi-empirical methods for projecting GMSLR, which depend on calibrating a relationship between global climate change or radiative forcing and the rate of GMSLR from observational data (Rahmstorf, 2007; Vermeer and Rahmstorf, 2009; Jevrejeva et al., 2010).



In light of these findings, the justification for the very high sea-level rise projections produced by the DICE model is not acceptable.   
  
Given the strong relationship between sea-level rise and future damage built into the DICE model, there can be no doubt that the SCC estimates from the DICE model are higher than the best science can allow and consequently, should not be accepted by the OMB as a reliable estimate of the social cost of carbon.   
  
We did not investigate the sea-level rise projections from the other two IAMs employed in the federal SCC determination, but such an analysis must be carried out prior to extending any confidence in the values of the SCC resulting from those models—confidence that we demonstrate cannot be assigned to the DICE determinations of the social cost of carbon.   
  
References:   
  
Gregory, J., et al., 2012. Twentieth-century global-mean sea-level rise: is the whole greater than the sum of the parts? _Journal of Climate_ , 26, 4476-4499, doi:10.1175/JCLI-D-12-00319   
  
Nordhaus, W. 2010a. Economic aspects of global warming in a post-Copenhagen environment. _Proceedings of the National Academy of Sciences_ 107(26): 11721-11726.   
  
Nordhaus, W., 2010b. Projections of Sea Level Rise (SLR), http://www.econ.yale.edu/~nordhaus/homepage/documents/SLR_021910.pdf   
  



"
"Increasing consumption of meat rich diets throughout the world in the 21st century raises pressing concerns about human health, animal welfare and environmental sustainability. Too much mass-produced meat is bad for us, bad for the livestock we eat, and bad for the planet on which we live. If we want to understand how the world arrived at this point, as well as how we might change it for the better, we should look back to the Victorian period, which laid the foundations for modern globalised meat production and consumption. Concerns today about what has become known as the “global meat complex” focus on the technologically driven overproduction and consumption of livestock. There’s a recognition in particular that “the middle classes around the world eat too much meat”, as a 2014 Friends of the Earth report put it. But the root of this problem can be traced to 19th-century Britain, when global meat markets emerged as a revolutionary way of dealing with a mid-Victorian “meat famine”. The famine was caused by a mismatch between a fast increasing, urbanising population and a levelling out in domestic meat production. What helped stave it off was the groundbreaking development of preservation and transportation technologies that enabled the British to eat livestock that was reared, slaughtered and processed in the Americas and Australasia. As a result of these innovations, products such as chilled and corned beef, frozen mutton and meat extracts including Bovril and Oxo became staples throughout British homes. Per capita meat consumption increased dramatically, rising from about 87lb per year in the 1850s to 127lb annually by 1914, despite the fact that Britain’s population nearly doubled in this period. Cost was the major factor driving this change. When one can get a half-price leg of mutton from the other side of the globe, remarked one prominent food writer, one sets aside “all sentimental considerations in favour of the roast beef of Old England”. Mass marketing campaigns alongside positive media coverage also helped promote these new forms of meat. Victorian commentators celebrated frozen meat’s capacity to feed the “energetic, flesh-fed men” required to sustain British industry and imperialism. Meanwhile “beef tea” was widely advertised as a life enhancing force in Britain’s fights against alcoholism, influenza, European rivals and imperial perils. Meat remained a luxury for the very poor in Victorian Britain. But as the 19th century came to a close, and as more and more British consumers grew accustomed to imported beef and mutton, the idea of meat – the more the better – as an essential part of everyday meals became increasingly popular among working-class as well as middle-class meat-eaters.  As global meat markets revolutionised the dining habits of the British nation, they also changed the face of the planet. Vast tracts of American and Australasian land were reshaped as pasture that supported the British breeds of cattle and sheep that Britons preferred to eat. And selective breeding programmes meant the bodies of these animals fattened faster and could be stored more easily in refrigerated holds: animals were bred with their carcasses in mind.  The globalisation of Victorian meat eating was revolutionary, then, but it was also highly controversial. Advocates of the canning and refrigeration industries championed their capacity to deliver healthy, wholesome, inexpensive and sustainable meat supplies from Britain’s colonies and the “new world”. But home-reared meat was seen to be of better quality and safer, especially early on in the development of these industries. Many potential customers were put off by scandals involving putrefied meat, as well as scare stories surrounding the meat’s origins. Metropolitan meat eaters feared that overseas farmers were feeding them offal or meat from diseased animals. In my archival research, I’ve even discovered concerns that boiled human babies were entering the food chain. It wasn’t just that the British were wary of eating long dead animals from far flung parts of the world. Overseas competition provoked demands to protect British agriculture, both to preserve traditional ways of life and to guarantee food security. Animal rights campaigners too were concerned at the increasingly intensive farming methods and assembly line slaughter techniques associated with developing meat markets. And at the same time, Britain’s growing vegetarian movement was promoting the economic, health and ethical benefits of a meat free diet. Writing in the 1880s, the prominent vegetarian and socialist Henry Salt predicted that “future and wiser generations will look back on the habit of flesh-eating as a strange relic of ignorance and barbarism”. Salt would be horrified by a 21st-century world struggling to cope with an ever growing demand for cheap, plentiful meat. Horrified, but perhaps not entirely surprised. The unhealthy, unethical and unsustainable way that the “global meat complex” operates today is the greedy, brutal and environmentally devastating extension of what his meat eating contemporaries did to the world. But this Victorian history can also help ongoing efforts to change the way our planet produces and consumes protein. First and foremost, it makes clear that there is nothing inevitable or “natural” about the way meat markets take shape. Hundreds of millions of people eat meat in the way and the quantities they do, not because they’re inherently designed to do so, but because of a global system set in motion by British imperial power. And we should keep in mind that this system’s development was an incredibly controversial process, marked by fierce debates as well as dramatic dietary change. At a time of year when many of us are thinking about how to transform our lives for the better, the prospect of giving up meat, or of eating insects or lab-grown meat, provokes widespread scepticism, hostility and disgust. We’d all do well to remember, therefore, that not so long ago the prospect of eating frozen lamb from the other side of the world provoked a similar range of reactions among the Victorian population."
"Unless you live in the tropical rainforests of South or Central America, most of the sloths you’ll encounter will be two-toed sloths. This is because they are able to eat quite a varied diet and are therefore relatively easy to keep in captivity. Their relatives, the three-toed sloths, on the other hand, have a very restricted diet, subsisting solely on Cecropia: a group of fast-growing tree species with soft wood and large, juicy leaves.  Or so it has always been thought. A paper published today by the Royal Society gives quite a different picture of the lifestyle of three-toed sloths. The authors of the paper looked at how the availability of different tree species, including those of the genus Cecropia, affected the survival and reproduction rates of sloths. Given that these trees are the sloths’ favourite food, this specialist sloth species might be expected to spend most of its time in them. However, the authors found that at certain life stages, sloths may desert their favoured tree for other species. Density of Cecropia is critical to the survival and reproductive success of adults, especially the males, but was not correlated with survival rates of juveniles. The authors attribute the differing importance of Cecropia at different life stages to the shape and growth habits of the tree, and they give a detailed analysis of its effects. Because Cecropia species grow fast and produce lots of leaves with few chemical defences rather than a few leaves that are defended by a lot of toxins, there are always young, palatable, easily-digestible leaves available for adult sloths. The leaves also contain essential nutrients that keep sloths in good health, which would suggest that juveniles should also favour them.  Cecropia foliage consists of a fan of large leaves at the end of a long stem or branch with no other leaves on it, giving it an “open structure”. This means the tree does not make a good hiding place for young sloths, who may be more vulnerable than adults to predators like jaguars or eagles, even though they are quite well camouflaged. Similarly, mothers with babies may choose trees that have a thicker canopy as their maternity ward, moving back to the Cecropia tree when the baby is older.  This open structure is important when it comes to mating. Sloths are solitary creatures with extremely poor vision and, when the time is right, they need to find a mate from a widely dispersed population. Since the males are not equipped to go rushing around the forest looking for a receptive female, it is vital that they are able to be seen and heard when communicating their intentions to the local females. The relatively sparse foliage of Cecropia species is ideal for this, allowing the mating calls of the lonely males to travel much further than in the denser canopy of other trees. The authors of this paper suggest that, when necessary, three-toed sloths are able to live in habitats that are less high quality than virgin forest. Young sloths and nursing mothers may use tree species that are less nutritious than Cecropia in order to avoid the risk of predation, and in conservation terms, that may mean that they can exist on a less specialised diet if it is necessary to move or breed away from their natural habitat. This may be an important finding for sloths in the wild, since cocoa cultivation is a very present factor in their environment. Cocoa trees require a shady environment and, in Brazil, are traditionally grown as an understory layer beneath native forest trees. This is great news for the three-toed sloth as these areas of “agroforest” provide both the open structured Cecropia trees and a variety of other, denser canopied species, so can accommodate all the life stages of the sloth. Because they are of commercial use to humans, the cocoa trees are also less likely to be felled, so the habitat is relatively secure.  Until now, it has been thought that three-toed sloths are not able to make use of this agro-forest as two-toed sloths can, but this paper suggests otherwise. Since the agro-forestry project in Brazil has an ultimate goal of 557,500 hectares of forest being used for cocoa production, it is important that sloths are able to make use of this habitat for at least part of their life cycle. The authors suggest that targeted conservation efforts, such as planting Cecropia trees as part of cocoa agro-forestry, could help sloths in areas such as Costa Rica, where they are of conservation concern.  This study may have significance for the conservation of other “specialised” herbivores across the globe, if it is found that sloths are not the only animal to be able to survive on less favoured plants. The authors remark that forests that are regenerating are better able to support specialist species than we thought – and given the current levels of deforestation globally, this must surely give us some hope for the future. 


      Read more:
      Sloths aren't lazy – their slowness is a survival skill


"
"
Share this...FacebookTwitterThe Climate-Berlin Wall surrounding Germany has been cracking for some time now, and crumbling with increasing speed. Die Zeit’s online smear The Abetters of Doubt takes aim at the skeptics, who have scaled the Wall and are exercising their right to free expression. They are hammering away at climate science dogmatism in Germany, much to the chagrin of the warministas. 
The warministas are horrified and have embarked on a campaign to intimidate and marginalize the freedom of expression that skeptics enjoy in Germany. Climate skeptics who speak up today do so while always having to look over their shoulder, knowing they could be hit by a vicious smear and attack campaign. This is hardly the environment for a free and open society. Yet, it confirms that warminista science is trapped in the corner.
The days of Germany’s once exemplary model of a free and open society would of course risk unraveling if the warministas got their way. Indeed skeptics have long since been denied a voice in Germany’s publicly funded media, and so as a result have moved to the Internet, where they’ve chipped away.
Like it or not, this discussion is going to take place. Live with it.
Greenshirts resorting to brown tactics
What does an arrogant class, so totally convinced of itself, do when exhausted of argument and finds itself badly losing the intellectual debate and, along with it, its dream of The Green Reich?
It does what Die Zeit newspaper has done in its latest piece called The Abetters of Doubt; it resorts to brown tactics. The latest from Die Zeit is a 4-piece attack and smear campaign, with the objective of intimidating, marginalizing and silencing climate skeptics. It’s the same we have recently seen from other major dailies like Der Spiegel and the Handelsbaltt, with the usual names popping up: Stefan Rahmstorf and attack canine Naomi Oreskes. Lacking journalistic talent, Die Zeit has stooped to rehashing old stories.
Not only does the piece smear skeptics and advocate they be denied a voice, it attempts to morally degrade them as well. Ironically, it is becoming obvious who is actually morally degraded. Being a dissident here behind the Climate Berlin Wall and watching these smear tactics, I’m beginning to have an idea of what it must have been like to be gay during Medieval times, or Jewish before Kristallnacht. Die Zeit’s message to the skeptics in Germany is clear: “Be worried – be very worried”.
Fortunately, it’s nothing more than a last desperate threat coming from an intellectually bankrupt media outlet and a few activist scientists hiding in the background.
Growing skepticism, and desperation
Die Zeit’s piece is eerily laced with a strange combination of fear, anger and desperation, and it makes clear that the warministas are fed up with the turn climate science has taken. For them, the science was settled years ago. Damn the skeptics, bloggers, Internet and Fred Singer. Damn EIKE and the few German politicians who are beginning to listen up. They have gone too far. Die Zeit frets that public opinion in Germany is waning and that it’s time to put an end to it. In its piece, Die Zeit puts the spotlight on the bloggers:
Last year’s failure in Copenhagen and the hacked e-mails from climate scientists, which supposedly proved falsification, have been making waves through the media. The deniers and skeptics of global warming have been gaining momentum. They are omnipresent, mainly in the Internet – and appear to strike a chord with people who are fed up with all the climate talk, or with people who don’t want to change their lives because of a warming planet.”
Damn that denier European Institute for Climate and Energy (EIKE)
Die Zeit then singles out Horst-Joachim Lüdecke, a retired professor who is now the Press Speaker of the European Institute for Climate and Energy (EIKE). Die Zeit haughtily implies that Lüdecke is imposturing as a climate scientist:
Professor emeritus for Physics and Computer Sciences, but to his audience he introduces himself as a ‘climate scientist.'”
Indeed Prof. Lüdecke has been busy spreading the skeptic message, and has been effective. Skepticism in Germany has almost doubled over the recent months – now 1/3 no longer believe CO2 is a problem. This has infuriated and alarmed the warministas. Lüdecke recently gave a presentation to the Nordoberpfalz business group. Here’s how Die Zeit describes it:
The audience was made up of company owners, the mayor, local dignitaries and decorated lieutenant colonels. Hardly anyone noticed that Lüdecke was citing outdated reports, asserting uncertainties that no longer exist and suppressed facts that were inconvenient.”
Not only is Lüdecke imposturing as a climate scientist, but he is also using phony data, Die Zeit wants its readers to believe.
Damn those bloggers and the Internet (again)
The warministas by far view the Internet and bloggers as their biggest problem. This ought not be a surprise, as skeptics have long since been denied their right to be given a voice by Germany’s massive public radio and television media, where they are viewed as unworthy of a public platform. Call it media-gatekeeping. So, naturally, skeptics use the resources that are available, i.e. the Internet, and the little money and time they have at their disposal.
On the topic of bloggers, Die Zeit interviews no. 1  crybaby Stefan Rahmstorf of the Potsdam Institute for Climate Impact Research(PIK), introducing him as: “one of the world’s leading oceanographers”. Die Zeit writes:
In Potsdam, at the Einstein-Science-Park on the Telegrafenberg, climate scientist Stefan Rahmstorf sits in front of his computer and moans. ‘In the Internet, the climate skeptics are by far the dominant force,’ says Rahmstorf. There, an amateur can hardly do any research. ‘There have always been skeptics since he’s been doing research, ‘but last year they have broken into the serious media.'”
It just really stinks when the opposition has a voice. Die Zeit continues:
Together with his colleagues, he [Rahmstorf] counters skeptic claims and erroneous media reports at the Internet blog KlimaLounge, where some think he is overly fervent. Rahmstorf says that this is no fun, but sees no alternative. No matter where he goes, in government offices, in politics, top management levels of business – everywhere you hear skeptic arguments.”
I find it astonishing that a scientist would spend his work time preoccupied with PR work and spin. I thought they are supposed to be doing science, and not PR damage control. Clearly Rahmstorf spends much of his time writing stories for the NYT Times, Die Zeit, Der Spiegel and blogging, and not on the work he is paid to do.
A big part of Rahmstorf’s problem is that he’s turned a lot of people off with his smear tactics, getting on the wrong side of a huge number of scientists. He’s got no shortage of enemies.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Damn Big Oil and industry
In the Die Zeit piece, out comes the old worn out tactic of linking skeptics to Big Oil oil or to the coal industry, which  only further confirms the bankrupt state of the global warming narrative.
Also the top managers of power giant RWE took part in a conference by mining engineers who presented ‘solidly established doubt’ on man-made climate change. Foremost, the coal lobby is spreading doubt about global warming in the 2009 annual report of the coal association one reads that the brakes have been put on climate change.
Employees of E.on, Bayer and BASF in the USA this year contributed at least 70,000 dollars to skeptic politicians.”
If Rahmstorf and the warministas had their way, private contributions to the organizations he disagrees with would be banned. Sounds undemocratic to me. Let’s not talk about the BILLIONS that flow into warmist coffers.
Falsehoods about Climategate and the IPCC are being spread
Die Zeit refuses to acknowledge any scandals in climate science, particularly Climategate and the IPCC 4AR, and twists itself into a pretzel defending the rogue institutes behind them. Not once did Die Zeit publish the damning e-mails, keeping them hidden and locked away from the public instead. Die Zeit claims that many of the ‘scandals’ have already been debunked, and even goes on to defend Mann’s crooked hockey stick. Die Zeit complains:
Even so, these supposed scandals have made their way through the Internet blogs thousands of times.”
Damn Internet. Of course, the entire climate science community knows that these scandals have not been debunked. Here’s a list of 94 scandals that have yet to be resolved. An updated and much expanded list is coming out soon. As far as the IAC is concerned, it Admits Well Is Poisoned, Yet Insists Water Is Safe.
Damn Fred Singer and the Heartland Institute 
Part 3 of Die Zeit’s piece focuses on Fred Singer, the Heartland Institute, tobacco in the 1960s and Oreskes’s Merchants of Doubt. This is old and is just a lazy rehash of what appeared already weeks ago at Der Spiegel here. Die Zeit goes on and marginalizes scientific debate:
As usual the debate took place for years in the scientific journals, and the uncertainty is pretty much cleared up. This is now just constant back and forth that has since taken place in the lurid light of the public,’ says Hartmut Grassl, the 71-year old doyen of German climate science. Skeptics cherry-pick uncertainties in such debates. But all this has nothing to do with skepticism, nothing to do with critique and testing.'”
In Die Zeit’s simple world, it all goes back to Fred Singer. And CO2 drives the climate, of course.
Damn EIKE and their climate conferences
Spreading skepticism in Germany, as it was done in USA, has been successful, thanks to Holger Thuss, spokesman and founder of EIKE. EIKE is funded exclusively by private donations and has less than a hundred members. This year in December it will hold its 3rd Climate Conference in Berlin, and this time they have just enough money to have the food catered instead of buying it at a supermarket like they had to do last year. Die Zeit:
The conference next week at the Maritim Hotel in Berlin shows that EIKE, despite its dubious science, has been successful in building a network. The list of speakers includes the former President of the German Steel Industry Association. One co-sponsor of the conference is the market-radical Berlin Manhattan Institute for Entrepreneurial Freedom, which has only a one-man office, but has a board filled with economic professors who convey an air of seriousness.”
Die Zeit also has jumped in on the mob-political-lynching of Marie-Luise Dött, German Parliamentarian and a central figure on Angela Merkel’s environmental committee, whose crime was to express skepticism on climate change. Read here.
Hans von Storch chimes in 
Die Zeit ends its piece quoting Hans von Storch, who assumes his comfortable perch on the fence.
“I’ve taken a look at such skeptic conferences twice.  The level for the most part was catastrophic’. Many go there to simply spread pre-packaged opinions. ‘A real interest in a discussion could not be detected.”
Detection is indeed a big problem in climate science. Some things that hardly exist, get overly detected, while other things staring right at you in the face are ignored. There’s a lot of confusion in climate science.
Skeptics’ reaction
Finally I asked EIKE for their thoughts on the Die Zeit piece, to which they kindly answered. In a nutshell, they didn’t seem the least bothered by the Die Zeit report, taking it in stride and actually welcoming it. A spokesman wrote it will bring much more attention to the discussion and generate even more interest in the Climate Conference in Berlin. Then he added, quoting Gandhi:
First they ignore you.
Then they ridicule you.
Then they attack you.
Then you win.
We are now at stage III.”
Like it or not, this discussion is going to take place. SO LIVE WITH IT.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe European Institute For Climate and Environment (EIKE) in Germany recently had a piece at it’s blogsite here, which I have summarized.
The USA has James Hansen, and England has Phil Jones. Germany now has Prof. Dr. Gerhard Adrian, the new President of the German Weather Service. His mission: to produce a trend to climate catastrophe as quickly as possible. Recently he said:
The average temperature in Germany has risen by in 1.1°C from 1881 to 2009. It could go up another 2 to 4°C by the end of the century.
and:
We’ll have a completely whole new set of extremes to deal with; that’s the threat.
This is the new message from the once respected German Weather Service. Suddenly, doom and gloom are the forecast.
Inconveniently for Dr. Adrian, his own data and earlier statements made by German Weather Institutes seem to contradict his claims.

Firstly, why choose a timescale that starts in 1881 when records go back as much as 300 years in Europe? The following graphics are temperature charts going back more than 200 years for some European cities (visit EIKE for better quality graphics (here).
Here’s the temperature chart for Berlin going back 300 years (same as above in the introduction):
There we don’t see much going on until about 1990. In fact the total trend is 0.08″C rise per century – statistically insignificant. Surely Dr. Adrian is aware of this.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Now let’s take a look at the temperature CO2 correlation.
Poor correlation there too. While atmospheric CO2 concentrations climbed from 1750 to 1980, Berlin’s temperature did the opposite. But when one is hired to promote global warming alarmism, then 1881 is a good place to start.
Let’s go back and look at the last 2000 years. Maybe that’ll reveal something more earth-shattering.
All the climate catastrophe talk put out by Hansen, Jones, and now Dr. Gerhard Adrian, simply do not materialize when you take an honest look at the statistics.
As far as weather extremes occurring, here’s what the German Weather Institutes said in the pre-Gerhard-Adrian days, just a couple of years ago. According to the German Weather Service, recently quoting the German Meteorological Society, 3/2002, p. 2:
When it comes to extreme weather events, no significant trend can be observed up to now. Also such events like the flooding of 2002 are part of the norm in our climate.
According to a German Weather Service press conference 24 April 2007, Berlin;
Up to now there has been no increase of extreme events:  Up to now – with the exception of the already mentioned heavy summertime precipitation – there has been no detectable systematic changes or shifts of extreme values.
And again, according to the German Meteorological Society 3/2002, S. 2, on the flood of 2002:
Also such events, like the big flood of 2002, are part of the norm in our climate.
Dr. Adrian ought to listen to his own data.
Share this...FacebookTwitter "
"It may surprise you to know that man’s best friend is not a friend of wildlife. Or that those cute cats from online videos are often mass killers. The dark side of pet ownership is something not usually addressed but in wildlife conservation it has become a burning issue. Last week the Australian government named domestic cats as one of the major threats to its unique wildlife. The problem of domestic cats and dogs killing wildlife is massive.  A recent study in the US found that cats, especially stray cats, kill four billion birds and up to 21 billion mammals annually.  If they were just killing mice and rats, which are invasive species, no-one would be concerned but no – they are also killing native wildlife and could make some species extinct. Cat and dog ownership is a global phenomenon and no country is immune. My research group in Brazil did some studies into why greater rheas, a large flightless bird, were being wiped out in Minas Gerais and we discovered the problem was farmer’s dogs. Cattle pasture makes ideal habitat for these birds. As part of our research we interviewed farmers about the problem and the response was always the same: their neighbour’s dog was to blame for killing this species but never their dog. The problem in regions with such rich biodiversity is that dogs in the countryside might technically be pets but in reality they are feral. And their instinct to hunt, especially if they are not being fed regularly takes over. Dogs hunt wildlife, but they also kill it indirectly by spreading disease. Studies conducted in Brazil have shown that they spread diseases such as canine distemper to wild endangered canines such as maned wolves. In Ethiopia, such diseases are among the most serious threats to the world’s rarest canine, the Ethiopian wolf. Scientists often use camera traps to estimate the numbers of medium-to-large mammals in the wild. I have many colleagues who employ this technology around the world.  And as a lover of wildlife I am always keen to see their photographs but instead of seeing peccaries, deer or even jaguars, the most common photographically captured animal is the domestic “feral” dog. The problem is so pronounced that Matthew E Gompper, professor of Mammalogy at the University of Missouri has edited a book on the subject. Cats appear to kill more wildlife globally than dogs.  This no doubt relates to the culture of people letting their cats roam free. Even well-fed pet cats will go out and hunt and data from a study in the UK estimated that domestic cats kill a total of 92m animals annually and this has an impact on native species conservation. The culture of letting cats and farm dogs roam is difficult to break. In some countries this problem has been dealt with by culling the offending animals. But this does not always work – in Brazil, this is not done because the owners of such animals may kill wildlife in revenge. In countries like New Zealand where the native wildlife never evolved with such predators and is essentially defenceless, culling may be the only solution if flightless parrots such as the kakapo are to be saved from extinction. We could of course sterilise the offending animals, but in many countries around the world this would require the owner´s consent. Plus with dogs the problem is with males – if you castrate them their behaviour changes, they become less aggressive and this may result in the influx of dogs from neighbouring regions. However there is now a single injection that sterilises male dogs and does not change their behaviour. Whenever possible, dog numbers should be kept under control. In more developed countries without large remote wildlife areas the problem can be dealt with by the law.  Dogs are supposed to be under the control of the owner at all times in public.  However, I have unfortunately witnessed people allowing their out-of-control dog to chase wildlife in the UK.  But with education and law enforcement this situation should be a minor issue. Pet cats are another issue. Putting a bell or sonic device on their collar halves their predatory efficiency, however, with experience cats can improve their hunting despite such devices.  There are even cities in Australia that operate cat curfews.  And farmers could be encouraged to control better the cats they keep for vermin control.   We will never completely stop pet carnivores killing wildlife, but through responsible ownership and control programmes we can reduce the number of victims from the billions to the millions: removing cats and dogs from wildlife’s most dangerous threat list."
"

The foreign policy record of the Clinton‐​Gore administration deserves a less than stellar grade. At the end of the Cold War, there was an extraordinary opportunity to build a new relationship with a democratic Russia; restructure U.S. security policy in both Europe and East Asia to reduce America’s burdens and risk exposure; and revisit intractable Cold War‐​era problems, such as the frosty relations with Cuba, Vietnam, and North Korea. The administration’s performance must be judged within the context of such an unprecedented opportunity for constructive change.



The record is acutely disappointing. True, the administration has scored some successes: improving the negotiating climate in Northern Ireland and the Middle East, pushing for permanent normal trade relations with China, and normalizing relations with Vietnam. But the failures greatly outnumber the successes. The administration needlessly meddled in the complex disputes of the Balkans, leaving to its successor two U.S.-led NATO protectorates (Bosnia and Kosovo) and a colossal mess of a nation‐​building commitment with no end in sight. A similar morass is emerging in Colombia as a result of the administration’s prosecution of the drug war.



U.S. policy toward long‐​time adversaries is on autopilot. The rote perpetuation of an economic embargo and occasional bombing attacks against Iraq have devastated the Iraqi people while barely bothering Saddam Hussein. Washington’s policy toward Cuba is equally sterile and cruel.



Worst of all is the growing list of missed opportunities. Instead of integrating a newly democratic Russia into the West, the Clinton administration needlessly antagonized Russia by expanding NATO’s membership and waging war against Moscow’s long‐​time allies in the Balkans. Relations with China have been damaged by an inconsistent, at times nearly incoherent, U.S. policy. Instead of embracing efforts for greater military self‐​reliance on the part of our European allies, the administration has engaged in carping criticism and apparently views such initiatives as a threat to America’s dominant position in the transatlantic relationship. Instead of viewing the end of the Cold War in East Asia as an opportunity to reduce America’s security burdens in that region, the United States insists on keeping 100,000 troops deployed seemingly forever. Administration officials even reacted with ambivalence to the recent summit between North and South Korea and gave highest priority to retaining the U.S. troop presence on the Korean peninsula.



Given the number of botched opportunities, the administration’s record merits a grade of D.
"
"
Share this...FacebookTwitterMatti Vooro presents his latest essay on colder winters in the UK. Matti’s last essay: Signs of strengthening global cooling, drew over 100 reader comments – a record at NoTricksZone.
================================================
Should Britons Buy Bermuda Shorts Or Long Johns?
by Matti Vooro
Snowfalls are now just a thing of the past. “
That was the headline in the UK’s The Independent newspaper in March of 2000. The CRU scientists claimed that within a few years winter snowfall would become “a very rare and exciting thing. Incapable of learning, even reconfirmed this as recently as January 10, 2010 when one of their scientists told the UK Mail:
The winter is just a little cooler than average, and I still think that snow will become an increasingly rare event”.
The Met Office then followed The Independent with their prediction of the 4°C temperature rise in only 50 years, predicting warmer temperatures, more heat waves and drought. The IPCC had the same message in their 2007 report with their prediction that:
Annual mean temperatures in Europe are likely to increase more than the global mean. The warming in northern Europe is likely to be largest in winter.”
Yet, only a few years after these predictions of unprecedented winter warming for UK and Europe, the exact opposite has emerged. Winters have been getting colder and there is no lack of snow. UK winters have declined in temperatures 4 years straight since 2007. So have the annual temperatures. The last two winters have been especially cold and wintry.
 Taking the UK as a whole and not just Central England or CET
2010 December [-1 C] coldest December since 1910
2009 December [2.1 C] 13th coldest December since 1910
2008 December [3.1 C] 26th coldest December since 1910
2007 December [3.77C] 56th coldest December since 1910
It is dramatic how the winter temperatures have shifted since 2007 winter, which was the 2nd warmest winter in the UK since 1910.
What follows are the mean winter temperatures for all of UK. The average mean winter temperature is around 3.6C
2007   5.56 C (2nd warmest)
2008   4.86 C
2009   3.21 C
2010   1.64 C (7th coldest)
The annual UK temperatures have been declining since 2006 as the following shows:
2006 9.73 C (warmest since 1910)
2007 9.59 C
2008 9.05 C
2009 9.17 C
2010 7.96 C (12th coldest since 1910)

Refer to the UK Met Office and the excellent data provided from the following source:
http://www.metoffice.gov.uk/climate/uk/datasets/Tmean/date/UK.txt
According to the Met Office:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




2010 was the 12 th coldest year in the 100 year series and the coldest since 1986. 2010 was the coldest year since 1919 in Scotland and Northern Ireland “
The following graph is a plot of winters in Central England as opposed to UK as a whole for the years 1948-2010. UK temperatures as a whole are similar but slightly lower by 0.5 to1 degree C.  Winter AO levels are also shown.

Typically La Nina winters used to be colder before the 1970’s but during the last 11 La Nina winters, 9 winters have been near normal or warmer for Central England. This did not happen in December 2010. January 2011 is more typical of La Nina winters. El Nino winters seem to set up colder and negative AO and colder winters except when they are extra strong like 1998. It would appear from field or observed data, that UK winters are not getting warmer as predicted but may actually be getting cooler instead and may be following the natural planetary cycles perhaps similar to what happened during the last cooler cycle about 1962-1987.
Man-made greenhouse gases have little to do with this cooling as CO2 keep rising in a minor way. The weather was supposed to get warmer as CO2 levels have gone up? It is not. Global warming science does not seem to be holding up and seems to need a serious rethink, see GWPF.
During the 26 years of the last colder period , 17  years or 2/3 of the winters were below the average mean winter temperature of 3.6 and about 12 (45%) were  much colder and below 3C,  Of the 78 winter months during 1962-1987,  41 months (53%) were below average mean of 3.6°C.
What was the main weather factor present during those cooler winters?
Number of winters where AMO was negative:   22 (84%)
Number of winters where AO was negative:   21 (80%) [dec/jan/feb]
Number of winters where PDO was negative: 15 (58%)
Number of winters NAO was negative:  13 (50%)
ENSO years neutral 8 years (30%), LA NINA 9 years (35%), EL NINO 9 years (35%)
Number of winters with a net negative AO [dec/jan/feb] during the last 60 years:
1950’s 6
1960’s 10 (very cold winters)
1970’s 6
1980’s 7
1990’s 4 (very warm winters)
2000’s 4 (very warm winters)
Clearly the presence of negative AO, AMO, PDO and NAO were the most frequently occurring climate factors happening during that time. With the exception of AMO, all these factors are again heading for or are already in their negative or cool mode.
The sun is also still in its low activity level and unusual extra warming seems unlikely. AMO is likely to go negative or cool within 5-10 years if not sooner. There were many instances of back to back months of very cold weather as well as back to back cold winters like the 1960’s.
Summary
What would you do if you were a member of the UK general public or an official in charge of transportation, roads, airports, fuel supply, electricity or other infrastructure, read here WUWT?
Clearly the some agencies charged with informing the public about seasonal or long term weather did not have their act together yet. There was a serious warming bias in many weather and climate forecasts due to an over-emphasis on global warming. For example, in the midst of the worst part of December 2010 winter storm, the focus of the chief climate scientist was not on how to help the public with better information during the crisis, but on global warming. Professor Slingo insisted in comments to the Independent newspaper on December 21, 2010:
The key message is that global warming continues.”
Some refuse to learn. People are finding out that a second opinion on winter weather is paying off. Many North American meteorologists like Joe Bastardi and Joe D’Aleo and independent UK meteorologists like Piers Corbyn, have been predicting cooler weather the last several years. 
In my judgment, no one can predict with certainty what the future of the climate will be for the next 10-30 years.
Each climate cycle is different. It will not be a mini ice age in my opinion as some are predicting, nor will all the global general temperatures go below those that existed before the 1976 Pacific climate shift, but more of a cyclic cooler period.
Once the North Atlantic ocean SST and AMO start to contribute to the global cooling in a more significant way, the global temperatures of US and Canadian east coasts, the western coast of Europe and the Arctic will be the cooling more consistently.
Share this...FacebookTwitter "
"London has long been one of the world’s most congested cities. Before a £5 “congestion charge” was introduced for vehicles entering the city centre, cars would spend a third of their time in peak hours at a complete standstill. The charge, introduced in 2003, was hailed as a triumph of economics as it forced those who contributed to congestion to pay a price that reflected its cost. As economics would suggest, traffic decreased and journeys became quicker. Traffic accidents and related injuries and fatalities weren’t the main target of the congestion charge, but they may have been influenced by it. On the one hand, reduced congestion means fewer cars are in central London with an expectation of fewer accidents. On the other hand, less traffic means higher travel speeds, which generally leads to more accidents – particularly in a dense area such as central London where cars, cyclists and pedestrians share the road.  My research with John Heywood of the University of Wisconsin-Milwaukee and Maria Navarro of Lancaster University is the first major study of the congestion charge’s effect on traffic accidents and their severity. One issue is that we simply don’t know what would have happened to accidents in the congestion zone had there not been a charge. Comparing the number of accidents before and after charge is not enough, as we might simply be observing something that would have happened anyway. It is well known that the number of traffic accidents has been steadily declining across the UK in the past decade.  This means we need to define appropriate comparison or control groups. In this case, we used the most populous 20 cities in Britain, not including London. We contrasted the change in accidents in the congestion charge zone to that change over the same period in these other cities. We use the fact that the downward trend in accidents happens in both London and the comparison group to more accurately identify the influence of the charge.   The congestion charge reduced traffic accidents in central London by 30 a month – an enormous 40% reduction. Accidents that result in individuals being killed or seriously injured also fell, by just under four a month, or 45 a year. This means around 500 people have avoided serious injury or death thanks to the congestion charge. Congestion charge means fewer accidents in London: We next explored any negative “spill-over” from the policy. Did the congestion zone just move traffic to other parts of London, other times of the day, or to non-charged vehicles? If the answer is yes, reduced accidents may be offset by more car crashes and fatalities elsewhere and at other times.  This isn’t the case, however. We examined areas surrounding the congestion charge zone – both a 2km and a 5km radius – and we found that, not only did accidents not increase in these areas, there was actually a large positive spill-over. The charge decreased accidents dramatically in the surrounding areas – about 20 fewer total accidents a month, with 3.5 fewer serious or fatal accidents. Our estimates also show that accidents and injuries were reduced in non-charged times (before 7 am and after 6 pm) and for exempt vehicles (largely bicycles, motorcycles, taxis and buses). The charge meant more people rode bikes into central London – that was the idea. Our research confirmed that one unanticipated effect of the congestion charge was a small initial increase in accidents involving cyclists, roughly 1.5 a month up to 2005. Yet, by the end of 2006 we found this had reversed and that the rise didn’t carry on. It seems likely to us that an initial response to the congestion charge was that more inexperienced cyclists took to the roads. In time, they either gave up or became more worldly. Is the congestion charge set at the right level? Since the initial £5 charge in 2003, it has been raised twice, £8 in 2005 and £10 in 2011. These changes and inflation allow us to see what effect a £1 real charge has on accidents. Our results show that each pound increase in levy reduces accidents by 5 per month. While we can’t say what is the “right” charge, our work suggests it’s not just the fact that the charge exists that matters. It may not be great news for commuters, but a more expensive charge might well reduce traffic accidents further still."
"

Some of the most pressing questions about global economies — whether governments are spending beyond their means, for example, and if so by how much — concern what economists call “fiscal imbalance.” Although this is a concept familiar to economists, it can often be difficult for non‐​economists to decipher. In“Fiscal Imbalance: A Primer” (White Paper), Director of Economic Policy Studies Jeffrey Miron provides a clear introduction to the concept of fiscal imbalance. Fiscal imbalance essentially concerns whether a government can continue forever to make the expenditures necessitated by its existing policies, given the expected revenues under those policies and the government’s debt. This includes its ability to borrow money in the future — which is not infinite. This imbalance, as Miron writes in **“U.S. Fiscal Imbalance over Time: This Time Is Different”** (White Paper), is growing. He projects fiscal imbalance for every year between 1965 and 2014, revealing that the United States has seen a rising fiscal imbalance since the early 1970s. “As of 2014, the fiscal imbalance stands at $117.9 trillion, with few signs of future improvement even if GDP growth accelerates or tax revenues increase relative to historic norms,” he warns. “Thus the only viable way to restore fiscal balance is to scale back mandatory spending policies, particularly on large health care programs such as Medicare, Medicaid, and the Affordable Care Act (ACA).”



 **THE COSTS OF GUN CONTROL**  
Gun control advocates persistently call for measures like universal background checks, a ban on high‐​capacity magazines, or a ban on so‐​called “assault weapons.” But, as associate policy analyst David B. Kopel argues in **“The Costs and Consequences of Gun Control”** (Policy Analysis no. 784), “Such proposals are not likely to stop a deranged person bent on murder.” Kopel examines the actual costs and benefits of these popular gun‐​control measures, demonstrating that they would prove largely ineffective. “Before adding new gun regulations to the legal code, policymakers should remember that several mass murders in the U.S. were prevented because citizens used firearms against the culprit before the police arrived on the scene,” he warns.



 **MURDER AS A THREAT TO FREE SPEECH**  
The brutal Charlie Hebdo killings last year were a shocking act of violence, but unfortunately, not the first violent reactions to speech perceived as blasphemy. As Robert Corn‐​Revere , a partner at Davis Wright Tremaine LLP, writes in **“To Confront the Assassin’s Veto, or to Ratify It?”** (Working Paper no. 36), “This was yet another grim marker in the cross‐​cultural conflict illustrated by events such as the Ayatollah Khomeini’s 1989 fatwah against Salman Rushdie for writing _The Satanic Verses_ , the 2004 murder of filmmaker Theo van Gogh on the streets of Amsterdam for perceived insults to Islam, and the violent reaction to the cartoons of Mohammad published in the Danish newspaper _Jyllands‐​Posten_ in 2005.” Corn-Revere’s paper confronts the question of how the law should deal with these sinister attempts to chill speech.



 **ESAS: EMPOWERING STUDENTS AND FAMILIES**  
“Every child deserves the chance at a great education and the American dream,” Cato policy analyst Jason Bedrick, Goldwater Institute education director Jonathan Butcher, and former Goldwater Institute vice president for litigation Clint Bolick — who has since been appointed to the Arizona Supreme Court — write in **“Taking Credit for Education: How to Fund Education Savings Accounts through Tax Credits”** (Policy Analysis no. 785). In an effort to improve education, several states have passed laws allowing students to receive an Education Savings Account (ESA) which parents can put toward alternative education services. In this analysis, the authors show how legislators can design ESAs that will work in the 40 states with constitutional provisions that prohibit the use of public funds at religious schools. “Tax‐​credit‐​funded ESAs would empower families with more educational options while enhancing accountability and refraining from coercing anyone into financially supporting ideas they oppose,” they write.



 **CHINA AT A CROSSROADS**  
China, as Cato vice president James Dorn puts it, is “at a crossroads.” It has made tremendous progress in recent years by expanding the market and strengthening property rights. But at the same time, its powerful one‐​party state maintains a strong grip on citizens’ private and commercial dealings. “The damage China’s illiberal state has inflicted on the nation is becoming evident as the economy slows, debts mount, and state‐​owned enterprises (SOEs) draw capital away from the more productive private sector,” writes Dorn in **“China’s Challenge: Expanding the Market, Limiting the State”** (Working Paper no. 34). He highlights the importance of renewing interest in China’s ancient culture and writings on topics like freedom and limited government — a legacy which its authoritarian leaders have obscured.



 **THE LUKEWARMING WORLD**  
In **“Climate Models and Climate Reality: A Closer Look at a Lukewarming World”** (Working Paper no. 35), Center for the Study of Science director Pat Michaels and assistant director Chip Knappenberger further the case for the “lukewarmers” — those who believe that the evidence for some human‐​caused climate change is persuasive, but that, contrary to the alarmists, this warming occurs in accordance with the lower end of expectations from mainstream science. They contend that the rate of warming over the past several decades has been so slow it was “completely unexpected” by any of the climate models — “a worrying indication that the current stateof‐ the‐​art climate models are not up to the task of simulating the actual behavior of the earth’s climate.” This consequently throws efforts to implement climate policy based on these models into serious doubt.



 **THE EVOLUTION OF WEAPONRY**  
Technological advances in recent years have led to a bevy of increasingly small, cheap, and sophisticated weapons. “This new diffusion of power has major implications for the conduct of warfare and national strategy,” U.S. National Defense University distinguished research fellow T. X. Hammes argues in “Technologies Converge and Power Diffuses: The Evolution of Small, Smart, and Cheap Weapons” (Policy Analysis no. 786). Hammes delves into the particular challenges posed by various types of emerging technology, like drones, artificial intelligence, and nanoenergetics, or explosives. With such abundant and affordable technology available, the United States may be exposed to much more danger when waging military campaigns in the future. “Increasingly,” he writes, “we will have to ask the question ‘Is the strategic benefit of an intervention worth the cost when the enemy can strike back in and out of theater?’”
"
"
Share this...FacebookTwitterAnyone know a good copyright attorney interested in splitting awards 50/50?
I wonder how many bloggers know what the symbol to the left means? Seems like there are some that don’t.
No, it doesn’t mean Christmas!
I really wish I didn’t have to write this. But I feel compelled to do so.
I’ve been blogging 9 months and I try to make sure I don’t violate copyright laws. Copyright laws do enter gray territory at times, and sometimes it isn’t really clear what is allowed and what is not. I hope that I’ve always stayed within the law. I use photos from Wikipedia, as it’s stated there what you can and cannot do. I hear a lot of people bring up “fair use”. Laws vary from country to country. It gets complicated and fuzzy.
When blogging, I often quote papers, online articles and provide excerpts, always making sure to cite and link to the sources. I think people who wrote the original material deserve credit and recognition for their work. I like linking to other sites, hoping that the little extra traffic I generate will make them happy. What goes around, comes around.
What do other bloggers think? I’d be interested to know. Maybe I’m naive about all this.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




I must say I’m surprised by the number of sites that simply copy and paste ENTIRE stories and paste them at their site, without even asking. This has been done to me more than once, and with increasing frequency lately. With one of my recent posts I found 6 other sites that did this with my story – not one asked for permission to do so. Sure they provided a link, but who needs to click on it when it’s all there?
I don’t want to sound like I’m crying about it, but geez, c’mon, it sure would be nice if people asked. Like hello! I’m not there to carry your water.
Don’t get me wrong, I love it when other blogs pick up on the stuff I write. It really feels rewarding. All I ask is that bloggers post the first 30% of the content or so, or excerpts, followed by a direct link to the original story. Is that too much to ask? Am I being naive?
Tom Nelson does an outstanding job in respecting other people’s work. Tom always posts a few main excerpts, some content, then followed by a link to the original story. This way everyone is happy.
I noticed some sites that copy and paste entire stories without permission also have advertisements at their sites, meaning they are gaining financially by copying and pasting. In this case the originators of the content have the right to compensation.
I’m not going to go after past infractions, but in the future I’m not going to let these things slide as much anymore. Bloggers who benefit financially from advertising at their sites, or receive donations, and decide to copy and paste entire stories from NTZ will have to expect to hand over a cut of their profits, willingly or unwillingly.
All in all, I’m just hoping everyone will agree to play fair in the future.  
Merry Christmas everyone.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterI know what a lot of people are thinking when they look at the Arctic sea ice graph since September 1st – my oh my how has the ice reduced! Indeed just take a look at the numbers themselves:
September 1:     5,332,344 sq km
September 18:   4,813, 594 sq km
That’s a drop in area of over 500,000 sq km. Still, I’m going to say that the ice has grown. You think it’s preposterous, right?

But now take a look at the following chart that compares September 1 ice to September 18 ice. Which would you prefer to be standing on?

These charts are taken from: http://www.ijis.iarc.uaf.edu/cgi-bin/seaice-monitor.cgi
Which ice looks thicker (more concentrated)?
Don’t sweat the ice area statistics. The thickness (er, concentration) is much greater today, and we could even say the volume is likely more.  Arctic temperatures above 80°N have been colder this summer and September. The ice area will rebound quickly, of course. I projected a 5.75 million sq km min. for 2011 a couple weeks back. I’m sticking to it.
Share this...FacebookTwitter "
"
UPDATE 1/28: Full text of Dr. Theon’s letter has been post on the Senate website and below.
This is something I thought I’d never see. This press release today is from the Senate EPW blog of Jame Inhofe.  The scientist making the claims in the headline, Dr. John S. Theon, formerly of  the Institute for Global Environmental Strategies, Arlington, Virginia, has a paper here in the AMS BAMS that you may also find interesting. Other papers are available here in Google Scholar. He also worked on the report of the Space Shuttle Challenger accident report and according to that document was a significant contributor to weather forecasting improvements:
The Space Shuttle Weather Forecasting Advisory  Panel, chaired by Dr. John Theon, was established by NASA  Headquarters to review existing weather support capabilities and  plans and to recommend a course of action to the NSTS Program.  Included on the panel were representatives from NASA, the National  Oceanic and Atmospheric Administration (NOAA), the Air Force, and the  National Center for Atmospheric Research.
For those just joining the climate discussion, Dr. James Hansen is the chief climate scientist at NASA Goddard Institute for Space Studies (GISS) and is the man who originally raised the alarm on global warming in 1988 in an appearance before congress. He is also the keeper of the most often cited climate data.
EPW press release below – Anthony

Washington DC, Jan 27th 2009: NASA warming scientist James Hansen, one of former Vice-President Al Gore’s closest allies in the promotion of man-made global warming fears, is being publicly rebuked by his former supervisor at NASA.
Retired senior NASA atmospheric scientist, Dr. John S. Theon, the former supervisor of James Hansen, NASA’s vocal man-made global warming fear soothsayer, has now publicly declared himself a skeptic and declared that Hansen “embarrassed NASA” with his alarming climate claims and said Hansen was “was never muzzled.”  Theon joins the rapidly growing ranks of international scientists abandoning the promotion of man-made global warming fears.
“I appreciate the opportunity to add my name to those who disagree that global warming is man made,” Theon wrote to the Minority Office at the Environment and Public Works Committee on January 15, 2009. “I was, in effect, Hansen’s supervisor because I had to justify his funding, allocate his resources, and evaluate his results,” Theon, the former Chief of the Climate Processes Research Program at NASA Headquarters and former Chief of the Atmospheric Dynamics & Radiation Branch explained.
“Hansen was never muzzled even though he violated NASA’s official agency position on climate forecasting (i.e., we did not know enough to forecast climate change or mankind’s effect on it). Hansen thus embarrassed NASA by coming out with his claims of global warming in 1988 in his testimony before Congress,” Theon wrote.  [Note: NASA scientist James Hansen has created worldwide media frenzy with his dire climate warning, his call for trials against those who dissent against man-made global warming fear, and his claims that he was allegedly muzzled by the Bush administration despite doing 1,400 on-the-job media interviews! – See: Don’t Panic Over Predictions of Climate Doom – Get the Facts on James Hansen  – UK Register: Veteran climate scientist says ‘lock up the oil men’ – June 23, 2008 & UK Guardian: NASA scientist calls for putting oil firm chiefs on trial for ‘high crimes against humanity’ for spreading doubt about man-made global warming – June 23, 2008 ]
Theon declared “climate models are useless.” “My own belief concerning anthropogenic climate change is that the models do not realistically simulate the climate system because there are many very important sub-grid scale processes that the models either replicate poorly or completely omit,” Theon explained. “Furthermore, some scientists have manipulated the observed data to justify their model results. In doing so, they neither explain what they have modified in the observations, nor explain how they did it. They have resisted making their work transparent so that it can be replicated independently by other scientists. This is clearly contrary to how science should be done. Thus there is no rational justification for using climate model forecasts to determine public policy,” he added.
“As Chief of several of NASA Headquarters’ programs (1982-94), an SES position, I was responsible for all weather and climate research in the entire agency, including the  research work by James Hansen, Roy Spencer, Joanne Simpson, and several hundred other scientists at NASA field centers, in academia, and in the private sector who worked on climate research,” Theon wrote of his career. “This required a thorough understanding of the state of the science. I have kept up with climate science since retiring by reading books and journal articles,” Theon added. (LINK) Theon also co-authored the book “Advances in Remote Sensing Retrieval Methods.” [Note: Theon joins many current and former NASA scientists in dissenting from man-made climate fears. A small sampling includes: Aerospace engineer and physicist Dr. Michael Griffin, the former  top administrator of NASA, Atmospheric Scientist Dr. Joanne Simpson, the first woman in the world to receive a PhD in meteorology, and formerly of NASA, Geophysicist Dr. Phil Chapman, an astronautical engineer and former NASA astronaut, Award-Winning NASA Astronaut/Geologist and Moonwalker Jack Schmitt, Award-winning NASA Astronaut and Physicist Walter Cunningham of NASA’s Apollo 7, Chemist and Nuclear Engineer Robert DeFayette was formerly with NASA’s Plum Brook Reactor, Hungarian Ferenc Miskolczi, an atmospheric physicist with 30 years of experience and a former researcher with NASA’s Ames Research Center, Climatologist Dr. John Christy, Climatologist Dr. Roy W. Spencer, Atmospheric Scientist Ross Hays of NASA’s Columbia Scientific Balloon Facility] 
Gore faces a much different scientific climate in 2009 than the one he faced in 2006 when his film “An Inconvenient Truth” was released. According to satellite data, the Earth has cooled since Gore’s film was released,  Antarctic sea ice extent has grown to record levels, sea level rise has slowed, ocean temperatures have failed to warm, and more and more scientists have publicly declared their dissent from man-made climate fears as peer-reviewed studies continue to man-made counter warming fears. [See: Peer-Reviewed Study challenges ‘notion that human emissions are responsible for global warming’ & New Peer-Reviewed Scientific Studies Chill Global Warming Fears ]
“Vice President Gore and the other promoters of man-made climate fears endless claims that the “debate is over” appear to be ignoring scientific reality,” Senator James Inhofe, Ranking Member of the Environment & Public Works Committee.
A U.S. Senate Minority Report released in December 2008 details over 650 international scientists who are dissenting from man-made global warming fears promoted by the UN and yourself. Many of the scientists profiled are former UN IPCC scientists and former believers in man-made climate change that have reversed their views in recent years. The report continues to grow almost daily. We have just received a request from an Italian scientist, and a Czech scientist to join the 650 dissenting scientists report. A chemist from the U.S. Naval Academy is about to be added, and more Japanese scientists are dissenting. Finally, many more meteorologists will be added and another former UN IPCC scientist is about to be included. These scientists are openly rebelling against the climate orthodoxy promoted by Gore and the UN IPCC.
The prestigious International Geological Congress, dubbed the geologists’ equivalent of the Olympic Games, was held in Norway in August 2008 and prominently featured the voices of scientists skeptical of man-made global warming fears. Reports from the conference found that Skeptical scientists overwhelmed the meeting, with  ‘2/3 of presenters and question-askers hostile to, even dismissive of, the UN IPCC’ ( See full reports here & here ]  In addition, a 2008 canvass of more than 51,000 Canadian scientists revealed 68% disagree that global warming science is “settled.”  A November 25, 2008, article in Politico noted that a “growing accumulation” of science is challenging warming fears, and added that the “science behind global warming may still be too shaky to warrant cap-and-trade legislation.”  More evidence that the global warming fear machine is breaking down. Russian scientists “rejected the very idea that carbon dioxide may be responsible for global warming”. An American Physical Society editor conceded that a “considerable presence” of scientific skeptics exists.  An International team of scientists countered the UN IPCC, declaring: “Nature, Not Human Activity, Rules the Climate”. India Issued a report challenging global warming fears. International Scientists demanded the UN IPCC “be called to account and cease its deceptive practices.”
The scientists and peer-reviewed studies countering climate claims are the key reason that the U.S. public has grown ever more skeptical of man-made climate doom predictions. [See: Global warming ranks dead last, 20 out of 20 in new Pew survey. Pew Survey:   & Survey finds majority of U.S. Voters – ‘51% – now believe that humans are not the predominant cause of climate change’ – January 20, 2009 – Rasmussen Reports ] 
The chorus of skeptical scientific voices grow louder in 2008 as a steady stream of peer-reviewed studies, analyses, real world data and inconvenient developments challenged the UN’s and former Vice President Al Gore’s claims that the “science is settled” and there is a “consensus.”
On a range of issues, 2008 proved to be challenging for the promoters of man-made climate fears.  Promoters of anthropogenic warming fears endured the following: Global temperatures failing to warm; Peer-reviewed studies predicting a continued lack of warming;  a failed attempt to revive the discredited “Hockey Stick“; inconvenient developments and studies regarding rising CO2; the Spotless Sun; Clouds; Antarctica; the Arctic; Greenland’s ice; Mount Kilimanjaro; Global sea ice; Causes of Hurricanes; Extreme Storms; Extinctions; Floods; Droughts; Ocean Acidification; Polar Bears; Extreme weather deaths; Frogs; lack of atmospheric dust; Malaria; the failure of oceans to warm and rise as predicted.
# # #
ORIGINAL FULL TEXT LETTER SENT VIA EMAILS:
—–Original  Message—–
From: Jtheon [mailto:jtheon@XXXXXXX]
Sent: Thursday,  January 15, 2009 10:05 PM
To: Morano, Marc (EPW) 
Subject: Climate models are  useless 
Marc, First, I sent several  e-mails to you with an error in the address and they have been returned to me.  So I’m resending them in one combined e-mail. 
Yes, one could say that I was,  in effect, Hansen’s supervisor because I had to justify his funding, allocate  his resources, and evaluate his results. I did not have the authority to give  him his annual performance evaluation. He was never muzzled even though he  violated NASA’s official agency position on climate forecasting (i.e., we did  not know enough to forecast climate change or mankind’s effect on it). He thus  embarrassed NASA by coming out with his claims of global warming in 1988 in his  testimony before Congress. 
My own belief concerning  anthropogenic climate change is that the models do not realistically simulate  the climate system because there are many very important sub-grid scale  processes that the models either replicate poorly or completely omit.  Furthermore, some scientists have manipulated the observed data to justify their  model results. In doing so, they neither explain what they have modified in the  observations, nor explain how they did it. They have resisted making their work  transparent so that it can be replicated independently by other scientists. This  is clearly contrary to how science should be done. Thus there is no rational  justification for using climate model forecasts to determine public policy. 
With best wishes, John 
# # 
From: Jtheon [mailto:jtheon@XXXXXX]
Sent: Tuesday, January  13, 2009 12:50 PM
To: Morano, Marc (EPW) 
Subject: Re: Nice seeing you 
Marc, Indeed, it was a pleasure  to see you again. I appreciate the opportunity to add my name to those who  disagree that Global Warming is man made.  A brief bio follows. Use as much or  as little of it as you wish. 
John S. Theon Education: B.S.  Aero. Engr. (1953-57); Aerodynamicist, Douglas Aircraft Co. (1957-58); As USAF  Reserve Officer (1958-60),B.S. Meteorology (1959); Served as Weather Officer  1959-60; M.S, Meteorology (1960-62); NASA Research Scientist, Goddard Space  Flight Ctr. (1962-74); Head Meteorology Branch, GSFC (1974-76); Asst. Chief,  Lab. for Atmos. Sciences, GSFC (1977-78);  Program Scientist, NASA Global  Weather Research Program, NASA Hq. (1978-82); Chief, Atmospheric Dynamics &  Radiation Branch NASA Hq., (1982-91); Ph.D.,  Engr. Science & Mech.: course  of study and dissertation in atmos. science (1983-85); Chief, Atmospheric  Dynamics, Radiation, & Hydrology Branch, NASA Hq. (1991-93); Chief, Climate  Processes Research Program, NASA Hq. (1993-94); Senior Scientist, Mission to  Planet Earth Office, NASA Hq. (1994-95); Science Consultant, Institute for  Global Environmental Strategies (1995-99); Science Consultant  Orbital Sciences  Corp. (1996-97) and NASA Jet Propulsion Lab., (1997-99). 
As Chief of several NASA Hq.  Programs (1982-94), an SES position, I was responsible for all weather and  climate research in the entire agency, including the  research work by James  Hansen, Roy Spencer, Joanne Simpson, and several hundred other scientists at  NASA field centers, in academia, and in the private sector who worked on climate  research. This required a thorough understanding of the state of the science. I  have kept up with climate  science since retiring by reading books and journal  articles. I hope that this is helpful. 
Best wishes, John 

Sponsored IT training links:
Best quality 640-553 dumps written by certified expert to help you pass 642-456 and 70-536 exam in easy and fast way.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e991001c7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
When we last checked in to the Nansen Sea Ice Graphs, it looked like they were heading towards the “normal” line in a hurry. Ice area seems to still be on that trend, while extent seems to be leveling off it’s growth rate. Area appears to be within about 200,000 square kilometers of the 1979-2007 monthly average and still climbing.
Sea Ice Area - red line is current value, shaded area represents 1 standard deviation
Of course the fact that the 2007 data is included in the average line, means the average is a lower than usual target than one might expect. If we compare to ice area over at Cryopshere today, they use a 1979-2000 mean, which is higher.  Still the rebound we are seeing is impressive.
Sea ice extent looks like this:
Sea Ice Extent - red is current value, shaded area is 1 standard deviation
These graphs will automatically update, so check back often.
For those of you wondering, here is the difference between area and extent, as described in the NSIDC FAQ’s page:
What is the difference between sea ice area and extent? Why does NSIDC use extent measurements?
Area and extent are different measures and give scientists slightly different information. Some organizations, including Cryosphere Today, report ice area; NSIDC primarily reports ice extent. Extent is always a larger number than area, and there are pros and cons associated with each method.
A simplified way to think of extent versus area is to imagine a slice of swiss cheese. Extent would be a measure of the edges of the slice of cheese and all of the space inside it. Area would be the measure of where there’s cheese only, not including the holes. That’s why if you compare extent and area in the same time period, extent is always bigger. A more precise explanation of extent versus area gets more complicated.
Extent defines a region as “ice-covered” or “not ice-covered.” For each satellite data cell, the cell is said to either have ice or to have no ice, based on a threshold. The most common threshold (and the one NSIDC uses) is 15 percent, meaning that if the data cell has greater than 15 percent ice concentration, the cell is considered ice covered; less than that and it is said to be ice free. Example: Let’s say you have three 25 kilometer (km) x 25 km (16 miles x 16 miles) grid cells covered by 16% ice, 2% ice, and 90% ice. Two of the three cells would be considered “ice covered,” or 100% ice. Multiply the grid cell area by 100% sea ice and you would get a total extent of 1,250 square km (482 square miles).
Area takes the percentages of sea ice within data cells and adds them up to report how much of the Arctic is covered by ice; area typically uses a threshold of 15%. So in the same example, with three 25 km x 25 km (16 miles x 16 miles) grid cells of 16% ice, 2% ice, and 90% ice, multiply the grid cell area by the percent of sea ice and add it up. You’d have a total area of 675 square km (261 square miles).


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b42a7d5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"In the flurry of the holiday season, many people will have missed the government’s verdict on the 2014 badger culls, published on December 18. Farmers’ representatives have branded these recent culls “successful”, and environment secretary Liz Truss claims that they show how culling “can work to reduce disease”, confirming her plan to extend this controversial approach across western England. Cattle farmers have suffered terribly as a result of bovine tuberculosis (TB). Many are desperate, and would welcome a cull of badgers, which research (including my own) has shown to be a source of infection for cattle. Sadly, a closer look at the evidence suggests that the 2014 culls bring little hope of succour. Despite the environment secretary’s optimism, there is so far no evidence that these pilot culls have reduced disease. The government has commissioned research to estimate the impacts of pilot badger culling on cattle TB but no results have been published to date, nor are any benefits anticipated so soon after the start of the annual culls. Culled badgers have not even been tested for TB. Since changes in cattle TB take so long to emerge, in the short-term the government measures culling success in terms of reduced badger numbers. This is an appropriate measure because, perversely, killing too few badgers increases cattle TB rather than reducing it.  In a randomised controlled trial conducted in 1998-2007, cattle TB was consistently elevated where culling reduced indices of badger numbers by 10-35%. By contrast, nearby farms saw gradual reductions in cattle TB where large-scale culling reduced the same indices by 69-73%. To achieve similar benefits (and to avoid increasing cattle TB), the 2013-4 culls were intended to reduce badger numbers by at least 70%. The first two culls, conducted in 2013, clearly failed to achieve this aim. Government scientists, overseen by an independent expert panel, estimated the reduction in numbers by identifying individual badgers from hair entangled in barbed wire traps. They estimated that between 37 and 51% of badgers were killed in the Somerset cull zone, with between 43 and 56% killed in Gloucestershire. For the second year of culling, the government discarded both independent oversight and the hair trapping method which had revealed the first year’s failures. Before the 2014 culls commenced, the government’s planned monitoring methods were so inadequate that I warned “any future claim that the 2014 culls have reduced badger numbers sufficiently to control TB will be completely baseless”. Although ministers and farming representatives do indeed now claim success, the numbers tell a different story. There are no published estimates of the percent reductions achieved by the 2014 culls. Instead, claims of success are based on the number of badgers killed in Somerset, which reached the minimum target required by the culling licence (the Gloucestershire cull spectacularly failed to meet its target, killing just 274 badgers against a target of 615). Yet the Somerset target was derived from the lower bound on the range of possible badger numbers, rather than from the best estimate. If the estimation method was accurate, there would be a 97.5% chance that the true population size was greater than this lower bound, and hence that the target was too low. Despite having met this target, statistically it is still far more likely than not that the 2014 Somerset culls failed to reduce badger numbers by 70% as planned. Simple calculations provide further evidence of ineffective culling in Somerset. Government scientists estimate that, before any culling took place, the Somerset zone contained between 1,876 and 2,584 badgers. The total number of badgers killed (341 last year plus 955 in 2013) comprises just 69% of the lowest estimate. Taking into account the fact that births and immigration would have increased badger numbers between the two culls, the population cannot have been reduced by “at least 70%” if the government’s population estimates were correct. Government documents describe Somerset’s low target as “precautionary”. But from the perspective of disease control – the justification for killing otherwise protected wildlife – it risked worsening cattle TB and was hence the opposite of precautionary. With separate maximum targets in place to avoid killing too many badgers, the only risk reduced by a low target was the risk of a cherished project being branded a failure. Failing to reduce badger populations sufficiently risks exacerbating cattle TB, potentially making a bad situation worse. Farming leaders have managed to press forward with badger culling in the face of scientific consensus, legal challenge, public opinion and a groundswell of protest. In future they may look back on such victories as Pyrrhic: one more such victory might undo the farmers they strive to support."
"Alok Sharma, the new president of the Cop26 climate conference to be held in Glasgow in November, has experience of working closely with developing countries on the climate crisis in his former role as secretary for international development. This may be valuable in helping him forge the “grand coalition” that experts say is needed to break the deadlock on international climate action.  The last round of UN climate talks, in Madrid last December, showed the massive task that Britain will face as host this year in trying to build consensus on the issue. While more than half a million protesters from around the world lined the streets of the Spanish capital, inside the conference centre government officials squinted at semicolons in a dense text on how countries can buy and sell carbon. For almost three decades, world governments have met every year to forge a global response to the climate emergency. Under the 1992 United Nations Framework Convention on Climate Change, every country on earth is treaty-bound to “avoid dangerous climate change”, and find ways to reduce greenhouse gas emissions globally in an equitable way. Cop stands for conference of the parties under the UNFCCC. The UK will host Cop26 this November in Glasgow. In the Paris agreement of 2015, all governments agreed for the first time to limit global heating to no more than 2C above pre-industrial levels, and set out non-binding national targets on greenhouse gases to achieve that. However, these targets are insufficient, and if allowed to stand would lead to an estimated 3C of heating, which scientists say would spell disaster. For that reason, the Cop26 talks in Glasgow are viewed as the last chance for global cooperation on the emergency, with countries expected to come with tough new targets on emissions. The negotiations will be led by environment ministers and civil servants, aided by UN officials. Nearly every country is expected to send a voting representative at the level of environment secretary or equivalent, and the big economies will have extensive delegations. Each of the 196 nations on earth, bar a few failed states, is a signatory to the UNFCCC foundation treaty. The Cops, for all their flaws, are the only forum on the climate crisis in which the opinions and concerns of the poorest country carry equal weight to that of the biggest economies, such as the US and China. Agreement can only come by consensus, which gives Cop decisions global authority. Fiona Harvey Environment correspondent Two weeks of talks produced little more than a tetchy agreement to gather again this year for Cop26 (the 26th conference of the parties), with proposals for strengthening national plans to reduce emissions. Even that, in the context of the disasters that had threatened the talks, was better than some had feared. Since the Paris agreement was signed in 2015, the willingness of governments to tackle the climate crisis has waned. Donald Trump’s election as US president was the biggest factor – he has called climate science a “hoax” and begun the process of withdrawing the US from the agreement. That withdrawal will not take legal effect until 4 November, the day after the next US election. Emboldened by Trump, other countries have also started to backslide. Jair Bolsonaro, the president of Brazil, has embarked on a programme of exploitation of the Amazon, and in Madrid his officials worked hard to scupper any climate deal. They fought over details of an obscure clause of the Paris agreement governing carbon trading, which will now have to be resolved in Glasgow. Other countries were less vocal but no less inimical to progress. Saudi Arabia tried to hold up consensus, and Russia is also hostile to Paris. India, by siding with Brazil on carbon trading, bolstered the wreckers, but in other forums called for more urgent action under Paris. China’s stance was viewed as encouraging by Paris supporters, but it had little new to say. The EU made the boldest announcement, of a European green deal to transform the economy and reach net zero emissions by mid-century, but the details of its commitments are still subject to wrangling by member states. Patricia Espinosa, the UN’s top climate official, showed some frustration in her assessment. “We need to be clear that the conference did not result in agreement on the guidelines for a much-needed carbon market, an essential part of the toolkit to raise ambition. Developed countries have to fully address the calls from developed countries for finance, technology and capacity building, without which they cannot green their economies. High-emitting countries did not send a clear enough signal that they are ready to ramp up ambition.” All of this leaves the UK with a diplomatic mess to sort out. At Cop26, countries are supposed to come forward with new plans for stringent emissions cuts, in line with the science. Time is running out for those new plans to take effect, and without strong signals from governments the required changes will not be made. Arguably, the task facing Sharma is even harder than negotiating the 2015 Paris accord – at least the French government could rely on Barack Obama’s support, and a US-China agreement was fundamental to the success of Paris. In the four years since Paris was signed, while governments have dithered, businesses have carried on investing in fossil fuels and emissions have risen by a further 4%. Climate science, meanwhile, has grown stronger: the Intergovernmental Panel on Climate Change, the body of the world’s leading experts, said in 2018 that catastrophic climate breakdown would become inevitable within this decade unless the world changed course and started to bring global emissions down dramatically. “We need to really get across the point that this is not some minor adjustment that is required,” said Mary Robinson, the chair of the Elders, a campaigning group of senior world figures, and a former UN climate envoy. “The reality is that we need every company, every city, every country to be carbon neutral by 2050. If we can get that, then Cop26 really will be a game-changer.”"
"

The Clinton administration has developed a nasty habit of using personal tragedy to further its global warming agenda. From the snowmelt‐​caused Red River flood last year, to Florida’s fires this summer (which blazed because there was too much vegetation), to Hurricane Mitch, if there’s any possible way to conflate human suffering with global warming, the administration will do so. 



Administration antics on Mitch, a real son of a gun when it came to flooding rain, began during the recent Buenos Aires conference on global warming. There, the head of the U.S. Agency for International Development, J. Brian Atwood, told CBS News that Hurricane Mitch, which killed an estimated 10,000 Central Americans, was a “classic greenhouse effect.” One hopes that Mr. Atwood actually knows better and is merely engaging in White House huckstering. 



In 1974 Hurricane Fifi killed the same proportion of the (then smaller) population of Honduras. In 1971 Hurricane Edith plowed into the northwestern tip of Honduras at Cabo Gracias a Dios as a Category 5 blaster. In 1955 Hurricane Janet, another Category 5 storm, had hit a couple of hundred miles to the south of where Edith landed. Only two storms of that magnitude have ever hit the United States. Flooding is another recurring phenomenon. In 1979 Tropical Storm Claudette produced five feet of rain in Texas — just like Mitch in Honduras — but killed about 10,000 fewer people. 



Certainly Atwood’s staffers could have apprised him of the refereed scientific literature on global warming and hurricanes. It contains two speculative papers saying that hurricanes may get worse and an overwhelming number of others proving that notion wrong. In addition, hurricane observations in warm years and during planetary warming argue more for the opposite — weaker storms. 



If there’s any possible way to conflate human suffering with global warming, the Clinton administration will do so. 



The first paper, published in 1987 in Nature by Kerry Emanuel, speculated that under unrealistic, physically impossible conditions, global warming would increase the strength of hurricanes. While that paper generated a lot of press, scientists knew it was merely an exercise in hurricane vortex mathematics. Emanuel threw more fuel on the fire when he published an article on “hypercanes” in the widely read American Scientist. 



The larger community of hurricanologists had by then had enough. In 1994 James Lighthill published in the Bulletin of the American Meteorological Society an extensive review finding no basis for Emanuel’s speculation. 



In 1996 a storm surge of literature blew apart Emanuel’s hypothesis. First, the aptly named Chris Landsea, a scientist at the National Hurricane Center, observed in Geophysical Research Letters that there had been a significant decline in the frequency of severe hurricanes in the Atlantic Basin over the last 50 years. The region warmed a few tenths of a degree during the period. 



Later that year Johnny Chan published in the same Journal an article finding no net trend in Pacific typhoons. Even the computer modelers got into the act. Europe’s Lennart Bengtsson published a paper in the journal Tellus showing that a computer climate simulation with an enhanced greenhouse effect predicted fewer hurricanes and lower average winds. 



Also in 1996 J. B. Elsner found that the regions in which hurricanes form had shifted in the last 40 years and now favor the development of weaker storms. And Landsea, in yet another report published by the United Nations Intergovernmental Panel on Climate Change in 1996, showed that there has been a statistically significant decline in the maximum windspeed measured in Atlantic hurricanes since World War II. 



Australian climatologist Ann Henderson‐​Sellers and 10 others re‐​reviewed the hurricane/​global warming situation in last January’s Bulletin of the American Meteorological Society. They called their work a “Post IPCC Assessment,” meaning that they believed it stood for the consensus of scientists on the issue. They simply could not find any increase in hurricane frequency or severity, and they looked everywhere. They also took pains to note that the conditions assumed in Emanuel’s initial work are just about impossible. That work and its ilk contain “known omissions [that] all act to reduce these increases” of hurricanes, Henderson‐​Sellers wrote. 



It’s rare to get such scientific consensus in climatology. But in one last attempt to bring the exaggerators and the alarmists to heel, Henderson‐​Sellers recently published in the journal Climate Change a paper detailing the whole sorry history of the campaign to hype hurricanes. Ironically, Henderson‐​Sellers herself is not shy about touting the dangers of global warming. 



Who gains here? Rumors persist that Vice President Gore has been advised to make global warming a central theme of his presidential run in 2000. Threatening hundreds of thousands with imminent drowning unless they vote for him is a crude but probably effective trick.
"
"Renewable energy is generally viewed as a long-term solution to climate change. It’s no surprise then that a great deal of effort is going into to powering the world by using only renewables, and researchers are even looking seriously at the prospect of Europe switching to 100% renewable energy by 2050.  However, there is a downside – renewable energy depends on natural resources that exist on planet Earth in fixed amounts and are very much non-renewable. The issue of rare earth elements, used in many technologies including solar panels and batteries, is well known. Although these elements are not always as rare as their name suggests, they are finite and not renewable. Also, just one country, China, presently has a monopoly on the production of most of these elements, which raises the question of energy security.  But apart from rare earths, there are other non-renewable materials used for renewable energy – and the metal lithium is a good example. As it’s highly reactive and relatively light, lithium is ideal for use in batteries. And the ability to store large amounts of energy is crucial to renewable energy, because sunshine and wind don’t simply appear at convenient times when humans need electricity.  Another major application of lithium is in the batteries of electric and hybrid vehicles. These vehicles certainly have lots of potential to reduce carbon dioxide emissions but, in the long term, their feasibility will be challenged by the use of lithium in their batteries. A quick calculation shows that, if all conventional cars (those using petrol/gas or diesel) were replaced by electric cars, the world would run out of lithium in around five decades.  I take the total amount of lithium from the US Geological Survey, which estimates there are currently 14m tonnes of proven reserves worldwide. I used industry figures for the total amount of passenger cars sold worldwide – about 69m in 2016. That same year, less than a million electric vehicles were sold, even including plug-in hybrids.  Now, if we imagine a future where all passenger cars were electric and the number of cars sold per year remains constant at 2016 levels, almost 69m (technically: 69.46m minus 0.75m) electric cars will have to be produced each year even at a very cautious estimate. Our assumption here that the demand of cars will remain constant is actually very conservative, as demand typically increases with time. Today, a compact electric vehicle battery (Nissan Leaf) uses about 4kg (9lb) of lithium. This means, around 250,000 tonnes of lithium would be required annually to produce enough electric cars to replace their petrol equivalents. At this rate, the 14m tonnes of proven reserves would be exhausted within 51 years.  The recycling of lithium from used batteries is not taken into account here. But it is important to note that electric cars are not the only product that use lithium. Currently, batteries use around 39% of total production, while the rest goes into ceramics and glass, lubricating greases, and other applications. So even if we imagine 100% of lithium in used batteries was recovered (not technically possible), much of that would still be used for other purposes, and supplies would still eventually be exhausted. The world is not running out of lithium yet because renewable energy and electric vehicles are nowhere near replacing fossil fuels completely. Demand will increase in future, however, which could prompt further exploration and perhaps the discovery of new reserves, or even improvements in mining technology to make more of the metal accessible to us. All these could make lithium last longer, but that does not mean we will be able to use huge amount of it indefinitely. Lithium is just one example of a worrying reliance within renewable energy on non-renewable natural resources that exist only in fixed amounts on Earth. Solar and wind power do have great prospects of coping with the problems of climate change, but much careful planning is needed and we cannot assume that renewables will solve all environmental problems.  Now is the right time to establish recycling plants for rare earth elements and other non-renewable natural resources used in renewable energy systems such as lithium. More importantly, it is necessary to reduce our consumption of natural resources. If we go on with mindless consumerism, we will only shift the problem from one natural resource to another."
"

In between inventing the automobile, penicillin and electricity, growing up as a missionary on the Amazon and supporting his fatherless family of 13 as a bootblack, and inspiring hit musicals and epic poetry, Vice President Al Gore is acting as a commander in chief wannabe. 



His role as propagandist on behalf of the administration’s disastrous war of aggression in the Balkans is reason enough to reject him in the year 2000. But his record on domestic issues is even worse. 



The vice president’s campaign minions are saying that he is a tough leader who pushed for military action against Yugoslavia. Mr. Gore certainly is talking tough: “Milosevic has barely begun to incur the damage he will feel.” 



Of course, Mr.Gore probably couldn’t do worse than Bill Clinton, who has bungled every step. Were the latter commander in chief during World War II, we would all be speaking German. 



However, Mr. Gore is attempting to do more than score political points by warmongering for peace. Of late, he’s been battling airlines over compensation for lost bags and pushing to create a special phone number to call about traffic jams. For this, newly independent American Colonies created a national government? 



A Gore associate explained that such measures will “add up to something more thematic, something bigger.” And they do. The vice president once said he believes government should be “like grandparents, in the sense that grandparents perform a nurturing role.” 



But Mr.Gore prefers to “nurture” with a mailed fist. As former ABC correspondent Bob Zelnick puts it in his devastating new book, “Gore: A Political Life” (Regnery): “Al Gore Jr. was a child of government and a student of government who grew up to be a man of government.” 



The vice president has been traversing the country telling audiences he embodies “practical idealism.” However, he has been able to cultivate the image of a moderate primarily because he once took more conservative stands on security and social issues. But 28‐​year‐​old candidate Gore ran a populist economic campaign — higher taxes on the rich, support for public jobs creation — to win election in 1976 to Congress from Tennessee. 



He generally fit well within the Democratic caucus. He was a reliable supporter of new spending programs, whether business subsidies or redistributive entitlements; higher taxes, especially on the upper‐​middle class; increased regulation, particularly for environmental purposes; and social engineering schemes, such as racial quotas. 



Mr.Gore was at his worst on taxes. Between 1981 and 1993, he opposed only one of 19 significant tax increases; he voted to collect an extra $9,000 per household. He supported a plethora of other tax increases, which failed to pass. 



At least all of these measures required votes. The vice president also backed the multibillion‐​dollar e‐​rate levy (or “Gore Tax”) on phone service, which has been imposed without public debate by the Federal Communications Commission. 



The vice president has placed himself on the extremist edge of environmental policy‐​making. In his book, “Earth in the Balance,” he declared: “We must make the rescue of the environment the central organizing principle of civilization.” 



He has pushed a variety of new energy taxes. He wants employers to subsidize workers who don’t drive to work. He advocates eliminating the internal combustion engine. He proposes banning packaging that is neither biodegradable nor recyclable. He advocates more foreign aid to Third World states for environmental purposes. 



Perhaps Mr.Gore’s most important environmental crusade involves global warming. In no small part due to his efforts, the administration signed the Kyoto Protocol to the Framework Convention on Climate Change, which mandates substantial reductions in global energy consumption. 



Yet years of scaremongering have proved to be inaccurate. Observed warming has been far below that predicted by the models upon which the convention was based. Even Mr. Gore admitted in 1995: “In truth, the scientists who are expert in this field will tell you that the precise causal relationship (between C02 and global warming) has not yet been established.” 



The Kyoto Treaty, as yet unratified by the Senate, would impose huge burdens on the United States. 



Yale University economist William Nordhaus figures the bill could run $2,000 per household every year. Wharton Econometrics Forecasting Associates estimates 1.8 million jobs could be lost; others predict losses of as many as 3 million. 



Mr. Gore has attempted to disguise his statist bias by heading up the president’s program to “reinvent” government. However, his claim to have saved $137 billion is belied even by the National Performance Review’s own reports. Federal employment has not fallen due to his efforts. Mr. Gore has held press conferences rather than recommend eliminating useless agencies. 



Commander in Chief Al Gore? It’s a terrible thought. But even worse would be President Gore running domestic policy.
"
"
Share this...FacebookTwitter
I’ve just read the latest climate horoscope at the Hannoversche Allgemeine Zeitung website, which delivers them almost daily.
The latest one comes from the fortune tellers and scryers at the Massachusetts Institute of Technology, led by psychic Paul O’Gorman, now available at the PNAS here.
The latest horoscope foretells that (later) in the 21st century, summers will be stickier and grittier, and winters will be stormier – this according to visions and images delivered by crystal balls and gazings into MIT scrying pools.
Apparently MIT diviners made contact with the spirits of 1981 to 2000, so writes the HAZ, and felt the unsettling vibes of mystic energy of atmospheres past, and the energy intensity of past climatological storms. MIT’s assortment of sophisticated scrying instruments, made of silicone and crystal, all delivered similar predictions for the 21st century – forebodings all confirmed by their climate tarot punch cards.
The bad vibrations and ill spirits foretell one thing only: doom!
The 21st century
The northern hemispheric middle latitudes will be haunted by severe meteorological storms between the autumn and spring equinoxes, becoming especially intense before and after the winter solstices.
“I see storms and doom!”
For periods surrounding the summer solstices, crushing doldrums will beset northern middle latitude regions. Stagnate atmospheres will cause pollutants, and the evil spirits they harbor, to accumulate in ever higher concentrations above cities, bringing misery to non-believers.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Be forewarned! The degree of misery about to haunt the middle latitudes in the end will depend on the amount of ice surrounding the magnetic North Pole at the fall equinoxes.
The southern hemisphere will be visited by other misfortune, so say the MIT instruments of clairvoyance, and the diviners who gaze into them. There, ruthless storms will occur year-round, from solstice to solstice, from equinox to equinox.
Careful though, as other celestial alignments may impact the fortune tellers’ predictions. These predictions may change as they depend on what parts of the atmosphere are heavily impacted. If the earthly layer of the atmosphere energizes, then other currents and eddies come into play.
In the northern hemisphere, however, the heavenly layers of the atmosphere shall warm, and this will act to calm the air mass energy.
Come back tomorrow for more predictions!
For personal mystic climate fortune-telling and face-palm reading, make an appointment with Paul O’Gorman: pog@mit.edu.
UPDATE: I’ve just received a photo of just one of MIT’s highly confidential fortune telling instruments, with one of the teams of divine scryers who gaze at it:


Share this...FacebookTwitter "
"
And the hits just keep on coming…1,318,794 page views for January according to WordPress.

Thanks everybody!


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e987edbe3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**Six residents of a Dumfries and Galloway care home have died after a Covid outbreak, it has been confirmed.**
Dumfries and Galloway Health and Social Care Partnership is working with the operators of Alma McFadyen Care Centre in Dalbeattie.
A spokesperson for the health and social care partnership said it was an ""upsetting and concerning"" situation.
They added that the correct protocols for dealing with the pandemic were in place at the 24-bed home.
""This is obviously a very upsetting and concerning situation,"" the spokesperson said.
""We want to credit and pay tribute to the operators of Alma McFadyen and their extremely dedicated staff for their response.
""Covid-19 is incredibly infectious, and containing its spread is not at all easy - even when all the correct protocols are in place to address the virus.
""The coronavirus can result in mild symptoms and sometimes none at all, potentially masking its spread to more vulnerable individuals where it can pose a high degree of risk."""
"
Hard lesson about  solar realities for NOAA / NASA
Reposted here: October 30th, 2008
by Warwick Hughes

The real world sunspot data remaining quiet month after month are mocking the  curved red predictions of NOAA  and about to slide underneath. Time for a rethink I reckon NOAA !!
Here  is my clearer chart showing the misfit between NOAA / NASA prediction and  real-world data.

Regular  readers might remember that we started posting articles drawing attention to  contrasting predictions for Solar Cycle 24, way back on 16 December 2006. Scroll to the start  of my solar threads.
Then in March 2007 I posted David Archibald’s pdf article, “The Past and  Future of Climate”. Well worth another read now, I would like to see another  version of David’s Fig 12 showing where we are now in the transition from Cycle  23 to Cycle 24.
Solar Cycle 24  Prediction Issued April 2007 from NOAA / NASA
NOTE from Anthony: We now appear to have a new cycle 24 spot, which you can see here:

See the most current MDI and magnetogram here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9b7a42a2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The holiday season must be a time of mixed emotions for environmentalists critical of electrified America. It may be the season of good cheer and goodwill toward all, but it is also the time of the most conspicuous of all energy consumption. For the last month of the year, billions of small light bulbs illuminate America. Somber darkness and everyday lighting are transformed into magnificent beauty and celebration. Christmas lights are a great social offering — a positive externality in the jargon of economics — given by many to all.



Although doomsayer Paul Ehrlich once railed against “garish commercial Christmas displays,” energy conservationists have not engaged a public debate of the issue. Yet holiday season lighting is a glaring exception to the goal of reducing energy usage wherever possible. If holiday electricity guzzling is forgiven, shouldn’t open‐​air heating and cooling, bright central lighting and instant‐​on appliances that “leak” electricity be excused? Walking around the hotel room to turn on individual lights or waiting for the photocopier to warm up, after all, squanders the most scarce and depleting resource of all, a person’s time. Surely energy uses for human comfort and convenience, even when extravagant, should have priority over purely celebratory uses of electricity.



What about the holiday humbug that celebratory electricity usage depletes hydrocarbons, fouls the air and destabilizes climate? Good tidings abound! The world’s proven reserves of oil, natural gas and coal are at record levels. If probable resources are added to proven reserves, world supply is officially estimated at more than 2 thousand years for coal, 200 years for natural gas and 150 years for crude oil. Substitutes within the hydrocarbon family and derivatives from biomass make oil and gas inexhaustible.



The quality of our air has dramatically improved in recent decades despite record consumption of each hydrocarbon. As the Environmental Protection Agency stated in its last annual air‐​quality summary: “Since 1970, national total emissions of the six criteria pollutants declined 31 percent, while U.S. population increased 31 percent, gross domestic product increased 114 percent, and vehicle miles traveled increased 127 percent.” Few advocates of clearer air from industry, government or the environmental community believe that technological improvement of power plants and motor vehicles will not continue to hasten the clean air revolution. The only question is in the short‐​run cost to cushion the transition for consumers wed to affordable energy.



Are global warming and other climate change attributed to increasing concentrations of carbon dioxide in the atmosphere reason to decrease our use of coal, oil and, eventually, natural gas? Good tidings exist here as well. The warming properties of increased greenhouse gas concentrations in the atmosphere are more modest (and beneficial) than the new doomsayers are letting on. Climate models predicting high warming scenarios hinge on an unproven positive feedback with the most prevalent greenhouse gas, water vapor. Our most reliable global temperature records, from satellites and balloons, indicate that the enhanced greenhouse effect is highly overrated.



Carbon dioxide has never been regulated for a reason: it is not a pollutant but an environmental tonic that helps to “green” the earth through enhanced photosynthesis, improved use of water by plants and longer growing seasons. Carbon dioxide cycling, in short, is a continuing windfall of the hydrocarbon era.



Discretionary electricity consumption during the holiday season is more than a gift of beauty and goodwill, it benefits ratepayers as a class. With today’s electricity rates well above the marginal costs of generation and distribution in virtually every region of the country for almost all of the year, increased consumption allocates the utilities’ fixed cost over more units to lower rates overall. A study by Citizens for a Sound Economy estimated that increasing electricity usage up to 25 percent across the United States during the off‐​peak season (including December) would lower rates by a like amount since existing facilities would be more fully utilized. More holiday lighting may be only a small step toward more efficient use of our electricity infrastructure, but it is a beginning. There is much to be thankful for in our energy economy this holiday season.



All economic and environmental indicators for conventional energies are positive and open‐​ended. In the 1970s pervasive price and allocation regulation led to public edicts and private efforts to curtail holiday lighting, but today we find that market‐​oriented policies have made Christmas lighting more plentiful and affordable than ever. May one and all in good conscience enliven the darkness and lower electricity rates this holiday season. And with a more competitive electricity market on the horizon, and constantly improving technologies coming into play, Americans can look forward to ever‐​greater holiday celebrations in the years and decades ahead.
"
"
Share this...FacebookTwitterAnd attention morons, drones, and other like-minded believers in superstition and ritual behaviour.
At 8:30 p.m CET tomorrow evening it is Earth Hour. It’s that time again to participate in yet another useless collective ritual of madness, by switching off the lights along with all the other drones, in a bid to get the climate gods to bestow nice weather upon us (and even stop bush fires – to and solve all the other world’s problems. Let’s call it: Be Like North Korea Hour.
Here’s one example drone-controller spokesperson, treating Australians like a bunch three-year-olds, asking them to take part in this ridiculous embarrassment of a modern rain-dance:

Now that we are all done throwing up, let’s continue.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In Germany, parents tell their toddlers to finish their food on the plate, otherwise it’ll rain tomorrow. Well now it’s live in the dark, or else it’ll rain a lot, or not enough, or whatever. “Let’s all turn off our lights altogether now so that we can have a better future. Aren’t we all just good little boys and girls! Hooooray!”
Yes, it’s time to call the doctor. Personally, they can ram this retardation where it’s permanently dark.
I guess some creatures and criminals of the night will especially like this. Women stay home (and leave the lights on outside and inside).
Hungary International Airport to switch off tarmac lights!
And get a load of this craziness: A bunch of moron officials at the Budapest Hungary International Airport are going to play around with the tarmac lights! Talk about making the world a safer place to live. Read here.
Of course, we can counter all this madness. For example I’m going to upgrade my light bulbs.
This is better. Osram 200-watt incandescent bulb. (Photo credit: C. Kemme)
The 500-watt flood lights work well too.  Do your part to light up the world. Make your neighbourhood safer tomorrow. Light it up!
Further reading:
Celebrating ignorance
Celebrate Human Achievement Hour
It’s Earth Century in North Korea
Make it look like Christmas
Share this...FacebookTwitter "
"Labor MPs have sought to downplay suggestions of a breakaway pro-coal group within the party, as the Coalition seized on the report to accuse the opposition of divisions over climate policy. Following a report by Channel Ten on Wednesday night that a group of about 20 right-aligned Labor MPs dined regularly to canvass policies in support of coal workers, attention shifted from the Coalition’s internal ructions over climate policy to Labor.  The Labor leader, Anthony Albanese, said he had been unaware of the factional dinner, but downplayed its significance, and said his MPs were united on wanting action on emissions reduction. “That’s what happens in Canberra – people go out and people chat about ideas. There is nothing unusual about this,” he said. “The Labor party is united in our position that climate change is real, that we need to act on lowering our emissions. “Good action on climate change means more jobs, lower emissions and lower energy prices.” The Channel Ten report said that the group of 20 MPs had become known as the “Otis group” in reference to their restaurant of choice in Canberra. The senior right-aligned senator Don Farrell said the group was made up of “good solid Labor people, interested in supporting coal workers”. The Coalition turned on Labor over the report in parliament on Thursday, with the prime minister, Scott Morrison, accusing the opposition of having “no alternative policy” on climate change. “When 20 members of the Labor party gather at the Otis restaurant, what I’m more mystified by is that they can actually find a consistent position of the leader of the opposition that they can actually oppose,” Morrison said. “I’m staggered that they can find any consistency in the leader of the opposition’s policy on emissions or electricity or coal or any of these things, because he has it each way every day. “My advice to those who are meeting down at the Otis regularly is to just wait until tomorrow because he’ll have another policy.” The deputy prime minister, Michael McCormack, who is facing an uprising from Queensland Nationals over his leadership style, used a question on the party’s approach to developing regional Australia to also target Labor over its “rebel group”. He read from a menu from the Otis restaurant to say that the Labor MP for the seat of Hunter, Joel Fitzgibbon, had “egg all over his face” over the report. McCormack faced a spill motion last week, largely driven by disgruntled Nationals MPs who want the Coalition to support the development of a new coal-fired power station in Queensland. Both the Coalition and Labor are grappling with how best to frame climate change policy ahead of the next election, with the parties under pressure to take action without losing political ground in coal-mining seats in Queensland and New South Wales. Nationals MPs are arguing that the Coalition won the election on the back of support from regional Queensland, where the LNP promised support for a feasibility study into a coal-fired power station in Collinsville in the seat of Capricornia. But moderate MPs want the government to adopt a more “ambitious” climate change policy to respond to the concerns in inner city Liberal-held seats, including some who want to sign up to the commitment of a net zero carbon emissions target by 2050. Two moderate Liberal MPs – Trent Zimmerman and Dave Sharma – have also both declared that the federal government should not be in the business of underwriting coal-fired power stations. The minister for emissions reduction, Angus Taylor, told parliament on Thursday that the government was considering “all technologies”, including renewables and coal. “We’ll support all technology that drives down the price of electricity, that keeps the lights on and that brings down emissions,” Taylor said. For Labor, the meeting of right-aligned Labor MPs comes as Albanese indicates the party won’t be rushed into announcing a new climate change policy. While the party’s shadow minister for climate change, Mark Butler, said after the election that the party’s policies for ambitious action are “unshakeable”, some of the party’s MPs have warned against moving too far to the left. On Sunday, the party’s deputy leader, Richard Marles, did not rule out Labor supporting new coal developments, saying coal would remain an important part of the economy for “decades to come”. Labor’s shadow resources minister, Joel Fitzgibbon, who suffered a large swing against him in the coal seat of Hunter, has also been agitating for the party to do more to support the mining industry. But amid the simmering tensions, MPs sought to downplay the Otis group’s significance, with Marles calling the report of the dinner a “total beat up”. “At the end of the day, people were having a dinner,” he told Sky News. “I don’t think it’s a big deal.” The Victorian senator Kimberley Kitching said on Twitter that she was “pro coal-worker” and all workers, but was also “passionate about renewables and the jobs they bring”. “ALP conferences determine policy not dinner parties,” she said."
"
Share this...FacebookTwitterIn the older days, whether selling detergents, applicances or food products, marketers often used the slogan: “new and improved!” to con consumers and to boost sales. The German online DIE WELT reports that companies have changed their marketing slogans to make their business more profitable and sustainable. The new slogan used to pitch products today is:
Almost every conceivable company is jumping on the enviro-bandwagon and claiming their products or services are “sustainable” and thus good for the planet, no matter if it’s an automaker, coal power plant, or an investment instrument. The “sustainable” product is better and safer for the environment. The movement indeed is religious. DIE WELT writes:
You can now invest sustainably, and even fight dandruff in a sustainable way.”
Well, I turned off the light in the room next door, and so now I’m blogging sustainably. In fact I just changed the slogan of my blog. I’m the first climate blogger to blog more sustainably – the world’s most sustainable climate science blogger. Blog here! Going to any other climate sicence blog means you’ll be ruining the planet.
Misuse of the word
As DIE WELT writes, not everyone is amused about companies slapping the “sustainable” slogan on the packaging of their products, and claim it borders on false advertising in many cases. (Not me. I really am blogging with the light off and drinking tap water). Author Ulrich Grober has written a book on the history of sustainability, and is quoted by Die Welt:
Indeed even oil companies like BP use the word “sustainable“ in their annual reports. ‘Recently in Switzerland the most “sustainable” autobahn of all time was inaugurated“, says Grober. it clouds the meaning of the word.’ “
Experts say the word “sustainable” is now being used so often and so incorrectly that it has virtually lost all its original meaning. Sociologist Klaus Kraemer says the word “sustainability is now being used in political debates as the ultimate moral argument. “Whatever is sustainable is not to be questioned.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Use of the word “sustainable” is dangerous
The term being misused is one thing. But using the word for the purpose of marketing may be “dangerous”, says chemist Michael Braungart. Die Welt quotes Braungart:
The concept is backwards-looking and puts the brakes on creativity because it is connected with feelings of guilt.”
I agree with that. If you don’t buy a product that is labelled as “sustainable”, then you are someone who is harming the planet, and so you ought to feel bad about it. That’s how a religion works. So in the end, I think there is going to be a backlash with respect to this blind sustainability movement. The whole thing is rather Medieval. Back to the Dark Ages.
Educated consumers really ought to feel insulted
How do I feel when I see the “sustainable” slogan being targeted at me? Of course it annoys the hell out of me because I feel the seller of the “sustainable” product assumes that I’m actually stupid enough to believe all the CO2 nonsense. I’m insulted that they’d treat me like that. I’m not a blind zealot in a cult. It’s a slogan that maybe works well with morons, dupes and religious greens. But it certainly isn’t a way to communicate with people who think for themselves.
The enviro-sustainable hacks and bosses behind this movement don’t even believe it themselves. See how they jetset all over the world and live lavish lives while raking it in as duped consumers gobble it up. To the half-witted believers out there – wake up – you’re being duped by this utter nonsense.
To my loyal readers, please do not think that I’m targeting you with my new slogan. I know you don’t believe the CO2/sustainability crap. The new blog slogan is aimed solely at other people, like dana1981.
(PS: Actually I have the lights on – and it’s daytime.)
Share this...FacebookTwitter "
"

Media Contact: (202) 789‑5200



The Cato Institute announced today the expansion of its Center for the Study of Science. Founded in 2012, the Center for the Study of Science was created to provide market‐​based ideas that could transition policy regarding energy consumption, environmental standards, and other science‐​related issues away from government planners.



Today, the Center is adding scholars to a team that will continue to use rigorous science to answer questions related to environmental regulation. The Center will be especially focused on the debate over climate change.



Patrick J. Michaels, who will continue to direct the center, acknowledges climate change is occurring partly due to human actions. He does not believe, however, that these temperature fluctuations are cause for great alarm.



“Yes — burning fossil fuels to get the energy we need to advance as a global society does create carbon dioxide that recycles warming in the lower atmosphere,” said Michaels. “But despite what some scientists and politicians tell you, life as we know it will not end next week, next month or even in the next 500 years due to a warming planet. Policy makers need to know that there is a respectable group of scientists out there who don’t buy in to the alarmist hype.”



President Obama is pursuing an international agreement on carbon emissions that sidesteps Congressional ratification, an issue he is expected to discuss at a United Nations climate summit in New York next week. Michaels warns the President is “playing fast and loose with the Constitution.”



“We believe that some highly qualified scientists should be taking a more clear‐​eyed look at the data policy makers are using to draw conclusions which have resulted in a regulatory structure that inhibits economic activity and stifles innovation,” said Michaels.



Ross McKitrick, who teaches environmental economics at the University of Guelph, and Terence Kealey, Vice Chancellor of the University of Buckingham and a professor of clinical biochemistry, have been named adjunct scholars at the Center. They join Distinguished Senior Fellow Richard Lindzen, an emeritus professor of meteorology at both MIT and Harvard; Adjunct Scholar Edward J. Calabrese, a professor of environmental health sciences at the University of Massachusetts, specializing in toxicology; and Paul C. “Chip” Knappenberger, assistant director of the Center.



The Center for the Study of Science will seek to provide a credible source for media and members of the public who want a fresh perspective on scientific claims made by government and other research organizations. Research areas will include energy use and taxation; use of government subsidies; global warming; and overall environmental regulation.



“The truth is, counter to what President Obama claimed in 2010, the science on climate change is not settled,” said Cato President and CEO, John Allison. “The time is now to build a critical mass of credible scholars who can engage in the type of debate the public needs to hear in order to make informed decisions.”



Michaels said additional scholars and scientists will be named to the Center for the Study of Science in the coming months.
"
"
Share this...FacebookTwitterMinus 30°C for days…13°C below normal…homeless people dying…hands and feet are freezing…
That’s what we are hearing from a few media outlets in Europe, those who have dared to mention the “cold-snap” word and to write about reality. It’s been cold in Scandinavia, much of Europe, North America and Russia too. Where’s all the warming? Heck, even the oceans are below normal.
Deep freeze is forecast to continue
The European part of Russia is stuck deep in the freezer, reports the Austrian online Krone.at. The extreme cold is due to a huge high pressure system in the Arctic which has kept Moscow in temperatures down as low as -30°C for days. Krone.at writes:
The Russian media have been talking about ‘the hardest winter in the last 100 years’, causing 10 million people to shiver.
”This abnormal frost has been an enormous challenge,’ says Moscow mayor Sergei Sobjanin. Meteorologists don’t see any let up in the days ahead, and even expect temperatures to drop further. In the European part of Russia, unusually deep cold has dominated the area over the last 14 days. The average temperature for February so far alone for Moscow is 11 to 13°C below normal.”
There are reports that homeless people are getting hit hard. Pleas for blankets and clothing are being made. Famous Moscow doctor Elisabeth Glinki says:
Many people on the street are dying, or their hands and feet are freezing.”
Looking at the above temperature forecast chart above, things are going to get even worse in the days ahead.
But we all know what the explanation for this is, right!
Share this...FacebookTwitter "
"
AIRS has higher resolution tracking of global CO2 - click for image
I’m going to make a formal post on this later, but I wanted to bring it up for discussion now since many people have been waiting for this paper to be published. For my previous perspectives and replies from authors, see this post here:
An encouraging response on satellite CO2 measurement from the AIRS Team
Hat tip to F Rasmin who writes with a link to the new paper:
Hello Anthony. Is this the awaited paper from the AIRS TEAM? ‘Satellite remote sounding of mid-tropospheric CO2′, published 9 September 2008 at:
http://www.agu.org/journals/gl/gl0817/2008GL035022/ 

REPLY: Yes it is. This was on my list of things to check this week, thanks for the tip! I’ll write it up sas soon as I can read it. In the meantime, feel free to post more comments on it in this thread.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9c4bac48',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

“Of course you realize, this means war!” – Bugs
War has been declared in the New  York court system over global warming regulation.
Indeck Corinth L.P., which operates the Corinth Generating Station, an  electric power plant in Corinth, NY, sued New York stateon January 29, 2009 claiming that the Regional  Greenhouse Gas Initiative (RGGI) that aims to reduce greenhouse gas emissions in  the Northeast U.S. is illegal.
Corinth Generating Station -click for interactive view- Source: Microsoft Live Earth
Maine, New Hampshire, Vermont, Connecticut, New York, New Jersey, Delaware, Massachusetts, Maryland, and Rhode Island have signed on to the RGGI agreement. You can read more about it here at:  http://www.rggi.org/home
This is the simple view of RGGI from their website:
The Regional Greenhouse Gas Initiative (RGGI) is the first mandatory, market-based effort in the United States to reduce greenhouse gas emissions. Ten Northeastern and Mid-Atlantic states will cap and then reduce CO2 emissions from the power sector 10% by 2018.
States will sell emission allowances through auctions and invest proceeds in consumer benefits: energy efficiency, renewable energy, and other clean energy technologies. RGGI will spur innovation in the clean energy economy and create green jobs in each state.
Indeck Corinth claims that New  York’s involvement with RGGI does the following:

Is ultra vires and violates the state constitution;
Imposes an impermissible tax not authorized by the state legislature; 
Is arbitrary and capricious as implemented by New York; 
Is pre-epmted by state and federal  regulations; 
Violates the Compact Clause of the U.S.  Constitution; and 
Violates Indeck Corinth’s due process and  equal protection rights 

See Indeck Corinth’s legal complaint.  (PDF)
Indeck Corinth and New  York State  are now arguing over the venue for the suit. Indeck Corinth wants the suit heard in Saratoga County where it is a major employer. New York wants the suit heard in Albany County where it has home field advantage.
This will be watched intensely by many on both sides of the energy -versus- environment issue.
h/t to Junkscience.com


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9708bedc',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The Indian environmentalist Rajendra Kumar Pachauri, under whose leadership a UN climate change panel shared the 2007 Nobel peace prize, has died after recent heart surgery. He was 79. Pachauri’s death was announced late on Thursday by the Energy and Resources Institute (TERI), a research group he headed until 2016 in New Delhi.  He chaired the Intergovernmental Panel on Climate Change panel from 2002 until he resigned in 2015 after an employee at his research firm accused him of sexual harassment. The IPCC and the former US vice-president Al Gore were awarded the 2007 Nobel for their efforts to expand knowledge about anthropogenic climate change and lay the foundations for counteracting it. Pachauri had undergone surgery in a New Delhi hospital this week. He died at his home on Thursday, the Press Trust of India reported. Pachauri won civilian awards from India’s government in 2001 and 2008. TERI’s chairman, Nitin Desai, hailed Pachauri’s contribution to global sustainable development. “His leadership of the Intergovernmental Panel on Climate Change laid the ground for climate change conversations today,” Desai said. The allegations against Pachauri included claims that he sent suggestive text messages, e-mails and WhatsApp messages harassing a 29-year-old female employee in his organisation. Pachauri had denied the charges and his attorneys claimed his messages were hacked in an attempt to malign him. New Delhi police filed a complaint in court but the trial could not be completed. Professor Jean-Pascal van Ypersele, the IPCC vice-chairman from 2002 to 2015, said coming from a developing country Pachauri should be credited for drawing the attention, long before others, to the importance of finding synergies between climate policies and sustainable developing agenda. “Unfortunately he was sometimes overconfident as when he refused to quickly acknowledge and correct the insignificant error that had been present in an IPCC report. This led to escalated and undue criticism of the organisation he chaired.” Pachauri is survived by his wife, a son and a daughter."
"There is a side to the Ebola crisis that, perhaps understandably, has received little media attention: the threat it poses to our nearest cousins, the great apes of Africa. At this moment in time Ebola is the single greatest threat to the survival of gorillas and chimpanzees. The virus is even more deadly for other great apes as it is for humans, with mortality rates approximately 95% for gorillas and 77% for chimpanzees (Pan troglodytes). Current estimates suggest a third of the world’s gorillas and chimpanzees have died from Ebola since the 1990s. As with humans, these deaths tend to come in epidemics. In 1995, an outbreak is reported to have killed more than 90% of the gorillas in Minkébé Park in northern Gabon. In 2002-2003 a single outbreak of ZEBOV (the Zaire strain of Ebola) in the Democratic Republic of Congo killed an estimated 5,000 Western gorillas (Gorilla gorilla). It’s hard to accurately count such elusive creatures but the WWF estimates there are up to 100,000 left in the wild – so a single Ebola outbreak wiped out a considerable chunk of the world’s gorilla population. There are of course additional factors behind the declining numbers of Africa’s great apes: illegal trading in wildlife and bushmeat, war, deforestation and other infectious diseases. The world’s remaining wild apes are being increasingly forced into isolated pockets of forest, which impedes their ability to forage, breed and to hide from hunters. There is also a growing body of evidence linking deforestation and subsequent changes in climate to the spread of Ebola and other infectious diseases. Back in 2003 an article on the decline of great apes, written by a team led by primatologist Peter Walsh, predicted that: Without aggressive investments in law enforcement, protected area management and Ebola prevention, the next decade will see our closest relatives pushed to the brink of extinction. Sadly, this prediction appears to have come true. Since 2008, the IUCN has listed the Eastern Gorilla (Gorilla beringei) as endangered and the Western Gorillas as critically endangered. If we do not act fast, these may prove to be the last decades in which apes can continue to live in their natural habitat. Unfortunately, there appears to be a lack of political will to implement policies which would bring viable solutions into effect.  We need both short-term solutions to halting the spread of Ebola and long-term ones to prevent future outbreaks. As a short-term strategy, vaccination could prove enormously useful in tackling the Ebola crisis in apes. Unlike for humans, a vaccine for gorillas and apes has been developed which thus far has been proven both safe and effective.  To date though, these trials have not involved “challenging” the vaccinated chimps with the live virus. Across much of Europe, medical research on great apes is either banned or highly restricted because of their cognitive similarity to humans. The question is whether or not we should make an exception in this case. In the long term, conservation efforts aimed at restoring forest habitat could also help curb the spread of the virus, as larger forested areas would reduce the chances of infected animals coming into contact with one another. In tandem with forest regeneration, greater protection for apes from hunters and strict laws to control bushmeat consumption would also be hugely beneficial, both for apes and for humans."
"

Not long ago, a group of Cato scholars entertained the question of whether the intellectual debate for free trade had been won.



There was near consensus that it had — in 1776 with publication of _The Wealth of Nations_. In the 240 years to follow, efforts to poke substantive holes and refute Adam Smith’s treatise failed and, today, nearly all economists agree that free trade, by expanding the size of the market to enable greater specialization and economies of scale, generates more wealth than any system that restricts cross‐​border exchange.



What that Cato confab failed to produce was agreement about whether the question under consideration was even pertinent. After all, how much does it really matter whether the intellectual debate has been won when, in practice, free trade remains stubbornly elusive, and the process of U.S. trade policy formulation is distinctly antiintellectual? Consider trade agreements. At the heart of negotiations that produce these deals rests the fallacy that domestic trade barriers are assets to be dispensed with only if reciprocated, in roughly equal measure, by negotiators on the other side of the table. That’s not Adam Smith. That’s neo‐​mercantilism, which posits that policy should aim to maximize exports and minimize imports. Yet Smith is credited with vanquishing mercantilism, which held sway in his day — and apparently still does today.



If the free trade consensus were truly meaningful, trade negotiations would be unnecessary. If free trade were the rule, trade policy would have a purely domestic orientation and U.S. barriers would be removed without need for negotiation because they would be recognized for what they are: taxes on consumers and businesses that impede the global division of labor and the creation of wealth. Apparently, the intellectual consensus for free trade coexists with an absence of free trade and a persistence of protectionism in practice.



For example, in the United States, there are “Buy American” rules that restrict most government procurement spending to U.S. suppliers, ensuring that taxpayers get the smallest bang for their buck; heavily protected services industries, such as transportation and shipping, that drive up the cost of everything; apparently interminable farm subsidies; quotas and high tariffs on imported sugar; high tariffs on basic consumer products, such as clothing and footwear; energy export restrictions; the market‐​distorting cronyism of the Export‐​Import bank; antidumping duties that strangle downstream industries and tax consumers; regulatory protectionism masquerading as public health and safety precautions; rules of origin and local content requirements that limit trade’s benefits; restrictions on foreign investment, and so on.



If an intellectual consensus for free trade exists, policy doesn’t reflect it and politicians appear to abhor it. If anything, the 2016 presidential election season reveals an American public — pitchforks and scythes in hands — ready to storm the ivory tower.



 **TRADE IS RIPE FOR DEMAGOGUERY**  
To cheering crowds, Donald Trump promises to slap duties on imports from China and Mexico and to use the tax code to punish U.S. companies that outsource parts of their operations abroad. Bernie Sanders vows to tear up NAFTA and other free trade agreements, calling them “a disaster for American workers.” Hillary Clinton, a co‐​architect of the Trans‐​Pacific Partnership trade agreement (TPP), now opposes that deal, while promising to disregard certain U.S. treaty obligations with China. Ted Cruz, projecting the pain of workers who have been displaced by import competition and outward investment (but, apparently, not those displaced by technology, changing consumer tastes, or poor business management), says trade has been “unfair” and pledges to “bring our jobs back from China.”



Scapegoating trade for problems real and imagined is nothing new. Blaming the Japanese, Mexicans, Chinese, and other foreigners for domestic woes ingratiates politicians to excitable elements of the electorate and helps them direct voter anger away from their own records. It has become a kind of quadrennial tradition ever since the NAFTA debate took center stage in the 1992 election.



Throughout the 2012 campaign, Mitt Romney assailed President Obama for failing to label China a “currency manipulator,” and the candidates exchanged accusations about who was more “culpable” for “shipping jobs overseas.” Promising to bring manufacturing jobs back home, Rick Santorum resonated with trade‐​skeptical voters, and even won the Iowa caucus that year. In 2008 Senators Obama and Clinton vied to be seen as the supreme trade‐​rules enforcer, each pledging to force U.S. trade partners back to the table to renegotiate NAFTA and various World Trade Organization agreements to make the terms “fair” for American workers. Demonization of trade was also a major component of John Edwards’s divisive “Two Americas” message that year.



John Kerry tapped into the same vein of public anxiety in 2004, referring to U.S. businesses that outsource call centers to places like India as “Benedict Arnold” companies. Blaming Mexico, Japan, and inside‐​the‐​beltway complicity for U.S. manufacturing decline and the erosion of American power, Pat Buchanan promised to punch back with force. His populist message energized the feisty “Buchanan Brigades” and helped him win the New Hampshire primary in 1996.



Trade‐​bashing became popular during the 1992 election, as books about the United States “trading places” with an ascendant Japan flew off the shelves and Ross Perot warned of the imminence of a “giant sucking sound” coming from south of the border. So campaigning politicians denigrating trade is nothing new. It seems to be inextricably woven into the fabric of our presidential elections. But something seems different this year. The tone is harsher. The digs are coming from across the political and ideological spectra. Two of the candidates — Sanders and Trump — seem genuine in their antipathy and their resolve to act. And their messages resonate especially well with primary election voters, who tend to hail from the extremities of the major parties, where trade and globalization are viewed with the greatest skepticism. But, again, these constituencies and their concerns aren’t particularly new either.



What is new — at least for the first time since NAFTA loomed large 24 years ago — is that a major trade agreement (indeed, the largest preferential trade agreement in U.S. history) is being debated and possibly considered for ratification by the U.S. Congress this year. Trade policy has featured prominently in the public square since January 2015, when the president and the new congressional leadership began their push to secure passage of Trade Promotion Authority (TPA) to facilitate completion and ratification of the TPP and, eventually, the Transatlantic Trade and Investment Partnership.



Although the TPA debate itself was shortlived, with the legislation passing in June of last year, anti‐​trade lobbies such as the Sierra Club, the AFL-CIO, and Public Citizen have been mobilizing for several years in anticipation of an epic battle over the TPP. Their anti‐​trade campaigns, with assertions and slogans evoking fantastical worst‐​case scenarios about the relationship between trade and climate change, trade and cancer rates, and trade and joblessness have played to popular fears, and have succeeded in winning more people to their cause. Protectionist ranks have been augmented by those with other kinds of economic grievances in a way that evokes _New York Times_ columnist Thomas Friedman’s 2001 description of the antiglobalization movement as the “well‐​intentioned but ill‐​informed being led around by the ill‐​intentioned and well‐​informed.” Though they are not necessarily wellinformed, the 2016 presidential candidates are complicit in creating this climate of misinformation.



 **UNSEEN CREATION**  
The case for free trade is not obvious. The benefits of trade are dispersed and accrue over time, while the adjustment costs tend to be concentrated and immediate. To synthesize Schumpeter and Bastiat, the “destruction” caused by trade is “seen,” while the “creation” of its benefits goes “unseen.” We note and lament the effects of the clothing factory that shutters because it couldn’t compete with lower‐​priced imports. The lost factory jobs, the nearby businesses on Main Street that fail, and the blighted landscape are all obvious. What is not so easily noticed is the increased spending power of the divorced mother who has to feed and clothe her three children. Not only can she buy cheaper clothing, but she has more resources to save or spend on other goods and services, which undergirds growth elsewhere in the economy.



Consider Apple. By availing itself of lowskilled, low‐​wage labor in China to produce small plastic components and to assemble its products, Apple may have deprived U.S. workers of the opportunity to perform that low‐​end function in the supply chain. But at the same time, that decision enabled iPods and then iPhones and then iPads to be priced within the budgets of a large swath of consumers. Had all of the components been produced and all of the assembly performed in the United States — as President Obama once requested of Steve Jobs — the higher prices would have prevented those devices from becoming quite so ubiquitous, and the incentives for the emergence of spin‐​off industries, such as apps, accessories, Uber, and AirBnb, would have been muted or absent.



But these kinds of examples don’t lend themselves to the political stump, especially when the campaigns put a premium on simple messages. This is the burden of free traders: Making the unseen seen. It is this asymmetry that explains much of the popular skepticism about trade, as well as the persistence of often repeated fallacies.



 **THE MYTHS**  
One of the most frequently invoked trade myths is the portrayal of trade as a competition between “us” and “them.” Central to this perception is that exports are Team America’s points, imports are the foreign team’s points, and the trade account is the scoreboard. Since that scoreboard shows a deficit, the United States is losing at trade, and it’s losing because the foreign team cheats — too often with impunity. Sound familiar?



This fundamental mercantilist fallacy about the nature of trade has a nationalistic appeal, where America is some monolithic entity best served by policies that strengthen her stature vis‐​à‐​vis some foreign monolith. But trade does not occur between countries. Trade is the culmination of billions of daily transactions pursued by individuals seeking value through exchange.



When we transact at the local supermarket, we seek to maximize the value we obtain by getting the most for our dollars. We strive to “import” more than we “export.” But when it comes to trading across borders or when our individual transactions are aggregated at the national level, we tend to forget these basic principles and accept the fallacy that the goal of trade is to achieve a surplus. But, as Adam Smith put it: “What is prudence in the conduct of every private family can scarce be folly in that of a great kingdom.” Never mind the intellectual consensus: This is common sense.



The benefits of trade come from imports, which deliver more competition, greater variety, lower prices, better quality, and new incentives for innovation. Arguably, opening foreign markets should be an aim of trade policy because larger markets allow for greater specialization and economies of scale, but real free trade requires liberalization at home. The real benefits of trade are measured by the value of imports that can be purchased with a unit of exports — our purchasing power or the so‐​called terms of trade. Trade barriers at home raise the costs and reduce the amount of imports that can be purchased with a unit of exports.



And as a result of globalization — the proliferation of cross‐​border investment and transnational supply chains — trade is more of a collaboration than ever before. Typically, about half of the value of U.S. imports is composed of intermediate goods and capital equipment — the purchases of U.S. producers.



How can imports be viewed as the other team’s points under those circumstances? Who, in fact, are “we” and who are “they”? The claim that the trade deficit means we are losing at trade — “losing billions of dollars every year to China and Mexico,” as Trump characterizes it — is another commonly invoked trade myth, which reflects a fundamental misunderstanding of international economics. By purchasing more goods and services from foreigners than foreigners purchase from Americans — trade deficit scolds claim — U.S. factories, farmers, and service providers are deprived of sales, which reduces domestic output, value added (GDP), and employment. That conclusion relies on the assumption that the dollars sent to foreigners to purchase imports do not make their way back into the U.S. economy. The dollars that go abroad to purchase foreign goods and services (imports) and foreign assets (outward investment) are matched nearly identically by the dollars coming back to the United States to purchase U.S. goods and services (exports) and U.S. assets (inward investment). Any trade deficit (net outflow of dollars) is matched by an investment surplus (net inflow of dollars).



This process helps explain why GDP and the trade deficit rise and fall in tandem, and why 41 consecutive years of trade deficits have had no adverse impact on the economy. The fallacy that trade killed U.S. manufacturing has long been a pretense for protectionism or industrial policy. Trump follows in these footsteps when he writes:





U.S. manufacturing is not only alive, it’s thriving. By all relevant metrics — output, value‐​added, revenues, exports, imports, investment, R&D expenditures — U.S. manufacturing remains a global “powerhouse.” With respect to most of those measures, year after year the sector sets new records. U.S. manufacturing attracts more foreign direct investment (FDI) than any other country’s manufacturing sector. In 2014 the stock of FDI in U.S. manufacturing surpassed $1 trillion, more than double the value of FDI in China’s manufacturing sector (and eight times the value in per capita terms).



If by “rapid deindustrialization” Trump means that manufactured goods account for a smaller share of U.S. output than in the past, he’s right about the statistic, but not the interpretation. Manufacturing’s share of the U.S. economy peaked in 1953 at 28.1 percent, whereas today manufacturing accounts for only 12.1 percent of GDP. But in 1953 U.S. manufacturing value added amounted to $110 billion, as compared to a record $2.1 trillion in 2015 — more than six times the value in real terms.



Bernie Sanders is wary of capitalism and in favor of equality of outcome. He perpetuates another common myth: Trade only benefits multinational corporations and the rich. But nothing could be further from the truth. Just like during the Gilded Age, the tariff remains the mother of the trust. And, like then, free trade should be the progressive position.



Protectionism benefits producers over consumers; it favors big business over small business because the cost of protectionism is relatively small to a bigger company; and, it hurts lower‐​income more than higherincome Americans because the former spend a higher proportion of their resources on imported goods.



The United States has relatively low tariffs on average — less than 2 percent. But tariffs on clothing (18 percent), footwear (14 percent), and food products (10 percent) are especially high. Meanwhile, U.S. antidumping restrictions on steel, lumber, cement, appliances, flooring, nails, and paint elevate the material costs of home building. Imports of life’s basic necessities — food, clothing, and shelter — are subject to some of the highest taxes. Why isn’t that too regressive for a progressive like Sanders?



 **WHAT DOES THE RHETORIC PORTEND?**  
Demagoguing trade has become an election year pastime. But trade issues tend to be of marginal concern to voters in the general election, and history suggests that cooler heads will prevail. Despite the abundance of antitrade rhetoric on the campaign trail, it is difficult to imagine an actual president of the United States supporting policies commensurate with the bluster. Every president since FDR, regardless of political party, has embraced or promoted trade liberalization.



While candidates might rail against unfair trade practices and unlevel playing fields on the stump, they change their tunes after taking the oath. Presidents prioritize broader, national interests over regional and parochial issues, and tend to see merit in projecting global economic leadership. They also view trade policy through the prism of foreign policy, and recognize the contributions that trade makes to economic growth and international stability.



Even if there were a President Trump or President Sanders, rest assured that the Congress still has authority over the nuts and bolts of trade policy. The scope for presidential mischief, such as unilaterally raising tariffs, or suspending or amending the terms of trade agreements, is limited. But it would be more reassuring still if the intellectual consensus for free trade were also the popular consensus.



What matters most is that Americans have realized progressively greater freedom to transact with people in other countries over the years. Many barriers still remain. But when the evidence of the economic benefits of liberalization is weighed against the myths and political aspersions, trade is exonerated on all counts.
"
"The feeding habits of an unusual 200-million-year-old fish have been tested in a ground-breaking study published in Palaeontology. This research is particularly notable as it wasn’t carried out by a leading professor but by Fiann Smithwick, a University of Bristol undergraduate. The study illustrates a change in the science of palaeobiology: what was once a somewhat speculative field has now become analytical, and different models can be tested against our expectations. The fish, Dapedium, is known from fossils found in the Lower Lias rocks near Lyme Regis in Dorset, on England’s south coast. It lived side by side with the great sea reptiles of the Jurassic such as the dolphin-shaped ichthyosaurs, long-necked plesiosaurs, and even some marine crocodilians.  Dapedium was one of a number of these animals first discovered by the pioneering 19th century fossil collector Mary Anning, which fascinated early palaeontologists such as Louis Agass and Henry De la Beche, who created some of the first paleoart based on Anning’s fossils. Dapedium was a deep-bodied fish, shaped like a dinner plate in side view, which could grow to more than half a metre in length. It probably escaped being caught by the larger predators pictured above because it was so thin-bodied it might be hard to see head-on. It had a tiny mouth with jutting front teeth and masses of pebble-shaped teeth further back.  But what were those teeth actually used for? To reconstruct the feeding behaviour of this ancient fish, Smithwick applied a new lever-based mechanical model developed by Mark Westneat at the University of Chicago as a means of quantifying jaw motions and forces in modern teleost fishes. Teleosts have more complex jaws than tetrapods such as ourselves – there is not a single jaw joint, but four, so when it is feeding a teleost can project its mouth forwards in the classic “pout”.  Modelling the exact lengths of the multiple cranks of the jaws, their angles to each other, and likely muscle forces, means scientists have a good idea how all modern fishes use their jaws. Depending on their diet, the jaws operate in different ways to snatch soft prey, crush shells, snip corals, or chew seaweed. Smithwick applied this model to 89 specimens of Dapedium in the Natural History Museum, Bristol City Museum, and the Philpot Museum in Lyme Regis, and measured the positions and lengths of the jaw bones.  He figured out the positions and orientations of jaw muscles and varied these to include all possible models.  His calculations showed Dapedium was a shell crusher. Its jaws moved slowly, but strongly, so it could work on the hard shells of its prey. Other fishes have fast-moving, but weaker jaws, adapted for feeding on speedier fish. In comparison with modern fishes, Dapedium matches closely the modern sea breams.  These fishes are also flat-sided and deep-bodied, and they crush shells in their small mouths, armed with blunt-topped teeth.  The new research takes what we know about the mechanics of modern fish such as the sea bream and predicts the jaw mechanics of fossil fishes within this framework. Speculation is not necessary. Smithwick was funded by a Summer Research Bursary from the Palaeontological Association, and he devised the project himself, learned the numerical techniques, and wrote it up himself.  It’s rare for an undergraduate to be able to do all this and pass the scrutiny of one of the world’s leading scientific journals."
"The EU has given its formal backing to 32 major gas infrastructure projects in a move critics say will lock Europe into burning fossil fuels for generations. MEPs voted to support the European commission’s proposal by 443 votes to 169 on Wednesday, with 36 abstentions, provoking environmental groups to lament Brussels’ “hypocrisy” over the climate emergency.  The total cost of the infrastructure is estimated to be €29bn. Under the EU’s funding programme up to 50% of the costs of each project could be covered by European taxpayers. The consulting firm Artelys has claimed in a report that the majority of the projects, stretching from Ireland to Croatia, are “unnecessary”. Environmental groups said gas firms were enjoying the fruits of heavy lobbying and the sector’s advantageous position within the commission’s decision-making institutions. An industry-led advisory body provides the European commission with forecasts on Europe’s future energy needs. Colin Roche, climate justice coordinator for Friends of the Earth Europe, said: “This climate hypocrisy has to end. Following unprecedented disasters like Australia’s wildfires, history will look unkindly on those who today backed building more fossil fuel pipelines and terminals. “A European green deal is not possible with more fossil fuels, and Europe needs to go fossil-free fast.” The European energy commissioner, Kadri Simson, from Estonia, had asked the parliament to back the proposal on the grounds that three-quarters of the 151 projects were electricity- rather than gas-based.  The parliament had the option of accepting the entire list, the fourth proposed by the commission under its “connecting Europe” programme, or voting it down in its entirety. Simson said: “An objection to the 4th PCI [Projects of Common Interest] list would mean that the 3rd PCI list remains in force – a list with 40% more gas projects than the new list. “As a consequence, key electricity interconnectors and energy transition projects such as the North Sea Wind Power Hub, new smart grid projects and new CO2 network projects would not be eligible for funding under the Connecting Europe Facility.” Simson said she would ensure the next project list “reflects the European green deal priorities”, in reference to the EU’s goal of carbon emission neutrality by 2050. She insisted that being on the new list did not guarantee funding. Ciarán Cuffe, an Irish MEP from the Green party, who voted against the projects, said: “Given the severity of the climate emergency and the little time we have left to avert climate catastrophe, it is wrong to spend more public money on fossil fuel investments. “We are past the time to channel public money into expensive and unnecessary fossil fuel projects when Europe should be investing in energy efficiency and renewables.”"
