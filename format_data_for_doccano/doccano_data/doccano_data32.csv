"The supermarket chain Iceland has been denied clearance to screen its Christmas advert on British television. Consisting mainly of Greenpeace’s short “Rang-tan” animation, the ad highlights Iceland’s commitment to eliminate palm oil from its own-brand products. According to the advertising clearing body, Clearcast, it was disallowed not because of its content, but because of its connection with Greenpeace, a “body whose objects are wholly or mainly of a political nature”. Iceland reacted swiftly, tweeting that its ad had been “banned” from television because it was “seen to be in support of a political issue”. The tweet was picked up by mainstream media such as the Guardian, which ran the headline: “Iceland’s Christmas advert banned for being too political.” Furious responses followed, with nearly 100,000 people sharing Iceland’s original tweet and over 650,000 petitioning Clearcast to reverse its decision. Most of these responses revolve around the (inaccurate but powerful) claim that Iceland’s ad was “banned” for being “political”. How, ask critics, could highlighting the destruction of the rainforest be political? How could saving orangutans be anything but worthwhile? As one tweet put it, “since when is outrage about losing such beautiful animals political?” Such responses portray environmental and conservation causes as above politics: as built on unquestionable, universal truths. As such, they are too important to be used for petty point-scoring. In this view, Iceland’s ad reveals a devastating apolitical reality that the world needs to see and respond to.  But show the same footage to rural communities on the islands of Borneo and Sumatra, where most palm oil is produced, and we may well get a different response. Many would see the ad’s message as entirely political, for several reasons. The forests that the ad exhorts viewers to save are also these people’s homes: places filled with specific histories, social relations, assets and other living beings. But the relationships that forest dwellers have to these places are not always understood or recognised by the state or conservation bodies. Wildlife protection laws and the expansion of protected areas – often driven by conservation initiatives – have complicated the situation. These have turned access to forests and their resources into highly political issues. Many rural villagers across Indonesia and Malaysia also rely on small-scale cultivation of oil palm (the tree which produces palm oil) for their livelihoods. Some smallholders work for or in partnership with oil palm companies, and others operate independently. While participation in the industry has generated its own problems, it has also generated income and infrastructure in rural areas. Such smallholders will be rightly concerned about the damaging effects of attacks on palm oil on their futures, and their access to necessities like food and medicine. The Iceland ad is also a classic example of how life is unequally valued across the global political terrain. In certain parts of Borneo and Sumatra, where my team and I are currently conducting research, many forest residents see orangutans as dangerous and not particularly special creatures that can damage their crops and livelihoods. Yet they are acutely aware that many well-meaning foreigners would privilege the well-being of orangutans over their own. Why, they ask, do governments, NGOs and tourists put so much time and money into saving this one animal when people like us are struggling to get by? 


      Read more:
      Palm oil boycott could actually increase deforestation – sustainable products are the solution


 Such concerns reveal a mismatch between Iceland’s conservation message and the experiences of rural people in Borneo and Sumatra. In Greenpeace’s film, humans are either intruders (represented by bulldozers) in the pristine rainforest home of the orangutan, or the “good guys” (represented by the girl) who are going to save them. This vision is built on a historically Western understanding of the world that treats “humans” and “nature” as fundamentally separate. But what it blots out are the people who live in and around the same forests, for whom such a separation is much harder – and by no means apolitical. The idea that environmental and conservation causes are above politics thus makes sense only from a particular Western perspective – one built around an image of the forest-as-wilderness that is not universally shared. The depiction of such causes as apolitical has facilitated their spread across the world, while shielding them from scrutiny and critique. Yet scrutiny and critique can reveal significant problems and oversimplifications in Iceland’s ad. The “palm oil kills orangutans” narrative, for example, sidesteps the fact that deforestation is only one of several factors driving orangutan extinction. Other key drivers include hunting and poaching, though these won’t be solved by oil palm activism. And as various analysts have pointed out, simply boycotting palm oil could ultimately backfire. A collapse in global demand would disproportionately affect smallholders, generating further poverty and resentment. It could also encourage the cultivation of other ecologically-damaging crops such as soy or rapeseed, which would displace rather than reduce forest conversion and biodiversity loss – an unfortunate geopolitical outcome. None of this mitigates the need to address the problems of environmental destruction and extinction. And it’s no bad thing that Iceland’s ad has helped raise awareness of these issues. But conservation isn’t a black-and-white morality tale, and depicting the advert’s message as apolitical is both misleading and counterproductive. For the sake of both orangutans and the people who share their forests, we need fewer emotive simplifications and more acknowledgement of the complex political realities at stake."
"Scott Morrison has struck a $2bn deal with the New South Wales government to increase gas supply and reduce greenhouse gas emissions from the electricity sector. The deal includes at least $450m of federal grants and $510m more of federal grants or loans for “NSW-based emissions reduction initiatives”, to be matched by $1.01bn in direct funding from Gladys Berejiklian’s government.  The deal is the first of a series of bilateral energy agreements between the federal government and its state and territory counterparts. Morrison told reporters in Sydney the $2bn could be spent on “clean technology” including hydrogen research, energy efficiency measures, and “coal innovation to commercialise and employ technologies to reduce emissions from extraction, preparation and the use of coal”. Berejiklian said the plan should reassure citizens “we have a clear plan … [to] reduce our emissions, which we know many people feel strongly about”. Under the plan, the federal and NSW governments will jointly underwrite the delivery of HumeLink and the Queensland-NSW interconnectors to strengthen grid reliability. The NSW government has committed to facilitating investment opportunities to inject an additional 70 petajoules of gas a year into the east coast market and to remove barriers to coal supply to the Mount Piper power station, which is facing an acute shortage. Berejiklian told reporters the Narrabri gas project – to drill 900 coal seam gas wells, including within the Pilliga state forest – “may very well be” the source of extra gas and “will meet” the requirement, although she noted the project is still subject to final approval. She said NSW had three options, including Narrabri, and import terminals at Newcastle and Port Kembla to import more gas. To supply Mount Piper, Berejiklian suggested that “other [coal] power stations will be coming to the end of life in the next little while, so then there are opportunities for us to transfer those [supply] arrangements to Mount Piper”. The federal government’s $1bn electricity generation underwriting scheme will be used to support as-yet unspecified new generation projects in NSW. The shortlist of 12 projects for that scheme includes five gas projects, two of which have  been approved, six pumped hydro schemes, and the coal baron Trevor St Baker’s proposal for a coal upgrade at Vales Point at Lake Macquarie. The deal also promises financial support for the establishment of a pilot renewable energy zone in the state’s central west to help large-scale renewable generators. “I want households and businesses paying less for their electricity and I want to continue to get emissions down – this deal does both,” Morrison said. “There is no credible plan to lower emissions and keep electricity price down that does not involve the greater use of gas as an important transition fuel.” Berejiklian said the state “already has a pipeline of around $26bn of large-scale renewable and non-renewable energy projects and the NSW government has introduced a range of rebates to help keep prices down as well as a five-year $1.4bn climate change fund.” “Our agreement with the commonwealth today will ensure we continue to strengthen and diversify our energy sector here in New South Wales – securing traditional energy sources whilst growing renewable energy investment across the state,” she said. Relations between the federal and NSW governments have been strained after the NSW environment minister, Matt Kean, linked the bushfires to Australia’s poor record on combating climate change, then suggested that Liberals in federal cabinet were pushing for more action. This earned a rebuke from the prime minister, who said Kean “doesn’t know what he’s talking about” with respect to federal cabinet. Morrison insisted that Australia was “carrying its load” in the global emissions reduction fight. He has refused to increase Australia’s targets but has opened the door to dropping the use of Kyoto carryover credits to achieve 2030 targets if it is possible to do so without increasing electricity prices."
"




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea69672d7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"All over the world, countless conservation projects are taking place, attempting to achieve aims from reducing habitat loss, to restoring populations of threatened species. However there is growing awareness that conservationists have not always done a good enough job at evaluating whether the things they do really work.  Efforts that fail to make things better for species and ecosystems waste the limited resources available for conservation, and result in missed opportunities to stem the loss of biodiversity. Given that monitored populations of wildlife species have declined by 60% in the last 50 years, and large scale loss of forest continues, this is bad news. So, research to show whether conservation efforts work really matters. And those doing conservation need easy access to the results of this vital evidence. In many fields, when researchers want to know whether something works they conduct an experiment. For example, patients are often randomly assigned to receive a new drug (or not) and the results are compared to determine if the new treatment has the potential to help people. Despite calls for more use of experiments in conservation, they remain extremely rare. One common approach to conservation is encouraging owners to manage their land in a way which provides benefits for the environment. This has been done in the UK for decades. For example, farmers are paid to maintain hedgerows and leave stubble on fields to help farmland birds. These kinds of payments for ecosystem services schemes are increasingly used in the tropics as well.  In 2017, an experiment in Uganda revealed that paying farmers not to chop down trees was a cost effective way to slow deforestation. Now we have published the results of only the second experiment at such a scale. Our study evaluates whether providing incentives to farmers to protect forest and keep cattle out of streams improves water quality.  The research focuses on the efforts of the Bolivian NGO Natura, which has been working with communities in the Andes to help protect the area’s incredible forests. These are home to spectacled bears and other wonderful wildlife, and are seen locally as important for supplying clean water. In Natura’s Watershared programme, upstream landowners were offered incentives to shift their livelihood activities away from clearing forest or letting cattle graze untended in the forest. Natura wanted to know if their innovative approach to conservation was working, so they took the unusual step of setting up an experiment to find out.  In 2010, 129 communities were randomly placed in a control group, or given the chance to enrol their land in Watershared agreements. Households in the latter “treatment communities” could then choose to enrol as much of their land as they wished in the programme. Analysing the results of this experiment, we found that while keeping cattle out of rivers is (perhaps unsurprisingly) good for water quality at the location where it happens, the treatment communities did not have cleaner water in their taps. Further investigation revealed that this was at least in part because of the low level of uptake of the programme, and that the land most likely to be important for improving water quality was often not enrolled. Natura is already implementing the results of this research to improve the design of Watershared. They are working with communities to ensure that protection is targeted to areas most likely to benefit water quality. And our experience with running such a large-scale experiment holds useful lessons for others interested in increasing knowledge about what works in conservation. Away from conservation, there has been an explosion in the use of randomised experiments to evaluate the impact of other large scale interventions – in development and education, for example. However, there has been backlash from opponents who have pointed out, among other things, that these kinds of investigations will not always provide valid answers to the most important questions because these experiments can only normally answer the question “does it work?”, rather than “why does it work?”, and so can’t really answer the other key question, “will it work in other situations?”. This debate has got quite heated, and even acrimonious, at times. Running an experiment to evaluate the impact of a large-scale conservation intervention is certainly very challenging. It is often not possible to randomise which areas receive a new conservation project (can you imagine a government randomly allocating where it puts national parks?). There are also issues with achieving adequate replication, and there can be ethical concerns which prevent experimentation. However, given the importance of knowing what works in conservation, more high quality evaluations (which won’t always be experiments) are certainly needed. Only by learning from current practice can the future effectiveness of conservation be improved."
"Over the past two decades automated wildlife cameras – known as camera traps – have proven invaluable in ecological research and conservation management. Their sensitive motion detectors have enabled scientific surveys of rare or shy animals in dense forest and as a consequence have seen broader use around the world.  But camera traps frequently take pictures of people as well as wildlife. This has important implications for privacy and human rights and may ultimately undermine conservation goals. We conducted a survey of researchers who had deployed camera traps in ecological or conservation projects. More than 90% of the 235 respondents said that their cameras had taken images of people as well as wildlife. Fewer than 9% of researchers who had captured images of people had initially set out to do so. But most said that once they had the pictures they made use of them. For example, almost half of respondents who had pictures of apparently illegal activities (such as poaching) subsequently used them to inform conservation management or law enforcement, sometimes by sharing them with third parties (most notably the police and park management staff). Initially, for that reason, the ability of camera traps to monitor human as well as wildlife activity in areas of conservation importance may look like a double win for conservation. But the fact that these cameras often take pictures of people can be highly problematic for two main reasons. Firstly, many respondents said that they had captured images that either they considered to be private (for example of people going to the toilet), or which showed a person trying to avoid the camera (for example, images of people covering their faces). In some countries, distributing images of people without their consent can result in substantial penalties. Even where this is not against the law, steps should be taken to ensure that camera traps do not infringe reasonable rights to privacy.  Second, even if images of people are not used or shared, camera traps can still generate fear and anger – and this can lead to local opposition to camera trapping. With the long-term success of most conservation projects depending on the support of local people, it is vital that their issues with camera traps are taken seriously.  Three-quarters of the researchers we surveyed reported local objections to cameras, either in the form of complaints or direct interference such as damage or theft. Sometimes people went to great lengths to interfere with the cameras. For example, one respondent said that they once found a patch of burnt ground, only one square metre in area, exactly where their camera used to be. When we asked what researchers thought had caused these cases of interference and objections, the most common answer was “fear or concern about what might happen with the images”. This fear can take extreme forms. For instance, one of our respondents in South America said that they had found a woman next to a camera trap who thought it was being used to take pictures of her children with a view to kidnapping them.  Clearly, conservation projects whose cameras are damaged or stolen are going to face the cost of replacing equipment and often also lose data. For these reasons alone, it makes sense to avoid angering or scaring local people. More broadly, the antagonistic relations with local people caused by camera traps can give conservation projects a bad name and even promote damaging activities: several researchers reported that those who objected to the cameras retaliated by killing wildlife. Despite the frequency of these issues, they are almost never discussed in conservation or ecological scientific studies. And although most of our respondents recognised the potential problems with the use of pictures of people, the dilemma of how to best to handle them is not being publicly debated. Luckily, researchers themselves are thinking about this problem. Protocols established by our respondents to manage pictures of people caught on camera included blurring images, or not publicly sharing them. This blurring could even be done automatically, using machine algorithms, so that images containing people are blurred before they are seen by human eyes. Some individual researchers have gone further and have substantially involved local communities in their projects – reassuring them that the cameras were not there for law enforcement purposes, involving them in the process of deploying the cameras and sharing the images with them. But, despite these independent efforts, to date there are no standard guidelines agreed by conservationists.  The implications of camera trap technology for people’s privacy and well-being need to be more widely and openly discussed, and good practice shared. Conservation projects need to make sure they have proper protocols in place to minimise social impacts and stop useful wildlife research tools from damaging both the short and long-term success of wildlife conservation projects."
"
Today I visited my friend Jim Goodridge, former California State Climatologist and the man with a garage full of data going back to before the Gold Rush.
He’s been quietly toiling away in his retirement on his computer for the last 15 years or so making all sort of data comparisons. He gave me two CD ROMS full of data that I’m just now wading through. One plot which he shared with me today is a 104 year plot map of California showing station trends after painstakingly hand entering data into an Excel spreadsheet and plotting slopes of the data to produce trend dots.
He used every good continuous piece of data he could get his hands on, no adjusted data like the climate miodelers use, only raw from Coopertive Observing Stations, CDF stations, Weather Service Offices’s and Municipal stations.
The results are quite interesting. Here it is:

Squint hard and you can see a pattern emerge.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea610a013',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Take a 12 hour ferry north from mainland Scotland, and you’ll reach the Shetland Isles – the northernmost place in the UK. The only part of Britain to be considered “subarctic”, the archipelago of about 100 islands is found at the same latitude as the southern tip of Greenland. It’s so far north that, on most maps of the UK or Scotland, Shetland is cut out and placed in an inserted box somewhere off the coast of Aberdeenshire or the Highlands. Yet this aspect of cartographic design has long been controversial among the 23,000 people who live on the islands. This was a point made recently by the their representative in the Scottish parliament, Tavish Scott MSP, who has passed an amendment to the Islands (Scotland) Bill which now prevents public bodies in Scotland from including the Shetland Islands in an “inset map” (in a box).  The Ordnance Survey, Britain’s national mapping agency, was against the move as it would imply “publishing maps that are mostly sea”, while we must now wait and see how this actually impacts maps produced by Scotland’s various public bodies, which do have a get-out clause if they use an inset, but explain their reasons. Nonetheless, the idea behind the law change is certainly laudable. With Shetland in a box off the coast, it is easy to forget how much of a logistical and financial challenge it is to travel from the island to the mainland. It is a full 125 miles from the northern coast of the mainland to Shetland – about the same distance going from London to Nottingham. Placing the islands in their true geographical location should remind people of this reality. However, Scott has perhaps forgotten why the islands were put in a box in the first place, and this reason hasn’t changed in the hundreds of years that people have been making maps of Scotland. Due to their geographical isolation, including them in a box allows the rest of Scotland to be shown in much more detail. If the Shetland Isles were mapped in their true location, we would end up with a map that is largely sea, with the rest of the Scottish mainland around 40% smaller. When creating any map, whoever is making it has to decide what to include and what not to include. If you had a map of the town or city where you live, would you include the internal layout of your home? Obviously not – marking out the distance you need to travel from your sofa to your kitchen to make a cup of tea would be both impractical and irrelevant for almost all users of the map. So what to include in a map depends on what the map is for. The London Underground map is a great example of one that only includes what it needs to: it lists the stations, the tube lines and is very clear on which lines connect where. (It also includes the River Thames, which is arguably not vital, but helps people orientate themselves and so was reinstated after it was removed in 2009). As a map to work out where to go in Central London on foot it is no help at all, but that’s not the point.  Hexagon-based maps used in parliamentary elections are another iconic example. Most viewers are not interested in the geographic area of each constituency, but do want to easily assess the total number of seats and which party has a majority. Decisions made at the design stage can alter how a map is interpreted, sometimes quite significantly. We can automate some of the process using Geographic Information Science (GIS) computer programs, but we have to use these tools carefully, with the end product in mind. Insets are just one element of a map that can have an impact on the message it conveys. The Worldmapper website uses cartograms to adjust maps to show variables other than space geographically, and Vox has a great video explaining different projection systems used to display the globe. And the Shetland Isles? Some maps of Scotland should include the islands in their true geographic location in order to highlight their remoteness and logistical issues, as Scott says. However, not all maps need to: sometimes this is not relevant to the map, and it is more important to give more prominence to the central belt of Edinburgh and Glasgow where many more people live.  It all depends on what the aim of the map is. The choices made by the map maker have a big impact on the output of the process and how the map is viewed. It is impossible to represent everything on a map, and sometimes what we decide to leave out is much more important than what we decide to include."
"Here we go again. The “sceptical environmentalist”, Bjorn Lomborg, has returned to warn against the excesses of an impending green dictatorship. The latest threat: taking away our burgers! Yes, you’ve heard correctly. According to Lomborg it could even go as far as the “UN dictating what people eat”. A well-known provocateur who now runs a think-tank in his native Denmark, Lomborg first made his name in the early 2000s with a series of outspoken but attention-grabbing attacks on mainstream environmentalism. He argued that the climate is changing but less dramatically than most suggest, and that there is no urgent need for action. He also claimed that forests aren’t disappearing and that species extinction has been wildly exaggerated.  Lomborg walks the line between sensible liberal thinking and outright denialism by cherry-picking or misrepresenting statistics. Though widely criticised by most scientists, Lomborg retains a large following today.  This is why his typically contrarian take on climate change and food attracts so much attention, and why it is worth responding to. Lomborg, a vegetarian for animal welfare reasons, explains that: “Almost all articles on this topic suggest going vegetarian could achieve emission cuts of 50% or more.” But apparently none of them have taken the time to “dig deeper”. As researchers who work in environmental impact analysis, we are acutely aware of the limitations of “food footprinting” studies and the danger of taking figures at face value. So, let’s dig deeper into his claims. Take the “systematic peer review” he cites which found that going vegetarian cuts personal emissions by around 5% rather than 50%. He’s correct that the cuts aren’t close to halving a person’s overall emissions, but there is good reason to believe it is double what Lomborg claims. Only two studies in the review he uses look at the major effect of meat consumption on emissions from deforestation, even though millions of hectares of forest are cleared each year to satisfy the world’s appetite for beef. As forests act as a carbon sink, while beef farms emit lots of greenhouse gases, this has a huge impact on net emissions. Meat consumption is incompatible with limiting deforestation and encroachment into natural land. Hence, we must take into account “deforestation emissions” when tallying the environmental burden of eating meat. Taking the more realistic figure from these studies we arrive at a 10% cut in personal emissions from going vegetarian. To put this into context: a shift to vegetarianism in the UK would be the equivalent of taking 8m (or one in four) cars off the country’s roads. The impacts of veganism would be greater still. In short, the impact of individual actions really does add up. What about Lomborg’s second claim that vegetarians take the money they save from “eating carrots instead of steak” and spend it on other things which have their own environmental impact, offsetting part of the benefits of giving up meat? We dug deeper and found that the paper he cites relies on data from 2006 and also does not factor in emissions from changing land use, linked to deforestation. The paper is a microeconomic analysis of what consumers in Sweden, specifically, might spend their extra cash on if they went vegetarian. Its author warns that her work must be “interpreted within a relatively narrow topical and temporal scope”, and that unrealistic market assumptions concerning fixed supply, demand, and pricing could lead to completely different conclusions when relaxed. Nevertheless, Lomborg does extrapolate the paper’s findings, against its author’s own suggestion, to trivialise the impact of vegetarianism on emissions across the industrialised world. Clearly, this is not just about the environment. It’s about our ability to choose. Lomborg prioritises the right to eat meat over our collective responsibility not to. Many of the world’s poorest are involuntary vegetarians, he argues. Our duty, he implies, is to support their “right to meat”. However, poorer countries stand to benefit from widescale adoption of a plant-based diet. Mortality linked to strokes, heart disease, diabetes, and cancer could fall by 5m to 6m avoided deaths and trillions of dollars could be saved in healthcare costs and by preventing productivity losses.  Moreover, producing meat is terribly inefficient as animals consume far more food than they yield. If we grew crops for human consumption, instead of animal feed, we could increase available food calories by as much as 70%, which could feed an additional 4 billion people, ending global hunger and reducing emissions, one carrot at a time. Lomborg summarises his argument: “Climate change is both trivialised and hampered by unrealistic senses of magnitude, and by silly suggestions that your or my actions can transform the planet.” To suggest you and I can do nothing to help prevent climate change is surely defeatist. This climate defeatism is the new climate denial. Lomborg offers techno-fixes where effective measures already exist. Although he knows consumers will fry the planet before they do lab-grown burgers, prescribing artificial meat helps kick the can further down the road. Although we need systemic change, the climate is also in our hands. Perhaps the only meaningful contribution Lomborg makes to avoiding climate breakdown is choosing carrots over steak."
"
Share this...FacebookTwitterDonations now welcome!
Before we jump into the data, readers may have noticed that NoTricksZone is now accepting donations – after close to 10 years of not doing so. Hosting services and the hourly rates specialists charge for troubleshooting are painfully expensive, and so any support, no matter how small, is very much appreciated.


		jQuery(document).ready(function(){
			jQuery('#dd_a992b20773ff8dfb5303892e42d2859f').on('change', function() {
			  jQuery('#amount_a992b20773ff8dfb5303892e42d2859f').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
I’ve gotten some donations since I’ve installed the donation button, and so it’s very encouraging. Thanks to those who have done so.
And no, I’m not nor have I ever been funded by Big Oil, The Heartland Institute, Big Coal, the Koch Brothers or anything of the sort. Everything has been out of my own pocket. Thanks, Pierre
Falling Canada mean February temperatures
By Kirye
and Pierre Gosselin
Today we look at the data from the Japan Meteorological Agency (JMA) mean temperature for Canada. Examined are the 9 stations that have almost complete data going back to 1983.
Nine of the 9 stations show February mean temperatures have had a cooling trend since 1996: 



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Data source: JMA 
Annual Canada temperatures steady since 1994
Looking at the JMA data for mean annual temperatures for Canada, we see that no significant warming has taken place at the 9 stations since 1994!

Data: JMA.
Yes, the climate is changing, but nothing unusual is happening. The changes are well within the natural range of variability and so cannot be wholly attributed to man’s activities.
In fact, using the data and results that are available, one can easily argue that the recent warming is related to ocean and solar cycles. There’s nothing to be alarmed about.
Worry about Corona and political threats, and not climate.
Share this...FacebookTwitter "
"

 _Here we introduce a new feature from the Center for the Study of Science, “On the Bright Side.”_ _OBS will highlight the beneficial impacts of human activities on the state of our world, including improvements to human health and welfare, as well as the natural environment. Our emphasis will typically focus on the oft-neglected positive externalities of carbon dioxide emissions and associated climate change. Far too often, the media, environmental organizations, governmental panels and policymakers concentrate their efforts on the putative negative impacts of potential CO 2-induced global warming. We hope to counter that pessimism with a heavy dose of positive reporting on the considerable good humans are doing for themselves and for the planet._   
  
_**—**_   
  
According to Piao _et al_. (2015), the reliable detection and attribution of changes in vegetation growth are essential prerequisites for “the development of successful strategies for the sustainable management of ecosystems.” And indeed they are, especially in today’s world in which so many scientists and policy makers are concerned with what to do (or not do) about the potential impacts of CO2-induced climate change. However, detecting vegetative change, let alone determining its cause, can be an extraordinarily difficult task to accomplish. Nevertheless, that is exactly what Piao _et al_. set out to do in their recent study.   
  
More specifically, the team of sixteen Chinese, Australian and American researchers set out to investigate trends in vegetational change across China over the past three decades (1982-2009), quantifying the contributions from different factors including (1) climate change, (2) rising atmospheric CO2 concentrations, (3) nitrogen deposition and (4) afforestation. To do so, they used three different satellite-derived Leaf Area Index (LAI) datasets (GLOBMAP, GLASS, and GIMMIS) to detect spatial and temporal changes in vegetation during the growing season (GS, defined as April to October), and five process-based ecosystem models (CABLE, CLM4, ORCHIDEE, LPJ and VEGAS) to determine the _attribution_.   




With respect to _detection_ , this work revealed that most regions of China experienced a greening trend indicative of enhanced growth across the time period studied (see Figure 1). Overall, 56 percent of the area studied experienced a significant increase in greening (95% level) when using the GLOBMAP dataset, compared with 54 and 31 percent using the GLASS and GIMMIS datasets. Those regions with the largest greening trends include southwest China and part of the North China Plain.   
  
_  


![Figure 1. Spatial distribution of the trend in LAIGS over the period 1982–2009 as calculated by the GIMMS dataset \(a\), GLOBMAP dataset \(b\) and the GLASS dataset \(c\). The frequency distribution of the significance level \(P value\) of the trends calculated for the three LAIGS datasets is shown in panel d.](/sites/cato.org/files/styles/pubs/public/wp-content/uploads/chinaidso.jpg?itok=fhC2To8Y)

_



  




**_Figure 1._** _Spatial distribution of the trend in LAI GS over the period 1982–2009 as calculated by the GIMMS dataset (a), GLOBMAP dataset (b) and the GLASS dataset (c). The frequency distribution of the significance level (P value) of the trends calculated for the three LAIGS datasets is shown in panel d._



With respect to _attribution_ , Piao _et al_. report that “the combined effect of CO2 fertilization and climate change with the effect of nitrogen deposition, leads to the conclusion that these three factors are responsible for almost all of the average increasing trend of LAIGS observed from the satellites” (see Figure 2). They also report that “at the country scale, the average trend of LAIGS attributed to rising CO2 concentration is estimated to be ... about 85% of the average LAIGS trend estimated by satellite datasets,” while noting secondarily that the enhanced nitrogen deposition driven by _fossil fuel combustion_ and _agricultural fertilization_ is likely the source of the remaining portion of China’s enhanced vegetation growth, citing the findings of Reay _et al_. (2008), Thomas _et al_. (2009), Fleischer _et al_. (2013) and Yu _et al_. (2014).   






**_Figure 2._** _Trend in China’s LAI GS over the period 1982–2009 at the country scale for the three satellite remote sensing datasets and five process models described in the text above. Significance levels of 95 and 99 percent are denoted with one and two asterisks, respectively. See the authors' original text (Piao et al., 2015) for additional explanation of this figure._



In considering the researchers' several findings, it is clear that the fossil fuel combustion that has resulted in the rise in atmospheric CO2 and enhanced nitrogen deposition over the past three decades has provided a great benefit to Chinese vegetation. As illustrated in Figure 2, led primarily by the increase in CO2, that benefit has been _more than sufficient_ to compensate for the negative effects of climate change that also occurred over that time period. Thus, it would seem far more prudent to _celebrate_ CO2 instead of _demonizing_ it, like so many people incorrectly do these days; for atmospheric CO2 is truly the _elixir of life!_   
  
  
  
**References**   
  
Fleischer, K., Rebel, K.T., Molen, M.K., Erisman, J.W., Wassen, M.J., van Loon, E.E., Montagnani, L., Gough, C.M., Herbst, M., Janssens, I.A., Gianelle, D. and Dolman, A.J. 2013. The contribution of nitrogen deposition to the photosynthetic capacity of forests. _Global Biogeochemical Cycle_ s **27** : 187-199.   
  
Piao, S, Yin, G., Tan, J., Cheng, L., Huang, M., Li, Y., Liu, R., Mao, J., Myneni, R.B., Peng, S., Poulter, B., Shi, X., Xiao, Z., Zeng, N., Zeng, Z. and Wang, Y. 2015. Detection and attribution of vegetation greening trend in China over the last 30 years. _Global Change Biology_ **21** : 1601-1609.   
  
Reay, D.S., Dentener, F., Smith, P., Grace, J. and Feely, R.A. 2008. Global nitrogen deposition and carbon sinks. _Nature Geoscience_ **1** : 430-437.   
  
Thomas, R.Q., Canham, C.D., Weathers, K.C. and Goodale, C.L. 2009. Increased tree carbon storage in response to nitrogen deposition in the U.S. _Nature Geoscience_ **3** : 13-17.   
  
Yu, G., Chen, Z., Piao, S., Peng, C., Ciais, P., Wang, Q., Li, X., and Zhu, X. 2014. High carbon dioxide uptake by subtropical forest ecosystems in the East Asian monsoon region. _Proceedings of the National Academy of Sciences USA_ **111** : 4910-4915.


"
"The oil industry is at risk of a global market shock that could halve the value of fossil fuel investments if governments delay setting policies to tackle the climate crisis, according to new analysis. A report by Carbon Tracker, a financial thinktank, warned that a “handbrake turn” in climate policy could have a “forceful, abrupt, and disorderly” impact on the global oil industry by derailing fossil fuel demand.  The report warned that the longer governments wait to set new regulations to drive climate action the tougher they will need to be to avert the risk of runaway greenhouse gas emissions and dangerous global heating. The report modelled the impact of a swift government crackdown on fossil fuels from 2025. It predicted that the impact could cause global oil prices to collapse, wiping out billions of dollars worth of fossil fuel investments. Andrew Grant, a Carbon Tracker analyst and author of the report, said oil companies “risk being left with stranded assets” by assuming that governments will stop short of “forceful action to limit climate change”. The thinktank urged policymakers to act soon to limit new investment in fossil fuel projects which risk being stranded, and warned oil companies to anticipate a step change in climate action. Under existing forecasts oil demand is expected to grow by 0.6% a year over the next five years, but the report found that a crackdown on greenhouse gas emissions could cause demand to shrink by 2.6% a year between 2025 to 2040. “The loss of value is driven not by the oil industry throwing money away but simply by investing based on signals sent by the oil price,” the report said. Many oil companies may have been “lulled into a false sense of security by industry scenarios” which continue to forecast steadily rising demand, according to Carbon Tracker. The thinktank also warned investors against oil market scenarios published by the International Energy Agency which predict a steady decline in demand rather than a sudden collapse. Oil companies in the US, China and Russia are more exposed to a sudden oil market shock than their European peers due to their slow progress in adopting cleaner energy sources alongside fossil fuels. Within the world’s largest listed oil companies, the so-called “super-majors”, ExxonMobil, ConocoPhillips and Chevron are most exposed to an oil price collapse. Europe’s most vulnerable oil company is BP, according to the report, followed by Norway’s Equinor, Paris-headquartered Total, Italy’s Eni and Anglo-Dutch energy giant Shell. However, Saudi Arabia could prove to be relatively resilient to an oil price crash because its reserves are by far the cheapest to produce in the world."
"Coral reefs are an invaluable source of food, economic revenue, and protection for millions of people worldwide. The three-dimensional structures built by corals also provide nourishment and shelter for over a quarter of all marine organisms.  But coral populations are threatened by a multitude of local and global stressors. Rising ocean temperatures are disrupting the 210m-year-old symbiosis between corals and microscopic algae. When temperatures rise, the coral animal becomes stressed and expels its algal partners, in a process known as coral bleaching. These symbiotic algae are a critical food resource for corals, and without them corals lose their primary source of nutrition. Fortunately, corals are mixotrophs and not solely dependent on nutrition from their algal partners. Despite their sedentary appearance, corals are voracious predators capable of capturing a wide variety of prey using their tentacles and mucous nets. Knowing how much corals eat via predation is essential for understanding how they can persist in a warming ocean. Numerous laboratory studies have shown that if coral feed, they are more capable of surviving the stress associated with warming temperatures and decreasing pH levels. Feeding can also increase the reproductive capacity of corals, which is key to repopulating reefs that have suffered high levels of coral mortality. Yet, almost 90 years since one of the first published accounts of coral predation, we still do not know much about how coral feeding varies as a function of food availability in the wild. However, our new study sheds light on this longstanding question. We combined field sampling with global satellite measurements and published data to reveal that corals respond to how much food is on their reef. This indicates that corals living in more productive (food-rich) waters consume more food, which changes our understanding of how corals survive and may aid in predictions of coral recovery in the face of climate change.  Studying variation in the diets of corals over large areas is no easy task. To determine if corals will change their feeding behaviour as a function of food availability, we sailed to the remote Southern Line Islands of Kiribati. These islands are ideal for studying variations in coral diets because they lack local direct human impacts (fishing and pollution) and are situated across a natural gradient of food availability fuelled by equatorial upwelling. This process delivers colder, nutrient- and plankton-rich waters to the surface ocean along the equator in the central Pacific. We examined coral diets across five islands using stable isotope analysis. Stable isotopes are atoms of the same element (in this case carbon) that differ in mass due to the number of neutrons in their nucleus. This subtle mass difference allows scientists to determine what an organism is eating based on how similar the isotopic composition of the consumer (coral) is to its food (zooplankton). The isotopic data showed that the corals on the more food-rich islands were capturing and consuming more planktonic prey than corals on islands with lower food availability. These findings suggested that the abundance of food might be important for corals in other locations, which inspired our team to evaluate if coral feeding habits can be used to track global food availability. Satellites can reliably measure the amount of phytoplankton around tropical islands – a useful proxy for estimating food abundance for corals. So, using satellite data from 2004-2015, taken from 16 locations spanning the Pacific and Indian Oceans to the Red Sea and the Caribbean, we compared published isotopic values from corals at each location. What we found was a striking relationship between the chlorophyll content of the water and the feeding habits of corals. Essentially, corals in more productive regions consume more planktonic food. The seemingly simple observation that corals eat more where there is more food has important implications for our understanding of how coral reefs function. It underscores the importance of the physical environment around reefs and suggests that food availability may be an overlooked driver of coral recovery potential. The capacity for corals to feed before or during thermal stress can improve their capacity for survival. These findings lay the foundation to begin investigating the possibility that reefs in more naturally food-rich waters have a greater capacity to resist or recover from disturbance events such as thermally induced bleaching. Reefs do show variations in how they respond to thermal stress events – some reefs bleach less than others – but the exact mechanisms behind these differences remain largely unclear. The relationship between coral feeding and ocean chlorophyll established in this study offers a roadmap to locating potentially more resilient coral reefs around the world. Such knowledge does not replace the need to urgently reduce greenhouse gas emissions and protect coral reefs from the increasing frequency of ocean warming events, however. Instead it should be used to guide strategic management actions in the inevitable interim."
"At 89, Claudia Andujar still has her work cut out. For five decades she has photographed the Yanomami indigenous people, an Amer-Indian tribe who number 33,000 and live in 192,000 square kilometres of rainforest that straddle the borders of Brazil and Venezuela. Until the early 20th century they had lived almost entirely in isolation from the outside world, but since then disease, deforestation and climate change have taken their toll. The election of Jair Bolsonaro in Brazil has proved a further threat. Vehemently against legislation protecting indigenous lands, last week the far-right president commented: “Indians are undoubtedly changing … They are increasingly becoming human beings just like us.”  “I now put all my efforts into the activism, into addressing Brazil’s political situation,” Andujar says as she prepares a retrospective at the Fondation Cartier in Paris, which shows a selection from a personal collection that runs to several thousand photographs. “I no longer take photographs, but use my archive to show how I saw the Yanomami. This present government has no respect for them. They have no understanding of who they are as people.” The changing nature of Andujar’s work is chronicled in the show. Though the earliest images are straight documentary, the artist notes she was always more interested in striking up a human bond with her subjects than in approaching the work with professional detachment. “I decided very early that I would not photograph if I felt I did not have a connection with the person whose picture I was taking,” she says. “Developing an intimacy with the individual and community came first. The photography was always secondary to that.” Immersing herself in the culture of the Yanomami, it soon became clear that she would never represent their worldview through conventional composition. Instead, employing techniques, including double exposure, long exposures, the use of coloured filters or smearing of vaseline on the lens, Andujar started to produce a body of work a lot stranger and more faithful, she says, to the experience of the Yanomami people. A room of the Paris show is dedicated to the shamanic rituals that are a key aspect to Yanomami cosmology. During these ceremonies the tribe believe spirits – xapiri – descend on the forest leaving trails of brilliant white light in their wake. Andujar represented this by shaking her camera as she took each photograph of the convulsing, gyrating shamans. One image shows a woman with her face obscured by curling trails of smoky white light dancing across the picture’s surface. In another, a man, naked but for streaks of body paint, lies on the ground near a burning ball of light emitting from an unknown source. A funeral ceremony, in which the deceased is encased in a woven casket hung from a tree, is photographed through an otherworldly orange filter. A group of Yanomami are shown in layered multiples, using a double exposure, a way of reflecting the rhythm, movement and intense noise of the indigenous ceremonies.  Davi Kopenawa, a Yanomami activist and shaman, first met Andujar in 1977. “It was quite unusual for a white woman to come, a woman who wasn’t a missionary especially. Claudia took her time to get to know us; she slept in our shabono,” he says, referring to the ring-shaped wooden buildings that the Yanomami live in communally. “When the white people invaded our land, they took us by surprise and we weren’t prepared to deal with that first contact. Non-indigenous people wanted us to vanish, they wanted us to die ... It is still a dangerous battlefield out there for us, but to survive we need to confront it. These pictures are part of that. She was able to show them, show us, to the people of the city.” Born in Switzerland in 1931, Andujar was raised in Oradea, a border town that has switched between Romania and Hungary. Her parents separated when she was nine. In 1944, as the German army closed in on the town, her mother took her to Switzerland, leaving her father, Siegfried, behind. He, and his entire extended Jewish family, perished in Auschwitz and Dachau. This, Andujar says, lies behind the affinity she felt with the Yanomami. “I want to help the Yanomami to survive like my family did not. I think my work is dependent on the suffering of my childhood. My friends from school all died in Auschwitz. Everyone. Nobody, nobody survived.” Kopenawa agrees Andujar’s childhood holds the key to her relationship with the Yanomami. “When the people of the world waged war against each other, Claudia suffered a lot. But that gave her the experience that is required to take images of the Yanomami.” After the war, Andujar moved first to New York, before settling in São Paulo in 1955. “I was conscious that I was looking for something that was missing from my life,” she says. “I never forced the relationship with the Yanomami, but there was something within me that was searching for a connection or purpose that they provided.” The longest of her repeated trips living with the tribe was over a year. Did she feel at home with them? “Yes, yes I did.” The exhibition in Paris makes clear that Andujar’s journey from outsider to champion of the Yanomami was a long one. On one wall, showing her early work, is a fashion shoot from 1970 for Sententa magazine, in which a white model poses against the exoticised backdrop of an indigenous village. Elsewhere, the Yanomami are shown sympathetically, romanticised perhaps, with no hint to what some conservative anthropologists claim to be a violent culture inherent to the tribe. In 1968, the American anthropologist Napoleon Chagnon reported the tribe lived in a “a state of chronic warfare”. While disputes undoubtedly occur, Kopenawa rejects the generalisation, a claim backed by numerous studies since. Andujar says that while she feels close to the Yanomami, and never felt scared, there is a power dynamic between her and them. “There are various beliefs in Yanomami about photography, and the capturing of what we call the soul,” Andujar says. “They have this fear, this suspicion of the camera. I only took pictures of Yanomami who got to know me first, when they knew why I was there. After a while they trusted me and would take no notice of the camera.” In 1976, Adujar was back in São Paulo, struggling to get permission to return to the Amazon, when she heard of an ensuing measles epidemic among the Amerindian people. “I heard whole villages had disappeared, so many I knew died.” Kopenawa says that the disease was part of the ongoing persecution of his people by Brazil’s military dictatorship. “The white men were bringing all these diseases as the Transamazonian road was being built. I saw my mother and father die.” The tragedy precipitated a major change to Andujar’s career. Bruce Albert, a French anthropologist working in the area at the time, recalls that there was a strong desire by the dictatorship to keep news of the situation spreading. “The government agencies were spreading a lot of xenophobic, malicious rumours about us. They were furious we were there and one guy went round telling the Yanomami people we Europeans had come to steal their riches.” Albert recalls Andujar arriving on the scene in a black VW Beetle. “I was asleep in my hammock when I heard this engine noise. Back in Paris I drove a white Beetle and this was the exact same sound. I thought I must be hallucinating, but I got up, went outside and there was Claudia silhouetted against the headlights of the car on the dirt track, composed as if it were one of her photographs. She was someone I had heard about from the Indians but not met. Here she arrived like a character in her own work.” “I was undoubtedly aware that I was causing the government problems,” Andujar says, “They were watching me.” Albert and Andujar, alongside Carlo Zacquini, a liberal Catholic missionary, went on to form Comissão Pró-Yanomami (CCPY), an activist group that campaigned on behalf of the Yanomami, and who were instrumental in pushing for a continuous tract of officially demarcated land for the tribe. Granted in 1992, this now stretches over 96,650 square kilometres. However, despite legal protections, Kopenawa estimates 20,000 illegal gold miners are operating in the Yanomami Park with the tactic permission of Brazil’s government. It was through the CCPY, working with a group of doctors from São Paulo, that Andujar began coordinating a medical programme and the camera became but a tool in this activism. Yet the change of direction gave rise to what have proved to be her most haunting photographs. Traditionally, Yanomami do not give each other names, referring only to their relationships with one another. In order to keep medical records, the team needed a way of identifying each patient. Each of the Yanomami were assigned a number, written on a wood necklace, which Andujar photographed them wearing. In their pragmatic simplicity, and numbering in their hundreds, the images show the Yanomami at their most vulnerable, facing the onslaught of sickness brought in by outsiders. And, while this process was born out of administrative need, it has become Andujar’s most biographical project, the young and old staring back at the camera, their numbers recalling those branded on the victims of the Holocaust. Despite the power inherent in her photography, Andujar says she does not think art has a political agency. Gesturing to her life’s work, the artist shrugs. “This won’t change the attitude of Bolsonaro. All I can hope is when people look at my pictures they experience a connection with these people as people, people who, under this president, are suffering again.” • Claudia Andujar: The Yanomami Struggle is at the Fondation Cartier, Paris, 30 January – 10 May. Now closed to coronavirus, the exhibition can be seen here"
"

A potentially informative and constructive debate about the costs and benefits of global warming has been lost to “political dramatization,” argue the authors of a new Cato Institute book.



In _The Satanic Gases: Clearing the Air about Global Warming,_ climatologists Patrick J. Michaels and Robert C. Balling, Jr., trace the development of global warming, writing that politicians blame the latest thunderstorm, flood, or change in the weather on global warming. They also assert that global treaties, protocols, and other policies are being signed and negotiated despite shoddy science.



Michaels, senior fellow in environmental studies at the Cato Institute and professor of environmental sciences at the University of Virginia, and Balling, director of the Laboratory of Climatology at Arizona State University, analyze the politics of global warming and provide a primer on the science. Acknowledging that industrial emissions of greenhouse gases have warmed the planet and will continue to do so over the next several decades, Michaels and Balling argue that future warming will be moderate, not catastrophic, and will have benign economic and ecological effects. They point out that the effects of climate change are already positively affecting mortality and agriculture, citing data that show the “greening” of the earth may be enhancing plant growth. The year 1998, during which temperatures warmed as a result of El Niño, produced record agricultural output. The authors expect that the earth’s average surface temperature will warm 0.65 to 0.75 °C (1.17 to 1.35 °F) by 2050, resulting in a decline in temperature‐​related mortality and a rise in crop yields that alone would feed one‐​quarter of today’s world population



The authors find that government funding of research has corrupted the scientific process as scientists compete for funding in a politically charged environment. Total federal spending on global climate change research has ballooned from a few million dollars to $2.1 billion annually in the last 15 years.



The book has already received much praise. Frederick W. Seitz, past president of the National Academy of Sciences, says it “should be read by every scientist and layman who has an interest in the topic.”



 _The Satanic Gases: Clearing the Air about Global Warming _can be purchased through the Cato Institute’s online bookstore.



 **Mises, Hayek Examined in _Cato Journal_** __



The latest issue of the Cato Journal (vol. 19, no. 2) commemorates the 50th anniversary of the publication of _Human Action_ by Ludwig von Mises (Yale University Press, 1949) and the 100th anniversary of F. A. Hayek’s birth. Editor James A. Dorn writes, “These two giants of market‐​liberal thought exposed the fallacies of central planning, pointed to the importance of private property rights and limited government in promoting a spontaneous market order, and explained the role of institutions in shaping incentives and behavior.”



Papers by Vernon Smith, Israel Kirzner, Kenneth Elzinga, and George Selgin (along with comments by Lawrence H. White, Gordon Tullock, Frank Machovec, and Richard Timberlake) were first presented at the 1999 meeting of the Western Economics Association in a session titled “Mises’ _Human Action_ : A Critical Appraisal after 50 Years.” Smith discusses how the experimental economics in which he is a pioneer has confirmed Mises’ analysis of cooperation. Selgin and Timberlake examine Mises’ views on the role of gold in the monetary system. Elzinga and Kirzner both note Mises’ understanding that the market is a constantly evolving process, not a path to a particular endpoint.



Stephen Macedo of Princeton University discusses three themes in Hayek’s work: his critique of political utopianism, his emphasis on the interdependence of law and liberty, and his faith in the power of ideas and institutions. Ronald Hamowy, a student of Hayek’s at the University of Chicago, offers some personal reminiscences and an examination of Hayek’s history of liberalism. Those papers were delivered at the Cato Institute on May 8, 1999, the centenary of Hayek’s birth.



Other papers in the Cato Journal discuss the regulation of addictive substances, politics and the IRS, and women’s sports. _Cato Journal_ is published three times a year. Most articles are available at www​.cato​.org; subscriptions and single copies are also available.



 _This article originally appeared in the May/​June 2000 edition of_ Cato Policy Report.
"
"Gino McDonald, 61, builder (on left), and Patto McDonald, 56, artist, Upper Brogo  Patto We were keeping an eye on the Fires Near Me app. By about lunchtime on New Year’s Eve we’d packed our daughter and grandkids up and sent them off. We stopped by the fire shed on the top of the hill. They seemed to think we were going to be all right, so we thought we’d be back the next day. We grabbed some dirty clothes baskets, a few things. But when we looked at Fires Near Me the next morning, we were horrified at how big it had got overnight. They reckon the fire spread at 60km an hour. It was a shock for days. It wasn’t until the third time we came back that I took everything in: the neighbours’ houses all gone, just a wasteland. It was as if we’d never been here before. We’ve lived here around 17 years. I had a lot of things – my grandmother’s and my mum’s. It’s taken me two weeks of saying, “Oh, it’s just stuff”, until my daughter said, “Stop saying that, Mum, it’s not just stuff.” But it was; that’s been a lesson for me, not to get attached to things, because it doesn’t matter. We don’t have any insurance, so we’ll be starting from scratch again. We started off here in a tent, anyway, so I guess we’ll be going back to one. It’s like Seasick Steve sang: “I started out with nothing, and I’ve still got most of it left.” We lived simply. We had one solar panel that was enough to give us lights, and a gas fridge; we used a lot of recycled materials. So that makes it a bit easier to start over. Our chief concern is our daughter, who moved in with us before Christmas with two babies; we’ll probably rush to get something up for her. Jenni Bruce, 63, artist, Upper Brogo, New South Wales I’ve lived here for 44 years. It’s fringe farming country and we’ve got all kinds of people: tradies, chippies, sparkies; life-changers as well, retirees and artists who have come to live in the bush. It was New Year’s Eve when we knew we’d have to evacuate, but we thought we had a great deal more time – that it wasn’t going to reach us until the next day. I was planning to stay in that night, but some of my friends had already evacuated to Quaama, a nearby village – I thought I’d just pop in and see them. When I turned around to come home, I saw the fire front. It was just a land-eating monster, tearing through with a ferocity like nothing I’d ever seen. I knew there was no going back into that. We waited in Quaama and watched until it became very evident that we needed to move. I helped a couple of locals evacuate, so I was probably one of the last cars out. By the time I was on the highway, the fire was only a few metres away; it was just so hot and so loud – the noise was unbelievable. At 5am we arrived in the tiny beach town of Bermagui and spent the day there. It was chaotic, a thousand people poured into a small space with only four toilets. None of us had much in the way of supplies, but the shops, bowling club, pub – everybody was doing their best to make people comfortable. Coming back home for the first time, after a few nights, was awful. You always hold out that tiny bit of hope that, just maybe, there’s something. But it was all gone. I had a great workshop with tools that I had spent a lifetime gathering. There was a beautiful garden, with lots of blueberry plants and fruit trees. I’d recently sold a lot of things to fix the place up, so I could stay as I grew older, and I’d just finished renovating. I had a huge collection of paints and canvases, and I was working on a new body of work to exhibit. It was just devastating to lose that amount of hard work. All I could do was keen – it’s a really weird noise; I’d only made it once before in my life, but I keened. There’s not a single person in the entire region who is not traumatised – all over the Great Dividing Range. I wish the people in power would listen; I wish they would stop using industries that are so bad for our environment. I hope that the underlying anger, because of mismanagement from our leaders, does not overrule the kindness and compassion that people are showing in the present moment. I don’t really know what my life lesson is: I’m just very glad to be alive. Sharyn Wotton, 61, teacher’s aide, and Tom Wotton (AKA Swampy Tom), 66, retiree, Wandella Sharyn Tom and I have been together 45 years and lived at Wandella for 24. We have 100 acres about 15km from the nearest town. It’s not a hobby farm – we don’t raise animals. We just live here for the peace and quiet, the serenity. We had always agreed we wouldn’t stay and fight a fire. We had used planks of foam, pontoons from Sydney harbour, as insulation for our walls. But the 75 acres we had behind us were wooded, and we’re in a gully, so we thought: we don’t have enough water. On 30 December, our eldest daughter, Carly, texted and said: “Mum, it’s doubled again.” Our two sons-in-law hooked up our 1979 Millard deluxe caravan, and we put a few bits and pieces in there and the car. I remember my daughter Teagan saying, “Mum do you want this, this?”, and I said, “No, I’ll come back tomorrow.” But there wasn’t a tomorrow. After a few days, we went to Canberra, where we got word from our neighbour who had stayed to defend his property. He said a huge firewall just came from nowhere, up one side of their house, and he knew then our place was probably gone. He was alive, and his kids were alive, but there have been so many horror stories. We’ve got friends who ran through firewalls to save themselves and ended up with burns. One family lost a father and son. We might have lost property, but there are people out there who are traumatised for ever. For me, it comes in waves. If I’m not talking about it, I’m OK. When we drove back, I was stunned. I joke that I’m a collector, not a hoarder, but I had a lot of stuff – big bookcases and stuff hanging from ceilings, and it’s all gone. We didn’t have insurance at the time of the bushfire. One of our children, without asking, started a GoFundMe page. When he found out, Tom was really angry and said, “We’re not a charity case.” Carly sat him down and said, “For 24 years, you have watered and fed people. People have come for two days and stayed for five because you make them feel welcome.” She said, “They’re donating to you. They want you to rebuild.” We’ve pissed mother nature off big time, and she’s paying us back. We’ve just been watching it get drier and drier – the whole valley’s been a tinderbox. Nobody heeded the warnings. Surely they’ll listen now.  Fina Montagner, 49, care nurse, and Anthony Montagner, 64, former electrician, with their sons Christian, 10, and Dylan, six, Upper Brogo Anthony We had gone into Bega to do some shopping on New Year’s Eve, and Fina and the boys decided to stay because everyone was talking about the fire. At about 5.30 that afternoon I came back here with the dogs to get the farm ready, thinking I could stay and fight the fire. At eight o’clock, there was no sign of fire. At 10 o’clock I went out to turn the generator off and noticed the glow to the west, 10, 15km out at least. I’ve had fires here in the past and I thought, well, that won’t be here till the morning. Forty-five minutes later, and the fire’s in my next gully. So I barely had 15 minutes to grab the dogs, a couple of cans of fuel, the kids’ bikes, a pair of jeans and I took off. I was only ever 10 minutes from the fire all the way to the coast.  I’ve had fires get away on me in the early years. This was a monster; it was racing as fast as my van would go. And the noise sounded like a couple of freight trains. It was sucking air in from all directions. It was a fire-breathing dragon, spitting hot flames everywhere. But it’s not climate change – I’ve heard of at least 180 people arrested for arson.  I haven’t been insured for 25 years. I’ve built what I had by scrounging. When I met Fina, she got a job so we were able to buy some new materials, but other than that everything in the house was built from bush wood. It was all repurposed junk. But I’m not going to do that again, there’s nothing left here to do it with. I’m not prepared to start scrounging again. We’ve got a caravan for the boys. We’ve got another coming for us. So we’re just going to camp here for the time being, maybe six months, a year.  Fina The little one seems OK, because he’s only six. But Christian is having issues. He gets very angry and then he’s anxious. He’s been really sad in the last few weeks. Anthony I don’t have one photo to show them what their grandmother looked like. I’m hoping my siblings might have some. I don’t have photos of my childhood or anything I’ve done as a teenager. It’s like I’ve never existed.  Fina We’re just hoping to be strong, emotionally, for the boys. They have got no one but us, so we need to show them that we are tough. Ron Corby Snr, 87, farmer; his son, Ron Jr, 67, farmer, and his wife Gloria, 71, housewife; granddaughter Tammie, 42, carer, with Brett Jee, 45, painter, and their sons (from left) Blake, 16, Myles, 12, Beau, 10, Mason, 14, in Wandella Ronald At 2.30am, I received an urgent call: “If you want to get out, get out now!” I looked out the window – everything looked normal and I went back to bed. Ten minutes later, my daughter pulled up and said, “Pop, get in the car and go! My house just burned down and the fire followed me, nearly as fast as I could drive.” When I came back a couple of days later, there was just charcoal. This fire was hotter than anything. I saw cars’ aluminium wheels melted and running down the gutter like a stream. The fire came from all angles, in whirling winds and twisters. It’s 35 years since the bush was last burnt out. The stuff in the mountains has been building up; you couldn’t walk through it, there’s that much rubbish. When the fire came, the whole mountain seemed to explode. I don’t think it was the climate crisis that caused this; I think it was neglect, not keeping the mountains clean.  I grew up in the Great Depression. Back then, it was a bit like this. We had nothing. But when someone was sick, the community would come together. I thought, in this modern age, that was gone – it looked like people would rather cut your throat. But I was wrong. A man who didn’t know me gave me A$500, and he’d probably lost his job the same as anyone. But the human love is there and stronger than ever.  Ron All my life I’ve fought bushfires, but there’s never been one like this: the fireballs, the way it roared, the flames, 50-60ft high. Sometimes it seemed to be above the ground, just burning the air. It sucked the roof off the house and threw our cars a kilometre away.  Tammie Forensics think two fire fronts collided and created their own storm. My brother had a bull that was killed in the fire and is still sitting upright. It was just instantly cooked, mummified. They told us that for that to happen, it had to reach 2,000 degrees. I don’t think it’s climate change; the bush here hasn’t been burned back in 15, 20 years. We have five boys. Mason has muscular dystrophy and we’d set up an aviary for him. He can’t do anything active, but loves birds and wanted to breed them. They were all lost. He was hard hit and cried quite a bit, but we’ve told him we’ll build him a new one.  We didn’t bring the boys straight out here. But our youngest, who’s 10, would not stop asking what it was like. Is this still OK? Is that still OK? So we thought we should show them that there is nothing salvageable. But he found a few garden ornaments and said, “Look, Mum, you can put this in the new garden.” Brett and I have been together 24 years. So when the insurance company says, “Can you write down a few lists of your assets?” there aren’t enough notebooks. I’ve not seen my husband cry like this. Brett’s father passed away a few years ago and he had a lot of his belongings here, tools and stuff you’ll never get again. But we’re just thankful that none of us are lost. All our family’s still here. Veronica Coen, 59, mental health clinician, and Murray Gibbs, 62, trike tour guide, Quaama Veronica We were away, visiting my daughter and her family. But this just wasn’t a fire that human beings could tame. It’s become very political. I call myself a fire refugee, and think about the people in the Pacific Ocean on the low-lying islands, and war refugees – because it’s as if there has been a war.  For a while I had a great deal of hope that the people who run the country would have to get it. That’s dissipating now. There has to be some kind of transformation in the way we govern. This two-party system is combative: they consume too much energy on the ego stuff and don’t respond to human need. A lot of people I’ve spoken to are clearly dealing with post-traumatic stress. I had a meltdown the other day and made someone else really sad. I was able to collect myself the next day and was welcomed back with open arms, because everybody’s falling apart. I’m a trauma-informed practitioner, and it’s a slow process. I’m not someone who usually posts my status on Facebook, but before I knew it, I just put it up there: “Don’t rush my recovery.” That’s really what it feels like.  In my work, we talk about collective trauma. Until now, I’ve only read papers and heard it spoken about in relation to Indigenous communities, for example. We talk about intergenerational trauma and collective trauma – this is what we have.  Pam Sweeny, 63, nurse, Cobargo My husband, Michael, and I had four children, and brought them up here in a way that’s kind to the environment. We’ve been using stand-alone solar power for 31 years. We’ve got our own water, and got rid of our own effluent in a way that’s friendly to mother nature. We had our family of native birds, too. We’d lie on the grass with the kids at night and watch the little gliders glide out. When we came back, the trees were still falling. It felt sad and unsafe. There was a big tree across the driveway, and piles of tin from the sheds that had been burned out. I wake up with tears, but then you just get busy. What we’ve done to Australia in the years that we’ve been here, us whitefellas, we’ve got to learn – we’ve done too much damage and mother nature’s hitting back. We didn’t even have a lock on the door, so we couldn’t get insurance. That is not unusual here. There’s loss, but we can replace it. New year, new beginning. We’ll be fine, but people who were used to spending five minutes in the shower and leaving their lights on [will struggle]. We’ve got to stop being such consumers, and think about where things come from: from mother nature. David Wilson, 59 (on right), and Kyle Moser, 41, both post office workers, Cobargo David I’m the licensee of Cobargo post office. Kyle and I moved here four years ago from the city, and we love the place. It’s the most unique community: 50% old-style farmers, 50% new people, often very musical or artistic. One in three says they’ve lost their house. We’re still hearing of people in hospital. Each one that dies is just a blackness, a real sadness. We didn’t really sleep on New Year’s Eve; we just watched and waited. It got to a point where it was coming in all directions. There’s only one road out, and it looked like the single bridge that leads into town was on fire, too. We headed for the ocean with four dogs, two cats and two rescued wallabies in the car.  The community response has been amazing: we had a relief centre, operational pretty much from that night, and it just grew. There were toilets, showers; they quickly got a generator on site. When Kyle and I came here, we were widely accepted as a gay couple. Quite a few families, although they might seem tough on the exterior, have a gay son or a gay nephew. During the marriage equality debate, we raised the rainbow flag, and Australia Post told us to take it down – they wanted to remain impartial. The response of the community was: “Well, if the boys can’t fly their flags, we’ll do it for them.” Shops put rainbow-coloured clothes over the balconies, the bookshop put rainbow-coloured books in the windows. It was amazing. We’re lucky that we’re insured, but precious things were lost. For my 50th birthday, we had a ring made with diamonds and my date of birth on – that’s gone.  But what upsets me most is the political inaction of the last two years, when they’ve been warned of these conditions. People had been asked for additional firefighting aircraft. They should be held criminally responsible.  Seraphina Leahy, 20, artist and barista, Wandella  I moved out here to get away from everyday life, and to try not to have an impact on the environment. When we heard there were fires coming, we packed up a few things and went to my mum’s house in Bermagui. Then the Rural Fire Service said we needed to evacuate. We went down to the beach, and when you looked back, you couldn’t see the mountain any more: it was pitch dark and a huge glow reflected on the ocean. We camped on the beach, and went fishing to calm our nerves, so we kept ourselves fed. We had my nana with us, who’s in her 80s. We had word that the house was gone, but until we saw it I didn’t really believe it. We have a lot of hard work ahead of us, mentally and physically. The support’s been amazing; if we didn’t have that we probably wouldn’t be doing OK. You’ve got social media, and sometimes it feels as if everyone’s always arguing, but when something like this happens, humans pull together and look after each other. We were doing our best not to make an impact, having a carbon offset with all our trees. In two years it’s gone from lush green pastures and forest, to the leaves on the trees just dropping. The flowers don’t come out, the animals are struggling, everything is yellow and brown.  I think the prime minister needs to have more empathy. The government cut so much funding to forestry and national parks. Everyone’s dry in terms of resources; we need the land managed properly.  • Additional research by Annette Widitz. The Cobargo Community Bushfire Recovery Fund is crowdfunding for the local area. This article has details on how to help with the Australian relief effort more generally. If you would like your comment on this piece to be considered for Weekend magazine’s letters page, please email weekend@theguardian.com, including your name and address (not for publication)."
"

In an abstract presented at the 26th PACLIM Conference that was published in a recent issue of Quaternary International, Verosub (2015) writes about the challenges of maintaining and utilizing water supplies in California. However, the geologist from the University of California notes that what is often missing from discussions of water security is a consideration of the effects of natural climate variability beyond the historical record. As an example of such variability, Verosub cites the fact that river flow and lake measurements during the 20th century “document the occurrence of several multi‐​year droughts in the past 100 years while tree ring records show that 20‐​year and 70‐​year droughts occurred during the last 300 years.”   
  
  
On an even _longer_ time scale, the scientist reports that “at least once and probably several times in the last few thousand years, there have been droughts severe enough to drop the level of Lake Tahoe by several tens of meters, which allowed Douglas fir trees to grow to maturity on exposed lake beds.” Furthermore, other data indicate episodes of extreme flooding, such as the water year of 1861–1862 that brought extensive rainfall from Oregon down through southern California.   
  
  
In consequence of these realities, Verosub concludes that “the paleoclimate history of California suggests that even in the absence of climate change due to anthropogenic greenhouse gases, decadal, multi‐​decadal, or even century‐​long droughts are a real possibility in the future for California as is flooding on a greater scale than was seen in the twentieth century.” According to Verosub, if such _natural_ events were to occur today, they would easily “wreck havoc with California’s delicately balanced water delivery system” in the case of drought, and “overwhelm the levee system and destroy California’s ability to transfer water from north to south” in the case of flooding. No doubt, such events would quickly be labeled by climate alarmists and advantage‐​seeking politicians as “human‐​caused.” Yet, given the historic periodicity of these events, there would be no way to prove that they weren’t natural. In fact, their mere occurrence would simply confirm that they _are_ natural, recurring over and over again throughout history, human influence notwithstanding. As such, the title of the author’s work provides some good advice for Californians: _Don’t worry about climate change; California’s natural climate variability will probably “get us” first_.   
  
  
  
  
  
**Reference**   
  
  
Verosub, K. 2015. Don’t worry about climate change; California’s natural climate variability will probably “get us” first. _Quaternary International_ **387** : 148.
"
"
Share this...FacebookTwitterBy Kirye
and Pierre Gosselin
It’s been a particularly mild winter in Europe this year. But that hasn’t changed the long-term trend over the past 30 years.
Now that the February 2020 data have been coming in, we plot the mean February temperatures for some countries in Europe.
Sweden
Three of 5 stations show February mean temperature in Greta Thunberg’s Sweden have had a cooling trend since 1988! The real data will probably make the climate alarmists upset.

Data source: JMA. 
Great Britain
Twelve of 14 stations in UK show February mean temperatures have had a cooling or no warming trend since 1992! (Note: those 14 stations have data dating back at least to 1980s):

Source: JMA
Finland


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




As a whole this northern European country has seen very little warming over the past 3 decades for the month of February:

Three of six stations in Finland show no warming since the 1980s. Data: JMA.
Ireland
Ireland, situated in the North Atlantic, also shows no warming for the month of February since 1987:

Four of 6 stations with data going back to the 1980s show no February warming. Data source: JMA
Netherlands
The story is the same in the Netherlands for the month of February. All 5 of 5 stations there with data going back to at least 1995 show February temperatures having a cooling trend:

Mean February temperatures have been falling at stations in the Netherlands since 1995. Data source: JMA. 
So it remains a mystery as to why global warming alarmists claim rapid warming is happening.


		jQuery(document).ready(function(){
			jQuery('#dd_78e475dfa13960a3458e73b79c2c48f9').on('change', function() {
			  jQuery('#amount_78e475dfa13960a3458e73b79c2c48f9').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
 There’s an interesting thorn in the side of the recent planning commission approval of Meriam Park that nobody seems to have brought up or discussed. Maybe there are plans I’m not aware of, but given the issues being raised with a cell phone tower elsewhere in the city, it seems the issue would have been vocalized by now.

There’s a giant cell phone tower jutting into the Meriam Park property. Most people think its a standard radio station. It was, but not anymore. It’s purely a cell phone site. It’s at the intersection of Bruce and Picholine Way, as seen at left.

I’ve also provided an aerial view from Google Earth.

The former KHSL-AM radio towers on Bruce Road no longer broadcast on AM 1290 as they did for 50 plus years. That transmitter was removed a few years ago but the towers remained. The FCC license was and property was sold to McCoy broadcasting and KPAY 1050 was converted to 1290.
In the early 1990’s, one of Chico’s first cell phone services was placed on the East tower and it remains in service today. The West tower is not transmitting anything at all. A few years ago, the Bruce road property was sold to one of the cell phone companies when Clearchannel bought KPAY from McCoy.
The old KHSL Radio tower cell site is probably the best in Chico due to its location and height. I don’t think they’ll be easily persuaded to give it up.
So now, some city councilors that may want to vote against a cell phone tower at the Elks Lodge for “health and safety” issues raised by enlightened citizens may find themselves in a pickle when it comes to approving Meriam Park homes that will be less than 500 feet from those “dangerous cell phone emissions”.
In the graphic below from the Meriam Park planning website, I’ve added the pointer showing where the cell phone tower is in relation to the rest of the plan.

The neighbors right across the street on Picholine fit in that zone already.
I wonder if the neighbors on Picholine Way were ever told of the nature of that tower? I wonder if they even care? Since the cell phone transmitters have been there about 15 years now, I wonder how many of the neighbors are suffering from debilitating health issues as is claimed by some detractors of the Elks Lodge cell tower?
‘Tis a quandary for sure.
UPDATE– It turns out New Urban Builders has purhcased the tower property, see comments.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea64c07c1',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterGerman broadcaster RTL here reported how Northern Hemisphere snow mass has reached the highest level in years.

Image: Finnish Meteorological Institute
The Finnish Meteorological Institute (FMI) reports the total amount of snow in the northern hemisphere this winter season has been well above the long-term average from 1982 to 2012.
This will come as a surprise to Europeans, who have seen one of the mildest winters on record. According to RTL, “In those places where it was cold enough for snow at all, there was a lot of snow. The snow is meters high, higher than usual.”
Snow cover trending upwards since 1990


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Looking at the northern Hemisphere snow cover charts from Rutgers University Global Snow Lab, northern hemisphere snow cover (area) has been on the rise since reaching a low in 1990.

Chart: Rutgers University Snow Lab.
Record-breaking snow across Montana and South Dakota
Meanwhile weather site Electroverse reports of “record breaking February snowfall” burying Montana and South Dakota. According to Electroverse, “The cold times are returning” due to reversing natural cycles.
Record Breaking February Snowfall Buries the U.S. States of Montana and South Dakota



		jQuery(document).ready(function(){
			jQuery('#dd_51c08b0b0f83667272b40339623e8ea9').on('change', function() {
			  jQuery('#amount_51c08b0b0f83667272b40339623e8ea9').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Wind energy has been identified as having an important role to play in the world’s move towards a low-carbon future. But, due to short-term planning rules, it may not have as big a part as it could in the UK’s own sustainable energy generation. To date, when most UK wind farms were under development, temporary planning consent of 25 years was granted. Under the terms of this consent, when the two and a half decade period comes to an end, the turbines have to be removed and the land returned to its previous use. Now, a significant number of the country’s wind farms are starting to reach the end of their permission period, 62 wind farms in England, Wales and Scotland are aged 15 and over and 22 of these are more than 20 years old. If existing sites are removed without replacement this could decrease the overall amount of energy generated from UK renewables.  There are other problems too: the government has warned that there is a risk of equipment being abandoned on some of the oldest sites, because some original planning consents failed to specify the removal of all of the infrastructure. In some cases, large equipment and cables do not have to be removed. And in 2015, the government created major planning hurdles for onshore wind farms and ended subsidies. As a result, there has been a 94% drop in applications to build new wind farms in England alone. But all is not lost. To combat the issue, in July 2018 it was announced that the repowering of existing wind turbines would not be subject to the same planning hurdles as new sites. And our analysis has now confirmed that repowering can massively increase the energy output of the UK’s wind farms. There are three options for wind farms that reach the end of their planning consent. First, they can be decommissioned, the infrastructure removed and the land returned to its previous condition. Another option is for the operational life of the existing turbines to be extended. This involves getting planning permission to keep the turbines in place, usually for another five to 10 years. Or the farm can be repowered, which means old turbines will be replaced by newer ones. So far, most wind farms that have reached the 25 year limit have been repowered and given a 25-year consent for the new turbines. Or they have had the permission for the original turbines extended, allowing them to continue working for up to 10 additional years. Across Britain, 23 sites have already been repowered and at least three have extended their life, while only two have been decommissioned. Repowering sites has proved to be a great opportunity to increase the energy produced. On average repowering has increased the output of sites by 171%. The following table shows the potential increase in power (measured in megawatts) from sites repowering in England, Wales and Scotland within the next 10 years. In all, 54 sites are due to come to the end of their life within five years, and 161 more within ten years. This data is based on 23 sites that on average (excluding sites that repowered early for technical reasons) repowered after 18 years of operation. Although not all farms will repower this early, or have the same increase in output, these estimates show that repowering has the potential to greatly boost wind farms’ contribution to UK energy supply. If all 215 sites did repower at this level within the next ten years the energy increase will be enough to power an additional 3.8 million homes. Repowering doesn’t come without its challenges, however. It can change how a wind farm looks, which is not always popular with the general public. On average, repowering has reduced the number of turbines on a site by 24%, but turbines have got 89.5% taller. This has caused difficulties for local planning authorities when assessing the visual impact of planning applications.  Public opinion and the benefits for local communities that come from repowering should be an important consideration – but this hasn’t played a major part in approval decisions, and the public response to repowering schemes has varied. Some approved schemes – such as St Breock wind farm in Cornwall – have received significant support, while others (Ovenden Moor in West Yorskshire, for example) have come up against local opposition.  Crucially, despite the evident benefits to repowering wind farms, there is still not enough governmental guidance to ensure that decisions can be made quickly and fairly in a way that balances energy production with local environmental, social and economic benefits.  This is not an issue that the country can sit on. With so many wind farms approaching their 25th year, we need to act quickly in order to maximise the potential benefits to energy generation and carbon reduction targets."
"The crisis is not imminent. The crisis is here. The recent infernos in Australia; the storms and floods in Brazil, Madagascar, Spain and the US; and the economic collapse in Somalia, caused in part by a devastating cycle of droughts and floods, are not, or not only, a vision of the future. They are signs of a current and escalating catastrophe. This is why several governments and parliaments, the UK’s among them, have declared a climate emergency. But no one in government acts as if it is real. They operate within the old world of incremental planning for a disaster that has yet to arrive.  Nowhere is this clearer than in the reports of the Committee on Climate Change (CCC), the official body that began with such hope and promise of holding the government to account, but that now seems to have abandoned scientific realities in favour of political priorities. Its latest report, on changing the UK’s land use, is so unambitious that, in some respects, it would take us backwards. For example, it calls for a 10% reduction in cattle and sheep numbers over the next 30 years. But it admits that over the past 20 years, their numbers have declined by 20%, so this would involve a slowing of the trend. Cultured meat and milk could replace these sectors almost entirely by 2050. The report makes no mention of rewilding or natural regeneration. The only means it proposes by which trees should return to the land is planting. This is often a slower, more expensive and less effective way of restoring habitats and sucking carbon out of the atmosphere than removing livestock or controlling deer numbers and allowing trees to return by themselves. Its target for reforestation is so feeble that the UK would still have less than half the average current European forest cover by 2050. One of the reasons for this timidity is its preposterous assumption that if land is unsuitable for commercial forestry, it’s unsuitable for trees. There are plenty of places where trees grow well, store carbon and provide magnificent habitats, but won’t produce straight 50-foot poles. The CCC envisages not wild woods, but plantations, whose purpose is the discredited policy of “bioenergy with carbon capture and storage”. This means growing wood to burn in power stations, then capturing and burying the carbon emissions. It is likely to cause more harm than good. Could the committee’s enthusiasm have anything to do with the fact that one of its members works for Drax, the energy company pioneering this disastrous technology? Throughout the report, business appears to come first; nature and climate last. All this, the CCC says, is consistent with the target it has set for the government, of net zero greenhouse gas emissions by 2050. It tells me that the rationale for this target “remains valid today”, meeting the UK’s obligations under the Paris agreement. This agreement commits governments to seek “to limit the temperature increase to 1.5C above pre-industrial levels”. But in November, the UN published a report showing that preventing more than 1.5C meant cutting greenhouse gas emissions by 7.6% every year between now and 2030: a much steeper trajectory than the CCC’s. The committee has set the wrong target, for the wrong date. But I think the problem runs deeper than this. It’s not just the target that’s wrong, but the very notion of setting targets in an emergency. When firefighters arrive at a burning building, they don’t set themselves a target of rescuing three of the five inhabitants. They seek – aware that they may not succeed – to rescue everyone they can. Their aim is to maximise the number of lives they save. In the climate emergency, our aim should be to maximise both the reduction of emissions and the drawing down of carbon dioxide already in the atmosphere. There is no safe level of global heating: every increment kills. Maximisation is implicit in the Paris agreement: it requires governments to pursue “the highest possible ambition”. In its land-use report, the CCC repeatedly admits that it could go further, but insists it doesn’t need to, because its policies will meet the target. The target has supplanted the ultimate objective, which is to respond appropriately to the climate emergency. This is a classic vindication of Goodhart’s law: “When a measure becomes a target, it ceases to be a good measure.” We are all familiar with the absurdities of target culture. We know how, in many workplaces, the target becomes the task. We know how official targets for depriving people of social security ruined thousands of lives. We know that the Windrush scandal – the persecution and wrongful deportation of people legally entitled to reside in the UK – was caused in part by the Home Office target for “enforced returns”. We know how targets encourage people to game the system, as hospital administrators do with their waiting lists, and cause Kafkaesque nightmares of overzealous officialdom, as David Boyle documents in his new book, Tickbox. But less discussed is the way in which targets can encourage officials to underperform. As soon as you set a target, you pull back from maximisation. Even if you say “this target is the minimum”, as the CCC does, politicians treat it as merely the line they need to cross. At this point, they fulfil their legal duty, even if they fail to fulfil their wider duty of care. Is a policy of maximisation possible? It is not only possible, it’s already happening, in exactly the wrong place. The 2015 Infrastructure Act introduced a legal duty to “maximise the economic recovery” of petroleum in the UK. If drilling companies fail to maximise their extraction of fossil fuel from an oilfield, they will be forced to surrender their licence to operate. In other words, while the government observes a legal minimum (the CCC’s target) for reducing greenhouse gases, it observes a legal maximum for increasing them. The appropriate response to the climate emergency is a legal duty to maximise climate action. The CCC’s board should be disbanded and replaced by people whose mandate is rigorously to explore every economic sector in search of the maximum possible cuts in greenhouse gases, and the maximum possible drawdown. We have arrived at the burning building. The only humane and reasonable aim is to rescue everyone inside. • George Monbiot is a Guardian columnist"
"
Share this...FacebookTwitterEurope storm leads to negative electricity prices
By Die kalte Sonne
(Translated/edited by P. Gosselin)
It almost hurts a little that “specialist for renewable energies” (own claim on Twitter) Prof. Volker Quaschning gets mentioned here so often. This is simply due to the absurd tweets the man continuously puts out.
His latest prank has to do with the storm Sabine, which earlier this week swept across Germany. It supplied a lot of energy in the form of wind, which made the wind turbines rotate strongly.
Even in the otherwise regulated electricity market, the laws of the market cannot simply be levered out. Supply and demand determine the price. If supply is higher than demand, the price falls.
In the case of electricity, even money might be included with the product when this electricity is purchased, meaning negative prices. Electricity is an extremely perishable “commodity”, it must be consumed at the moment of production. However, the “expert” Quaschning does not blame this oversupply and the negative prices on the volatile wind power plants, but rather on nuclear power and coal. They deliver very reliably and not wildly fluctuating like wind power.
Prof. Volker Quaschning tweeted in response to the negative electricity prices from overproduction which Germany saw during the recent storm:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Sturm und viel Wind sorgen wieder für negative Börsenstrompreise. Ein klares Zeichen, dass Kohle- und Atomkraftwerke zu unflexibel sind und nicht als Backup für erneuerbare Energien taugen. Wir brauchen darum einen schnelleren #Kohleausstieg. #FridaysForFuture #Scientists4Future pic.twitter.com/w7CRrEruoq
— Volker Quaschning (@VQuaschning) February 10, 2020

In English: “Storm and lots of wind are again causing negative stock market electricity prices. This is a clear sign that coal-fired and nuclear power plants are too inflexible and are not suitable as back-up for renewable energies. We therefore need a faster #coal exit.
#FridaysForFuture #Scientists4Future.”
A logical train of thought actually would have been to realize that highly volatile power sources such as wind and the simultaneous provision of base load are difficult to reconcile. Unfortunately, the energy source gas is also being massively fought by people like Quaschning, although it is more flexible and at the same time more CO2-friendly. In any case, however, it is only a crutch that might have to supply a great deal of energy, namely when we have the well-known lulls in wind and sun.
Every wind turbine and every photovoltaic system needs a backup. And anyone who has ever wondered why the wind countries of Denmark and Germany have such high electricity prices knows the reason. We are paying for a double power infrastructure. The prices will not decrease with an increasing share of renewable energies, but rather will continue to rise.
Share this...FacebookTwitter "
"

I’ve long argued that enviros don’t have anywhere near the electoral clout most people think and that no one is going to gain much political capital donning the garb of “Mr. Green Jeans.” Today, the trade publication Greenwire (subscription required) agrees. And believe me, these are the last people who want to make this argument. 



**CAMPAIGN 2006: Voters cool to climate issue in torrid midterm races**   
  
  
**Darren Samuelsohn, _Greenwire_ senior reporter**   
  
  
Five Northeastern Republicans facing fierce re‐​election battles turned just before the latest congressional recess to global warming in hopes the issue would boost their chances in their suburban House districts.   
  
  
But the lawmakers apparently got little traction from climate change in a campaign dominated by voter concerns about the Iraq war, President Bush’s unpopularity and overall dissatisfaction with Republican leadership.   
  
  
“It’s been very difficult for any of these incumbents whose problems are bigger than themselves, or whose problems have been themselves,” said Bernadette Budde, a senior vice president for the Business and Industry Political Action Committee. “They have had a hard time changing the subject.”   
  
  
The five — Reps. Curt Weldon (Pa.), Mike Fitzpatrick (Pa.), Christopher Shays (Conn.), Nancy Johnson (Conn.) and Rob Simmons (Conn.) — cosponsored in September what some consider the most aggressive bill to date aimed at limiting heat‐​trapping greenhouse gas emissions. The bill’s lead sponsor is Rep. Henry Waxman (D‐​Calif.), the presumed new chairman of the House Government Reform Committee if his party wins a majority of House seats.   
  
  
“Doing it before Congress goes off to campaign is telling,” said Howard Reiter, chairman of the political science department at the University of Connecticut. He added that global warming is a nuanced subject that comes with an important caveat: It may require constituents to make sacrifices in their day‐​to‐​day lives.   
  
  
“The problem with global warming is its incremental,” Reiter said. “It’s not as if there’s an immediate crisis people can see.”   
  
  
Massie Ritsch, spokesman for The Center for Responsive Politics, a nonpartisan organization that tracks campaign spending, said the recent media frenzy over climate change — from Hollywood‐​style documentaries to mainstream press coverage — did little to stir voters this year. “For all of the attention Al Gore’s movie got, it hasn’t stayed a major election issue,” he said.   
  
  
The lack of voter interest in climate change is not due to a lack of effort from environmental groups .…   
  
  
_Reporter Michael Burnham contributed to this report._
"
nan
"
Warren Meyer, one of the first surfacestations.org volunteers, delivered Tucson for us Saturday. It was discovered during an analysis of climate stations around the USA on the Climate Audit blog that Tucson had the greatest positive temperature trend for any USHCN station after the TOBS adjustment was applied. The TOBS adjustment corrects for differences in local times of observation of temperature by the observer. The picture says it all:

Yes folks, this is an official climate station of record, the temperatures it measures go into our National Climatic Database and are used in research such as the graph produced by NASA Goddard Institute for Spaceflight Studies here:

There’s a British word that has been bandied about to describe the reaction to pictures like this one: “gobsmacked”. The word applies even more so since this station is operated by science faculty members at the University of Arizona.
They are so proud of this station they even had a sign made for it to hang on the chain link fence enclosure:

The complete photo essay is available at the Tucson album at www.surfacestations.org The satellite and aerial photo images there are telling of the environment being measured.

Besides the obvious questions like “why is it in the middle of a parking lot?” and “why would scientists who should know better allow such a bizarre siting for a USHCN climate station of record?” Then there is this burning question: “Why did they go to the trouble of installing a precision aspirated temperature sensor and then not even bother to place it at the standard observing height?”. 

It appears that the Stevenson Screen serves no other purpose except as an equipment holder, as Warren Meyer reports the Stevenson Screen to be empty. Originally the inside standard mounting board for the mercury max/min thermometers were mounted about 1.5 foot higher than the air inlet of the precision aspirated temperature sensor. So the lower mounting height for the precision sensor adds a positive bias.
Is there no diligence left in basic measurement? Is this what they teach in college science departments these days?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4ef3691',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

New York Attorney General Eric Schneiderman demands out-of-state charities disclose all donors for his inspection. He does not demand this of all charities, only those he decides warrant his special scrutiny. Schneiderman garnered national attention for his campaign to use the powers of his office to harass companies and organizations who do not endorse his preferred policies regarding climate change. Now, it seems he seeks to do the same to right-of-center organizations that might displease him. Our colleague Walter Olson has cataloged Schneiderman’s many misbehaviors.   
  
He’s currently set his sights on Citizens United, a Virginia non-profit that produces conservative documentaries. While Citizens United has solicited donations in New York for decades without any problem, Schneiderman now demands that they name names, telling him who has chosen to support the group. Citizens United challenged this demand in court, arguing that to disclose this information would risk subjecting their supporters to harassment and intimidation.   
  
These fears are not mere hyperbole. If the name Citizens United rings a bell, it’s because the organization, and the Supreme Court case of the same name, has become the Emmanuel Goldstein of the American left, complete with Democratic senators leading a ritualistic two minutes hate on the Senate floor. In 2010, the Supreme Court upheld its right to distribute _Hillary: The Movie_, and ever since “Citizens United” has been a synecdoche for what Democrats consider to be the corporate control of America. Is it unwarranted to think that their donors might be subjected to the sort of targeted harassment suffered by lawful gun owners, or that Schneiderman might “accidentally” release the full donor list to the public, as Obama’s IRS did with the confidential filings of gay marriage opponents?   
  
The Supreme Court has long recognized the dangers inherent in applying the power of the state against the right of private association. The cornerstone here is 1958’s _NAACP v Alabama_ _._ For reasons that hardly need be pointed out, the NAACP did not trust the state of Alabama, in the 1950s, to be good stewards of its membership lists. “Inviolability of privacy in group association may in many circumstances be indispensable to preservation of freedom of association, particularly where a group espouses dissident beliefs,” wrote Justice John Marshall Harlan II, who went as far as to compare such demands to a “requirement that adherents of particular religious faiths or political parties wear identifying arm-bands.” More recently, Justice Alito pointed out in a similar context that while there are undoubted purposes served by reasonable, limited disclosure requirements, the First Amendment requires that “speakers must be able to obtain an as-applied exemption without clearing a high evidentiary hurdle” regarding the potential harms of disclosure.   
  
But the Second Circuit Court of Appeals has decided it knows better than the Supremes. On Thursday, it ruled that Citizen United’s challenge should be thrown out without even an opportunity to prove their case. In the process, it effectively turned _NAACP_ into a “Jim Crow” exception to a general rule of unlimited government prerogative to panoptic intrusion into citizen’s political associations. While there can be no doubt that the struggle for civil rights presented a unique danger for its supporters, this should not mean that _only_ such perils warrant First Amendment protection.   




The marketplace of ideas is often fraught with contention, and those who support controversial causes must shoulder some risk. As the late Justice Scalia argued, “running a democracy takes a certain amount of civic courage.” But anonymity in such pursuits serves important purposes, and the premise that concealment of one’s identity is a sign of ill-will would have surprised James Madison, who published numerous defenses of the new constitution, convincing his fellow citizens of the virtue of the endeavor; he signed them “Publius.”   
  
In our schismatic political climate, many people could suffer if their political views were made widely known. This could include everything from adverse employment actions to outright violence. Some groups, such as those in the “antifa,” have openly advocated violence against political opponents. It’s odd that some on the modern left find themselves on the same side as the state of Alabama in 1958: arguing that those who support some political views should be disclosed to the state, even if violence might result. Although an appeal has not yet been filed, the Supreme Court should take the case and reverse the Second Circuit, making it clear that a compelling government interest is required before the government can force the disclosure of people’s political affiliations. 


"
"

A recent investigation by the Financial Times says that the new Carbon Credit Industry may already be rife with fraud. Hmmm…now where have we heard that before?
Among the findings:
■ Widespread instances of people and organisations buying worthless credits that do not yield any reductions in carbon emissions.
■ Industrial companies profiting from doing very little – or from gaining carbon credits on the basis of efficiency gains from which they have already benefited substantially.
■ Brokers providing services of questionable or no value.
■ A shortage of verification, making it difficult for buyers to assess the true value of carbon credits.
■ Companies and individuals being charged over the odds for the private purchase of European Union carbon permits that have plummeted in value because they do not result in emissions cuts.
From the article:
Some companies are benefiting by asking “green” consumers to pay them for cleaning up their own pollution. For instance, DuPont, the chemicals company, invites consumers to pay $4 to eliminate a ton of carbon dioxide from its plant in Kentucky that produces a potent greenhouse gas called HFC-23. But the equipment required to reduce such gases is relatively cheap. DuPont refused to comment and declined to specify its earnings from the project, saying it was at too early a stage to discuss.
The burgeoning regulated market for carbon credits is expected to more than double in size to about $68.2bn by 2010, with the unregulated voluntary sector rising to $4bn in the same period. 
Seems like the “green” here is not about Gaia…but all about Benjamins.
There’s no mention of how much these companies pay gamers to have virtual trees planted in video games.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6dacc7b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Carmakers are bracing for a hybrid electric car price war this year as they try to avoid steep EU fines for carbon dioxide emissions. Some carmakers are struggling to hit tough new EU carbon emission rules introduced in January, which force them to reduce the amount of carbon dioxide their vehicles emit. This may force auto firms to slash prices on lower-emitting plug-in hybrid electric vehicles (PHEVs) to encourage consumers to buy them, according to the investment bank UBS. Average CO2 emissions of almost all cars sold in the EU during the next two years must fall below 95g a kilometre, with heavy fines for carmakers that miss individual targets designed to meet the goal. The rules include the UK until at least 1 January 2021. Around €8.4bn (£7.1bn) will be wiped from carmakers’ profits over the next two years as they try to comply with the regulations. That’s €1bn more than previously expected, due to electric car delays and consumers’ desire for more polluting SUVs, according to calculations by UBS analysts led by Patrick Hummel. Sales of PHEVs, which can be charged from an external source without using the internal combustion engine, need to grow sevenfold between 2019 and 2021, they said. “As [carmakers] likely need to aggressively push PHEVs to their customers, we see risk of a price war in the course of 2020,” wrote Hummel. Germany’s Daimler, the maker of Mercedes Benz cars, and France’s Renault are particularly at risk of missing their targets and having to pay large fines. Fiat Chrysler Automobiles is also at risk, despite a deal thought to be worth hundreds of millions of euros to “pool” its emissions with Tesla’s zero-emissions cars. While legal, the deal has been heavily criticised by campaigners. Jaguar Land Rover, owned by India’s Tata Motors, is also exposed, with costs of £179m expected over the next two years. Britain’s largest carmaker, which has just launched a second round of tough cost-cutting as it struggles with electric investments, needs to increase its hybrid sales rapidly, UBS said. The green credentials of some hybrid cars are controversial because of their continued use of polluting internal combustion engines alongside battery motors. Julia Poliscanova, clean vehicles director at Transport & Environment, a Brussels-based thinktank, said there is evidence that plug-in hybrids rarely achieve the emissions reductions predicted by lab tests because users fail to charge them. “In the real world, their emissions are often two or three times higher,” she said, referencing findings from the Miles Consultancy which said many business users never used their charging cables. If plug-in hybrids are not charged, they can even emit more carbon dioxide and air pollution than the equivalent car without a battery, because a smaller engine is pulling more weight and therefore running less efficiently. Nevertheless, plug-in hybrids, which can be charged from external sources, are a key part of carmakers’ emission-reduction efforts, because they can use the same factories in which they have already sunk billions of euros in investment. The emissions limits could hardly have come at a worse time for the car industry, with China’s market suffering two years of recession even before the coronavirus outbreak and sales of their profitable diesel cars in freefall following the “dieselgate” emissions cheating scandal. At the same time, consumer confidence is weakening across some key European markets and Brexit uncertainty has dented sales in Britain. “The new car market is soft at the moment so dealers are having to work hard,” said Mike Hawes, the chief executive of the Society of Motor Manufacturers and Traders SMMT, the UK car industry’s lobby group. “It’s a buyer’s market.” The costs of complying with the regulations are expected to vary across carmakers, which have radically different strategies. For instance, Volkswagen, the world’s largest carmaker by volume, has launched an all-out electric assault, with plans to produce 1m battery electric cars by the end of 2023. Rival executives believe the company panicked after its reputation was sullied by the dieselgate scandal. Rival manufacturers such as BMW and Daimler have instead focused in the short term on hybrids, which are easier to produce using existing factories and avoid problems such as shortages of lithium ion batteries. Car executives argue that hybrids are essential to reduce society’s emissions. However, they are also particularly attractive from a regulatory perspective. All hybrids that emit less than 50g of CO2 per kilometre under test conditions qualify for “supercredits” for the next two years. This means they count double under the EU regulations, so carmakers are equally incentivised to sell hybrids and battery electric vehicles. “Plug-in hybrids are a short-term compliance strategy for vehicle manufacturers,” said Poliscanova. “They are unavoidably costly because you are locked into two engines and a motor. They should not be part of the future.”"
"At 07:30 on July 1, 1916, a private in the British army – let’s call him Tommy Atkins – scrambled out of a trench in front lines and advanced across no man’s land towards the German trenches in the Battle of Albert, the opening phase of the Battle of the Somme. He had covered no more than 30 yards when he felt as if someone had hit him on the back of the head with a mallet. Checking himself dazedly, he realised that he must have taken a bullet in the left shoulder because the whole area was suddenly bright crimson. Private Tommy Atkins had just become one of the 57,470 British casualties on the now-notorious first day of the Somme. His actions in the next few seconds would ultimately save his life. He tore off the field dressing sewn to the inside of his uniform jacket, ripped it open and applied the contents to his wound before making his way back to the aid post. Tommy Atkins was lucky, not merely because the bullet had not killed him. He was lucky because this was 1916 and not 1914, and the nature of his field dressing therefore meant he had a good chance of staying alive rather than subsequently dying of sepsis caused by infection of his wound.  Death as a result of even relatively minor injuries due to sepsis and gas gangrene had been a major problem in the first years of World War I. Appalling conditions in the trenches meant that wounds were generally accompanied by fragments of dirty clothing and the bodily filth plastered about the battlefields.   In 1915, however, an army surgeon named Charles Cathcart recalled that even quite terrible wounds had been successfully dressed on the battlefield in ancient times using a group of bog mosses known as sphagnum. On the basis of some successful trials, he instigated a nationwide programme of bog moss collection to create what would become the standard field dressing issued to all UK and Imperial land forces as an integral part of their uniform. People gathered the humble bog moss across the country, from Bodmin Moor to the far north of Shetland. Moss provided a number of improvements over the cotton-based dressings used in the early period of the war. Sphagnum is primarily adapted to storing water – vast quantities of it – and it was capable of absorbing more than twice as much blood and fluids as cotton wool, thus initially helping to dry out the wound.  It was fibrous like cotton, and therefore helped to seal the wound, but unlike cotton wool it also appeared to prevent infection of the wound in some mysterious way. It was later discovered that sphagnum releases a chemical called “sphagnan” which inhibits nitrogen uptake by decomposer organisms, sending them into a form of stasis. This is what saved Private Tommy Atkins and thousands like him from a lingering death by sepsis even though the original injury trauma may have been successfully treated. Despite this record of success, the medical use of sphagnum was largely discontinued after World War I. In part, this may have been because supply chains relied on the rather laborious process of harvesting the moss in the wild. Its later use in certain sanitary products ended more recently, however, when significant concerns were raised about the environmental damage caused by such wild harvesting.   Indeed, it is the environmental damage caused by the unsustainable harvesting of sphagnum and its semi-decomposed remains – which we know as peat – that has led colleagues and I to investigate a new type of farming called “paludiculture”, namely harvesting wetland products on the re-wetted soils of former wetlands. In their natural state, such peat soils represent long-term carbon storage, often on millennial timescales. If such soils are damaged, this long-term carbon store is progressively oxidised, releasing lots of carbon dioxide into the atmosphere. Studies have indicated that turning peat soils into farmland is potentially the largest source of greenhouse gas emissions per unit area in the UK lowlands. Working with various partners, our research is investigating ways of growing sphagnum as a commercial crop on re-wetted organic soils. We are seeking to produce a sustainable growing medium with a view to replacing the use of peat – which is essentially just ancient sphagnum – within both the professional horticulture industry and the retail trade.  We are also exploring with Greenwich University the possibility of re-establishing sphagnum as a modern medical material for use in wound treatment and anti-microbial action, thereby coming full circle to that day on the Somme when the life of Private Tommy Atkins was saved by the extraordinary properties of the material in his field dressing. Listen to The Anthill podcast on remembering World War I here, or subscribe wherever you get your podcasts."
"

The Current Wisdom _is a series of monthly articles in which Patrick J. Michaels, director of the Center for the Study of Science, reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press. In this special issue, we focus on the climate implications of a carbon tax._   






We calculate, you decide.   
  
Once you make your selections, the calculator will return the amount of global temperature rise that will be averted as a result of your choices by the year 2050 and also by the end of the century.   
  
Try it using this example. Choose a 100% reduction of carbon dioxide emissions from the United States and the IPCC’s sensitivity value of 3.0°C. Hit “Submit.” The amount of temperature savings that results is 0.052°C by the year 2050 and 0.137°C by the year 2100. (Why we are using three significant digits is in the fine print at the end of this article.)   
  
[block module=""carbontaxform"" delta=""carbontaxform""][/block]   
  
Sorry, Major Kong (h/t to “Dr. Strangelove”), those _are_ the figures. That’s the right answer. Assuming the IPCC’s value for climate sensitivity (i.e. disregarding the recent scientific literature) and completely stopping all carbon dioxide emissions in the U.S. between now and the year 2050 and keeping them at zero, will only reduce the amount of global warming by just over a tenth of a degree (out of a total projected rise of 2.619°C between 2010 and 2100).   
  
If you think that a rise of 2.482°C is vastly preferable to a rise of 2.619°C then all you have to do is set the carbon tax large enough to drive U.S. emissions to zero by mid-century—oh yeah, and sell that tax to the American people.   
  
To explore other alternatives, use our handy-dandy calculator.   
  
Have fun!   




*********



 **The fine print:**   




The results from our calculator are produced from climate change calculations performed using the MAGICC climate model simulator (MAGICC: Model for the Assessment of Greenhouse-gas Induced Climate Change). MAGICC was developed by scientists at the National Center for Atmospheric Research under funding by the U.S. Environmental Protection Agency.   
  
We are not creative enough to have made that acronym up. MAGICC is itself a collection of simple gas-cycle, climate, and ice-melt models to efficiently emulate the output of complex climate models. MAGICC produces projections of the global average temperature and sea level change under user configurable emissions scenarios and model parameters. MAGICC is run using its default model parameter settings except for climate sensitivity, which you can choose from between 1.5°C and 4.5°C.   
  
The baseline emissions scenario against which all climate dioxide reductions were measured is scenario A1B from the IPCC’s Special Report on Emissions Scenarios (SRES). Scenario A1B is a middle-of-the-road emissions pathway which assumes rapid carbon dioxide emissions growth during the first half of the 21st century and a slow CO2 emissions decline thereafter. Emissions are prescribed by country groups. Our “Industrialized Countries” group is the OECD90 countries (which includes North America, Western Europe, and Australia, New Zealand and Japan.) In order to obtain the baseline emissions from the United States to which the emissions reduction schedule could be applied, the U.S. emissions were backed out from the OECD90 country grouping. To do so, the current percentage of the total group emissions that are being contributed by the United States was determined—which turned out to be right around 50%. We assume that this percentage will be constant over time. In other words, that the U.S. contributed 50% of the OECD90 emissions in 2000 as well as in every year between 2000 and 2100. In this way, the future emissions pathway of the U.S. was developed from the group pathway defined by the IPCC for the A1B scenario. From these baselines (either the U.S. baseline or the OECD90 baseline), carbon dioxide emissions reductions were applied linearly from 2005 to 2050 to obtain the user-specified total reduction. The new (reduced) emissions were recombined with the other (unadjusted) IPCC country groupings to produce the global emissions total. It is the total global emissions that are entered into MAGICC to yield global temperature projections. The results using the reduced emissions pathway were then compared to the results using the original A1B pathways as prescribed by the IPCC, with the baseline against which temperature changes were calculated set to the year 2010.   
  
We assume that a carbon tax would only be applied to reduce carbon dioxide emissions. In practice however, the only way to reduce carbon dioxide emissions is to reduce the burning of fossil fuels. Reducing the burning of fossil fuels will have co-impacts such as reducing the emissions of carbon monoxide (CO), volatile organic compounds (VOCs), nitrogen oxides (NOx), and sulfur oxides (SOx). The first three chemical compounds generally enhance warming while the latter generally retards it. Sensitivity tests using MAGICC indicate that for the OECD90 countries under the A1B pathway, the effect of collective changes in these co-emissions is largely compensative.   
  
Additional fine print on precision: The temperature savings are presented to three significant digits in order to tell the results apart. In the real world, the impacts from the emissions reduction pathways are not nearly so precise and, in fact, the temperature savings from most of the different carbon dioxide emissions reduction pathways are scientifically impossible to tell apart from each other, and in many cases, are impossible to tell apart from the original A1B scenario, i.e., they are same thing as doing nothing.


"
"
Share this...FacebookTwitterElsevier has accepted a new paper by Lüdecke et al, 2020, showing natural oceanic and solar cycles play a large role in modulating Europe’s climate. Offers new chances for robust midterm temperature prognoses. 
The paper, in press, journal pre-proof, analyzes natural variability in European monthly temperatures on decadal and multidecadal timescales and their possible drivers.
NAO, AMO, sun behind temperature variability 
The authors claim to have established characteristic correlations of temperature with ocean cycles, here NAO and AMO, and solar activity for many regions and seasons. This means it is likely that NAO, AMO and solar activity are the actual drivers of a lot of the temperature variability.
Figure 1 of the soon-to-be-published paper follows:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Image: Decadal and multidecadal natural variability in European temperature, Lüdecke et al, Journal of Atmospheric and Solar-Terrestrial Physics (journal pre-proof).
The authors did NOT look at the anthropogenic component of the long term warming of the past 150 years and its attribution but feel the results will hopefully help to better attribute shorter-term temperature changes and their typical patterns.
Chance for midterm temperature prognoses
This is important because efforts by the scientific community are progressing to better predict NAO and AMO for months and a few years in advance, This opens up new chances for more robust mid term temperature prognoses that also includes natural climate variability which the  paper documents.
The paper’s preliminary abstract (emphasis added):




European monthly temperatures undergo strong fluctuations from one year to the other. The variability is controlled by natural processes such as Atlantic cycles, changes in solar activity, volcanic eruptions, unforced internal atmospheric variability, as well as anthropogenic factors. This contribution investigates the role of key natural drivers for European temperature variability, namely the Atlantic Multidecadal Oscillation (AMO), the North Atlantic Oscillation (NAO) and solar activity changes. We calculated Pearson correlation coefficients r for AMO, NAO and sunspots compared to monthly temperature data of 39 European countries for the period 1901–2015 in order to search for ‘fingerprints’ of the natural drivers. A cross correlation window of 11 months was chosen for AMO, NAO and of 120 months for SILSO to account for possible time lags or phase shifts. The r coefficients were mapped out across Europe on a monthly basis to document regional and seasonal changes of the correlation strength. The NAO dominates European temperature variability during the winter months, with strongest relationship in February. The AMO modulates temperatures in March to November, with best correlations occurring in summer, but also in April. Regions of strongest AMO and NAO impacts shift across the continent from month to month, forming systematic patterns. Direct correlation of the solar 11-year Schwabe cycle with temperatures was identified only in some countries in certain multidecadal intervals during February, March, June and September. Previous studies have suggested a significant solar influence on the AMO and NAO. It is likely that the greatest impact of solar activity on European temperatures is of a non-linear, indirect nature by way of interaction with Atlantic cycles.”






The paper’s graphs show an oscillation of temperature that challenge the sharply rising GISS-like narrative for Europe.


		jQuery(document).ready(function(){
			jQuery('#dd_d64e9e9adc2a2d68f814d56480aff4dc').on('change', function() {
			  jQuery('#amount_d64e9e9adc2a2d68f814d56480aff4dc').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000


Share this...FacebookTwitter "
"
The picture below comes to me via my website www.surfacestations.org from volunteer site surveyor Bob Meyer. It is the USHCN climate station of record for Waterville, Washington.
In addition to the now commonly seen attempts at measuring the temperature of parking lots, this station sports another new feature: volcanic cinder rock under the station to complement the tidy sidewalk. Note the convenient drive through teller window nearby so that you can cash your paycheck while on the way to the Post Office to mail in your COOP observer form to the National Climatic Data Center.

There’s also a nearby building about 10 feet away, and of course, convenient close-by parking just a few feet from the MMTS temperature sensor. Note that published NOAA/NWS siting standards require a 100 foot distance from buildings.

The USHCN “high quality” set of climate monitoring stations keeps getting curiouser and curiouser.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea594507e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
 
In Today’s Chico News and Review, the cover story is about Internet Radio and all
the trouble the Copyright Royalty Board recently caused with a draconian ruling
on the cost to Internet Radio Stations. Regular over the air
broadcasters don’t have such limits because they are seen to ""promote the music
industry"" Its an alliance as old as payola.
But good news comes today. A bill introduced in Congress today could nullify
the new rates set by the Copyright Royalty Board (CRB) which advocates say would
put Internet Radio webcasters out of business, such as our own local
Radio Paradise.
Rep. Jay Inslee (D-WA) and Rep. Don Manzullo (R-IL) have presented the ""Internet
Radio Equality Act"" which aims to negate the controversial March 2nd
decision which puts royalty of a .08 cent per song per listener, retroactively
from 2006 to 2010 on internet radio.
Advocates of Internet Radio have dreaded the CRB ruling, which they say could
raise rates between 300 to 1200 per cent for webcasters. Earlier this month, the
CRB threw out an appeal by commercial webcasters, National Public Radio and
others to review the new rates and postpone a May 15 deadline for the
introduction of the royalty schedule.
If passed, today’s proposed bill would set new rates at 7.5 per cent of the
webcaster’s revenue — the same rate paid by satellite radio. Alternatively,
webcasters could decide to pay 33 cents per hour of sound recordings transmitted
to a single user.
This bill is a critical step to preserve this new growing medium, and would
present a level playing field where webcasters can compete on the same royalty
terms with satellite radio. It would also reset royalty rules for non-profit
radio such as NPR. Public radio would be required present a report to Congress
on how it should determine rates for their internet streaming media.
I hope this passes, not so much because local radio needs more competition,
but because this insane CRB ruling makes it nearly impossible for local broadcasters to
compete on the Internet at all. This would give everybody a fair chance and at
the same time bring in millions, perhaps billions in royalties for artists.
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6ce3daa',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Deer, bison and pronghorn traverse the plains in large herds…scavengers quickly sniff out carrion, sockeye salmon leap upstream, wolves attack in packs surrounding their prey, geese fly in fixed formations, possums play dead, rodents scamper into tree hollows, grizzly bears bluff charge when threatened and birds of prey soar on thermals. That may sound like a mountie’s report on the Canadian wilderness, but it’s actually how Rockstar recently promoted Red Dead Redemption 2 – its critically acclaimed game, which transports players to a sprawling and immersive Wild West. Red Dead Redemption 2 features more than 200 species of animal in a variety of habitats, and its record breaking success  suggests that authentic natural environments which mimic the ecology of the real world will become a mainstay of future titles.  Video games have grown in scale and complexity to the point where intricate virtual ecosystems of this kind are now possible, with flora and fauna living and behaving in these virtual worlds as they do in ours. As of 2018, the worldwide games industry was estimated to be worth around £100 billion. To put that into perspective, it’s 1.5 times bigger than the movie industry and five times bigger than the music industry, with one in three people on the planet being a gamer. Not bad for an industry that is only around 50 years old. Alongside the huge financial success of modern games is the ever-growing size of “open-world games”, in which players are free to explore vast and interactive virtual worlds. These virtual environments have gone from simple mono-block representations of landscapes to dynamic and interactive ecosystems. They have plants that can be foraged and a variety of wildlife that demonstrate complex AI-driven behaviour, interacting with the player and each other.  Within Red Dead Redemption 2, apex predators such as alligators lurk patiently underwater, anything (including other animals) in the game that venture too close to the water’s edge quickly meets its demise. Deer will also react to unseen predators, alerting the player to cougars lurking in nearby grass. Horses, one of the most important animals in the game, also react to other wildlife – bolting at the sign of a bear or hidden rattlesnake – demonstrating authentic animal intelligence. Guerrilla Games’s open world role playing game Horizon Zero Dawn features machine as well as organic “animals”. The machine animals in particular exhibit behaviours that don’t primarily rely on the player’s interaction. “Corrupted” machines will often attack their non-corrupted counterparts, with the player often coming across the bodies of dead machines, alluding to a dynamic world that exists outside the player’s attention.  The bodies of fallen machine animals, like in any real ecosystem, are not wasted. If not engaged in combat or roaming territory, “scrappers” (machines resembling hyenas), and “glinthawks” (giant vulture-type machines) will consume fallen machine animals they detect nearby – replicating decomposition and nutrient cycling.  Nintendo’s open world game Zelda: Breath of The Wild uses “virtual foraging” which is required to progress through the game. However just like the real world, players also need to be careful as flora and fauna can be easily over-foraged, forcing the player to  wait for stocks to replenish. All of this is more impressive when we consider that it has all been achieved in a single generation. Video games as a medium are relative newcomers – the industry only emerged in the 1970s. After the same length of time, films were still black and white. One can only wonder what gamers will be playing ten, 20 or even 50 years from now. Ecosystems in games are increasingly dynamic and “lived-in”, which opens the potential for education. Anna Groves, an American ecologist and gamer explained:  A kid who loves lighting the Hyrulian grassland on fire just might get excited about grassland restoration ecology when they find out it involves lighting real-life grasslands on fire. As games increasingly use ecology as a core gameplay feature, its value and relevance as a subject field will inevitably increase – exposing children to an academic subject in an accessible and enjoyable manner. Video games offer unparalleled creative freedom to explore subjects like ecology. Designers can create environments filled with long extinct species or pristine ecosystems that recreate how wilderness may have looked before human intervention. Children may “play” with imagined scenarios of the natural world in an intuitive, immersive and fun manner, far surpassing what is possible in traditional educational approaches.  As a result, they may gain a deeper appreciation of what natural states are possible through conservation than even a student engaging with depleted ecosystems in the real world could. With the advent of virtual ecology, video games are increasingly functioning as “conduits” to other disciplines. Landscape architecture and psychology are increasingly feeding into contemporary game design. In the future, disciplines such as engineering, geology and even medicine could start to inform the next generation of games.  When designing the worlds we play in, future game designers might increasingly be educated in “traditional” elements of landscape design, including ecology and architecture. With this also comes the opportunity for people in different fields to collaborate in shaping the worlds of future video games, radically reshaping both professions in the process."
"Coalition MPs and the Australian newspaper have accused the New Zealand government of hypocrisy over its objections to Australia’s use of what are known as carryover credits to meet climate targets under the Paris agreement. Are they right?  The Australian published comments by four Coalition backbenchers suggesting New Zealand’s climate change minister, James Shaw, was a hypocrite for saying there had been an “allergic reaction” in the international community to countries wanting to use an accounting measure to meet their 2030 climate target under the Paris agreement. There is no question Shaw was referring to Australia: it is the only country planning to use the measure – carryover credits claimed for having emitted less than previous self-set targets – under the Paris agreement. The backbenchers said New Zealand government was itself relying on carryover credits, in this case to meet its 2020 target under the Kyoto protocol. The newspaper followed up with an editorial on Wednesday headed “Jacinda Ardern’s climate policy virtue signal exposed”, saying among other things that New Zealand was not on track to meet the 2030 target it lodged in Paris. Jason Falinski, a moderate from New South Wales, said NZ had shown a “lack of consistency and standards” in criticising countries for using the credits. James Paterson, a Victorian senator and former staff member at climate change-denying Institute of Public Affairs, said it was appropriate for Australia to “reserve the right to get credit for its success in reducing emissions”. The former deputy prime minister Barnaby Joyce said New Zealand should not be lecturing Australia about environmental policy. Gerard Rennick, a Queensland senator who does not accept mainstream climate science, suggested Australia could reduce its emissions by deporting 600,000 New Zealanders. The Australian editorial adopted the language of the Morrison government, including claiming it was “working towards” meeting its 2030 target (a 26% to 28% cut below 2005 levels) “with or without Kyoto carry-over credits”, while New Zealand was forecast to miss its target. At one highly qualified level, yes: New Zealand says it will use 27.7m tonnes of carry-over credits to meet its 2020 target under the Kyoto protocol. Like Australia, it “beat” its target under the first stage of the Kyoto protocol, the precursor to the Paris agreement, which included targets for developed countries up to 2012. It means that, like Australia, it could carry those credits forward and count them against its target for the second stage of the Kyoto deal, which ends this year. Unlike Australia, New Zealand would need the credits to meet its 2020 Kyoto target (a 5% cut below 1990 levels). Australia will meet its 2020 target without them. Not really. But before we get to that, a couple of caveats. Firstly, New Zealand did not actually sign up to the second period of the Kyoto protocol. The former National party government opted not to make its 2020 target legally binding, but said it would still meet the goal using UN rules. Jacinda Ardern has focused on the Paris agreement, including committing the country to net zero emissions by 2050, with a lower target for methane from agriculture. Secondly, it is arguable whether Australia is actually meeting its 2020 target. The government usually describes its 2020 target as a 5% cut below 2000 levels. But, according to government projections, national emissions this year will be only 0.3% lower than in 2000. The reason that, despite this, it can still claim to be beating its 2020 target is explained here. New Zealand deserves criticism for not meeting its Kyoto target. But under the most important test – is what the countries are doing enough? – both Australia and New Zealand have failed under the Kyoto protocol. Both set inadequate targets based on what scientists say would be their fair share of emissions cuts would be under a meaningful climate deal. Both used creative accounting and loopholes to further limit action. New Zealand is not meeting its low 2020 target and assessments suggest it is not on track to meet its 2030 target (a 30% cut compared with 2005). But it recently passed a zero-carbon act for 2050 and is revising its emissions trading scheme. Australia is meeting its low target using accounting rules, but has not reduced its emissions below what they were 20 years ago. Despite what the government says, it has no evidence it it is on track to meet its 2030 target without using carryover credits. It has no long-term target or, at this point, new policies. Shaw is not being hypocritical in suggesting Australia shouldn’t use Kyoto carryover credits to meet its 2030 Paris target for one, simple reason: the agreement struck in the French capital is a completely different treaty to Kyoto. The rules of the Kyoto protocol are clear: countries can claim credit against future targets if they beat earlier targets. The Paris agreement does not mention carryover credits, and is legally separate to the Kyoto agreement. Credits from Kyoto have no formal status under it, and New Zealand has no plan to use them its 2030 target. The implied goal of the Paris deal is net zero emissions, which means boosting cuts over time, not finding ways to limit them. While the Morrison government claims otherwise, Australia’s attempt to use Kyoto credits under Paris has been widely criticised by other countries and remains a point of unresolved disagreement at UN negotiations. Laurence Tubiana, an architect of the Paris accord, told the Financial Times: “If you want this carryover, it is just cheating. Australia was willing, in a way, to destroy the whole system, because that is the way to destroy the whole Paris agreement.” Shaw’s claim that there has been an “allergic reaction” to Australia’s planned reliance on carry over credits might be colourful, but is not hypocritical nor inaccurate. With at least 100 countries signed up to groups that oppose them, it seems a pretty reasonable call. Bill Hare, from policy and science institute Climate Analytics, puts it more bluntly. He says New Zealand should be applauded for rejecting the use of carryover in the Paris deal, and the hypocrites in this exchange are the Australian politicians who profess to be concerned about climate change while promoting policies, including the use of carryover credits, that will result in higher emissions."
"

From the ""you’ve GOT to be freaking kidding me"" department:
Dell’s Virtual Plant a Tree for Me program into the computer game Second Life has many tech savvy people wondering if this represents a new low in Earth Day marketing tie-ins. It looks like in the rush to pander to green-ness, some Dell executives maybe didn’t think beyond the boardroom door.
You may wonder, too, after reading Dell’s invitation to its Earth Day Party at Dell Island in the Second Life game  where they say proudly “get your own tree sapling to plant in Second Life!”.
Yes that’s’ right, you can plant a virtual tree in a video game for Earth Day. And, Dell is only too happy to take a couple bucks from you in the process as well for their real tree planting program designed to assuage your guilt at using a computer that uses electricity.
You have to wonder just how hypocritically lazy some people might be to take this offer seriously, though with 5.7 million ""residents"" in the Second Life game, I suppose its hard to deny that this offer would have an impact.
Just how much electricity is used by PC’s in pursuing this pointless exploit in ""green-ness""? And with Dell soliciting and online Earth Day Party, that will tie up PC’s, routers, and Servers nationwide, using even more electricity. There’s no mention in Dell’s press release of the expected carbon footprint on this bogus promotion. Maybe Gore will fly in on his private jet to make a “virtual appearance” to preach to the faithful.
But since some people nowadays seem incapable of disconnecting themselves from the virtual world of gaming, it stands to reason that a virtual eco-delusional activity might very well appear valid to them.
Maybe next the researchers at Berkeley can tap into the seti@home background processing idea and instead of searching for intelligent life in radio-telescope signals, we could program our wasted CPU cycles to grow virtual trees on a screen-saver. It could boast onscreen counts of virtual carbon sequestered, and virtual O2 produced. I can smell the virtual fresh air already!
We’re doomed.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea705a03b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"
An astute letter to the editor writer in Arkansas has found the reason for global warming:

This actually happened, as attested to on the rumor/urban legend verifcation website Snopes.com


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6b2c1e4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"

You may recall that back in May I did a simple preliminary experiment to give me guidance on a hypothesis: That changes in paint on Stevenson Screens over time make a measurable difference on the temperatures recorded inside them. This stems from the fact that when the Weather Bureau commissioned the design in 1892, whitewash was specified. But whitewash is no longer commonly available, and the National Weather Service changed the specification in 1979 to be semi-gloss latex paint.
But, cured whitewash is composed of Calcium Carbonate, while latex paint uses Titanium Dioxide as a pigment. While they both appear “white” in visible light, they have vastly different properties in infrared.
My first simple experiment used thermistors in boreholes into 3 wood slats; 1 bare wood as a control, the other two painted with whitewash and latex, showed me that there was a measurable difference in the temperature of the wood by as much as 2-4 degrees at times. I needed to do that experiment before I embarked on the full scale test, since each of the Stevenson Screens you see here in the pictures cost me about $1000.00 Since I’m doing this out of pocket, with no funding or grants, I had to try a small scale test first.
The photos show 3 standard Stevenson Screens as used today in the United States. One is bare wood, unpainted, as a control, the middle one is latex, as sent from the supplier, and the third is painted with a historically accurate (for early 20th century) whitewash mixture that I obtained both materials and formula from the head chemist at the National Lime Company.

The device on the tripod is a stacked plate IR shield with a small fan to pull air through, commonly called an aspirated shield. It is the air temperature reference and placed at the same exposure height as the thermistiors in the screens. Also nearby but not shown is a pyranometer to measure solar insolation and wind speed/direction sensors that are being datalogged as well.
Each Stevenson Screen and the air temperature reference sensor are fitted with matched, calibrated thermistors, NIST traceable with certificates, that are connected to a calibrated data-logger, also with a certificate. The resolution is .01 degree Fahrenheit with an accuracy of +/- 0.1 degree over the range.
I expect that the air temperature differences inside the screens will be less than the 2-4 degrees I observed in the paint slat test. It’s possible that there will be no significant difference at all. I won”t know until I run about a months worth of datalogging.
The site, while not ideal due to the trees, is the best I could get permission to use.  Fortunately the trees do not directly shade the screens except for a short portion of the day. It’s also out of the way, so vandalism will not be likely. Since it had to be an unwatered grass field, concerns over fire danger were raised from some I asked because of the electronics package, so I had limited choices. Perhaps later I’ll be able to find a better site but for now it will have to do.
The whitewash on the third Stevenson Screen is still curing, as the chemical reaction is not yet complete to convert Calcium Hydroxide to Calcium Carbonate. In about a week, I’ll make the data available via a web link in near real-time.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea536d1ac',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
One of the really odd discoveries that I’ve made while surveying climate monitoring stations around the USA is the fact that many of the official stations are located at sewage treatment plants. For example, the one in Colusa, CA is at their sewage treatment plant. I’ve visited it.
A couple of volunteers for www.surfacestations.org have been going around Washington and Oregon locating stations there and have also reported a number of stations at waste-water treatment facilities. I’ll get to why locating a temperature monitoring station at these facilities is a really bad idea later, but first I want to tell you why many of them are located at these places.
It has to do with the fact that somebody must read the thermometer once a day, write down the max and min temperatures for the last 24 hours in a logbook, then send in the page of the logbook to the National Climatic Data Center (NCDC) once a month. When stations were assigned to cities, they needed to locate them at a place where there was somebody 7 days a week. Sewage is a 24/7 operation. Police and fire stations have some stations for the same reason, somebody is always there.
Ok this picture comes in today from from surfacestations.org volunteer Steve Tiemeier, who visited the climate station of record located at the Urbana, Ohio Waste Water Treatment Plant:

The small item in the center of the picture labeled “MMTS” is the temperature sensor that is used to submit monthly climate reports to NCDC.
Now in case you don’t see some of the obvious problems with this location and why its a terrible place to measure temperature, I’ll list them one by one:
– Sensor is attached to the building, just mere inches away from brickwork
– Sensor is near windows, which radiate heat from heated interior rooms in winter
– Sensor is directly above effluent grates for waste-water, Waste-water is often warmer than the air many months of the year
– Sensor is between three buildings, restricting wind flow
– Sensor is between three buildings, acting as a corner reflector for infrared
– Several exhaust fans near sensor, even though one is disable, there are two more on the walls (silver domes)
– Air conditioner within 35 feet of sensor, enclosed area will tend to trap the exhaust air near sensor
– Sensor is directly over concrete slab
– Refrigeration unit nearby, exhausts air into the enclosed area
– Shadows of all buildings create a valley effect related to sunlight at certain times
– There are two nearby digester pools, which release heat and humidity in the sensor vicinity
– Heat and humidity plume over the site from digesters is often tens of degrees warmer than the air in the wintertime
Here is wider view that shows the temperature sensor in relation to the digester tank:

More picture on my image database here: http://gallery.surfacestations.org/main.php?g2_itemId=5322
I don’t know if any readers of this blog have ever driven by a sewage treatment plant in the winter, in the midwest, as I have, but I can tell you from experience it looks like a hot springs with steam rising into the air.
Talk about your urban heat island effect…not only that, sewage treatment plants effluent volume is a direct indicator of population growth. So as more water is treated, more local effects from the heat/humidity plume occur, which can affect the temperature readings.
There are dozens, possibly hundreds of USHCN climate monitoring stations sited at sewage treatment plants around the USA. I’ll have more reports on this in the future.
Who knew? I’ve been working in meteorology 25 years and I didn’t until this week.
here are some other stations at a sewage treatment plants:
http://gallery.surfacestations.org/main.php?g2_itemId=1489
http://gallery.surfacestations.org/main.php?g2_itemId=4658
http://gallery.surfacestations.org/main.php?g2_itemId=4388


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5d45db9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Polychaetes are a class of segmented worms that live under a wide range of oceanic conditions. Often, they are the dominant organisms found living in the sea floor, but they also thrive in the open ocean. According to Ricevuto _et al_. (2015), although knowledge of the potential response of these organisms to ocean acidification is growing, much remains to be learned, including “how their trophic behavior might change in response to low [less basic, or more acidic] pH.” In an effort to fill this informational void, Ricevuto _et al_. thus set out to examine food-chain interactions of three polychaete species ( _Platynereis dumerilii_ , _Polyophthalmus pictus_ and _Syllis prolifera_ ) and their organic matter (food) sources (macroalgae, seagrass and epiphytes) in a naturally acidified region of the Mediterranean Sea.   




The location for their study was a shallow water reef area on the north-eastern coast of Ischia, an island off the coast of Italy known for volcanic features, including underwater vents that release copious quantities of CO2. The vents produce a pH gradient in the area that provides “a natural laboratory for ocean acidification studies,” which the researchers further describe as “an ideal model system to conduct experiments investigating the effect of climate changes (particularly ocean acidification) on benthic community composition and structure, as well as on functional aspects, such as tropic interactions,” which was the focus of this study. And what did the study show?   
  
After collecting data and conducting a series of complex analyses, the three Italian researchers report “increased pCO2 did not alter the trophic interactions dramatically,” adding “there seems to be a resilience in the trophic pattern, possibly due to the tolerance of the target species to acidification and potential local acclimatization and/or adaptation (see Calosi _et al_., 2013).” Such “phenotypic plasticity” (the ability to alter biochemical reactions based on environmental changes such as increasing temperature or acidity) observed in the three polychaete species studied, according to Ricevuto _et al_., “may allow them to respond well to alterations in the environment and eventually offset near-future ocean acidification scenarios.” Thus, as the researchers ultimately conclude, “for some species, like the ones considered in this study, ocean acidification may not represent a dramatic stress.” And that’s good news worth reporting.   
  
  
  
**References**   
  
Calosi, P., Rastrick, S.P.S., Lombardi, C., de Guzman, H.J., Davidson, L., Jahnke, M., Giangrande, A., Hardege, J.D., Schulze, A., Spicer, J.I. and Gambi, M.C. 2013. Adaptation and acclimatization to ocean acidification in marine ectotherms: an _in situ_ transplant experiment with polychaetes at a shallow CO2 vent system. _Philosophical Transactions of the Royal Society of London B Biological Sciences_ **368** : 20120444.   
  
Ricevuto, E., Vizzini, S. and Gambi, M.C. 2015. Ocean acidification effects on stable isotope signatures and trophic interactions of polychaete consumers and organic matter sources at a CO2 shallow vent system. _Journal of Experimental Marine Biology and Ecology_ **468** : 105-117.


"
"Britain’s energy regulator has said it will change how it governs the industry to help meet the government’s climate targets, after coming under fire for failing to prioritise the climate emergency. The regulator published a wide-ranging climate action plan on Monday, which aims to help get 10m electric vehicles on our roads by 2030 and support a fourfold increase in offshore wind generation, while protecting homes from rising energy bills. The nine-point manifesto also includes plans to support low-carbon home heating, tariffs that encourage homes to help balance the energy system, and a crackdown on “greenwash” energy deals. Ofgem’s incoming chief executive, Jonathan Brearley, set out the regulator’s climate manifesto after critics warned that its outdated statutory duties were not aligned with the government’s climate policies. Ofgem was set up to regulate energy companies and safeguard consumer interests, often against price increases. The regulator admitted that it faces trade-offs between supporting ambitious green investments – paid for through energy bills – and the need to protect homes from rising costs. The Guardian reported last year that Britain’s biggest business group, the CBI, was concerned that the regulator’s existing mandate sent “negative signals” to low-carbon investors. The CBI called for a legal change to the regulator’s statutory duties to directly prioritise tackling the climate crisis. On his first day as the regulator’s new boss, Brearley signalled that Ofgem plans to balance the tension between green investments and energy bills without a legal overhaul. “We are taking an approach that recognises that our role protecting consumers includes achieving net zero,” he said. Gillian Guy, the chief executive of Citizens Advice, said: “Ofgem has set out the right challenges, it now needs to deliver on them. If we don’t get the difficult decisions about the low-carbon transition right, it will ultimately be those who can least afford it who end up hardest hit.” The first point in Ofgem’s new climate manifesto will change how the regulator controls the spending of energy companies, in order to avoid stifling investment in low-carbon technologies with a rigid eight-year spending plan. The regulator came under fire from the National Audit Office last week for allowing energy networks to rake in bigger than expected profits by not being tough enough in the price-control plans. Under Ofgem’s new plan, network companies may have the chance to ask for their spending plans to change during set windows within the eight-year period, so that they can adapt their green investment plans as the sector evolves. The change appears to answer fierce criticism from the boss of Scottish Power, Keith Anderson, who last year accused the regulator of hindering the UK’s electric vehicle rollout due to its “colossal disconnect” with Britain’s climate policies. Scottish Power had hoped to make extra investments in car-charging in anticipation of an electric vehicle boom, but Ofgem said the plan would not offer good value for money if the demand failed to materialise. Brearley said: “As low-carbon renewable energy grows and more transport goes electric, the energy system needs to be more flexible to respond to peaks and troughs in both supply and demand. Our new price controls for network companies will clear the path for this, providing the incentives for investment for the future.” Ofgem will also create a fund specifically to unlock investment in innovation on tackling greenhouse gas emissions, and help find ways to encourage homes to use their car batteries to help balance the energy system and use more renewable energy when it is available. Nicola Shaw, the UK boss of National Grid, said: “It’s critical that the regulator, government and industry are aligned to decarbonise the energy sector in the journey to net zero at the lowest cost to consumers, and we both welcome and share Ofgem’s commitment to achieving this.” Make price controls more adaptable to help firms invest in clean energy. Set up a regulatory fund to help invest in climate-change solutions. Explore ways to create a “lowest cost” offshore grid to support wind power. Work with government and industry to decarbonise heating. Make UK energy systems fit for a net-zero future. Create a more flexible electricity system to help move towards net zero. Develop a regulatory strategy to help get 10m electric cars on the road by 2030. Support energy firms to create low-carbon products and services for consumers. Change its regulatory approach and take “big decisions” on decarbonisation faster."
"It is 2020, the world is on fire, and some of us are in the midst of a gripping debate about exactly what we should have in our cereal. As we learn about the environmental impacts of mass cattle farming, the future is looking bleak for the beef and dairy lobbies. US citizens need to cut milk by 60% and beef by 90% to prevent global heating changes, according to researchers. If dairy’s out, then, what milk is best for the world? You can now get milk from soy beans, cashews or tiger nuts - just to name a few. All are far better for the planet than dairy, according to a 2018 study. (The Guardian’s own analysis of vegan options was published this week.) But the dairy lobby is not going down without a fight, going as far as demanding that milk alternatives cannot be called “milk”. Today, Virginia’s legislature has proposed to make alternative milks parading as real milk “unlawful”. What is real milk, you ask? According to them, it is “the lacteal secretion of a healthy, hooved mammal.” (Yum!) This has led to existential questions such as “do we have to change the name of peanut butter?” and “what about breast-milk?” There is, in fact, an exception in the bill that accepts that breast-milk is the real deal, but the fact that a loophole needs to exist does make you wonder what this whole thing is really about. In the words of Shakespeare, “a rose by any other name would smell as sweet.” Perhaps that’s the problem. This article was amended on 31 January 2020 to correct that the Virginia legislature has proposed the change, but it has not been signed off by the Senate and governor. "
"A recent online video took what seemed like an inspirational moment viral. The video, shot by Dmitry Kedrov using a drone, shows a baby bear climbing up and falling down the side of a mountain near Russia’s Sea of Okhotsk. After repeated efforts, the cub finally reaches the top, joining his mother and winning the hearts of viewers around the world. But after the initial enthusiasm for the video came some controversy when scientists pointed out that the incident may have been caused by the drone risking the cub’s life by interrupting its efforts to climb to safety. This prompted some online commenters to call for drones to be banned on grounds of environmental impact, while others defended the responsible use of this technology. My colleagues and I have been researching the impact of drones on wildlife, and found that they pose very similar kinds of threats as other disturbances such as people, cars and conventional aircraft. This suggests that rules and guidelines that took animals into account would make a big difference to how much harm remotely and autonomously controlled aircraft could cause to wildlife through their noise and visual presence. When animals come into contact with drones, they may experience physiological changes such as an increased heart rate, behavioural responses such as running or flying away, or even suffer stress that could disrupt their reproductive process. If they decide to avoid specific areas as a result of frequent disturbing drone encounters, this could fragment and ultimately damage the whole population.  Unfortunately, there is no reliable indicator that can give us an idea of the extent to which these flights are affecting wildlife. But this does not mean that there is no need to worry, because drone use is expected to increase in coming years. Exactly how serious the threat from drones is depends on how often and how intensely they disturb the animals. If they are frequently disturbed, the animals will likely abandon the area, but they could also eventually become used to the drones. At worst, if drones fly too close to animals, collisions or attacks can cause wounds or death. Also, not all animal species nor individuals react to drones in the same way, and they may be more vulnerable in certain moments, such as breeding season, or in areas without protection or escape routes. With all this in mind, drone operators should try to minimise the impact they have on wildlife. To start with, they should consider why they want to fly into or near an animal’s habitat and whether they really need to. When scientific projects are planned, they have to be approved by ethical committees and the potential disturbance has to be justified by the interest of the project. Risking the life of an animal to create a popular online video is unacceptable. But if you are trying to gather data for a conservation project, a small disturbance to wildlife in order to protect it can be justified, and drones may be the least impactful way to do it. After all, any method of gathering animal data involves a certain degree of disturbance. Cars and manned aircraft are substantially more noticeable and noisier than drones. Monitoring a bird breeding colony on foot causes considerable chaos. Trapping animals involves considerable risks and equipping them with sensors or GPS can harm them. If using a drone is the best option, it’s best to minimise the risk of disturbance and accidents by using an experienced pilot and a reliable, small and low-noise drone that doesn’t resemble the shape of a predator. Missions should be as short and at the highest altitude possible, using a regular, back-and-forth flight pattern over the animals and not complicated manoeuvres directed towards them. If it’s convenient, drones should take off and land at least 100 metres away from the animals and avoid disturbing them during breeding periods and at times of day when they may be most vulnerable. It’s also important to monitor the target animals during flight so you can check if they are being disturbed and abort the mission if necessary. You should also avoid protected areas, flying over sensitive species or abundant wildlife. Flying drones around animals requires basic knowledge and respect for wildlife. But the reality is that drone users without a wildlife background may not be aware that they are flying into a raptor breeding territory or that the drone noise may disturb animals on the ground. We encourage drone manufacturers to help by including this kind of basic advice with the instructions that come with the drones.   But what about people who choose not to follow these kind of guidelines? I think we need a legal framework so that appropriate actions can be taken when wildlife is negatively affected by irresponsible drone operators. Then perhaps people won’t be so keen to risk disturbing animals for the sake of YouTube views."
"

Dr. Gavin Schmidt, a lead researcher with NASA’s Goddard Institute for Space Studies (GISS) that does leading climate change studies, replied to one of my posts and made an assertion that the USHCN and GHCN stations and station data being discussed here in my blog are not used in validating climate models. This is surprising to me.
Here is the full correspondence:
Schmidt’s first post:
> Don’t let me get in the way of your efforts here, but please stop saying that “This data is in fact used in climate modeling to predict our climate future”.
>
> This is simply not so.
>
> You’ve downloaded the GISS model – perhaps you’d like to show me where these station data are used? You won’t be able to because they aren’t.
>
> Observational data at large scale (not individual stations) are used to evaluate the models after they’ve been run – but again generally only at the continental scale and above. The evaluation is not just with trends but
> with patterns of variability (El Nino responses, NAO etc.) and obviously, the better the data the more reliable the evaluation.
>
> Note that the climate model hindcasts for this area are around 0.5 over the 20th Century – significantly less than this individual station. Should this record therefore be shown to contaminated, it would actually improve our confidence in the models, not lessen it!
I responded to this on June 21st 2007 as follows:
> Gavin,
>
> I thank you for commenting on my blog, Watts Up with That? I’m honored
> that you would take the time. Rather than reply immediately, I thought
> I’d give some thought and research to my response, hence the delay. I
> also thought you’d appreciate a direct reply rather than a blog post.
>
> You wrote on the blog:
>
> “You’ve downloaded the GISS model – perhaps you’d like to show me where
> these station data are used? You won’t be able to because they aren’t.”
>
> I did some looking at a paper you authored, I found Schmidt et al 2006,
> from BAMS, which is also posted on your website:
> http://pubs.giss.nasa.gov/docs/2006/2006_Schmidt_etal_1.pdf
>
> You wrote on page 168 of the BAMS article:
>
> “We endeavor to compare the model simulations to as many suitable
> datasets as possible. … . Where useful gridded datasets exist of
> selected in situ data we use those.”
>
> After reading through your paper, I agree that you did not show any
> comparisons to GISS gridded data and I will withdraw any implication
> that you used GISS station data. However, I must say that I’m surprised
> to learn that GISS gridded data did not meet the standards of Schmidt et
> al 2006 of being either “useful” or “suitable”. Thank you for drawing
> this to my attention.
>
> However, later in the article, on page 176, you show comparisons of
> model output to CRU surface temperature data on two occasions:
>
> “Surface air temperatures (SATs; Fig. 17) show a general warm
> continental bias in comparison to the updated Climate Research Unit
> (CRU) data (Jones et al. 1999).
>
> Figure 23 on page 187 shows Taylor diagram comparisons among the
> selected models for the December-February (DJF) and June-July (JJA)
> extratropical NH CRU surface air temperature (SAT)”
>
> It is my understanding that CRU uses GHCN station data, which includes
> the USHCN sites discussed here in my blog. So, my answer to your
> question is that Figures 17 and 23 of Schmidt 2006 et al use the station
> data discussed here via the CRU gridded data. It has always been my
> understanding that adjusted GHCN and USHCN surface station data (also
> listed on the GISS webpage) including the ones I show plots of, is
> applied to a gridded data scheme for use in the computer models, such as
> model E. If I am in error in that assumption, I welcome you pointing out
> that error.
>
> If you felt that I was speaking of a specific station data being “used
> to predict our climate future” that of course is not my intent. If that
> was the case, I’ll revise the wording to make it clearer.
>
> Regarding your mention that “contamination of station data would improve
> your confidence in your model”, I must say that I’m a bit surprised at
> this. I’m not really in a position to dispute this yet, but would
> appreciate some additional clarification as why you are so certain of
> this without even seeing the impact of contaminated data. I surmise the
> opposite to be true, but I welcome further understanding.
>
> Again I thank you for your comments, and I welcome any correspondence or
> suggestions you may have.
>
> Best regards,
> Anthony Watts
Dr. Gavin Schmidt replied on June 22nd, 2007 with:
My comments stand. The station data are not used *in* climate models, and
they are not used to predict future climate. So yes, the sentence you have
is just wrong. I’m not sure how you could edit it to make it correct.
We compare the models to the gridded products that deal with individual
station problems as best they can. We have used the GISTEMP and CRU
products to do so. (Semantic note, ‘compare to’ is not the same as
‘include in’). For the specific station you have highlighted, the grid
point trends in the products (~0.5 deg – eveballing it, since I’m on
travel) are significantly less than the trend you show (2 deg or so).
Climate model results for the 20th C are similar (i.e. 0.5 deg). Thus
reductions of the trend at this station would actually improve the match
to the model – always being clear that you shouldn’t really compare
model grid boxes to individual stations…
If you are of the opinion that this station is contaminated, then you have
to admit that the process designed to remove artefacts in the GISS or CRU
products has in fact done so – (i.e. that grid box in the product does not
have a 2 deg/Century trend).
Improvements to that process and the data are always welcome, but do not
ascribe consequences to your project that clearly do not follow.
Gavin
*——————————————————————–*
| Gavin Schmidt NASA/Goddard Institute for Space Studies |
| 2880 Broadway |
| Tel: (212) xxx-xxxx New York, NY 10025 |
…| |
*——————————————————————–*
[email address and tel# removed by Anthony for privacy/spam purposes]
So one has to wonder.
If Dr. Schmidt’s point is only the observation that they do not reconcile their models with every individual station (as opposed to gridcell composites calculated by GISS and CRU), then there is no misunderstanding.
However, it is very clear that the NASA GISS and CRU (Climate Research Unit) use this station data in arriving at their gridcell values which are what is presumably used in testing the models.  From 53 USHCN site surveys done so far we know that a number of stations do not meet published WMO (World Meteorological or NOAA (National Oceanic and Atmospheric Administration published standards.
There is no evidence at present that NASA GISS or CRU have made any effort to verify quality control standards at these USHCN stations. Whether these quality control issues will have a significant impact on overall averages remains to be seen.  The only way to tell for certain is by examining individual stations though the site survey process as is being done on www.surfacestations.org and then doing an assessment of how pervasive the quality control problems are and what the potential impact of these problems may be.
But, any problems in individual USHCN stations will affect gridcell values. For non meteorologists, a gridcell is a box on a map that has been divided up into a x-y  lines and specific data applied to each box. This helps in computer modeling because with computer programs it is easier to divide into cells, then calculate and display. Below is an example  map that may help you visualize gridcells:

Whether it’s a big problem or a little problem remains to be seen, but it’s odd for Dr. Schmidt to pretend that it’s not a problem because they use the gridded version of the data.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5f332f6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Stephanie Kelton’s _The Deficit Myth_ is quite the talk of the town. To quote Amazon’s webpage:



It’s an attractive vision, but it doesn’t work.



I am reminded of Einstein’s time at the Swiss Patent Office where he used to check applications to patent perpetual motion machines. They don’t work, but the fun is working out why. The same applies to proposals to bring about prosperity that depend on loosening the monetary spigots. MMT is a perfect example.



MMT is a macroeconomic school of thought in the post‐​Keynesian tradition. Its central tenets: fiscal deficits don’t matter; monetary policy should be subordinate to fiscal policy; and the monetary authorities should be willing to issue base money to finance government spending. MMT is associated with large‐​scale government spending, a focus on ending involuntary unemployment, and programs to alleviate poverty and fight climate change.



Kelton’s book builds on earlier work by Warren Mosler and Randall Wray but has its roots in Abba Lerner’s system of “functional finance,” which goes back to the 1960s. She builds on Lerner primarily by adding a federal job guarantee that would eliminate involuntary unemployment and provide an automatic economic stabilizer.



MMT makes _big_ promises. It would “build a more just economy that works for the many and not just the few” and put “people and planet first.” “MMT’s lens enables us to see that another kind of society is possible, one in which we can afford to invest in health care, education, and resilient infrastructure. In contrast to narratives of scarcity, MMT promotes a narrative of opportunity.”



But does MMT deliver? Let’s see what she says.



“The idea that taxes pay for what the government spends is pure fantasy,” writes Kelton. Really? Let’s go back to basics. The government must finance all its expenditures. In a world in which it does not issue debt and does not issue currency, and assuming away any gifts it might receive, all its expenditures must be financed by current taxation.



If the government can issue debt but not issue currency, then it can finance its expenditures by current taxation or by issuing debt. But to issue debt is to pass on the obligation to repay that debt to future taxpayers. If that debt is to be repaid, then it must be repaid out of future tax proceeds.



If the government can issue its own currency and monopolizes the issuance of currency, then it can also pay off its debt obligations as they come due by issuing additional base money (“printing money”). Does this mean that printing money allows the government to avoid the need to raise taxes? No, because printing money lowers its value against goods and services, and so operates as a tax on money holdings and other holdings of wealth that are fixed in nominal terms (such as level annuities). So, barring gifts, all government expenditures must be financed by taxation in one form or another.



Kelton explains:



This sounds great: involuntary unemployment eliminated and everyone willing to work gets a high federal minimum wage or more, to the extent that market wages are forced higher to compete. But hold on. If it is such a good idea, why not raise the minimum wage beyond the $15 an hour she suggests? Why not $30 an hour? Or $50? The problem is that there are a raft of jobs that are profitable to provide at existing wages but would disappear at higher wages.1 It is not just the existing unemployed who would end up on federal payrolls but these newly unemployed too, and many of their employers. Think of the restaurant sector. That sector and others in the same position could only survive by hiking their prices: dining out would become a lot dearer. Ordering in, too. The federal government, the employer of last resort, would find itself with the problem of what to do with all these people also turning up for guaranteed jobs. The feds would have crowded out much of the labor market and wiped out the lower paid sectors of the economy.



“Why does the financing have to come from Uncle Sam?” asks Kelton. “Simple. He can’t run out of money.” Imagine that the government pays debts coming due by handing over dollar bills that it has in a chest in the basement. If it runs out of bills, then it will default the next time a payment comes due. But then imagine if it can also print money. If it runs out of bills, it can avoid default when the next payment comes due by printing more. It does not follow, however, that the government can _always_ meet its payment obligations by printing more money.



Suppose the government prints money at an accelerating rate and we end up with an accelerating hyperinflation. The traditional tax‐​collection apparatus will break down because the tax revenue will be worth almost nothing by the time it comes in. Similarly, the government will effectively be unable to borrow in its own currency because the borrowed funds would also be worth next to nothing by the time they come in. As the hyperinflation accelerates further, the real value of the revenue from printing money also goes to zero. The government then faces the prospect of default despite being able to print any amount of its own money. To give an example, by the end of the Hungarian hyperinflation of 1946, the total value of all Hungarian notes in circulation was a thousandth of a U.S. cent (see Judt 2006: 87). The Hungarian government didn’t have a cent, let alone a dime. The Hungarian government would have been unable to make repayments denominated in other currencies or make inflation‐​linked payments in its own.



The mistake is to presume that what is correct at the margin (i.e., that the government can avoid default by issuing a few extra dollar bills) is also correct under any circumstances, that is, at any scale. There is also the related point that issuing a small amount of money will have a negligible impact on prices but issuing a lot of money will not.



It is a “myth,” writes Kelton, “that deficits will burden the next generation.” This claim is also wrong. Suppose Congress passes a Boomers Boomtime Act to provide for a humongous 75th birthday payout to each surviving member of the first Boomer cohort born in 1946. They will reach 75 in 2021. These payments are to be financed by a zero‐​coupon bond with a 40‐​year maturity. Since none of the beneficiaries will be around to pay taxes when the bond is due to be repaid, they get a free handout.



Who bears the burden of paying for it? When the Boomer bond comes due in 2061, the government faces the following choices: (a) pay it off by raising taxes, (b) pay it off by issuing money, (c) default, (d) pay it off by rolling over, that is, by issuing a new bond.



If (a), then the burden is borne by taxpayers in 2016.



If (b), the subsequent price level is higher, so the burden takes the form of a tax on money holdings and other instruments of fixed nominal value.



If (c), default, the burden is borne by those who suffer the adverse consequences of default.



If (d), then the rollover will mean that there will more debt after 2061 than there would otherwise have been and we have the same choices again when the new payments come due. If the decision is to roll over each time, then the debt/​GDP ratio will hit a level at which the government defaults sooner than otherwise.2 Thus, however the government responds when the Boomer bond matures, some group born after 1946 bears a burden from it.



More generally, any arrangement that involves one group issuing a debt that another group is expected to pay for _necessarily_ burdens the second group. The injustice is all the worse because the second group has no say in the matter.



There are also the government’s entitlement programs, Social Security, Medicaid, etc. This takes us to Kelton’s “myth” that “entitlements are propelling us toward a long‐​term fiscal crisis.… There is absolutely no good reason for Social Security benefits, for example, to ever face cuts. Our government will always be able to meet future obligations because it can never run out of money.”



These programs however are just another form of debt insofar as they create obligations on the government’s part to make future payments. Consequently, my earlier argument, that programs that create future obligations burden future generations, applies here also.



Entitlements are large, so the corresponding burdens would be large as well. To illustrate, there are perhaps $210 trillion in entitlements, and possibly more.3 If these entitlements are to be paid for by future taxation, then that is a lot of future taxation. If they are to be paid for by rolling over, then we would anticipate the ratio of debt (including entitlements) to GDP rising considerably, possibly to default levels.



Then there is the option of meeting those obligations by printing money. Given that the current stock of base money is just over $5 trillion, that response implies a possible 42‐​fold‐​plus expansion of the monetary base. That, in turn, implies a considerable increase in prices. Making entitlement payments is one thing, but the purchasing power of those payments is another.



We have here another instance of the margin vs. scale issue. The government can increase entitlements a little with next to no impact on their real value. But if the government creates huge entitlements to be financed by printing money, then those entitlements are going to be greatly devalued in purchasing power terms. And what the government must absolutely _not_ do is create huge entitlements that are inflation‐​linked and then rely on printing money to finance them. If it does that, it will produce both hyperinflation and default. The “myth,” that the national debt is a problem, is not a myth.



We can break down MMT into a set of policy _ends_ (what the government spends _on_ ), and a set of policy _means_ (how the government finances its spending), which in the case of MMT would involve large deficits and a lot of borrowing and money printing. The expenditure and the financing of that expenditure are two different issues. Bernie Sanders might use MMT to advance a more right‐​wing version of the Kelton agenda, but Donald Trump might seize upon the spending opportunities promised by MMT to advance his own, even more right‐​wing, agenda, for example, to promote policies that work for the few and not the many.



My point is that it is short‐​sighted and potentially counterproductive to promote a particular policy package such as MMT because _you_ can use it to finance projects that _you_ like, because someone else might use it to finance projects that you do _not_ like. To the extent that MMTers persuade people that MMT‐​based government finance is a good idea, they can hardly restrict that message to people who share their own political views. If you think of MMT as a government‐​financing package, then that financing package can be used to finance any government spending program, whatever its political hue.



A deeper issue is that any policy that gives policymakers the appearance of being able to spend a lot without having to bear the unpopularity of the high taxes needed to finance that spending is a dangerous one and invites abuse. Any such policy entails a major shift in power away from the legislative branch to the executive branch, because it gives the latter additional means of finance that bypass constitutional constraints against government overspending. To elaborate, the Constitution says that fiscal policy, the power to tax and spend, is constrained by the need to obtain congressional approval. If there were no Fed, or if the Fed were genuinely independent, then Congress could deny appropriations for spending projects of which it does not approve. But if the executive branch has the power to print money, then it has a potential means to circumvent Congress. If Trump wants his wall and Congress denies the appropriations, he can then order the Treasury secretary to print the necessary money instead. From this perspective, MMT is something of a constitutional abomination.4



It is a fundamental principle of constitutional political economy that economic policymakers operate under rules that constrain the decisions they make, and that these rules should be designed in ways that prevent undesirable behavior on their part. Under this way of thinking, the rules operate as bulwarks that constrain policy makers in order to protect everyone else from the misuse of the powers entrusted to those policymakers.



For proponents of MMT and for many other advocates of big government, however, those rules serve no real purpose and merely constrain policymakers from achieving the lofty ends that they seek to pursue. Yet they fail to appreciate that lofty ends do not justify giving those in power unrestrained discretionary powers. As Adam Smith observed:



Should MMT be adopted, then the Fed would become subservient to the Department of the Treasury, but in a more nakedly obvious way than it was during the years before the 1951 Treasury‐​Fed Accord, and without operating under the constraints of the Bretton Woods watered‐​down gold standard. Personally, I would suggest that, if MMT were adopted, then the government should make the new monetary policy arrangements transparently obvious. The Fed’s independence, such as it was, would be history and there would be no point pretending otherwise. The government should nationalize the Fed and make it a division of the Treasury, whose responsibilities would then be to issue currency and manage the national debt. Nationalizing the Fed would highlight the underlying chartalism of MMT (i.e., the idea that money is a creature of the state),5 and would also simplify analysis going forward because it would cut out the need to consider Fed/​Treasury interactions that only mask the underlying reality. We could then talk openly about the _government printing money_. If the United States is going to embark on a monetary policy worthy of a banana republic, then it should look the part.



Suppose then that the government goes full throttle MMT à la Kelton. This spending must be financed, however, and the government would have to do so by some combination of levying taxes, borrowing, and printing money. In essence, she proposes high government spending and a big deficit financed by borrowing (“debt finance”) and printing money (“monetary finance”).



If such a policy were launched at a time when there is considerable unemployment, then one would suppose that unemployment would fall. But there must eventually come a time when the economy returns to more or less “full” employment, whether because of those policies or despite them being a separate question. What happens then? To examine this question, I built a model along “unpleasant monetarist arithmetic” (see Sargent and Wallace 1981) lines and got some interesting results.



Let’s consider the following three possible MMT policies: (1) the government pursues pure debt finance; (2) the government pursues pure monetary finance; and (3) the government pursues debt finance for as long as possible, up to the point where it is about to default, and then switches to monetary finance.



Let _d_ be the long‐​term growth rate of the deficit and _g_ the long‐​term rate of economic growth. It is reasonable to suppose that MMTers would want _d_ considerably in excess of _g_ , so let us assume that this is so. Under a policy of pure debt finance, the debt/​GDP ratio would grow relentlessly and must at some point reach a level at which the government defaults. Hence, debt finance makes government default inevitable if pursued for long enough—that is, pure debt finance is unsustainable.



Under a policy of pure monetary finance, _d_ becomes the key driver of the inflation rate. If _d_ is steady in the long‐​term, then the inflation rate will converge to a long‐​term steady state. But if _d_ itself grows, so the deficit grows at an _accelerating_ rate, then long‐​term inflation will also accelerate.



Under the third policy, the debt/​GDP ratio rackets up to the brink of default, then the government switches to monetary finance with similar long‐​term outcomes as pure monetary finance.



It is important to emphasize that under monetary finance there is no way in which the government can simultaneously pursue an inflation target. Instead, the inflation rate becomes a residual outcome from the government’s fiscal policy and the government loses all control over the inflation rate. To make matters worse, the inflation rate also becomes the macroeconomy’s main shock absorber, so any shocks that produce unexpected increases in the deficit will lead to unexpected increases in inflation, which makes inflation highly uncertain too. A policy of monetary finance is therefore dangerous, because it can easily lead to runaway inflation or even hyperinflation. Externally, these effects on prices and inflation will be reflected in a falling, volatile and uncertain exchange rate.



In sum, we have a variety of possible long‐​term consequences, ranging from merely bad (highish and uncertain inflation, loss of control over inflation, a volatile exchange rate) to positively catastrophic (huge levels of national debt, high future taxation, national default and all that might entail, runaway inflation, hyperinflation). I could go into a long discussion of why any other fiscal‐​monetary policy mix would produce better long‐​term outcomes. On the fiscal side: a balanced budget or lower deficit financing. On the monetary side: a monetarist rule, a nominal GDP target, a Taylor Rule, a gold standard, whatever.



Suffice to say that the poor performance of MMT is not a coincidence. On the fiscal side, it encourages a much more rapid run‐​up of the debt/​GDP ratio than any alternative, which has got to be the worst possible fiscal policy in the long term. On the monetary side, it throws away any attempt at controlling inflation or maintaining monetary stability by making monetary policy subservient to runaway government spending. MMT performs so badly _precisely because_ it represents the extremes of fiscal and monetary excess.



Indeed, MMT does not even work on its own terms. Kelton herself indicates that an MMT policy package is constrained by the requirement that inflation should not rise. Unfortunately, she hasn’t thought it through.



Current U.S. inflation is 2.3 percent. Is she suggesting that the government should pursue MMT subject to the constraint that the inflation rate should not rise above 2.3 percent? If so, she should advertise the fact to help dispel the concerns of those who might have gotten the impression that MMT is some sort of funny money scheme. If she did so, much of the knee‐​jerk opposition from sound‐​money people would dissipate. The problem, however, is that a commitment to maintaining inflation at no more than its current rate would severely constrain the ability of MMT to deliver on its promises.



A looser interpretation of her “inflation shouldn’t rise” constraint would apply to inflation in the long run. But then consider my unpleasant monetarist arithmetic results. Since pure debt finance is fiscally unsustainable, any MMT package must involve some element of monetary finance. But, _ex hypothesi_ , the monetary finance must be constrained by the need to avoid rising inflation in the long run. A necessary condition to prevent rising inflation is that the rate of growth of the deficit should not itself increase. This constraint is not as severe as requiring that current inflation should never rise, but it is a severe constraint nonetheless. The problem is that it is not at all clear how much of what she promises can be delivered while satisfying this constraint. That she does not address this issue is the central failing of her book.



In effect, she offers us the prospect of a bunch of goodies but doesn’t explain why those goodies fall within the economy’s production possibility frontier, that is, are actually attainable—an intriguing oversight for an economist.6



She offers a revealing anecdote when she recalls a discussion between James Tobin and President Kennedy:



“Everything else is just talk” captures it perfectly. Indeed, it undermines the entire book.



We shouldn’t forget what subsequently happened. Government spending on the Vietnam War and the Great Society overdid it, and the United States ended up with rising inflation and the monetary troubles of the 70s and early 80s. MMT’s fiscal proposals are akin to the Great Society and its monetary proposals are akin to relying on the pre‐​NAIRU, or fixed, Philips Curve that was discredited by Milton Friedman. From this perspective, MMT has a distinct sixties feel to it.



In both cases, the root problem is the same: the absence of a coherent theory of inflation. The Keynesians of the sixties didn’t have one and neither does Kelton. Now, as then, the solution to that problem is the same: their macro model needs some version of the quantity theory of money to connect the money supply to the price level, and thence to the inflation rate. If the central bank or the government pursue policies that lead to too much monetary growth, then the result will be a higher than desired inflation rate. The solution to that problem is also the same as it was then: to rein in the rate of monetary growth. In short, MMT is not particularly modern and the monetary theory has too much money and not enough theory.



Kelton might object that these negative outcomes would not occur under MMT because counter measures would be taken once (or even before) inflation started to rise. So, what counter measures would she pull from her MMT toolbox? The answer is a rise in taxes.7



One can imagine the howls that would follow a proposal to raise taxes “merely” because inflation had gone over some “arbitrary” threshold. Why abandon The Project? Why stop policies that were on the verge of making the world a better place just as the going gets tough? MMTers attempting to stick to the “if inflation rises” script on which The Project was predicated would be cast into the role of fiscal conservatives. Don’t they know that deficits don’t matter, etc.? They then reap the downside of overpromising.



If and when taxes were increased, it is doubtful that doing so would get inflation back down again. We know from monetarism that the inflation rate will only come down once the underlying monetary growth rate has slowed. But since the MMTers lack a decent model of inflation the likely policy responses would be some muddle akin to what we experienced in the late 1960s and much of the 70s, and with similar results: rising inflation followed by stagflation, a new Keynesians vs. monetarists controversy, and inflation only being brought under control again when policymakers relearn the lessons learned then—namely, the importance of the quantity theory of money.



The poor long‐​term performance of MMT under my simulations is a perfect illustration of why policymakers need to be constrained by rules. To illustrate the benefits of such rules, consider the following. Recall that we can think of MMT as a set of policies that break down into two subsets:



(1)MMT = {MMT spending program; MMT financing program}.



The former is about what goes out of the government’s coffers and the latter is about what goes in.



It seems to me that for most people inclined toward MMT, the big attraction is the MMT spending program. For the sake of argument, let’s hypothetically agree with that spending program and then ask if we can replace the MMT financing program with something better.



The MMT financing program consists of a combination of high deficits, tax, borrowing, and monetary finance. This financing program is a key reason why MMT performs so badly, so let’s replace it with an alternative financing program that is a combination of, let’s say, more restrained deficits, tax and borrowing, and no monetary finance. The monetary side of this program would be taken care of by some monetary policy or rule that focuses on a stable inflation rate or something similar. We then come to:



(2)Alternative to MMT = {MMT spending program; alternative financing program}.



My models indicate that this “Alternative to MMT” would produce considerably better outcomes, including a less rapid run‐​up of national debt and lower inflation.



Why then would you not prefer the “alternative” to MMT? The “alternative” still delivers the spending goodies you want, but in a less damaging way in the long‐​term. But the “alternative” is old‐​fashioned tax and spend!



My point is that even if you support MMT because of its spending program, there is no good reason to support MMT in preference to some tax and spend policy mix with the same spending program. Whatever your preferred government spending program, MMT is a poor way to finance it.



So, if you are a hard‐​left socialist who supports the Kelton government spending platform, you should support tax and spend, not MMT. And if you do not support her spending platform, say because you are not politically hard left or because you support sound money, then you would also not support MMT. Whatever your politics, MMT is not for you; MMT is just bad economics.



In the end, MMT comes down to this: the government spends a lot, issues a lot of debt, and prints a lot of money. It is not as if it hasn’t been tried before.



Haskins, R. (2015) “The Federal Debt is Worse Than You Think.” Brookings Institution blog (April 8).



Judt, T. (2006) _Postwar: A History of Europe Since 1945_. New York: Penguin.



Sargent, T. J., and N. Wallace (1981) “Some Unpleasant Monetarist Arithmetic.” _Federal Reserve Bank of Minneapolis Quarterly Review_ (Fall): 1–17.



Smith, A. (1994 [1776]) _An Inquiry into the Nature and Causes of the Wealth of Nations_. New York: Modern Library.



Kevin Dowd is an adjunct scholar at the Cato Institute, and a professor of finance and economics at Durham University in England.



ShowHide

Endnotes



1 “There’s no reason every job—all the way down to retail clerk or fast food worker or janitor in a luxury Chicago hotel—can’t be a good job, with dignified pay, hours, security, and benefits,” she says. The question however is how many of these jobs would still exist.



2 I implicitly assume, as seems reasonable in this (MMT) context, that the rate of growth of the national debt, including entitlement commitments (see below), exceeds the economic growth rate. In that case, the ratio of debt (including entitlements) to GDP will keep growing, and default is then inevitable unless the government resorts to (a) taxation or (b) printing money.



3 This figure comes from Laurence Kotlikoff’s testimony to Congress in 2014 (see Haskins 2015).



4 Few presidents would have the self‐​restraint to refrain from taking advantage of such powers, but the point is that if the Constitution were properly followed, we wouldn’t have to rely on his or her self‐​restraint in the first place.



5 One might even go as far as to say that MMT is the apotheosis of chartalism and I do not mean that as a compliment. Chartalism maintains that the state is entitled to monopoly privileges regarding the issue of currency. In response: a government monopoly is always a bad idea, period.



6 As she says, “The real challenge lies in managing your available resources—labor, equipment, technology, natural resources, and so on—so that inflation does not accelerate.” This passage describes the problem nicely but does not give the solution to it.



7 Wrong again. The only way to reduce inflation is to rein in the excessive monetary growth that is the proximate cause of rising inflation. So, Kelton’s statement that “MMT … offers a more sophisticated array of techniques for managing inflationary pressures than what we have today,” does not instill much confidence.
"
"Palm oil can be found in food and cosmetics everywhere: in fact, half of the world’s population uses palm oil in food. But public awareness about the loss of wildlife through deforestation caused by palm oil crops is growing, and there’s mounting pressure on retailers to reduce their sales of palm oil products, or boycott them altogether.  The debate has become especially heated since a Christmas advert by UK-based supermarket chain Iceland – which dramatises the link between palm oil, deforestation and the death of orangutans - was barred from being broadcast in the UK, on the basis that it would have breached political advertising laws, because the animation was originally produced by Greenpeace.  In the first four days of its release, the video was viewed 13m times. A petition to overturn the advert ban has so far attracted more than 720,000 signatures. But while Iceland’s campaign has been a great way to bring more public attention to food sustainability issues, an outright boycott on palm oil products could actually lead to more problems for forests and wildlife. 


      Read more:
      Iceland Christmas ad: barred, but it will help 2018 go down as the year of 'corporate caring'


 A recent report by the International Union for the Conservation of Nature, concluded that boycotting palm oil would merely shift – rather than counter – losses to rainforests and wildlife caused by agriculture. Put simply, boycotted palm oil would need to be replaced by other types of vegetable oil to meet global demand – and that could actually make matters worse.  This is because, compared to other common sources of vegetable oil – such as rapeseed and soybeans – palm oil crops yield four to ten times more oil per unit of land, and require far less pesticide and fertiliser. In fact, palm oil makes up 35% of all vegetable oils, grown on just 10% of the land allocated to oil crops.  So, if other crops such as soybean replaced a shortfall in palm oil, this would not only shift more production to the Amazon (a major soy-producing region), it would also require more land, leading to more deforestation. Indeed, soybean farming is already responsible for more than double the deforestation of palm oil. In the context of other food sources, livestock and beef production has led to more than five times the amount of deforestation, compared to palm oil.  Certification – a mechanism by which consumers pay higher prices for more responsibly sourced products – is one way to help safeguard rainforests, and the wildlife which lives in them. Palm oil certification is spearheaded by the Roundtable on Sustainable Palm Oil (RSPO), who are leading the market toward environmentally and socially responsible palm oil that doesn’t contribute to deforestation.  As the RSPO meets to renew its sustainability commitments, one major challenge facing the sector is that less than 20% of the world’s palm oil is currently certified as sustainable.  There is little incentive for producers to seek certification – or for retailers to promote environmentally and socially responsible products – as long as the debate continues to focus on boycotting palm oil altogether. As a result, only about half of sustainable palm oil is actually sold as certified, because a large proportion of the market is not willing to pay the premium for sustainable products.  Despite this, many large retailers and leading brands (including Nestlé, Unilever and Palmolive) and supermarkets (such as Morrison’s, Waitrose and Sainsbury’s in the UK) are already using certified palm oil in their products, but cannot heavily promote this due to the persistent negativity toward any type of palm oil. To help the palm oil industry to safeguard wildlife, conservation scientists are working with certification bodies and producers to improve how palm oil cultivation affects biodiversity. It can be as simple as growing the crop on non-forested areas. But it can also involve protecting forests along rivers, such that they join up patches of high quality forest within the palm oil landscape, allowing wildlife to move more freely. If certification of palm oil becomes more popular, it will improve prospects for wildlife, including orangutans. This is why major conservation organisations – including leading orangutan charities and Greenpeace – continue to support certified palm oil, rather than a boycott. And now, environmentally conscious consumers can check where they can buy products that contain responsibly sourced palm oil. Hopefully the interest sparked by Iceland’s advert will bring positive changes for rainforests and their wildlife. But a boycott is not the best answer. The best thing retailers can do is support their suppliers to bring more responsibly sourced products to the supermarket shelves this Christmas."
"We are only just beginning to learn how aquatic organisms will respond to climate change, and the effect that this will have on their communities and ecosystems. One way to find out more is to look at whether species will be able to compensate for changes in their environment. Particularly if they can survive any immediate fluctuations in temperature, and reductions in ocean pH brought about by increasing levels of atmospheric CO₂.  Coastlines and estuaries are already challenging places for marine organisms to live. The physical properties of seawater – salinity, temperature, pH and oxygen levels – vary frequently. And with further environmental fluctuations due to climate change, they are becoming even more demanding. Patterns of sea surface salinity are changing, as fresh water input increases, due to exceptional storm events and runoff from flooding.   Scientists have started to examine the combined effects of global warming and a reduction in seawater pH – otherwise known as ocean acidification – on marine communities. To date, it has appeared that multiple factors have more of an effect on these creatures than each factor in isolation. Together they influence the ability of species to compensate and survive the changes.  However, not much is known about the combined effects of ocean acidification and seawater dilution on these organisms. This is important as changes in salinity tolerance are known to influence distribution patterns of marine species and their community structures.  For our newly published study we decided to look at this combination of factors by focusing on two species of marine crabs: the edible crab (Cancer pagurus) and the shore crab (Carcinus maenas). Both are common to UK waters, but experience different degrees of environmental variation in their natural habitats. For edible crabs, home is typically the low intertidal shallow shelf waters for juveniles, and down to 100 metres for adults away from the influence of freshwater. While shore crabs typically live in estuaries and experience dilute seawater on a regular basis.  We studied how the crabs reacted to what are predicted to be the business as usual levels of CO₂ in 2100 (1,000 micro-atmospheres) and a biologically relevant reduction in seawater salinity. We were interested to see whether the edible crab will be less capable than the shore crab which regularly experiences salinity variations. We were also keen to find out why one species is likely to be more vulnerable than the other by investigating the ways they naturally compensate for environmental changes. We exposed juveniles of both species to the different CO₂ and salinity conditions for up to one year. The crabs were fed regularly and they continued to grow by moulting throughout the exposure time. We found that the shore crab was fully capable of surviving the conditions for up to a year, but the edible crab struggled.  The shore crab – which is a widely invasive species in countries outside Europe – increased its response to a stimulus (upregulated) its capacity to exchange bicarbonate ions across the gills. This mechanism helps buffer changes in body fluid pH associated with increased CO₂ in seawater. The edible crab, meanwhile, showed no such upregulation, and had limited ion transporting capacities. Instead, this species accumulated CO₂ within its haemolymph (crustacean blood) supply.  There was some attempt at compensating for the conditions, but remarkably the edible crabs were better off in dilute seawater. This was a surprise as the edible crab typically spends all of its adult life living in marine environments separated from the influence of freshwater. The reason behind it is difficult to explain, but it may come down to passive changes associated with exposure to dilute seawater making the haemolymph more alkaline.  Our work demonstrates that the juvenile edible crabs could survive elevated CO₂ conditions by moving into freshening seawater – but only for limited periods. This species also proved to be vulnerable to longer term exposures to dilute seawater. Our study helps us appreciate that there are fundamental differences in the biological capacities of marine species to compensate for climate change. Even within a taxa of crustaceans that is generally regarded as being relatively tolerant to change.  Fully marine species, such as the edible crab, with its preference for stability, are poorly equipped for survival in a variable natural environment. They are likely be to more vulnerable to climate change and further studies on this and similar species are urgently needed."
"Over the last few years Arctic scientists have reported a surprising finding: large areas of the Arctic are turning brown. This is in part due to extreme events linked to winter weather, such as sudden, short-lived periods of extreme warmth. These events are occurring as the climate warms, which is happening twice as fast in the Arctic compared with the rest of the planet. Extreme events are therefore happening more and more often, with increasingly severe effects – including widespread damage and death in Arctic plants. This “browning” of plant communities has happened over thousands of square kilometres or more. However, until recently we knew very little about what this might mean for the balance between carbon uptake and release in Arctic ecosystems. Given that the Arctic stores twice as much carbon as the atmosphere, this is a pressing concern. Now, our study has shown that extreme climatic events can significantly reduce the ability of Arctic ecosystems to take up carbon –- with implications for whether the Arctic will help combat climate change, or accelerate it. To understand how extreme events are affecting Arctic heathlands, we travelled to the Lofoten Islands in northern Norway where coastal, sub-Arctic plant communities act as a bellwether for future climate change in the far north by exhibiting the effects of warming in the region first. Here we found the effects of two extreme winter weather events. First, “frost drought” had caused extensive plant dieback. Frost drought occurs when the insulating layer of snow which usually protects plants from the harsh Arctic winter is melted, typically by unusually high winter temperatures. If plants remain exposed to cold, windy conditions for long enough, they continually lose water and are unable to replace it from the frozen soil. Eventually, they succumb to drought. The second event was “extreme winter warming” – a sudden burst of high temperatures during winter which melts the snow and tricks evergreen plants into preparing for spring by shedding their cold tolerance. When the warm period is over, the return of cold temperatures usually kills the plant. In this case, however, we found something unexpected. Heathland plants had survived this extreme winter warming event, but were showing evidence of severe stress, visible as a deep, persistent dark red colour in shoots and leaves. We measured how much carbon dioxide was being taken in and released by the plants in three vegetation types: damaged heathland (where the dominant evergreen species had been killed by frost drought), stressed heathland, and healthy, green heathland which had escaped the effects of either extreme event. This was done in three measurement periods across the growing season. We found that these extreme winter conditions reduced how much carbon was absorbed in heathland ecosystems by up to 50% across the entire growing season. This is a huge reduction in the ability of a widespread Arctic ecosystem to remove carbon from the atmosphere.  Surprisingly, this was the case both in damaged heathland, where a large part of the vegetation had been killed, and in stressed heathland. Although the processes driving this change were different in each type of heathland, this clearly shows that we need to consider the role of plant stress in limiting plant carbon uptake to fully appreciate the consequences of extreme climatic events. What does this mean for the Arctic? We now know that extreme climatic events could significantly reduce the ability of Arctic ecosystems to take up carbon and combat climate change. This is especially concerning as the impacts of browning are in stark contrast to those of a better understood response of Arctic ecosystems to climate change: “Arctic greening”, or the tendency for plants to become taller and more productive as Arctic summers warm. Many climate models currently assume arbitrary levels of greening across the Arctic,  and therefore that Arctic ecosystems will take up more carbon in the future – slowing climate change. The scale of the browning we’ve seen in recent years combined with the negative impacts on carbon uptake reported here suggests that the reality may be more complex, calling into question our understanding of the role of the Arctic in the Earth’s climate. What does this mean for us? The impact of extreme weather events in the Arctic has global consequences. It is clear that our current efforts to tackle climate change are dangerously inadequate, but ambitious action now could cut how much the Arctic is expected to warm by as much as 7°C. This is critical to minimising the impacts of climate change both in Arctic ecosystems and worldwide."
"Red Dead Redemption 2, a new video game about an outlaw gang on the American frontier in 1899, has been met with huge adoration. Journalists have lauded it as a “landmark” title, a “technological masterpiece”, even a “watershed moment” in entertainment. Much of the praise has focused on how developer Rockstar Games has coded a “living” game world that oozes character and aesthetic richness. However, now that the digital dust has started to settle, that same world has come in for criticism. Gamers have dubbed the title “boring” and “slow,” with their enjoyment of the game noticeably impeded by “clunky controls” and the lack of easy “fast travel” between destinations. Matt Reynolds for Wired recently complained how Red Dead Redemption 2 ultimately “feels like a chore.” The ability to kill a life-like female suffragette in the game also courted controversy, with YouTube first banning then restricting the gratuitous footage. Most of the criticism reflects a growing problem with video games: the pursuit of realism. For decades now, mainstream developers (with the notable exception of Nintendo) have committed their energies to crafting visually realistic game worlds. More than any other studio, Rockstar has dedicated itself to the holy digital grail of verisimilitude, throwing millions of dollars (and controversially long work hours for its staff) chasing reality.   In the 1970s, arcade Westerns such as Midway’s Gun Fight featured barely-animated stick figures “firing” dots at each other across a landscape populated by a single pixelated cactus. By contrast, in Red Dead Redemption 2, game character Arthur Morgan’s hair, wardrobe and waistline grows and stunning digital vistas rival Albert Bierstadt’s paintings of the 1800’s American Interior.  In Rockstar’s stunning quest for the life-like, more than 170 species of animal wander the digital terrain. In the 1870s, Eadweard Muybridge, seeking to answer the question of how horses gallop, captured their movement through a series of photographs. Nearly 150 years on, Rockstar uses 21st-century motion capture to perfect literally hundreds of “living”, “breathing”, and galloping horses on the digital screen, to such detail that, as lad magazines recently enthused, even their testicles shrink in the cold. Not only has Rockstar strived for visual realism, but the studio has constructed a world of realistic player activities and responsibilities. From their inception, video games have digitised the “every day”. From delivering newspapers in Atari’s Paperboy (1984) to collecting airport baggage in Apollo’s Lost Luggage (1982), video games have transformed mundane tasks into moments of digital revelry. Released back in the 1990s, Sega’s Shenmue (1999), set in a “living” Japanese city, incorporated a range of daily tasks including driving a forklift for money. Such titles have provided moments of “playful realism”.  Red Dead Redemption 2 represents a new scale-up and seriousness to the enterprise. As the character Arthur Morgan, the player is expected to hunt and skin wild animals, maintain his guns and wares fastidiously, and feed and groom his horse. In the process, Red Dead Redemption 2 undoubtedly offers a more meaningful adventure. But the game also tests boundaries over notions of play and digital experience. Unfortunately, performing work-like tasks and living the “every day” in games can easily test our patience. The closer a game gets to any semblance of reality, the greater the player notices its flaws. In “reality”, most of us (at least on a basic level) can choose when to do things, perform tasks freely and organically, and process multiple sensations while doing them (such as the weight of an item, or our own limited strength).   In ultra-realistic games, those expectations are quickly frustrated: we push a complex sequence of buttons to perform simple actions (such as drawing a gun), we lose authorial control (and voice) to orchestrated story arcs (Red Dead’s set missions), and narrow visual cues become an excuse for human experience. In-game realism is quite a different property, then, to the world outside. Too much realism also rails against the basic appeal of games: to escape, to play, to indulge in fantasy (in other words, the “unreal”) and most of all, have fun.  Writing in Homo Ludens (1938) on the concept of games, Johan Huizinga declared “the fun-element” crucial to “the essence of play”. In the case of Red Dead Redemption 2, Rockstar has pushed the boundaries of gamic realism to new heights of maturity and sophistication. Rockstar shows how we can find beauty in a digital place and a digital moment, and is actively testing what “gameplay” means. However, while a landmark title, few reviewers have applauded the game as enjoyable. The true limit of gamic realism may not come in terms of technological hardware, programmer hours or dollars spent, but in our basic human desire for games to be fun."
"

Scott Brown’s stunning upset in the Massachusetts special election may have done what the best policy arguments could not – defeat the Democrats’ plans for a massive government takeover of the U.S. health care system.



Democrats will undoubtedly offer a variety of excuses for Brown’s win. The Democratic nominee, Attorney General Martha Coakley, was a poor candidate. The “political climate” was bad. The dog ate their ballots. But in reality, there can be no denying that this election was a clear cut rejection of the Democratic health care bills.



There were no blurred differences on this issue. Scott Brown made his opposition to the bill a centerpiece of his campaign. He promised to be the 41st vote to sustain a filibuster and kill the bill, even signing autographs as “Scott41.” Coakley, on the other hand, pledged to vote for the bill.





[T]here can be no denying that this election was a clear cut rejection of the Democratic health care bills.



The issue was featured in ads, debates, and public discussions. In the end, according to polls, in the home of Ted Kennedy, more than half of voters opposed the version of health care reform being rushed through Congress. Voters knew what they were saying. And what they were saying was a resounding “No!”



What do Democrats do now? House Speaker Nancy Pelosi says that they will pass health care reform “one way or another.” Those “ways” are:



 **Hurry up and stall:** New York Democratic congressman Anthony Wiener both named and defined this strategy. Democrats would slow‐​walk certification of Brown’s victory, preventing him from taking his seat in the Senate. Massachusetts Secretary of State William Galvin has hinted that he won’t certify election results for at least the 10 days that local officials have to report on absentee and overseas ballots and has noted that state election law gives him as long as 50 days beyond that. Meanwhile, Pelosi and Harry Reid will rush their negotiations to merge the House and Senate bills, allowing the appointed interim Massachusetts Senator Paul Kirk to vote on the bill before Brown takes his seat.



Democrats legitimately fear that such a blatant disregard for the democratic process would spark an enormous backlash. There would be no way to pretend it was anything other than the most corrupt power politics. After previous special elections the winners have been seated within days. In fact, when Kennedy first won the seat in a 1962 special election, he was sworn in the very next day. Would Democratic moderates, already frightened by the election outcome, be willing to go along with such an approach?



 **The House Surrenders:** Democrats could try to avoid a Senate vote altogether by having the House simply pass the already Senate approved measure. But that would require the House to accept the Senate bill with no changes at all. Pro‐​life Democrats like Bart Stupak (D‐​Mich.) would have to accept much weaker Senate restrictions on government funding of abortions. Liberals would have to accept the so‐​called “Cadillac tax” on high value insurance plans — without the exemption demanded by labor unions. And what about nervous moderate and blue dog Democrats? Are the Massachusetts results going to make them more or less likely to go out on a limb for health care reform? Remember, the House only passed their bill by a three vote margin, and one of those, Rep. Robert Wexler (D‐​Fl.) has since resigned his seat.



 **Let It Snowe:** Democrats could try to reach a compromise with Republican moderates like Olympia Snowe of Maine. Snowe did vote for a version of health reform in the Finance Committee and has spoken positively of the need for reform. But she also voted in favor of a resolution declaring that the individual mandate was unconstitutional and has raised a number of other objections. Any bill she agreed to would have to be substantially different than the ones currently being considered. That would almost certainly jeopardize already tenuous support from liberals. If the current Senate version is hard for them to swallow, just imagine how they will react to one watered down even further.



 **Go for 51:** The last, desperate gasp would be to use an arcane procedure known as reconciliation to pass health care reform with just 51 votes. But doing so would require Senate Democrats to overcome all manner of procedural hurdles. Reconciliation cannot be used for policy as opposed to budgetary issues. That means Democrats would have to drop some of their more popular proposals like the ban on preexisting conditions. They would be left with a bill that did little more than expand Medicaid and other subsidies, raise taxes, and cut Medicare. How popular would that be?



So far Democrats have been willing to do almost anything, cut any deal, sacrifice any principle, to force this bill through. But they may be running out of options at last. And for that, we can thank Scott Brown and the voters of Massachusetts.
"
"Electricity generated by fossil fuels is increasingly unsustainable and a shift towards renewable energy – principally from the sun and wind – is vital. Renewable generation is already less expensive per unit than its polluting counterparts, but the fact the sun doesn’t always shine and the wind doesn’t always blow presents an obstacle to a serious takeover of the energy sector. Energy storage could overcome this pressing “intermittancy” issue. If storage was available at sufficiently low cost and high performance, renewable energy would rapidly displace all other generation forms. Energy is already stored, of course, in batteries or various other technologies. Even reservoirs can act as huge stores of energy. However nothing that exists or is in development can store energy as well, and as cheaply, as compressed air. The concept seems simple: you just suck in some air from the atmosphere, compress it using electrically-driven compressors and store the energy in the form of pressurised air. When you need that energy you just let the air out and pass it through a machine that takes the energy from the air and turns an electrical generator. Compressed air energy storage (or CAES), to give it its full name, can involve storing air in steel tanks or in much less expensive containments deep underwater. In some cases, high pressure air can be stored in caverns deep underground, either excavated directly out of hard rock or formed in large salt deposits by so-called “solution mining”, where water is pumped in and salty water comes out. Such salt caverns are often used to store natural gas. Compressed air could easily deliver the required scale of storage, but it remains grossly undervalued by policymakers, funding bodies and the energy industry itself. This has stunted the development of the technology and means it is likely that much more expensive and less effective solutions will instead be adopted. At present, three key problems stand in the way of compressed air: The above description of how it works is an over-simplification. CAES is, in fact, not a single technology but a wide family that includes compression machinery, expansion machinery, heat exchangers, the design of air stores and the design of thermal stores. These all require meticulous engineering to get right.  At the moment, wind and solar still make up only a small proportion of the overall sector. As electricity generated from fossil fuels can cover the overcast or wind-free days, renewable energy is often used straight away and only needs to be stored for short amounts of time. For these situations, batteries work quite well and can be economically viable.  Large-scale decarbonisation will require us to store energy for much longer periods, however, for instance from a sunny day to use on a cloudy day. CAES is especially suited for storage durations of some hours through to several days.  All affordable energy storage involves converting energy from the form of electricity to some other form and storing it in that other form. For pumped-hydro storage, for instance, the other form is water that has been lifted up to a great height. For CAES, that other form includes both heat and high-pressure air. For such systems, there are separate costs for the equipment that does the conversion and for the storage itself. Systems like CAES and pumped-hydro involve relatively expensive equipment for the power conversion but very inexpensive provisions for the storage of energy. These systems, where small amounts of power can fill up very large amounts of storage, are therefore very economical for storing energy over a long period. Private investment requires high rates of return. An indirect effect of this is that investors place less value on what utility may be left in an asset in the longer term.  In most CAES systems, costs are concentrated in things that naturally have very long lifetimes. For example, a solution-mined cavern in a salt deposit might reasonably be expected to operate for at least 100 years, while high power machines for compressing and expanding air can typically operate for 50 years or more. With returns over such a long timescale, there is a strong argument that at least some large-scale compressed air installations should be treated as national infrastructure projects financed by governments. Two large compressed air plants were built decades ago, one in Huntorf, Germany and the other in McIntosh, Alabama. Both are still working extremely well. Many refer to these two plants to draw conclusions about how efficient CAES can be and how much or little it can cost.  But this is misleading and pointless. Both plants were designed with very different priorities from those relevant today. It is imperative that we now think again about compressed air energy storage and evaluate it properly in light of what can be achieved by exploiting modern methods and knowledge."
"In a bid to reduce our carbon footprint, confront greenwashing and increase our focus on the climate crisis, the Guardian this week announced it will no longer run ads from fossil fuel extractors alongside any of its content in print or online. The move will come into immediate effect, and follows the announcement in October last year that we intend to reduce our net emissions to zero by 2030. Once upon a time, a newspaper was a rather straightforward business. You generated enough material of interest to attract a significant number of readers. You then ‘sold’ those readers to advertisers happy to pay to get their ideas, products or brands in front of consumers with cash to spend.  Of course, digital disruption over the past 20 years has upended that model, but advertising remains an important part of the media business ecosystem. At the Guardian, it is still responsible for about two-fifths of our income. But what happens when the readers don’t like the adverts? What do you do when the message that advertisers want to spread jars awkwardly with the work your journalists are doing? What if your journalists are some of the best in the world at revealing and investigating the deepening climate catastrophe and the disaster that is fossil fuel growth, while some of your advertisers are the very people digging the stuff out of the ground? This contradiction has bothered us - and some of you - for some time. We came up with a rather bold answer this week: turn away the money and double down on the journalism. “It’s something we thought about for a long time,” says Anna Bateson, the interim chief executive officer of Guardian Media Group, the Guardian’s parent company. “We always felt it was in line with our editorial values but were cautious for commercial reasons.” She said it was the logical next step after the Guardian committed last year to becoming carbon neutral by 2030 and was certified as a B Corp – a company that puts purpose before profit. But she added that the move had to be weighed carefully, given the fact that the Guardian only recently returned to breakeven after years in the red. “You have to be careful you are not making cavalier decisions,” she said. “ We are still having to fight for our financial future. But because of the support we get from our readers, it is less of a risk.” On the advertising side of our business, Adam Foley said there were no complaints at all that potential customers were suddenly off-limits, adding that staff felt that “being part of a company that shares their values” was the biggest motivation for his teams. “A statement like this reaffirms to all of us that we’re contributing to a business that really lives those values - to the extent where it is prepared to sacrifice profit for purpose.” The response from the wider world has been a pleasant surprise. Hundreds of you have written in, pledging your support, and in some cases, one-off contributions to start making up the shortfall.  The environmental movement was instantly appreciative, with activists quickly urging our peers to follow suit. “The Guardian will no longer accept advertising from oil and gas companies,” Greta Thunberg tweeted. “A good start, who will take this further?” Greenpeace called it “a huge moment in the battle against oil and gas for all of us.” Some readers have been calling for the Guardian to go the whole hog and forsake advertising from any company with a substantial carbon footprint. Bateson said that was not realistic, adding that such a move would result in less money for journalism. She said the fossil fuel extractors were specifically targeted because of their efforts to skew the climate change debate through their lobbying effort.“We are committed to advertising,” she said. “It will continue to be part of our future. We want advertisers who want to be appear alongside our high quality journalism.” And how will we know if this has worked?“We will listen to our readers, we will listen to our advertisers. The response so far has been gratifying. If we continue to hear positive noises from our readers and supporters, then it will have been a success.” Responses from our supporters That is such a brilliant decision and it will be tough, but it is the correct one and I am very proud of The Guardian. Barbara Syer Following the Guardian’s decision to ban ads from fossil fuel companies I’m making a monthly contribution to support its fearless journalism: reader support is essential for independent scrutiny of the powerful in business, finance and politics. Titus Alexander, Hertfordshire, England I live at present in Canada, home to the Alberta Tar Sands: another name for ecological devastation resulting from fossil fuel extraction. I fully support The Guardian’s action in ceasing to be a vehicle for advertising by fossil fuel extractive companies, and I’m proud to be a supporter. My monthly donation is small, but when I can I will make it much greater. Rosemary Delnavine, Canada Congratulations. At this time it may be a bold step, indeed, within this industry, but true leaders have to take bold steps for the betterment of the quality of life, and more importantly for the life of future generations. I applaud this decision, and will spread the word. Raphael Sulkovitz, Boston MA What a bravery! This is what the life on earth needs, thank you. Karri Kuikka, Finland  Keep it up. Here in Canada, we’re still trying to have it both ways -- sell the product internationally but discourage buying domestically. As I recall, it was the same with tobacco. Eventually, it took a change in public opinion to solve the problem. As a news source, your efforts are part of this solution. Robert Shotton, Ottawa I applaud your decision to”walk the talk.” I will therefore continue to contribute to The Guardian. Bob Wagenseil Bravo yr decision to eschew $ from the FFI. Please do continue to hold to the fire(s) the feet of the deniers and the willfully ignorant. Sydney Alonso, Vermont, US I am very happy to hear that good news. It’s quite courageous on your part, and I’m happy to support you! Have a great year ahead, you’ll have my continuous support! Julien Psomas I completely support your plan to refuse ads from fossils, despite the  financial hit to the Guardian. I have made a donation to help out. David Thompson A very commendable decision, very much in keeping with the Guardian’s position as leader of green issues to leave a better planet for following generations. Richard Vernon, Oxford Yay! I’m so proud of the Guardian! We can no longer support or fund in any manner the fossil fuel industry if we have any chance of survival as a civilization on this planet. You’ve taken a courageous and moral step that will hopefully embolden others to join you. Good on you! Best, Carol Ross, Missouri, US Good decision. I’ll support you as much as I can, which unfortunately is not much as I live on age pension only. Keep up the good work, we need it desperately! Ursula Brandt, South Australia I am absolutely delighted by this decision. So many people pledge to do something about Climate Change, but few actually are willing to get uncomfortable and DO it. I am very proud of you as my favourite source of Information and this only makes a case for me to donate next time to you again. Christiane Gross It was great reading what The Guardian is doing re the climate. As a Guardian on-line reader from The Netherlands I’m going to contribute monthly now instead of ‘now and again’. The amount will be relatively small as I do not have a great income. I really hope more of your supporters will do so, because it is really great what you are doing.  With kind regards, Aleida Oostendorp, Netherlands I congratulate you and your team on taking this step regarding fossil fuel companies. The Guardian’s stance on the environment and its excellent coverage of related stories and events is the major reason for my support. Well done, and good luck in the future. Deirdre Moore Love your new policy about accepting money from fossil fuels. Will contribute more to help make up for the shortfall. Todd Misk I live on a fixed income with a strict budget so my continuing support of your excellent news organisation represents my commitment to the fight to address climate change. Every step counts. Barbara Hirsch, Texas, US Only when we speak truth to power can change take place. thank yo for your courageous and expensive decision. Nancy Shepherd, Vermont, US Love your journalism, especially your investigative work and the climate change topic. And with the bold statement about not receiving any more sponsorship from the fossil extracting companies? Well, the already great newspapers became even more impressive now. Keep up the good work. Miroslav Řezníček, Czech RepublicThank you for taking the bold step of refusing advertising from fossil fuel extractive companies. I think it is the right thing to do & hope many more companies do the same. We must all work together if we want to save our planet. It is one of the most important issues of our times. Ginger Comstock, New York, US"
"As I write these lines, I tell myself there is no reason to fear. Far-right politics are on the rise globally – yet the universities in Brazil that were recently raided in a clear attack on freedom of expression seem a long way from the UK. This has not prevented, however, my personal and professional selves becoming increasingly entangled as I watched Jair Bolsonaro, one of the most obnoxious figures in Brazilian politics, become the country’s president-elect.  As a Brazilian citizen, and an academic interested in ethics, social justice and sustainability, I have caught myself asking how to resist in times of “Bolsonism”. After all, in my everyday life I live by values which are in direct opposition to it. The answer I found, which might make some fellow scientists (including social scientists) raise their eyebrows, is to be overtly political.  If our practices as researchers and educators are already value-laden, we should then ask ourselves which values we want to pursue. As a form of academic activism, we should be ready to lobby – not necessarily for funding, which is already the most established form of scientific lobbying – but, through our research, for diversity, human and environmental rights. Many would argue that science and politics operate on different grounds and work by different logics. Scientific research is, however, just as instrumental for policy and regulation, as dependent on them. For instance, scientific evidence has been the centre of attention in recent cancer lawsuits, while governments are considering supporting research on artificial intelligence for military systems.  Ideals of scientific neutrality and disinterestedness have been challenged and the politics of knowledge production were unveiled a long time ago. Science is not immune to human values precisely because it is a human endeavour. As with politics, science not only is highly fallible, it is also shaped by specific interests, it delivers on specific goals and it is subject to cultural standards.  Of course, it wouldn’t be a surprise to see some supporters of Bolsonaro take advantage of this narrative and use it to undermine science’s authority. Such a strategy seems to have already worked well for climate change deniers. So where would this leave us? If there is very little, or nothing at all, that separates science from other human practices, is there anything special about science in the context of far-right politics? Scientific actors are powerful players in the fabrication of collective imaginaries in modern societies. The interaction between “pure” science and its application is fundamental in the creation of shared ideas on how society should be and which goals we should pursue. The science studies scholar Sheila Jasanoff has shown that anything from mundane devices through to high-tech projects on nuclear energy or bioengineering, are all motivated by broader sets of values and norms which are adopted and shared within communities or even nations. For example, increasingly individualistic societies will favour scientific and technological developments that allow for the reproduction of these values, such as self-driving cars or smartphones.  In her book Dreamscapes of modernity Jasanoff writes that such “sociotechnical imaginaries” might be held collectively but they “can originate in the visions of single individuals or small collectives, gaining traction through blatant exercises of power or sustained acts of coalition building”. This reminds us of the importance of humility in scientific and technological decision making. The projects we push forward help make certain views of the world become reality and generate benefits and harms which may be unequally distributed among different societal groups. In times of Bolsonism, we should therefore ask ourselves what kinds of imaginaries we want to build and to which social, environmental and political objectives our research is contributing.  How can we ensure, for example, that industrial biotechnology will not undermine the livelihoods of subsistence farmers or contribute to agricultural models that cause deforestation? That the scientific solutions to tackle diseases and improve human well-being will also prioritise the needs of the poorest? Or that the research we produce will help debunking certain beliefs on class, gender and race hierarchies? Such questions are especially urgent when, as with Bolsonaro, those are the kinds of concerns that will likely be ignored. With this comes great responsibility and a duty of care, for which an unashamed and committed politicisation of science is required. Human values intersect our practices and shape our vision of public good. How should we respond to neo-liberal development policies which are detrimental to the environment, to social conservatism and the violation of human rights? Can we scientists, researchers and academics ask, every day, how are we promoting a culture of care through our professional selves as we typically ask of our personal selves? The philosopher of science Isabelle Stengers has reminded us that resistance is achieved by an intelligent, inclusive and collective endeavour against the establishment. Brazil’s future president supports vile beliefs in men’s superiority over women, hate towards the black, indigenous and LGBT communities, as well as the weakening of long-fought protection of Brazilian natural resources and of the autonomy of leftist educators. One way to resist his discourse and agenda is by saying and doing otherwise: by preaching responsibility over complacency and arrogance, and care over violence and destruction."
"
This picture, taken by www.surfacestations.org volunteer Don Kostuch is the Detroit Lakes, MN USHCN climate station of record. The Stevenson Screen is sinking into the swamp and the MMTS sensor is kept at a comfortable temperature thanks to the nearby A/C units.

The complete set of pictures is here
From NASA’s GISS, the plot makes it pretty easy to see there was no discernible multi-decadal temperature trend until the A/C units were installed. And it’s not hard to figure out when that was.

But hey, thy can “fix” the problem with math and adjustments to the temperature record.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4e14bff',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"

The Bush Administration caught unshirted hell from environmentalists last month for ordering a 20 percent increase in energy efficiency standards for air conditioners sold after 2006. The Clinton administration, you see, had proposed a 30 percent increase in standards, so the Bush administration is once again being hammered for relaxing environmental standards, promoting wasteful energy consumption, accelerating global climate change, and risking California‐​style blackouts during hot summer months for years to come.



While the Greens would have us believe that Bush was once again carrying water for corporate America, the president was actually doing us all a small favor. 



First, the Clinton standards would have imposed huge costs on consumers. The U.S. Department of Energy (DOE) estimates that the Clinton standards would have increased the price of your typical air conditioner by a whopping $332 to $435. The Bush administration standards would increase prices by only (!) $144 to $213. 



Second, the DOE had failed to adequately consider the possibility that some manufacturers would be driven out of business by the Clinton standards, thus reducing competition that helps restrain prices. So even those expected astronomical price increases were almost certainly low‐​ball estimates. 



Third, the DOE concluded that over 40 percent of consumers buying air conditioners that meet Clinton’s standards would never recover the higher costs through energy cost savings. Under the Bush administration standards, only about 25 percent of consumers would be net losers. 



The Greens are right, however, to point out that the Bush standards will accomplish little on the environmental front, but the Clinton standards would not have achieved much either. Consider: The DOE estimates that the Bush standards will reduce growth in energy consumption by “about 3 quads” between 2006–2030, a figure deemed by the department as “significant.” The nation, however, is expected to use about 3,200 quads of energy from 2006–2030, so “3 quads” equals but 9/100 of 1 percent of projected energy use. The DOE’s claims about avoided emissions of carbon dioxide and nitrogen oxides are equally overblown.



While we should be grateful that the Bush administration has adopted efficiency standards that are somewhat less costly than those planned by the Clinton administration, the unwarranted economic burden and restrictions on consumer choice remain too great. Consumers, not bureaucrats, should have the final say about what’s in the home.



The chance of recovering the higher cost of air conditioners through reduced energy consumption, after all, depends heavily on electricity prices and the frequency with which the air conditioner is used. Given that electricity prices vary widely throughout the United States (highest unit prices are more than double the lowest), and that consumers’ usage patterns vary for dozens of reasons (e.g., geographic location, family size, tolerance for warm temperatures), paying more up‐​front at the cash register to reduce operating costs by a small amount for years to come makes sense for some but not for others. 



While that observation should be rather obvious, it is — for practical purposes — ignored by appliance efficiency standards that apply to everyone whether it makes sense or not. The result is that many consumers are forced to incur higher costs than they would if allowed to base their decision on factors affecting their energy bills. The requirement is roughly equivalent to an attempt to restrain hat costs by requiring that everyone wear a size seven hat. 



Moreover, the entire regulatory exercise is fraught with uncertainty. The government, for instance, assumes that it can accurately forecast the conditions affecting future appliance manufacture, sales, and use during the next 30 years, including changes in technology, markets, energy prices, consumer and appliance manufacturer behavior, raw material, labor and overhead costs, and wholesale and retail markups. How likely is that, particularly when much of the data comes from appliance manufacturers with a direct financial interest in the DOE’s decisions? 



The upshot is that all analytic uncertainties in the exercise are resolved in favor of tighter standards because only the well organized — typically those positioned to gain the most from government action — can affect the rulemaking process. Big appliance manufacturers, for instance, are happy to have the government impose regulatory barriers to entry into their market, barriers that serve to cartelize the industry. Bureaucrats are always on the lookout to expand their power and reach. Green political lobbies consider energy conservation a religious virtue regardless of economic realities and are in the business of delivering such mandates in return for contributions from the faithful. 



Everyone wins but the poor consumer who is for the most part oblivious to these regulatory machinations undertaken at his expense. The administration did us a favor by minimizing the hunk of flesh taken out of our economic hides. But it would be better to junk these standards and make the consumer, once again, king of his own pocketbook. 
"
"

Many in the energy and environmental industries thought Donald Trump’s victory in November meant certain death for the Clean Power Plan (CPP), a piece of low‐​hanging fruit in Trump’s promise to revitalize coal country. This regulation, which many argue is one of the most expensive in American history, was key to Obama’s climate legacy and, indeed, the President’s Executive Order issued this week does kill the CPP. Until, that is, the environmental activists file for a stay, which could happen any day now.



As with Trump’s promises for the revitalization of coal country, all of this will be more complicated than suggested.



Legally, the Supreme Court’s 2007 decision, _Massachusetts v. EPA_, held that if the EPA determined carbon dioxide is a pollutant causing harm to human health and welfare, then it is empowered to regulate it under the 1992 amendments of the Clean Air Act.





The conversion from coal to cleaner burning natural gas has led to the decoupling of economic growth from an increase in carbon emissions — something many said would only be possible through government coercion.



Trump’s executive order cannot call on the EPA to cease and desist from its Clean Power Plan until it somehow determines that carbon dioxide, after all, does not cause endangerment, or that the science is simply not there to show that it does. As science moves slowly, and with the federal government itself providing a vast majority of all climate science funding, this will be a difficult battle.



Undoing regulations is typically more difficult than creating them. However, the selection of Scott Pruitt, who defended the rights of Oklahomans to set their own environmental standards, shows the Trump administration is serious. While many left‐​leaning environmentalists tend to believe Pruitt is “against” the environment, the truth is that most Republicans strongly value the environment — they just wish to regulate it at a state level, where local knowledge and values can be applied. Pruitt is not an anti‐​environmental zealot; as for the EPA, he’s said “Clearly the mission of the EPA is to protect our natural resources, protecting our water quality, improving our air.”



And, as many have noted, even the elimination of the Clean Power Plan will not itself bring coal back to anything like its former life. The major reductions that the US has made in its greenhouse gas emissions stem not so much from a war on coal (indeed, the previous administration was surely belligerent toward the industry), but from the market itself.



Dramatic advances in geolocation and hydraulic fracturing have made natural gas, which only emits half as much carbon dioxide as coal when used for power generation, and the equipment used to burn it, cheaper than coal. It also burns much cleaner, so the expensive scrubbers and bag houses required to capture coal’s bad residuals are not necessary.



This conversion from coal to cleaner burning natural gas has led to the decoupling of economic growth from an increase in carbon emissions — something many said would only be possible through government coercion. Instead it was accomplished by greed and genius.



It’s hard to predict the legal fate of Mr. Trump’s latest executive order. What we do know, though, is it will be a long time before the dust settles, and unless many fundamental changes occur legally, diplomatically, and scientifically, any new administration can bring Obama’s policies back to life with a pen and a phone.
"
"

Above: Tifton, GA Sewage Treatment Plant – a good place to measure climate?
There have been some claims on the blogosphere of limited or no value to the taking of pictures for the www.surfacestations.org project. This is my view of why pictures are vitally important to an assessment of the accuracy of the near surface temperature record gathered by USHCN and other weather stations, where the data gathered is used in climate studies.
Photography is well established as a diagnostic tool in many fields. Take astronomy for example. If data and computer models of the universe is all that was needed to move the science forward, we certainly wouldn’t need the Hubble Space Telescope.
Do pictures work very well in illustrating problems that need correction?  Well I say ask any doctor who uses xrays, or MRI images, or ultrasound. Do you think doctors can define an illness solely on chart data such as BP and body temperature? No of course not, they need pictures. They DEMAND pictures.
Or how about the NASA’s loss of the space shuttle Columbia in 2004? The spacecraft is covered in sensors, yet after a photo showed foam striking the shuttle during booster burn, engineers pleaded to get photos under the wing from Department of Defense DOD. NASA Engineering made three separate requests for DOD imaging of the shuttle in orbit to get photos to determine if there was damage. NASA management did not honor the requests for DOD photos and in some cases intervened to stop the DOD from assisting.
On reentry, sensors on the shuttle started showing problems, and flight controllers struggled to understand what was happening. Photos and video taken by amateurs on the ground showed clearly what had happened. I don’t recall CNN showing pictures of sensor data in announcing this failure to the world.
Given NASA’s unwillingness to listen to engineers first with Challenger (frost and o-rings) and Columbia (possible wing damage – just get us a picture so we can be sure) I have even less respect for the NASA armchair UHI analysis called “lights = x” ironically done by counting the number of streetlights near weather stations using DOD nighttime photos. This method can give an approximation of the urbanization around a weather station, but it can’t possibly discern the nearby microsite effects like asphalt and air conditioners that have seen so far.
The worlds of science, engineering, medicine, forensics, astronomy, biology, and many more use photos to cross check gathered data or to confirm observations or theory. Climatology shall be no exception.
We are getting pictures of stations, lots of them, and we’ll get every one if possible. Then we are going to analyse them against existing published standards, and then we will publish the results of that analysis. And unlike some prominent climatologists, the pictures, the methods, the code, and the results will be publicly available to anybody, be it scientist, layman, or citizen. And, it will be done without wasting once cent of taxpayer money.
Then after that, critics can determine just how useful the pictures are.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea50af6b3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Claire O’Neill, the former UK energy minister who was to lead the UN climate talks this year in Glasgow, has been removed from the post. Her sacking comes as Boris Johnson prepares to launch the UK’s strategy for hosting November’s crunch climate talks, known as COP26.  O’Neill, under her Twitter handle of @COP26President, wrote on Friday evening: “Very sad that the role I was offered by Boris Johnson last year has now been rescinded as Whitehall ‘can’t cope’ with an indy COP unit. A shame we haven’t had one climate cabinet meeting since we formed. Wishing the COP team every blessing in the climate recovery emergency.” The dramatic last-minute change of plan follows murmurings over the past month that O’Neill, known under her previous married name of Perry when she was a minister, lacked the gravitas for one of the most important jobs in international politics this year. A source in the COP26 unit said: “Claire has seriously underperformed, including at Davos and on a recent ministerial visit to India. She had said ‘the Paris agreement is dead’ in key meetings to the surprise of everyone. “She didn’t seem to get that this is a diplomatic job. The senior team of officials in the unit couldn’t work with her and her erratic behaviour and poor performance has spooked key stakeholders in the UK and internationally. She had to go. The PM now needs to show he is taking this seriously by appointing a heavy-hitting minister.” Governments must come to COP26 prepared to scale up their commitments on cutting greenhouse gas emissions, or risk the failure of the 2015 Paris agreement on the climate. Achieving the consensus needed among world governments will be a mammoth task – many countries, including the US, Brazil, Saudi Arabia and Russia, are now hostile to the Paris agreement, and others including China and India have been unwilling to make big new public commitments under the accord. O’Neill’s previous experience peaked at junior minister, which was regarded by some diplomats as not enough to gain her the respect needed to get meetings with premiers and top officials around the world. Observers of the UN talks interpreted her removal as a sign that Johnson was taking COP26 more seriously. He will make his first public intervention on the issue next Tuesday, when he will launch the UK’s COP26 strategy, at an event with Sir David Attenborough, the climate expert Lord Stern, the outgoing governor of the Bank of England, Mark Carney, the UN’s climate chief, Patricia Espinosa, and a host of dignitaries. “A good COP president makes all the difference between success and failure,” said one former high-level diplomat and COP veteran. “They direct the negotiations, they play the key role in determining the outcome.” Tom Burke, the co-founder of the environmental group E3G, said he thought that either William Hague or Michael Howard, both former Conservative party leaders, could be possible choices to replace O’Neill. “If this is a sign that the government really wants to signal its intent to make a success of COP26, it has to appoint some more senior people, and those two candidates come to mind,” he said. The Cabinet Office, which has been leading on the COP26 talks, declined to say whether a successor had been chosen. There have been rumours among climate activists that Zac Goldsmith could take on the role, and O’Neill’s tweet appeared to suggest that responsibility for COP26 could be taken away from the Cabinet Office. The Cabinet Office put out a statement: “Claire Perry O’Neill will no longer be UK COP26 president. The prime minister is grateful to Claire for her work preparing for what will be a very successful and ambitious climate change summit in Glasgow in November. Preparations will continue at pace for the summit, and a replacement will be confirmed shortly. Going forward, this will be a ministerial role.” That could leave the way open for a House of Lords appointee. Responsibility for the UK’s participation in the annual talks was previously held by the Department for Business, Energy and Industrial Strategy, after the closure of the Department of Energy and Climate Change. Another COP expert said whoever took on the job must have the clear and public backing of the prime minister, and that Johnson must take a much stronger public role on the issue. In her parliamentary career, O’Neill was best known for having to apologise when, unable to catch the Speaker’s eye, she wondered aloud in the House of Commons to whom she needed to “give a blowjob” in order to get her say. She also issued a putdown to David Davis when he confused her with another female Tory minister, Caroline Nokes. Referring to Davis’s previous campaigning slogan, she is reported to have told him: “David, let me help you: Caroline is a C cup, I am a double D.” Her exuberant manner also led to trouble. In November 2018, three unions wrote to BEIS, where she was a minister, to raise allegations of shouting and bullying civil servants. O’Neill started out as a political radical. The environmentalist George Monbiot recalls her at Brasenose College, Oxford, where she studied geography, as “a leftwing firebrand who wanted to overthrow capitalism and nationalise the banks. She was impressive and persuasive, and had some influence on my thinking. You can imagine my disappointment when she took a City job and became a Tory.” She entered parliament with the coalition government in 2010, her no-nonsense approach belying the patronising “Cameron cutie” label. O’Neill announced her decision to resign as an MP last September, citing the pressures of the COP26 presidency, and she stood down in December’s election. That left her free to jet to foreign capitals, but outside cabinet discussions. Johnson’s government is vulnerable to charges that ministers are not prepared to make the hard decisions required against vested interests in fossil fuels and finance. There was a major hiccup at the Africa summit last week when, as Johnson pledged not to invest in coal in Africa, the Guardian drew attention to the almost £2bn finance from the UK pouring into African oil and gas."
"
Share this...FacebookTwitterDr. Knut Wittkowski, the former head of the epidemiology department at Rockerfeller University, says doing nothing would have been more effective – and ultimately cost fewer lives – than the “containment” strategy now in operation across the world.
By restricting movement and confining people in their homes we are unnecessarily prolonging or widening the curve instead of just flattening it.
The only way to eliminate any respiratory virus is not by developing vaccines or with pharmaceutical intervention, but by natural herd immunity. This means we should be allowing children to attend school.
When 80% of the population becomes infected – and the vast majority of the population won’t even know it because they won’t have symptoms – a common coronavirus like this one can be exterminated within about 4 weeks.
By trying to contain the virus, we are practically ensuring there will be a “second wave” of infections in the Northern Hemisphere fall, as not enough people will have been infected in recent months to exterminate this particular coronavirus strain.
Dr. Wittkowski asserts he is able to talk candidly about what should have been done in response to this COVID-19 outbreak because he is not paid by the government and therefore he is able to “actually do science”.

Transcribed commentary (and image) from a YouTube interview

Share this...FacebookTwitter "
"
Share this...FacebookTwitterMany of us have been noticing there’s a not-so-subtle hate-campaign against the elderly going on.
Not long ago we saw a German production which featured kids singing “my mother is old environmental scumbag“or how Brexit voters were portrayed as incontinent scum.
At a recent performance in Dortmund, Leftist hip-hop band K.I.Z. told a cheering crowd of predominantly women:
People are scared of some stupid virus. The truth is, only old white men die!”
“Corona is practically healing the planet”
Now we have a production again by ARD/ZDF German Public Television, Browser Ballet, that tops it all. The satire is titled: “Corona is rescuing the planet”.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





In the skit the moderator begins by telling viewers:
We here at Browser Ballet say ‘yes’ to Corona because this virus is practically healing the planet by itself. Interesting is how fair this virus is. It’s ravaging the elderly, but the youth are withstanding this infection almost without effort. That’s only just because it is the generation 65 and over that has run this planet into a wall over the past 50 years.”
Especially responsible, says the moderator, are the overweight and ill adult Americans, who have been recklessly practicing earth-destroying “turbo capitalism”:
Maybe the Corona virus is merely a response to turbo capitalism, and it is working. Air travel has collapsed, production has been cranked back, consumption is declining. There couldn’t be better news for this planet. Air pollution in China, thanks to Corona, has decreased in a very short time. If that continues, then we may experience a new green paradise!”
He adds:
And isn’t it the problem that there are just too many of us? Less people means less shortages of resources, which means less hunger, which means less war, and that means less causes for refugees. So, probably the Corona virus is simply a nice reflex of nature to tell people who’s the boss here. That’s why: enough with this silly egoism. Corona is here simply because we don’t deserve anything better. “


		jQuery(document).ready(function(){
			jQuery('#dd_c281537b1a33a0b9fbc65907b25ba34a').on('change', function() {
			  jQuery('#amount_c281537b1a33a0b9fbc65907b25ba34a').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterSeveral new studies use evidence from temperature-sensitive plant species and megafauna remains to reconstruct an Arctic climate that was 6°C to 22°C warmer than today when CO2 concentrations lingered near 300 ppm.
Navigating the Arctic Ocean
William Barentsz discovered Arctic Svalbard as he sailed through an open-water Arctic Ocean using a wooden boat in early June, 1596.

Image Source: Wikipedia
In September, 2019 (the month of the year with least extensive sea ice), 16 scientists needed to be rescued by helicopters because the massive ship they were using to study climate change couldn’t cut through the ice-covered waters near Svalbard.
In the 1500s, the Western Arctic was sea ice free for about 4-5 months of the year. Today – and steadily since 1800 – the Western Arctic is sea ice free only about 2 weeks of the year (Porter et al., 2019).

Image Source: Porter et al., 2019
In fact, according to Rosel et al., 2018, Arctic sea ice was actually thicker in 2015 (1.56 m) and 2017 (1.65 m) than it was in 1955 (0.94 m).

Image Source: Rosel et al., 2018
The paleoclimate Arctic record
1. Voldstad et al., 2020  A much broader distribution of thermophilous (warmth-dependent) plant species suggest the sea surface temperatures near Svalbard were as much as 6°C warmer than they are today earlier in the Holocene, which effectively means the Arctic was sea ice free throughout the year.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Image Source: Voldstad et al., 2020
2. Kirillova et al., 2020  About 115-100,000 years ago, “semi-dwarf” wooly mammoths thrived in an Arctic Siberia teeming with lakes, rivers, aquatic plants, grassy meadows, and forests. July temperatures were 8-10°C warmer than today’s. CO2 levels ranged between 260-280 ppm.

Image Source: Kirillova et al., 2020
3. Schenk et al., 2020  Plant species’ thermal tolerance limits (Finland) suggest July temperatures had already reached “near-modern” (~16°C vs. 17°C) levels 15,000 to 11,000 years ago, or during the last late glacial, when CO2 hovered near ~220 ppm CO2. By about 10,000 to 9,000 years ago, Greenland was “4-7°C warmer than today”.

Image Source: Schenk et al., 2020
4. Fedorov et al., 2020  The Eurasian Arctic was up to 8°C warmer than today during the last interglacial (130,000 to 110,000 years ago). Extensive forests lined the Arctic Ocean coast from 10,000 to 3,000 years ago, when the region was also much warmer than today. Today’s Eurasian Arctic coast is treeless tundra. The cold-preferring collared lemming has 25% more habitat now than it did earlier in the Holocene due to today’s tundra expansion and cooler climate.

Image Source: Fedorov et al., 2020
5. Rybczynski et al., 2013  Giant camels and horses were hedged by plants, forests, and wetlands in a balmy High Arctic until at least the late Pleistocene (~79k yrs ago). CO2 ranged from 190 to 280 ppm back then (Pleistocene) and averaged about 300 ppm during the Pliocene, but yet the Arctic climate was 18.3°C warmer than today.

Image Source: Rybczynski et al., 2013
6. Fletcher et al., 2019  From 2 to 4 million years ago, as CO2 pivoted around 300 ppm, the Arctic was 15-22°C warmer than it is today.

Image Source: Fletcher et al., 2019


		jQuery(document).ready(function(){
			jQuery('#dd_bf2a70f196bcfaac008638389ee41e60').on('change', function() {
			  jQuery('#amount_bf2a70f196bcfaac008638389ee41e60').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
nan
"
Share this...FacebookTwitterNesting red kite shot dead because of wind energy?
By Die kalte Sonne
(Text translated by P. Gosselin)

Red kites have little chance against wind turbines. Image: Thomas Kraft (ThKraft) – Own work, CC BY-SA 2.5
Red kites and wind power just do go well together. These predatory birds can find good prey especially where farmers mow meadows or plow fields. Lethal are cases such as the one in Baden-Württemberg, where areas with green fodder have been planted in the immediate vicinity of a wind park.
When these fields are mowed, the red kites search for food within the hay. It is ideal for them, but also possibly deadly because they cast their view downward when hunting, and not forward. The Hilpensberg wind farm was even approved in a red kite area. Now one of the beautiful animals has fallen victim again, as the Nature Conservation Initiative reports:
According to biologist Immo Vollmer, the conclusion can only be that we should not build any more wind turbines in areas where red kites nest or where buzzards often seek food. Otherwise the red kite, which has its largest distribution center in the world in Germany, will have no future here, because the loss rate is already almost in the same order of magnitude as the rate of offspring.”
And another sad case has just been reported in North Rhine-Westphalia. A female, nesting red kite was shot dead near Paderborn.
In an earlier trial, a judge even gave the controversial wind projects approval – precisely where the shot bird was found – under the condition that no protected species be proven to exist there. Now that the animal has been executed, this condition has been met. Probably just a coincidence, or maybe suicide, to make the wind turbines possible and to get out of the way?


		jQuery(document).ready(function(){
			jQuery('#dd_9aefd3a91f352f2c5132dfecb2a10391').on('change', function() {
			  jQuery('#amount_9aefd3a91f352f2c5132dfecb2a10391').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

The major criticism that East Asian officials would make of the outgoing Bush administration’s foreign policy would be Washington’s focus on the geostrategic problems in the broader Middle East in the past eight years, and the resulting sidelining of China and most of East Asia on the US global agenda. This neglect of China needs to change. 



Secretary of State Condoleezza Rice’s recent trips to South Asia (to try to defuse Indo‐​Pakistani tensions in the aftermath of the Mumbai terrorism) and to the Middle East (to attempt to re‐​energise Israeli‐​Palestinian negotiations) have been highlighted in leading US newspapers. Treasury Secretary Henry Paulson’s meetings in Beijing, as part of the ongoing Strategic Economic Dialogue, have, however, only been minor news as far as the US media is concerned. 



After president‐​elect Barack Obama recently unveiled his national security team, most of the discussion among Washington’s pundits centred on how the selection of Hillary Rodham Clinton as secretary of state and the retaining of Robert Gates as defence chief would affect US policies in the Middle East. China and East Asia were largely ignored. 



Earlier, in the foreign policy debates during the presidential election campaign, when China was mentioned, it was mostly in the context of criticising its trade policies, and warnings of its rise as a geoeconomic “threat” to US interests. 



Certainly Mr Obama needs capable people in his administration to manage the challenges in the Middle East. But he and his foreign policy aides must realise that all the major geostrategic and geoeconomic problems facing the US in the next four years, including energy policy, climate change, nuclear proliferation — and the current global economic crisis — will require co‐​operation with Beijing. 



During his discussions with officials in Beijing, Mr Paulson expressed concern that lowering the value of the yuan to stimulate the Chinese economy could worsen the US slowdown by keeping Chinese export prices relatively low and import prices high, which would hurt US exporters. In the past, US lawmakers have threatened to punish China if it refuses to change its exchange rate policies. But it is unlikely that Congress will risk a trade war now, given that America’s effort to spend itself out of recession will depend so much on the willingness of the Chinese to continue financing the US deficit. 



This dilemma highlights the need for a long‐​term strategy to manage the Sino‐​US relationship in a way that encourages China to assume greater leadership in multilateral economic organisations like the International Monetary Fund. 



During the cold war, the first question on the minds of America’s allies and rivals following the election of a new president was: “How is he going to handle Moscow?” Today, and in the future, America’s friends and adversaries should be more concerned about the approach a new White House occupant will take towards Beijing. 
"
"Britain is highly dependent on imported food. By value, imports make up more than 90% of the fruit and vegetables consumed in the UK and half of the meat. Brexit is expected to increase trade costs and make food imports more expensive, something that could lead to changes in diets and dietary risk factors that influence health. In fact, Brexit could lead to up to 5,600 diet-related deaths per year by 2027, additional healthcare expenditure of £600m, and increase the GDP losses of Brexit by up to 50%. That’s according to estimates my colleague Florian Freund and I have published in a new Oxford Martin School Working Paper. Analysing the potential implications of Brexit is a tricky business. The concrete details of Brexit remain unclear. Proposals range from various forms of “soft Brexit” that include a new trade agreement with the EU, to a “hard Brexit” in which the UK falls back on the (higher) tariffs set out by the World Trade Organization. We evaluated these opposing ends of the spectrum and compared them to a no-Brexit (“remain”) scenario. We used an agriculture-economic model to simulate the impacts that changes in tariffs and regulatory measures could have on the agricultural sector in the UK. And we used a national disease model to estimate what the resulting dietary changes would mean for mortality from chronic diseases, such as coronary heart disease or cancer.  Currently so-called “dietary risks”, including not eating enough fruits and vegetables or eating too much red and processed meat, are the second biggest risk factor for mortality in the UK, after tobacco. Our analysis suggests that Brexit could further increase those dietary risks. As a result of increasing trade costs from customs checks, new regulation, and higher tariffs in the case of a hard Brexit, we estimated that prices for most foods would increase. Foods that are critical for good health would be especially affected. Fruit and vegetable consumption could be reduced by up to one portion each per person per week under a hard Brexit, and by half a portion each under a soft Brexit. The consumption of nuts and legumes could decrease by up to 7%. Other foods would be affected as well. Dairy consumption could go down by up to one portion per week, meat consumption by half a portion, and total calorie intake could decrease as well. Some of those changes have health benefits, such as reduced intake of red meat or, for overweight people, reduced calorie intake. But we estimated that those potential benefits would be outweighed several times by the reductions in health-promoting foods. According to our estimates, the Brexit-related changes in food consumption could lead to 5,600 additional deaths per year under a hard Brexit, and to 2,700 additional deaths under a soft Brexit. This represents an increase in overall mortality of 0.9% in the Hard Brexit scenario (about 610,000 people are projected to die in the UK in 2027), and 0.4% in the Soft Brexit scenario. For premature mortality (before age 70), the increases are slightly higher. Most of the additional deaths would be due to cancer, coronary heart disease and stroke, which are associated with reduced consumption of fruits, vegetables, and nuts. The health impacts of Brexit also have economic implications. In our analysis, we quantified the economic costs of the additional diet-related deaths by summing up the extra healthcare expense, and by using valuation techniques commonly used by government when looking at the costs and benefits of projects that could affect mortality, such as new nuclear power plants or roads. Accounting for the costs to healthcare and related services resulted in increases in healthcare-related expenditure of £600m per year under a hard Brexit, and of £290m under a soft Brexit. And valuing the changes in mortality using cost-benefit analysis – based on the willingness of society to pay to reduce risks to life – led to costs close to £12 billion under a hard Brexit, and £6 billion under a soft Brexit.  The Brexit bill for health adds to the impact that leaving the EU is expected to have on other sectors which will also face higher costs of trade and production. We and others have estimated losses in real GDP (a measure of output corrected for inflation) to range from 0.5% for a soft Brexit to 1.4% for a hard Brexit (in nominal terms, that would be 1.6-3.6%). The health costs of Brexit amount to 0.3% to 0.6% in terms of real GDP, and therefore increase the overall economic losses of Brexit by 40-50%.  Given the UK’s import dependence, in particular for fruit and vegetables, any Brexit-related increase in trade costs will make it harder to get hold of foods that are critical components of healthy diets and chronic-disease prevention. Whatever form Brexit might take, our analysis suggests that it will significantly impact the British food system and negatively affect the health and welfare of British citizens."
"Bitcoin recently turned ten years old. In that time, it has proved revolutionary because it ignores the need for modern money’s institutions to verify payments. Instead, Bitcoin relies on cryptographic techniques to prove identity and authenticity. However, the price to pay for all of this innovation is a high carbon footprint, created by Bitcoin mining.   Fundamental to that mining process is a peer-to-peer network of computers, referred to as validators, who perform Proof of Work. In essence, this involves computers solving computationally-intensive cryptographic puzzles that prove blocks of transactions, which are recorded in a public asset ledger, known as a blockchain. This ledger is publicly viewable by all computers, which helps the system achieve consensus in an unreliable network of participants. Validators are called miners because the computer, or node, that successfully validates one of those blocks is rewarded with “mined” Bitcoin. Thus mining is also the process by which Bitcoin adds new coins to the network. But these processes consume a vast amount of power. In my 2016 article, Socialism and the Blockchain, I estimated Bitcoin mining’s annual energy use at 3.38 TeraWatt hours (TWh), which I equated to the total 2014 annual consumption of Jamaica. Recent estimates show the currency’s annual consumption rising exponentially, currently reaching an incredible 55TWh. Indeed, a new paper in Nature Sustainability suggests that the energy costs of mining cryptocurrencies exceed the costs of mining physical metals. Furthermore, the paper estimates that Bitcoin emitted between 3m and 13m metric tonnes CO₂ in the first half of 2018. A team in Hawaii even suppose that, if Bitcoin’s adoption continues to rise, within a couple of decades, such emissions could help push global warming above 2°C. However, both the study in Nature and the team in Hawaii make assumptions about the means of energy generation. In the light of the recent disturbing UN 1.5°C Report, humanity would be wise to act on the recommendation for an “unprecedented shift in energy systems”. The hope is that such a shift towards large-scale renewable energy does occur, thus invalidating the assumptions made in those papers.  Nevertheless, concerns over Bitcoin’s energy consumption remain, so Ethereum, another cryptocurrency, is investigating a more energy efficient consensus algorithm known as Proof of Stake. This method differs from Proof of Work because miners on this network use their economic stake to prove transactions and therefore, they are not performing energy intensive calculations. That introduces some complications – not least, how to ensure that people in this network act honestly, as they would have nothing to lose by behaving dishonestly? Ethereum’s proposed solution is to introduce penalties through measures such as penalising miners for simultaneously producing blocks on two versions of the blockchain. After all, only one of those blockchains is valid. Bitcoin’s Proof of Work overcomes such problems implicitly because it includes natural penalties since miners have to expend energy to prove transactions. In economic game theory, a Nash Equilibrium is said to be reached when a system stabilises because no one gains by changing strategy from that which produces the stable state. Since Bitcoin rewards are given to miners only if their blocks help form the valid Bitcoin blockchain, the most profitable outcome, or the Nash Equilibrium, is for each miner to act in consensus with the majority. As a result, Bitcoin’s Proof of Work algorithm has proven effective, despite the excessive energy consumption. In essence, my work looks at whether blockchains are a rebuttal to the hierarchies of capitalism. If Bitcoin promotes a way of organising that does not rely on capitalist consumption, might that indirectly drive down society’s energy use and help lessen its environmental impact? After all, consider the recent alarming WWF report, which all but blamed capitalism for the dramatic decline in wildlife populations. We need alternatives. Perhaps, then, Bitcoin’s revolutionary offer, as an alternative to capitalism, means its energy use is a price worth paying? That argument holds some weight if it drives down consumption in other areas of society because Bitcoin mining is not the primary driver behind climate change. However, even then, given the urgency of environmental degradation, if we continue to produce energy in a manner that creates so much warming CO₂, that argument may provide scant consolation.  Perhaps alternative consensus schemes, such as Ethereum’s Proof of Stake, provide part of the solution. However, Bitcoin or not, if humankind is to avoid climate catastrophe, we need to take urgent action and find solutions that produce clean, sustainable energy. If we do that, humanity will benefit, and as a by-product, so will Bitcoin."
"
I decided I’d drop some more fun with entropy your way. Here is the USHCN
station of climate record in Redding, CA GISS number # 425725920010 and used in
the climate modeling database
It is now operated by the US Forest Service at their HQ located at the
Redding Airport. It used to be operated by the National Weather Service, but
that WSFO closed in the mid 90’s.
Like Marysville, the site is surrounded by asphalt, and the surface is
unnatural – its wood chips over weedmat, and I’ll have to say it was hot as heck
to walk on during mid-day..
But the kicker is the “accessories” they’ve added for convenience of running
the hygrometer and for night observations. Yes it is another fine high-quality
USHCN climate recording site. I wonder how many times they forgot to turn off
the light? It looks like there might be room for a hot plate to keep your coffee
warm while making observations.



The blower is used to run air past the wet bulb hygrometer…its not the
correct way
to do it (manual aeration by rotation is specified).

Here is the satellite picture from Google Earth



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea62cfaea',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Think of it as a hypnotist’s trick, because that’s what it is. Scott Morrison says it over and over: there is no dispute about the need to take action to reduce Australia’s emissions. No dispute. You are getting sleepy. No dispute. In fact there is a dispute, and a serious one. By failing to do what is necessary, the government Morrison leads maintains a serious dispute with what the climate science tells us needs to be done both in Australia and internationally to avert the most dangerous risks associated with global heating.  When it comes to mitigation, Morrison’s government is in dispute with the facts. It is in dispute with the evidence. It is in dispute with the truth. Day in, day out. So despite his gritted-teeth soothing and head-patting from the podium at the National Press Club on Wednesday after a summer of calamity, there’s a dispute alright, and it’s one of the most important disputes of our time. This dispute is about the future, and how our government shapes it on our behalf. The dispute turns on whether climate change is now all about adaptation, about adjusting and adapting to the new reality – or whether Australia, as a responsible, reformist, middle power, is still at the policy and diplomatic barricades trying to avoid the worst case scenario. Dear prime minister. That’s a big dispute. Big with a capital B. And this is a dispute that every citizen of this country, and every citizen of the world, has a direct stake in. Here’s one piece of advice for all you good folks who watched Morrison’s scene setter at the press club: do not consent to having your head patted. Do not consent to the high stakes hypnotism. When the prime minister says “taking action is agreed”, do feel free to say to yourself or to anyone around you: “No, it isn’t agreed.” If so inclined, add this: “Climate action remains contested, and it remains contested because the Coalition chose to weaponise climate change at the precise moment in history when we needed to get about solving it.” If you feel on a roll at this point, add this: “And this political party still weaponises climate change against its opponents if it feels there’s an electoral advantage in doing so.” Morrison told us, blithely, on Wednesday, that solving the challenge of climate change goes beyond targets and summits. It will be driven by technology, not by taxes – a convenient sort of rationalisation – as if this was all unavoidable, inexorable, a fixed set of conditions, rather than a choice, and a choice the Coalition has been at the epicentre of. Some facts. When we had a “tax” on carbon in Australia (that wasn’t a tax, either then, or now), emissions came down, which is what the science tells us needs to happen. When the Coalition repealed that “tax” (that wasn’t a tax, then, or now), emissions crept up, which is what the science tells us is dangerous. As my colleague Adam Morton has reported countless times, national emissions peaked in 2007, the last year of the Howard government, came down each year under the Rudd and Gillard Labor governments, and have flatlined since the Coalition was elected in 2013. Reversing that positive trajectory of abatement was a choice the Coalition made. It wasn’t something that the universe imposed arbitrarily on the government, a bit of happenstance. It was a choice these people, including Morrison, made. Eyes wide open. Another fact. When Australia went to the United Nations climate talks late last year and argued we should use carryover credits from the Kyoto period – an accounting fix that means Australia will promise to reduce our emissions by 26% but in practice only reduce our emissions by about half that headline number – that was a choice too. We made a choice to do less than is necessary, hurting ourselves in the long term, and making it harder to sustain any sort of global consensus for ambitious action. These things aren’t happening because of strange, alien forces beyond our collective control. They are happening because our government is choosing to fail on climate change mitigation. I’ll say it again because it’s important, and inexplicable, no matter how long you look at the same set of facts hoping to comprehend the incomprehensible: our government is choosing to fail, and trying to make a virtue of it. Our government has access to the science, to the best advice available, and yet it continues to shirk the mitigation challenge. It continues to gamble with the future in the worst way imaginable. Morrison still has time to turn this appalling behaviour around, and there are some interesting markers if you can penetrate his various maxims and misdirections. Working with the states on bilateral agreements to reduce emissions is one hint worth watching. I suspect our prime minister is still hedging his bets and refining his thoughts. But until actions follow words, there is only one reliable measure to judge the government on. Its record."
"
Share this...FacebookTwitterScientists suggest relative sea level changes are well-correlated with natural variability and accelerated sea level rise is a “recurring feature” of what has been observed for over 300 years. Five of six studied regions along the North American Atlantic coast show declining sea level rates (mm/yr) in recent decades.
After retreating into the sea until about 1960, for the last five decades the Atlantic coast of North America has, on net, reversed course, expanding at a rate of about 5 centimeters per year (Armstrong and Lazarus, 2019).
This is likely the exact opposite of what would be expected given the reports of accelerated sea level rise for this region in recent decades.

Image Source: Armstrong and Lazarus, 2019
The lead author of a new study, Professor Roland Gehrels, has previously found much more rapid rates of sea level rise prior to 1950 than in recent decades in Southern Hemisphere locations, such as along the coasts of Tasmania and New Zealand (Gehrels et al., 2012).

Image Source: Gehrels et al., 2012
In a new study, Gehrels et al. (2020) also found rapid rates of sea level rise reaching up to 3 millimeters per year during the 1700s along the Atlantic coast of North America. He suggests “those rapid episodes of sea level rise on the north east coast of North America in the 18th Century have a natural cause”.
Interestingly, of the 6 locations chosen for the study, only 1 (Connecticut) indicates sea level rise rates have been steadily accelerating throughout the second half of the 20th century and in recent decades. The 5 others (Nova Scotia, Maine, New Jersey, North Carolina, and Viðarhólmi) all show the millimeters-per-year rates of sea level change have either not been rising or even rapidly falling.
This would not appear to be consistent with a driving anthropogenic influence in sea level rise trends since the 1950s, or since CO2 emissions have risen dramatically.

Image Source: Gehrels et al. (2020)
Share this...FacebookTwitter "
"
Share this...FacebookTwitter

War on windpark-blocking red kites?
Authorities are offering a €1000 reward for information leading to solving 11 cases of dead red kite protected birds. Nine of the deaths were due to a long-banned poison. 

Protected red kites being poisoned in north Germany. Image: Thomas Kraft (ThKraft) – Own work, CC BY-SA 2.5
The German Presseportal.de here writes that a total of eleven dead red kites have been reported to the LLUR (State Office for Agriculture, Environment and Rural Areas) since 2017 from the area south of Neumünster in northern Germany.
“Nine of these rare birds of prey died of a banned insect venom,” the Presseportal.de reported. Now the Hunting Association of Schleswig-Holstein e.V. (LJV) is offering a reward of 1,000 euros (1,100 US dollars) for information leading to solving the cases.
“Banned poison”
Authorities say “the 9 red kites died from an insect poison which had been banned for many years” and that four dead red kites with suspected poisoning have been reported since March alone.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“Three birds were found close together by a local hunter in the community of Rendswühren in the Plön district. The police departments Segeberg and Kiel have taken over the investigation,” according to the Presseportal.de.
More than half of the worldwide population lives on the territory of the Federal Republic of Germany, where wind energy proponents have been actively lobbying to build wind parks. Red kites and other protected species have ling been obstacles against the construction of wind parks.
A few days ago we reported here how one red kite had been found shot dead in an area where a wind park could be built under the condition that “no protected species be proven to exist there”.
1000-euro reward
In 2008, authorities and wildlife clubs jointly signed the Kiel Declaration on the Protection of Birds of Prey, under which the costs of examining dead birds of prey can be borne by the state,” the Presseportal reports. “Anyone who finds a dead bird of prey where the circumstances of discovery indicate an illegal act is requested to contact the LLUR (+49) 4347-704-0 or the Lower Nature Conservation Authority of the respective district.”
The Presseportal.de also informs that persons may also contact the Landesjagdverband Schleswig-Holstein e.V., which is offering a reward of 1000 euros for information that would lead to solving the cases. Evidence from citizens are accepted under (+49) 4551/884-0 (Police Bad Segeberg UVS) or (+49) 431/160-1503 (Police Kiel UVS).
Your contact at the Landesjagdverband Schleswig-Holstein e.V (State hunting association of Schleswig-Holstein e.V.) is
located at Böhnhusener Weg 6, 24220 Flintbek. Telephone: (+49) 4347-9087-0.


		jQuery(document).ready(function(){
			jQuery('#dd_25ae9455058c240ae34b8e284e82d5e9').on('change', function() {
			  jQuery('#amount_25ae9455058c240ae34b8e284e82d5e9').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000


Share this...FacebookTwitter "
"Ecosystems will continue to collapse around the world unless humanity listens to the expertise of indigenous communities on how to live alongside nature, a prominent Amazon leader has warned. Tuntiak Katan of the Ecuadorian Shuar people, who is vice-president of the pan-Amazon organisation representing communities in the river basin, said governments were spending millions of dollars on environmental consultants while largely ignoring the land management skills of the planet’s indigenous people that could help combat the climate crisis and biodiversity loss.  Speaking to the Guardian from the Ecuadorian Amazon, Katan, who became the first indigenous representative at a UN climate action summit last year, said environmental “catastrophes” such as the fires that devastated the world’s largest rainforest in 2019 would continue unless the contributions and human rights of indigenous people were respected. Indigenous communities support around 80% of the planet’s biodiversity despite accounting for less than one twentieth of the human population, according to the World Bank. Katan’s warning came as a new study revealed that parts of the Amazon rainforest under the stewardship of indigenous peoples sequester carbon better than areas with little protection, leading to less deforestation and degradation. “We are the defenders of nature, of the life of the forests, of our territories,” said Katan, vice-president of Coordinator of the Indigenous Organizations of the Amazon River Basin (Coica). “The world is investing lots of money to implement public policy to combat climate change, help conservation and restoration. But these policies are made in offices by technical experts with little or no knowledge of the Earth.” Biodiversity loss was named as the third biggest risk to the world in terms of likelihood and severity this year by the World Economic Forum, ahead of terror attacks, infectious diseases and interstate conflict. Despite the concerns expressed by the global elite in Davos, there was no indigenous representation at last week’s forum in the Swiss ski resort, according to Katan. He said he would welcome the opportunity to attend next year’s forum to outline an indigenous economic model based on maintaining the health of the world’s soils, rivers and the forest. “If the proposals, knowledge and management practices of indigenous people are not listened to, there will be more big catastrophes. The issue of fires in the Amazon will continue, the degradation of forests and water will continue, deforestation will continue,” Katan added. This month, the UN unveiled the draft of a Paris-style agreement on nature calling for a commitment to protect at least 30% of the planet, dramatically reduce pollution and promote the participation and practices of indigenous people. Katan said: “We are well-coordinated with our brothers and sisters from Indonesia, the Congo, communities in the Arctic and from the Pacific. We’ve been discussing issues with our brothers and sisters from all parts of the world. “In Indonesia, for example, they also have a lot of knowledge about how to manage tropical forests. But the same story is being repeated here as in other parts of the world: the lack of recognition of their knowledge and the lack of respect for the human rights of indigenous populations.” Anti-indigenous sentiment is increasing in some parts of the planet. This month activists said they would sue Brazil’s far-right president Jair Bolsonaro for his latest racist comments in which he questioned the humanity of indigenous communities. In one of his weekly Facebook broadcasts, Bolsonaro declared: “Indians are undoubtedly changing … They are increasingly becoming human beings just like us.” The new study from the Woods Hole Research Center in Falmouth, Massachusetts, found that between 2003 and 2016, 90% of net emissions came from outside protected lands in the Amazon. Scientist and lead author Wayne Walker said: “Our work shows that forests under the stewardship of indigenous peoples and local communities continue to have better carbon outcomes than lands lacking protection, meaning that their role is critical and must be strengthened if Amazon basin countries are to succeed in maintaining this globally important resource, while also achieving their commitments under the Paris Climate Agreement.” The findings add weight to the recommendations of a report on land use and the climate crisis by the Intergovernmental Panel on Climate Change (IPCC), which found that areas held or managed by indigenous peoples had much less human impact on the environment. The report also highlighted the lack of consideration of indigenous views and knowledge in understanding large regions and ecosystems. Find more age of extinction coverage here, and follow biodiversity reporters Phoebe Weston and Patrick Greenfield on Twitter for all the latest news and features"
"The bus company Greyhound Australia has ruled out any extension of work on the controversial Adani coal project after a backlash from climate change campaigners. On Sunday the SchoolStrike4Climate group launched a campaign to boycott travel with the company until it publicly ruled out working on the mine.  Guardian Australia revealed last week that Greyhound had written to staff warning they could be caught “in the crossfire” of anti-Adani campaigners after the company took a three-month contract at the coal project, with an option to extend. The Indian-owned Adani mine and railway project is the first to begin work to extract the vast coal reserves of Queensland’s Galilee basin. Greyhound is providing transport to workers for the construction company BMD, which is building the railway to take the coal to Adani’s Abbot Point port. In a statement, Greyhound Australia said it had “received numerous messages, emails and phone calls from people expressing their thoughts both for and against the Carmichael Rail Network and Adani Carmichael project”. It said: “Following considered deliberation, and in the best interests of our staff, customers, and partners, Greyhound Australia has decided to not enter into a contractual agreement with BMD to service construction of the Carmichael Rail Network beyond our preliminary 31 March 2020 commitment.” The company declined to comment further. Within hours of Guardian Australia’s report on the contract, the conservation group Citizens of the Great Barrier Reef Foundation announced it had terminated a partnership with Greyhound. Greyhound’s chief executive, Alex de Waal, also resigned as chairman of the foundation. #STATEMENT from Greyhound Australia re. Carmichael Rail Network. pic.twitter.com/wW2WRjIRnI Varsha Yajman, a spokesperson for SchoolStrike4Climate, said the group had launched a campaign to boycott Greyhound buses on Sunday. “We thank Greyhound for not throwing young people under a bus by continuing to help Adani build their climate-wrecking coalmine,” she said. After a summer of “bushfires and heatwaves”, she said, Greyhound’s decision had given her hope. “It shows that we can push companies to be part of the solution to climate change and consider the impact of their actions. “Thanks to Greyhound listening to young people, students can now still go to school camps and on excursions by Greyhound buses.” The climate activist group Galilee Blockade told Guardian Australia it had now cancelled a planned protest targeting Greyhound Australia at Brisbane’s Roma Street travel interchange on Wednesday. Ben Pennings, spokesman for Galilee Blockade, said: “Greyhound took a stupid risk but quickly saw sense. Most Australians don’t want the Adani mine and every single company with a retail brand has listened to their customers and dumped Adani. “We’re already experiencing climate chaos and corporations simply have to take heed of an angry public increasingly willing to risk legal sanction for a liveable climate.” In January the German technology company Siemens said it would honour its Adani contract after it reviewed a reported $30m contract with the project. The engineering firm GHD ended a 10-year association with Adani in December after that company was also targeted by campaigners. Julien Vincent, the executive director of Market Forces, a group that tracks the relationships between corporations and fossil fuel industries, told Guardian Australia: “As Greyhound have discovered, and Siemens are in the middle of discovering, Adani’s climate-wrecking mine is a surefire way to destroy your reputation. “Now we need the likes of Siemens to realise what an appalling and ill-informed decision they have made to work on Carmichael and also pull the pin on their relationship with Adani.” A BMD spokesperson said in a statement: “Our contracts with clients and subcontractors are commercial in confidence and, as such, we do not disclose them publicly.”"
"

 _ **Editor’s note**_ _: In 2014, Cato released_A Dangerous World? Threat Perception and U.S. National Security, _an edited volume of papers originally presented ata Cato conference the previous year. In each chapter, experts on international security assessed, and put in context, the supposed dangers to American security, from nuclear proliferation and a rising China, to terrorism and climate change._



 _As part of ourProject on Threat Inflation, Cato is republishing each chapter in an easily readable online format. Even six years after its publication, much of the book remains relevant. Policymakers and influencers continue to tout a dizzying range of threats, and Americans are still afraid. We invited each author to revisit their arguments and offer a few new observations in light of recent events. The first of these, by Brendan Rittenhouse Green, appeared ___here__ _last week._



 _Paul R. Pillar, a n_ _on‐​resident senior fellow at the Center for Security Studies of Georgetown University, and a non‐​resident fellow of the Quincy Institute for Responsible Statecraft, provides his thoughts below. His reflections on hischapter are informed by his 28‐​year career in the U.S. intelligence community, and his voluminous writing and research, including his most recent book, _Why America Misunderstands the World: National Experience and Roots of Misperception _(Columbia University Press, 2016), whichhe discussed at Cato in late 2016. _



—–



Prevailing American thinking about substate threats—and more specifically the thinking that shapes U.S. policy—exhibits at least as much of a disconnect between perception and reality as when _A Dangerous World?_ was published six years ago. The policy players and their principal bugbears have changed, but broader patterns my earlier essay identified persist. Perhaps the most glaring demonstration of this persistence is the continued presence of U.S. troops in Afghanistan—more than eighteen years after the original intervention, in what has become America’s longest war. A major impediment to withdrawing those troops continues to be the notion of Afghanistan as a unique “safe haven” for terrorists who, because of that haven, are supposedly more likely than they otherwise would be to inflict harm on Americans. The result is an interminable military expedition that in important respects is doing more harm than good.



The evolution of international terrorism during the last six years has challenged other common but flawed thought patterns about terrorism. The biggest development in that evolution has been the rise and, as a territorial entity, fall of the Islamic State or ISIS. This group’s split from, and competition with, Al Qaeda underscore the error of the earlier tendency to treat violent Sunni radicalism as monolithic, with the accompanying habit of applying the label “Al Qaeda” to the whole phenomenon. ISIS’s history also further refutes the thinking about terrorist safe havens. When ISIS had its mini‐​state in Iraq and Syria, it was focused primarily on running and maintaining that entity and less focused on international terrorism than it has been when lacking such a territory.



The Trump administration appears to have centered its threat perceptions more on states than on substate phenomena. Nonetheless, its foreign policies demonstrate some of the patterns identified in the earlier essay, including the tendency to divide the perceived world simplistically into competing camps of good guys and bad guys. A prime example is the administration’s idea of a NATO‐​like security alliance in the Middle East that would unite the United States, Israel, and some favored Arab states against a presumed bad guys’ bloc led by Iran. Nonstate actors such as Lebanese Hezbollah, the Houthi movement in Yemen, and some militias in Iraq are placed in the bad guys’ camp because of their association with Iran. The idea hasn’t gotten anywhere partly because it does not correspond to the more complicated lines of conflict and competition in the Middle East.



The administration’s obsession with Iran also illustrates a corollary to a pattern the earlier essay identified regarding perceptions of revolutionary violence and regime change. The pattern is the habitual assumption that regime change in any state the United States currently considers a friend or ally is assumed to be a threat to the United States. The corollary is that any regime change in a state the United States considers an adversary is assumed to be good. Thus, the Trump administration presses on with its “maximum pressure” campaign against Iran, which, in the absence of feasible demands or constructive diplomacy, can only be aimed at collapse of the current Iranian regime. It presses on—and in so doing raises the risk of escalation to a wider war—oblivious to the likelihood that a replacement regime, such as a Revolutionary Guard dictatorship, would be even worse than what Iran has now.



Now the United States and the world are confronting a nonstate threat, in the form of the COVID-19 pandemic, that is inflicting death and damage orders of magnitude beyond what was ever inflicted by the substate actors that for years have been the focus of American threat perceptions. Unlike with, say, terrorism, there certainly has been no problem of previously prevailing threat perceptions exceeding the reality. With terrorism, more sober voices have had to point out that in most years more Americans drown in bathtubs than fall victim to terrorism. Even after an outlier event such as 9/11, the casualties have been many times fewer than, say, the number of Americans who die in traffic accidents. But in only a couple of months, COVID-19 has left bathtub drownings in the dust and has killed more Americans than a year’s worth of traffic deaths.



One pattern applicable to other nonstate threats that does apply to the current pandemic is the tendency—a characteristically American tendency—to overstate the newness of a threat. The novel coronavirus may be novel in terms of virology, but infectious disease epidemics certainly are not. Plagues go back to ancient times. A failure to think in such terms is one factor underlying the inadequacy of preparations to deal with the likes of COVID-19.



Some of the U.S. responses to COVID-19 can be attributed to Trump’s habits, such as the flagellation of China as a way to deflect blame and attention away from the administration’s performance. But a more general American tendency is in play as well. COVID-19 is a nonstate threat, but it also is a nonhuman threat. As such, it does not conform well with the way Americans habitually think of their _bêtes noires_. Americans have long looked for monsters to destroy, but they expect the monster to have a face, in the form of a loathed leader, regime, or substate group. They have difficulty thinking ahead about meeting faceless threats such as a disease or a changing climate.



This is one reason to temper silver‐​lining hopes that the pandemic will get people and their government to think more about threats that are most likely to kill them and less about foreign regimes or groups that are unlikely to do so. Just look at how the Trump administration has continued with its maximum pressure campaign against Iran. As thoughtful and expert observers on both sides of the Atlantic have observed, any nation’s inability to get the virus under control impedes efforts to contain the pandemic globally and thus threatens other nations’ citizens. A prudent step, therefore, would be to ease the U.S. sanctions that are impairing Iran’s ability to contain COVID-19. At a time when tens of thousands of American deaths ought to make control of the pandemic an overriding priority, the Trump administration ignores this advice.



- Paul Pillar, Washington, DC
"
"

Last week, the _New York Times_ delivered the worrisome news that a team of scientists has concluded that maximum hurricane winds will increase 6 percent by the 2080’s, thanks to global warming. I was very upset to read that news, but not because I’m afraid my great‐​grandchildren will get blown away. My concern is what those scientists’ work says about the state of climate science.



The researchers reached their conclusions using a series of climate models called General Circulation Models. They assumed that the concentration of atmospheric carbon dioxide–the main global warming gas–will increase by 1 percent per year, compounded yearly. That would warm the ocean, which would create slightly stronger storms.



But there’s a problem: Any atmospheric scientist who is worth his or her salt knows that atmospheric carbon dioxide is not increasing at that rate and has not been doing so for decades. And that makes a real difference in the modeling results.



The increase has been about four‐​tenths of a percent per year, averaged over the last 30 years–not 1 percent. Charitably, throw in another tenth of a percent because of other human “greenhouse” emissions (though the two major ones, chlorofluorocarbons and methane, are declining or holding steady). That means that the researchers’ models are envisioning twice the actual increase in carbon dioxide as has been occurring for decades. 



The reason that carbon dioxide is growing so slowly is because the world is gradually becoming more energy‐​efficient as its people become more affluent. That results in both a reduction in per‐​capita emissions and a reduction in the number of “capits” that are born, as rich folks have fewer kids. Among big countries, this trend started in the United States. It is now spreading globally as the enriching world buys more‐​efficient cars and power plants.



This trend isn’t going to change anytime soon. That means the growth rate in carbon dioxide over the next few decades is likely to be the same as the rate for the last few. Using the more realistic rate delays the time that hurricane winds will increase by 6 percent from the 2080’s to the 2180’s–175 years from now.



And it’s pretty hard to speculate what impact humanity will have on nature over nearly two centuries in time. To understand that, let’s go backwards in time 175 years, to 1830, and think about the changes in energy and technology that have occurred since then.



The fact is that, just as folks in 1830 could not possibly imagine the many technological changes we have today (cars, planes, rockets, nuclear bombs, computers and Viagra come to mind), so can we have absolutely no vision of 2185. The only reasonable bet is that it will be dramatically different than today, and our fossil fuel‐​powered society will seem as remote in the future as one driven by horses and slavery seems remote to us today. So why would anyone make a prediction of what effects humanity will have on the environment some two centuries from now, based on what we’re doing today?



Or, in the case of the researchers’ exaggerated percentage increase in carbon dioxide, what we’re not doing today? That leads to an interesting question: Because carbon dioxide increases have been bouncing around four‐​tenths of a percent per year for three decades, why do climate modelers insist on using the wrong number? It seems peculiar that people who have the equivalent of doctorates in applied physics (which is what climate science is) would somehow be perfectly happy to do something they know is wrong.



I began asking that question at scientific meetings a decade ago. At that time, I asked Kevin Trenberth, a highly visible atmospheric scientist from the U.S. National Center for Atmospheric Research, who often testifies to Congress on climate issues. He told me it was done because it was “convention.” 



That answer doesn’t set well with me, because it’s awfully easy to program a computer to increase a variable by half a percent instead of 1 percent per year.



That leads to the final, nagging question. There are literally hundreds of scientific papers out there in which climate models use this wrong number. Each of those papers gets sent to three outside peer‐​reviewers. The fact that 1 percent continues to be used only means one thing: when it comes to global warming, hundreds of scientists must prefer convention to truth.



But why? Is it because, when the real numbers are put in, there’s no story for the _New York Times_ to report? 
"
"

Here are some notes on the tax proposals in the new federal budget: (See Table S-6; All figures are 10‐​year totals) 
"
"It’s been a good year for apples. Across Europe the apple harvest is the biggest it has been for a decade. But the handful of apple types you see on supermarket shelves only tells part of the story. There are actually 7,500 varieties of eating apple grown all over the world, and growers and scientists are making efforts to conserve and extend this. Many people will have heard the story of Granny Smith apples, every one of which can reportedly be traced back to a single seedling plant found growing in Australia in 1868. Though not all plants have found the fame that this crisp green apple has, there are numerous varieties that are – like the Granny Smith once was – peculiar to a local region and rarely, if ever, grown elsewhere. The UK has more than 3,600 registered apple varieties recorded in the National Fruit Collection (NFC). Though it was once thought that 200 types were being grown in Wales, only about 50 (with investigations ongoing) are known to exist today. That this number is not lower is thanks to the pioneering endeavours of the likes of nurseryman Ian Sturrock, who began propogating the now world-famous Afal Enlli – along with other rare Welsh heritage apple trees – after its rediscovery in 1998. All Afal Enlli trees now sold come from one tree that may have been cultivated by generations of monks who lived on Bardsey Island at the tip of the Llyn Peninsula around 1,000 years ago.  Sometimes it is easy for experts to identify a type of apple as being an existing variety, or even a new variety never seen before, when they have a very distinct look. But that can’t always be done. So modern researchers have been turning to DNA profiling technology, similar to that used by forensic scientists. DNA profiling has become an essential tool for characterising the genetic diversity of apples and shaping collection strategies. The technique works by identifying small sections of DNA called simple sequence repeats (SSRs). These stretches of DNA do not code for genes, but the number of repeats within them varies between individuals. By analysing a number of SSRs, a unique “fingerprint” for each individual can be built up. These fingerprints are then compared to the profiles in the NFC database and either matched to an existing variety or, when there is no match, we can be sure it has never been characterised before and is possibly a newly discovered or rediscovered variety.  This is precisely what the Welsh Perry and Cider Society’s Jayne Hunt has been doing, as part of the first concerted effort to identify and conserve old unknown varieties of apple and pear trees growing in Wales. Hunt’s team extracted DNA from hundreds of apple leaves collected from derelict orchards throughout Wales, created a genetic profile for each tree, and compared them with the NFC database.  Though the work did identify previously declared Welsh Heritage varieties as having pre-existing duplicates in the NFC database (enabling current collections to be rationalised), the results have overall been fascinating and many unique trees have been found. Of course, there is a proviso. Just because a variety is declared unique, it doesn’t make it useful and worth conserving. The nature of breeding means that every seedling grown from a pip will be unique, combining characteristics from the mother tree and its pollinator. Often the seedlings are but poor reflections of the parent trees. This is why another part of Hunt’s project is really important: histories and anecdotes from farmers, growers and members of the public have been documented, to share their intimate knowledge of the trees and their uses. With these verbal historical records and further investigation of the variety’s properties it is possible to determine if the newly discovered or rediscovered apple variety is a real gem worth conserving. At Aberystwyth University, we are currently planning extension projects to take this work further. We have been sampling trees from derelict orchards on university land that are at least 60 years old. Our own DNA profiling has found that, by and large, the trees are existing varieties popular at the time of planting and were most likely obtained from the catalogues of English nurseries – varieties such as Bramley’s Seedling, Cox’s Orange Pippin and Blenheim Orange and then the slightly more unusual Allington Pippin and Lady Sudeley. These were likely to have been planted simply because they were popular at the time and not necessarily because they were suited to the local weather conditions.  But two trees have been found to be unique and we are currently evaluating their properties. Whether their existence lies in their specific adaptation to the local climate and aspect, or is more due to random chance, we don’t really know – but they certainly extend the genetic diversity of our global resource. Never has it been more important to preserve our crop genetic diversity, not only for our increased pleasure but, more pressingly, to provide a reliable and economically sustainable source of food in the face of climate change.  And maybe one of the newly identified Welsh varieties will become a global success like the Granny Smith."
"
Russ Steele is out on vacation and doing several surveys while traveling. This one below is from St. George, UT. Here we see an MMTS measuring the temperature near the surface of an elevated parking lot. The effect of the asphalt and vehicles that park near it, engine forward, probably dwarfs the effect of the nearby a/c unit. The shading may help daytime temps some, but the asphalt likely biases Tmin the most. The complete photo survey is available on surfacestations.org




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4b945b5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Ameth Diagne points to a single tree submerged in the ocean. It is barely visible from the patch of land where he is standing, 50 metres away. The few branches emerging from the water mark the place where he proposed to his wife 35 years earlier. It used to be the town square of Doun Baba Dieye, a vibrant fishing community on the outskirts of Saint-Louis in northern Senegal. The village has been wiped off the map, with only the tree and crumbling walls of an abandoned school remaining as testament to its existence. Everything else is 1.5 metres under water.  “This was home. I was born here. Everything which was important to me happened here,” says Ameth, the former village chief. Doun Baba Dieye is in the southern part of Langue de Barbarie, a thin, sandy strip of land protecting Saint-Louis, former colonial capital of Senegal, from the ocean. Saint-Louis, a city of 230,000 and a Unesco world heritage site, is nestled between the mouth of the Senegal river and the Atlantic. The French chose Saint-Louis as the capital because of its strategic location, which allowed the city to flourish in colonial times. But today the “Venice of Africa” is being eaten up by the rising waters. Crossing the Faidherbe bridge, which connects the colourful city centre to the mainland, it seems as if you can almost touch the water. This state of a permanent flood alert has become the city’s new normal. In Saint-Louis, the consequences of climate crisis are tangible: thousands of people uprooted; houses destroyed; hundreds of children attending classes in the evening instead of in the morning because their school has been swept into the ocean. The World Bank, which recently allotted €24m (£20m) to combat the effects of climate change in Saint-Louis, estimates that 10,000 people in the city are either already displaced or live within 20 metres of the waterline, the high-risk zone. And this is just the beginning. According to a study commissioned by the Senegalese government, 80% of Saint-Louis territory will be at risk of flooding by 2080, and 150,000 people will have to relocate. Most of west Africa’s coastal cities, home to 105 million people, face a similar threat. Mangone Diagné, from the regional division of Senegal’s environment ministry, puts it bluntly: “Saint-Louis is surrounded by water and is incredibly vulnerable to climate change. But the damage was caused both by nature and by men.” He is referring to an engineering mistake, which contributed to the deterioration of the Langue de Barbarie. In 2003, heavy rainfall caused the Senegal river to rise rapidly, putting Saint-Louis at risk of flooding. As a quick fix, local government dug a four-metre-wide breach, or canal, cutting through the Langue de Barbarie. The effect has been the opposite of the one intended. Although at first the river level dropped, the breach quickly started to expand. It is now 6km wide and has cut off part of the peninsula, turning it into an island – and flooding Doun Baba Dieye. It has also upset the delicate balance of the local ecosystem. The canal brought seawater into the river, increasing its salinity level. This has affected the population of rare bird species and river fish – forcing fishermen to venture into Mauritanian waters, which is dangerous and illegal – as well as wiping out the coconut trees and mangroves that once protected the shores. Local crops, already destabilised by irregular rainy seasons and sand storms, were further damaged. Ameth, along with more than 800 former inhabitants of Doun Baba Dieye, are among the victims forced to move away from the peninsula without any support from the state. “We are fishermen, we know the ocean, so we left at the good moment – and nobody got hurt,” says Ameth, before correcting himself: “We were fishermen.” Since his community was dispersed, Ameth’s fishing boat, a pirogue, remains largely unused: “We live too far from the ocean now. I was not only deprived of my home, but also of my livelihood.” Inhabitants of the Langue de Barbarie are from the Lebou ethnic group who have been fishermen for centuries. Boys learn the necessary skills from an early age. But Ameth wants his sons to have an education: three of them are in primary school and two are at university. “For us Lebou, the ocean is our lifeblood. But it is hard to make a living as a fisherman today. This is why so many young men from our community are migrating. I want my sons to have a choice.” North of the Langue de Barbarie peninsula, the fishing district of Guet N’dar was hit by a 4-metre wave one night in September 2017. The ensuing flood affected more than 100 houses, a school, a mosque, and part of the cemetery. The homeless were put in a makeshift camp next to the airport. The ocean eventually receded, leaving a scene of devastation. Against the backdrop of crumbling walls and half-wrecked buildings, French president Emmanuel Macron visited in February 2018 and pledged an extra €15m to build a sea wall to protect the remaining infrastructure. But by April of that year, a part of the sea wall had collapsed, letting the ocean take over. Ameth is counting neither on the World Bank, nor on Macron. Thanks to financing from the UN, he returns once a year to the place where his old village stood, and plants trees. He hopes that mangroves and filaos, an exotic species of pine, can stop the destruction of the shoreline. “Although we had to move physically, my mind and spirit stayed here. And I hope one day I can move back,” he says."
"

Sen. Dianne Feinstein has introduced the Bot Disclosure and Accountability Act, a proposal to regulate social media bots in a roundabout fashion. The bill has several shortcomings.   
  
  
Automation of social media use exists on a continuum, from simple software that allows users to schedule posts throughout the day, to programs that scrape and share information about concert ticket availability, or automatically respond to climate change skeptics. Bots may provide useful services, or flood popular topics with nonsense statements in an effort to derail debate. They often behave differently across different social media platforms; Reddit bots serve different functions than Twitter bots.   
  
  
What level of automation renders a social media account a bot? Sen. Feinstein isn’t sure, so she’s relinquishing that responsibility to the Federal Trade Commission:   




The term ‘‘automated software program or process intended to impersonate or replicate human activity online’’ has the meaning given the term by the [Federal Trade] Commission



If Congress wants to attempt to regulate Americans’ use of social media management software, they should do so themselves. Instead, they would hand the hard and controversial work of defining a bot to the FTC, dodging democratic accountability in the process. Moreover, the bill demands that the FTC define bots “broadly enough so that the definition is not limited to current technology”, virtually guaranteeing initial overbreadth.   
  
  
While the responsibility of defining bots is improperly passed to the FTC, the enforcement of Feinstein’s proposed bot disclosure regulations is accomplished through a further, even less desirable delegation. The Bot Disclosure and Accountability Act compels social media firms to adopt policies requiring the operators of automated accounts to “provide clear and conspicuous notice of the automated program.” Platforms would need to continually “identify, assess, and verify whether the activity of any user of the social media website is conducted by an automated software program”, and “remove posts, images, or any other online activity” of users that fail to disclose their use of automated account management software. Failure to reasonably follow this rubric is to be considered an unfair or deceptive trade practice.   
  
  
This grossly infringes on the ability of private firms, from social media giants like Facebook to local newspapers that solicit readers’ comments, to manage their digital real‐​estate as they see fit, while tipping the balance of private content moderation against free expression. Social media firms already work to limit the malicious use of bots on their platforms, but no method of bot‐​identification is foolproof. If failure to flag or remove automated accounts is met with FTC censure, social media firms will be artificially incentivized to remove more than necessary.   
  
  
The bill also separately, and more stringently, regulates automation in social media use by political campaigns, PACs, and labor unions. No candidate or political party may make any use of bots, however the FTC defines the term, while political action committees and labor unions are prohibited from using or purchasing automated posting software to disseminate messages advocating for the election of any specific candidate. It is as if Congress banned parties and groups from using megaphones at rallies. Would that prohibition reduce political speech? No doubt it would. How then can the prohibitions in this bill comport with the constitutional demand to make no law abridging the freedom of speech? They cannot.   
  
  
Feinstein’s bill attempts to automate the process of regulating social media bots. In doing so, it dodges the difficult questions that attend regulation, like what, exactly, should be regulated, and foists the burden of enforcement on a collection of private firms ill‐​equipped to integrate congressional mandates into their content moderation processes. Automation may provide for the efficient delivery of many services, but regulation is not among them. Most importantly, the bill does not simply limit spending on bots. It _prohibits_ political (and only political) speech by banning the use of an instrument for speaking to the public. Online bots may worry Americans, but this blanket prohibition of speech should worry us more.
"
"Roland Barthes proclaimed the death of the author in 1967, arguing that once a text is produced it is an independent entity to be interpreted and understood by the audience without the author’s intentions, idiosyncrasies and personal history getting in the way. The Journal of Controversial Ideas is Barthes’ idea made manifest – it proposes to allow academics to publish papers on controversial topics under a pseudonym. The hope is that this will allow researchers to write freely on controversial topics without the danger of social disapproval or threats. Thus the journal removes the author’s motivations, conflicts of interests and worldview from the presentation of a potentially controversial idea. This proposal heralds the death of the academic author – and, unlike Barthes, we think believe this is a bad thing. First, we need to distinguish between anonymous and pseudonymous authorship: a paper is anonymous when it does not list a name, and it is pseudonymous when it lists a name which is not the author’s given name. Both practices have long histories in academic research. The Philosophical Transactions of the Royal Society – the world’s longest-running scientific journal – was initially published without the names of researchers who carried out the experiments. It was only after the development of the legal institution of authorship in the 17th century that named authors become the norm. The Victorian bestseller, Vestiges of the Natural History of Creation, which put forward an early version of evolutionary theory, was initially published anonymously. Its readers had to wait 40 years and 12 editions to discover that it was written by Robert Chambers. Malthus’s An Essay on the Principle of Population, which develops his theory of population growth, was also first published anonymously. More recently, there are some notable examples of pseudonymous authorship. Starting in 1939, a rotating group of mathematicians have used the collective pseudonym “Nicolas Bourbaki” to publish the ongoing Elements of Mathematics series, which has 11 volumes published over 70 years. The Polymath project, which crowdsources solutions to mathematical problems, has also published some of its papers pseudonymously, under “D.H.J Polymath” – the initials standing for the density Hales-Jewett theorem, the first problem the project worked on. Other examples include the philosopher David Lewis, who published a response to one of his own papers under the nom de plume Bruce Le Catt. Indeed, just this year, three academics in the US had seven papers, written under pseudonyms or borrowed identities, accepted at various humanities journals as part of an elaborate hoax. In many cases, the identity of these writers was an open secret at publication – everyone knows who is behind the Polymath project, for example. Even when the intention is to conceal, the writer’s identity is typically revealed. How best to assign authorship is currently an open question. Issues such as ghost authorship, hyperauthorship, and hyper-productive researchers all challenge traditional notions of authorship – and there are numerous revisionary proposals in the air. The Journal of Controversial Ideas should be understood as part of a conversation about what authorial practices are best suited to the aims of academic inquiry. To consider the viability of pseudonymous authorship, we need to think about what the point of having authors is. One important reason for a researcher to attach their name to a paper is to enable them to claim the credit for the paper. Although details are sketchy, the editors of the proposed journal seem to have planned a system that would allow writers to claim ownership of papers for the purposes of hiring and promotions.  Likewise, writers may also choose to conceal their papers for the purposes of hiring and promotion, if not claiming ownership would be advantageous to their careers. For the reader of a paper, attaching authors to papers is important to help them decide how seriously to take the results. Here the difference between anonymous and pseudonymous authorship becomes important: if an author uses the same pseudonym over a period of time, the academic community can begin to get a sense of how good their work is (consider the Bourbaki pseudonym, which has been in use long enough to get a track-record), but if a publication is anonymous, the audience must rely solely on the credibility of the publishing journal and its editors. But the most important function of having authors is to facilitate responsible publishing. If the 1998 Lancet paper linking the MMR vaccine to autism had not listed Andrew Wakefield as its lead author, it would not have been possible to hold him to account for producing fraudulent work, or for contributing to a dangerous anti-vaccination narrative. Authorial responsibility has both an intellectual and a moral flavour: we want to hold people responsible both for producing shoddy research, and for the moral consequences of their publications. Holding authors responsible functions as part of academic quality control. If researchers know that producing bad work has social and career consequences, this incentivises more careful and diligent work. Similarly, holding authors morally responsible motivates a healthy degree of caution on topics which might cause real harm. We worry that pseudonymous authorship is likely to lead to a problematic asymmetry between praise and blame. If the The Journal of Controversial Ideas can reliably keep researchers’ identities secret – and we have our doubts, given the history of pseudonymous authorship – researchers will be able to publish papers, claim them if they get a positive reaction, and disown them if they do not.  Perhaps the journal’s interest in establishing a good academic reputation will lead them to put in place reviewing procedures to ensure that only high-quality work is published, but their interest in controversy suggests that the journal may be much more lax when it comes to moral responsibility. We worry that The Journal of Controversial Ideas will respond to moral criticism by pointing out that their mission is precisely to publish controversial and dangerous ideas. Defenders of The Journal of Controversial Ideas see it as a forum for true academic freedom. While academic freedom is important, it is not an unlimited right. Freedom without responsibility is recklessness. It is a lack of regard for the danger or consequences of one’s ideas. True academic freedom does not mean that writers get to choose when to avoid controversy. The pseudonymous authorship proposal allows authors to manipulate the credit and blame systems of the academy in the name of academic freedom. When it is working well, academic inquiry is a conversation. Researchers make claims and counterclaims, exchange reasons, and work together to open up new fields of inquiry. A conversation needs speakers: we need to keep track of who is talking, what they have said before, and who they are talking to. Pseudonymous authorship is an opt-out from the conversation, and the academic community will be worse off if its members no longer want to engage in intellectual conversation."
"
Share this...FacebookTwitterGreenland’s largest glacier (Jakobshavn) has quite abruptly thickened since 2016. The thickening has been so profound the ice elevations are nearly back to 2010-2011 levels. The nearby ocean has cooled ~1.5°C – a return to 1980s-era temperatures.
The world’s glaciers have not been following along with the CO2-driven catastrophic melting narrative.
Alaska

For example, in a study of 50 Alaskan glaciers for the warming period between 1972-2012, researchers (McNabb and Hock, 2014) found there was

“…no corresponding change in the number of glaciers retreating nor do we see corresponding acceleration of retreat rates. To the contrary, many glaciers in the region have advanced…”

Image Source: McNabb and Hock, 2014
Antarctica
In the Southern Hemisphere, an accumulating collection of (29) referenced studies (Lüning et al.,2019) indicate that not only has the Southern Ocean, Antarctic Peninsula, West Antarctica, and East Antarctica been cooling or not warming in recent decades, but many regional glaciers have begun advancing again.

Image Source: Lüning et al.,2019
Greenland
Greenland’s ice sheet mass losses have significantly decelerated since 2013 – a reversal from the rapid retreat from the 1990s to 2012 driven by cloud forcing and the NAO (Ruan et al., 2019).
The 47 largest Greenland glaciers also experienced a “relatively stable” period of rather insignificant retreat from 2013 to 2018 (Andersen et al., 2019).


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Only 21 of the 47 Greenland glaciers retreated in 2018, 12 advanced, and the other 14 showed no trends in either direction (Polar Portal, 2019).
Greenland’s largest glacier, Jakobshavn, earned headlines in 2019 for it’s surprising and non-predicted rapid thickening in recent years.


Image Source: BBC, 2019
New Study
A new study (Joughin et al., 2020) finds that the Jakobshavn glacier thickening that began in 2016 has continued apace, and ice elevation has now nearly completely returned to 2010/2011 amplitudes.
The authors attribute much of the glacier advance to the rapid 1.5°C ocean cooling impacting the region in recent years.
Ocean temperatures have returned to 1980s-era levels.

Image Source: Joughin et al., 2020
Share this...FacebookTwitter "
nan
"
Share this...FacebookTwitterAn observational analysis of photometric evidence suggests solar forcing of Earth’s atmosphere could vary by as much as ±4.5 W/m² since 1750, which is “far larger than the IPCC estimate of −0.30 to +0.10 W/m²” (Judge et al., 2020).
A 2017 study suggested the solar activity during the “modern maximum period from 1940 to 2015” is a “relatively rare event, with the previous similarly high levels of solar activity observed 4 and 8 millennia ago” (Yndestad and Solheim, 2017). Variations in solar activity since the 18th century were shown to have ranged between about 1357.5 W/m² and 1362 W/m² (~4.5 W/m²).
In contrast, the total radiative forcing due to the increase in the CO2 concentration since 1750 is suggested to be 1.82 W/m² (Feldman et al., 2015).

Image Source: Yndestad and Solheim, 2017
A new study (Judge et al., 2020) also affirms our highly uncertain estimations of solar forcing variations since 1750 may be “of the order of 3 W/m², far larger than the IPCC estimate of −0.30 to +0.10 W/m²” and also greater than the uncertain IPCC estimates of total anthropogenic forcing (+2.2 ± 1.1 W/m²) since 1750.
Large estimate ranges for solar forcing variability should reduce the certainty that Earth’s radiative forcing has been dominated by anthropogenic activity in recent centuries.

Image Source: Judge et al., 2020


		jQuery(document).ready(function(){
			jQuery('#dd_d215e380a9bc673ec0197eb9c75076b2').on('change', function() {
			  jQuery('#amount_d215e380a9bc673ec0197eb9c75076b2').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"If we want to relieve the strain of a globally growing – and commuting – population, we need to rethink how and where we work. Working more flexibly – both in timing and location – could have a massive impact on transportation and electricity production, two of the main contributors to greenhouse gas emissions. How does it work? Organisations adopt a four-day working week, the daily head count in the office drops by approximately 20% and the number of cars on the road drops by at least a fifth. It’s a win-win-win scenario for employees, employers and the environment.  If the four-day week catches on in Auckland, for example, and organisations across the city cut down on their daily in-office head count by 20%, the number of cars on the road each day drops by at least a fifth, and by up to 40% if parents are routinely permitted to work five shorter days in order to do school drop-offs and pick-ups. A 2017 report by the New Zealand Institute for Economic Research on the benefits from Auckland road decongestion means we know exactly what this decrease in traffic volume would mean for the city’s economy. Productivity could be boosted by at least NZ$1.3bn per annum (1.4% of Auckland’s GDP), the authors say if use of the road network could be optimised. Additionally, if the average speed across the Auckland network was close to or equal to the speed limit (known as free-flow), the benefits of decongestion during weekdays were estimated at around NZ$3.5 m per day, or between NZ$1.4 and $1.9bn (between 1.5 and 2% of Auckland’s GDP). Imagine these results extrapolated for New York City or London or Buenos Aires. The intensity of congestion and the lengthening of commutes are a byproduct of the way we work today, with billions of hours, dollars, fuel gallons and pounds of carbon dioxide expended each year in developed countries, where the term “rush hour” has been part of the lexicon for as long as anyone can remember. Even if we leave aside the climate change question and apply a pure economic lens, a widespread model of working which prioritises productivity and efficiency over a robotic adherence to working hours (which were once dictated by the sun and are now mostly arbitrary) is a no-brainer. When we turn our minds to the welfare of the planet, the answer is just as obvious. The human resources department of UC Davis in 2018 bluntly made the environmental case for work flexibility: “Not going into work could be one of the most environmentally sustainable things you can do as an individual employee.” In another 2018 study, researchers analysed data from the US Bureau of Economic Analysis and Bureau of Labour Statistics and found households with longer work hours have significantly larger carbon footprints. According to UC Davis, the two main contributors to US greenhouse gas emissions are transportation (29%) and electricity production (28%), with about 135 million Americans commuting to work. 50% of those workers have jobs they could do remotely some of the time, and the emissions-reduction value of those workers avoiding their normal commute on half of their usual work days is equivalent to removing 10 million cars from the road. A flexible work arrangement programme at UC Davis has provided options for employees such as flexitime (altering the start or end times of the work day); a compressed week of fewer, longer days at work; and remote working for part of the week. Every option means skipping the commute or evading rush hour at least some of the time. That could put a big dent in transport emissions: University of Reading researchers asked business leaders and owners how a four-day week would affect their commuting habits. When scaled up across the United Kingdom, workers estimated that they would drive 557.8m fewer miles per week if they worked fewer days. Of course, none of this evidence matters unless our political and business decision-makers are willing to upend the status quo in service of our planet’s viability. Changing to a four-day working week won’t by itself solve the climate crisis, but combining it with other progressive policies will be part of the global climate mobilisation we indisputably need. Andrew Barnes is the CEO of Perpetual Guardian. The 4 Day Week: How the Flexible Work Revolution Can Increase Productivity, Profitability and Well-being, and Create a Sustainable Future is published by Little, Brown on 6 February 2020. https://4dayweek.com/ "
"

An article today in BRIDGES Weekly Trade News Digest ( _What? You don’t subscribe??_ ) contains an explicit rejection by India’s trade minister of the idea that carbon border tax adjustments belong in the WTO’s agenda. Border tax adjustments in this context refers to _de facto_ tariffs that would “level the playing field” for domestic producers competing with foreign producers not subject to climate change policies of an equivalent rigour, also called “border carbon adjustments” or variations on that theme.   
  
  
While Minister Khullar predicts that these sorts of measures will be in place in 2–3 years time, he rejects that the WTO is the forum to deal with environmental issues.   
  
  
Furthermore, countries introducing such measures can expect litigation: 



India and other developing countries will undoubtedly challenge the true impetus behind the [border carbon adjustment] measures.



“Such measures imposing restrictions on imports on the grounds of providing a ‘level playing field’, or maintaining the ‘competitiveness’ of the domestic industry, etc are likely to be viewed as mere protectionist measures by the developed world to block the exports of the poorer nations,” [a recent report from an Indian think‐​tank closely connected with the Indian government] reads. “This is because there is little empirical evidence that companies relocate to take advantage of lax pollution controls.”   
  
  
The [report] argues that such unilateral trade measures will inevitably lead to tit‐​for‐​tat trade retaliation that could spiral into an all‐​out trade war. Such warnings have also been raised by China and several think tanks following the issue.



I’ve written before on the dangers of introducing climate change issues into the WTO (and Dan Griswold has written more broadly on why labor and environmental standards don’t mix well with the aim of freeing trade) but this is yet another firm, unequivocal warning to developed countries that their proposals (and they are still just proposals at this stage) will have consequences. Developed country politicians who insist on forcing rich‐​world standards on the poor world should listen carefully.
"
"

 _ **Editor’s note**_ _: In 2014, Cato released_A Dangerous World? Threat Perception and U.S. National Security _an edited volume of papers originally presented at_ _a Cato conference_ _the previous year. In each chapter, experts assessed and put in context the supposed dangers to American security, from nuclear proliferation and a rising China to terrorism and climate change._



 _As part of our_ _Project on Threat Inflation_ _, Cato is republishing each chapter in an easily readable online format. Even six years after its publication, much of the book remains relevant. Policymakers and influencers continue to tout a dizzying range of threats, and Americans are still afraid. We invited each author to revisit their arguments and offer a few new observations in light of recent events. You can view previous entrieshere, here, and here and on the Project on Threat Inflation homepage._



 _This week’s entry comes from Christopher Fettweis, a professor of political science at Tulane University, and the President of the Board of the World Affairs Council of New Orleans._



In “Delusions of Danger,” I made the case that fear in the United States is out‐​of‐​proportion to the dangers it faces. In the time since I wrote, I have tried to explain why this is so, where that fear comes from and how the nation can be reassured. Then my country elected Donald Trump to be its president, and I was tempted to delete the whole project and join my cousins in the drywall business.



Donald Trump is a manifestation of American fear. He is the nightmarish personification of everything that scares us, from immigrants to crime to job loss, and he stokes those fears on a daily basis. Even the COVID-19 crisis has provided opportunities for him to remind us of dangers, this time those emanating from China, the WHO and (apparently) arms control treaties. Trump’s entire existence is defined by his enemies, who for his supporters become the enemies of the nation. Voices of reason are easily drowned out by the cacophony of madness. Fear will be the defining feature of American politics as long as we are led by Donald Trump.



The COVID-19 crisis had the potential to mitigate American fears. We could have used this opportunity to recognize our common humanity, to realize that we faced the same challenges and work together toward solutions. Empathy toward the plight of others, whether in China or Iran, could have grown. A better, more rational, less fearful world could have emerged from this ongoing tragedy. Unfortunately we are led by a man incapable of thinking in such directions.



Perhaps it is not, however, time for despair and drywall. Surely it is possible for U.S. foreign policy to emerge stronger and wiser after Trump leaves office. It may prove useful for this country to have its fundamental assumptions challenged, in order to examine its most sacrosanct beliefs and evaluate their value. Trump is the equivalent of what political scientists call a systemic shock, and there is no reason to believe that what follows him will be worse. Perhaps the wreckage he leaves behind can be reconstructed in new ways; perhaps the final chapter in the story of this most pathological of administrations will be a positive one, a happy ending in which the assumptions that drive our decisions can be re‐​thought. The first few post‐​Trump years will be crucial: Do we return to the same fearful delusions that led to war after war (and to Trump), or do we seek to improve U.S. policymaking? Can we learn from our hideous national mistake?



Cato’s Project on Threat Inflation could not have come at a better time. With it and the establishment of the Quincy Institute, momentum is building toward more rational foreign policy, and perhaps a better post‐​Trump world. I worry that restraint will be wrongly tainted with the Trump stench – an odor that will not wash off easily – but that is the subject for another essay. Much of what ails American foreign policy in the short term can be cured with one election. Long‐​term trends, however, will not change unless acted upon by a force. Such a force may be growing in DC policy circles, but whether it can overcome the power of fear remains to be seen.



-Christopher J. Fettweis
"
"
Share this...FacebookTwitterBoth during the last interglacial (~120,000 years ago) and from roughly 2000 to 7000 years ago, relative sea levels were from 6-10 meters to 1-3 meters higher than they are today, respectively.
For a list of over 100 other scientific papers indicating sea levels across the world were multiple meters higher when Earth’s CO2 concentrations were about 150 ppm lower than they are today (~260 ppm), see our database here.
The Mid-Holocene, 2000-7000 years ago
Lopez-Belzunce et al., 2020 (Mediterranean)
“Regarding the stabilization of the RSL [relative sea level], our data show it to be 1.20 m above the present-day level at 3000 cal yr BP and 1 m higher at 2000 cal yr BP.”
Burley et al., 2020  (Polynesia)
“At the time of first Lapita arrival at Nukuleka, sea levels were 1.2–1.4 m higher than present (Dickinson 2007).”
Lopes et al., 2020 (Brazil)
“The late Pleistocene-middle Holocene post-glacial marine transgression (PMT) that started around 18 ka b2k in response to the melting of ice caps and glaciers, together with increased precipitation, would have led to another lake highstand (Figure 3A). Sea-level curves obtained from several sites along the Brazilian coast show that a mean sea level (m.s.l.) equal to the present one was reached at ~7 ka b2k, and continued to rise until reaching up to +5 meters between 6 and 5 ka b2k (Martin et al., 2003; Angulo et al., 2006). In the CPRS the PMT formed the Barrier IV, and the estimates based on geologic and fossil records indicate that it reached amplitude of about 2-3 meters above the present m.s.l. (Barboza and Tomazelli, 2003; Caron, 2007; Lima et al., 2013; Dillenburg et al., 2017).”
“The altitude of the terrace T3 above the fossils of Toxodon found in situ indicates this was cut by the Holocene sea-level highstand that reached a maximum altitude of 3 meters [above present] between 6 and 5.1 ka b2k. At that time Mirim Lake was invaded by the Atlantic Ocean through Taim and São Gonçalo channel, becoming a large paleo-lagoon with conditions suitable for its occupation by marine organisms, including sharks, rays, teleost fishes and whales. The coastal waters were warmer than today, as indicated by the presence of fossils of the shark Carcharhinus leucas, common in tropical areas.”

Image Source: Lopes et al., 2020


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Brocx and Semeniuk, 2020 (Western Australia)
“The Holocene stratigraphy in the Walpole–Nornalup Inlet Estuary shows that mean sea level was 1 m higher than present some 2900–1200 years BP (Semeniuk et al., 2011).”
Helfensdorfer, 2020 (Australia)
“This study presents a well-constrained model of the geomorphic evolution of the lower Murray River and Murray estuary with a specific focus on the response of the system to the Holocene sea-level highstand. Hydrodynamic modelling of the lower Murray River and Murray estuary was conducted to evaluate the primary drivers of palaeo-environmental change during the Holocene and constrain the plausible response of the Murray estuary to the +2 m higher-than-present sea level of the Holocene sea-level highstand.”
Martin et al., 2020 (Western Australia)
“Sea level high stands (~2 m higher than present) occurred at ~7 and 4 ka (Gouramanis et al., 2012) that likely caused seawater intrusion events into the aquifer”
The Last Interglacial (LIG), ~120,000 years ago
Muh et al., 2020  (Bahamas, Bermuda)
“Corals with closed-system histories collected from patch reefs on NPI have ages of 128-118 ka and ooids/peloids from beach ridges have closed-system ages of 128-116 ka. Elevations of patch reefs indicate a LIG paleo-sea level of at least ∼7 m to ∼9 m above present. Beach ridge sediments indicate paleo-sea levels of ∼5 m to ∼14 m (assuming subsidence, ∼7 m to ∼16 m) above present during the LIG. …. Results of this study show that at the end of the LIG paleo-sea levels could have been as high as 11-13 m above present (at localities close to North American ice sheets) to as little as 5-8 m above present (at localities distant from North American ice sheets).”
Helm et al., 2020  (South Africa)
“Around 126 ka, sea levels were 6.6-8 m higher than present levels on the Cape south coast [of South Africa]. … Chronological context11 suggests an age of MIS 5e (the Last Interglacial). As sea levels during MIS 5e in this area were up to 6-8 m higher than at present, a warmer climate capable of supporting large reptiles on the Cape south coast can be inferred.”
Share this...FacebookTwitter "
"From wildfires to rising tides, the climate crisis is already bringing many threats. Now scientists say it may also bring a shortage of many popular wines. Researchers looked at the land suitable for 11 popular varieties of wine grape and found that 2C (3.6F) of warming above pre-industrial levels – a rise the world is on track to exceed – would result in a 56% loss of suitable land within current wine-growing regions compared with the 1970s, before the most serious impacts of global heating. The white grape variety ugni blanc (also known as trebbiano toscano) is expected to lose 76% of its suitable growing area, and riesling 66%. The red grape grenache is predicted to lose 31% of the area currently deemed suitable for growing the variety. But the team said the glass was not necessarily half empty. Ignacio Morales-Castilla, the co-author of the study from the University of Alcalá, Spain, said: “The positive message is that we can still adapt viticulture to climate change – and diversity is a very interesting tool to do that. But the warning … is we should limit warming [as much as] possible, because the more warming we have, the fewer options for adaptation.” Writing in the Proceedings of the National Academy of Sciences, Morales-Castilla and colleagues report how they built a computer model that takes into account the timing of processes such as budding and fruit ripening for the 11 different varieties, as well as the climate in areas where these varieties are currently grown. From this, they identified areas within current wine-growing regions suitable for each of the 11 varieties. The model suggests global heating may hit the wine cellar hard: if no action is taken, a 2C rise would result in a 56% loss in land for the 11 varieties . A 4C rise would mean 85% of these areas would be lost. While Morales-Castilla said that was partly down to factors such as changes in rainfall, the main driver was heat. He noted this might damage plants, or speed up ripening and make the grapes too high in sugar. But the model also shows that if these areas could be replanted with a more suitable wine grape, or newly suitable areas planted, only 24% of growing area within current regions would be lost under a 2C temperature rise – a reduction in loss of more than a half. Under a 4C rise, 58% of such an area would be lost if varieties were switched or newly suitable areas planted – about a third lower than if no such action was taken. For example, many areas of wine land suitable for pinot noir, including in South Africa and Burgundy, will need to be switched to grapes such as syrah, monastrell and grenache, which produce fruit later in the year and are better able to tolerate a warmer climate. The team said some countries might be more affected than others, with countries already warmer and less able to compensate for future losses: land loss for the varieties could hit 90% for Italy and Spain under 4C of heating. And there’s more: the team found that new areas around the world – including parts of the UK – would become suitable for wine grapes as the planet continues to heat, with early-ripening varieties such as pinot noir moving north. The study has limitations, including the fact that it only looks at a handful of the more than 1,100 varieties of wine grape. Morales-Castilla suggested other varieties might offer greater potential for adaptation as the climate continued to heat up. The team said mitigation efforts were not without their problems: replanting or regrafting vineyards is expensive. There are also complex rules about how wines are labelled: for example, the name “champagne” can legally be used only if the sparkling wine comes from the Champagne region of France. Prof Steven Penfield, of the crop genetics department at the John Innes Centre, who was not involved in the research, welcomed the study. He said: “[It shows] that if growers are willing to adapt by changing the varieties they grow, there are ways to maintain yields in the face of rising temperatures, at least in the less extreme emissions scenarios. “The challenge for the industry will be that local varieties often add distinctive characters to wines, and there will be a reluctance to let go of traditional varieties, especially in areas with strong cultural heritage. Can you imagine a burgundy without a pinot noir grape, for instance?”."
"
Share this...FacebookTwitterBy Die kalte Sonne
The past winter in central and northern Europe was quite warm. Why is that? The Norwegian Centre for Climate Research CICERO explains it in an article from 6 January 2020:
Unseasonal temperatures for Norway
The unusual warm temperatures this winter and forecasts indicating milder winter conditions for January, February and March in Europe are partly due to an atmospheric circulation pattern called the North Atlantic Oscillation, or NAO. This atmospheric circulation pattern explains well the weather we get in Europe, especially in winter.
As explained in a CICERO-article from November 2019, seasonal forecast models are sometimes able to correctly forecast the phase of the NAO. 6 different seasonal forecast models are run at the beginning of each month by their respective weather centres from around the globe. The October simulations gave us a hint that we might get a positive phase of the North Atlantic Oscillation for November, December and January. The signal was quite strong, and 4 out of the 6 models were clearly in that direction, while 2 suggested normal winter conditions. The November simulations gave similar results, the majority of the models showed a positive NAO for December, January and February. Experts were a bit puzzled, as at the same time the snow cover over Siberia was already quite extensive, and the Arctic was very warm, two things that usually suggest a cold winter for Europe.
And then came the December simulations, the most recent ones, where all six models hinted to a positive NAO for January, February and March, and therefore milder conditions than normal for northern Europe in particular.
Read more at CICERO. An excellent report, which also applies to Central Europe.
For those who don’t know it yet: The NAO (North Atlantic Oscillation) controls the winter temperature in Northern and Central Europe. Positive NAO brings warm winters, negative NAO brings cold winters. Please note!
Now some of you will ask, what is this NAO actually? Well, it is the difference in air pressure between Iceland and the Azores. If the difference is big (pronounced low and pronounced high), then the NAO is positive (NAO+). If low and high are a bit thin, so the difference is smaller, then the NAO is negative. It’s as simple as that.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Why is it important? This pressure difference pushes the westerly winds a little bit more to the north or south. In an NAO+, the westerly wind belt lies further north and meets central Europe. This is where the humidity (wet February 2020, does anyone remember?) and the relative warmth of the Atlantic occur.
In a negative NAO (NAO-), the westerly winds blow further south and discharge their humidity as rain in Portugal and Spain.
For those who want to know more, the NAO website of the British MetOffice is recommended. We take the liberty of reproducing the two most important graphs of the website here. And this is how it looks with a positive NAO:

And this is how a negative looks:

And now, of course, you want to know where to check the current NAO status. To do so, simply google NAO and NOAA, or click on this NOAA page. There you can follow the last months of the NAO in high resolution. There is also a forecast for the next 2 weeks.
We see: In fact, the NAO was mostly positive during the winter. The forecasts for the next 2 weeks are not consistent. Bad luck. But if all models show a sharp downward trend in winter, you urgently need to buy road salt.
If you understand the NAO, you will get along better in life and in the climate change labyrinth. Finally we allow ourselves the question:
Why can’t the DWD German Weather Service explain such contexts to us?

Source: NOAA
Stay healthy!


		jQuery(document).ready(function(){
			jQuery('#dd_16efd3924a8804ec558ac63db78e3d5e').on('change', function() {
			  jQuery('#amount_16efd3924a8804ec558ac63db78e3d5e').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Cape Town’s recent drought brought into sharp relief how people behave and the choices that they make when resources become scarce. The South African city was teetering on the brink: a sustained dry spell meant that residents were faced with “day zero” – the very real possibility that water supplies could be turned off, leaving them queuing for access to this most important of natural resources. Day zero was averted in large part by people’s willingness to use dramatically less water than normal, while waiting for rain to fall. This was at once a worrying story about the impact of these “mega droughts”, which, according to top scientists from around the world, have become substantially more likely due to climate change. At the same time, Cape Town told an optimistic tale of a society pulling together in a crisis. My research, recently published with colleagues Oliver Vitouch and Judith Glück from the University of Klagenfurt, shows that we perhaps shouldn’t be so surprised by the way that the people of Cape Town responded to water scarcity. In fact, people may be more willing to share scarce resources they really need than apocalyptic Hollywood movies may suggest. As such, the results offer hope that, in a world facing a dramatic population increase and a potentially greater struggle for resources, it will not be every person for themselves. Our research asked how a crucial resource is shared by individuals when it is critical to their physical needs and well-being. The premise was simple: are people more or less generous with a “primary” resource such as water than with a “secondary” one like money, where needs may strongly vary across individuals and which is also more abstract?  The results revealed that even when people themselves were in need of water, they still acted in an altruistic way by sharing the very valuable and very limited supply of water they had equally with others. This was true even when they didn’t know who they were sharing their water with. This also contrasted with how they shared money. In our study, participants were invited into the lab in pairs (but they did not have a chance to meet or get to know each other). In separate rooms, they were asked to ride a stationary bike for half an hour to work up a thirst. They did not have access to a drink for an hour before, during or after the study.  We then used a used a common experiment that’s used in experimental economics research called the “Dictator Game”. One person was made an “allocator”, in control of giving out money and water. The other was the “recipient”.  The pairs were placed in two experimental conditions. In one condition, allocators received water and money as a windfall to give anonymously to recipients. In the other, those doing the allocating were informed that they had “earned” the water and money during the training session. Recipients in both conditions indicated how much they expected to receive. This was designed to test if their expectations were linked to whether or not allocators had “earned” the resource. The results were intriguing: the allocators who earned the small amounts of water gave about the same as those who hadn’t earned it. In both groups, they were willing to share water more generously than money. Similarly, recipients expected to receive more water than money. This goes against the idea that people are purely self-interested. One may argue that thirsty allocators were less generous because water simply had less value compared to money. But findings from other research suggest that what we observed may be guided by increased empathy towards others with a similar need which makes altruistic behaviour more likely. This may also explain why a related study conducted at UCL showed that when thirsty recipients were allowed to reject water offers, they were less likely to accept the ones that were very small and may thus have been perceived as unfair. Perhaps such a strong sense for fairness also speaks to the age-old wisdom that despite money’s value, we know deep down that it’s no substitute for the basics we and others need day to day. Ultimately, a thirsty man knows he can’t drink the money in his pocket; and others can’t either. Research such as ours aims to better understand what drives people’s behaviour – and how likely they are to cooperate – when resources are scarce. This may allow us to predict how individuals will behave before they run out of a resource. We now need to learn how sharing varies in different contexts, with different resources, when scarcity occurs in the real world and over long periods of time, like during the drought in Cape Town. The events in Cape Town are another warning about the consequences of climate change and the increasing need to preserve the world’s natural resources. But our results may also indicate that it is when crises are at hand and resources are scarce that our shared humanity is more likely to show."
"
Share this...FacebookTwitterBy Kirye (photo right)
and Pierre Gosselin
Today we post before-and-after mean annual temperature charts for 6 US stations in the midwest region with a low brightness index (BI), meaning low impact from the urban heat island (UHI) effect, which arises from widespread asphalt, concrete and infrastucture.
The low BI index tells us that the stations are sited in a rural-type environment. Five of the six stations have a BI of 0, while one (Thibodaux, LA) has a relatively low BI of 11.
Shown will be comparisons of NASA GISS Version 4 unadjusted, versus Version 4 adjusted. In each case the unadjusted data showed a cooling or little warming, while the adjusted data all ended up to show warming.
First we plot the station at Plainville, Kansas, which I already posted at Twitter. Shown is the plot going back over 100 years, before NASA adjustments and after adjustments.

It's obvious, as far as Plainville's temperature goes, NASA made the false warming trend.Why do many media pretend to know nothing about that?https://t.co/35Itt16sN0~#地球温暖化? #温暖化？ #気候変動 #ClimateChange pic.twitter.com/WTD380BGxN
— キリエ (@KiryeNet) June 7, 2020

Data source: NASA GISS.
As the 2 plots show, the data from the past were changed by NASA and made cooler. The new result: a warming trend! In other words, a cooling climate was fudged into one that is supposedly warming.
Next we move to the station of Hobart, Oklahoma. A slight cooling trend there was transformed by NASA into a warming trend:

Data source: NASA GISS.
The third station we look at is Carrizo Springs, Texas:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Data source: NASA GISS.
Originally in the V4 unadjusted, the past at Carrizo Springs was warmer than today. But NASA didn’t like that, and so they adjusted the temperatures from earlier in the 20th century downward. Again a modest cooling trend was changed into warming.
The story is the same at the station at Conception Missouri:

Data source: NASA GISS.
At Conception, Missouri, we originally saw no warming over the past 130. years. But then NASA fiddled with the data and now tell us there’s been warming at there as well.
Looking at the temperature charts for the station at El Dorado, Arkansas:

Data source: NASA GISS.
Note how warm it was in the 1920s. But NASA said that this couldn’t be right, and so cooled the mean annual temperatures in the early 20th century byalmost a whopping 2 degrees! Result: (fake) warming!
Finally we plot the NASA GISS data from the station located at Thibodaux, Louisiana – i.e. the U.S. South:

Data source: NASA GISS.
Above we see how the unadjusted V4 data were changed to create more warming.
NASA changed the data several times until they got the warming they want to us believe is taking place. The original data tell us there has been any real warming over the past century at these 6 stations.


		jQuery(document).ready(function(){
			jQuery('#dd_9484675fc1afd13abd685b1cca163d6c').on('change', function() {
			  jQuery('#amount_9484675fc1afd13abd685b1cca163d6c').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"In his acceptance speech, Brazil’s president-elect Jair Bolsonaro said he will “change the destiny of Brazil”. He may be right. For the Amazon and its indigenous residents, things will get worse before they get better, but this election may also be a tipping point for change. The staunchly anti-environment Bolsonaro may sow the seeds for radical environmental politics both in Brazil and worldwide. When the night comes, I wonder what might happen, who will be next, and hope that the dawn will come soon. With these words, Kum'tum, a leader of the indigenous Gamela people in the heart of the Amazon, shared his fear of being an environmental defender in Brazil. His community had been trying to occupy a portion of their ancestral lands claimed by farmers but, in April this year, they were attacked by men armed with machetes and firearms. Some had their hands cut off. Others like Kum'tum were shot. Indigenous groups have good reason to be scared. Over the past decade, Brazil has been the most dangerous country in the world to be a land or environmental defender and 57 of these people were murdered last year alone. According to the NGO Global Witness, this kind of violence stems from sections of the agribusiness sector, in particular parts of the Amazon’s cattle industry which is the largest single cause of deforestation globally. The Amazon and its indigenous residents may now be persecuted with even more ferocity, as Bolsonaro promises a new alliance between the security state, agricultural interests and far-right political power. Fortunately, where there’s hegemony, there’s resistance. NYU sociologist and theorist Steven Lukes famously broke power itself down into “three faces”: decision-making power, non-decision-making power, and ideological power. The Amazon’s future and that of its residents will be fought across these three key battlegrounds. First, Bolsonaro’s government will have to win the visible battles of decision-making processes shaping Brazil’s environmental governance. For instance, Bolsonaro would have to break free from present institutional and legal frameworks tying Brazil to its environmental commitments, such as the Paris Climate Agreement and the Sustainable Development Goals. And that’s tricky because of the global backlash. Perhaps this is why Bolsonaro has now scrapped his pledge to quit Paris.   Other challenges to Bolsonaro’s decision-making power will come from below. While Brazil’s balance of power has historically favoured big corporations like those in the agribusiness sector, the country also hosts massive grass roots movements of resistance. These include the Marxist-inspired Landless Workers’ Movement (MST), with an informal membership of 1.5m people and, in the Amazon itself, the Cainquiama, a resistance coalition of tens of thousands of persecuted indigenous peoples and other communities.  In this case, the “non-decision-making power” concerns Bolsonaro’s ability to set the development agenda. The country’s new leader wants to reverse green agreements and return to a form of violent pro-growth neoliberalism which is blind to both democracy and environmental concerns.  But the Amazon acts as the world’s lung and is a globally shared priority. The existential threats it faces would be met in a similar combative spirit as those facing the melting Arctic – by environmental defenders, pro-sustainable development governments, global institutions and civil society. Collectively, these actors have the moral power to set the agenda and pressure for legal, institutional and economic sanctions and limits to stop the predatory march of Brazilian agribusiness further into the forest. If they can manage to frame actions against the Amazon as crimes against humanity and the planet, they could even be effective. The third conflict is the invisible ideological battle to control Brazilian hearts and minds. By democratically electing a far-right candidate, Brazilians have sent a crystal clear message that they want radical change for the better. The problem is what people understand by “better” and what this “better” means for the environment.  The “Bolsonaro way” is ideologically powerful because it is ambiguous about what it is, but sharply clear on what it is not. Just like Trump, it is unashamedly pro-growth regardless of the collateral damage to the environment, but Bolsonaro’s vague development utopia lacks depth. It resurrects zombie ideas of dictatorship style governance and pro-growth violent neoliberalism, fuelled by popular disillusionment with corruption and a failure to lift people out of poverty. Another ideological battle will concern the church, a powerful institution in the world’s largest Catholic country. In his 2015 encyclical message Laudato Si, Pope Francis attacked environmental degradation and climate change and called for “swift and unified” global action. That, and his later claim that to harm the environment is to sin, may prove a powerful ideological tool for environmental activism. The rising wave of Brazilian evangelicals may also question how Christian it would be to slay the Amazon and its people. A final key ideological battle concerns the normalisation of indifference towards the environment. According to the World Values Survey and Latinobarometro, the largest opinion polls that have investigated what Brazilians value as important, the only topics ranking high in everyday life are corruption, the unstable political situation and the economy. The challenge is that too many Brazilians fail to see how crucial the environment, the Amazon and its indigenous inhabitants are to their own well-being and development."
"

This week’s report, by Elizabeth Thomas and colleagues from the British Antarctic Survey, that snowfall has been increasing in Antarctica is hardly surprising. What is different that it is much more comprehensive than previous studies, which were largely limited by a virtual lack of pre‐​1957 data. That was the “International Geophysical Year”, in which systematic observations of Antarctica’s climate began.   
  
  
The new study looks at the last 200 years of snowfall trapped in 79 ice cores taken from around the continent. It supplements other recent findings that also made headlines.   
  
  
Determining Antarctica’s overall ice balance has been, well, slippery. One favored method has been to look at gravitational data measured by satellite. Thicker ice means more mass, which means greater gravity. These studies usually come up with a net loss, translating to from 6/1000 of an inch of sea level rise per year to 12/1000 (both values being rather small beer). But different measurements show otherwise. Three years ago, Jay Zwally and his colleagues at NASA used satellite‐​based altimetry and concluded Antarctica was undergoing a net gain in ice.   
  
  
Common sense dictates that it should be snowing more in Antarctica. Think of it as Buffalo on steroids when it comes to snow. In the fall, when Lake Erie isn’t frozen, cold air passing over it from the west picks up evaporated moisture and dumps it on the land in the form of snow squalls. The warmer the water and/​or the colder the air is, the more is snows. Unlike a mere Great Lake, Antarctica is surrounded by a largely unfrozen ocean, and when any atmospheric disturbance sends moisture onshore, it snows too.   
  
  
Around Antarctica, there’s been a slight—meaning a couple of tenths of a degree—warming of the surrounding ocean, which means that the air blowing over it picks up a bit more moisture than it used to. Unlike Lake Erie, the Southern Ocean is huge, and any atmospheric disturbance that shoves more oceanic air up onto the continent is going to be pushing a substantial stream inland with ever more moisture, even for a very slight ocean temperature rise.   
  
  
The “surface mass balance” of a glacier or an ice sheet is the difference between accumulated snowfall and what either melts or evaporates. In anticipation of increased snowfall, the last (2013) scientific summary by the United Nations’ Intertgovernmental Panel on Climate Change shows that the projected 21st change in the Antarctic mass balance to be weakly _positive_. That’s why it’s perplexing that the new finding is so newsworthy.   
  
  
But now we know that the snow has been increasing down there for the past 200 years…and that the increase started before the major emissions of atmospheric carbon dioxide.
"
"A report by the WWF published on October 30 reveals how our actions are degrading the natural world – the very basis on which our livelihood depends. The Living Planet Report 2018 shows that between 1970 and 2014, vertebrate – mammal, fish, bird, amphibian and reptile – population sizes have been reduced by 60%. South and Central America have been hit particularly hard, suffering population declines of 89%.  The report is one of the most comprehensive global analyses of biodiversity, yet it does have its limitations. It only tracks vertebrates, sampling is not standardised across different biomes, and it ignores genetic diversity.  It’s also worth noting that other global studies have reported different figures for biomass decline. A study in Nature looking at plant and insect species, estimates declines in species abundance of around 11%, and a study from Germany found a 75% decline in flying insect biomass in the 27 years up to 2016.  These are large discrepancies and clearly this topic needs further exploration. However, all these studies support the conclusion that we are losing biodiversity at an alarming rate. There are two main strands of argument when it comes to the loss of wildlife. The first is that the loss of nature is a necessary and acceptable consequence of human progress. Historically, our wealth has increased through exploiting the natural environment, and it has allowed us to live richer lives with more freedom of opportunity.  Counter to this, the argument runs that we can only push biodiversity loss so far before we threaten the life support systems of our small planet – the capacity of the biosphere to regulate our climate, pollinate our crops, purify our water and decompose our waste. The biologist Paul Ehrlich once made the analogy that losing species in an ecosystem is like progressively removing rivets from an aeroplane: the plane may fly on for a while, but eventually it will fall out of the sky. Such concerns have led to attempts to quantify “safe limits” of biodiversity loss, or so-called planetary boundaries that we must not cross else we risk a catastrophic tipping point. Although a compelling concept, there remains serious issues in implementing it. One is the uncertainty in the extent of biodiversity loss, the other is in the impact these losses will have on human livelihoods.  To make a comparison with climate change, many governments only committed to action after the likely economic impacts were quantified through meticulous analysis combining climate science and economics. Therefore, new approaches to more precisely quantify risk are urgently needed in order to galvanise action. But even if we can ascertain the risks, will we actually be able to stop biodiversity loss?  We know with some confidence the risks of global warming, yet countries are struggling to stick to their Paris commitments, let alone the even greater emission reductions needed to avoid a warmer world.  I was recently involved in an interdisciplinary analysis of the global food system (one of the major culprits of biodiversity loss), which identified a range of mechanisms that keep our food system “locked” into an unsustainable trajectory.  People often feel powerless to change such global systems and point to factors at the level of government policy, such as the upcoming extension and renewal of the Convention for Biological Diversity. Although wise governance is essential, many factors that contribute to a decline in biodiversity operate at the individual level, such as our dietary and consumer choices. Also, the structure of our institutions ultimately reflects our individual mindsets, so we have the opportunity to initiate positive change by acknowledging our dependency on nature. Rising levels of individualism, however, have encouraged an economy that provides for private interests at the expense of nature.  Through our purchases we can destroy the environment on the other side of the world, which is why the WWF report calls for better data to connect consumers to the consequences of their actions. On the positive side, our increasingly connected world could allow for social contagion of positive and responsible ways of acting. Small individual changes can cascade and cause a different kind of “tipping point” towards a more sustainable way of life.  If we really want to halt biodiversity loss and ensure a safe course for current and future generations on Spaceship Earth, we need to think beyond government, and forget the selfish “I” – the solutions start with “us”."
"The global crisis of climate change is one of the most complex and wicked of problems we currently face. It is a physical, technological and economic challenge, and one that raises questions right at the heart of our relationship with the environment in which we live. In the light of the IPCC’s most recent report, we face difficult decisions that will change every aspect of how we live. Yet providing people with more scientific information has been shown to have little effect on the degree to which people care about the climate or understand the impact of human activity. Something else is needed to jolt us out of our current trajectory. I am exploring the role of art as a route to knowing the environment in an alternative way. For my latest work, The Matter of the Soul, I hacked the electronics of lab equipment to transform them into musical instruments that play the sounds of melting ice. In the musical compositions and sculptural installations for this work, I explore the possibility of art to engender empathy with the Arctic ecosystem, and how dispersal of water, human movement and digital identity are three intrinsically interlinked processes of transformation relevant to climate change. I began the research for The Matter of the Soul sailing around Baffin Island in the Canadian High Arctic. I set up a temporary studio on the top deck of the ship Akademik Sergei Vavilov, where I could tinker with all of my equipment and explore the aesthetic of this fascinating place, so different from my European home. The light in the Arctic was incredible. It felt like the sun touched me differently. But what struck me most was how much personality the water there had. Lumps of ice and mini-icebergs are strewn across the barren landscape, perching on top of rocks like seagulls or floating next to tiny islands as if they’re biding their time, waiting for their chance. The meltwater from glaciers crashed joyously down rock faces, surging into the ocean. Icebergs created a time all for themselves, distorting our reality when we intersected them. Around the ship, and on land in the open, rocky landscapes, I interviewed visitors to and residents based around Baffin Island. But I also captured the voice of the ocean and ice. I wanted to draw an analogy between bodies of water and human culture. To capture the voice of the water, I decided to explore the chemical consequences of ice melting. Ice and seawater are the coming together of individual water molecules, just as culture emerges from the coming together of individual human beings. Water in ice both has its behaviour shaped by its environment and constructs this environment, just as we human beings are shaped by and construct our culture. When glaciers and icebergs melt, individual water molecules begin an adventure of dispersal that could take them as far as Mexico. The behaviour and trajectories of molecules are changed by the ocean that they have joined and become part of, just as when we travel – either for tourism or migration – we change and exchange with the cultures we encounter, and within ourselves. There are plenty of scientific instruments for gathering data on how water changes due to climate change, but not many pianos that do it. I decided to merge artistic and scientific methods of encountering the world, by hacking the scientific instruments to make sounds that I could use in my musical compositions. I took a pH meter and conductivity meter, which measures the water’s saltiness, with me to the Arctic. To get sound out of these scientific instruments I used a process called circuit bending. This process allows me to make audible the changes in voltage in the equipment when they measure the physical properties of the water. I can then record them. It took a while for me to succeed in this. Immersed in the Arctic, having spent days in open water due to fog and ice, I had been struggling to find the sound in the circuit of the conductivity meter. Hunched in my little studio, dipping the probe in and out of my sample of “Open Seawater 2”, I suddenly heard it. A faint hum, followed by a diminishing click, with periodic surges in the sound. I had found a temporal representation of the salinity – the sound of the the dying off of the saltiness of the sea as icebergs melt. The sound of the dying of the ice. Importantly, these recordings are not directly representative of the value of the measurements. They are rather derived from what happens inside the machines during the process of measurement. As such, the recordings are not a sonification of data, but rather a reflection on the process of measurement as a passive way of knowing, which can evoke the same sense of achievement as acting (and therefore can act as a placeholder for real action). On October 23, I played my hacked instruments at Howard Assembly Rooms at Opera North in Leeds, accompanied by pianist Matthew Bourne and cornet player Alex Bonney. The hour-long symphony tells the narrative of transformation and dispersal in the Arctic: of ice and seawater, of changing culture and climate change. The raw material and compositions from the project will also be released online under Creative Commons, with a call for others to remix the work’s identity as it disperses through the internet. I am also releasing all compositions of The Matter of the Soul on audio cassette, with the full symphony coming out on cassette in November. I chose this physical format because it’s easy to remix it while preserving the audio quality. We are at a tipping point, a moment where the decisions we make will drastically affect the future well-being of humanity. A wicked problem like climate change can’t be fixed by one action alone. It takes a collection of actions to tip the system. I hope that by connecting in an embodied way with the processes of transformation in the Arctic, it might be possible for a few of us to connect with transformations within ourselves, enabling us to take action without."
"A novel “floating pipe” to recover plastic from the ocean has just arrived on its maiden voyage to the Great Pacific Garbage Patch. Run by Dutch start-up Ocean Cleanup, the scheme involves a 600m-long floating pipe connected to a net, which herds plastic into place before it is gathered and taken to shore by specialist boats.  The question of why we should bother to clean up the oceans may seem obvious to you but, as an economist who studies these things, I like to put a number on it. We can therefore say that plastic in the ocean has a direct financial impact through things such as lost tourism, damaged ships or fewer fish to catch. But it also has a wider and harder to quantify economic impact on lost marine life or reduced beach and water quality.  These damages, estimated at US$1.25 billion annually, imply that recovering marine plastics is worthwhile. But my research suggests that it might not be financially viable to do so. This is partly because the clean-up is so expensive. Unsurprisingly, towing a massive boom out to the middle of the ocean and then periodically transporting plastics to and from it is not cheap.  The Ocean Cleanup’s own 2014 feasibility study suggested that, once a full fleet of 100km of these floating barriers was deployed at a cost of US$372.73m (currency converted by myself in August 2018), it would collect plastic at around US$5.32 per kilogram.  This wouldn’t be a problem if discarded plastic was more valuable. The scheme could even pay for itself. But the clean-up will remain unprofitable for the time being because the market price for discarded plastic remains incredibly low. I looked at four possible options for recovered plastic: 1. Landfill: This is the easiest option although it leads to actual net losses rather than any benefit. Revenue per kg: -$0.12 2. Incineration: Burning all waste generates electricity which is reportedly as much as 60% cleaner than a fossil fuel equivalent. However, this negates the possibility of recycling or reusing the plastics. Revenue per kg: $0.10 3. Pyrolysis: Similar to incineration, except the plastic is heated in the absence of oxygen, so it doesn’t burn. Instead, the process generates oils which can be refined and sold. However, the viability of pyrolysis is dependent on economies of scale which may not suit it to the infrequent collection of marine plastics. Furthermore, at a low level, it is unlikely that the generated oil from plastics can be price competitive with conventional oil sources. Revenue per kg: $0.27 4. Recycling: This is the preferred option, as it is a more efficient use of existing resources. But volatile recycled plastic prices and low virgin plastic prices suggest that this, too, is unlikely to be a profitable option. Revenue per kg: variable, but a weighted average of $0.15 is a reasonable assumption All this means it costs more than $5 to gather a kilo of plastic from the ocean, while that same plastic will only be valued at – at best – 30 cents. With about 8 billion kilos (8,000 tonnes) of plastic added to the ocean each year, the costs – and losses – involved are huge. Of course, such a massive loss is not likely to persist in the long term. For instance, increasing oil prices will push up the price of virgin plastics hence making recycling a more valuable option. Furthermore, the high cost expectations are primarily due to this being the first system of its type. With further experience, research and development in recovering debris at sea, it is likely that a lower-cost method will arise. However, there is no immediate indication of such improvements, and so big losses are expected to persist in the short to medium term. The financial loss contrasts with the economic benefits of recovering marine plastics. Even conservative underestimates of the costs of marine plastics suggest annual damages to be in the billions.  Using estimates of how much plastic is in the ocean, I was able to estimate that removing each kilogram would lead to a net benefit of at least US$7 and as much as US$38. But that still leaves us with a direct financial loss of nearly US$5 per kilogram recovered, versus a more than US$7 net benefit to society for every kilogram recovered. So we are at an impasse. It is nowhere near profitable to recover marine plastics, yet it is imperative to do so. Thankfully, a few solutions are at hand.  The first is crowdfunding. This avenue worked before for the scheme in question which managed to raise more than $2m in just 100 days. But there are doubts about how viable such a fickle and inconsistent funding source is for a longer-running scheme that would operate at a global scale.  Philanthropy is another option. Conceivably, with the support of just a few wealthy benefactors, viable ocean clean-up could be a reality. However, it is unclear whether leaving ocean clean-up to the whim of a few individuals is a sustainable model in the longer term. Finally, the international nature of plastic pollution suggests no single government is going to foot the bill – especially if all the financial benefits are privately appropriated. However, one suggestion is that if taxpayers are already paying environmental charges aimed at reducing plastic pollution, such as the plastic bag tax or proposed latte levy, then perhaps these revenues could be earmarked to fund the scheme.  So future policymakers must pay particular attention to the various mechanisms and agreements that may bridge the gap between financial losses and economic benefits. Indeed, evidence suggests a healthy degree of public support for cleaning up the environment, but whether the public feels strongly enough to support efforts via crowdfunding or earmarked taxes remains to be seen.  Plastic causes $13 in damages per kilogram per year. The race is on to determine how we can clean up the world’s oceans without bankrupting ourselves."
"
We had ""yellow"" level geomagnetic activity on the sun last night, and more may
come tonight and tomorrow night. Its coming from Sunspot 953, which is about 3 times the size of the Earth.

Sunspot 953 is crackling with mild
B-class solar flares. Credit: SOHO/MDI 

Image of sunspot 953 taken today by Sebastien Kersten of Le Cocq, Belgium:
Here is the dispatch:
From: solarxactivity@bbso.njit.edu
Date: April 28, 2007 9:24:59 AM CDT
To: xxxx@rice.edu
Subject: BBSO Solar Activity Warning 28-APR-2007 14:19:18 UT
Region NOAA 10953 is currently beta-gamma magnetic class, and may increase in complexity.
The region is bright in H-alpha as well. This region has a chance of producing M-class
events.
NOAA 10953, S10 E41.  Beta-gamma region. Position as of April 28, 2007 at 13:30 UT.
And this is in the middle of our solar minimum, indicating our sun still has a few belches to pass out before completely settling down.
One of the best tools we have is the ACE Spacecraft, which monitors the sun 24/7 and provides us with a plethora of real-time data, of the magnetic
field, the solar wind, and  inter-galactic cosmic ray counts.

For the latest ""dial"" info (including our ""space weather stoplight"") go to
http://space.rice.edu/ISTP/dials.html
For the latest 10-minute averages of the Boyle Index from realtime ACE
spacecraft data, go to http://space.rice.edu/ISTP/wind.html
Some guides to interpret the gauges
If the hourly-average of the Boyle index exceeds 110, then Kp 4-6 storms
will likely occur within the next three hours
If the hourly average of the Boyle index exceeds 200, then major magnetic
storms will occur within the next three hours
If the hourly average of the Boyle index exceeds 250, major low-latitude
auroras will occur within the next three hours.
A magnetic storm generally occurs about an hour or two after the CME arrives at Earth, which is roughly 26-48 hours *after* a major solar flare. The Boyle Index is derived from real-time ACE spacecraft data, which gives about 45 minutes of warning before it hits the Earth.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6a441e7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The climate activist Greta Thunberg has said she has applied to register her name and that of the Fridays For Future movement she founded in 2018, which has gone global and catapulted her to international fame. The move would allow legal action against persons or companies trying to use her name which are not in line with her values or that of her movement, she said. “I assure you, I and the other school strikers have absolutely no interests in trademarks. But unfortunately it needs to be done,” she said on Instagram on Wednesday. Thunberg said she had also applied to trademark Skolstrejk for klimatet (school strike for the climate in Swedish) – the wording on the placard she has held since she started her protest outside the Swedish parliament in 2018. “My name and the #FridaysForFuture movement are constantly being used for commercial purposes without any consent whatsoever. It happens for instance in marketing, selling of products and people collecting money in my and the movement’s name,” she wrote on the social network. Thunberg, who took centre stage at the World Economic Forum in Davos this month, and her fellow young activists in the movement want politicians to listen to climate scientists and take action to tackle global heating."
nan
nan
"As the Brexit negotiations wrap up and Theresa May’s deal is lambasted by Remainers and Leavers alike, it’s still far from clear what the future holds for the United Kingdom. On March 29 2019, it is due to leave the European Union. Brexit is the first time a member state has voted to withdraw from the EU and it has caused a geopolitical earthquake, unleashing uncertainty in the UK and abroad. We don’t know what the impact on the UK will be when (and if) it actually leaves the EU. If it does so on poor terms, or via the still possible “no deal” eventuality, there are a wealth of devastating projections which may materialise. The only thing that we can be sure of is that Brexit represents a moment of huge social, political and economic rupture. However, history tells us that such moments are also moments of opportunity for radical departure from the status quo. Let’s be frank, Brexit is not a progressive endeavour. It threatens social and economic turmoil in which the most vulnerable in society will – as always – be the hardest hit. Leaving the EU could jeopardise benefits to UK citizens in the form of workers’ rights, environmental protections and food standards. The political climate outside of the EU also offers an increasingly undesirable community of potential allies and traders, dominated by the rise of the far-right in North and South America. On the other hand, uncritical adoration of the EU overlooks the reality of what the Greek economist Yanis Varoufakis has described as “a regressive set of vile institutions”. It cannot be denied: the EU is a large and brutal force for neoliberalism. From this critical stance on the EU, I still voted Remain in the referendum. I believed then – and still believe now – that regressive forces will profit from the UK’s exit and that the vulnerable will suffer. So, as we approach the March deadline, if the UK does indeed crash out of the EU, the left needs to be prepared with visions of alternative futures, and be ready to fight for them.  The realities of a post-Brexit UK appear bleak, certainly in the short term. But separation opens the door for alternatives to the dominance of free-market fundamentalism. We could move from a society centred around financialised capital and the City of London, to one that promotes social and environmental justice in the UK and internationally. Reports claim that Brexit will mean lower levels of economic growth for the UK. For politicians this is a horrifying prospect. But falling growth need not be feared, if it is integrated within a broader transformation of society. The degrowth movement emerging amongst academics and activists argues that the logic of infinite growth is driving ecosystem collapse and climate breakdown. As stated in the latest IPCC report, we now have only 12 years to radically restructure society to cap global temperature at 1.5 degrees Celsius above pre-industrial levels. If we fail, we will face catastrophic climate impacts.  Degrowth argues that the wealthy and heavily polluting countries of the global north – such as the UK – must undergo a phase of managed and socially equitable economic contraction. This is necessary to downscale rich economies to within safe ecological limits. The need for endless economic growth pushes us to produce more, consume more and make more profit. It has left our society overworked, over-stressed and plagued by extreme levels of inequality.  These dire social conditions have been blamed for the Brexit vote itself and unlimited growth also fuels climate breakdown, with the UK as a big contributor to global carbon emissions. Simply, our slavish devotion to growth is making us miserable and destroying the planet. Degrowth could liberate us by arguing that more growth is not the solution, but the problem. We can and must live better with less, shared more fairly. Degrowth would rid our society of pointless production and consumption. We could say goodbye to “bullshit jobs” – the pointless make-work that keeps workers stressed without any obvious value to society beyond enriching corporate elites. Production and consumption could be organised in service of social and environmental well-being rather than profit. This degrowth transition could be pursued through ideas which confront the relentless treadmill of work, such as a four-day week. Poverty and inequality could be tackled by implementing a universal basic income and a maximum income. A fundamental decentralisation of the UK’s political and economic landscape could end London’s dominance by distributing more democratic autonomy to the regions. Degrowth thus acknowledges that liberating society from the growth imperative is not only an ecological necessity, but also loosens the grip of the capitalist wage-labour market. This frees people to dedicate more of their lives to the things that really matter to them. Is degrowth a likely future for the UK after Brexit? Certainly not in the short term. But, as Brexit and climate breakdown destabilise our politics, nothing much is certain. Only that we must be prepared with visions of a better future, and be ready to fight for them."
"One of the principal arguments in favour of HS2 was the positive effect it would have on the environment – and this was rooted in the belief that high-speed electric trains could help the UK cut its carbon emissions. The rail project, supporters have argued, will lessen demand for carbon-intensive air travel, road freight and car journeys by linking northern England – and potentially Scotland – with the Midlands, London and HS1 to the Channel tunnel and continental Europe. Providing low-carbon alternatives is urgently needed because transport (mostly road) is now Britain’s largest greenhouse gas emitting sector, accounting for 28% of all GHG emissions in 2017. The government has a target of net zero emissions by 2050, but there is disagreement over whether HS2 will really help deliver. At face value, the project should be a good way to cut carbon. Electric trains, particularly if powered by renewable energy, provide low-carbon transport. HS2 claims it can achieve 8g of carbon emissions per person per km. The same journey by car would generate 67g of emissions – and by plane, 170g. But carbon modelling – predicting a new infrastructure’s carbon emissions – is, like all future-gazing, imprecise and dependent on computer modelling. The government’s own calculations for HS2 suggest its carbon emissions could exceed potential savings, even over the railway’s projected 120-year lifetime. HS2 will not cut carbon emissions. According to HS2’s own forecasts, even over 120 years, its overall construction and operation cause carbon emissions of 1.49m tonnes of carbon dioxide equivalent. This represents just 1.18% of Britain’s annual transport emissions, but critics say it is still an increase when emissions need to be falling rapidly to reach net zero. HS2’s construction requires vast quantities of concrete and steel, as well as diesel-powered machines moving millions of tonnes of earth. A 2019 report for the High Speed Rail Group (HSRIL), which represents companies with an interest in high-speed rail, estimates construction could be reduced by 20% to 30% with low-carbon innovations, such as hydrogen-powered floodlights and hybrid excavators. HS2’s calculations show its emissions will be offset by the wider decarbonisation of transport it helps create. Some road freight will move to rail as HS2 will enable the wider rail network to take more freight. There will also be a “modal shift” – people choosing to travel by HS2 instead of driving or flying – but this is likely to be small. The Department for Transport suggests only 1% of HS2 passengers will be people who would have flown, and 4% those who would have driven. Some analysts argue that HS2 has presented an overly pessimistic carbon-saving forecast because it is obliged by law to give worst-case scenarios. The HSRILG report points out HS2’s forecasts of 4% and 1% are based on assuming that driving and flying become more affordable while rail fares increase above inflation. In practice, the European average for high speed rail modal shift is 15% from cars and 30% from planes. Furthermore, HS2’s forecasts do not include carbon-saving from increased use of local passenger rail, with HS2 freeing up lines such as the west coast mainline to provide better local services. Julia King, the deputy chair of the committee on climate change, reviewed HS2’s carbon forecasts for Tony Berkeley’s minority report on HS2 and judged them “sensible and conservative” – they do not include highly speculative future scenarios, such as HS2 leading to fewer new roads. If HS2 was well integrated with the European high speed network (and critics say it is not, because it does not connect to HS1) it could become part of a European-wide system saving up to 5m tonnes of carbon dioxide equivalent by 2050 if all journeys under 1,000km moved to rail. Potential emissions Construction: • Huge quantities of steel and concrete including concrete slab-track • Moving construction materials to site • Tunnelling (more carbon-intensive than open-air construction) • Construction machines • Removing soil by truck • Manufacture of rolling stock • Journey to work of HS2 employees Operation: • Power source not guaranteed to be renewable. Speed of HS2 requires more power • Ongoing maintenance • Increased car journeys to HS2 stations • HS2’s better airport connections could increase flying • Domestic flights reduced by HS2 could lead to increase in international routes Potential savings • Increased rail capacity shifts freight from road to rail • Increased capacity leads to more local/regional rail journeys • Modal shift with travellers choosing rail over more carbon-emitting road • Travellers also switching from flying to high-speed rail • Carbon sequestration from tree-planting • HS2 could be powered by all-renewable energy • HS2 prevents other carbon-intensive infrastructure projects Sources: HS2 Limited, 2019, High Speed Two phase 2a, Informationa Paper, E27: carbon Friends of the Earth, Opportunity Costs of HS2, 2019 Lord Berkeley’s Dissenting Report, 2020 [Size of emissions not included because different scenarios give different estimates; figures have not been modelled for all these factors] But Lord Berkeley and other critics argue there are risks that HS2 could increase emissions. More people might drive to HS2 “parkway” stations. And by connecting airports more effectively in London, Birmingham and Manchester, HS2 could lead to an increase in flights. If HS2 cuts demand for domestic flights, says Friends of the Earth, that could simply encourage the aviation industry to switch domestic flight slots to more profitable, and carbon-intensive, international routes. HS2 could be made less carbon intensive. Critics such as Berkeley point out that if HS2 was not built to such a high specification (with top speeds supposedly in excess of continental high speed rail), emissions would be reduced both in operation and construction; it wouldn’t require carbon intensive concrete slab-track, for instance. An independent report by the rail consultants Greengauge in 2012 said HS2’s sustainability would be maximised by reducing its top speed, fully using freed-up capacity on the existing rail network, and creating city-centre stations rather than edge-of-town parkways. Supporters claim HS2’s plans for parkway stations are already morphing into “sustainable urban extensions” with high-density housing and tramways. Ultimately, HS2’s future emissions will depend on wider – and more joined-up – transport and energy policies. It would use less carbon if there was a decarbonisation of electricity supply (rather than another dash for gas), and coherent national policies to reduce car use and air travel.  Friends of the Earth and other critics highlight the “opportunity cost” of HS2: its £106bn price (at least £5bn a year for 20 years) will lead to less investment in more effective carbon-saving transport, such as regional rail, buses, cycling and walking. Berkeley concluded that electrifying existing railways would have a much greater environmental benefit. Friends of the Earth suggest an even less glamorous low-carbon option: making all UK buses free would cost £3bn a year."
"Scientists, environmentalists and animal rights activists have said it for many years. Now conclusive analysis has confirmed their argument.  The global meat industry not only damages our health and is ethically dubious – it is unsustainable because of the damage it does to the environmental prospects of our planet.   Yet politicians have little desire to do anything meaningful about it. Instead, they have a long history of ignoring or suppressing inconvenient evidence that is detrimental to the major industries of a free market economy.  But we need them to speak out. Back in 1722, when Dutch explorers landed on Easter Island in the south Pacific, they found a human population in terminal decline. The Rapa Nui people had deforested most of the island, and the variety of plant and fauna had considerably decreased.  Left alone for centuries, and without governmental legislation to protect the environment from human behaviours, the inhabitants of the island had been slowly committing ecocide against the terrain that sustained their very existence.  This historic example – and our current relationship with the environment – present interesting questions about human denial, idleness and avoidance within the individual and collective psyche. It also suggests that only compassionate authoritarianism, which holds that our ecosystem is more important than individual and collective egos, can prevent us from our current path towards global ecocide.  The meat industry is one of the largest political lobby groups across the world, providing financial support to many mainstream political parties and their candidates. Politicians, many of whom are meat eaters, stay clear. They likely conclude that confronting such a powerful interest group is not in their career (or meal time) interests.  The mainstream media also regularly falls short. Earlier this year, the BBC’s Today programme included an interview with a sheep farmer from Northumberland. It followed publication of a study which pointed to its detrimental impact on the environment. The interview was essentially the meat industry’s PR response – broadcast on Radio 4’s flagship news show.  In the programme, the farmer was referred to as a “shepherdess”, and her work described in terms of romantic walks on windswept moors. The discussion was emotional rather than rational, with the main argument of the interviewee being that if the meat industry was to decline it would be “very sad”.  She was then given a platform to make claims, without challenge, about what she thought were the main causes of environmental degradation (unsurprisingly, not the meat industry).  Such instances regarding environmental issues are unfortunately all too common. The Today programme could have (instead, or in addition) interviewed an independent academic on the matter. But it appears to have considered the opinion of a commercial sheep farmer to be at least on a par with the latest scientific evidence.  Indeed, my collaborator Rachael Hillyer and I have found that most politicians and mainstream media continue to place environmental concerns in a sphere of debate where industry interests are presented as having equal importance as the future of the planet.  This at a time when scientific evidence on the environmental effects of the meat industry, and how those effects can be reduced, has been compellingly presented. That this debate continues to sit in the sphere of “legitimate controversy” is a bit like having a debate in 2018 on whether smoking is detrimental to your health.  But then democracy has never been very good at tackling the global issue of environmental degradation. Instead politicians often go to great lengths to avoid the topic. When they do engage, they do so begrudgingly, putting all their rigour into a division of responsibility that excuses themselves to the greatest extent.    On the whole, democracies are dominated by chronic short term decision making. And while they often act as safeguards to individual human liberties, democracy, and its preference for compromise, are often part of the problem when it comes to the environment – the biggest issue of them all.  Politicians avoid the reality that only immediate alterations to human behaviour can prevent this crisis. Put simply, the planet urgently needs more compassion for the environment and much less individual ego. The weight of democratic political experience also sits heavy on the minds of politicians. Previous democratically elected leaders have tried to persuade their electorates to think more collectively and to consider the environment before their own selfish pleasures.  US President Jimmy Carter (1977 – 1981), for example, was a keen but moderate environmentalist. Despite his considerable personal wealth he led by example by living modestly, and tried to encourage Americans to lower their carbon footprint and energy consumption.  However, it turned out that America did not like being told to rein in their habits – and Carter was decisively beaten by Ronald Reagan in 1980.  Reagan’s neo-liberal campaign message was: “Make America great again.” Yes, the very same message used by Donald Trump during his presidential campaign. It was a campaign which emphasised the primacy of the economy over the environment.  To this end, democracy cannot fix our environmental issues. Because for every democratically elected environmentally conscious politician in a leadership position, there is another one waiting in the wings to denounce and depose them for economic weaknesses. And in doing so, to relinquish the voters of their duty towards the upkeep of this planet."
"

Hillary Rodham Clinton, the secretary of state who no doubt thinks of herself as “fourth in the line of succession,” tells a European audience how the Obama administration will pass an agenda that Americans have previously rejected: “Never waste a good crisis … Don’t waste it when it can have a very positive impact on climate change and energy security.”   
  
  
As I’ve written several times, governments throughout the decades have taken advantage of wars and economic crises to expand their size, scope, and power. Bob Higgs wrote about “Crisis and Leviathan” long before Naomi Klein called it “The Shock Doctrine.”   
  
  
But the striking thing about the Obama administration is that they openly acknowledge that’s what they’re doing — using a crisis to ram through their entire policy agenda while people are in a state of panic. Projects like national health insurance, raising the price of energy, and subsidizing more schooling — the three prongs of President Obama’s speech to Congress — have nothing to do with solving the current economic crisis. But the administration is trying to push them all through as “stimulus” measures. And they keep proclaiming their strategy.   
  
  
First it was Rahm Emanuel: “You never want a serious crisis to go to waste. And this crisis provides the opportunity for us to do things that you could not do before.” Then Joe Biden: “Opportunity presents itself in the middle of a crisis.” Not to mention Paul Krugman and Arianna Huffington. And now Hillary.   
  
  
Not since George Bush the elder told the media that his campaign theme was “Message: I care” has a president been so open about his political strategy. But these people are displaying a contempt for the voters. They’re telling us that we’re so dumb, we’ll go along with a sweeping agenda of economic and social change because we’re in a state of shock. They may be right.   
  
  
But voters and members of Congress should remember Bill Niskanen’s sobering analysis of previous laws passed in a panic.
"
"

On Tuesday the Supreme Court will hear arguments in _South Dakota v. Wayfair,_ which will force it to decide whether to end the ban on states charging sales taxes on goods sold via the internet from retailers without a nexus in the state. Should it effectively reverse a pair of previous Supreme Court decisions and permit states to do such a thing, it will constitute a significant change in our economy—but those changes won’t include rescuing embattled terrestrial retailers or filling states’ coffers with new tax revenue.



The main outcome will be slower economic growth.



The Supreme Court’s 1992 ruling in _Quill v. North Dakota_ imposed a prohibition on states taxing the sales of remote retailers that exists to this day. The Court found at the time that it was impossibly complex for a remote retailer to know or compute the sales tax owed in thousands of different jurisdictions, and determined that if a retailer had no operations–such as a store or warehouse—in a state, then it did not make sense for it to pay taxes on sales made in that state.



The timing of the _Quill_ case proved to be propitious: Less than two years after its decision Jeff Bezos introduced Amazon to the world. Today, of course, internet retail is enormous, and many of the terrestrial retailers that are struggling these days blame the company for their woes.





The Supreme Court will decide whether to end the ban on states charging sales taxes on goods sold via the internet from retailers without a nexus in the state.



The states have been enthusiastic in having the court—or Congress—end that ban as well: They have blamed the expansion of internet retail for everything from their budget woes to climate change.



However, the notion that states will reap a revenue bonanza by taxing remote retailers is as dubious as the Glengarry leads. Every estimate that has previously been made of the revenue to be gained from such a tax vastly overstated reality, simply because most sales on the internet are already taxed (roughly two thirds, according to a recent study) and internet sales still comprise a small fraction of all retail sales—roughly 11 percent in 2016.



The Government Accountability Office estimates that ending _Quill_ would raise an additional $8 to $13 billion for the states, or roughly 1–2 percent of the $700 billion of revenue the states anticipate collecting in 2018. Or, put in a different context, it roughly amounts to Walmart’s annual tax bill.



And that revenue comes at a significant opportunity cost to the economy. A plethora of research over the last two decades has found that it has been the hyper‐​competitive retail economy of the U.S. that has driven much of the productivity gains that the country has achieved since the mid‐​1990s. Both Walmart and–next–Amazon have ruthlessly pursued methods to reduce costs and increase worker productivity, and they have forced their suppliers to follow suit.



Productivity growth is important because it ultimately determines a nation’s standard of living, economists believe, and the retail sector plays an outsized role in its determination. William Lewis, the founding director of the McKinsey Global Institute, and former Obama CEA chair Jason Furman have both concluded that the nation’s competitive retail sector distinguished us from most other developed countries, and that this is an important reason–if not _the_ main reason–that we have had greater productivity growth the last three or four decades.



Right now, Amazon is winning that productivity race hands down–its sales per employee exceeds $100,000, or twice that of Walmart. It is likely that as Amazon expands its terrestrial offerings and Walmart beefs up its online presence that this gap will shrink, but no one thinks it will disappear anytime soon.



If Amazon can withstand any competitive threat from Walmart, Target, or other big‐​box retail stores, then who _is_ the next retailer who has a chance of beating it at its own game? We can safely venture that it will originate as an internet retailer, much like Amazon did.



However, taxing remote retail constitutes a barrier to future internet retail startups. Amazon undoubtedly benefited in its early years from not having to pay sales taxes in most states when it was competing remotely from a relatively small number of distribution centers and faced more difficult shipping and return logistics, and that ultimately came to benefit consumers–that advantage helped Amazon to hasten the internet marketplace.



Today, Amazon now has a presence in all fifty states in the form of its logistics network and therefore pays sales taxes in each state that has one—to provide the level of service it feels compelled to offer in order to compete against Walmart and Target it has to have warehouses and stores and pickup facilities near its customers.



The retail sector of the economy has, of course, changed dramatically in the last quarter century and in ways that no one anticipated then, or even a decade later. Until the last decade or so retailers were, generally, either solely remote retailers or else sold their wares only via their stores. Today, almost all retail sales are completed by an entity that is an omnichannel retailer, with both an internet presence as well as one or more physical stores.



The notion—implicit in the plaintiffs’ argument—that there are distinct internet and distinct non‐​internet retailers does not at all reflect the reality today. Most existing internet‐​only retailers are small mom and pop businesses selling goods that aren’t readily found elsewhere–a chia pet in the shape of Jerry Garcia’s beard or vintage Buffalo Braves shirts, for instance. A sales tax on these operators reduces the breadth of goods available to consumers without increasing demand for anything from their terrestrial “competitors.”



And for all of the bluster and talk of Amazon acting like a monopoly, it is worth remembering that its annual sales are only one third that of Walmart and behind CVS as well, and it is unclear whether its retail operations—which it doesn’t break out from its other operations—have ever turned a profit for the company; its most lucrative division appears to be its cloud services.



The history of retail shows that a company can dominate the field for only a short period of time. Before Walmart and Amazon there was Sears, Kmart, JC Penney, Montgomery Ward, and Woolworths, each of which innovated the retail environment in some way and became the top dog in the market before being supplanted by a competitor. Today, these companies are defunct or nearly so.



It is a history that Bezos is keenly aware of: He constantly refers to his company being at Day One of its history, explaining that his goal is to forever maintain that same competitive culture so that it can resist the new competitors that will inevitably arise. It is a noble goal but an impossible one.



But one way Bezos can put off its future rivals is to make it more difficult for small companies to grow—and imposing a retail sales tax on even the smallest remote retailer presents a significant barrier for retail start ups. That is undoubtedly the reason why Amazon now _advocates_ for a sales tax on all internet retail.



Imposing an internet sales tax on remote sales won’t make us like France, which carefully circumscribes the retailers’ hours, prices, number of sales, and other behaviors in order to keep at bay the nonexistent bogeyman of unfair competition, but it’s a step in that direction.



State governments with a fiscal problem—and there are many, despite the fact that we are in the ninth year of an economic expansion and unemployment rates are nearing record lows—agitate for the right to tax out of state retailers because they need to place blame somewhere besides their profligacy and inability to coherently govern. Taxing out‐​of‐​state retail sales won’t fix anything–and it will hurt the economy to boot.



Despite the ancient principle of _stare decisis_ , the betting seems to be that this court will indeed rule with the plaintiffs and effectively overturn _Quill._ If so, the U.S. economy would be worse off for it in the long run.
"
"
Share this...FacebookTwitterTemperature stations along the coast of the Antarctic Peninsula indicate “marked statistically signficant cooling” has occurred since 1991, with the Larsen Ice Shelf cooling at a rate of -1.1°C per decade.

Image Source: Bozkurt et al., 2020
Bozkurt et al., 2020
“Observed near-surface temperature trends indicate important contrasts between summer and autumn for the period 1991−2015. A notable summer cooling exists on the northern peninsula (Frei and Marambio stations) and leeward side (Larsen Ice Shelf station). The largest summer cooling trend is observed at the Larsen Ice Shelf station [−0.92°C (10 yr)−1, p < 0.05]. On the other hand, in autumn, San Martin station on the central windward coasts exhibits the largest warming trend [+0.64°C (10 yr)−1 , p < 0.05]. Autumn warming is also notable at the other stations except the Larsen Ice Shelf station. At the annual time scale, there is a clear warming trend at San Martin station [+0.52°C (10 yr)−1 , p < 0.05], whereas at a close latitude on the leeward side the Larsen Ice Shelf station exhibits a marked statistically significant cooling [−1.1°C (10 yr)−1].”
Share this...FacebookTwitter "
"With their own sense of dark irony, the bushfires that have ravaged much of Australia over summer are now closing in on Canberra – just as our leaders prepare to return to the capital to put the nation back together. The profound human cost of lives lost, sacrifices made, wildlife destroyed and dreams shattered has moved the nation, the work of the volunteer firefighters tapping a spirit of mutuality and civic-mindedness that we thought we’d lost. The economic cost of the fires is still being reckoned: from insurance to tourism to ruined local economies, the numbers guys are starting to draft their response to Bill Shorten’s unanswered pre-election question: what is the cost of climate inaction? But what is less clear is the political impact the fires will have on climate policy.  Here are a few things we do know. The prime minister has taken a serious hit to his personal authority for his dereliction of duty before, during and since the fires. He limps back to parliament a diminished leader, facing twin crises of a looming pandemic and an orchestrated vote-buying scandal.  Yet as I pointed out last month, the PM is holding on to his base, aided and abetted by those who create a parallel universe where the fires are the work of greenies and arsonists and nothing the experts say will convince them differently. Despite these determined efforts to gaslight the debate, there is a sense that the summer carnage will shift the needle on climate change and our global laggard of a government will be compelled into more meaningful action. To understand the moment we find ourselves in, I’ll defer to a construct that political scientists use to model how political activism can deliver meaningful policy change: the Overton window.  The Overton window of political possibilities holds that for every issue, there are range of policy responses that sit on a spectrum from radical to sensible and back to radical. As a rule, support for these policy options will follow a bell curve: policies seen as sensible will also be the most popular. When a policy is seen as sensible and popular, it will offer a window for political action. In a static world this would hardly be a revelation – indeed it would be a recipe for both incrementalism and stasis. Except there’s a twist: the Overton window moves over time, influenced by public activism, policy advocacy and external events. That was the idea first laid down by Joseph Overton, head of a libertarian Michigan thinktank in the mid-1990s when trying to articulate a plan to deregulate education in that state. Rather than championing radical, unpopular ideas direct to legislators, he urged the long game, advocating instead to extend the window of what was acceptable, and only when it was in your zone would you pounce through it. Overton died in a plane crash a few years later and never lived to see the malign impact of his theory, but its thinking has driven the right’s success for more than three decades, from mainstreaming market deregulation to marginalising healthcare reform. The left has been less inclined to adopt the window, too often pushing for policy purity rather than the long game of moving the policy spectrum, although the successful campaign for marriage equality is a great example of how this type of fenestration can work. Indeed, the failure of the 2019 election campaign can be explained in part by Labor’s failure to situate their policies within this framework. Instead of demanding policy be set within the Overton zone, Labor presented a patchwork of radical deals without the necessary ballast to make them viable. So is there an Overton climate window? And if so, have the bushfires extended it? We have known for many years that there is passive support for climate action. Our benchmark polls show about 60% of Australians accept climate change is real and a similar number believe the government is not doing enough.  The problem has been when concrete policies have become contested: support for market mechanisms, when compared to the perceived costs in jobs and prices, left the Overton window shut. There has also been consistent support for renewable energy, but the cashed-up efforts from the fossil fuel lobby and their ties to media and government have provided an effective counterpoint to seriously scaling the industry in Australia. But as results in this week’s Essential Report suggest, now there is overwhelming support for concrete action to back renewables and increase targets, while a number of policies that have sat on the fringes of the debate now have majority support. On first blush these are strong numbers for comprehensive climate action, but I would lay down a couple of caveats before we break any glass. First, while the combined support for measures looks strong, only the accelerated development of renewable energy has more passionate supporters than passive. Additionally, by choosing not to offer the chance to give a “no response”, we were forcing a choice on the issue, rather than allowing a shrug of the shoulders. This in itself is an element of the political challenge. Other points worth contemplating are that longer-term options are more popular than shorter-term targets (compare the 2030 and 2050 targets), while proposals for the government to actually ban new coal mines garner much stronger opposition. Based on these findings, the Overton window then would be open around aggressive support for renewables and long-term emissions targets, perhaps with some specific measures to ensure mining companies contribute to the cost of fires. This is reinforced when you look at support for measures via voting intention, where a majority of conservative voters appear to be within the edges of the window. They are right on board with renewables, and while there is majority support for the next layer issues, that support slips below majority once targets become more ambitious and government action more punitive. The idea of pushing for centrist, reasonable and sensible policies may chafe when the world is on the brink. It does not dispel the need to campaign hard at the margins – climate rebellions and school strikes are essential to shifting the window to make other policy change possible. But the risk is to confuse the movement with the moment. If political change is the answer and Australia can’t wait until 2022, then locating the Overton window and finding a way through it now seems the only viable way forward. • Peter Lewis is an executive director of Essential, a progressive strategic communications and research company"
"At the southern tip of the Maldives, on the tiny island of Villingili, a patch of ground rises to tower a whole 2.4m above the sea. It’s the world’s lowest high point.  With most islands just a metre or so above the sea level, it is often suggested that the world’s lowest country may drown beneath rising sea levels by the end of the century. For tourists, this ranks the Maldives atop bucket lists of destinations to visit before they disappear. For the 400,000 people who live on the islands, things are rather more serious: rising sea levels could render them climate change refugees.  However, such scenarios of inundation and drowning assume that the land surface remains static and unchanged. But, what if the land could build vertically as sea level rises?  This is what colleagues and I have been examining in our research, now published in Geophysical Research Letters. We studied five reef islands in the southern Maldives and found that they were actually built when sea levels were higher than they are today. The Maldives is a nation of around 1,200 coral reef islands. Reef islands are unique landforms in that they are formed entirely of sediments produced by organisms such as corals, molluscs and gastropods that live on coral reefs in the surrounding waters.  However, this reliance on the coral reef for island-building sediments, combined with elevations rarely more than a few metres above the sea, means that reef islands are often considered among the most vulnerable environments to climate change, particularly to sea level rise. This is of particular concern for nations such as the Maldives that are built entirely on reef islands, and have nowhere else to go.  


      Read more:
      Venice flooding is getting worse – and the city's grand plan won't save it


 To improve predictions of how reef islands may respond to future environmental change, it is important to understand how they responded to environmental change in the past. To this end, we reconstructed the island-building histories of five islands in the southern Maldives. We first collected 28 reef island “cores”. This essentially involved sledgehammering an aluminium pipe into the reef island until it reached the island “foundations” – a point lower than the live coral in the surrounding ocean. The cores enabled us to access the layers of sediment that have built up throughout the island’s history. We then analysed these sediments under the microscope to find out what exactly the island is made of. In addition, we radiocarbon dated the sediments to determine the when the various layers were created. Results showed that the key phase of reef island building occurred between 4,200 and 1,600 years ago, when sea levels reached around 0.5m higher than they are today.  In addition, this was likely under the influence of large wave events caused by distant storms. These waves would have had the power to break pieces of coral off the reef. Over time, these pieces of coral, as well as sand from the the reef, built up to form the islands. Climate change will mean rising sea levels and even stronger large wave events. It may therefore recreate conditions that are conducive to reef island building, which may enable these islands to keep growing vertically.  This would make the islands more resilient and may even be necessary simply to keep pace with rising sea levels. Our work complements other studies which are showing that islands are in fact dynamic landforms that are able to move and adjust in response to environmental change. All this should make reef islands in the Maldives more physically resilient. However, large waves can also make islands less habitable for humans, for instance by damaging buildings and farmland, or by dumping salt into supplies of fresh water. Reef island nations will have to develop infrastructure that can withstand, or be adaptable to, such powerful waves. Such infrastructure must still allow natural processes to take place however, so that reef islands can maintain active connections to their surrounding coral reefs. While our study suggests that rising sea levels could benefit reef islands in some regards, they still remain at risk. For instance, we also found the islands in our study were made predominantly (about 75%) of coral. This means a healthy reef will be vital if the islands are to keep growing in future, and the Maldives are to remain above the waves. However, coral reefs are also threatened by climate change, not just by rising sea levels, but also by warmer and more acidic oceans. Under climate change, we may therefore end up in an odd situation where we have the perfect conditions to build coral reef islands, but an absence of any building materials."
nan
"
Share this...FacebookTwitterMonster carbon footprint: Six German researchers fly all the way to Ecuador to study how humans are impacting the earth during the Anthropocene – and do lots of hiking at the expense of the public – on a 17-day “expedition”. Some of them, including a musicologist, are of questionable scientific disciplines. 

Chimborazo in Riobamba, Ecuador, Photo David Torres Costales, CC BY-SA 4.0, via Wikipedia here. 
In spring 2020, six members of Die Junge Akademie from the Berlin-Brandenburg Academy of Sciences and Humanities and the German National Academy of Sciences Leopoldina from a range of disciplines – departed on “Expedition Anthropocene”.
“One of the focal points of the expedition,” the website says, “is climate change and its consequences for the environment as well as the transition of a region over the past 200 years.”
“In this, humans are consistently viewed as the instigators, those affected and the observers of these events.”
Lots of hiking
Their research took them to Ecuador and the volcanic mountain Chimborazo. Due to its location close to the equator, the summit of Chimborazo is the highest point on the planet when measured from the Earth’s center.
Impacts from “advancing climate change”
According to the six German researchers, “Together with our local partners, we go in search of traces of human activity in this environment” by using “methods from glaciology, biology, chemistry, acoustic ecology, computer science and medicine, we will investigate the human impact on Chimborazo at different altitudes – from advancing climate change and its consequences for humans, glacial retreat and biodiversity, to acoustic ecological changes and the question of whether microplastics can be detected in the snow and ice.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A junket disguised as science?
But already some are criticizing the expedition. Not only because of the carbon footprint the long travel and extensive accommodation, but because of it has the appearance of a junket disguised as a scientific expedition.
“The first stop on our expedition is Quito, the capital of Ecuador and the world’s highest capital city. We will spend a few days in the city to give ourselves time to acclimatize to the altitude, and set out from here on our first day trips,” the site explains.

Locations to be studied by among others,  a musicology professor. Chart source: Expedition Anthropocene.
The team were planned to hike to the active volcano Pichincha and the inactive volcano Chimborazo – the highest point above Earth’s center – as well as to the Ambato and the Llanganates National Park where they would “spend a few days in a complementary vegetation zone to the mountainous regions of the Andes.”
Not a single climate-related scientist – one musicologist
Of course long hikes to sites are all part of many expeditions and thus perfectly legitimate. But controversy swirls concerning the background of members of the German team.
Many have nothing or little to do with climate science. German science site Die kalte Sonne here noted: “No single geologist, geographer or glaciologist is involved. Instead there is a musicologist, a medical doctor (okay, maybe because of the altitude), a computer scientist and (EVEN!!) a physicist. The physicist comes from the PIK!”
“Six young people simply claimed that they would study the consequences of climate change and without further ado they jetted off to Ecuador. A nice example of a real-life satire: young up-and-coming artists (some of them really artists) are “researching” climate change in South America in Humboldt’s footsteps,” Die kalte Sonne writes.
Share this...FacebookTwitter "
"
Boulder is home to National Institute of Standards and NOAA’s research lab…big government facility and probably the most secure weather station in the USA, I had to go through metal detectors, have mirrors run under my vehicle, be photographed, and my drivers license verified.
Took 2 hours…on the road at the moment to get another station in Colorado, blogging via WiFi from Starbucks
Will post new pix soon.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea41b293b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The latest Living Planet report from the WWF makes for grim reading: a 60% decline in wild animal populations since 1970, collapsing ecosystems, and a distinct possibility that the human species will not be far behind. The report repeatedly stresses that humanity’s consumption is to blame for this mass extinction, and journalists have been quick to amplify the message. The Guardian headline reads “Humanity has wiped out 60% of animal populations”, while the BBC runs with “Mass wildlife loss caused by human consumption”. No wonder: in the 148-page report, the word “humanity” appears 14 times, and “consumption” an impressive 54 times. There is one word, however, that fails to make a single appearance: capitalism. It might seem, when 83% of the world’s freshwater ecosystems are collapsing (another horrifying statistic from the report), that this is no time to quibble over semantics. And yet, as the ecologist Robin Wall Kimmerer has written, “finding the words is another step in learning to see”. Although the WWF report comes close to finding the words by identifying culture, economics, and unsustainable production models as the key problems, it fails to name capitalism as the crucial (and often causal) link between these things. It therefore prevents us from seeing the true nature of the problem. If we don’t name it, we can’t tackle it: it’s like aiming at an invisible target. The WWF report is right to highlight “exploding human consumption”, not population growth, as the main cause of mass extinction, and it goes to great lengths to illustrate the link between levels of consumption and biodiversity loss. But it stops short of pointing out that capitalism is what compels such reckless consumption. Capitalism – particularly in its neoliberal form – is an ideology founded on a principle of endless economic growth driven by consumption, a proposition that is simply impossible. Industrial agriculture, an activity that the report identifies as the biggest single contributor to species loss, is profoundly shaped by capitalism, not least because only a handful of “commodity” species are deemed to have any value, and because, in the sole pursuit of profit and growth, “externalities” such as pollution and biodiversity loss are ignored. And yet instead of calling the irrationality of capitalism out for the ways in which it renders most of life worthless, the WWF report actually extends a capitalist logic by using terms such as “natural assets” and “ecosystem services” to refer to the living world. By obscuring capitalism with a term that is merely one of its symptoms – “consumption” – there is also a risk that blame and responsibility for species loss is disproportionately shifted onto individual lifestyle choices, while the larger and more powerful systems and institutions that are compelling individuals to consume are, worryingly, let off the hook. The WWF report chooses “humanity” as its unit of analysis, and this totalising language is eagerly picked up by the press. The Guardian, for example, reports that “the global population is destroying the web of life”. This is grossly misleading. The WWF report itself illustrates that it is far from all of humanity doing the consuming, but it does not go as far as revealing that only a small minority of the human population are causing the vast majority of the damage.  From carbon emissions to ecological footprints, the richest 10% of people are having the greatest impact. Furthermore, there is no recognition that the effects of climate and biodiversity collapse are overwhelming felt by the poorest people first – the very people who are contributing least to the problem. Identifying these inequalities matters because it is this – not “humanity” per se – that is the problem, and because inequality is endemic to, you guessed it, capitalist systems (and particularly their racist and colonial legacies). The catch-all word “humanity” papers over all of these cracks, preventing us from seeing the situation as it is. It also perpetuates a sense that humans are inherently “bad”, and that it is somehow “in our nature” to consume until there is nothing left. One tweet, posted in response to the WWF publication, retorted that “we are a virus with shoes”, an attitude that hints at growing public apathy.  But what would it mean to redirect such self-loathing towards capitalism? Not only would this be a more accurate target, but it might also empower us to see our humanity as a force for good. Words do so much more than simply assign blame to different causes. Words are makers and breakers of the deep stories that we construct about the world, and these stories are especially important for helping us to navigate environmental crises. Using generalised references to “humanity” and “consumption” as drivers of ecological loss is not only inaccurate, it also perpetuates a distorted view of who we are and what we are capable of becoming.  By naming capitalism as a root cause, on the other hand, we identify a particular set of practices and ideas that are by no means permanent nor inherent to the condition of being human. In doing so, we learn to see that things could be otherwise. There is a power to naming something in order to expose it. As the writer and environmentalist Rebecca Solnit puts it: Calling things by their true names cuts through the lies that excuse, buffer, muddle, disguise, avoid, or encourage inaction, indifference, obliviousness. It’s not all there is to changing the world, but it’s a key step. The WWF report urges that a “collective voice is crucial if we are to reverse the trend of biodiversity loss”, but a collective voice is useless if it cannot find the right words. As long as we – and influential organisations such as the WWF, in particular – fail to name capitalism as a key cause of mass extinction, we will remain powerless to break its tragic story."
"It is widely agreed that today’s global agriculture system is a social and environmental failure. Business as usual is no longer an option: biodiversity loss and nitrogen pollution are exceeding planetary limits, and catastrophic risks of climate change demand immediate action. Most concede that there is an urgent need to radically transform our food systems. But the proposed innovations for more sustainable food systems are drastically different. Which we choose will have long-lasting effects on human society and the planet. Suggested innovations in food systems can be broadly understood as either seeking to conform with – or to transform – the status quo.  Some want to keep the agriculture industry as close to existing practices as possible. This is true of the increasing number of corporate and financial actors who seek to solve the food crisis by developing new technologies. These technologies are envisaged as being part of what is being called the “fourth industrial revolution” (4IR). The “answer” here is thought to lie in a fusion of technologies that blurs the lines between physical, digital and biological domains. For example, the World Economic Forum is currently supporting agricultural transitions in 21 countries through its “New Vision for Agriculture” initiative. This initiative supports “innovation ecosystems” to re-engineer food systems based on “12 transforming technologies”. In this imagined future, next generation biotechnologies will re-engineer plants and animals. Precision farming will optimise use of water and pesticides. Global food systems will rely on smart robots, blockchain and the internet of things to manufacture synthetic foods for personalised nutrition. Like previous green revolution technologies in agriculture, this effort is designed by and for powerful agricultural giants. These technological innovations reinforce the concentration of political and economic power in the hands of a small number of corporations. Indeed, the latter have a growing monopoly control over the “12 transforming technologies” protected by patents. Most notably, the spread of these technologies will expand the technosphere at the expense of the biosphere. Flying robots will pollinate crops instead of living bees. Automated machines will replace farmers’ work on soil preparation, seeding, weeding, fertility, pest control and harvesting of crops.  These hi-tech innovations radically depart from most farming practices. They are moving us towards an increasingly people-less food system. Yet they show a remarkable continuity with the logic of capitalist accumulation – hence their staying power despite their significant risks. The spread of automated, de-localised and digitalised production and commercialisation of food is part of the “financialisation” of the global food system. Financial markets play an increasing role in controlling food systems from a distance. This generates huge social and human risks. For example, the significant growth in the sale and purchase of financial products linked to food commodities was one of the determining factors in the 2008 world food crisis. But there is an alternative to this future. Agroecology involves the application of ecological principles for the design and management of sustainable agroecosystems. Our research on agroecology focuses on how it can contribute to food sovereignty, which emphasises the democratisation of food systems. Agroecology’s contribution to the Sustainable Development Goals is now recognised. In contrast to the technological vision described above, agroecological innovations promote circular systems that involve recycling, reuse and combining resources to reduce dependency on external inputs, in particular fossil fuels. They mimic natural cycles and the functional diversity of natural ecosystems.  Farming systems are designed in a way that is based on beneficial interactions between plants, animals and environments. Trees and shrubs might be planted amongst or around crops, say. Or two or more crops might be grown in proximity. Agroecology reduces the dependence of food producers on expensive external inputs, distant commodity markets and patented technologies. This is achieved by relying on appropriate biodiversity to ward off pests and increase farm yields. At broader scales, agroecology involves circular systems that combine food and energy production with water and waste management. Pollution is minimised and synergies achieved by carefully clustering industries into functional wholes. The re-localisation of production and consumption within territories enhances local economic regeneration and sustainability.  Agroecological innovations in transitions to sustainable food systems are being driven largely from the bottom up by civil society, social movements and allied researchers. In this context, priorities for innovations are ones that increase citizen control for food sovereignty and decentralise power. This is in direct contrast to the monopoly control enabled by 4IR technologies. Government, civil society and private sector representatives will soon meet in Rome at the United Nations Food and Agriculture Organization to discuss the future of farming. Who controls the global governance of innovation will be a hotly debated topic. But given these highly contested views on innovations for food and agriculture, it is vital that everyone is able to exercise their right to have a say on the future of their food supply. Deliberative and inclusive processes such as citizens’ juries, peoples’ assemblies and community-led participatory processes are urgently needed to decide priorities for food and agricultural innovations. This is all the more important in today’s context of rapid global change and uncertainty.  So. Do you want to live in a world in which artificial food is produced by intelligent robots and corporations that put profits before people? Or one where agroecological innovations ensure we can nourish ourselves and our communities in a fair, ecologically regenerative, and culturally rich way?"
"
Share this...FacebookTwitter
Image: NASA Earth Observatory. Public Domain
Prof. Fritz Vahrenholt’s Monthly Solar Report
The global mean temperature in April 2020 was again significantly lower than in February and March, at 0.38°C above the average from 1981 to 2010. The average temperature increase on the globe from 1981 to February 2020 was 0.14°C per decade. The further development promises to be interesting, especially since a number of research institutes expect a higher probability of a cooling La Nina in the Pacific towards the end of the year. March’s solar activity was very low with a sunspot number of 1.5.  Activity in April rose slightly to 5.4. The first sunspots of the new cycle are showing.
What causes the sun to have an 11-year cycle?
Since the Dessau pharmacist Heinrich Samuel Schwabe discovered in 1843 that the sunspots of the sun increase and decrease in an 11-year cycle, science has been puzzling over the reason why this cycle lasts 11 years and why the solar magnetic field also changes its polarity in this rhythm: the north pole becomes the south pole and vice versa.
In July last year, scientists at the Helmholtz Centre in Dresden Rossendorf made a little-noticed but exciting discovery. Every 11.07 years, the planets Venus, Earth and Jupiter are aligned quite precisely. At this point in time, their gravitational force acts jointly in one direction on the Sun.
“The agreement is amazingly accurate: we see a complete parallelism with the planets over 90 cycles,” explains Frank Stefani, one of the authors of the publication published in Solar Physics. Just as the gravitational pull of the Moon causes the tides on Earth, planets could move the hot plasma on the surface of the Sun. But the effect of a simple gravitational force is too weak to significantly disturb the flow in the Sun’s interior, so the temporal coincidence has long been ignored.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Now the researchers assume that the layers of the plasma are subject to a Taylor instability. The Taylor instability is known from the behavior of liquids of different densities at their interface (we know the turbulence that occurs when milk is poured into a cup of tea).  Taylor instability is sensitive to even very small forces. A small burst of energy is enough for the polarity of the solar magnetic field to swing back and forth every 11 years. The necessary impulse for this could be provided by the tidal action of the planets – and thus ultimately determine the rhythm in which the sun’s magnetic field reverses its polarity.
The tidal forces of the planets could have other effects on the Sun in addition to their role as pace-setter for the 11-year cycle. For example, it would be conceivable that they could change the stratification of the plasma in the boundary area between the inner radiation zone and the outer convection zone of the Sun, the tachocline, in such a way that the magnetic flux could be more easily dissipated.
Under these conditions, the strength of the activity cycles could also change, just as the “Maunder Minimum” once caused a significant decrease in solar activity over a longer period, the researchers write on the Helmholtz Center website. It is an unusual idea that the activity of the sun is controlled by the planets, including the earth itself. This sounds like astrology – but it is the latest in solar research.
One of the first researchers who assumed an influence of the solar activity by the planets was Theodor Landscheidt, who already in 1988 in his book “Sun-Earth-Man” predicted the decreasing strength of the solar cycles 22 and following. However, he assumed a different mechanism, according to which the planets cyclically move the sun out of the center of gravity (barycenter) of our solar system. Landscheidt died in 2004.
And also in our book “The Forgotten Sun” we had invited Prof. Nicola Scafetta for a separate chapter, who already then interpreted the conjunction of Saturn and Jupiter as the cause of a 60-year cycle. In a publication published in Solar Physics in February 2020, he also relates the longer-term oscillations (Hallstatt -2400 years ,Eddy – 1000 years, Suess-de Vries – 210 years) to influences of the large planets of Jupiter, Saturn, Uranus and Neptune. The long version is accessible here.
Fritz Vahrenholt


		jQuery(document).ready(function(){
			jQuery('#dd_51502fcc0d8cc70a6aed31f4b468444e').on('change', function() {
			  jQuery('#amount_51502fcc0d8cc70a6aed31f4b468444e').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe COVID-19 pandemic has exposed how scientific dissent is not only being suppressed and marginalized in Germany when it comes to climate science, but with virology and public health.

The online German national daily Die Welt here writes, “Virologists and physicians fear for their freedom of expression in the Corona crisis.” Climate scientists are getting some company!
Scientific freedom “under threat”
Citing the results of a recent survey, to Die Welt reports “many experts believe that freedom of expression in science is under threat” and “virologists have begun to change their attitude towards the measures taken by the German government.”
As the economy reels from the stringent restrictions enacted by authorities across Europe, the discussion about how to respond to the spread of the virus has become bitter. Public health experts opposing the restrictions and lock downs have found themselves marginalized and attacked by the media, other virologists and most politicians.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Shift: COVID-19 alarmism waning, frustration growing
Using a survey, “Researchers at the University Hospital Eppendorf in Hamburg (UKE), the Society for Virology (GfV) and the University of Tübingen have tried to determine the mood among experts,” Die Welt reports. “178 experts from the fields of virology, immunology, hygiene, internal medicine and intensive care medicine” were surveyed anonymously. The results show that “more than more than 70 percent of the participants support the distance rule of two metres and the prohibition of major events” but that the other restrictions were “far more controversial”.
One third feel freedom in science is “being threatened”
What’s surprising: “One third even see freedom of expression in science as being threatened” and today 63 percent think “it would be sensible to restore public and economic life”. Also: “social distancing” is apparently losing support, according to the survey.
Overall, the findings show that the pandemic skeptics are finally beginning to assert their views to the public.
Unfortunately this is not even close to happening in climate science, where in Germany the ultra-alarmists continue to control the message. That freedom to express science is under threat, “is probably what many climate scientists secretly think,” Die kalte Sonne site here commented.
Perhaps COVID-19 tells us how moods can change quickly once restrictions start eroding freedom and prosperity.
Share this...FacebookTwitter "
nan
"
Share this...FacebookTwitterA new study assesses a reduction in tree cover via urbanization or by clearing forests for cropland can warm up a locality by 1°C within 10 years. In contrast, transitions from croplands and urban centers to forests leads to cooling. Europe has been cooling recently (1992-2015) from land cover transitions to forests.

Image Source: Huang et al., 2020
Urbanization adds multiple degrees of warming over decades
A few years ago a compelling analysis (Levermore et al., 2018) found the urban heat island effect can reach intensities of 8°C warmer temperatures than nearby rural sites.
Further, reducing the green (trees and vegetated areas) in an urban center by as little as 11% can lead to a 0.21°C per decade (non-climatic) warming trend in the local thermometer record.

Image Source: Levermore et al., 2018
Global warming can be reversed via land cover changes
While forest losses can heat up local temperatures by as much as 1°C within 10 years (Alkama and Cescatti, 2016), a new study (Huang et al., 2020) has assessed the opposite can occur too.
From 1992 to 2015, there were about 70 million hectares (Mha) of land cover changes (LCCs) occurring across the European continent.
A substantial portion of these LCCs were “cropland-to-forest” transitions due to agricultural abandonment.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




When a region returns to forest and tree cover, cooling ensues.
And with a growing percentage of European forested areas returning, a “predominant regional biophysical cooling” with “an average temperature change of −0.12 ± 0.20 °C, with widespread cooling (up to −1.0 °C) in western and central Europe in summer and spring” has swept across Europe due to LCCs in recent decades.

Image Source: Huang et al., 2020
The substantial impact of land cover changes
The implications of this study are profound.
First, the human effect on CO2 concentration changes appears to have minimal effect on local and regional temperatures relative to the much larger impact from land use changes.
More importantly, if reducing global warming is indeed the goal of policy makers, then denuding forests so as to install hundreds of steel-and-concrete wind turbines would appear to achieve the opposite of what it is claimed to do (reduce warming).
So why on Earth are we doing this?

Image Source: The Telegraph
Share this...FacebookTwitter "
"
The picture below is from Oregon State Climatologist George Taylor. You may have heard of him, the Governor of Oregon tried to get him fired for not jumping on to the global warming bandwagon because he doesn’t see enough supporting evidence.

The picture is of Forest Grove, Oregon, and the temperature plot below shows how it is warming. But George says:
“Yes, it’s a window air conditioning unit to the east and the edge of a large asphalt parking lot to the north, northwest, and west. The pic is shot looking northeast. For those of you that may not immediately realize this, air conditions exhaust hot air to the outside.

Not only that, but Forest Grove is located in Washington County, Oregon’s fastest-growing county (in terms of population growth, not percentage) for the last 40 years. No wonder it’s seeing unprecedented high temperatures…”
It looks like the air conditioner may have been installed around 1985, notice the sustained 1 degree jump that started about then and sustained a plateau.
And this is a station of record, a US Historic Climatology Network station that is used in global climate models by NASA, in fact the plot is from that database.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea603b6f9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"A group of more than 200 scientists will on Monday urge returning parliamentarians to urgently reduce Australia’s total greenhouse gas emissions, and work diplomatically to achieve coordinated global climate action, after a catastrophic summer of fires. In an open letter timed to coincide with the resumption of the parliamentary year in Canberra, the group says scientific evidence unequivocally links human-caused climate change to the increasing risk of frequent and severe bushfires in the Australian landscape.  It says that same science tells us “these extreme events will only grow worse in the future without genuine concerted action to reduce global emissions of greenhouse gases”. The letter says the science suggests a need for immediate action to reduce total greenhouse gas emissions, and manage a rapid transition to net zero emissions by 2050. One of the signatories to the open letter, the Australian National University climate scientist Nerilie Abram, says the letter is “the product of despair as scientists witnessed the deadly fire season unfold”. “Scientists have been warning policymakers for decades that climate change would worsen Australia’s fire risk, and yet those warnings have been ignored,” she said. Given the catastrophic summer, with the bushfires triggering a barrage of commentary about the inadequacy of the Morrison government’s current policy, a number of Liberal MPs are returning to Canberra for the opening session parliament with the view that the government needs to do more on climate change and energy policy. Cabinet has also discussed the issue. The strongly worded appeal by scientists comes as a new energy market analysis by Reputex predicts Australia will hit 50% renewable electricity by 2030, despite the lack of a federal energy policy. But it also warns a slump in new investment in wind and solar investment could threaten a continued decline in wholesale electricity prices. Reputex says the current drive to 50% renewables by 2030 is being driven by state renewable energy targets and rooftop solar schemes that are predicted to make higher cost gas and coal-fired power less competitive. It says that transition is expected to deliver a decline in wholesale prices in the national energy market from around $80 per megawatt-hour (/MWh) in 2020 toward $70/MWh over the next three years, which translates as a fall of 15% from today’s levels. But it warns investment in renewable energy plunged by 50% last year compared with the year before, and it predicts that decline will continue – a development that will put upward pressure on wholesale electricity prices, particularly as major coal-fired facilities begin to close. “In the absence of an effective policy framework to guide new investment, the decline of our ageing generation fleet will lead to higher electricity prices before the new supply is developed, hurting both businesses and consumers,” the assessment says. In an effort to emphasise a positive message on the issue, the treasurer, Josh Frydenberg, told the ABC on Sunday the Morrison government’s 2030 emissions reduction target of 26% on 2005 levels by 2030 was a floor and not a cap, “and we hope to beat our target”. But while declaring the government wanted to beat the 2030 target, Frydenberg said the target wasn’t going to be adjusted. “We took to the Australian people a very clear target, so we’re not about to lift that target,” the treasurer said. “What we are endeavouring to do is to meet our commitments now.” Frydenberg did not mention that the government intends to use an accounting measure, carryover credits from the Kyoto period, to meet the 2030 target, with carryover credits, not practical abatement, supplying about half the pollution reduction load to 2030. The treasurer also played up the government’s commitment to renewable energy. Frydenberg said on Sunday that more than $7bn was invested in renewables last year. He omitted the Coalition’s efforts under Tony Abbott to gut the renewable energy target (RET), and the fact the RET winds down from this year – with policy uncertainty triggering the decline in new investment captured in the Reputex analysis. Asked whether he believed there was now a climate emergency, the treasurer hedged. “Climate change is a significant challenge, a global challenge that needs a global solution,” he said. “We’re doing our bit and we’re also working on the international stage.” Pressed to explain how climate change was not an emergency, Frydenberg said “it’s an important issue, but as the prime minister has outlined, there are a lot of things that we can do with mitigation and adaptation to try to reduce the impact of climate change on the Australian community”."
"
Share this...FacebookTwitterGerman online site Stromreport writes that since the year 2000 the average electricity price for private households has risen from 13.94 to 30.43 euro cents per kilowatt hour (2019).
German electricity prices for households are among the highest worldwide.

Image: Statista.com
The price increase has little to do with demand or markets, but almost everything to do with government interference. According to Stromreport, “Taxes, charges and levies have tripled since 2000 [from 5.19 to 16 cents]. In total, German government charges now account for more than half of the electricity price [52.5%].”
Electricity becoming a luxury
Annually hundreds of thousands of German households see their power cut off due to unpaid power bills. For example in 2018, the Tagesspiegel here reported: “In the past year, almost 344,000 households in Germany had their electricity turned off. This is according to the monitoring report of the Federal Network Agency on the electricity market.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Of course the high prices hit the poor the hardest.
More price hikes in 2020
And things are not going to improve for Germany’s overburdened power consumers in 2020. Stromreport writes that 403 suppliers have already raised electricity prices by an average of 5.3% this year already, bringing the price to a whopping 30.43 cents per kilowatt-hour. “A 3-person household currently pays almost 89 euros for its electricity. That is 27% more than 10 years ago [69.09 euros].”
Now comes the CO2 charge
The price in 2020 is expected to reach 31.47 cents per kilowatt-hour. Also the wholesale prices for electricity are expected to rise in 2020, due to “rising CO2 prices”…”which will make electricity from coal and gas more expensive on the electricity exchange,” says Stromreport.
Another major component of the German power price are the green energy feed-in tariffs for power coming from, for example, wind and sun. German consumers pay 6.756 cents for kilowatt-hour.
Hat-tip: Die kalte Sonne.


		jQuery(document).ready(function(){
			jQuery('#dd_79cf92e8191e2141d3be52723b8df435').on('change', function() {
			  jQuery('#amount_79cf92e8191e2141d3be52723b8df435').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterHardcore German left-wing activist Tom Radke – citing selfish leaders, fraudulent science, psychological manipulation and a cult-like atmosphere within the German Greens – has had enough of the Fridays For Future (FFF) Germany movement and has announced his resignation.  

German left-wing activist, former FFF participant Tom Radke quits movement after experiencing its inner working. Image cropped from Tom Radke.de.
In a statement published at his site, Tom Radke wrote he wishes “to concentrate on left-wing patriotic politics: environmental protection, a strong welfare state and peace” instead of “Green voters and ‘green’ corporations”.
FFF not about the environment
Radke writes how he “found out through very negative practical experiences with Fridays for Future that many people never cared about environmental protection” and that he “misjudged the other ‘climate activists”.
He wrote: “Of all activists at FFF Hamburg, only a few were really interested in preserving our environment, clean air and healthy food.”
Radke blasts the FFF leader “Longhaul Luisa” Neubauer who he believes was “obviously in it for a career” and “her luxury life” while she called on others to save money.
Greta “taken advantage of”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Radke also had the impression that FFF movement leader Greta Thunberg was “being taken advantage of by her family but that she was “personally a nice person”.
Climate science “largely manipulation and fraud”
He recognized “the nonsensical and anti-people content of the climate movement” and that the movement’s “climate science is largely manipulation and fraud”.
Movement based on “pure emotions and blind faith”
Radke also explains how he came to realize that the FFF activists in fact had very little knowledge about climate science itself and that “their fanaticism is largely based on pure emotions and blind faith” and “also based on fear-mongering”.
“The young people are told that they, their families and everyone they love will die if we don’t act immediately.”
“Leaving the cult”
Radke accuses the German Green Party Using “a lot of psychological pressure” to coerce donations from followers despite the fact that “there are major donors in the background, who are completely unknown to the ordinary members.”
Radke also writes that the FFF movement “has nothing to to do with real environmental protection” and that “the CO2-tax serves to squeeze even more out of ordinary people.
He summarizes his departure as follows: “I feel a little like a person who is leaving the cult. It is a liberating feeling. Through my experience I will try to help other students to leave FFF and the climate religion.”


		jQuery(document).ready(function(){
			jQuery('#dd_47acf6f0d6d4644f09cfbc459ffd60db').on('change', function() {
			  jQuery('#amount_47acf6f0d6d4644f09cfbc459ffd60db').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
nan
"
Share this...FacebookTwitterThe “Streetscooter”: Electric Mobility’s First Large Bankruptcy

Image: From Superbass – own work, CC BY-SA 4.0, commons.wikimedia.org
By AR Göhring, European Institute for Climate and Energy (EIKE)
(Text translated and summarized by P. Gosselin)
Manufacturer “Streetscooter”, purchased by Deutsche Post (German Post) in 2014, will be scrapped. The German media of course blame it on “bad management” by the large company.
Which city dweller doesn’t know the small, yellow electric scooters of the German post office that the postmen and women deliver letters and small packages to citizens comfortably and efficiently? Not long ago I received news via Facebook on how the e-delivery-vehicles just barely made it back to the post office, especially in winter, and only when the heating is off.
Now the management of the Swiss Post is also following suit and ending the experiment with delivery street scooters.
The company used to be a small startup, a young dynamic private company in a “sexy” field – just like artificial intelligence or climate protection technology. Deutsche Post bought the company with the benevolent support of the eco-loving press and used it to polish up its otherwise staid image a bit.
However, any PR coup based on electro-chemistry ultimately has to prove itself in everyday life over years. The post office scooters obviously couldn’t. Pushing an electric vehicle still loaded with letters back to the local depot when the battery is empty is not possible: the scooter is too big and heavy for that. Or you have to plan shorter routes (in winter), which reduces efficiency. Since letters are only delivered during the day, the scooters can be conveniently charged at night. But if you have to reload during working hours, it takes hours, and you don’t have the time for that.
Berlin e-bus failure


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Take, for example, the Berlin E-bus experiment: the lithium buses run from 8 to 12 a.m., then the diesel vehicles take over. Our speaker Prof. Alt talks in this context about a double infrastructure, which is of course also roughly twice as expensive. Presumably Deutsche Post had to manage a similarly inefficient double fleet of about 13,000 street scooters. The scooters broke down more often and then soon had to be repaired, and replaced by diesel-powered delivery vans.
A commentator from ntv television, however, blames it on the slow management of Deutsche Post: A project like an electric fleet of electric cars has to be run by flexible start-up managers with heart and attitude, then it would work.
The Streetscooter deserved a dynamic, creative and risk-taking management – and the opportunity to obtain the necessary funds independently on the capital market.”
This claim is not convincing. Whether it’s a startup or the Deutsche Post, both must adhere to the main laws of physics and economics. One thing must never be forgotten: Deutsche Post is a business group that has to make money.
The city administration of Berlin, on the other hand, can waste money at will with misguided planning. They work with funds from taxes levied by force. And Berlin’s eco-socialist politicians, who are poor in arithmetic, are elected and are not held accountable for their failures with their own private assets.
Of course some will claim that Tesla has achieved what the N-TV quote above calls for. But this is not true: Elon Musk is an eco-media darling who has already received billions of dollars in US subsidies. Without these billions he would have long since gone bankrupt or become a mini-manufacturer for a niche.
We Germans are now experiencing the same thing in Brandenburg: because Merkel’s “grand coalition” wants to have a share in the media sexiness of Tesla, the “Gigafactory” is being heavily subsidized there.
The fact that an entire forest is being cut down and cheaper Polish workers have to be hired is of no consequence to someone like Federal Economics Minister Peter Altmaier. The press as well.


		jQuery(document).ready(function(){
			jQuery('#dd_3a04978ab8a5ce78325435c2a3e12065').on('change', function() {
			  jQuery('#amount_3a04978ab8a5ce78325435c2a3e12065').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"This is an article from Head to Head, a series in which academics from different disciplines chew over current debates. Let us know what else you’d like covered – all questions are welcome. Details of how to contact us are at the end of the article. Sharon George: Plastics are ingrained in our everyday lives. Since 1950, it’s estimated that we have produced billions of tonnes of plastic, and most of this is not recycled. Plastics have spread around the world through oceans, rivers and the air to every part of the planet. In rivers and oceans, plastic moves vast distances and is now found right through the water column of the oceans, from the surface to the deepest trenches. We don’t yet know how long this material will prevail in these environments but it will certainly be longer than the lifespan of the person who used it. And it’s accumulating. This impact, like rising CO₂ levels, is man-made. No amount of recycling schemes and ocean clean-ups are going to totally remove the mark that we have left. Plastics are a scar that will hopefully warn future generations of the folly of unsustainable over-consumption. Matt Edgeworth: As well as spreading through air, rivers and ocean currents, plastics are finding their way into soils, landfills and deep ocean sediments. As such, they are infiltrating into strata – layers of rock and mud in the ground – thus becoming part of the archaeological and geological records. They are not only to be found in surface environments, where they are highly visible, but are getting into subsurface layers too.  In some of these buried environments, plastic objects stand a good chance of being fossilised – a process whereby the hard material may decay or dissolve, but its exterior form survives as a mould which then gets filled with other minerals and becomes a cast of the original object. In this way, familiar forms of commonplace plastic items such as drinking straws may survive as traces in the ground not just for a few hundred, but for millions of years. SG: Polymers from natural materials were used by people as early as 1200BC, when the Olmec people used latex and vine extract to create rubber. In the 1840s, sulphur was used to vulcanise rubber, stabilising it and making tyre production possible. A closely-related material, gutta-percha, is a natural latex. This early thermoplastic was used from the mid-1800s, enabling telegraph wires to be laid at the bottom of the sea and electrical wires to be insulated. Other natural polymers resembling modern plastic were developed from cellulose, a natural polymer found in wood. The first, parkesine, was developed to produce celluloid in 1870, a medium for cinema film. But it was during the 20th century that plastics really took off. In 1907, Leo Baekeland invented the first synthetic plastic, bakelite, from fossil fuel-based chemicals. These revolutionary plastics were easy to mould and could be quickly mass produced. These materials were popular, cheap and built to last. The pace of development increased and by 1935 other polymers, such as polystyrene, polyester, PVC, polythene and nylon were all being manufactured from fossil fuels. ME: That’s an excellent historical summary of plastics as a modern material. But as an archaeologist, I see the development of plastics as part of a much longer and broader set of technological trends, extending back over the last 20,000 years or so to the first appearance of “novel materials” in strata.  Novel materials are characterised by their geological novelty, being unprecedented in earlier deposits. Made by humans rather than by natural process, they are entirely new in the four-and-a-half-billion-year history of the Earth. Ceramics appeared first – then bricks and tiles, glass, metal alloys, concrete, paper and so on. Look in any rubbish dump today and you will find all these novel materials and more in the profusion of items thrown away.  Plastics may be a relatively new development, but they are part of this trend towards greater diversification of humanly manufactured materials, all of which eventually find their way into the ground. SG: Despite their relative novelty and permanence, we continue to produce and pour plastics into the environment. Today it is thought that around 80% of the 8.3 billion tonnes of plastic ever made is still somewhere out there. ME: A large proportion of plastic waste is being dumped in landfill, where it remains “out of sight, out of mind”. What we see in the oceans is only the more visible tip of a largely invisible iceberg. Let me give an example. Close to my home in south Bedfordshire are a series of landfilled quarries, now low artificial hills. The old clay pits, some of them over a kilometre wide and up to 55m deep, made convenient receptacles for landfill waste from London and other nearby cities and towns from the 1980s on. Even when the pits were full to the brim, more landfill was mounded up to form hills.  If you stand on top of the highest of these newly created hills, where the former quarry was deepest, over 65m of compacted landfill lies directly beneath your feet. In landfills of similar age in the USA, the proportion of plastic was found to be 20-24% by volume when sorted, reducing to about 16% by volume when compacted in the ground. Assuming the same proportion of plastic at this location, that would be the equivalent of a layer of plastic 10m thick.  Meanwhile, landfill material by no means always stays where it has been deposited. For example, many thousands of landfills are situated in lowland situations and therefore at risk from marine incursion, especially in view of anticipated sea-level rise due to climate change. Coastal erosion, river flooding and tsunamis can decimate landfills, leaving the heavier material where it is but taking the lighter and more mobile material such as plastic away. A significant proportion of plastic currently in the oceans derives from inundated landfills. SG: Once plastic escapes from landfill it will continue to degrade into smaller fragments. It can be ingested by creatures like birds and fish and get into drinking water. Microplastics and nanoparticles are already showing up in our food chain and tapwater.  The risks to human health of ingestion of nanoplastics is not fully known. But with rising numbers, our exposure is bound to increase and people reliant on fishing from highly polluted regions will be more exposed. The pollution is wreaking havoc on wildlife, with animals being entangled or ingesting the plastic. Around 90% of seabirds have ingested plastic. We are adding around 8m tonnes of waste every year to the ocean and unless this stream of waste is cut off, the problem is going to get much worse. ME: But with all the focus today on the harm caused by plastics in rivers and oceans, is there not a danger that putting plastic waste into landfill might be seen as a less controversial alternative? In conveniently burying it out of sight, often oblivious to the fact that it might get released back into the wider environment at a later date, are we just storing up problems for future generations? Should we not be focusing on the dangers of disposal of plastics in earth as well as in water? SG: Yes, we should be focusing on the dangers of plastic disposal in earth as well as in water. I think there is a real danger that people could assume that locking it in landfill means it will stay put until it finally degrades. ME: It is generally assumed that plastics will decay in just a few hundred years or less, but more scientific research needs to be undertaken on this crucial point. It seems likely that plastic in the ocean will break down relatively quickly into microparticles, to be ingested into the food chain or otherwise to sink into the sediment on the ocean floor.  But plastics buried in earth could prove to be much longer lasting. Archaeological studies show that some modern landfills are so tightly sealed and capped they protect material within from erosive forces, effectively mummifying them. Neither rain nor sunlight nor air can penetrate in, and decomposition processes typically slow down after 20 years or so. In such an artificially sheltered environment, materials like paper and plastic may survive for surprisingly long times. SG: Deep sea environments at low temperature, low light preventing photo degradation and higher pressure are thought to have a preserving effect too. But landfill could be just preserving this waste as well? It’s amazing to think that future archaeological finds could be the everyday items and gadgets we are using today.   ME: Well, a recent excavation of Atari computer game products buried 10m down in a New Mexico landfill in 1983 revealed that the plastic game cartridges with associated packaging and shrink-wrapping showed little sign of decomposition after 30 years in the ground! Those cartridges which were not crushed were still playable, and were sold on eBay for thousands of dollars. SG: Despite its durability, we are using plastic as if it is disposable. Efforts to recycle have been shown to be seriously flawed, with supply chains for waste not all that they seemed.  Earlier this year it was reported that the UK sends out around half of its recycling abroad with insufficient checks on what was actually happening to it. The system was found to be open to fraud, leading to concerns that instead of being recycled the waste was being dumped in landfill, rivers and oceans. Exporters of UK waste were sending out contaminated and worthless mixed waste and fraudulently claiming the recovery notes that they would then sell.  It is clear that recycling alone, in its present form, does not work. We have to either stop using plastics or find alternative routes to dispose of plastic waste in a more sustainable way. Consumers are much more aware of the impact of plastics in the environment and are receptive to change, but there are limited choices. ME: Yes. We need to explore possible alternatives, for example by re-using plastics in large-scale engineering projects such as road and building construction. There are promising projects currently underway looking at the feasibility of using processed plastic waste as a replacement aggregate in concrete manufacture. SG: Using plastic waste to replace raw materials makes a lot of sense. Pyrolysis is a really good way to break down the plastic to produce raw materials.  We could also develop new ways to break it down faster into useful chemical components. One way would be to digest the plastics using enzymes. Fungi and bacteria have been found that have a taste for plastic and can break it down to be able to use the carbon from it. Scientists in Portsmouth tried to reproduce the enzyme that PET-eating bacteria uses, and accidentally produced an even more efficient enzyme.      Reprocessing the plastic like this would help cut the amount of waste building up as pollution. Long-term, even if we stop pouring this waste into our environment, we will still shed millions of fibres and microplastics through washing synthetic clothes and wearing down tyres.  The problem is that conventional plastics are just too cheap. A price increase would level the playing field to make the use of new plastics more expensive compared to recycled plastic. Because the price of plastic should include the true downstream disposal costs. If this was the case, then we could afford better waste sorting and more viable recycling facilities to prevent the exporting of waste. ME: We’re both agreed that the amount of plastic being produced and discarded without being recycled is doing irreparable harm to other creatures and habitats. So much plastic is choking river and ocean environments that many forms of life are threatened. So much plastic is entering geological cycles that it is creating a substantial stratigraphic signal of the Anthropocene in its own right. Even when the plastic itself has decomposed, the forms of some of the plastic objects that now litter the ocean floor may be preserved as trace fossils in sedimentary rock.  We might imagine picking up a stone in tens of millions of years’ time and finding – instead of the shells of former sea creatures – the shapes of cotton buds, coffee spoons, fishing nets, CD cases, water bottles, biros … SG: and game cartridges. If there’s a specific topic or question you’d like experts from different disciplines to discuss, you can:"
"

The Current Wisdom _is a series of monthly articles in which Patrick J. Michaels, director of the Center for the Study of Science, reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press._   




The new paper’s lead author is Jonathan Gregory of the U.K.’s University of Reading, and the other authors are a who’s who of sea level researchers (repeating my professions nauseating belief that putting a large number of authors (most of whom have—at best—just read the manuscript) somehow makes it more persuasive). The paper concludes that the causes of sea level rise, and its temporal variations, across the 20th century were many, and that a link to anthropogenic global climate changes has been weak or absent over this period. Basing future sea level rise projections on a presumed historical relationship between anthropogenic global warming and corresponding sea level rise turns out to be a bad idea.   
  
Here is how Gregory et al., 2012 put it:   




The implication of our closure of the [global mean sea level rise, GMSLR] budget is that a relationship between global climate change and the rate of GMSLR is weak or absent in the past. The lack of a strong relationship is consistent with the evidence from the tide-gauge datasets, whose authors find acceleration of GMSLR during the 20th century to be either insignificant or small. It also calls into question the basis of the semi-empirical methods for projecting GMSLR, which depend on calibrating a relationship between global climate change or radiative forcing and the rate of GMSLR from observational data (Rahmstorf, 2007; Vermeer and Rahmstorf, 2009; Jevrejeva et al., 2010).



And here are the main conclusions, now seriously questioned, from the semi-empiricical citations included in the above quote:   
  
Rahmstorf (2007):   




When applied to future warming scenarios of the Intergovernmental Panel on Climate Change, this relationship results in a projected sea-level rise in 2100 of 0.5 to 1.4 meters above the 1990 level [by 2100].



Vermeer and Rahmstorf (2009):   




For future global temperature scenarios of the Intergovernmental Panel on Climate Change’s Fourth Assessment Report, the relationship projects a sea-level rise ranging from 75 to 190 cm for the period 1990–2100.



Jevrejeva et al. (2010):   




With six IPCC radiative forcing scenarios we estimate sea level rise of 0.6–1.6 m, with confidence limits of 0.59 m and 1.8 m.



Seems like three strikes against projecting those high rates of sea level rise.   
  
For a little reality check, the current rate of rise is somewhere in the range of 1.8 to 3.5 mm/yr (0.07 to 0 .14 in/yr) depending on the time period over which you calculate the trend.   
  
Further, as we have previously written, it doesn’t look as if the recent increased rates of ice loss from Greenland and Antarctica are sustainable—much less going to linearly increase to the end of the century. All of this strongly argues that the 21st century sea level rise is not a problem that we can’t keep up with.   
  
**References:**   
  
Gregory, J., et al., 2012. Twentieth-century global-mean sea-level rise: is the whole greater than the sum of the parts? _Journal of Climate_ , doi:10.1175/JCLI-D-12-00319.1, in press.   
  
Hansen, J.E., 2007. Scientific reticence and sea level rise. _Environmental Research Letters_ , **2,** doi:10.1088/1748-9326/2/2/024002   
  
Jevrejeva, S., et al., 2010. How will sea level respond to 1019 changes in natural and anthropogenic forcings by 2100? _Geophysical Research Letters,_ **37** , L07703, doi:10.1029/2010GL042947.   
  
Rahmstorf, S., 2007. A semi-empirical approach to projecting future sea-level rise. _Science_ , **315** , 368–370, doi:10.1126/science.1135456.   
  
Vermeer, M. and S. Rahmstorf, 2009. Global sea level linked to global temperature. _Proceedings of the National Academy of Sciences,_ 106, 51, 21527–21532, doi:10.1073/pnas.0907765106.


"
"
I just watched a presentation Elsi Sertel from a university in Turkey showing how easy it is to introduce true land cover data into a climate model. Her study area was around the Black Sea near Istanbul, and used LANDSAT imagery along with a pixel by pixel truthing technique to determine the type of land cover, sea, forest, urban, etc and apply it to use in a GCM.
Her premise was that current GCM’s use land surface info that isn’t fully representative, out of date, and in some cases just plain wrong.
Her study showed a technique that allowed for a significant amount of automation to the process, to allow improved and current land surface types to be easily integrated into the grid cells of a GCM. Unfortunately, some GCM gridding schemes are too coarse to handle such data.
From what I’ve seen in this conference so far, and I’ve seen presentations now from Europe, Turkey, China, Australia and the USA, it is becoming more clear that land use is a major driver of climate change, and perhaps dwarfs even GHG effects. That’s just a hunch. One study from Australia showed the effects of removing a woody type bush over a large area over the past century, and the results on rainfall and temperature were profound.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4691079',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Climate breakdown and the global crisis of environmental degradation are increasing violence against women and girls, while gender-based exploitation is in turn hampering our ability to tackle the crises, a major report has concluded. Attempts to repair environmental degradation and adapt to climate breakdown, particularly in poorer countries, are failing, and resources are being wasted because they do not take gender inequality and the effects on women and girls into account.  Campaigners called for governments and institutions to take note, saying that the impacts on women and girls must be at the heart of any viable strategies on the climate and ecology. The International Union for the Conservation of Nature (IUCN) carried out what is understood to be the biggest and most comprehensive study yet of the issue, taking two years and involving more than 1,000 sources of research. “We found gender-based violence to be pervasive, and there is enough clear evidence to suggest that climate change is increasing gender-based violence,” said Cate Owren, a lead author of the report, published on Wednesday. “As environmental degradation and stress on ecosystems increases, that in turn creates scarcity and stress for people, and the evidence shows that, where environmental pressures increase, gender-based violence increases.” Six in 10 respondents to a survey by IUCN, with more than 300 responses from organisations around the world, said they had observed gender-based violence among female environmental rights defenders, environmental migrants and refugees, and in areas where environmental crimes and environmental degradation were taking place. More than 80 case studies clearly showing such links were uncovered as part of the research. Gender-based violence includes domestic violence, sexual assault and rape, forced prostitution, forced marriage and child marriage, as well as other forms of the exploitation of women. The report found human trafficking rises in areas where the natural environment is under stress, and links between gender-based violence and environmental crimes such as wildlife poaching and illegal resource extraction. “Gender-based violence is one of the most pervasive but least talked-about barriers that face us in conservation and climate work,” said Owern. “We need to take the blinders off, and pay this concerted attention.” Owren found abundant examples of the close links between gender-based violence and the exploitation of women and girls, and the competition for resources engendered by the impacts of global heating and our destruction of the natural environment. For instance, sexual abuse was found in the illegal fishing industry in south-east Asia, and in eastern and southern Africa fishermen reportedly refused to sell fish to women if they did not engage in sex. The illegal logging and charcoal trade in the Democratic Republic of Congo is linked to sexual exploitation, and in Colombia and Peru illegal mines are strongly associated with an increase in sex trafficking. There have also been numerous examples of gender-based violence directed against environmental defenders and activists, who try to stop the destruction or degradation of their land, natural resources and communities. Sexual violence is used to suppress them, undermine their status within the community and discourage others from coming forward. Yet few projects that are aimed at conservation and improving the environment, or tackling the climate crisis, display any recognition of these issues, according to the report.  Global heating puts pressure on resources, as extreme weather, including heatwaves, droughts, floods and fiercer storms, grows more frequent and devastating. In most parts of the world, women are already disadvantaged and lack land rights and legal rights, so are vulnerable to exploitation. When the additional stresses caused by the climate crises bite, they are the first to be targeted. For instance, in some communities, young girls are married off as early as possible when the family faces hardship exacerbated by the climate. Globally, about 12 million more young girls are thought to have been married off after increasing natural disasters, and weather related disasters have been shown to increase sexual trafficking by 20-30%. Women and girls are also burdened with tasks such as drawing water and finding firewood, which are becoming more scarce in many areas under the ecological impact of our scramble for resources, and which expose them to further dangers of violence. Grethel Aguilar, acting director-general of the IUCN, said: “Environmental degradation now affects our lives in ways that are becoming impossible to ignore, from food to jobs to security. This study shows that the damage humanity is inflicting on nature is also fuelling violence against women around the world – a link that has so far been largely overlooked.” At the UN climate conference in Madrid last December, governments were criticised by campaigners for ignoring the plight of women and children and the threats they face.  Some governments are moving to put action for women and girls into their climate and development policies, and the UN in Madrid moved to include a gender action plan as part of the climate negotiations. Campaigners and some countries are hoping for even greater focus on the issue at the crunch UN climate talks in November, to be hosted by the UK in Glasgow. The UK’s department for international development said it was already factoring in gender issues in climate change funding, including a large-scale study on violence against women and girls during the humanitarian crisis in South Sudan, where about three quarters of women and girls who had been in a relationship experienced violence at the hands of their partner. A spokesperson said: “Women and girls can be disproportionately affected by climate change. This is why we’re spending UK aid on helping to promote gender equality, as well as leading the fight against climate change.” Bob Ward, policy and communications director at the Grantham Research Institute on climate change and the environment at the London School of Economics, who was not involved in writing the report, said: “This report highlights the complex but clear link between growing climate change impacts and violence against women and girls.  He pointed to the role that female campaigners were playing in bringing the world’s attention to the problems. “When we see the inspiring leadership of female activists like Greta Thunberg, we should recognise that the lives and livelihoods of women and girls around the world are particularly threatened by climate change,” said Ward. “The empowerment of women and girls and their protection from the direct and indirect consequences of climate change must lie at the heart of the just transition to zero-carbon and climate-resilient societies.” The report also provided a timely reminder that “concerted action to tackle inequality can unlock new opportunities for climate action and women’s empowerment”, added Mary Robinson, chair of The Elders. “We need to recognise the unequal effects of the climate crisis on women, but also that women’s participation brings with it creative and sustainable solutions to both the climate emergency and social injustices. Tackling climate change and environmental degradation without the full inclusion of women will not succeed: gender equality is a prerequisite to the collective effort needed to address the climate emergency.”"
"How would you move through a space when you can’t see the obstacles ahead? For example, how would you find your way out of a maze if you were blindfolded? You could either use your other senses, such as touch, to find your way out – or better yet, you could get someone who can see the way out to direct you. But either way you need information. For birds, non-visual information can provide the same helping hand while flying. Even though they can see the world around them, the air is a dynamic, invisible environment – and airflow is much more complex environment to move through than the ground with its static obstacles. Imagine you are hanging from a glider and racing to a finish line. You can see two other paragliders ahead of you, one looks like they are having a smooth ride and moving quickly, the other looks like they are in trouble and finding it difficult to control their glider. You would choose to follow the first one, right? By observing the other pilots around you and responding in accordance to what happens to them, you tap into information that helps you make a good decision and keep up with the race. Similarly, it makes sense that an animal may do the same to move through their environment – observing those around them that have the same objective. Soaring birds not only move through the air, they rely on updrafts, such as thermals (a column of warm rising air), to gain lift rather than flapping their wings. It’s a bit like a big game of invisible snakes and ladders – but the costs of sliding all the way down without finding the next ladder are high. They must reach that next ladder before they hit the ground. Like the gliders, birds that rely on soaring – including vultures – often share the air with other birds. But until now we didn’t know if soaring birds do indeed observe each other to “see” these invisible thermal ladders. For our recently published study, we designed an experiment that would test this idea. We tracked the movements of each bird in a small group of vultures at a bird of prey centre in the mountains of France, and recorded their behaviour. Only by using the latest tagging technology could we investigate this concept. Each bird had a backpack with a GPS logger, a movement logger and camera recording all aspects of their movement. In the movement logger was a range of sensors, sensitive to different movement types – an accelerometer to pick up wing beats, a magnetometer for directional changes, and an airspeed sensor.  There is a well known theory, by aeronautical engineer Paul MacCready, which states that birds and gliders should glide at high airspeed when they have just left a strong thermal and expect to be approaching another strong thermal. But gliding quickly is risky, as the flyer is more likely to hit the ground before reaching the next thermal. So our hypothesis was that these vultures, and other soaring birds, are able to take this risk and glide quickly when they have clues provided by the soaring of others, on the whereabouts of the next thermal.  When we mapped the movements of all the vultures and analysed their gliding airspeeds, we found that – on making a decision to leave one thermal and glide to the next – vultures which had tapped into this extra information by “eavesdropping” on the movements of others (they weren’t flocked together but were watching how the other vultures were acting) chose to take the risk and adopt significantly higher airspeeds than those going it alone.  This finding helps us to understand what is going on in the invisible world above us and just how these birds make decisions to navigate this challenging environment. We are all quite used to seeing birds such as pigeons or starlings flying in flocks, and it may be quite reasonable to assume that by flying together these birds are interacting with each other. But our work reveals for the first time that even when birds are not flying together in a flock, they may observe others to sense the world around them. This suggests, for vultures at least, that it is important that there are other birds in the sky with them, as numbers may be needed to maintain a healthy network of information."
"

The diagram above is central to the paper’s examination of the “spiral” nature of the earth to sun distance relationship, which affects noit only seasons, but longer term climate cycles.
Every once in awhile some thing comes along that really “clicks” with a  lot of people in the science community.
A new paper from New Zealand titled: Linkages between solar activity, climate predictability and water resource development is one of those that has “clicked” with a lot of people recently. It is the first scientific paper I’ve ever seen that pulls all the interdisciplinary fields of solar physics, astronomy, meteorology, hydrology, and climatology together to prove that in fact the sun is the major driver, even with its “small” fluctuations often ignored by climate scientists as being too small to matter.
It does matter, I’ve written about it many times, and this paper really has strong evidence supporting it. This is not just another paper talking about sunspots and the maunder minimum, no this one has some strong empirical evidence that directly links climate changes on earth to a myriad of changes in the sun-earth relationship.
What’s even better, this paper is readable. It’s not written in techno-speak with accents on using words 99% of the general population doesn’t use. It’s refreshing. Read it here (Adobe PDF)
The abstract reads: “This study is based on the numerical analysis of the properties of routinely observed
hydrometeorological data which in South Africa alone is collected at a rate of more than
half a million station days per year, with some records approaching 100 continuous years
in length. The analysis of this data demonstrates an unequivocal synchronous linkage
between these processes in South Africa and elsewhere, and solar activity. This confirms
observations and reports by others in many countries during the past 150 years.
It is also shown with a high degree of assurance that there is a synchronous linkage
between the statistically significant, 21-year periodicity in these processes and the
acceleration and deceleration of the sun as it moves through galactic space. Despite a
diligent search, no evidence could be found of trends in the data that could be attributed
to human activities.”
My hat’s off to these scientists: W J R Alexander, F Bailey, D B Bredenkamp, A van der Merwe and N Willemse


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5c2b952',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Lon Glazner, a fellow blogger and local electronics engineer made some comments about my post on the NASA/CSU study on California temperatures. Well that got me started…so below are Lon’s comments and my reply along with a fun technical challenge. For those of you that read this blog, but disagree with my views, I invite you to read this carefully.
Anthony,
You make a number of good points.  Particularly in the fact that the writers may have applied changes in urban temperature measurements over large regions for graphical impact.
As someone who has designed and built electronic temperature sensors I have certain concerns about the data itself.
Unless temperature sensors are regularly calibrated I think it is unreasonable to expect accuracy of greater than a couple of degrees.
Even some that are calibrated may not have good accuracy.  The LM34 which is a commonly used semiconductor for measuring temperature is +/-2 degrees F.  This is pretty typical of analog or digital semconductor sensors.  The temperature error for this part is also non-linear, and so it’s not a simple offset that you have to account for during data collection.  Furthermore, there are lots of additional errors that can creep into a temperature measuring device beyond the sensor itself.
http://www.national.com/pf/LM/LM34.html
One could argue that numerical analysis done on data points would tease out errors.  But if a scientist doesn’t know the exact accuracy of a temperature sensor then they couldn’t account for errors in their system.
Some of the temperature sensing stations may be  very accurate and regularly calibrated.  But maybe they’re not?
I have a hard time trusting that the data is accurate to the level of identifying 1 or 2 degree changes over decades.  This is especially true since the techniques of making these measurements have changes over that time frame.
Lon

Lon, thank you for the comments. FINALLY somebody who understands the kind of biases that creep into temperature measurements!
I’m innately familiar with National Semi’s LM34 and it’s accuracy problems. One of my early jobs at my university as a research assistant was to create remote electronic weather stations. I soon learned how inaccurate many electronic devices can be in temperature measurement.
The problem with the National Weather Service temperature data sets (and world data sets too) is that they are full of biases and errors that I’m not sure have been accurately accounted for. People such as Jim Price, from CSUC who is on the IPCC say they have been, yet nobody has shown me any hard evidence of such. I’d be a lot less skeptical if I could see how the IPCC accounted for temperature measurement biases. But they won’t share.

Some people that I try to explain this to accuse me of splitting hairs. But these bias problems in temperature measurement are quite real.
What works against my arguments about the difficulty in getting accurate temperature records is the everyday simplicity of temperature and its common measurement. We live by temperature, we have it reported constantly, we all have thermometers at home, we measure our childrens fevers with thermometers, we barbeque with thermometers.
Measuring temperature is easy right? You just stick the thermometer in whatever gas, liquid, or solid you want to measure the temperature of and voila’  there it is. People tend to think of thermometers as perfect devices. Some very expensive calibrated thermometers, are close to perfect, especially when taking measurements in a closed system, like a fermenatation vat at Sierra Nevada.
But in an open system in our atmosphere, there are many many more biases that can affect the measurement within a few inches or feet of the thermometer. Here’s just a few:
– Reflected sunlight from nearby building or objects
– Re-radiated infrared from nearby cement or asphalt surfaces or the ground itself (which is why airports make terrible places for temperature measurement)
– The structure that the thermometer is mounted to, can conduct heat to the thermometer
Now add to that:
– Accuracy of the thermometer itself
– Linearity of the thermometer over its measurement range
– Long term repeatability of the thermometer’s accuracy
– Long term repeatability of the thermometer’s linearity
And then we have urban effects such as:
– Localized vegetatation removal or addition over time
– Localized building changes over time
– Localized asphalt or concrete surfaces addition or removal
And finally within the global temperature records data set we find instances of:
– Changing the location of the weather station and/or its thermometer
– Changing the thermometer itself at some point – i.e. repair/replace
– Changing the thermometer type, from mercury, to electronic (thats been done at thousands of weather stations worldwide)
– Variations in temperature measurement devices from country to country, even though the World Meteorological Organization has specifications, they are not always followed.
– Changes in thermometer shelter, different types of paint over time, all which have different absorptive and reflective properties.
– Changes in the observer recording the temperature, some may round up, others round down numbers. BTW for about 75 years, all temperature records were manually recorded.
Ok with all these biases and possible errors that you have to account for to make long term temperature measurement reflect the true temperature of the location, can you be absolutely sure of the data integrity? Especially when you are looking for trends that may be 1 degree or less over 50-100 years? I can tell you that I’ve looked at these climatological data sets, and NONE of them come with a calibration record for the thermometer, or even a description of the make/model used at that location. There are notations in the records that say things like “station relocated to accomodate construction” or “thermometer replaced” which can give clues to the data integrity possibly changing but the climate researcher is left to make a judgement call on the viability of the data without anything to gauge the sensor or its local environment.
Or lets try a thought experiment Lon, you’ve been commissioned by the IPCC to make a new thermometer for use around the world at climate measurement stations. As an electrical engineer, could you design an air temperature thermometer that is:
– Linear to within 0.1% over a temperature range of -20F to 120F
– Accurate to within 0.1 degree F over that same range
– Repeatable in linearity and accuracy defined above for a period of 20 years. Or even 10 years.
– Identical withing the specs above, so that if one fails, it can be immediately swapped with another one from parts stock with no worry about introducing bias
Ok there’s your challenge. Could you do it?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7248471',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Our comment primarily concerns the Department of Energy’s (DOE) use of the social cost of carbon (SCC) in the cost/​benefit analysis of the Energy Conservation Program: Energy Conservation Standards for Walk‐​In Cooler and Freezer Refrigeration Systems proposed rulemaking. The DOE’s determination of the SCC is discordant with the best scientific literature on the equilibrium climate sensitivity and the fertilization effect of carbon dioxide—two critically important parameters for establishing the net externality of carbon dioxide emissions. It is also at odds with existing Office of Management and Budget (OMB) guidelines for preparing regulatory analyses. It is based upon the output of Integrated Assessment Models (IAMs) which have little utility because of their great uncertainties, including uncertainties within the critical physical parameters upon which their simplified climate model are built. They provide no reliable guidance as to the sign, much less the magnitude of the social cost of carbon. Additionally, as run by the Interagency Working Group (IWG) (whose results were incorporated by the DOE in this action), the IAMs produce illogical results that indicate a misleading disconnection between climate changes and the SCC value. Further, we show that the sea level rise projections (and thus SCC) of at least one of the IAMs (DICE 2010) is not supported by the mainstream climate science.



Until this entire situation can be properly rectified, the SCC should be barred from use in this and all other federal rulemaking. It is better not to include any value for the SCC in cost/​benefit analyses such as these, than to include a value which is knowingly improper, inaccurate and misleading.
"
nan
"
This afternoon there will be several presentations that embrace the measurement systems used for the near surface temperature and precipitation records.
Of great interest to me is a presentation outlining the new US CRN (Climate Reference Network) by Bruce Baker of NCDC. Another is by Glenn Conner, former Kentucky State Climatologist whose talk will be about the role of station histories in identifying biases in climate records.
My presentation follows those two – it should be a lively afternoon.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea450bc7f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterUnusual cold and snow are expected to sweep across North America and Europe over the coming days, thus threatening crops. 
Not that mid May is approaching, global warming alarmists tell us we should already be expecting heat waves. But right now the opposite is in the forecast: snow and extreme cold! Who would have thought?
Snow for Boston and new York?
Meteorologist Dr. Ryan Maue tweeted that both New York City and Boston might see snow on Saturday as a “rare & powerful May ‘bomb cyclone’ Nor’easter” is projected to develop off the east coast.
Over a foot of snow might fall in Maine, Maue asserts:

While snow won't stick on the ground for long, if at all, parts of Maine might see more significant accumulations over a foot. ❄️
Flurries for NYC and Boston perhaps on Saturday as rare & powerful May ""bomb cyclone"" Nor'easter develops off New England coast. pic.twitter.com/aOHT4APBfp
— Ryan Maue (@RyanMaue) May 7, 2020



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Maue earlier had tweeted that the snow and cold could act as a “triple whammy” because “hard freezes limit agriculture, forces people to remain indoors w/central heating & provides outdoor environment favorable for coronavirus.”
Snow to blanket parts of northern/central Germany 
Not only the Northeast has to worry about winter striking so late in the season, but also an intense cold front will be sweeping across a vast swath of northern Europe, reports German weather site daswetter.com here.
One can also call it an unusual cold snap for almost mid-May. The air in the north will warm up to only 7 to 12 degrees. […] With the polar air, the temperatures will plunge, and so will the snowfall line. When the cold air reaches the south in the night from Monday, it will snow slowly until the middle of the day. Around 400 to 600 m, up to 10 cm of fresh snow is possible in the middle of May. Even up to 300 m wet snow can fall.”
“Five to 6 nights of frost warnings” in UK
At Twitter David Birch tweeted a GIF animation showing the projected movement of the cold front, writing that Britain might see 5 or 6 nights of frost warnings next week:

Likely the UK could see 5/6 consecutive frost warnings throughout next week. pic.twitter.com/paLeD4vvOK
— DavidIBirch 🇬🇧 (@dbirch214) May 7, 2020



		jQuery(document).ready(function(){
			jQuery('#dd_91b09e226f3683c039c0d47c6040e31f').on('change', function() {
			  jQuery('#amount_91b09e226f3683c039c0d47c6040e31f').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

Another parking lot being measured for climate change: Newport, TN
My surfacestations.org project has reached an important milestone.
With the submission of #222, Lexington, VA, submitted by John Goetz, we are now below the 1000 mark (out of 1221) stations left to survey. It was a 3 -way race to #222 between power surveyors John Goetz, Kristen Byrnes, and Don Kostuch.
Thanks to ALL of the wonderful volunteers for helping to reach this important benchmark! We currently stand at 231 surveyed stations and 990 left to go.
I still need help in the midwest and the south, particularly Kansas, Nebraska, Montana, the Dakotas, Oklahoma, Mississippi, and Alabama. If you live in the areas want to make a lasting contribution to science, please visit www.surfacestations.org and sign up as volunteer. Its easy to do, and it makes for a fun science learning experience.
To see more weather stations like this one, see my “How not to Measure Temperature” series on this blog. This is only a small sample of the 231 surveyed to date, but it will give you an idea of the problems that have been seen so far.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4d334ac',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"
Share this...FacebookTwitterCorona shows the stark attitude differences between the sciences of climate and virology. While one arrogantly claims to monopolize the truth, the other acknowledges the great uncertainties.

Image: CDC
By Dr. Sebastian Lüning, Die kalte Sonne
(Text translated, edited by P Gosselin)
The corona virus with all the effects is currently pushing all other issues completely to the sidelines. This also includes the climate topic.
Climate activists like Professor Volker Quaschning or Professor Stefan Rahmstorf know it. Currently they fear for their livelihoods as they try to resist this with all their might, sometimes with absurd tweets or, as in the case of Professor Stefan Rahmstorf, with an article in Spektrum der Wissenschaft: Denial of science in times of corona“.
In his article, Professor Rahmstorf (Oceanography and Paleoclimatology!) also fancies himself a corona expert, and he puts those who do not agree with him in climate research in the same bag with the scientists who are critical of the corona crisis and approach. This is done completely without any basis because these are two completely different issues.
Climate activists seeking attention
It is a contemptible attempt to desperately link corona and climate, no matter what. This is being done simply because corona is the top issue right now. And before his own issue gets completely washed away, Rahmstorf is stooping to such means to try to get some attention. This blog here admits that it lacks the expertise to properly judge the corona debate.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Beholders of the truth?
But something completely different is crucial here. Anyone who looks at or listens to the regular statements or podcasts of experts such as Berlin virologist Professor Christian Drosten will see the pleasant difference to climate alarmists like Professors Quaschning or Rahmstorf. Unlike them, Professor Drosten does not consider himself to be the sole beholder of the truth. On March 20, 2020, he explained in his podcast:
“There is no research data on long-range curfews. Caution is also called for when dealing with numbers.” …”Summer can have at least a small effect on the virus.”
Uncertainty is acknowledged
This has been the case since the beginning of his podcast series. Professor Drosten has stated more than once that science does not yet know certain things or that certain findings have since become obsolete. Imagine if he applied the popular killer argument “the science is settled” to corona in the same way as Professors Rahmstorf or Quaschning do with climate.
Models have failed
Yet, climate models that fail to reflect reality still remain the basis for future scenarios. They are taken at face value, even though their calibration fail when past data are applied. And even worse, people are being told that there is a control over the climate that is now being lost.
Is this a lack of knowledge, a lack of respect for nature, or just their own blunt agenda?
Alarmist scientists don’t contribute to healthy science
The climate debate will also fail because of the fact that the aforementioned people are sitting in ideological trenches and hurling grenades in the direction of the other side. This is something you can do in a war, but unfortunately it does not lend to a social discourse.


		jQuery(document).ready(function(){
			jQuery('#dd_99d26721b518a0ab9826796b8202153e').on('change', function() {
			  jQuery('#amount_99d26721b518a0ab9826796b8202153e').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterSo far no signs of another super hot-dry summer for Europe, which media have been alarming about. 
Veteran Swiss meteorologist Jörg Kachelmann tweeted the 45-day projections for Europe.
In terms of precipitation, Europe saw drought conditions over the past two summers (2018 and 2019) and climate alarmists claimed this would be the new normal. And recently the European public got bombarded by media reports stemming from the World Meteorological Organization (WMO) of a blistering super hot and dry summer this year.
But look what the ECMWF now projects for the next 45 days (upper chart). Kachelmann comments: “Even according to the latest 46-day trend of the ECMWF, which runs to mid-July, the drought summer seems to be completely called off for the time being.”

Ein ""Hitzesommer"" war noch nie in den Vorhersagen drin und das scheint zumindest bis Mitte Juli auch so zu bleiben, auch wenn es später im Juni wieder sommerlicher wird. pic.twitter.com/6G97p1NJmY
— Jörg | kachelmannwetter.com🇨🇭 (@Kachelmann) June 5, 2020

And in terms of temperature, see the lower chart, nothing unusual is projected to happen over the next 45 days. The Swiss meteorologist notes: “A ‘hot summer’ was never in the forecasts, and it seems it’ll stay that way at least until mid-July, even if it gets more summery again later in June.”
Keep in mind these long range forecasts come with much uncertainty, and change with every run. But right now it looks like all the recent doomsday projections of a scorched euro-summer were overblown.


		jQuery(document).ready(function(){
			jQuery('#dd_ffb24e69e1916c9a909c9e37aa3ef08a').on('change', function() {
			  jQuery('#amount_ffb24e69e1916c9a909c9e37aa3ef08a').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

That’s the topic of my _Washington Examiner_ column this week. In it, I discuss last week’s budget battle and the failure of “policy riders” designed to rein in the Obama EPA’s attempts to regulate greenhouse gases without a congressional vote specifically authorizing it. The Obama team believes it has the authority to implement comprehensive climate change regulation, Congress be damned. Worse still, under current constitutional law–which has little to do with the actual Constitution–they’re probably right. Thanks to overbroad congressional delegation, “the Imperial Presidency Comes in Green, Too.” At home and abroad, the legislative branch sits on the sidelines as the executive state makes the law and wages war, despite the fact that “all legislative powers” the Constitution grants are vested in Congress, among them the power “to declare War.”   
  
  
Yet, as I point out in the column, Congress retains every power the Constitution gave it–powers broad enough that talk of “co‐​equal branches” is a misnomer. Excerpt: 



The constitutional scholar Charles Black once commented, “My classes think I am trying to be funny when I say that, by simple majorities,” Congress could shrink the White House staff to one secretary, and that, with a two‐​thirds vote, “Congress could put the White House up at auction.” (I sometimes find myself wishing they would.)   
  
  
But Professor Black wasn’t trying to be funny: it’s in Congress’s power to do that. And if Congress can sell the White House, surely it can defund an illegal war and rein in a runaway bureaucracy.   
  
  
If they don’t, it’s because they like the current system. And why wouldn’t they? It lets them take credit for passing high‐​minded, vaguely worded statutes, and take it again by railing against the bureaucracy when it imposes costs in the course of deciding what those statutes mean.



Last year, in the journal _White House Studies_ [.pdf], I explored some of the reasons we’ve drifted so far from the original design: 



_Federalist_ 51 envisions a constitutional balance of power reinforced by the connection   
between “the interests of the man and the constitutional rights of the place.” Yet, as NYU‘s Daryl Levinson notes, ―beyond the vague suggestion of a psychological identification between official and institution, Madison failed to offer any mechanism by which this connection would take hold.… for most members, the psychological identification with party appears greatly to outweigh loyalty to the institution. Levinson notes that when one party holds both branches, presidential vetoes greatly decrease, and delegation skyrockets. Under unified government, “the shared policy goals of, or common sources of political reward for, officials in the legislative and executive branches create cross‐​cutting, cooperative political dynamics rather than conflictual ones.”



Individual presidents have every reason to protect and expand their power; but individual senators and representatives lack similar incentive to defend Congress’s constitutional prerogatives. “Congress” is an abstraction. Congressmen are not, and their most basic interest is getting reelected. Ceding power can be a means toward that end: it allows members to have their cake and eat it too. They can let the president launch a war, reserving the right to criticize him if things go badly. And they can take credit for passing high‐​minded, vaguely worded statutes, and take it again by railing against the executive‐​branch bureaucracy when it imposes costs in the course of deciding what those statutes mean.   
  
  
In David Schoenbrod’s metaphor, modern American governance is a “shell game,” with We the People as the rubes. That game will go on unless and until the voters start holding Congress accountable for dodging responsibility.
"
"

As many readers know, the www.surfacestations.org effort has been gaining a lot of attention, and also volunteers. I’m now at over 130 volunteers nationwide.
The results of the effort attracted national attention. I never went seeking it, but when Bill Stiegerwald of the Pittsburgh Tribune stumbled across it, he wrote a column about it. Little did I know his column was nationally syndicated. Last week I found myself being asked to give radio interviews. One interview, at KIRO in Seattle surprised me when I found myself being co-interviewed with Dr. Thomas Peterson of the National Climatic Data Center (NCDC) the keeper of weather records, including weather station records. The exchange was congenial and stuck to science. That was Thursday June 21st. I am certain NCDC is aware of the effort that is going on to document the stations. Part of the reason the effort exists is that NCDC has been pressed to do this by scientists that want to do exactly what I’m doing, studying the measurement environment, and NCDC has failed to do it. We’ll come back to that.
Part of the method I and volunteers are using to do this project relies on a database of weather station information provided by NCDC. In some cases stations are at airports, fire stations, sewage treatment plants, and ranger stations. In other few cases, they are at the residences of observers that have volunteered to record weather data and submit it to NCDC. Since the latitude and longitude provided in the database is fairly coarse, volunteers have to rely on a database entry called “Managing Parties” to find the name of the location, be it a fire station of the name of the volunteer observer.
You can access the database yourself, its a public record: http://mi3.ncdc.noaa.gov/mi3qry/login.cfm
Use the “Guest Login” button
I last used the NCDC database system this way to locate stations on Sunday evening, June 24th it went down Monday Morning June 25th and displayed a message:
“You are not authorized to view this information. Your IP address has been logged”
When it came back up Monday afternoon, the “managing parties” field identifying the location of the weather station was gone. I would note that I shared a radio interview with Dr. Thomas Peterson of NCDC last week, so I am certain NCDC is aware of the effort.
No notification was given, nor even a professional courtesy to advise of the change, nor any notice on the website. The records were simply removed from public view where they existed before. Given the timing, and because the this same data had been visible on the same system for years It seemed this was a response to the efforts to photograph and document the USHCN network.
Without this information, its is very difficult to locate the stations, and in some cases where the official climate station is in some one’s backyard, completely impossible. For example, fellow blogger and surfacestations.org contributor Russ Steele had a very difficult time locating the official station for Ft. Bragg, CA. The observer did consent to having photos posted by the way. Had Russ not been able to contact the observer, the station would likely never have been found as it’s surrounded by trees and garden.
One of my volunteers wrote a query to NCDC and got this back:
Your inquiry was forwarded to me by our webmaster. I’m glad you’ve found
MMS to be a useful tool in your research.
MMS is our primary source of station metadata for National Weather Service
Cooperative Observer and several other networks, and we are
actively working to provide increased detail for a larger number of stations.
It sounds as though you’ve used the system enough that once you’ve located
a station using the search, you’re clicking on the station name hyperlink
and opening a separate station details window. The managing party for a
station has always been visible by clicking on the “Other Parties” tab. In
the case of NWS Coop stations (the USHCN research network relies upon a
subset of stations in the NWS Coop program), this is usually the NWS office
that administers the site. This information was previously included at the
bottom of the Identity tab’s “form view,” but was removed from that view
early this week because in some cases it also revealed the name of the
Cooperative observer.
Cooperative observers are volunteers who donate their time in the interests
of the public good with a reasonable expectation that their personal
information will remain private. It is the NCDC’s policy to protect
observer details, based upon Freedom of Information Act (FOIA) Update, Vol.
X, No. 2, 1989, which exempts the application of FOIA in certain cases and
establishes privacy protection decisions in accordance with the Privacy Act
of 1974 (2004 edition). This exemption applies when the personal privacy
interest is greater than any qualifying public interest for disclosure.
If you have other questions regarding MMS, please feel free to contact me.
I am often away from my desk, so my response may not be immediate.
I was shocked to say the least. So were others in the scientific community.
Data which was once public for years, has now been removed, and the timing is very suspect.
The claim that it was done to protect the privacy of observers doesn’t stand up to certain tests:
1) COOP weather observers are gathering climate data which is published and publicly available. The program is publicly funded. Data and methods from a publicly funded program that is not classified for national security reasons should be available for public inspection. Clearly results from surefacestations.org so far show some problems with the climate measuring network.
2) That published data is used in a multitude of publicly funded research. Some of that research guides policy decisions. The effects of a public policy decision based on data gathered by a volunteer individuals can affect millions of people. The right of the individual to FOI privacy is trumped by the greater need of the general public’s right to know if the data produced by that observer is accurate.
3) The data has been publicly available for years, removing it now is clearly in response to the effort to examine a public program given the timing of it having been removed four days after an NCDC official became aware of my efforts.
4) The data that has been removed also includes locations of public entities such as fire stations, police stations, sewage treatment plants, park headquarters, state run agricultural experiment farms, and many more. These locations are public entities and have no expectation of privacy whatsoever.
I can understand wanting an individual volunteer’s privacy protected. But the method used so far has been to contact the observer ahead of time, tell them what the project is about, and ask for consent. If consent has not been given, no visit is made, and no photographs are taken. See the rules that each volunteer to surfacestations.org must follow
So you have to wonder this: Is NCDC asserting that the privacy interests of police and fire stations, park headquarters, waste water treatment plants, and a handful of individuals, outweighs the public interest in examining quality of data produced in NCDC records and subsequent NOAA reports and publicly funded research? 

Does this waste water treatment plant measureing temperatures for the climate record really need privacy protection?
I said earlier we’d get back to something.
Dr. Roger Pielke, a senior climate researcher, of the University of Colorado, posted on his blog, his outrage at this action, calling it a “cover up”. Those are strong words coming from a congenial scientist. He also posted something even more shocking:
Pictures of these weather stations already exist, but they are being held from public view. Apparently some time ago weather service offices were issued digital cameras and told to do this work. The pictures were submitted to NCDC, and an archiving process begun, then stopped again for “privacy concerns”.
This is my position:
Given what has been seen so far at weather stations that have been inspected by myself and volunteers, it is clear that parts of the USHCN climate monitoring network are out of compliance with published siting standards and in disrepair. Given that the output of this network drives in part NOAA’s climate assessment, the public should demand a full and open accounting of the condition and data accuracy. If volunteer observers using NOAA equipment at private residences do not wish to have their location and the data it produces scrutinized by quality control methods, they have that right. But the data [produced by these stations should be removed from the climatic dataset because it will be unverifiable.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea58423f3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Could it be the _Washington Post_? Bannered across the top of the _Post_ ’s op‐​ed page today is a piece titled “Copenhagen’s political science,” titularly authored by Sarah Palin. I’m delighted to see the _Post_ publishing an op‐​ed critical of the questionable science behind the Copenhagen conference and the demands for massive regulations to deal with “climate change.”   
  
  
But Sarah Palin? Of all the experts and political leaders a great newspaper might call on for a critical look at the science behind global warming, Sarah Palin?   
  
  
What’s even more interesting is that the _Post_ also ran an op‐​ed by Palin in July. But during this entire year, the _Post_ has not run any op‐​eds by such credible and accomplished Republicans as Gov. Mitch Daniels; former governors Mitt Romney or Gary Johnson; Sen. John Thune; or indeed former governor Mike Huckabee, who might be Palin’s chief rival for the social‐​conservative vote. You might almost think the _Post_ wanted Palin to be seen as a leader of Republicans.   
  
  
I should note that during the past year the _Post_ has run one op‐​ed each from John McCain, Bobby Jindal, Newt Gingrich, and Tim Pawlenty. (And for people who don’t read well, I should note that when I call the people above “credible and accomplished,” that’s not an endorsement for any political office.) Still, it’s the rare political leader who gets two Post op‐​eds in six months, and rarer still the _Post_ op‐​eds by ex‐​governors who can’t name a newspaper that they read.
"
"

As the air’s CO2 concentration rises in the years and decades to come, the negative impacts of drought on wheat biomass and grain yield should diminish, a conclusion that can be derived from the recent work of Dias de Oliveira _et al_. (2015).   
  
The five-member Australian research team noted that “elevated CO2 and high temperature are climate change drivers that, when combined, are likely to have an interactive effect on biomass and grain yield,” leading to three possible outcomes: (1) a “reduced positive effect of elevated CO2,” (2) an “amelioration of the effect of high temperature, or (3) a “synergistic effect where high temperature increases the positive effect of elevated CO2.” They also note that the resultant response “may be influenced by [plant] genotypic differences.” In an effort to study these interactions and possibilities, Dias de Oliveira _et al_. designed a field experiment to determine the interactive effects of CO2 and temperature, as well as those of a third variable—drought—on two pairs of sister lines of wheat ( _Triticum aestivum_ L.) over the course of a growing season, where one of the contrasting pairs of wheat sister lines differed in tillering, or branching (free vs. reduced), while the other differed in early vigor (high vs. low). The experiment was conducted out-of-doors in Western Australia in poly-tunnels under all possible combinations of CO2 concentration (400 or 700 ppm), temperature (ambient or + 3°C above ambient daytime temperature), and water status (well-watered or terminal drought post anthesis). So what did it reveal?   




After presenting a very long list of findings, Dias de Oliveira _et al_. summarized their results as follows: (1) elevated CO2 “increased grain yield and aboveground biomass,” (2) terminal drought “reduced grain yield and aboveground biomass,” but elevated CO2 “was the key driver in the amelioration of [its negative] effects,” (3) “temperature did not have a major effect on ameliorating the effects of terminal drought,” and (4) although “the mechanisms by which [the CO2-induced] enhancements were brought about differed in each pair of sister lines,” there was “no difference in aboveground biomass or grain yield within each pair.” Thus, it would appear that the overall outcome the researchers observed in this study was one in which elevated CO2, acted alone, overpowered the negative effects of a debilitating environmental stress (drought). Consequently, as the air’s CO2 concentration rises in the years and decades to come, the negative impacts of drought on wheat biomass and grain yield should diminish. And that is good news worth celebrating!   
  
**Reference**   
  
Dias de Oliveira, E.A., Siddique, K.H.M., Bramley, H., Stefanova, K. and Palta, J.A. 2015. Response of wheat restricted-tillering and vigorous growth traits to variables of climate change. _Global Change Biology_ **21** : 857-873.


"
"
The recent photo submissions at surfacestations.org have demonstrated that many NOAA/NWS climate monitoring stations feature convenient close-by vehicle parking.
Not to be outdone, the Paso Robles USHCN Climate Station of Record features freeway on-ramp access to California’s Highway 101. The weather station is just feet from the street, with the temperature sensor placed just high enough to catch full view of vehicles over the fence.
 My thanks to surfacestations,org volunteer Ed Hahn for this photo. His complete photo essay is available here
Here is the NASA GISS plot for Paso Robles:

Curiously the GISS database still classifies this station as a “rural area”.
I find it interesting that the temperature was trending down in the 70’s then a huge offset occurred just about 1980. I wonder if that was when the freeway access was added? Nothing in the MMS records seem to indicate a station move or other change at that time. Or maybe that’s when somebody got the bright idea to pour a concrete slab under the the station?
From NOAA’s own siting specs: “The sensor should be at least 100 feet from any paved or concrete surface.”
Close enough for government work…


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea547028d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"According to the UN’s Intergovernmental Panel on Climate Change, urgent and unprecedented changes are needed to avoid a climate change catastrophe. Although efforts are already being made to reduce the production of greenhouse gasses, they are by most estimations not enough.  It is therefore critical that we find ways to drastically reduce the amount of pollutants in the atmosphere. Ecosystems capable of absorbing and storing large amounts of carbon dioxide know as “carbon sinks” are ideal for this.  In principle, all living organisms – all animals, plants, algae and bacteria – consist of carbon and so function as a carbon sink. For example, as long as a tree lives it will absorb and store carbon. Given the sheer volume of all the trees contained in tropical forests, it’s no wonder most people imagine such forests when they think of a carbon sink. However, once chopped down and turned into firewood, the carbon in those trees will be released and emitted back into the atmosphere as CO₂. So while a forest is a moderately efficient carbon sink, its capacity to retain carbon in the forest floor is limited.  In fact, new research by colleagues and I has found that such forests are actually only the fifth most efficient ecosystem in the carbon storage cycle behind salt marshes, mangrove forests, seagrass meadows and, best of all, tundra. Tundra is found in polar or mountainous regions where temperatures are too low for trees to grow, and the landscape is dominated by grasses or moss. As a large part of the carbon is stored in the frozen soil and so is harder to disturb, it makes a very efficient sink. However, rising temperatures are melting the tundra in many parts of the world, releasing stored carbon back into the atmosphere, and as a consequence its capacity to store carbon is decreasing.  While forests and tundras are losing capacity for carbon storage, another often forgotten ecosystem may hold the answer: seagrass.  Seagrass plants have an excellent capacity for taking up and storing carbon in the oxygen-depleted seabed, where it decomposes much slower than on land. This oxygen-free sediment traps the carbon in the dead plant material which may then remain buried for hundreds of years.   Seagrass meadows are, for the most part, in recession across the globe due to human activity. As a result the re-establishment of these meadows will make it possible to greatly increase the carbon storage potential of our oceans. Many factors influence the exact amount of carbon that can be taken up by a seagrass meadow, but rough calculations show that if we restore one hectare of seagrass, it would correspond to at least ten hectares of dry-land forest and even as much as 40. Planting vast areas of seagrass meadow is also an eminently doable task as these plants are not seaweeds, but plants with flowers, leaves and roots just like plants on land. This means they produce seeds that can be sown in the seabed or small shoots that can be planted by divers. To develop new techniques for actually planting all this seagrass on a massive scale, colleagues and I have been involved in the Novagrass project, which trialled seagrass planting in the coastal zone around Denmark. We tested various techniques, involving both seeds and seedlings, and had the most success when planting seedlings in chequerboard patterns on the seabed. The lessons from this project are now being applied in a larger scale trial, where muddy seabed is topped up with a layer of sand before seedlings are planted. We are waiting on the results, but so far this technique appears to be a promising way to re-establish eelgrass in coastal areas. There are about 60 seagrass species in the world to choose from, but we focused on common eelgrass (Zostera marina). It cannot tolerate warm seas but it’s the most common species in temperate areas and grows well around coasts in the northern hemisphere. Seagrasses thrive in coastal zones, they have the potential to grow all over the world (except Antarctica) and are even expanding into the Arctic as the ice recedes.  There is some evidence of natural recovery after excessive nutrients from fertilisers and other human pressures have been relieved. But much more action is needed to avoid further loss – and indeed new growth – of these valuable ecosystems."
"

 ** _Editor’s note_** _:_ _In 2014, Cato released_A Dangerous World? Threat Perception and U.S. National Security _an edited volume of papers originally presented ata Cato conference the previous year. In each chapter, experts on international security assessed, and put in context, the supposed dangers to American security, from nuclear proliferation and a rising China, to terrorism and climate change. _



_As part of ourProject on Threat Inflation, Cato will be republishing each chapter in an easily readable online format. Even six years after its publication, much of the book remains relevant. Policymakers and influencers continue to tout a dizzying range of threats, and Americans are still afraid. We invited each author to revisit their arguments and offer a few new observations in light of recent events. _



_The first response comes from Brendan Rittenhouse Green, an assistant professor at the University of Cincinnati, and a recently namedCato adjunct scholar. _



——-



Many world leaders today could tell you, earnestly and genuinely, that their country faces major security threats. Historically, such threats have been endemic to the international system, and they have tended to consume most of the time, attention, and social resources of national policymakers. Moreover, statesmen from the past and present alike could probably adopt a common definition of what a “security threat” is: the possibility of outside actors using large scale violence to menace a state’s sovereignty, territorial integrity, or the physical safety of a substantial portion of its populace; or the emergence of a state that could obtain enough material power to do these things.



But the modern United States does not have this kind of problem. To be sure, its foreign policy discourse has been suffused with the language of security threats for a hundred years. The regnant American grand strategy, which I term primacy, is justified largely—though not exclusively—on security grounds. Yet no state with enough military power to reach inside the Western Hemisphere is likely to emerge any time soon. In short, there is a major disjunction between the language sometimes used to explain and justify American foreign policy commitments and the actual purpose of its strategy.



This, at any rate, was the premise of my essay “Security Threats in Contemporary World Politics.” In it, I made three basic arguments. First, I tried to show that America’s most powerful rival, China, looks nothing like the most plausible past security threats faced by the United States—the Nazi and Soviet empires. Indeed, China would have to jump over a series enormous hurdles before it even came within shouting distance of such dangerous states. Second, I claimed that the political commitments entailed by primacy had only a small prospect of reducing competition in China’s backyard below what it otherwise might be. That is, primacy has a “goldilocks problem”: the highly revisionist states that would propel any East Asian competition are likely to be either absent, or too highly motivated for American power to discourage them from risky behavior. Third, I argued that American political commitments were themselves the most plausible sources of threats to national security. Though unlikely to successfully depress regional competition, primacy’s political connections provide several mechanisms by which America could become involved in a major war.



Looking back on this essay from nearly a decade’s distance, I continue to endorse its major claims. Though I might make a few marginal changes here and there, my views are still roughly the same. But national security discourse, recent history, and my own intellectual temperament have all been altered in important ways. These changes would make for a very different essay, were it written today.



For one thing, the essay’s overwhelming focus on security issues seems less necessary today. Over the past decade, national security discourse has increasingly centered on the defense of the “liberal (or rules‐​based) international order” as the key object of American foreign policy. I think the idea of the “order” borders on conceptually incoherent. But it does have a key virtue: it has enabled more and more analysts to admit that American grand strategy is concerned with something other than traditional security problems. It has therefore made the trade‐​off at the heart of American grand strategy more obvious: American leaders are risking major war, and thereby making the American people less secure, for the purpose of shaping the international environment in ways they consider favorable. Was I re‐​writing this essay today, I would devote more attention to examining the supposed benefits of the international order. Essays by Daniel Drezner and Eugene Gholz from _A Dangerous World?_ provide excellent examples of this kind of analysis.



Another idea I would emphasize more is the idea of “tail risk.” The world today is living through a global pandemic, which will probably kill hundreds of thousands of people and induce the worst economic crisis since the Great Depression. This turn of events was unexpected, even though the potential for a devastating global pandemic has been well‐​known for decades. Nevertheless, most countries were underprepared.



Many rare phenomena pose a similar problem. Society lacks the data that would justify the assumption that certain kinds of apparently rare events are in fact extreme outliers on a bell‐​shaped curve of event frequencies, rather than merely uncommon results of some other kind of frequency distribution.. In fact, it turns out that many rare events—for example, earthquakes, rogue waves, and importantly, war—do not follow a normal distribution. In many cases, the statistical likelihood of such events is far greater than the traditional bell‐​shaped curve would imply—the tail ends of the actual distribution of events are “fat.”



Societies are therefore likely to underestimate the risk associated with rare events. I suspect that the probability that America’s primacy strategy will produce a major war is similarly underestimated. The probability may be relatively low, but the scale of disaster would be very large. Over the long‐​term I worry that the chances of such a war would exceed the tolerance threshold of even the most aggressive strategist. Considering and analyzing this possibility seems like an especially salient task in light of recent events.



Finally, if I wrote the essay today, I would focus more attention on the idea of “second best” strategies. Early in the last decade, I still had something of the zeal of youth about me. I retained hopes that normal politics might produce non‐​trivial change in American grand strategy. After all, the country had been somewhat chastened by its exhausting wars in Southwest Asia. The Tea Party, whatever its faults, was a live political force that had managed to achieve temporary restraint in the defense budget, a feat whose last occurrence had required the collapse of the Soviet Union. Obama was pursuing a second‐​term foreign policy that, if not exactly worth defending, at least challenged the elite consensus on grand strategy in a couple of respects.



Well, there is nothing that the world likes better than nice, tasty hopes. The forces enumerated at the end of Christopher Preble and John Glaser’s lead essay turned out to be significantly stronger than I estimated. American power has proven so extensive that a grand strategy explicitly justified in terms of many varied goals like the liberal order is now plausible to the foreign policy establishment. The material and ideological consensus in favor of primacy among the national security elite has proven so robust that American commitments have been able to resist the election of a president like Donald Trump, who is no one’s idea of an internationalist. The American people turned out to give even less of a damn about foreign policy than I expected.



Today I believe that the probability of normal politics producing a genuinely restrained grand strategy is exceedingly slight. The best hope for a major change is probably a crisis that exposes the unexpected risks and costs of primacy. For this reason alone, the task of making the case for restraint remains vital: policymakers will need to have good ideas lying around if and when the bankruptcy of primacy is revealed.



However, I increasingly believe that more effort should be devoted among partisans of restraint to “second‐​best” policies, in case my pessimistic political assessment proves out. And I am not confident that the standard answer—that the second‐​best policy is “less of whatever primacy is proposing”—is always true.



For instance, if we are not going to abandon American alliances, I am not certain that loosening those ties is worthwhile, as it may encourage bad behavior among allies and adversaries alike. If we are going to retain American political commitments, then I suspect that will require more robust military capabilities than I would like as a matter of first preference. I worry that grand strategies may best be plotted on a U‐​shaped curve, where the tail strategies of primacy and restraint both produce reasonably coherent and stable outcomes, but where the strategies in the middle—“off-shore balancing,” “selective engagement,” and “liberal internationalism”—turn out to be ineffective and destabilizing to world politics.



But working out whether there is anything to these concerns would be the subject of a completely different essay. And the present essay, I believe, retains real value. Its fundamental conclusion is still true: “the United States spends hundreds of billions of dollars a year—and risks war—largely to stop other people from fighting among themselves. The common story that reducing regional competition abroad makes America more secure at home is close to being backwards.”



My essay is not the most original or brilliant exposition of this basic point — but as bottom line conclusions go, I think one could do a lot worse.



–Brendan Rittenhouse Green



Cincinnati, OH
"
"
Ok the next session is starting in a few minutes, less than 2 hours from now I’m going to know if the work I and all of the volunteers at www.surfacestations.org has been scientifically fruitful, or if I’m going to get pelted on the stage with rotten fruit.
My presentation is updated with some late breaking photos Russ Steele got yesterday from St. George, UT, loaded into the presentation laptop, and my remote control has been tested. I’m as ready as I’ll ever be.
At the very least, after sitting through a bunch of Powerpoint presentations, my use of the same software I use for doing TV weather presentations should break the mold.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea442c16c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

My friends at coffee this morning got a huge laugh out of Chico Peace and Justice Center member Sherri Quammen’s claim in a vitriol filled letter to the editor that I’m the “real WMD”.
For somebody who professes “peace and justice”, she sure seems to have a lot of anger to vent. She’s sent letters to all three newspapers, the ER, Chico Beat, and you’ll see the same letter come Thursday at the Chico News and Review I’m sure. Lately, the message of “peace on earth” seems to have lost the accessory clause of “goodwill towards men”. Though its hard to tell through her rant just what she dislikes about me most, it appears that my views and research into climate change must be the main factor.
I sent her a nice note last week, offering to meet and get aquainted over coffee or tea someday, (since we’ve never met) after the letter appeared in the Chico Beat, so far no response.
But that’s OK, being a public person, criticism comes with the territory. It’s an occupational hazard. I guess I should be honored that my threat level has been elevated. Poor Al Gore takes all sorts of flak daily.
Sooo….since I’ve been labeled a WMD, I think that I’ll have to look over my shoulder a lot to make sure I’m not being followed by police officers intent on giving me a ticket in case I go off in the Chico city limits. That’s a $500 fine you know.
To make it easier for people to spot me, I think I’ll get a T-shirt that says simply “BOOM”.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6794214',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter
By Kirye (photo)
and Pierre Gosselin
Today we look at the mean annual temperatures of western USA stations that have a Brightness Index (BI) of 0, meaning they are not subjected to urban heat island impacts.
Many people are claiming that temperatures worldwide are rising due to greenhouse gas emissions from human activities.
First we begin with the station located at the town of Fort Bragg in California. Using NASA data, we plot the annual temperatures going back to 1935!

Data source: NASA GISS 
Above we plot the V4 unadjusted versus the V4 adjusted data. Neither show any warming since 1935. The adjusted data, however, turns a cooling trend into one of no cooling.
Recently I tweeted an animation that compares the v4 unadjusted data to the V4 adjusted data for the Beowawe station in the state of Nevada:

GHCN V4 Unadjusted data show Beowawe, State of Nevada has had a cooling trend since 1891!Needless to say, NASA changed the data by a large margin.https://t.co/XfEZRXWoFl~#地球温暖化? #温暖化？ #気候変動 #ClimateChange pic.twitter.com/bDIleXLYuB
— キリエ (@KiryeNet) May 10, 2020

Note how the Beowawe data of the past was substantially altered (reduced) in order to create a warming trend from a previously cooling trend. Here the warming is man made – but statistically by researchers at NASA.
The story is similar for 4 other stations located in the western US.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




At the Manti station in Utah, modest warming was adjusted to created more warming:

Data source: NASA GISS
The same is true for the Seligman, Arizona station:

Data source: NASA GISS. 
The mean annual temperatures measured by the Cheesman, Colorado station used to show a cooling trend since 1903, before NASA tampered with the data and changed them into a warming trend: 

Data source: NASA GISS.
Finally we look at the data from the station for Hachita, New Mexico:

Data source: NASA GISS.
Here for Hachita, NASA changed the data so that modest warming was changed to produce greater warming.
Why do the new, adjusted data plots always end up warmer and never cooler? This seems to be Deep State science, and not real science which the public expects to get and is owed.


		jQuery(document).ready(function(){
			jQuery('#dd_016dc3091d95560b9168dfc61dda50bc').on('change', function() {
			  jQuery('#amount_016dc3091d95560b9168dfc61dda50bc').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterRecently I wrote here how Germany’s now infamous record-setting weather station in Lingen was producing readings that were 2-3°C hotter than surrounding stations, yet the German DWD weather service refused to acknowledge the station was likely producing bad data. Today they admit the station has problems and that they will be moving it to a better location.
Last year’s all-time record high is now in question.
Lingen’s heated readings
Last summer the Lingen station, located in northwest Germany near the Dutch border, smashed the country’s all-time record high when the ‘mercury’ rose to a scorching 42.6°C during a late July heat wave. The previous all-time high for Germany was a comparatively cool 40.3°C.

Lingen’s readings of late July 2019 compared to other stations in the surrounding region (July 23 – July 27).
Today, t-online.de reports that Germany DWD national weather service has now reversed and realized that something may be very wrong with the Lingen station after all, and so will relocate it and examine its recorded data.
“The weather station in Lingen: The DWD will not publish the temperatures measured here anymore – there are doubts about the data,” T-online reports. “Now it is being relocated.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Moreover, the temperature measurements previously recorded at the station will be checked, and there are now doubts whether last year’s all time record will be allowed to stand.
“Astonishing temperature values”
“We don’t know yet whether the value will stand,” admitted a DWD spokesperson on Friday. “Again and again and more and more frequently astonishing temperature values” had been coming from Lingen.
The DWD experts believe the distorted values are linked to “certain weather conditions,  especially on hot summer days when there is little wind. The station in Lingen is located in a depression and is now surrounded by trees, which means the air gets stuck in place and so heats up.
The decision to stop using data from the current Lingen station and to relocate it represents a position reversal by the DWD. Earlier the DWD had told Bild daily how it planned to stick to the Lingen readings, deeming them to be of good scientific quality and that an earlier site assessment had found that the station conditions had “no serious influence on the temperature measurements” and therefore “did not contradict the WMO standards.”
The DWD now acknowledges the station has siting issues and its data are suspicious after a number meteorologists criticized the station’s poor siting.
“The quality of our measurements has the highest priority,” said Jürgen Schreiber, DWD’s Chief Technical Officer. “We have decided to no longer publish the observational data from the Lingen station, but to use them internally for scientific tests only”.
The DWD spokesman said a second sensor would be used to measure the temperature at another location in parallel. Though after two days it is still too early to make scientific statements, but already there have been noticeable deviations in the temperature measurements. Now the tests are to be carried out with meticulousness.
T-online.de confirms that construction work for a new location has begun and that by next spring there could be temperature data coming from Lingen again.
Share this...FacebookTwitter "
"
Pictures have been coming in to www.surfacestations.org from many places. This one is from Fort Morgan, Colorado’s USHCN climate station of record. Fort Morgan is in the eastern plains of Colorado, about 100 miles northeast of Denver.
In such a place, with all that open space, you’d think it would be an easy matter to place something as important as an official NOAA temperature sensor used to contribute measurements to the national climatic database in some of that open space.
No such luck. In fact, the sensor recording the wide open plains has four air conditioners near it!

But lets not forget, in keeping with current observed trends, that any weather station with air conditioning also needs close-by parking.

It’s not like there’s no other open space to put the sensor in Fort Morgan.

The pictures above, courtesy of the Pielke Research Group shows an electronic Min/Max Temperature Sensor placed near a grain elevator office. Cable length limitations on this sensor have caused hundreds of similar placements in the USHCN network where Stevenson Screens used before could be placed a good distance away from such influences.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea556c7ca',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
This picture below comes to me via surfacestations.org volunteer Kristen Byrnes, a 15 year old budding scientist that has created a bit of a stir with her critique of Al Gore’s Inconvenient Truth. Her website,”Ponder the Maunder” also has more photos of weather stations.
It is the USHCN Climate Station of Record for Lewiston, Maine, placed at the Union Water Power Company there.

It features an air conditioner unit, a portable barbecue grill, pavement and a nearby building. No close-by parking though as we’ve seen with other stations.
It also features a curious non-standard instrument shelter, of a design I’ve not seen before. The observing height appears to be non-standard, and lower to the ground than usual.

In addition to the close by hard surfaces like concrete pavement, the shelter also is located on an up-slope. That’s a no-no according to NOAA siting specs for a good reason – hot air rises.
Ms. Byrnes found another interesting station in Eastport, Maine. Ms. Byrnes found another interesting station in Eastport, Maine. While it is not part of the USHCN climatic network it is worth looking at because it shows how something simple and obvious that was missed can skew any experiment.
This station is a state operated, NOAA funded special monitoring station with high accuracy, very expensive laboratory grade sensors. The temperature sensor is aspirated, meaning it has a powered fan to draw air in from the outside, and is considered the most accurate way to measure air temperature. The same temperature sensor is used in the US Climate Reference Network (USCRN) specs of which can be seen here and photos here.
The setup also has a portable electronics building to go with it, to house all the data logging and analysis electronics. All that electronics needs to be kept cool, so these building are fitted with an air conditioner.
But the scientists who placed the temperature sensor were apparently so transfixed on the goal, they didn’t notice the air conditioner for the electronics building:

Fortunately, the US Climate Reference Network sites I’ve seen are much better thought out than this station in Eastport Maine.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5298c60',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The rotting remains of a number of tigers, lions and cougars were recently discovered in a raid on a house in Prague. This disturbing find was the culmination of a five-year investigation that revealed an illegal trade in exotic wildlife blooming in the heart of Europe. Czech authorities managed to identify the main figures behind an international crime ring who had been processing and selling wild cat parts as traditional Chinese medicine. Claws, teeth, bones, skin and extracts from their bodies known as “tiger wine” or “broth” were smuggled to Asia or used to supply the domestic demand in tiger products. The slaughtered tigers came from the country’s largest private breeding facility for lions and tigers – where, officially, these protected wildcats are bred for circuses, roadside attractions and petting zoos. This story provides a stark reminder of the cruelty engendered by captive breeding. Even zoos heralded as the beacons of endangered species conservation play a controversial part in this story.  With only 3,900 left in the wild, the tiger family (Panthera tigris) is the only big cat listed as endangered, with two subspecies critically endangered. The captive population, meanwhile, is abundant.  In 2014, the WWF alerted us to the alarming news that there are “more tigers living in American backyards than in the wild”. The organisation called on the US government to introduce a ban on private ownership of big cats. No such federal bill has been passed since, but 21 states ban all dangerous exotic pets, while the rest allow certain species or require permits. Out of 5,000 captive tigers in the US alone, only 350 are held in zoos and other facilities accredited by the Association of Zoos and Aquariums. The estimated number of tigers in the Czech Republic, meanwhile, is 390, only 39 of which are kept in zoos.  A growing number of cities around the world close their gates for circuses that use wild animals. According to Czech law, captive breeding of big cats requires special permits, while the environmental inspectorate records each tiger’s birth, sale or death. Following the discovery of the tiger slaughterhouse in Prague, the European Association of Zoos and Aquariums issued a statement urging authorities to take immediate action in ensuring that all captive tigers serve noncommercial purposes such as research, education and conservation breeding. The idea of protecting endangered species through captive breeding in zoos is relatively new, but has a much longer and darker history.  Exotic animals first entered private collections in Europe as diplomatic gifts. Tigers were particularly highly priced in royal and aristocratic menageries as dangerous predators were seen to embody the political and physical prowess of their owners. Wild cats were also exhibited for popular audiences in circuses and other travelling shows. The intensive traffic in wildlife was largely facilitated by colonial expansion. That is why European port cities, as the centres for colonial commerce, were the first to open public zoos. In the aftermath of decolonisation and the introduction of the Convention on International Trade in Endangered Species in 1973, the lucrative business of capturing and trading exotic animals came to an end. Faced with the termination of a supply of specimens caught in the wild, zoological parks resorted to captive breeding.  They did so, on the one hand to ensure they retained rare species in their collections and, on the other hand, to redirect their mission: from entertainment towards conservation. Devising so-called “Species Survival Plans”, accredited zoos have collaborated since 1981 to breed endangered species and manage all captive individuals of every species as one population to ensure genetic diversity.  But even after this period, research, education and conservation did not always drive captive breeding in zoos. Even non-commercial breeding does not always prioritise animal welfare.  Many zoos, for example, are still devoted to breeding white tigers. Only two years ago the Czech Liberec Zoo celebrated the birth of two white cubs, that were transferred to Pont-Scorff Zoo in France in July this year. This rare variation of the Bengal tiger has distinctive white fur colouring with pale chocolate stripes and mesmerising blue eyes. The extraordinary coating results from a genetic mutation, which as a recessive trait is expressed only if both parents carry the mutation. This inclined the zoos to practice inbreeding, often pairing off siblings in hope for a white-furred offspring. All 250 white tigers in captivity today are related, having a common ancestor captured in 1951 – the wild-caught cub named Mohan that was the pride of Maharaja of Rewa, an Indian royalty who was determined to breed these rare wild cats. After several failed attempts, in 1957 the first white cubs were born in India from the union of Mohan and his daughter Radha.  In 1960, the Smithsonian Institution procured one of the female cubs for $10,000. Today she would be worth eight times more. While the royal ancestry of this exotic feline vividly stimulated the imagination of American zoogoers, her main task at the National Zoo was to produce more offspring of her kind. The demand for these extremely rare animals often justifies pairing off closely related tigers, even though inbred animals are prone to acquiring crippling defects including shortened legs, kidney problems and crossed eyes, as well as psychological issues.  The tigers slaughtered in the Czech Republic were not bred in zoos but in a private facility, yet their story should put captive breeding in general into question.  Today, tigers are bred outside of their natural habitats for a variety of reasons: for zoos, exhibitions, circuses performances or as pets. Tiger cubs are often displayed in petting zoos and subjected to the cruel practice of declawing. Adult tigers are drugged to pose in photos. People still see these extremely dangerous carnivores as proxies for luxury and sexiness. But hopefully attitudes are changing. In 2017, Tinder launched a campaign to encourage its users to stop posting “tiger selfies”. And most recently, due to public pressure, China was forced to reinstate a newly lifted ban on using tiger bone and rhino horn in medicine. Of course we need to pay attention to the conservation of today’s wild tigers threatened by habitat loss due to human activity, poaching, loss of prey and the swelling human-wildlife conflicts. But more attention should be paid to the plight of the enormous captive population of tigers across the world. This article was updated on November 26 to correct the stated number of captive tigers in the US."
"One of the few silver linings of the horrendous bushfires across the country has been the sight of people setting aside their differences to help one another during this time of crisis. “Natural” disasters don’t discriminate based on class, ethnicity, religion or culture: everyone in the line of these fires is equally at risk, and directly affected by the devastating outcome. The only real divide is between people living in dangerously exposed regional and rural areas, and those residing in the relative safety of our cities. Even so, the shocking levels of smoke haze blanketing Sydney, Canberra and Melbourne have meant that city-dwellers cannot avert their gaze from the impact of these massive fire fronts, so devastatingly charged by the impact of climate change on our arid land mass.  So is it too much to hope that when the fires are, at least temporarily, quelled and the smoke subsides, this sense of national unity might extend beyond the immediate crisis? Because to address the urgent threat of climate change and ensure the wellbeing of all Australians, we must stop dividing people along the faultlines of class and culture and rediscover the solidarity that has driven every major social change in our history. Unfortunately, recent comments from those within our political class point to a failure to grasp the nature of this challenge. Since the federal election in May, we have heard repeated calls for the ALP to abandon its “blue collar” base, forget about regional Australia, and throw in its lot with urban small-l liberals of the post-materialist, cosmopolitan class. This way, suggest too many commentators on the fringes of the Labor party, lies an election-winning coalition to return the ALP to federal power. To what end is unclear. First, surely a Labor party that no longer represents working people is no longer worthy of the name. For the ALP to win an election by casting off its obligation to improve the material conditions of the working class and reduce inequality would be the ultimate exercise of seeking power without purpose. More fundamentally, these analyses reveal shallow thinking about the nature of power, and a fundamental misunderstanding of both working class culture and the history of progressive political change. Real progress, both social and economic, is impossible without the working class. Incremental social progress, on issues such as marriage equality and refugee rights, may be championed by educated people of wealth and means, but meaningful action to redistribute power and ensure a fair share of our common wealth for all citizens is never realised through the benevolent deeds of those in positions of social and economic privilege. Throughout history, working people have been at the forefront of progressive change. From the chartists agitating for political reform in the UK in the mid-19th century, to the cotton mill workers in Lancashire refusing to touch slave-grown cotton from the US during Lincoln’s presidency; from Australian unionists fighting for the eight-hour day, to the blockades by the labour movement that were so instrumental in fighting apartheid; from the role of workers in the US civil rights movement in the 1960s to the active involvement of the Australian union movement in the campaign for marriage equality: real social and political change has been driven by the solidarity and mobilisation of working people. Yet today, both here and internationally, the working class communities that powered so many of these achievements are abandoning leftwing political parties and throwing in their lot with rightwing populists and demagogues. And so we hear in response that the left should accept their rejection by the people they are meant to represent, and capitulate to the divisive politics that is effectively undoing two centuries’ of social progress towards a more equal society. Working people, some seem to have decided, are too socially conservative, too selfish and too ignorant to be saved. This is breathtakingly snobbish, woefully ignorant, alarmingly defeatist, shamefully irresponsible – and politically myopic. The left’s pursuit of progressive social policies isn’t the cause of the alienation of working class communities from social democratic politics; rather, it is a loss of trust that the political left still has the will or the capacity to defend the interests of working people against the forces of extreme market capitalism that have fractured their communities and destroyed their livelihoods. The ravages of trickle-down economics have left people scared and insecure. Yet even when the economic project built on deregulation, small government and the relentless pursuit of private profit hit the wall during the GFC, the left was woefully unprepared to step in with an alternative vision to rebuild our common wealth and ensure secure livelihoods for all. A decade later, in the face of repeated defeats, it seems too many people who call themselves social democrats are still seeking short-term political fixes to win government without disrupting the power and privilege of the capital class to which so many of them now belong. The answer to building a progressive coalition to support meaningful action on climate change, address economic inequality and build a better society isn’t to talk down to people and try to outdo the far right in the culture wars by pandering to fear and division; it’s to link social progress to economic progress, and fight for a better society for all Australians. This begins with rebuilding the solidarity of working people, through the bonds of community and social action. It requires us to think beyond the tactical measures required to win an election, and to engage in the hard work of creating a movement for change – one which focuses on solutions to decarbonising our economy that create new, secure jobs for people on the frontlines of ecological and economic change; and the promise of a social contract that allows people the security and certainty to think beyond their immediate material needs and conceive collectively of the conditions required to improve the lives of their neighbours, friends and fellow citizens. This collective action of ordinary people in pursuit of a better society is the means by which social progress has been always been achieved, and it always will be. It’s time to stop navel-gazing and get on with the job. • Emma Dawson is executive director of public policy thinktank Per Capita"
"
Share this...FacebookTwitterBy Kirye 
and Pierre Gosselin
So what’s going on?
NASA and other government agencies keep telling us that the globe is warming and ice becoming more rare, yet when look out the window, things often appear to be going the opposite direction.
Rare cold, snow grip Greece
For example, the Greek Reporter here informed how a “rare spring snow” blanketed large parts of northern Greece. It reported: “Of course, Northern Greece is used to low temperatures and snow, but even for their standards, such an intense snowfall in April is rare.”
Moreover, the widely read Electroverse weather site here reported how southeast Europe had seen its “coldest April morning in a decade”, potentially causing widespread crop damage.
So why are such events happening when they aren’t supposed to be?
Altering: from cooling to warming
Today we look at the NASA data from two stations in Greece: Makedonia and Larissa, shown below:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




These two stations have data going back to 1892 and 1899 respectively.
As we know NASA has been busy over the last years adjusting and homogenizing data from stations around the world, again and again. Data which once stood for years, suddenly got altered – in many cases very substantially. Greece here is another example.
What follows are side-by-side plots of the two stations: V4 unadjusted and V4 adjusted:

The two GHCN V4 unadjusted mean annual temperature data plots of the respective stations clearly show a cooling trend.
So no wonder we are seeing “rare” snow and cold in April.
NASA’s claimed warming is statistically fabricated
But NASA altered the data for the two Greek stations and named the two new data sets “V4 adjusted”. Clearly the adjustments magically produce the warming they like to scream about.
The warming didn’t occur because the air warmed up, but rather because NASA changed the data. The warming is a fake.
Though lots of people, media and politicians are listening and believing it, the weather isn’t. Snow and cold continue to make their appearance, Greece and other places are showing.


		jQuery(document).ready(function(){
			jQuery('#dd_525a8a6412dd739f5bdb3ff44afa9033').on('change', function() {
			  jQuery('#amount_525a8a6412dd739f5bdb3ff44afa9033').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterOne highly visible person often seen at Greta Thunberg’s side at public Fridays For Future appearances is German 23-year old climate activist Luisa Neubauer of Hamburg.

Climate activist Luisa Neubauer at the center of infighting between FFF organizers. Image: Andol – own work, CC BY-SA 4.0
The Green Party member was a driving force behind the launch of the Fridays For Future movement in Germany and Europe. However, it appears other FFF organizers have had enough of Neubauer constantly hogging the spotlight amid signs of growing discord and infighting among FFF organizers.
MOPO.de here reports how Neubauer has been “disqualified as permanent speaker” by FFF organizers and “will also not speak at the controversial citizens’ meeting in Berlin in June.”
Moreover, according to MOPO: “Neubauer already was replaced” by 17-year-old schoolgirl Helena Marshall at the Siemens Annual Shareholders’ Meeting on February 5.
Jet-setting Greenie “Longhaul Luisa”
The problem, MOPO writes, is that “Neubauer is too much in the public eye” and that she may not be “suitable as a representative” because it was reported how she had flown often. Neubauer was recently dubbed “Longhaul Luisa” after having posted images of herself on foreign trips in social media.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Over the last years, climate activist Luisa made it around the world by plane several times, via Austria, Switzerland, Italy, Belgium, the Netherlands, Sweden, Poland, England, Scotland, France, Canada, China, Hong Kong, Nepal, Morocco, Namibia, Tanzania, Indonesia, etc.
Once in a talk show, Luisa was asked what she personally would do to protect the climate. She answered: “Fly as little as possible”.
Open discord between Greta and Luisa
Evidence of open discord among the FFF movement organizers also surfaced in a video showing Greta Thunberg expressing her displeasure at comments made by Neubauer during the World Economic Forum earlier this year.

Strife is undeniable as Greta openly shakes her head as Neubauer speaks.
MOPO reports the “first trouble” began “already at the beginning of spring 2019” when co-organizers felt “the distribution of speeches and appearances was too one-sided.”
Greta skips France
Today Greta tweeted she was skipping appearances in the marches in Grenoble and Paris this Friday, citing “family reasons”.


		jQuery(document).ready(function(){
			jQuery('#dd_2eb4fb5288a5586c6e0d291efb22ada4').on('change', function() {
			  jQuery('#amount_2eb4fb5288a5586c6e0d291efb22ada4').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Australia’s current position as “ground zero” for both the impacts of climate change and policy uncertainty presents an opportunity for the country to emerge as a leader in responding to the climate crisis, according to Australian Research Council laureates. In a letter signed by 80 ARC laureate fellows, some of Australia’s top researchers said claims strong action to cut emissions would be economically destructive have no basis and are not “consistent with Australia’s traditional optimism and ingenuity, nor with historical experience”.  “Reducing emissions is a global challenge that requires collective action,” the letter said. “But Australia’s current visibility as ground zero for both climate impacts and climate policy uncertainty presents a unique opportunity for us to emerge as a leader on this challenge.” The ARC laureate fellows are a small group of researchers selected by the ARC as the top researchers across all fields in Australia. The letter, whose signatories include decorated academics in mathematics, science, economics, and language and culture, said the government’s focus on adapting to changed fire patterns “is not enough”. It was written as the country’s unprecedented bushfire season continues, with emergency warnings in place on Tuesday for a fire burning in the Namadgi national park near Canberra. “We welcome government actions to help current victims and improve adaptation to future fires, as well as its acceptance of a role for climate change in the catastrophe,” the letter said. “But this is not enough, because the greenhouse gas amounts driving warming are still rising: the world is only at the beginning of the climate change phenomenon.” The bushfire emergency has brought the Coalition government’s climate policies into sharp focus. The prime minister, Scott Morrison, was criticised for his handling of the crisis through Christmas and early January, and for his failure to meet with former emergency chiefs who warned of the coming catastrophe last year. Morrison said this month that the government’s response to the increasingly visible effects of the climate crisis would be to address “adaptation and resilience” rather than strengthening policies to reduce emissions. But the letter warns that without stronger action to curb emissions, the impacts of further temperature rises could be such that adaptation is not achievable. “This dire outlook demands stronger mitigation of carbon emissions,” it said. “Many argue that actions to achieve this would be economically destructive. This claim has no basis, nor is it consistent with Australia’s traditional optimism and ingenuity, nor with historical experience.” They wrote that achieving net zero emissions was a large but achievable task, and “far less risky and irresponsible” than allowing continued global heating. Australia faced international criticism as one of a handful of countries at the United Nations climate conference in Madrid in December that were responsible for thwarting a deal on the rulebook for the Paris climate agreement. Australia is the only country that plans to use carryover credits from the Kyoto period to meet its targets under the Paris agreement."
"

 _Global Science Report_ _is a weekly feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
Carbon dioxide regulations promulgated by the EPA are based upon the assumption that they will actually _do_ something about climate change in the U.S., and that the rest of the world, which had been needling the U.S. for decades of inaction, will now follow our virtuous lead.   
  
Neither is going to happen.   
  
This _Report_ is based upon just-released data from the U.S. Energy Information Administration showing that the amount of carbon dioxide emitted from the U.S in the last year was the about the same as was emitted in 1994—nearly two decades years ago. During that time, emissions grew steadily for 14 years, peaking in 2007, and then fell dramatically (Figure 1). The emissions in 2012 were 12% less than those of 2007.   






**Figure 1. U.S. annual carbon dioxide emissions, 1994-2012 (data source: U.S. Energy Information Administration).** Given this non-trivial decline in carbon dioxide emissions, let’s see how the government’s assumptions are holding up.   




**Assumption 1: Reducing U.S. carbon dioxide emissions will do something about global warming-related weather/climate impacts in the U.S.**   
  
The U.S. National Oceanic and Atmospheric Administration (NOAA) compiles the number of “Billion Dollar Weather/Climate Disasters” in the U.S. The higher this number is the more media attention they get and the more global-warming-is-making-the-weather-worse-and-we-must-immediately-act-to-stop-it furor it engenders.   
  
With U.S. emissions on the decline, so too, certainly, are the number of billion dollar weather disasters, right?   
  
Figure 2 shows the annual tally of the number of billion dollar “disasters.” Despite the rapid U.S. emissions decline for the past 5 years, the number of weather disasters in the U.S. is increasing.   






**Figure 2. Number of billion dollar weather/climate disasters in the U.S., 1994-2012 (data compiled by the U.S. National Oceanic and Atmospheric Administration).** There are two primary reasons why this is the case. The first is that the number of people and the amount of stuff that we all have continues to increase (i.e., there is more stuff in harm’s way), and the second, _there is no relationship between U.S. carbon dioxide emissions and extreme weather events in the United States_. U.S. emissions could have been _zero_ and the result would have been even worse—because rebuilding infrastructure in the extremely expensive energy economy that would ensue would cost much, much more.   
  
  
  
**Assumption 2: The rest of the world will follow our lead and reduce emissions.**   
  
As we like to point out, the magnitude of future global warming (and accompanying climate change) rests not on the future carbon dioxide emissions pathway of the U.S., but rather on that taken by the rest of the world. In the typical mid-range emissions scenario employed by the UN’s Intergovernmental Panel on Climate Change (IPCC), the amount of warming projected to occur between 1990 and 2095 is 2.8°C (5.0°F). Of this amount, only about 0.2°C (0.38°F)—about 7%—is expected to result from U.S. emissions. Big deal.   
  
Further, these forecasts are likely way too high. We’ve experienced 0.33°C of warming since 1990, or about 12% of the forecast total, even as 22% of the forecast period is has already passed.   
  
So then why is there so much focus by President Obama on regulations aimed at reducing U.S. carbon dioxide emissions when they will result in no meaningful climatic consequence?   
  
Because our emissions declines are supposed to set an example and other nations of the world will follow suit.   
  
That’s not happening, either.   
  
Figure 3 shows the carbon dioxide emissions from the rest of the world over the same time period as the U.S. emissions in Figure 1. When U.S. emissions increased 14% from 1994-2007, emissions from the rest of the world increased. When U.S. emissions then declined 12% from 2007-2012, emissions from the rest of the world increased even faster.   
  




**Figure 3. Global (less U.S.) annual carbon dioxide emissions, 1994-2012 (data source: U.S. Energy Information Administration updated through 2012 according to Peters et al., 2013).** Why? Because the emissions growth in the rest of the world is primarily being driven by China and other developing countries that are more interested in growing their economies then they are in emulating the U.S.   
  
All which has to make you wonder. As the data show that the premises upon which U.S. greenhouse gas regulations have been promulgated are wrong, why do we persist with such silly notions?   
  
**Reference:**   
  
Peters, G.P., et al., 2013. The challenge to keep global warming below 2°C. _Nature Climate Change_ , **3** , 4-6, doi:10.1038/nclimate1783.  

"
"
Share this...FacebookTwitterTwo new papers use tree ring proxy evidence to suggest modern European temperatures are neither unusual nor higher than they were during the Medieval Warm Period.

Image Source: Esper et al. (2020)
Esper et al. (2020) have produced a new temperature reconstruction for Southern Europe to complement past reconstructions for Northern and Central Europe.
They find “the warmest 30-year period since 730 CE occurred during high Medieval times (876–905 CE=+0.78 °C w.r.t. 1961–1990) and has been slightly warmer than the recent period from 1985–2014 (+0.71 °C)“.
The proxy evidence and instrumental record also show there has been no obvious net warming in Southern Europe since the 1940s.
Past reconstructions for Northern and Central Europe also show no unusual warming has occurred over the last century, with as-warm or warmer temperatures during the 1940s.
Ljungqvist et al., 2020  cite tree ring temperature studies from Scandinavia, Scotland, Continental Europe, and the Pyrenees that also show the 1930s and 1940s were as-warm or warmer than recent decades.

Image Source: Ljungqvist et al., 2020
Share this...FacebookTwitter "
"During my time as a zookeeper I had the privilege of working with both Sumatran and Amur tigers. If they did not both have stripes, you would think they were different species altogether.  The Sumatran tiger is the smallest alive today. At around 100kg, it’s “only” about the weight of a large adult male human. It is suited to the warm and wet forests of the Indonesian island of Sumatra, which is reflected in its smaller size and short, dark rusty orange coat which has many thin black stripes to conceal it in dense vegetation from their prey.  The Amur – or Siberian – tiger is much larger, averaging around 170kg (though there are historic reports of males clocking in at 300kg or more) and is now found mainly one corner of far-eastern Russia. It has a thicker but relatively pale coat, with sparse dark brown stripes, which enables it to survive in freezing and snowy winters.  Tiger experts have long debated what such differences mean scientifically. Should the biggest of the big cats be divided into various subspecies, or are all tigers simply “tigers”? It’s an issue with serious implications for conservation. About 3,500 or so tigers remain in the wild, in just 7% of their former range. And if those tigers are all the same, or if even most of them are the same, then saving individual populations matters slightly less – and tigers can be moved around to assist breeding in the wild.  Traditionally, eight subspecies were considered to exist. They are the two already mentioned, plus the Bengal tiger, found mainly in India, the Indochinese, the South China tiger and then three extinct subspecies: the Bali (extinct in the 1940s) and Javan (80s), both closely related to surviving tigers on nearby Sumatra, and the Caspian tiger from Central Asia which went extinct in the 1970s. As genetic techniques evolved, a 2004 study found there was little genetic diversity among tigers, but enough to support the separation of subspecies. It also suggested that Indochinese tigers living on the Malayan peninsular were different enough to those living further north to warrant a ninth subspecies: the Malayan tiger.  These ideas were contested by a group of researchers in 2015, who argued that the relative lack of variation among the mainland Asian subspecies and large overlaps in their shape, size and ecology meant that all tigers from India to Siberia or Thailand should be considered the same subspecies. The researchers called for just two recognised subspecies: the continental tiger, and the Sunda tiger, found on the various Indonesian islands. However the various subspecies are classified, one of the consistent findings is that tigers follow Bergmann’s rule: a principle in zoology which states that animals within the same overall species will tend to be larger in colder environments and vice versa. The Amur tiger, for instance, benefits from the fact that larger animals are better at retaining heat as they have a smaller surface area relative to their overall mass.  This is where a new study published in the journal Current Biology fits in. Researchers from China and the US looked at the whole genomes of 32 representative tigers and found that there were indeed nine subspecies of tiger – of which six survive today. But their work also demonstrates that the various adaptations to temperature – Amur big and hairy, Sumatran small and sleek – were triggered by significant prehistoric events that changed global and local temperatures.  The findings confirm previous speculation that the low genetic diversity in tigers was caused by a population decline during an ice age 110,000 years ago. Thousands of years later, the earliest split from a single common ancestor species occurred between island and mainland subspecies, with the former developing a smaller body size thanks to natural selection. The super eruption of the Sumatran volcano Toba 75,000 years ago followed by an extreme cooling period was the likely cause. Further splits into more specialised tigers reflect other significant extreme climatic changes.  So why is this important in terms of tiger conservation? As past research has argued, the lack of genetic and morphological differences between mainland tigers could allow them to be managed as single subspecies. Theoretically individuals from any region, wild or captive, could be relocated to repopulate former areas or increase numbers of failing local populations. This could help to increase general tiger numbers and local genetic diversity.  But the recent study suggests that tiger adaptations may be more subtle and intricate than first appeared. If tigers are allowed to hybridise either in captive or wild populations it could drive the more vulnerable subspecies to extinction before we fully understand exactly how they have adapted to their particular area.  There is a downside to considering tigers as separate subspecies and attempting to protect them on this basis, without mixing in tigers from elsewhere. Numbers of each subspecies are very small – there are only around 500 wild Amurs, for instance – and smaller populations are more vulnerable to extinction. This could be caused by the regular threats of habitat loss and poaching or simply due to reduced genetic diversity making a small population vulnerable to disease and other selective pressures. Genetic diversity is key for adaptation and ultimately species survival. As our understanding increases, more informed decisions can be made regarding how best to conserve the tiger. We might not have enough time to solve all the riddles but perhaps this is one step closer to ensuring one of the world’s most iconic animals does not disappear forever."
"
I’m surveying climate stations of record around California and documenting their condition as part of a larger project I’m doing. You’ll see more about it here in the near future.
Today I visited Marysville’s Fire Station, just off Hwy 70 at 9th and B Street, where they have the station of record for the city using the MMTS electronic sensor installed by the National Weather Service. The data from this station is part of the USHCN (US Historical Climatological Network) and is used in the computer modeling used to predict climate change.
The Marysville station is located behind the fire department building on a patio and is probably the worst site visited so far. In addition to the sensor being surrounded by asphalt and concrete, its also within 10 feet of buildings, and within 8 feet of a large metal cell tower that could be felt reflecting sunlight/heat. And worst of all, air conditioning units on the cell tower electronics buildings vent warm air within 10 feet of the sensor. Oh and lets not forget the portable BBQ the firefighters use a “couple times a week.” The area has been constantly added to, what was once a grass rear yard was turned to a parking lot, then more buildings added, then a cell tower with one, then two electronics buildings and the air conditioners…no report on how long the firefighters were BBQ’ing back there, when they figured out why I was asking all the questions they clammed up.
I can tell you with certainty, the temperature data from this station is useless. Look at the pictures to see why, and is it any wonder the trend for temperature is upward?
 



Above: Vehicles with hot radiators park within 6 feet of the temperature sensor!

Now compare Marysville to Orland, just 50 miles away, where there’s not been any significant change in the last 100 years at the measuring location. Its obvious that Marysville is measuring UHI (Urban Heat Island) effects.

So the question is, how does bad data like this slip into the NASA GISS model database?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea65b785c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
In my previous post, NOAA Throws a roadblock my way I talked about how NOAA/NCDC has thrown a roadblock into the work being done to survey weather stations citing “privacy concerns” of observer’s name being included in station data being used to locate stations.
Alert blog reader Gerald Ingle passed this info on to me.
It appears that NOAA does not follow their own edicts, as they have a web page dedicated to cooperative observer newsletters and awards.

http://www.nws.noaa.gov/om/coop/2002-Awards.htm
On this web page you can find names of the observers, the station name, their PHOTOS in front of their stations, and in some cases their partial life history!
They also have a gallery of images in addition to the newsletters about COOP observers.
For example:

http://www.nws.noaa.gov/om/coop/2002/2002-15.htm

Britt, IA, Cooperative Observers Dianne and Keith Hansons show
off their 10 Year Length of Service Award.
This blows the NOAA/NCDC “privacy concerns” out of the water. They were worried about names appearing with MMS station data, well here we have names, photos, and more on NOAA’s own website.
They can’t have it both ways. Here is the link for the NOAA newsletters page.

http://www.nws.noaa.gov/om/coop/coop_newsletter.htm
Note the link where ANYBODY can sign up their email and get the newsletter chock full of names, stations, and photos of observers

Get on the free newsletter mailing list
It’s not even a confirmation email signup, just type in anybody’s email and it appears to accept it.
No confirmation email was received when I signed up, so apparently having somebody getting spammed isn’t an issue either.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5742f5f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterNow that severe restrictions concerning mobility and social distance have been put in place and led to a shut down of a large part of the economy, climate activists claim that already we are seeing huge environmental benefits, among them: cleaner air.
One person here in Germany even tweeted that he had not seen such clean air since his childhood, and attributed it to scale-down of human activity.
But German wetteronline.de here reports the clean air central Europe has seen recently since the COVID 19 crisis began is not the result of the shutdown, but is mostly due to the current weather pattern over Europe.
Wetteronline.de reports:
The lower volume of traffic and the largely idle economy certainly has an impact on the concentration of dust and dirt in the air. However, the current weather situation is much more important. On the verge of a powerful high over the Baltic, dry and cold air is being carried from Siberia to Central Europe. It is very clear and pure. In addition, there is a gusty easterly wind, which leaves no chance for a so-called inversion. Under such an inversion the concentration of dust, soot and dirt would increase rapidly.”
Wood burning the biggest threat
Moreover, a heated debate has been unleashed by Swiss meteorologist Jörg Kachelmann, who says the biggest threat to clean air in Germany is the now increasing use of wood burning for home heating.

Das ist immer die grösste Lüge. Nichts verbrennt dreckiger als Holz mit so vielen Zusatzsubstanzen.
Siehe den #Thread hierhttps://t.co/gp9ZexphxD


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Und auch auf der Tabelle die Folgen zu sehen im Vergleich.
Jeder Holzofen ist eine Umweltkatastrophe für sich.@davidermes https://t.co/QDgtshMd9g pic.twitter.com/O6XUjboWLq
— Jörg | kachelmannwetter.com🇨🇭 (@Kachelmann) April 5, 2020

40 times dirtier than natural gas
According to the veteran meteorologist, “nothing burns dirtier than wood and its additives” and “every wood stove is an environmental catastrophe”. The following chart shows the fine particle emissions from various heating fuels:

Wood pellets are 40 times worse than natural gas.
With governments moving to restrict fossil heating fuels and encouraging “renewable” wood, more and more Germans are opting for wood heat. Thus climate activists have to expect the air to become dirtier – much dirtier – and not cleaner should restrictions be enacted against the fossil fuel economy.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe Corona crisis has reduced car traffic, yet air quality has not improved. This suggests that the automobile’s role in air pollution has been vastly exaggerated. 

Image: NASA (public domain)
There are fewer cars on the road in major German cities due cities to the massive COVID-19 restrictions and “green zones”, “and yet it apparently does not look as if this will significantly improve air quality,” reports the online German Nordkurier here.
“Despite existing driving bans in large cities and the corona protection measures, nitrogen oxide pollution remains the same and is even increasing in some cases, according to the FDP in Mecklenburg-Western Pomerania.”
Environmentalists and climate activists like to claim that modern cars and industry have been polluting the air with dangerous particulate matter or nitrogen dioxide, but since restrictions were put in place 3 weeks ago, no improvement has been detected thus far.
“Less of an impact than previously assumed”
“The only conclusion that can be drawn from this is that air pollution from internal combustion engines has less of an impact than previously assumed,” said René Domke, regional chairman of the Free Democrat Party (FDP), in the north German city of Schwerin.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“He demanded that the future of internal combustion engines must be managed objectively again after these figures become known,” report Nordkurier.
Skeptics claim that there’s been way too much hysteria and baseless activism surrounding automobile traffic and the pollution they allegedly cause.
“Correlation broken, causality clearly refuted. Any further discussion about driving bans in view of these undeniable facts is completely unnecessary,” said Mr. Domke.
“Renewable” wood-burning the culprit
The real air quality problem now in Germany, critics say, is caused by the ever-increasing use of wood-burning for home heating, especially in the wintertime.
Often viewed  as a renewable source of heating energy, wood-burning increasingly has been shown to be a major cause of pollution in German cities. The leftist Guardian here reported in 2018, for example, that wood-burning has in fact been “suffocating cities” in the UK.
Since traffic has been reduced over the past weeks, air quality has  not improved, indicating that pollution from automobiles has long been over-hyped.
According to Domke, “The downright hysteria which the Deutsche Umwelthilfe and other NGOs have created  combustion engines, which at times dominated everyday political life, was and is completely unfounded.”
“In order to ensure mobility in the mainly rural Mecklenburg-Western Pomerania, we urgently need individual transport,” said Domke.


		jQuery(document).ready(function(){
			jQuery('#dd_3c88cf748d15ad302ac45b5484cb6404').on('change', function() {
			  jQuery('#amount_3c88cf748d15ad302ac45b5484cb6404').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterIn an interview with Punkt.Preradovic, finance Prof. Dr. Stefan Homburg of the Leibniz University of Hanover said Germany’s lockdown has “amounted to nothing”, has had no effect on the spread of the corona virus and that the spread had already slowed down below a reproduction number of 1.0 before the lockdown.
Citing data from Robert Koch Institute (RKI) 
In the interview, the prominent professor, once an adviser to former chancellor Gerhard Schröder, cited a chart from the Robert Koch Institute (RKI) that was issued on April 15th:

As the RKI chart shows, in early March the reproduction number had risen rapidly before reaching a peak on about March 10. By March 21st, the reproduction number dropped below 1.0.
“Ineffective”, “completely unnecessary”
It wasn’t until March 23 that the German government decreed a lockdown. As the chart shows, since the lockdown was enacted, the reproduction number did not change at all. It’s had no effect.
“It is not the case that the reproduction number went down after the lockdown”, Professor Homburg says. “There are two points we can draw from this: First, the lockdown was not necessary because the number was below 1, and secondly, the lockdown was not effective because the number didn’t drop afterwards.”
“Enormous economic damage”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Homburg agrees that the lockdown led to “enormous economic damage” and was “completely unnecessary”. In view of the data, Homburg does not know why the lockdown continues even today. Currently the reproduction rate stands at 0.7.
Homburg tells Preradovic that the politicians issued the lockdown in panic, came too late and thus so served no purpose. “It was not only unbelievably damaging for the economy, but also for other human factors. It’s about suicides and delayed operations.”
Panic fanned by absurd numbers
Citing the RKO numbers, Homburg also says: “There is not going to be any terrible epidemic. All the panic was fanned by the Robert Koch Institute, who said on March 20th that in the best case we will see 300,000 dead, and maybe 1.5 million dead.” Today the number is well under 5000.
One single alarmist paper
Homburg explains that the origin of the alarming death projections were adopted by the RKI from one single alarmist paper and that “it’s unbelievable that the government allowed itself to be so misled.”
Financial ruin for no reason at all
On the overall situation, Homburg comments: “It’s completely unimaginable. Huge damage is being caused. People and businesses are being financially ruined without any reason at all.”
On why it’s happening, Homburg says that German politicians, such as Angela Merkel, have gotten full cover from the media, and today enjoy high approval ratings because of the appearance of being competent crisis managers. And as long as the polls remain so, Homburg says, there’s little incentive for politicians to relax the lockdown.
Trump may wish to take a look at what’s happening in Germany, and move fast to end the destructive lockdown in USA. 
Share this...FacebookTwitter "
"People in Sydney woke on the morning of Thursday November 22 to see a bright orange sky as the sun was blotted out by a dust storm. Visibility and air quality were very poor and people with heart and lung conditions were advised to stay indoors. Ambulance call-outs spiked and extra paramedics were called upon to deal with the crisis. The dust cloud originated from north-western New South Wales, where strong winds ahead of a cold front met with dry, dusty soil. This lofted tiny dust particles into the atmosphere, which were carried hundreds of kilometers towards Sydney, and then onwards over the Pacific Ocean. Any parts of the world which have an ample supply of soil dust particles, dry conditions, and strong winds are susceptible to dust storms. The “dust belt” –- a train of dust hotspots stretching from the Sahara desert across the Middle East to Central Asia, dominates global dust activity.  Once dust particles are lifted into the atmosphere, they are transported thousands of kilometres by the prevailing winds due to their small size –- around 1 to 10 microns – smaller than the width of a human hair. Over the Sahara during summertime, intense solar heating and convection allow dust to reach altitudes of up to 5 km, from where the dust particles can take up to ten days to sediment out of the atmosphere and travel as far as the Americas, degrading Caribbean air quality.  Although Australia has its fair share of dust hotspots, they are more abundant across the global dust belt and atmospheric dust loadings are much higher in these regions. However, many of the world’s largest dust hotspots are in sparsely inhabited regions such as the Sahara, so some of the worst impacts of dust storms are less frequently experienced by humans. So what can we expect from the future in terms of dust storms? Predicting their frequency, intensity and transport requires forecasts of surface wind speeds, surface conditions such as land surface type and soil moisture, and therefore also precipitation. Accurately simulating these properties over remote, sparsely observed regions such as the Sahara even in the present day is a challenge to models due to the complex nature of the interaction between these parts of the climate system. Climate models are a valuable tool for forecasting dust storms and how they will affect us in the future. It might be expected that, in a warmer future with more drought conditions, we would experience more dust storms. However, the environmental conditions that drive dust storms must be considered in combination.  For example, one study actually predicts a decrease in African dust emissions throughout the 21st century due to a slowdown of tropical circulation and therefore surface winds, which drive the dust uplift. This scenario would result in improved air quality in West Africa.  However elsewhere in North America, dust activity is predicted to increase by the end of the century over the Southern Great Plains largely due to reduced precipitation, enhanced land surface bareness and increased surface wind speed, while the opposite trend can be expected over the northern Great Plains. Globally, the trend in dust activity projected across the next century by climate models is highly variable regionally and seasonally. For Australia, climate models predict more dust activity during the southern hemisphere autumn, driven by increases in surface wind speeds. Our ability to forecast dust both in the short and long term requires an accurate understanding of the processes which drive dust uplift. Climate models need to be able to simulate surface wind speeds, precipitation, soil bareness and soil moisture, as well as the processes which drive transport in the atmosphere and allow dust particles to be carried over long distances.  Dust storms highlight how vulnerable societies are to these extreme weather events and how difficult predicting them is under a changing climate. They’re affected by soil conditions on a microscopic scale and large-scale atmospheric circulation, and so dust storms are uniquely challenging to predict and we urgently need to find out more about them if we are to adequately prepare ourselves for the future."
"
Share this...FacebookTwitterMarch mean temperatures over Northern Europe showed an overall cooling trend and so with it a later start to spring, the data from the Japanese Meteorological Agency (JMA) show. 
By Kirye
and Pierre Gosselin
Europe saw a very mild winter – one of the mildest on record – and so people living here believe it’s warming and that every year spring is arriving earlier.
Yet mean the temperature data from the Japan Meteorological Agency (JMA) going back 2 decades and more tell us the opposite is in fact the case in northern Europe and elsewhere: March has been cooling.
First we look at the mean March temperatures at 14 stations across the United Kingdom.

Data Source: JMA
Most stations plotted above show a cooling trend for the month of March. Obviously CO2 is not the driving factor, rather likely it has to do with the North Atlantic oceanic cycles.
The story is the same across the Netherlands when we look at data going back to the time the Kyoto Protocol was adopted, in 1997.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Data source: JMA.
The northern European country of Finland shows no climate change, at least in march, over the past 30 years:

Data source: JMA. 
Moving to the Scandinavian country of Sweden, the home of teen activist Greta Thunberg, we see a trend that is more in line with United Kingdom, possibly being influenced by the North Atlantic cycles as well:

Data source: JMA. 
The six Swedish stations examined show cooling in March over the past 30 years.
Why everyone is still speaking about warming even though the planet is showing more a mixed bag remains a mystery. It probably has a lot to do with the global warming obsessed media.
Summary: Northern Europe early Spring (March) is not getting warmer, and is in fact cooling modestly, when we look at the untampered data from the JMA.
Plotted in this post were the stations for which the JMA had complete or almost complete data sets.


		jQuery(document).ready(function(){
			jQuery('#dd_1a2510940116542717cdb96031995e99').on('change', function() {
			  jQuery('#amount_1a2510940116542717cdb96031995e99').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"In part three of the BBC’s new nature series Dynasties, the protagonists, Charm and Sienna, show us how hard it is to be a successful lioness in a land filled with enemies.  Under constant threat of marauding hyenas and cub-killing male lions, the two mothers have to fight for their lives to ensure their offspring have a chance of making it to adulthood. But the episode also shows us that the biggest enemy of lions isn’t other wild predators – it’s humans. Down from as many as 200,000 lions a century ago, some experts believe that we could now have as few as 20,000 individuals remaining in the wild – and that number is likely to be falling by the day. Worryingly, the general public are mostly unaware of their precarious conservation status. We have done a bad job of showing the perilous state of these big cats. Lions face attack by humans on many fronts. Panthera, a wild cat conservation organisation, believes the most serious causes for their decline include habitat loss, humans killing them to protect their livestock, wild prey depletion, accidental snaring, poorly managed trophy hunting and the illegal wildlife trade. Since their threats are so varied, there is no single solution for protecting lions and overcoming these threats will be no mean feat. It will require locally-tailored solutions that fit each specific context. For instance, for lions that reside alongside people in areas outside national parks, research has shown that it is absolutely vital to reduce the perceived costs of lions to local people, like livestock depredation, while increasing their benefits, such as income from photographic tourism or trophy hunting. For lions inside protected areas, some experts argue that we must fence lions in to stop them causing problems with people. However, this has earned criticism from others, who believe that fences incur significant ecological and economic costs by disrupting the migration of herbivores. The issue over “to fence or not to fence” has turned into a bit of cat fight and shows the political nuances and ecological complexities of conserving such a charismatic species. In a bold attempt to reunite conservationists, Pride, the Lion Conservation Alliance, has brought together five lion NGOs to pool their efforts and share funding. It may come as no surprise that, like the species they’re fighting to conserve, they have realised the benefits of coming together and working as a team rather than competing.  Focusing on lion populations in Kenya, Mozambique, Tanzania and Zambia, their community conservation efforts empower locals to be stewards of wildlife. By turning lion poachers into guardians, their initiatives have reduced lion killing by up to 99% in some of the areas in which they work.  By building on the cultural significance of lion hunts, young warriors that would usually show their bravery by killing lions are now employed to track lions and monitor their activities. They also inform their community if lions are approaching so that farmers can guard their livestock. While TV shows such as Dynasties are helping to raise the profile of this threatened carnivore, what the lion needs now more than anything is funding. Conserving lions is an expensive business: one recent paper showed that to effectively manage the protected areas where lions currently reside would require a whopping US$0.9 billion to US$2.1 billion in additional income per year – on top of the money that is already raised.  Where this cash comes from remains a bit of a mystery. We have to go beyond financing conservation from the meagre income of photographic tourism in national parks. Solutions could involve more corporate partnerships and financially linking lion lovers in the West to Africans living with lions.  An idea from Sir David Attenborough himself argues that companies that use lions in their marketing should pay for lion conservation. What is abundantly clear is that if we want lions to have a future, we must start stumping up the cash for their conservation. Many commentators have suggested BBC’s Dynasties takes on the gripping, conflict-ridden format of storytelling that Game of Thrones perfected. If this is the case, humans would surely play the vicious and selfish King Joffrey. It is us, after all, who terrorise lions the most. But it is us, too, who have the power to guarantee their survival."
"
About two weeks ago I published this story about the loony idea that was proposed by some researcher in Europe about “cell phone radiation may be killing bees”. I pointed out that it was garbage then, as it is now. Here’s a portion of the original post I made:


There’s an article on UK’s The Independent website about a most unusual scientific theory: “Cell Phones kill bees.”

Well today in the LA Times,  it seems that UC San Francisco researchers have uncovered what they believe to be the real cause, and its not loony ideas like cell phones. Its fungus.
From the article:
A fungus that caused widespread loss of bee colonies in Europe and Asia may be playing a crucial role in the mysterious phenomenon known as Colony Collapse Disorder that is wiping out bees across the United States, UC San Francisco researchers said Wednesday.
Researchers have been struggling for months to explain the disorder, and the new findings provide the first solid evidence pointing to a potential cause.
Other researchers said Wednesday that they too had found the fungus, a single-celled parasite called Nosema ceranae, in affected hives from around the country — as well as in some hives where bees had survived. Those researchers have also found two other fungi and half a dozen viruses in the dead bees. 
The researchers caution that the results are preliminary, and data sampling represents just a fraction of hives, but they are encouraged by the findings. Hopefully they’ll be able to come up with a solution.
Yet it appears that the “Cell Phones kill bees” lunacy has caught on, since there’s a comment today in the ER’s “Tell it to the ER” that furthers that nutball idea. What a public disservice that column is.
Thanks to Lon Glazner for the tip.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6c06c18',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterIn recent decades there have been “notable cooling trends” throughout many regions of the globe according to several new studies.
A year ago NoTricksZone (NTZ) announced Greenland Has Been Cooling In Recent Years – 26 Of Its 47 Largest Glaciers Now Stable Or Gaining Ice.
Six months ago NTZ cited several scientific papers indicating The Region From 50-70°S Has Cooled Since The 1980s As North Atlantic SSTs Have Cooled 1°C Since 2004.
Three months ago we reported A Massive Cooling Of 2°C In 8 Years (2008-2016) Has Jolted Large Regions Of The North Atlantic.
A few days ago we shared a New Study Finds The Larsen Ice Shelf (Antarctic Peninsula) Has Cooled More Than 2°C Since 1991.
Now we shine the light on 3 more studies that assess “Eurasia, North America, Africa, Australia, South America, and Greenland experienced notable cooling trends” from 2002 to 2013 (Xu et al., 2020), and both West and East Antarctica have been rapidly cooling since the mid-2000s (Hrbáček and Uxa, 2020 and Fatras et al., 2020).
At some point the question may need to be asked: Just how global is recent “global warming”?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Xu et al., 2020
“Concurrent with the slowdown of global warming during 2002–2013, the wintertime land surface air temperatures over Eurasia, North America, Africa, Australia, South America, and Greenland experienced notable cooling trends. … The slowdown concurs with a negative phase of the Pacific Decadal Oscillation (PDO), indicating that PDO plays an important role in modulating the global warming signal. Not all ensemble members capture the cooling trends over the continents, suggesting additional contribution from internal atmospheric variability.”

Image Source: Xu et al., 2020
Hrbáček and Uxa, 2020
“A significant air temperature decrease started around 2000 along most of the Western AP. The cooling triggered by natural variability of cyclonic activity and increasing sea‐ice concentrations near coastlines caused MAAT trends of −0.16 to 0.05°C y−1 in the period 2006–2015. In contrast, the MAAT on JRI was increasing at a non‐significant rate of 0.10°C y−1, which corresponds to observations from other sites of the north‐eastern AP where positive, but non‐significant, trends between 0.02 and 0.08°C y−1 have been reported.10 Unlike MAAT , there was a non‐significant negative trend of −0.05°C y−1 for MAGT 5. Interestingly, the MSAT and MSGT 5 trends were positive only in autumn (MAM), at 0.30 and 0.13°C y−1, respectively, while they were negative in the other three seasons. Yet, the north‐eastern AP region exhibited a MAAT more than 1°C lower in the period 2006–2015 compared to 1996–2005, and autumn (MAM) air temperature was even about 1.5°C lower.”

Image Source: Hrbáček and Uxa, 2020
Fatras et al., 2020
“The mean annual Sea Surface Temperature (SST) variations from ECMWF ERA interim database are displayed on Fig. 4b. They present no particular trend for the 1979–2018 period, with variations contained between -1°C and +0.15°C. Nevertheless, the mean temperature between 1979 and 2000 is -0.45°C and decreases to -0.64°C during the 2000–2015 period.”

Image Source: Fatras et al., 2020
Share this...FacebookTwitter "
"
I’ve been involved in meteorology in one way or another since 1976, and while I knew of the vast number of COOP stations around the USA, I never knew that a good number of them are at sewage treatment plants until I started my surfacestations.org project. It seems to me, that given the physical makeup of these facilities, they are one of the worst possible environments to measure air temperature. But like many historical stations, they weren’t chosen with the environment in mind, but rather if there was a human being present 7 days a week whom could take the high/low temps and rainfall and write it down on an NCDC B44 form.
This week I visited a few stations in southern California, and Santa Barbara is one of those USHCN stations that is also a sewage treatment plant. Conicidentally, a few other USHCN stations that are also WWTP’s were posted by www.surfacestations.org volunteers. So I thought I’d give you the grand tour.

Above: aerial view of Santa Barbara WWTP and USHCN climate station of record

Above: Placement of Santa Barbara’s MMTS Temperature Sensor – looking NW

Here’s one from Tifton, GA taken by Joel McDade:

more pictures here
Cheraw, SC taken by L. Nettles:

more pictures here
Albany, GA from Joel McDade:

more pictures here
Zumbota, MN from Don Kostuch

more pictures here
And let’s not forget Urbana, OH, by Steve Tiemeir

more pictures here
There’s lots more, but you get the idea.
surfacestations.org volunteer Don Kostuch wrote this to me about WWTP’s recently:
“I spoke with the curator in New Hampton IA. He gave me these figures for his plant last January:
780,000 gal/day
Incoming temp 55F
Outgoing temp 43F
I calculate this heat loss is about 3 million btu/hr.
The population is about 3500 so each person releases about 1000 btu/hr at the plant on a cold day.
The effect on the sensor depends on the placement, temperature, wind, location of the tanks, etc. which I have not attempted to analyze, but it seems to be worth some careful attention.
The worst example I saw was in Winnebago, MN where the  sensor is above and in the middle of four large tanks all huddled together in about a 100
ft square. The population there is about 1500 so the heat released would be about 1.5 million btu/hr in an area of about 10000 sq.ft.”
And, as population grows in a city so would waste water volume. So it stands to reason the a temperature sensor at a WWTP would be directly sensing waste heat produced by population growth, and the amount of waste heat would grow proportionately with population.
Perhaps we should call the WWTP effect “P-UHI”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4fe67e5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

In an historic event next week, Pope Francis will make his first visit to the United States. It is expected to generate as much political interest as it will religious concerns. On Thursday, he will address a joint session of Congress, and on Friday he will speak to the United Nations General Assembly. He is widely expected to focus on climate change, a topic on which he shares much political ground with President Obama.



This will not be the first time Pope Francis has ventured into the global warming debate. The June 2015 release of his encyclical “Laudato si” marked his initial foray into the discussion. Therein, Pope Francis echoed President Obama’s tune, claiming there exists “solid scientific consensus” that human activities are causing a “disturbing warming” of the climate, which left unchecked will result in a type of planetary Armageddon manifested by escalating temperatures, melting polar ice caps, rising seas, more frequent and more severe weather, ecosystem degradation, and plant and animal extinctions, all of which he claimed will severely affect humanity.



Given that this was the pope’s stated position on global warming a mere three months ago, look for a familiar refrain to accompany his remarks in Washington and New York next week. He will likely repeat a challenge first issued in his June encyclical, which called for humanity to “recognize the need for changes of lifestyle, production and consumption, in order to combat this warming,” which he believes is “aggravated by a model of development based on the intensive use of fossil fuels.”





Contrary to what Pope Francis says, fossil fuels are good for the poor and the Earth.



 **The Consensus Isn’t**



But are the pope’s concerns over potential global warming based upon the best available science? Or are they significantly overinflated? Is the biosphere rapidly spiraling downward toward planetary Armageddon? Or is it marching forward toward biospheric rejuvenation? Is limiting fossil fuel use a policy prescription panacea? Or is it a recipe for social and economic disorder and regress?



With respect to the science, those who promulgate a fear of planetary Armageddon often conveniently fail to disclose that literally _thousands_ of scientific studies have produced findings that run counter to their view of Earth’s climatic future. As just one example, and a damning one at that, _all_ of the computer models upon which this vision is based failed to predict the current plateau in global temperature that has continued for nearly two decades now. That the Earth has not warmed significantly during this period, despite an 8 percent increase in atmospheric CO2, is a major indictment of the models’ credibility in predicting future climate, as well as the assertion that debate on this topic is “settled.”



Numerous other problems with the apocalyptic vision of our future climate have been filling the pages of peer‐​reviewed science journals for many years now, evidenced most forcefully by the work of the Nongovernmental International Panel on Climate Change, which has highlighted the results of thousands of scientific studies challenging the alarmist and model‐​based vision of the planet’s future. This large and well‐​substantiated alternative viewpoint contends that rising atmospheric CO2 emissions will have a much smaller, if not _negligible_ , impact on future climate, while generating several biospheric _benefits_.



 **Global Warming Could Be Good**



Concerning such benefits, it is a well‐​established _fact_ that atmospheric CO2 is the major building block of nearly all life, as it is used by plants in the process of photosynthesis to construct their tissues and grow. As numerous scientific studies have conclusively demonstrated, the more CO2 there is in the air, the better plants grow. They produce greater amounts of biomass, become more efficient in using water, and are better able to cope with environmental stresses such as pollution, drought, salinity, and high temperatures.



The implications of these benefits to society are enormous. One study, for example, calculated that over the 50‐​year period of 1961 to 2010, the direct monetary benefits atmospheric CO2 enrichment conferred on global crop production amounted to a staggering $3.2 trillion. Projecting this positive externality forward in time reveals it will likely bestow an additional $9.8 trillion in crop production benefits between now and 2050.



By ignoring these realities, policy prescriptions calling for a reduction in fossil fuel use are found—on this basis alone—to be ill‐​advised. Yet there are still other important reasons to reject them.



 **The World Needs More Energy, Not Less**



We live in a time when approximately half the global population experiences some sort of limitation in accessing the energy they need for the most basic of human needs, including the production of clean water, warmth, and light. One‐​third of those thus impacted are children. An even greater portion finds its ranks among the poor. How can a society turn its back on these individuals and deny them the right to increase their energy and fossil fuel use so that they can increase their living standards? It is reprehensible to even consider such an action and it is certainly morally wrong to do so. The world needs _more_ energy, not less.



Taxing or regulating CO2 emissions is an unnecessary and detrimental policy option that should be shunned. Why would any government or religious institution advocate to increase regulations and raise energy prices based on flawed computer projections of climate change that will never come to pass? Why would any government or religious institution advance policy that seeks to destroy jobs, rather than to promote them? Why would any government or religious institution want to deny increasing energy access to those in the world who are in most need of it? And why would national governments or religious institutions actually “bite the hand that feeds them”?



It is high time for our world leaders to recognize and embrace the truth. Contrary to misguided assertions, political correctness, and even government or religious edicts, carbon dioxide is _not_ a _pollutant_. Its increasing concentration only minimally affects our climate, while offering great benefits to the biosphere. Efforts to regulate and reduce CO2 emissions will economically burden society and yield little to no measurable impact on Earth’s climate.
"
"Hydraulic fracturing, or fracking, for shale gas has been a major concern for people in the UK since the first site to open, at Preese Hall, Lancashire, caused two earthquakes in 2011 that were strong enough for humans to feel. With local magnitudes (ML) of 2.3 and 1.5, these earthquakes were subsequently linked to the reactivation of a previously unknown natural geological fault.  Fracking was temporarily banned but after a seven-year hiatus it resumed in October 2018 at a new site at Preston New Road in Lancashire. Three days later the British Geological Survey began to detect earthquakes related to the site on their local seismic network.    Fracking works by fracturing underground shale rock, which creates pathways along which trapped gas in the shale can be extracted. Because earthquakes occur when rock breaks, fracking is, by design, intended to bring about the very process which results in earthquakes. However, the depth of fracking operations, about 2km underground at the Preston New Road site, mean that they are too small to be felt by humans. On the other hand, larger earthquakes with magnitudes of ML 0.5 or greater probably indicate that natural pre-existing geological faults have been reactivated, releasing stress already stored in the shale. The result can be earthquakes large enough to be felt by humans.  To reduce the likelihood of these felt earthquakes, the UK uses a regulatory traffic light system, which requires injection to be temporarily suspended if an earthquake with ML 0.5 or larger occurs. So far, six of the 36 earthquakes induced at Preston New Road have triggered this system, with operations resuming after an 18-hour monitoring period and well integrity check. The well has now produced its first natural gas.  Only the largest earthquake in the sequence, an ML 1.1 event on October 29, was felt by people, but it cannot be ruled out that more will occur during the operation.  The shale formations that are currently targeted by fracking in England are highly (naturally) faulted, and induced earthquakes used to be relatively common as a result of coal mining. The new challenge, however, is working out how stressed these faults are and how common human-felt earthquakes might be. Good seismic monitoring of initial fracking operations will provide important data to help answer this.  There also needs to be consideration given to the current traffic light system. At what magnitude above ML 0.5 should fracking sites be required to close permanently? Alternatively, should the current stop-start cycle at Preston New Road be allowed to repeat indefinitely providing the well maintains integrity and earthquakes are not damaging or causing nuisance to local residents?  Furthermore, is a traffic light system based on magnitude really suitable? Magnitude characterises the size of an earthquake but is not a measure of ground shaking, which is the effect felt by humans at the surface. An ML 0.8 earthquake on October 26 produced far less ground shaking than a lorry typically produces as it drives past. The ground shaking was also well below the regulatory limit for quarrying operations. A traffic light system based on the intensity of ground shaking may therefore be more appropriate for fracking. At the end of the day it is those who may be affected by the earthquakes whose opinion should carry the most weight. It remains to be seen if local residents are willing to tolerate induced earthquakes in exchange for local economic benefits. Co-operation of stakeholders (including local residents) and research by independent organisations such as the Researching Fracking (ReFINE) consortium and the British Geological Survey will be required to tackle these questions. The answers will play a key role in determining whether UK shale gas has a future – and how it may contribute to a sustainable, affordable, and secure energy future for the nation."
"Three bushfire survivors have joined environment group Friends of the Earth in a claim against ANZ, accusing it of financing the climate crisis by funding fossil fuel projects. The case, lodged under international guidelines agreed by members of the Organisation for Economic Co-operation and Development (OECD), demands the bank disclose its greenhouse gas emissions, including “scope three” emissions resulting from its business lending and investment portfolio, and set ambitious targets that align with the Paris climate agreement. The claim was inspired by a successful complaint against ING bank in the Netherlands by Friends of the Earth, Oxfam and Greenpeace. Mediation following that complaint led to ING committing to measure and publish its indirect emissions, reduce its thermal coal exposure to near zero by 2025 and make its portfolio consistent with the Paris goal of keeping global heating well below 2C above pre-industrial levels. The complainants in the ANZ claim include Jack Egan, who was approached by Friends of the Earth to join the action after his home near Batemans Bay, on the New South Wales coast, was destroyed on New Year’s Eve. Egan said there was a clear link between ANZ and other institution’s ongoing support of fossil fuels and the extreme hot and dry conditions that exacerbated the fire that left him homeless. “We are not seeking damages or compensation from ANZ, I just want them to stop fuelling dangerous climate change,” he said. Friends of the Earth announced the action at a protest outside ANZ headquarters in Melbourne’s Docklands. It lodged the claim with the federal government’s OECD national contact point, a section of the federal treasury responsible for hearing complaints of corporate wrongdoing under the OECD guidelines for multinational enterprises. The national contact point’s initial role is to attempt to broker a mediation between the parties. If agreement cannot be reached, it can make recommendations, but cannot force parties to take action. ANZ declined to comment on Thursday. The environment group alleges ANZ’s breaches of the OECD guidelines include misleading consumers by claiming to support the Paris agreement targets while continuing to invest in projects that undermine the meeting of those targets. Emila Nazari, a legal officer with Friends of the Earth, said the bank had increased investment in coal 34% over the past two years as it lent $8.8bn to the fossil fuel sector. She said the bank was Australia’s largest financier of fossil fuel industries, and continued to invest billions of dollars in “climate wrecking projects” while bushfires raged across Australia. “It is illegal for someone to light a bushfire, and we believe it is illegal for companies to finance the burning of our common home. This case is one of many to come against climate criminals,” Nazari said. The other names attached to the action are Joanna Dodds, a Bega Valley Shire councillor and member of the group Bushfire Survivors for Climate Action, and Patrick Simons, a Friends of the Earth renewable energy campaigner whose family lost their home in NSW."
"Often when the topic of glaciers and climate change is discussed, focus shifts to those in Greenland and Antarctica. But there are glaciers elsewhere too, such as in the Himalayas, which play a vital part in supplying water to people who live downstream. Now, our research has found that these glaciers may react more sensitively to predicted future climate change than previously thought, which could lead to them melting at a faster rate. In 2017 and 2018, our EverDrill research team travelled to Nepal, to measure ice temperatures (using a converted pressure washer) on the Khumbu Glacier. Khumbu, whose ice is sourced in the Western Cwm of Mount Everest, flows down the flanks of the mountain from around 7,000 to 4,900 metres above sea level. Along with many other glaciers in the Everest region and across High Mountain Asia, the meltwater from Khumbu contributes to the water resources of huge populations in the mountain foothills.  We spent two field seasons (six to eight weeks each) drilling boreholes using a jet of hot, pressurised water to incise into the ice. In total we drilled 27 boreholes, ranging between 1-192m deep, across five sites. These boreholes are the first, deepest, and most spatially extensive achieved to date in the Himalaya using hot-water drilling. Once the boreholes were drilled, we measured ice temperatures by installing strings of pre-built thermistor sensors linked to data-loggers located at the surface. We left them for six months and collected the data on a return trip in November 2017.  The main finding from this research was that the ice was warmer than we expected, with the coldest ice measuring –3.3°C. As the ice is formed on the flanks of Mount Everest, where the mean annual air temperature is –13°C at 7,000 metres elevation, we might have expected the ice to be at this temperature. Our borehole data reveal that this was not the case. Not only was the ice not this cold in our boreholes, but ice temperatures also increased towards the glacier terminus. Why does it matter that we found warmer ice temperatures than we expected? Glaciologists acknowledge two thermal types of ice: “Cold” ice and “warm” ice. Cold ice is below the melting-point temperature, so when additional heat is applied (from the sun or warmer air temperatures), the ice simply becomes less cold (going from –15°C to –14°C, for example). Warm ice, alternatively, is at the melting-point temperature, so any heat input melts the ice to become water. In short, this means that Khumbu will respond more sensitively to any future additional heat inputs, such as warming air temperatures. It has been shown that air temperature increases are amplified at high elevations. For example, a global temperature rise of +1.5°C has been predicted to result in a +2.1°C warming across High Mountain Asia and a significant loss of ice mass across the region. As these glaciers melt and recede, their contribution to water resources will initially increase. However, as the volume of ice mass remaining decreases, this contribution will steadily decline. While the timescale over which this might occur is unknown, predictions suggest that “peak water” may be reached as soon as the middle of this century. Our finding of warm ice within Khumbu supports the predicted sensitivity of such glaciers to warming air temperatures. But it’s not all bad news for the region’s glaciers. First, we don’t know if the temperatures we measured on Khumbu are representative of all glaciers in the area. More measurements are needed on other glaciers to determine this. Second, Khumbu and many other Himalayan glaciers are debris-covered glaciers, which contain a surface layer of rocks and boulders that typically increases in thickness towards the terminus, up to several metres depth. This debris layer complicates the amount of ice surface melt that is produced: where the layer is thick, it acts like a blanket and insulates the glacier from warmer air temperatures, reducing melt rates. However, this insulation of the lower glacier also results in the location of maximum melt shifting further up the glacier, to where the debris cover is thinner. In this area, the glacier melts by surface lowering, which in an extreme future scenario could lead to the detachment of the lower glacier, forming a new terminus at this higher elevation. While the lower, detached ice would become stagnant, it would still be protected from instant melting by its debris blanket. The new terminus above this would not, and melt rates could increase. Yet, our deepest borehole in this area of surface lowering was 192 metres, and did not reach the bed so the ice thicknesses remains unknown – but it could be that there is plenty of ice left in Khumbu to stop this happening. These ice temperatures will now be fed into an ice flow model to better predict how the Khumbu Glacier will respond to climate warming and contribute to river discharges in the future. Meanwhile, we are still collecting temperature data to analyse and better understand how the highest glaciers in the world will be affected by climate change."
"
I’m sitting in a presentation by William R. Cotton, of Colorado State University where he’s talking about the effect of Urban Heat Islands (UHI) on precipitation. He’s making a convincing pitch showing how the UHI factors into downwind delayed convection initiated by the city UHI along with a significant contribution of aerosols and ice nuclei that seed the precipitation. He’s been able to demonstrate that in St. Louis, downwind from the city (typically NE to SE based on prevailing winds) there are increased precipitation from thunderstorms by as much as 160% during the life cycle of the storm.
Yesterday, I saw a very similar study done by Indiana State Climatologist, Dev Nyogi, where he studied Indianapolis, IN and came to similar conclusions. The midwestern cities make good case studies because they are singular islands of urbanization (as opposed to sprawling cities like Los Angeles and Chicago) that essentially become point heat sources at the mesoscale level.
The summary is this: Urban and-use has the biggest control on locations and amounts of precipitation and that condensation nuclei added by the city also have a significant effect. Heat and particles contributed by the city can make bigger, more precipitating thunderstorms.
Of course studies by Parker tells us there is no significant UHI effect, so this presents yet another challenge to what is looking ore and more like a flawed study by Parker.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4780e8a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"
Share this...FacebookTwitterRoland Tichy’s site here reports that although car traffic in the German city of Stuttgart has decreased significantly due to COVID-19, the drop in NO2 content has only been “slight”.
 So it cannot be the evil diesel engine cars that are “choking” our cities.

Image: NASA here (public domain) 
“The Corona crisis is bringing it to light: car traffic has decreased significantly, but the air quality in city centers has hardly changed,” Tichy comments.
Environmental claim exposed as false
Recall that a major reduction of diesel engines was supposed to improve air quality. After all, experts at the Baden-Württemberg State Institute for the Environment (LUBW), for example, have attributed a large 80 percent share of air pollutants especially to diesel vehicles – so they have to be banned soon.
Yet Tichy notes: “If this is true, the ‘shutdown’ would have to have a drastic effect. But it does not.”
It turns out that the involuntary “corona experiment” with its widespread stop of car traffic has exposed bare the false claim made by environmental activists: Diesel cars are responsible for polluting the air of German cities.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Diesel car bans “pointless”
According to Tichy, the “Corona Experiment” exposes just how pointless driving bans issued by the green transport ministers can be. “They obviously have no effect on the NO2 concentrations in the air.”
“The measured values, for example, at the Am Neckartor station in the Stuttgart city center were already below the limit value of 40 µg/m3 in February and March, ” writes Tichy. “At that time, traffic was still flowing and ‘shutdown’ had not yet been announced.”
Other larger factors
Tichy adds that engineer and measurement expert Martin Schraag accuses proponents of car bans of data “manipulation” and reminds that today’s “newer vehicles and those retrofitted with software updates hardly emit any exhaust gases. This should also have been reflected in the results.”
Schraag notes that NO2 values fluctuate strongly and depend heavily on Stuttgart’s weather conditions and wintertime  heating can be the cause.
“The weather, if you look at the data, has a decisive influence,” Tichy comments. “The experts of the LUBW environment office obviously did not care about these influences and officially know nothing about them. They still assume that traffic accounts for 80 percent of air pollutants.”
At a loss for words
Now they are facing difficulties in explaining the situation. As Schraag suspects, the 80 percent figure cannot be correct if there are significantly fewer cars on the road and yet the values have not changed.
The Bavarian State Office for the Environment also confirmed that the air pollutants in the city of Würzburg – hometown of former NBA superstar Dirk Nowitzski – had hardly changed either although traffic has decreased significantly.


		jQuery(document).ready(function(){
			jQuery('#dd_b0ff816632eacdd59517875595db342e').on('change', function() {
			  jQuery('#amount_b0ff816632eacdd59517875595db342e').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
nan
"The outer layer of the Earth, the solid crust we walk on, is made up of broken pieces, much like the shell of a broken egg. These pieces, the tectontic plates, move around the planet at speeds of a few centimetres per year. Every so often they come together and combine into a supercontinent, which remains for a few hundred million years before breaking up. The plates then disperse or scatter and move away from each other, until they eventually – after another 400-600 million years – come back together again.  The last supercontinent, Pangea, formed around 310 million years ago, and started breaking up around 180 million years ago. It has been suggested that the next supercontinent will form in 200-250 million years, so we are currently about halfway through the scattered phase of the current supercontinent cycle. The question is: how will the next supercontinent form, and why? There are four fundamental scenarios for the formation of the next supercontinent: Novopangea, Pangea Ultima, Aurica and Amasia. How each forms depends on different scenarios but ultimately are linked to how Pangea separated, and how the world’s continents are still moving today.  The breakup of Pangea led to the formation of the Atlantic ocean, which is still opening and getting wider today. Consequently, the Pacific ocean is closing and getting narrower. The Pacific is home to a ring of subduction zones along its edges (the “ring of fire”), where ocean floor is brought down, or subducted, under continental plates and into the Earth’s interior. There, the old ocean floor is recycled and can go into volcanic plumes. The Atlantic, by contrast, has a large ocean ridge producing new ocean plate, but is only home to two subduction zones: the Lesser Antilles Arc in the Caribbean and the Scotia Arc between South America and Antarctica.  If we assume that present day conditions persist, so that the Atlantic continues to open and the Pacific keeps closing, we have a scenario where the next supercontinent forms in the antipodes of Pangea. The Americas would collide with the northward drifting Antarctica, and then into the already collided Africa-Eurasia. The supercontinent that would then form has been named Novopangea, or Novopangaea.  The Atlantic opening may, however, slow down and actually start closing in the future. The two small arcs of subduction in the Atlantic could potentially spread all along the east coasts of the Americas, leading to a reforming of Pangea as the Americas, Europe and Africa are brought back together into a supercontinent called Pangea Ultima. This new supercontinent would be surrounded by a super Pacific Ocean. However, if the Atlantic was to develop new subduction zones – something that may already be happening – both the Pacific and Atlantic oceans may be fated to close. This means that a a new ocean basin would have to form to replace them.  In this scenario the Pan-Asian rift currently cutting through Asia from west of India up to the Arctic opens to form the new ocean. The result is the formation of the supercontinent Aurica. Because of Australia’s current northwards drift it would be at the centre of the new continent as East Asia and the Americas close the Pacific from either side. The European and African plates would then rejoin the Americas as the Atlantic closes. The fourth scenario predicts a completely different fate for future Earth. Several of the tectonic plates are currently moving north, including both Africa and Australia. This drift is believed to be driven by anomalies left by Pangea, deep in the Earth’s interior, in the part called the mantle. Because of this northern drift, one can envisage a scenario where the continents, except Antarctica, keep drifting north. This means that they would eventually gather around the North Pole in a supercontinent called Amasia. In this scenario, both the Atlantic and the Pacific would mostly remain open. Of these four scenarios we believe that Novopangea is the most likely. It is a logical progression of present day continental plate drift directions, while the other three assume that another process comes into play. There would need to be new Atlantic subduction zones for Aurica, the reversal of the Atlantic opening for Pangea Ultima, or anomalies in the Earth’s interior left by Pangea for Amasia.  Investigating the Earth’s tectonic future forces us to push the boundaries of our knowledge, and to think about the processes that shape our planet over long time scales. It also leads us to think about the Earth system as a whole, and raises a series of other questions – what will the climate of the next supercontinent be? How will the ocean circulation adjust? How will life evolve and adapt? These are the kind of questions that push the boundaries of science further because they push the boundaries of our imagination."
"

For those of us that hate having Winnie the pig presented to us as part of our local weather report, I’d like to offer this solution that cuts all of us annoying weather middlemen and weather forecasting pigs right out of the picture and give you total control over your weather report.
Its called the ViziFrame – now you can program your own local weather channel at home, or at the office, or at the marina, or the golf course, your school, a truck stop, gas station, or wherever there may be an interest in weather to make a go/no go decision. You can view it on your own terms, and unlike the Weather Channel, you don’t have to wait a half hour to get the info you need.
And the graphics, look at good as anything on TV. For those of you with a profit in mind, it can have advertising and other information too. Its way cool, inexpensive, and trouble free. It works with any TV, big screen, or computer monitor. It updates its information via WiFi or a regular Internet cabled connection to a home DSL/cable router or T1 router.
The Chico Chamber of Commerce is going to put a bunch of them (the premium model that also does video and audio clips with touch screen interactivity) around town at hotels, restaurants, city hall and other public places that cater to visitors. Local artist Gregg Payne worked out a cool design for the front fascia that looks like the Hooker Oak tree…a concept view is below along with the current weather page and forecast…which are live content links soon to be on the Chamber of Commerce web page. Kris Koenig and Anita Berkow of Interstellar Studios are doing the interactive kiosk presentation for it. Look for these around town soon!



Note the current conditions page – it has solar irradiance on it – I figured if we were going to become a solar powered city, getting a real-time indicator that people can use to calculate solar panel efficiency would be a good first step, so I invested in the equipment to do that. I’ll have an entire blog entry on this service later.
The graphics are made in my rendering system as weather data arrives at my office here in Chico, there’s actually about a hundred plus graphics that are available.
But you can get a weather channel for your home or business too. See www.viziframe.com I’ve sold several of these already and people at home just connect them up to a spare video port on their big screen TV, and when they want weather, just switch to it. No waiting, no pigs, no hassle.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea7156f18',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterA new study finds rising CO2 concentrations (and warming) have driven the rapid increase in Earth’s photosynthesis processes, or greening. CO2-induced planetary greening leads to an enormous expansion of Earth’s carbon sink. By 2100 this greening-sink effect will offset 17 years of equivalent human CO2 emissions. This easily supersedes the effect of Paris Agreement CO2 mitigation policies.
In a break from the deflating global news of viral infections and rising death rates, a groundbreaking new study (Haverd et al., 2020) affirms the “beneficial role of the land carbon sink in modulating future excess anthropogneic CO2 consistent with the target of the Paris Agreement” via the fertilization effect of rising CO2.
There has been a 30% rise in global greening since 1900. CO2 fertilization is the “dominant driver” of these greening trends, with an additional positive contribution from climate warming.
When CO2 levels double (to 560 ppm), this CO2-fertilization-greening effect is expected to increase to 47%.
Growth in the land’s carbon sink – absorbing excess CO2 emissions – will reach 174 PgC by the end of the century.
This is the equivalent of eliminating 17 full years of human CO2 emissions.

Image Source: Haverd et al., 2020


		jQuery(document).ready(function(){
			jQuery('#dd_fba188469f13af433635491d7989d885').on('change', function() {
			  jQuery('#amount_fba188469f13af433635491d7989d885').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
This post is an outgrowth of comments I made on Commission Impossible on the new proposed strengthened tree ordinance.
I like trees, and I recently planted four, but at the same time I’ve had to remove a couple of trees from my home and business property. In the latter case, the City did the work because they agreed with me that the tree was unsafe and posed a public hazard.
This tree ordinance thing is taking on overtones of the abortion battle, except that the roles seem to be reversed, with the “right to life” being on the left. Lately, it seems that meadowfoam, garter snake habitat (see Sundays letters to the editor) and beetle habitat Elderberry bushes are more important than the rights and lives of people.
Case in point – how many people have died at the Highways 70/149/99 interchanges in the 10+ years that environmentalists have placed roadblocks in front of that project? I remember one little boy, about two years ago, who died when the car he and his mom were riding in was broadsided by a car on 70 as she turned onto 149. If the road had been improved on schedule, that never would have happened.
Was that worth 10 years of delay to protect some Meadowfoam and beavers? I think not. Meadowfoam is being grown in quantities at reserves near Vina and commercially in Oregon, and the Limnanthes Flococcus Californica aka Butte County Meadowfoam can just as easily be grown with it. Beavers relocate with ease too. Anybody who tells you otherwise is just pushing an agenda.
Now we have the City saying there’s a delay in authorizing a bid to fix drainage problems for a man made stormwater retention basin near south Chico street Paseo Campaneros that becomes a West Nile hotspot. Two people have died on that street from West Nile in the past year…yet the “garter snake habitat” aka man-made retention basin gets hands-off priority according to what the city said recently.
It’s lunacy and its morally wrong. Public health, be it an accident prone intersection or a festering man-made mosquito pool should always trump protecting bugs, plants, snakes, and the occasional beaver. If you think these things are more important than the health of the community, then you have your social priorities reversed.
Environmentalists digging in their heels on this only hurts their cause, because it makes them look unreasonable, and maybe they are. But most people I know, on either side of the political spectrum actually want to protect the environment, me included, but they want to protect their children and grandparents more.
Making them choose through obstruction is a no-win polarizating situation. We CAN have it both ways.
In the case of trees, the folks pushing this strengthened law act as if the tree, once cut down, could never be replaced. We’re not talking giant Redwoods here…more like Dogwoods and Pines, available at Home Depot.
Compromise folks…compromise.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6f63713',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Not heard of the “Extinction Rebellion” before? Then you heard it here first. Because soon, everyone is going to have heard of it. The Extinction Rebellion is a non-violent direct action movement challenging inaction over dangerous climate change and the mass extinction of species which, ultimately, threatens our own species. Saturday November 17 2018 is “Rebellion Day” – when people opposed to what they see as a government of “climate criminals” aim to gather together enough protesters to close down parts of the capital – by shutting down fossil-powered road traffic at key pinch-points in London. I’m a Reader in Philosophy at the University of East Anglia and I have thrown myself headfirst into this movement. Our long-term aim is to create a situation where the government can no longer ignore the determination of an increasingly large number of people to shift the world from what appears to be a direct course towards climate calamity. Who knows, the government could even end up having to negotiate with the rebels. As someone who is both a veteran of non-violent direct actions over the years and an academic seeking to make sense of these campaigns, I’ve been thinking quite a lot about what’s old and what’s new about the Extinction Rebellion. Here are my conclusions so far. The Extinction Rebellion is rooted in longstanding traditions exemplified by the radical nuclear disarmament movement. The founders of the Extinction Rebellion have thought carefully about past precedents, and about what works and what doesn’t. They’ve noted for instance that you don’t necessarily need active involvement from more than a tiny percentage of the population to win radical change, provided that you have a righteous cause that can elicit tacit backing from a much larger percentage. The Extinction Rebellion is also quite different from its predecessors. True, the disarmament movement was about our very existence, but nuclear devastation was – and still is – only a risk. Extinction Rebellion’s aim is to prevent a devastation of our world that will come – and quite soon, unless we manage to do something unprecedented that will radically change our direction. Climate activists often compare their struggle to victories from the past. But in my view comparisons which are often made – to Indian independence, the civil rights movement or the campaign for universal suffrage, for example – are over-optimistic, even fatuous. These historical movements were most often about oppressed classes of people rising up and empowering themselves, gaining access to what the privileged already had.  The Extinction Rebellion challenges oligarchy and neoliberal capitalism for their rank excess and the political class for its deep lack of seriousness. But the changes that will be needed to arrest the collapse of our climate and biodiversity are now so huge that this movement is concerned with changing our whole way of life. Changing our diet significantly. Changing our transport systems drastically. Changing the way our economies work to radically relocalise them. The list goes on. This runs up against powerful vested interests – but also places considerable demands upon ordinary citizens, especially in “developed” countries such as the UK. It is therefore a much harder ask. This means that the chances of the Extinction Rebellion succeeding are relatively slim. But this doesn’t prove it’s a mistaken enterprise – on the contrary, it looks like our last chance. This all leads into why I sat in the road blocking the entrance to Parliament Square on October 31, when the Extinction Rebellion was launched – and why I will be “manning the barricades” again on November 17. As a Quaker, I cherish the opening words of the famous Shaker hymn: Tis the gift to be simple. What does it mean to live simply at this moment in history? It means to do everything necessary so that others – most importantly our children (and their children) – can simply live. It isn’t enough to live a life of voluntary simplicity. One needs also to take peaceful direct action to seek to stop the mega-machine of growth-obsessed corporate capitalism that is destroying our common future. That’s why it seems plain to me that we need peaceful rebellion now, so that we and countless other species don’t face devastation or indeed extinction.  The next line of that Shaker hymn goes: “Tis the gift to be free.” In our times, to be free means to not be bound by laws that are consigning our children to purgatory or worse. If one cares properly for one’s children, that must entail caring for their children, too. You don’t really care for your children if you damn their children. And that logic multiplies into the future indefinitely – we aren’t caring adequately for any generation if the generation to follow it is doomed. As mammals whose primary calling is to care for our kids, it is therefore logical that an outright existential threat to their future, and to that of their children, must be resisted and rebelled against, no matter what the pitifully inadequate laws of our land say. I’ve felt called upon to engage in conscientious civil disobedience before, at Faslane and Aldermaston against nuclear weapons and with EarthFirst in defence of the redwood forests threatened with destruction in the Pacific Northwest of the USA.  But the Extinction Rebellion seems to me the most compelling cause of them all. Unless we manage to do the near impossible, then after a period of a few decades at most there won’t be any other causes to engage with. It really now is as stark and as dark as that. If you too feel the call, then I think you now know what to do. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"

The picture above is of the official USHCN climate station of record in Quitman, GA and comes to me via www.surfacestations.org volunteer Joel McDade.
It is located at a residence, the observer has consented to having this NOAA weather equipment at his home.
Besides the usual problematic close-by parking of vehicles that we’ve seen before, and buildings less than 100 feet from the temperature sensor, we have a new issue to contend with: inoperable vehicles and abandoned appliances near the temperature sensor. Such big chunks of metal have thermal retention, which means that heat is retained past sunset and re-radiated near the sensor. This may bias overnight lows.
I thought the old washing machine was a nice touch though. It illustrates how little quality control of the temperature measuring environment is being done with the US Historical Climatological Network.
Additional pictures of the site are available at the surfacestations.org online database.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5b237d1',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Wind turbines are, it appears, everywhere. Even if you can’t see some on the horizon on your way into work every day, it is hard to miss the continual news coverage of new developments. Clearly, efforts to move away from fossil fuels are – at least in part – working, and from the perspective of combating climate change, this must surely be a good thing. However, much of the news coverage of turbines highlights negatives such as a perceived degradation of the landscape, or their impacts on wildlife. There is good cause for concern in this regard, particularly with respect to wildlife.  Reams of published scientific papers show that birds and bats can be killed (sometimes in relatively large numbers) by colliding with the spinning blades. Clearly, where turbines are poorly placed or where rare or vulnerable species are affected, this is a problem. Images of dead birds of prey or rare vagrant birds under wind turbines are easily turned into emotive and sensational news stories, and are terrible PR for the wind industry. At first glance, a new study published in Nature Ecology & Evolution appears to add to the evidence of widespread lethal effects. The authors, a team based at the Indian Institute of Science and led by Maria Thaker, make the attention-grabbing observation that the effects of wind turbines are “akin to adding an apex predator to natural communities”. Apex predators are animals right at the top of their food chains, like killer whales or tigers. Adding an “apex predator” surely means that these man-made metal mincing-machines kill off birds and bats in large numbers, right?  While it certainly is true that collision with turbines can cause direct mortality, what the new research actually shows is far more interesting and complex than this. The result of many years of data collection, the work shows that through the (direct or indirect) effects on birds of prey, wind turbines can have broader and more subtle effects on the wider ecosystem – including on some unexpected species. Specifically, they looked at a lizard that lives only in the Western Ghats mountains of India, the superb fan-throated lizard (Sarada superba). They found that the number of lizards was considerably higher in areas with turbines installed compared to otherwise similar areas without turbines.  In areas with turbines, they also found fewer birds of prey such as buzzards and kites (the lizards’ main predators), and a lower frequency of attacks on lizards. Putting the two together, the authors propose that the presence of turbines is associated with lower predator activity, and thus higher prey numbers. In some ways, it may indeed seem as if wind turbines act similarly to the introduction of an “apex predator” into the food chain: by reducing the number and activity of intermediate predators such as birds of prey, predation pressure on smaller animals may be reduced. However, it is of course important to stress that almost all biological predators are, in the end, limited by the availability of prey. By contrast, turbines are not limited in this way and will continue to be present regardless of whether their “prey” goes locally extinct.  Perhaps more important is the non-lethal effects on the wider ecosystem that is highlighted by Thaker and colleagues. By reducing predation or reducing the activity of predators, the physiology, behaviour and population density of prey populations can be changed in unpredictable and subtle ways. What this work really highlights is that wind farms can have effects that are indirect or not immediately visible. This poses a huge challenge for impact assessments and survey work. Perhaps understandably, many assessments of wind turbines focus on vulnerable species, which tend to be the birds and bats most at risk of direct collision. Similarly, where carried out, post-construction monitoring often focuses on collision casualties. These are things that can be directly measured or counted, over clearly defined periods.  By contrast, assessing the longer-term effects on indirectly impacted species such as lizards takes a lot more time and money. Unfortunately, this luxury is typically not available for commercial impact assessment studies. What is more, it is likely to be significantly harder to convince regulators that impact assessments should be broadened to consider wider ecosystems, particularly where these cannot be immediately seen or counted as dead animals. At least in the UK, there is some evidence that consideration of such “synergistic” effects of developments is now increasingly expected in impact assessments. While this is encouraging, more work along the lines of study by Thaker and colleagues is needed to build up a better understanding of the extent and importance of similar effects in different areas. From a pragmatic perspective, it may well be that in the future impact assessments will move away from focusing on direct collision mortality and towards more targeted and in-depth assessments of “downstream” ecological impacts."
"
Share this...FacebookTwitterClouds regulate Earth’s climate. New studies suggest uncertainty in clouds’ surface radiative effects reach 17.4 W/m² per year (±8.7 W/m²/year). Total CO2 climate forcing is said to be just 0.02 W/m² per year. The difference in these magnitudes preclude detection of a CO2 signal in climate forcing.
According to Feldman et al. (2015), a 2 ppm increase in CO2 per year (22 ppm over the 11 years from 2000-2010) results in a surface radiative forcing influence of 0.02 W/m², or 0.2 W/m² per decade. This is said to be just “ten per cent of the trend in downwelling longwave radiation” when clouds and water vapor are considered.
In contrast, the influence of clouds in total longwave forcing is substantially larger, with radiative forcing trends reaching ±4 W/m² per decade.
From 1978 to 2010, the total longwave anomalies reached amplitudes of about ±2 W/m² per year (Loeb et al., 2012, below image). These anomalies dwarf the 0.02 W/m² per year radiative influence from CO2, and thus factors other than CO2 must be driving the variability.
Further, the overall trend in longwave or greenhouse effect forcing appears to have been flat during this 30-year period. CO2’s 0.2 W/m² per decade contribution may have had no net impact on the trend.

Image Source: Loeb et al., 2012
Even if a CO2 forcing signal was detectable in an overall trend over the last few decades, it would be lost amid the uncertainty and noise of the radiative effects of clouds.
According to L’Ecuyer et al., 2019, the global annual net cloud radiative effect (CRE) at the Earth’s surface is estimated to be -24.8 ±8.7 W/m². In other words, when cloud cover increases, surface temperatures cool because the net shortwave effects of cloud (-51 W/m²) exceed the net longwave effects of cloud (+26.3 W/m²). The uncertainty value associated with this overall surface forcing estimate, ±8.7 W/m², has a range of 17.4 W/m².
If CO2’s net radiative effect in surface forcing is 0.02 W/m² per year and the uncertainty the radiative effects of clouds is ±8.7 W/m² per year, this means that uncertainty is 870 larger than the CO2 influence.
A CO2 forcing signal is therefore not detectable in the Earth’s energy balance.

Image Source: L’Ecuyer et al., 2019


		jQuery(document).ready(function(){
			jQuery('#dd_717fa080377e4740ab19829655e23af7').on('change', function() {
			  jQuery('#amount_717fa080377e4740ab19829655e23af7').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Every year massive amounts of valuable resources are deemed “waste” and consigned to landfill. Take the UK – around 540 million tonnes of products and materials enter the country annually, but only 117 million tonnes are recycled.  However if we truly thought that waste equals value, and ensured that resources are kept in the economy for longer and thus reducing the use of raw materials, we could make serious steps towards diverting all this waste from landfill. This is what’s known as the circular economy. For the circular economy to thrive, we must be serious about reducing waste. And yet we’re still locked into an “iron cage of consumerism” – shifting to a future in which we’re more efficient with materials will require significant changes in the way we consume things and in the way businesses are organised.  To make this shift possible, we must recognise that a lot of customers simply don’t consider sustainability the main selling point of a product. We must be realistic, and businesses have to target what consumers really want. Recycling has thus far been the main focus of waste production, but this is far from ideal as it takes a great deal of energy to reprocess and remanufacture materials, as well as transport them. We need to focus on making new stuff last longer. In short, we need to do away with the “consumer culture”. To do this, businesses need to think differently about the ways in which products are provided to consumers. Providing better information to consumers about durability of products would be a good start. Giving accurate lifetime information on product labels could keep them in use for a longer time.  The average consumer expects a washing machine to last six years before it needs to be replaced, and Which? magazine cites quality and reliability as strong buying criteria.  So why don’t people actually keep their washing machines for that long? It’s because they’re confused by the labelling. Consumers tend to assess the expected lifetime of large household appliances through the manufacturer’s warranty. However most just give between one or two years warranty, making it difficult for the consumer to really assess how long it would last.  Rectifying this would mean businesses and their brands become more trusted, enabling them to build better and more prolonged relationships with consumers. When people aren’t switching brands all the time, products will be considered less disposable. Leasing isn’t a new concept, but it could pay dividends for the environment – it’s one of the business models which could reduce the use of materials in the longer term. Why can’t household appliances like washing machines be rented rather than bought? They could be rented from manufacturers. As manufacturers would be responsible for repair and replacement rather than consumers, this should help to remove any commercial benefit to be gained from making products short-lived. More leasing would thus significantly cut the use of materials. Appliances could be re-used from one consumer to another while all repairs would be undertaken by the manufacturer, rather than machines being scrapped when they break down. As well as providing added revenues for manufacturers that would spread over time, this would trigger a change in the design of washing machines which would enhance the focus on longevity. It is true that consumers already have well-established buying habits, and that firms would encounter significant costs when setting up new business models. But the benefits it seems could out-weigh the immediate barriers.  There would be major cost savings on manufacturing materials, something which may become very important in the future. The more scarce materials become, the more expensive they will be – a real pressure point for the industry of tomorrow. Re-use and repair services and leasing models will inevitably start to become more appealing to businesses which are looking at ways to turn a profit in the face of increasing costs. In the past few years, the sharing economy or collaborative consumption has demonstrated new ways of distributing goods and services to consumers. As goods are shared instead of owned, the sharing economy represents an alternative to our consumerist society. Take for example Bla Bla Car, where people can share a ride, or Airbnb where travellers can rent a spare room in someones home, as an alternative accommodation. This peer-to-peer business models had prove to be successful. However, there still some barriers as they depend on trust between people. Similar business models could be applied to other products such as large household appliances. A laundrette is the perfect example. In Scandinavian countries, famed for their sustainability, it is very common to find laundrettes in private apartment blocks. However, in the UK and many other countries laundrettes are often perceived as unhygenic and people can be snobby towards them. Such attitudes aren’t helpful. We should reinvent the laundrette as a key part of the shift towards a more sustainable, sharing economy."
"There has been a lot of debate recently on the extent of the global fishing footprint. A recent paper claimed that fishing affects 55% of the world’s oceans. Given that many people in the developing world rely on fish as their main source of protein, and the increasing preference for luxury fish products in countries such as China, such statistics might seem plausible. To calculate the 55% figure, the researchers relied on the automatic identification system (AIS). Primarily intended for safety purposes, AIS combines radio and satellite monitoring with other electronic data such as speed, heading and destination port, to track, monitor and even predict vessel activity. All vessels over a certain size must have an AIS transceiver, so this widespread monitoring produced the huge amounts of data that allowed the researchers to estimate the global fishing footprint. However, when determining what proportion of the ocean is being fished, the scale at which the fishing activity is mapped makes a significant difference to the accuracy of the overall result. Using higher resolution data, with grid squares of between one and 3km², rather than 1,000km² for example, produces a footprint which differs by a factor of more than ten. We need to manage the impact of human activity on ecosystems, but doing so demands a more accurate understanding of fishing. In a similar way to other food production systems, fisheries can have a wide range of effects on the ecosystems and species that they interact with. Wild capture fisheries are also vulnerable to over-fishing unless they are well managed and regulated. For this management to be effective, we need to know how much fishing is taking place, where it is happening, and the exact type of fishing that it is.   Such a high level of accuracy is especially important when looking at the impact of bottom trawling. Many forms of fishing have a wide impact on the ecosystem. But no fishing activity exemplifies this better than bottom trawling, which sees large, heavy nets being towed across the seabed. It is associated with the removal of species from the seafloor and temporary or longer-lasting modification of habitats. This has become a particularly emotive issue with campaigns seeking to outlaw its use.   However, bottom trawl fishing is a key source of food, accounting for 25% of global landings. With such high demands, we can’t simply stop the practice dead in its tracks. The impacts can be managed, but first we need to understand where bottom trawling occurs and how often, so it can be done in a sustainable way.  For our newly published research, we looked into the true extent of bottom trawling around the world. We used high resolution vessel monitoring data to reconstruct fishing footprints for 24 regions of the sea of less than 9km² each. We found that, on average, only 14% of this area was affected by bottom trawling. There were, however, major regional differences. For example, trawling affects less than 10% of the Australian and New Zealand seabed, compared to over 50% in some European seas.  Funded in part by the Marine Stewardship Council, this research will facilitate the implementation of sustainable fishing practices around the world. It also demonstrates that when fisheries are well-managed and sustainably fished, the associated impacts on the seabed are reduced, compared to other less well managed fisheries. In other words, if you manage the fishing of the target species appropriately you’ll probably also succeed in reducing other effects of fishing activity.  Some habitats are also highly resilient to the effects of trawling, whereas others are more vulnerable and take decades to recover. Recent advances in our understanding of the effects of bottom trawls mean that it is now possible to predict their impact and to suggest pragmatic ways to reduce those impacts through different management options, such as by directing fishing activity away from sensitive areas of the seabed to more resilient areas. These new advances in understanding pave the way for a truly ecosystem-based approach to fisheries management, in which we can manage target species and the wider effects of fishing on the seabed. This is the approach taken by the Marine Stewardship Council fisheries standard, which integrates the full range of ecosystem effects of fishing.  So, what challenges remain? While AIS data is publicly available, it covers only a proportion of the world’s fishing fleet. Vessel monitoring systems (VMS) are used for a larger selection of the world’s fishing fleets, but can be shrouded in confidentiality issues (the data is usually considered the property of fishermen). In addition, small-scale vessels, which in many places account for the largest proportion of fishing boats, are not monitored at present.  However, much data does still exist that can guide improvements in the management of fishing impact. It is critical that we interpret this data properly, using appropriate resolution mapping, to avoid inaccurate representations of the size and extent of fishing’s footprint on the oceans."
"

This morning, I took my children out front, and we placed three flags in our front yard. Each child got one little flag on a wooden stick to plant in the front garden, while mommy and daddy got the big flag to hang from the porch.
After a little discussion on why we did this on Memorial Day,” to remember those who keep us free”, my son William remarked, “ok…can we wash the car now?” (that was our next project).
Well maybe it’s a little early at nearly 4, to install some patriotism. But later when William and I drove to the hardware store together he said “Daddy, how come those houses don’t have flags? We have flags”. It was then I realized we were the only house on our entire street displaying flags today.
Good question son, good question.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea63cd550',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
This photo comes come to me from NOAA’s Weather Service Forecast Office in Monterey.
This is the official USHCN climate station of record for Livermore, CA. USHCN # 44997 The temperature sensor is located in a backyard of a residence within six feet of the swimming pool.

Here is the temperature trend from NASA GISS:

The question is: can an unbiased and accurate reading of temperature be obtained in somebody’s backyard next to their pool? With NOAA siting requirements saying a minimum of 100 feet from buildings, I would assume this would apply to pools too.
I couldn’t make this up if I tried.
You can see the picture without the annotations on the NWS website with this direct link:
http://www.wrh.noaa.gov/images/mtr/cpm/4997.jpg


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5653ce6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
You know your presentation was successful when:
1) Nobody threw rotten fruit
2) People came up to me afterwards and said “I have photos I can get to you”
3) A high level official at NCDC requests a copy of my presentation “as soon as you can get it to me”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea43650d4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
This mornig s session is all about drafting a set of suggestions to forward to other key members of the climate research community using the group knowledge gained from this conference. I have submitted my suggestion, and it has been accepted for inclusion in the publication. It reads:
 It has become clear that many surface weather stations, possibly a
significant number, may have undocumented biases that may or may not
be correctable using data analysis and data adjustment techniques.
After completion of weather station surveys for USHCN and other
networks, Why not identify the known good stations that have long term
records, few station moves, and no obvious microsite biases and
separate their data into a subset. Study the data and trends the known
good station subsets produce separately to see what can be learned.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea427e598',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter
Image cropped from Met Office here. 
By Die kalte Sonne
(German text edited by P. Gosselin)
On June 3, 2020, npj Climate and Atmospheric Science published a study by Athanasiadis et al. 2020, in which the authors investigated the question of whether changes in the frequency of blocked weather situations in the North Atlantic and Central European region are predictable.
“Quite some nonsense”
Previously, scientists inclined towards climate alarmism had told us that CO2 would lead to more and more blocked weather situations. Quite some nonsense as it now turns out, because the blockings are more likely to be due to the 60-year AMO ocean cycle, which in turn affects the NAO. These are exciting results.
Here’s the abstract:
Decadal predictability of North Atlantic blocking and the NAO
Can multi-annual variations in the frequency of North Atlantic atmospheric blocking and mid-latitude circulation regimes be skilfully predicted? Recent advances in seasonal forecasting have shown that mid-latitude climate variability does exhibit significant predictability. However, atmospheric predictability has generally been found to be quite limited on multi-annual timescales. New decadal prediction experiments from NCAR are found to exhibit remarkable skill in reproducing the observed multi-annual variations of wintertime blocking frequency over the North Atlantic and of the North Atlantic Oscillation (NAO) itself. This is partly due to the large ensemble size that allows the predictable component of the atmospheric variability to emerge from the background chaotic component. The predictable atmospheric anomalies represent a forced response to oceanic low-frequency variability that strongly resembles the Atlantic Multi-decadal Variability (AMV), correctly reproduced in the decadal hindcasts thanks to realistic ocean initialization and ocean dynamics. The occurrence of blocking in certain areas of the Euro-Atlantic domain determines the concurrent circulation regime and the phase of known teleconnections, such as the NAO, consequently affecting the stormtrack and the frequency and intensity of extreme weather events. Therefore, skilfully predicting the decadal fluctuations of blocking frequency and the NAO may be used in statistical predictions of near-term climate anomalies, and it provides a strong indication that impactful climate anomalies may also be predictable with improved dynamical models.”

Share this...FacebookTwitter "
"

In the third of a century since its founding, the Cato Institute’s scholars have issued a wealth of predictions about the likely effects of government policies and programs. While sometimes ignored or belittled, these predictions have often proved prescient.



Most famous was Joe Stilwell’s Policy Analysis published in 1982. In “The Savings & Loan Industry: Averting Collapse,” Stilwell warned that, “regardless of changes in the economic climate, numerous S&Ls will be unable to meet their financial obligations.” Few in government listened then. Through the remainder of the decade, Americans would have been better off if they had, before the taxpayers had to come up with a $500 billion rescue plan.



In 1982, Cato founder and president Edward H. Crane wrote about his recent visit to the Soviet Union. “It is a society that appears to be crumbling from within,” Crane wrote. He added, “If we can avoid confrontation with the Soviets over the next 20 years, their system should collapse of its own bureaucratic weight.” Such a prediction sounded crazy at the time. And indeed Crane’s estimate was off target.



The Soviet Union vanished, in not 20 years, but 9.



Stanley Kober, a research fellow in foreign policy studies at the Cato Institute, warned in a 1996 paper that “the terrorist attacks in Saudi Arabia, Israel, and other countries suggest that the trend in the Middle East is not nearly as hopeful as it appeared just a few years ago,” and he identified Osama bin Laden as a particular terrorist threat to the United States.



In a study he published in February 2001, Daniel Griswold wrote, “A domestic recession would reduce the trade deficit, as it has in the past, but at great cost to U.S. workers and their families.” A month later, the U.S. economy slipped into recession and the trade deficit declined in 2001 compared to 2000, after having risen in each of the previous five years. Then came the Great Recession, beginning in 2008. The trade deficit in 2009 was $300 billion smaller than in the pre‐​recession year of 2007.



In few areas have Cato scholars been more consistently correct and more consistently outside the mainstream consensus than the Iraq war. In 1999, Ted Galen Carpenter argued that “removing a thug like Saddam … is extremely ill‐​advised. It will make Washington responsible for Iraq’s political future and entangle the United States in an endless nation‐​building mission beset by intractable problems.” William Niskanen wrote in the _Chicago Sun‐​Times_ in December 2001, “Another war in Iraq may serve bin Laden’s objective of unifying radical Muslims around the world in a jihad against the United States.” In 2002, Doug Bandow warned that, “If Iraq’s forces don’t quickly crumble, the U.S. might find itself involved in urban conflict that will be costly in human and political terms.” And in March 2003, Christopher Preble argued America’s experiences with nation‐​building in Germany and Japan advise against attempting the same with Iraq. “If these ‘success’ stories reflect the model for post‐​war Iraq,” Preble wrote, “we should expect the U.S. to remain in this troubled region for many years.” Returning to domestic affairs, in March 2007, Jim Harper said in congressional testimony: “Mr. Chairman, the REAL ID Act is a dead letter. All that remains is for Congress to declare it so.” More than three years later, REAL ID, an attempt by the federal government to establish a national personal identification system, has gone nowhere, and two major implementation deadlines have passed.



In February 2009, when President Obama’s approval rating was in the mid‐​60s and most political opinion makers thought he was on the cusp of radically remaking America, Gene Healy published his first weekly column in the _D.C. Examiner_. Healy wrote, “When he fails to fully heal our financial troubles, fix health care, teach our children well, provide balm for our itchy souls, and so forth, his hopeaddled rhetoric will seem all the more grating, and the public will increasingly come to see him as the source of all American woes.” By July 2010, according to Gallup, President Obama’s approval rating had fallen to 44 percent, the lowest of his presidency, and his party was fearing considerable losses in the upcoming congressional elections.



As Healy predicted, President Obama did fail to fix health care. Instead, he ushered through Congress the ill‐​considered legislation known as ObamaCare.



Michael Cannon predicted in September 2009, six months before the bill’s passage, that ObamaCare’s individual mandate would force as many as half of all Americans with private insurance to switch to a more expensive plan. At the time, the administration insisted this was fantasy. In June, it all but admitted Cannon was right, prompting the _New York Times_ to write that “the rules appear to fall short of the sweeping commitments President Obama made while trying to reassure the public in the fight over health legislation.” Even earlier was Michael Tanner’s 2006 paper, “Individual Mandates for Health Insurance: Slippery Slope to National Health Care.” Later that same year, Massachusetts enacted health care legislation that included an individual mandate. The results have followed Tanner’s script exactly. RomneyCare’s individual mandate took effect in 2006, along with health insurance exchanges. Subsequently, 16 mandates have been added to the original list of benefits that health insurers must provide in the Bay State. Massachusetts now has the most rapidly increasing premiums in the nation. The most recent attempt to control costs, as Tanner predicted, was to simply prohibit insurers from increasing premium rates, leading insurance companies to predict that they will suffer from hundreds of millions of dollars in losses this year. In addition, wait times have increased to see both primary‐​care physicians and specialists, just as Tanner’s paper said they would.



The fact that policymakers failed to take Cato scholars’ warnings of the last 30 years to heart, makes it only more crucial that they do so in the next 30.
"
"
It appears that in the quest to save our planet from dangerous chemicals, people will blindly sign anything.  Read more about this dangerous chemical here: http://www.dhmo.org/ It’s “an odorless, tasteless chemical” that can be deadly if accidentally inhaled.

This chemical is so dangerous that a local city council in Aliso Viejo, CA put it on the agenda to ban foam containers made with it. I expect our liberal city council may soon address this danger like we’ve already done for nuclear weapons in the city limits.
Penn and Teller provide an entertaining look into mindless activism.

with a hat tip to Mark…


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea66a15b8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
This is the USHCN climate station of record for Bainbridge Georgia. It comes to me by way of surfacestations.org survey volunteer Joel McDade. Joel wins the award for finding the USHCN station closest to an air conditioner, at 8.9 feet. That honor was previously held by Oregon State Climatologist George Taylor at just over 10 feet in his picture of Forest Grove Oregon.

In addition to the air conditioner, this USHCN climate monitoring station sports several other features:
– A building just 14.3 feet away
– Convenient close-by radiator forward parking for your vehicle within feet of the MMTS sensor
– An asphalt road within 10-15 feet of the sensor
– A mature shade tree that changes shade patterns with the season
– A station move of about 150 feet closer to the building to accommodate the new MMTS sensor cable length
The station is operated by the International Paper Company. The plot of temperature below illustrates some data gaps and jumps that may be related to station moves.

Full details on this site are at the surfacestations.org online image database


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5e3246c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter The radiative impact of CO2 on the ocean’s thermal skin layer cannot penetrate deeper than 0.01 mm. This effectively eliminates the potential for CO2 to be a driver of global warming.
According to mainstream anthropogenic global warming (AGW) science, 93% of global warming is manifested in the 0-2000 m oceans. Just 1% of global warming is manifested as a change in atmospheric temperature.

Image Source: IPCC (ipcc.fandom.com)
Consequently, for anthropogenic CO2 emissions to be a driver of global warming, CO2 concentration changes must drive changes in the Earth’s ocean heat content.
Oceanographers Wong and Minnett (2018) point out that total CO2 forcing can only radiatively exert an impact on the top 0.01 mm of the ocean’s thermal skin layer. (Human hair is about 0.06 mm thick.)

Image Source: Wong and Minnett, 2018
Problematically, the amount of solar radiation absorbed in the upper 0.01 mm layer of the ocean is just 4.9 W/m².
Thus, CO2 concentration changes may, at most, affect 0.049% of the global oceans’ thermal skin layer.
This is the total extent of the radiative impact for CO2 in global ocean heat content changes.
CO2 may therefore be ruled out as a driver of global warming.
Share this...FacebookTwitter "
nan
"

 _Global Science Report is a feature from the Center for the Study of Science, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
Last fall, the press pounced on the results of a new study that found that global climate change was leading to an increasing frequency of heat waves and thus resulting in greater heat-related mortality. Finally a scientific study showing that global warming is killing us after all! See all you climate change optimists have been wrong all along, human-caused global warming is a threat to our health and welfare.   
  
Not so fast.   
  
Upon closer inspection, it turns out that the authors of that study—which examined heat-related mortality in Stockholm, Sweden—failed to include the impacts of adaptation in their analysis as well as the possibility that some of the temperature rise which has taken place in Stockholm is not from “global” climate change but rather local and regional processes not at all related to human greenhouse gas emissions.   
  
What the researchers Daniel Oustin Åström and his colleagues left out of their original analysis, we (Chip Knappenberger, Pat Michaels, and Anthony Watts) factored in. And when we did so, we arrived at the distinct possibility that global warming actually led to a reduction in the rate of heat-related mortality in Stockholm.   
  
Our findings have just been published in the scientific journal Nature Climate Change as a Comment on the original Oustin Åström paper (which was published in the same journal).   
  
We were immediately skeptical because the original Oustin Åström results run contrary to a solid body of scientific evidence (including our own) that shows that heat-related mortality and the population’s sensitivity to heat waves was been declining in major cities across America and Europe as people take adaptive measures to protect themselves from the rising heat.



Contrarily, Oudin Åström reported that as a result of an increase in the number of heat waves occurring in Stockholm, more people died from extreme heat during the latter portion of the 20th century than would have had the climate of Stockholm been similar to what it was in the early part of the 20th century—a time during which fewer heat waves were recorded. The implication was that global warming from increasing human greenhouse gas emissions was killing people from increased heat.   
  
But the variability in the climate of Stockholm is a product of much more than human greenhouse gas emissions. Variations in the natural patterns of regional-scale atmospheric circulation, such as the Atlantic Multidecadal Oscillation (AMO), as well as local impacts associated with urbanization and environmental changes in the direct vicinity of the thermometer are reflected in the city’s temperature history, and the original Oudin Åström et al. publication did not take this into account. This effect is potentially significant as Stockholm is one of Europe’s fastest growing cities.   
  
But regardless of the cause, rising temperatures spur adaptation. Expanded use of air conditioning, biophysical changes, behavior modification, and community awareness programs are all examples of actions which take place to make us better protected from the dangers associated with heat waves. Additionally, better medical practices, building practices, etc. have further reduced heat-related stress and mortality over the years.   
  
The net result is that as result of the combination of all the adaptive measures that have taken place over the course of the 20th century in Stockholm, on average people currently die in heat waves at a rate four times less than they did during the beginning of the 20th century. The effect of adaptation overwhelms the effect of an increase in the number of heat waves.   
  
In fact, it is not a stretch to say that much of the adaptation has likely occurred because of an increased frequency of heat waves. As heat waves become more common, the better adapted to them the population becomes.   
  
Our analysis highlights one of the often overlooked intricacies of the human response to climate change—the fact that the response to climate change can actually improve public health and welfare.   
  
Which, by the way, is a completely different view than the one taken by the current Administration.   
  
References:   
  
Knappenberger, P., Michaels, P., and A. Watts, 2014. Adaptation to extreme heat in Stockholm County, Sweden. Nature Climate Change, 4, 302-303.   
  
Oudin Åström, D., Forsberg, B., Ebi, K. L. & Rocklöv, J., 2013. Attributing mortality from extreme temperatures to climate change in Stockholm, Sweden. Nature Climate Change, 3, 1050–1054.


"
"

Our comment primarily concerns the Department of Energy’s (DOE) use of the social cost of carbon (SCC) in the cost/​benefit analysis of the Energy Conservation Program: Energy Conservation Standards for Small, Large, and Very Large Air‐​Cooled Commercial Package Air Conditioning and Heating Equipment proposed rulemaking. The DOE’s determination of the SCC is discordant with the best scientific literature on the equilibrium climate sensitivity and the fertilization effect of carbon dioxide — two critically important parameters for establishing the net externality of carbon dioxide emissions. It is also at odds with existing Office of Management and Budget (OMB) guidelines for preparing regulatory analyses. It is based upon the output of Integrated Assessment Models (IAMs) which have little utility because of their great uncertainties. They provide no reliable guidance as to the sign, much less the magnitude of the social cost of carbon. Additionally, as run by the Interagency Working Group (IWG) (whose results were incorporated by the DOE in this action), the IAMs produce illogical results that indicate a misleading disconnection between climate changes and the SCC value. Further, we show that the sea level rise projections (and thus SCC) of at least one of the IAMs (DICE 2010) is not supported by the mainstream climate science.
"
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   




Climate change is a moral, non‐​partisan and pragmatic issue which can be addressed by solutions with multiple co‐​benefits. We urge legislators to join global business, faith, scientific, health and military leaders in acknowledging that climate disruptions are real, happening now, and requiring our nation’s leaders to act.



It is interesting that they juxtapose a “moral issue” with calls for “policies to reduce national and global greenhouse gas emissions.” Interesting, we say, because there is a soon‐​to‐​be released and incredibly compelling book written by the Center for Industrial Progress’s Alex Epstein titled _The Moral Case for Fossil Fuels_. Its main premise is that both the short‐ and long‐​term benefits of using fossil fuels greatly outweigh the risks of any climate change that may occur as the result of the accompanying carbon dioxide emissions. Epstein argues that the “moral” thing to do is to continue (and expand) the use fossil fuels:   




If we look at the _big picture_ of fossil fuels compared with the alternatives, the overall impact of using fossil fuels is to make the world a far better place. We are morally obligated to use more fossil fuels for the sake of our economy and our environment.



The primary case against expansion of current fossil fuel use involves the risk from anthropogenic climate change. However, here, the threats are overstated—especially by organizations (like many of those behind The People’s Climate March) that favor centralized government control of energy production (and most everything else).   
  
  
The sea level rise concerns that are to be described in the Hill briefing will undoubtedly fall into the “overstated” category. According to the briefing’s flier:   




“The U.S. National Climate Assessment projected that sea levels will rise 1 to 4 feet by 2100, affecting 39 percent of the U.S. population and impacting the very futures of many coastal communities and small island nations.”



We imagine that the focus will be on the high end of the 1 to 4 foot range (and beyond), even as a plethora of new science argues for an outcome nearer to the low end.   
  
  
The current decadal rate of sea level rise is about 3 mm (.12 in) per year, which would result in about a foot of sea level rise during the 21st century. There is a lot of recent research that concludes that a large increase in this rate of rise as a result of the melting of Greenland’s and/​or Antarctica’s glaciers is unlikely.   
  
  
The statistical models most responsible for the high‐​end sea level rise projections used have been shown to be questionable and thus unreliable. And finally, and perhaps most importantly, the future projection of temperature rise made by climate models (upon which the sea level rise projections are based) have been shown by a growing body of scientific research to be overestimated by about 40 percent.   
  
  
Taken together, the latest science argues that the case for rapid and disruptive sea level rise is flimsy at best.   
  
  
Undoubtedly, sea levels will continue to rise into the future, in part, from the earth’s temperature increase as a result of human carbon dioxide emissions resulting from our use of fossil fuels. Appropriate adaptations will be necessary. However, signs point to a rather modest rise in sea levels accompanying a rather modest rise in temperature—a pace at which our adaptive response can keep up.   
  
  
So long as this is remains case, the continued use of fossil fuels to power the developed world and the expanded use to help provide safe, reliable, and cheap electricity to the more than 1 billion people in the underdeveloped world that currently live without any (or very minimal) access to it is a no‐​brainer. That’s where the moral imperative should lie.
"
"
Share this...FacebookTwitterLast Thursday evening in Münster, Germany, amid an atmosphere of loudly protesting students and Extinction Rebellion activists outside shouting obscenities and beating drums, prominent SPD social democrat and climate science critic Prof. Fritz Vahrenholt spoke on why Germany was headed down the wrong path with its now flailing transition to green energies, dubbed “Energiewende“.

Prof. Fritz Vahrenholt. Image: GWPF. 
Vahrenholt called the Energiewende: “An impending disaster.”
According to the Westfälische Nachricten here, “Scientists for Future activists handed out leaflets to emphasize that in their opinion the climate models of the IPCC (‘Intergovernmental Panel on Climate Change’) accurately depicted climate warming and that only trace gas CO2 was responsible for it.”
Hat-tip: Die kalte Sonne


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




But Vahrenholt, former environment senator of Hamburg, refuted the claims and showed why he thought CO2 is only half responsible for climate change today and that the rest was due to natural factors like sun and clouds.
In his 45-minute presentation, Vahrenholt showed those in attendance how Germany’s foray into green energies was doomed to fail. As leaders in Germany continue to insist wind energy is able to supply the country’s energy needs, Vahrenholt – an environmentalist and one of the founders of Germany’s modern environmental movement – pointed out the major technical obstacle: the inability to store wind energy for periods of low wind.
“Not even in the grid, like one well-known Green politician claimed,” said Vahrenholt, taking a shot at Green party leader Annalena Baerbock, who once famously claimed the power grid could store energy.
German electricity prices among world’s highest
Vahrenholt also reminded that the Energiewende has made Germany’s electricity prices among the highest in the world and that it would hit the poor especially hard. “I never understood the SPD here,” said Vahrenholt, criticizing his own party. The retired professor said it would take 90,000 wind turbines to supply Germany with electricity, a number that would lead to the country having a turbine every 2 kilometers.
The Westfälische Nachrichten sums up on whether the Energiewende is going to work:
At the end of the complex, 45-minute presentation, the majority in the hall were probably convinced: it can’t. The facts and figures presented by the environmentalist were too overwhelming.”
Share this...FacebookTwitter "
"
Share this...FacebookTwitterA new study (Stallinga, 2020) assesses the climate sensitivity to rising CO2 concentrations is just 0.0014°C per ppm. 
Dr. Peter Stallinga has published a comprehensive analysis of the Earth’s greenhouse effect. He finds an inconsequential role for CO2.
Doubling CO2 from 350 to 700 ppm yields a warming of less than 0.5°C (500 mK).
Feedbacks to warming are likely negative, as adding CO2 may only serve to speed up natural return-to-equilibrium processes.
As for absorption-reemission perturbation from CO2, “there is nothing CO2 would add to the current heat balance in the atmosphere.”

Image Source: Stallinga, 2020


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A portion of Dr. Stallinga’s paper worth highlighting – which he mentions only in passing – refers to the early history of the Earth’s greenhouse effect paradigm.
K. Ångström receives little attention as a pioneer of the conceptualization that warming and cooling resul from radiative imbalances within a planetary greenhouse effect.
About 120 years ago, Ångström (1900) contradicted the oft-cited Arrhenius (1896) – the atmospheric physicist referred to by proponents of anthropogenic global warming.
Ångström suggested Earth’s greenhouse effect is already saturated in its current (1900) state, and therefore increasing CO2 will have “no effect whatsoever” on climate (Stallinga, 2020).
Ångström’s conclusions were largely ignored.

Image Source: Arrhenius, 1896 and Stallinga, 2020
Share this...FacebookTwitter "
"

In his May encyclical, Pope Francis captured reader interest with an appeal to the deep sense of awe stemming from our attempts to comprehend complex natural processes of geology, biology, climatology, and others that comprise our ecosystem.



Unfortunately, interwoven with the science are familiar Malthusian ideological themes: excessive consumption by wealthy nations is responsible for climate change and the world’s poor are negatively and disproportionately affected’.





It is now within our power to export energy to poor, friendly nations and trading partners to help them keep their lights on, warm their homes, refrigerate their food and power their industry.



Not to worry though, because the encyclical follows up with matching ideological solutions. It implies that wealthy Western nations need to: quickly restrict and eventually eliminate all fossil fuel consumption, rush to develop renewable energy resources to power the Earth and clean up the environment, enable third world nations to eventually begin using renewables in their own grid, and, of course, foot the bill for all of it.



Part of the encyclical’s theme actually mirrors EPA’s recently published Clean Power Plan (CPP). Both documents attempt to foist expensive, unreliable, and unworkable “renewable” solar and wind power upon situations where it will not deliver for either wealthy nations or third world countries.



Pope Francis should realize we are truly blessed with our abundant energy resources. Just this year, the United States became the world’s top producer of oil and gas, and there is no end in sight to the vast energy supplies contained within our country and in other nations. Oil and natural gas production, particularly from the enormous U.S. shale‐​bearing basins, are at record‐​breaking levels and prices are continuing to drop — an enormous help to the poor, who spend an inordinate amount of their earnings on energy.



Now, for the first time in many years, we are taking more control of our energy and environmental future, and that of our allies. Supplies of oil and gas appear to be abundant worldwide. Thankfully, using cleaner natural gas in place of coal for power generation has cut carbon emissions by over 50 per cent, and that number is continuing to fall.



It follows that other countries are slowly gaining the extensive knowledge required to produce from shale, and some undeveloped countries can now see themselves participating in the “shale revolution” and producing sufficient energy to build their economies and address their national dreams and desires, just as we have done in the U.S. and in other Western nations.



As the Pope surely knows, the scriptures (Psalm 104:24 for example) teach that mankind is endowed with the wealth from the earth — from the rocks, surface waters, lifeforms, oceans, and atmosphere — and that we derive various forms and quantities of energy from each of these environments to sustain our lives and our progeny. It would appear that the papal encyclical’s discussion on man’s use of energy resources contains a major contradiction that must be addressed.
"
"

Tuesday on page 7A of the Enterprise Record there was a full page ad for the Oriental Buffet at the corner of East Avenue and Esplande that touted a copy of the most recent Butte County Health Department inspection with the words in bold “Compliance Achieved” on the newspaper ad.
You may remember the previous restaurant left an indelible mark in the minds of many Chicoans when it was closed down over a year ago due to massive health violations. Here is the ER Article.
Everybody deserves a chance to succeed, but I have to wonder about the wisdom of opening a door like this by putting your health report in a newspaper ad because it invites people to take a further look. It was the topic of my morning discussion group on Tuesday, so I decided to look for myself.
You can see Food Facility Inspection Reports for the Chico Area online here
And the inspection reports starting 5/07/07 for the Oriental Buffet are here:
( you’ll need Adobe Acrobat PDF reader to view these – its free here )

Oriental Buffet, 2539 Esplanade, Chico 05/07/07 Inspection 

Oriental Buffet, 2539 Esplanade, Chico 05/08/07 Re-Inspection 

Oriental Buffet, 2539 Esplanade, Chico 05/09/07 Re-Inspection 

Oriental Buffet, 2539 Esplanade, Chico 05/11/07 Re-Inspection 
On the first inspection on 5/07/07 there were 7 major violations and 14 minor ones, for a total of 21 violations. The inspector made 22 notations on the issues filling two pages. The next day on 5/08/07 they were down to 4 major violations and no minor ones. On 5/09/07 they were down to 3 major violations. On 5/11/07 they finally achieved “compliance”. The restaurant has been open since April 8th.
But I have to wonder, compliance for how long? You have to wonder that when a restaurant runs a full page ad touting “compliance” given the visually dramatic stigma the building has attached to it maybe the owners don’t fully understand what they are up against. Like I said, everybody deserves a chance to succeed, but perhaps a different theme would be the way to do it in this case.
To be fair though, I’ll point out that the inspection reports show that Egg Roll King on Palmetto needed 4 attempts to reach compliance this year , as did Gen Kai on Pillsbury, and Big Al’s needed 4 last year and so did Rice Bowl this year, and so did Sin of Cortez. Thai House on Broadway needed 5 inspections this year.
The all time high was Happy Garden on Cohasset with six consecutive inspections required last year before compliance was acheieved.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea61e5f6c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Wildfire is an integral part of the Earth system and has been for over 400m years. It is also an important and natural part of many of the world’s ecosystems. Indeed, some ecosystems, such as savannas, would not exist without fire – although others, such as the rainforests, cannot survive with wildfires and so work to maintain a damp climate. We have evidence of controlled fire from more than 400,000 years ago. But ascertaining the onset of our ability to kindle fire is more difficult. We certainly know it existed 40,000 years ago, but potentially as far back as 400,000 years ago. Over several thousand years, human populations have used fire to alter landscapes, hunt for food, and for comfort and to prepare food in the home. We have learned some control over fire and also how to respect it. But over the past hundred years or so, there has been a major transition in how we view fire –- what researcher Stephen Pyne has termed the “pyric transition”. This change, coupled with climate change, is behind the current wildfire crisis engulfing mansions in Hollywood and threatening lives in major towns and cities, not only in California but across the world. Although this problem has been widely [discussed in the scientific literature], many lessons remain to be learned by developers and residents living where wildfires are an environmental reality. As towns and cities have become larger, fire has been excluded – and if it breaks out at all it is carefully controlled before it can spread. As a result, urban populations have lost their understanding of fire and fire has been demonised. This demonisation has become worse as urban dwellers blur the boundary between human habitation and wilderness, expanding into remote areas and, in many cases, into flammable vegetation. We now face a situation in which all fires get rapidly extinguished when to let them burn may sometimes be the best option. This is because extinguishing fires in flammable landscapes can produce a build up of fuel, meaning future fires may be larger and more intense.  We need to learn about the biology of fire responses, which is key to recognising fire-dependent versus fire-sensitive plant species and ecosystems which exclude or cultivate fire. This is central to devising appropriate fire management systems and to deciding where we build. The twin problems of flammable non-native invasive plants and climate change are posing a bigger challenge to fire policy across the world. Flammable plants are spreading into vegetation which is unable to recover from wildfires, such as in the Saguaro cactus (Carnegiea gigantea) areas of the southern US. As a result, this ecosystem may cease to exist within only 20-30 years. In many cases –- even in areas that burn regularly –- non-native plants can cause significant problems, such as with cheatgrass (Bromus tectorum) in North America and Eucalyptus plantations in Portugal. Many of the non-native grasses were originally introduced into new areas as feed for cattle but have spread uncontrollably before it was appreciated that they were particularly flammable and could alter fire regimes. The relationship of fire with the evolution of plants and animals is a long one and many have adapted to a fiery landscape.
It has even been suggested that some plants encourage fire, by being more flammable but also by taking advantage of fire when it occurs by rapidly expanding in to the vacant space. Where wildfire is a normal part of the landscape, such as in the western US, earlier spring snow melt and a longer dry – and hence, fire – season thanks to climate change have combined with the spread of invasive grasses and people building in this flammable landscape to produce more frequent and larger fires. Fire cannot be excluded from these landscapes and we must learn how to live in fiery environments. We need to plan more carefully, not only regarding building materials, firebreaks and transport infrastructure, but also in how to maintain safe habitation in flammable landscapes. Attempts are being made to create fire-wise communities, but clearly we have some way to go and time is not on our side. Even in the UK, we face considerable challenges with the changing climate. Surrey is the most wooded county in England with a large population and built environment. A small heathland fire could easily turn in to a much more significant crown fire, threatening people and property. In such places, we need to plan for fire even if fire is not yet an everyday occurrence. Above all, we need to rethink fire . It has always been with us and will be on Earth when we are all long gone. We need to learn lessons from more than 400m years of fire on Earth if we are going to cope with fire in the future. We may kid ourselves that we can control fire, but wildfire is an ancient force that will easily escape our attempts to tame it."
"

Beginning with the Carter “hit list” and continuing with the fiscal conservatism of the Reagan administration, westerners have been obliged to realize that the days of concrete and steel solutions to water problems are gone. In stressing the West’s need to adjust to the new realities of water, Gov. Richard D. Lamm of Colorado described the change that has taken place as follows:



When I was elected governor in 1974, the West had a well‐​established water system. … Bureau [of Reclamation] officials and local irrigation districts selected reservoir sites and determined water availability. With members of the western congressional delegation, they obtained project authorization and funding. Governors supported proposals, appearing before congressional committees to request new projects, and we participated in dam‐​completion ceremonies.



In 1986, the picture is quite different. The boom in western resources development has fizzled, though tourism remains an economic mainstay.… Congress, including members of the western delegation, has to worry about how to cut spending, not which [water] projects to fund.… Farmers are trying to stay in business and are recognizing that their water is often worth more than their crops. Policymakers recognize that the natural environment must be protected because it is a major economic asset in the region.[1]



The current political, social, and economic climate is ushering in a whole new era in western water. In the face of efforts to curtail runaway government spending and protect the environment, water institutions must foster the conservation and efficient allocation of existing supplies. They must also take water’s growing recreational and environmental value into account. The crucial question is, can the current water institutions meet today’s requirements?
"
"

China’s born‐​again planners, led by President Jiang Zemin, are addicted to the idea that social and economic order areimpossible without the firm hand of the state. The Ninth Five‐​Year Plan (1996–2000) reconfirms the Chinese CommunistParty’s (CCP’s) faith in socialism and distrust of capitalism. The plan embraces the illusion that it is possible to revitalizestate‐​owned enterprises by a change in management rather than ownership. 



To think that China’s ruling elite can control an economy of 1.2 billion people is a fatal conceit. The failure of central planningand of communism throughout the world is testimony to the ill‐​fated desire to engage in social engineering. Indeed, recentstudies have shown that those countries that have fostered economic freedom have experienced greater wealth creation thanhave those that have failed to protect economic liberties. And as people have acquired greater economic freedom, they havedemanded other significant rights, including the right to participate in political life. That demand is exactly what worries China’shard-liners. 



Institutional innovation in China since 1978 has produced the fastest economic growth in the world and enlarged opportunitiesfor many people, but it has not eliminated the main obstacle to China’s future prosperity — the CCP. The party’s monopoly ofpower has been eroded by the market‐​based reforms of China’s paramount leader Deng Xiaoping, but with the Deng eraending, China is at a crossroads. The nation must decide whether to deepen reform and risk pressures for political change orslow reform and risk alienating China’s newly emerging middle class and fomenting social unrest. The dilemma is complicatedby the advent of democracy in Taiwan and the return of Hong Kong in 1997. 



**China at a Crossroads**



In less than two decades China has become one of the top 10 trading nations in the world. Over the past decade nearly $90billion in direct foreign investment has flowed into China’s emerging markets. If the Chinese economy continues to grow at anaverage annual rate of 9 percent, it could soon become the world’s largest. 



The challenge for China is to develop the hard institutional infrastructure of a market economy. The centerpiece of thatinfrastructure is a rule of law that protects property rights and limits the power of government. Therein lies the difficulty, for theCCP is unlikely to give up its power and let freedom reign. Nevertheless, there are internal and external forces at work thatmay push China in the direction of greater liberalization and democratization. Internally, China’s reformers have created aneconomic space that allows individuals the freedom to improve their living standards outside the state sector. Externally,China’s “open‐​door” policy has allowed foreign competition and know‐​how to help the nonstate sector grow. In the process,new business practices have evolved along with legal norms associated with a market‐​liberal order. 



The Ninth Five‐​Year Plan may slow China’s march toward the market, but it won’t stop it. The forces for change are toostrong. And the further the market advances, the more costly it will become for the CCP to try to reverse it. Yet it is clear fromChina’s saber rattling on the eve of Taiwan’s ascension to democratic rule that China’s authoritarian rulers are willing to incurconsiderable economic losses to protect their positions of power and privilege. That willingness is further evidenced by China’svow to crush any democratic movement in Hong Kong after 1997. 



The dilemma facing Western leaders is whether to contain China and risk military confrontation or to peacefully engage Chinawith the knowledge that the best way to foster human rights is by opening markets and cultivating exchanges — not by using theblunt instrument of economic sanctions or the rhetoric of China bashing. 



In evaluating their options, Western leaders should heed the lesson of Taiwan — a police state that turned into a free‐​marketdemocracy because economic liberalization led to political liberalization. Taiwan’s leaders were willing to experiment withinstitutional change and had the courage to let the people speak — in the market and in the polity. 



China, too, has created a new economic space but has resisted political change. Even so, there is reason to believe China mayfollow Taiwan’s “quiet revolution.” According to Lee Teng‐​hui, Taiwan’s first democratically elected president, “Vigorouseconomic development leads to independent thinking. People hope to be able to fully satisfy their free will and see their rightsfully protected. And then demand ensues for political reform.” Thus, he predicts, “The model of our quiet revolution willeventually take hold on the Chinese mainland.” 



**Deng’s Experiment**



The key to China’s progress has been its willingness to allow institutional change on a trial‐​and‐​error basis and to promotesuccess. Like Taiwan, China has reduced the relative size of the state sector by cultivating the nonstate sector, not byprivatizing large state enterprises. State‐​owned firms now account for only 25 percent of China’s total output (includingagriculture and services), and their share of industrial output has fallen from 80 percent in 1978 to 40 percent today. 



On the heels of the failure of central planning, Deng had no grand vision for institutional change, but he was willing toexperiment. His guiding principle was, “Once we are sure that something should be done, we should dare to experiment andbreak a new path.” 



Deng began to break a new path in 1978, when he launched his agricultural reform. Communal ownership of land wasabolished and a system of contractual relations was introduced through the “household responsibility system.” Rural familieswere allowed to hold long‐​term leases and acquired the right to use the land at their disposal. They could sell their crops in theopen market, provided they first satisfied the state quota. Under the new incentive structure, farmers increased production andbegan to invest their profits in town and village enterprises (TVEs), which are beyond the reach of state ministries. 



Although TVEs are legally owned by local governments, individual households are allowed to share profits, hold(nontransferable) shares, and receive dividends. Wages are tied to profits, and the managers of TVEs face hard budgetconstraints, unlike the politically motivated managers of state‐​owned enterprises (SOEs). As a result, TVEs have mushroomedwhile SOEs continue to whither on the vine of state subsidies. 



In addition to creating new ownership arrangements, Deng’s reforms decontrolled prices, opened China to the outside worldthrough trade liberalization and the establishment of Special Economic Zones, devolved power from the central government tolocal governments, and instituted a system of fiscal contracts that limited Beijing’s share of tax revenue and provided localofficials with an incentive to promote markets — a system Yingyi Qian and Barry Weingast have called “market‐​preservingfederalism.” Those institutional changes resulted in a parallel economic structure to compete with the SOEs, reduced thecentral government’s share of tax revenue from 60 percent in 1978 to 40 percent in 1993, and helped weaken the centralgovernment’s grip on everyday life. 



Those reforms, however, have failed to create a genuine market system founded on the principles of private ownership andfreedom of contract. The goal of China’s born‐​again planners is not market liberalism but market socialism. The resultant lackof clear rules at the enterprise level and attempts to plan the market are, in the absence of a constitution that protects propertyand contracts, reflections of what F. A. Hayek aptly called the “fatal conceit” of socialism. 



**Revitalizing Civil Society**



The quiet revolution that has been taking place in China’s economy since 1978 is combining with the information revolution tostrengthen the fabric of civil society and weaken the CCP. As China has expanded the freedom to earn a living outside thestate sector, individuals have gained greater control over their lives. In its 1994 report on human rights, the U.S. Department ofState noted the connection between economic rights and human rights: “A decade of rapid economic growth, spurred bymarket incentives and foreign investment, has reduced party and government control over the economy and permitted everlarger numbers of Chinese to have more control of their lives and livelihood.” 



People are learning how markets work by participating in the growing nonstate sector and by engaging in foreign trade. As themarket has replaced Marx, newly acquired ideas and wealth have given rise to a spirit of independence and to a rebirth of civilsociety, especially in China’s southern coastal provinces. Commenting on China’s cultural transformation, Jianying Zha writes inher book _China Pop_ , 



The economic reforms have created new opportunities, new dreams, and to some extent, a new atmosphereand new mindsets. The old control system has weakened in many areas, especially in the spheres of economyand lifestyle. There is a growing sense of increased space for personal freedom [so long as people stay out ofpolitics].



Anyone who has visited China and seen the vibrancy of the market, the dynamism of the people, and the rapid growth ofurban areas will concur with Zha’s cautious optimism. 



New towns and cities are evolving naturally as people flee the countryside for improved living conditions and the chance tostrike it rich in the nonstate sector. Villages that were once small fishing centers along the southern coast are now booming withthe flow of trade and people. The new urban centers, such as Shishi in the province of Fujian, are characterized by the market,not the plan. Their model of development, writes Kathy Chen of the _Wall Street Journal_ , is “small government, big society[ _xiao zhenfu, da shehui_ ] — which advocates less involvement by cash‐​strapped governments and more by society.” 



Ambitious young people want to become capitalists, not communists. A recent survey found that young people ranked beingan entrepreneur first among 16 job choices and employment with the national government eighth. Freer labor markets have ledto a growing demand among college students for business courses, and universities are responding. The CCP has lost much ofits credibility and is no longer the major route to success. 



The freedom to trade is an important human right in China. As trade expands, there will be a growing middle class with a largestake in China’s future. Moreover, China’s high savings rate gives all those who sacrifice current consumption and invest theirearnings in the nonstate sector a strong incentive to further depoliticize economic life. The formation of economic and civilsociety will lead to a natural call for greater participation in political life. Yet as long as the CCP stands in the way of thespontaneous market order, controls the flow of information, and prevents free association, the future of China’s civil societywill be in jeopardy. 



**Institutional Change and Democratization**



If democratization is to proceed in China, the government must continue to allow experimentation and new forms ofownership. Yuan Mu recently articulated the key role of ownership reform in the Beijing press: “We should discover the bestmodel for ownership by the whole people [notice the bias against privatization], so that they will genuinely become the mainbody of market competition and operate with vigor and vitality in accordance with the rules of the market economy.” 



Those rules will evolve as individuals grope for ways to lower the costs of exchange and expand markets. In _China Pop_ , Zhaquotes Liu Ge, a lawyer trained in both China and the United States, as saying, 



Gradually, there will be more laws and rules; the market will be more mature, more compatible withinternational standards, the competition more fair and open. Then, China will have been structurallytransformed! Political change will come after that.



According to Zha, “A lot of the educated urban Chinese … echo this way of thinking.” There is reason to believe, therefore,that institutional change in China will bring about what Princeton University professor Pei Minxin has called “creepingdemocratization.” 



Pei points to the upward mobility of ordinary people, occasioned by the deepening of market reform, and to the positiveimpact of China’s “open‐​door” policy on political norms. In his view, public opinion and knowledge of Western liberaltraditions, such as the rule of law, “have set implicit limits on the state’s use of power” and have promoted the democratizationof the legal system. People are starting to use the court system to contest government actions that affect their lives, liberty, andproperty. There has been a sharp rise in the number of civil lawsuits against the state, and individuals are beginning towin — perhaps as many as 20 percent of — their cases, according to official sources. 



The opening of the legal system is important because it paves the way for the transition from “rule by law” to “rule of law.“Marcus Brauchli of the _Wall Street Journal_ writes, 



The state’s steel‐​clad monopoly on the legal process, which makes the courts just another arm of government,is corroding. China’s economic liberalization … has spawned a parallel legal reform that raises the prospect ofrule of, not merely by, law.



Nevertheless, Brauchli recognizes that “legal ambiguity” remains “a ruthless weapon” for harassing the population. Until thatfacet of China’s institutional structure changes, no one’s rights will be secure. 



**China’s Future**



The challenge for China is to get out of the way of the market and let it grow naturally along with civil society. Doing so,however, requires an understanding of the institutional infrastructure that makes the market system tick and an appreciation ofthe spontaneous order that emerges when private property and freedom of contract are protected by a rule of law.Democracy is neither necessary nor sufficient for a market system — as the experience of Hong Kong has illustrated. What isnecessary is a stable legal framework that protects life, liberty, and property. If China is to prosper in the global economy, thenation will have to adopt common‐​law practices and abide by international commercial codes and customs. Old habits arehard to break, but the forces for change are strong, and there is reason to believe that China will “creep along in the rightdirection.” 



China has been willing to experiment, but it has not yet provided the climate of freedom necessary for growing market‐​liberalinstitutions. In fact, there is an effort to give the central government greater power by ending the system of fiscal federalism.Putting more money into the pockets of Beijing bureaucrats by recentralizing the tax system, however, is not the answer toChina’s problems. Nor will improving the management of SOEs do anything to solve the problems of loss‐​ridden enterprisesthat have no real owners. 



Real stability will come to China only when its leaders abandon their fatal conceit and realize that it is impossible to plan themarket or society. Although the leadership is willing to tolerate gradual reform to keep the economy strong, there is noindication that they will tolerate political reform. The crackdown on dissidents, especially the arrest of Henry Wu and WeiJingsheng, is a clear signal to Hong Kong and the rest of the world that democratic rule is unacceptable. The West should notconfuse economic liberalization with a desire for democratization. 



Foreign pressure is unlikely to foster positive political change in China. Indeed, such pressure is likely to be counterproductive.Beijing’s frosty attitude toward the United States and our confrontational approach to China will do little to promote stability inEast Asia or to advance human rights in China. Economic sanctions and partial removal of most‐​favored‐​nation trade status forChina would surely damage China, but they might damage the wrong people. Sanctions or higher tariffs could inflict harm onthose who are fleeing the state sector for greater opportunities and freedom in the market sector, and protectionist measuresclearly would harm U.S. consumers and Americans who do business in China. 



To depoliticize economic life, China needs constitutional change and new thinking. As Chinese scholar Jixuan Hu writes, “Bysetting up a minimum group of constraints and letting human creativity work freely, we can create a better society withouthaving to design it in detail. That is not a new idea, it is the idea of law, the idea of a constitution.” Ultimately it is up to theChinese people to shape their own institutions and to secure their fundamental rights, including the right to self‐​government. 



The United States, as the world’s leading constitutional democracy, should spread its ethos of liberty by keeping its marketsopen and extolling the principles that made it great. It should not play the dangerous game of pitting human rights activistsagainst free traders. China should be admitted to the World Trade Organization as soon as possible and be givenmost‐​favored‐​nation status unconditionally. 



It may take another generation for China’s quiet revolution to succeed, but with patience and foresight China may yet joinTaiwan in a mutually beneficial alliance based on free markets and free people. 



_James A. Dorn is vice president for academic affairs at the Cato Institute. He has lectured at Fudan University in Shanghai and is coeditor of_ Economic Reform in China: Problems and Prospects _._
"
"In the early hours of 11 October 2016, I closed a safety valve on an oil pipeline in Skagit county, Washington. I was acting as part of the “Valve Turners” direct action against climate change. Five of us, in locations across four states, succeeded in shutting down all five pipelines carrying Canadian tar sands oil into the US for a day. We were careful, transparent, civil and nonviolent. We put a premium on minimizing damage to pipeline property, and carefully considered ways to minimize any violations of the law. We called the pipeline companies beforehand, and waited around afterwards for the police to arrest us (nearly an hour in two cases).  Our motley crew of mostly retirees included a former IT manager, a retired tribal government attorney, a psychologist, a poet and, in my case, a climate activist and part-time handyman. None of us had ever been charged with a major crime. We were moved to action because the world is marching toward climate cataclysm, with almost nothing being done to change that. We acted out of distress for our children and grandchildren. We acted on behalf of the poorest peoples of the world, who have contributed almost nothing to the climate problem yet will suffer the most from its effects. We acted for all the wild things and wild places which have no voice. So it was stunning, and chilling, to learn that our protest was listed as an act of “domestic terrorism” by the US Department of Homeland Security, as the Guardian recently reported. In an intelligence report, the DHS catalogued 34 deaths and numerous cases of violence in recent years. Those included acts of terror by Dylann Roof, who “used a Glock 41 pistol to conduct a shooting at a bible study at Emanuel African Methodist Episcopal Church in Charleston, SC”, killing nine; Robert Dear, who “used an AK-47 to conduct a shooting at the Colorado Springs, CO Planned Parenthood”, killing three; and Micah Johnson, who killed five police officers in a shooting spree in Dallas. Tucked between those murderous rampages, the DHS reports that “suspected environmental rights extremists coordinated the shutdown of five pipelines in Minnesota (2), Washington, North Dakota and Montana”. That’s me and my friends, trying to do something before it’s too late. For my part in that action, I will be tried for a third time next month. My first trial, in January 2017, ended in a hung jury. At a second trial, in June, 2017, the jury hung one count of sabotage and voted to convict on the charge of burglary. Last April, an appeals court overturned that conviction because I was not permitted to argue that my action in shutting down the pipeline was justified by the greater need of addressing the climate crisis. Next month, a jury will consider that question. They will weigh the testimony of climate experts, and listen to my own explanation of why this kind of action, at this time, is necessary. There is no doubt of the straits we’re in. Each day brings more devastating ecological news, and millions of people are displaced by the extreme weather events triggered by our changing climate. Yet the US government ignores the increasingly frantic voices of the world’s climate scientists and drags us further down the path of no return. That is the real environmental extremism, and that is the extremism we ought to be fighting. Our government is directly complicit in this crisis. By subsidizing fossil fuels and leasing public lands to the carbon industry, the US is in large part responsible for the current state of our planet. All the while, our government has been working overtime to quash any prospect of addressing climate change. Last week, responding to repeated motions from our government, a federal court threw out the lawsuit Juliana v United States. The suit was brought by 21 youth plaintiffs, who charged that the US government has “sanctioned, permitted and authorized a fossil fuel system” that violated their rights to a stable climate. There is next to no possibility that the immediate steps required to stave off widespread catastrophic climate change – including ending the burning of tar sands oil and coal – will be undertaken by the Trump administration, our divided Congress or by the voluntary action of the fossil fuel industry. It has become clear: we cannot wait for our government to save us when they have created the problem in the first place. This article was amended on 31 January 2020. An earlier version misstated the dates of the first two trials. They occurred in 2017, not 2015. This has been corrected"
"The average UK commuter spends about 1.5 hours a day at the wheel. While not great for stress levels in general, there are other ways that the daily churn through traffic can negatively affect health. Research by my team at the University of Surrey has shown how drivers and pedestrians are being exposed to very high levels of air pollutants at traffic lights. The World Health Organisation links air pollution to seven million premature deaths every year. It’s well known that road vehicles in particular emit polluting nanoparticles which contribute to respiratory and heart diseases. Despite efforts to encourage a reduction, car usage has remained fairly constant in recent decades. Our team monitored drivers’ exposure to air pollutants at various points of a journey and found traffic intersections were high pollution hot-spots due to the frequent changes in driving conditions. With drivers decelerating and stopping at lights, then revving up to move quickly when lights go green, peak particle concentration was found to be 29 times higher than that during free-flowing traffic conditions. Also of course, while travelling by road we are generally pretty close to the air pollution source, which is the tailpipe of preceding road vehicle.  Though drivers spend just 2% of their journey time passing through intersections managed by traffic lights, this short duration contributes to about 25% of total exposure to these harmful particles. It’s not always possible to change your route to avoid these intersections, but drivers should be aware of the increased risks at busy lights and at least try to avoid regularly taking routes that force them to sit in traffic inhaling potentially harmful fumes. Where this is unavoidable the best way to limit exposure is to keep vehicle windows shut, fans off and try to increase the distance between the cars in front where possible. Pedestrians regularly crossing such routes should consider whether there might be other paths less dependent on traffic light crossings. But there is more to it than asking drivers to take circuitous routes. Local transport agencies could also help by synchronising traffic signals to reduce waiting time and consider alternative traffic management systems such as flyovers. They could also consider the appropriate placement of traffic lights. The use of these systems in built up residential areas, near schools or hospitals may serve to manage traffic flow but at the cost of trapping higher concentrations of harmful pollutants in exactly the areas where residents, and vulnerable members of society will most regularly commute or walk. I have written before about the use of low-cost sensing to capture air pollution hotspots in urban areas. The kind of data such projects could deliver feed directly into research such as this. The more we understand about where pollution hot spots are, the more direct action we can take in our own lives and the more we can push for greener, cleaner planning. The UK’s Environmental Audit Committee recently described air pollution as a “public health crisis”. These considerations are not just a “nice to have”, they have a direct effect on our health and wellbeing."
"
The picture below is of the USHCN climate station of record for Newport Beach, CA When I first visited this site I did a double take. Then started searching for the “real” temperature sensor.


I couldn’t believe that NOAA allowed them to use consumer grade equipment. I was sure I just hadn’t located the MMTS sensor. It wasn’t until I looked up the MMS metadata entry for equipment for NB and saw “miscellaneous” listed for rain and temperature sensors, that I began to get concerned.

I then went back a second time to be sure I hadn’t missed the station, after checking lat/lon on my GPS…because I just didn’t think it possible NOAA would allow a consumer grade sensor in the USHCN dataset. Then I found somebody in the harbor patrol office to ask, and he confirmed that was the station they use to send readings to NOAA.
I was reminded of that famous quote from the movie “Treasure of the Sierra Madre” lampooned in the movie Blazing Saddles; “We don’t need no stinking badges!”. Except, what was playing in my mind then was “We don’t need no stinking homogeneity!”
Note to NOAA: standards exist for a reason.
Apparently the observer wanted wind too, (the wind sensors are on top of the tower, not shown in these pictures)and while I can appreciate that being located at the harbor patrol office, NOAA could have supplied standard equipment in addition to the shiny new consumer grade Davis station. In fact a standard rain gauge and MMTS did exist, but was removed in 1998 in favor of “miscellaneous” equipment.
Now don’t get me wrong, Davis makes a great weather station, but we can’t just replace sensors with other types willy-nilly and have a homogeneously rigorous data set.
But there are other issues too, such as the rooftop proximity, the diesel generator, and the parking lot it sets in the middle of. More pictures available on surfacestations.org


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5193595',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"
Share this...FacebookTwitterOn May 26, the World Meteorological Organization (WMO) issued a press release warning of “another record-breaking heat season” for the northern hemisphere this summer, along with the potential of the COVID-19 pandemic amplifying the health risks of the hot weather.
Media outlets picked up the WMO warnings and spread panic stories of mayhem and climate breakdown among the public.
But veteran Swiss meteorologist Jörg Kachelmann, citing models from the ECMWF, doesn’t see any evidence of another “record breaking summer”. He tweeted:

Dear @WMO I've wondered since you wrote thishttps://t.co/2JEahOzwIi
What forecast data led you to the conclusion that we could expect the hottest summer for the Northern hemisphere ever in this year. Wherever I look, I can't see any evidence.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Can you help me? Thanks. pic.twitter.com/zMrlZRtCTC
— Jörg | kachelmannwetter.com🇨🇭 (@Kachelmann) June 12, 2020

So far Kachelmann has yet to receive an explanation from the WMO.
Like the ECMWF model for the next 45 days shows, the northern hemisphere has extensive cool patches, and so no signs of a “record breaking northern hemisphere summer this year.
Whether it’s the World Health organization (WHO or the WMO, global institutions set up to guide policy are doing a lousy job and are in need of extensive reform, as some leaders have already called for.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterWe grieve the loss of Dr. S. Fred Singer
By Michael Limburg, EIKE
(Translated, edited by P. Gosselin)

Dr. Siegfried Frederick Singer

Yesterday, our mentor and good friend, the outstanding scientist S. Fred Singer passed away – peacefully and quietly at the age of 95. Without the constant encouragement we received from this outstanding scientist from the very beginning, the founding of EIKE and our commitment to the dissemination of the scientific facts on climate change would not have been possible here in Germany.


Dr. Fred Singer as the keynote speaker at the first European Climate and Energy conference on May 30, 2007.
Distinguished career spanning 7 decades
Dr. Singer was the keynote speaker at our very first Climate Change Conference in Berlin in 2007 at the premises of the Institute for Entrepreneurial Freedom (IUF) on May 30, 2007, immediately after our founding. And he remained loyal to us in all subsequent years, even though in recent years his physical condition made the long journeys from his home in Virginia increasingly difficult.
But his unrestrained desire not to let science degenerate into a water-boy for politics, which was particularly evident in the increasing appropriation of environmental science by politics, allowed him to marshal all the strength his body could muster.
Fortunately for us all, he was able to do so for almost one and half decades. No one would have been more predestined than him to see exactly this monopolization, because he came directly from science and always worked there in outstanding positions. A short and partial look at his extraordinary curriculum vitae shows.
Father of U.S. weather satellites
His scientific work has also been published over 200 times in leading scientific journals. In 1954, President Eisenhower even awarded him a special prize from the White House for his work.
Without any exaggeration it can be said that S.Fred Singer can be called the father of the US weather satellites. Atmospheric physics was his domain.
Politicization of science “highly dangerous for democracy”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Because he saw that the emerging environmental movement was striving for a symbiosis with politics in particular, which was highly dangerous for democracy, he founded the Science and Environmental Policy Project (SEPP) in 1990 and the Nongovernmental International Panel on Climate Change (NIPCC) in Vienna in 2008. Both institutions were active in the collection and dissemination of scientific facts, against the increasing ideologization of the environmental idea – and the emerging panic-mongering about supposedly man-made climate change.
Over 200 scientific publications, wealth of books
A wealth of scientific books (Climate Change Reconsidered, or Unstoppable Global Warming, Every 1,500 Years, together with Dennis Avery) and many works written during this fruitful period, many of them with the support of Heartland and CFACT, bear eloquent witness to this.

Fred Singer at the 5th Climate and Energy Conference in Munich in 2012.
Escaped Nazi Germany, “unspeakable cruelty”
Our friend, my good friend S. Fred Singer, was also a living example of the unspeakable division and cruelty our continent saw in the last century. Born in Vienna in 1924 as the child of a Jewish family, he left his home country at the early age of 14 after the annexation of Austria by Nazi Germany in 1938, fled first to the Netherlands, where he was apprenticed to an optician, and from there emigrated further via England to the USA.
After serving in the U.S. Navy, he studied physics at Princeton and received his doctorate in 1947. Later he also studied electrical engineering at Ohio State University, where he graduated with a diploma. In addition to English, Fred spoke German, Swedish and Dutch.
Defamed by environmental activists who spread lies
However, neither his resume nor his extraordinary scientific merits kept the growing opposition from the green-left camp from attacking and muzzling him using unspeakable defamation and lies instead of scientific debate. The German WIKIPEDIA issue (here) offers readers an example of this. Among other things, the lie is repeated repeatedly that Singer would have let himself be bought by the tobacco lobby because he – himself a lifelong non-smoker and chairman of a non-smoker’s association – had truthfully stated that the carcinogenic effect of passive smoking could not have been scientifically proven.
My last e-mail contact with him is dated October 8, 2019, when we, the board of directors of EIKE, congratulated him on his 95th birthday. We didn’t get an answer to that, his mind was still alert, as we know, but his body refused  to go on.
Farewell, good old friend, rest in peace. You have done so much for this society. I am very proud to have had you as my friend.
Michael Limburg
European Institute for Climate and Energy
===========================
Also this blog, NoTricksZone, was in large part inspired by Dr. Singer, his occasional emails to me and particularly his book: Unstoppable Global Warming Every 1500 Years. And I know other skeptics here who also say they were inspired by him.
I recall wishing hm a happy 90th birthday with this blog post. He’s going to be missed.
Share this...FacebookTwitter "
"Fifty-one per cent of voters believe the prime minister, Scott Morrison, should have sacked the Nationals minister Bridget McKenzie over her handling of the $100m sports rorts affair, according to the latest Guardian Essential poll. The latest survey of 1,080 respondents, taken in the wake of revelations that McKenzie used a sports grants program to favour groups in Coalition-held or targeted seats, also shows voter support galvanising for an independent federal anti-corruption body, with 80% backing the proposal. When asked about the conduct of McKenzie in allocating grant funding to marginal seats to favour the Coalition in the election, 51% said they believed the prime minister should have stood McKenzie down from cabinet over the issue, while just 15% endorsed Morrison’s decision to support his colleague. About a third (34%) of participants said they had not been following the issue. Coalition voters were the least likely to say that Morrison should have stood the minister down, with 41% of voters saying she should go, 27% supporting the prime minister and the remainder unsure. The result comes after an explosive ABC report on Tuesday revealed that Sport Australia raised concerns that the Morrison government’s administration was compromising its independence in the weeks leading up to the election, and as the merit assessment scores of more than 2,000 projects were made public. The fresh revelations will heap further pressure on Morrison to move against the agriculture minister, whose conduct is being examined by the head of the prime minister’s department, Phil Gaetjens. The Essential survey, taken from 20 to 27 January, shows that support for McKenzie’s removal is higher than voter support in December last year for the removal of Angus Taylor from cabinet. At that time 35% of voters supported Taylor’s removal after police initiated an investigation into the doctored documents scandal. Voters were also asked in the same poll if they supported the establishment of an independent federal corruption body to monitor the behaviour of our politicians and public servants, with 75% supporting the idea. This has since grown to 80%. The number of respondents “strongly supporting” a new federal anti-corruption body has also risen, going from 42% in December to 49%, with support highest among Labor voters at 86%. Across all voting groups, only 7% opposed the idea, while 13% reported they were unsure. The survey also tested voter sentiment about the government’s climate change policies, asking about a range of hypothetical policy responses that could underpin the government’s pledge to “evolve” its response to the climate crisis. The most popular response was for the accelerated development of new industries and jobs powered by renewable energy, with 81% of respondents supporting the measure, including 75% of Liberal and National voters. Almost three-quarters (71%) of those surveyed supported a zero-carbon pollution target to be set for 2050, but support was high among Labor and Green voters (81% and 89% respectively) and low among Coalition voters at just 56%. When asked if they supported the prevention of new coalmines opening in Australia, the view of respondents split dramatically, with 48% of Coalition voters opposed to the idea, and 84% of Greens voters and 70% of Labor voters supportive. Across all potential policies, support was higher among Labor and Greens voters, and lower among Coalition supporters. The survey on climate policies comes as Morrison faces mounting pressure to explain how he intends to “evolve” the government’s climate change policies after he indicated the government would do more in the wake of the bushfire crisis. The prime minister has conceded the severity of the fire season is partly caused by climate change, but has sought to shift the focus to “adaptation and resilience” and a “practical” response, rather than on emissions reduction measures. In a speech at the National Press Club on Wednesday, Morrison will focus on the government’s “practical” response to the bushfire crisis, flagging a change to how the federal government can respond in times of national emergency. Morrison will say that while it is appropriate for states to take a leading role, “where, when and how the resources and capabilities of the federal government should be engaged is less clear”. “To date, the role of the commonwealth in responding to natural disasters has been limited to responding to requests for assistance from state governments. They judge the time and form of support needed,” draft excerpts of Morrison’s speech say. “The scale of the bushfires this season – not least their simultaneous reach across state borders – has unequivocally demonstrated the limits of those arrangements.” He will flag a review of the constitutional and legal framework to allow the commonwealth to declare a national state of emergency, the legal interface with the states and territories on responding to national emergencies, and an enhanced national accountability regime for natural disaster risk management, resilience and preparedness. The Essential poll, released on Wednesday, also canvassed support for a new national day to recognise Indigenous Australians either to replace or to sit alongside Australia Day. The survey found that compared to last year, support has decreased 2 percentage points with an overall 50% of respondents supporting the concept. Of those surveyed, 32% supported including a separate national day with a further 18% supporting a replacement of Australia Day. Support for a separate day was highest among Greens (73%) and Labor (58%) voters, and among those aged 18-34 (65%). Young people in this cohort were also far less likely to celebrate Australia Day compared to last year, down from 45% in 2019 to 32%. The poll’s margin of error is plus or minus 3%."
"

In politics, timing is everything, Howard Dean being a wonderful example. So is Al Gore, who chose to give a completely paranoid speech about global warming in New York two weeks ago, on a day when the temperature was 22 below normal. In a remarkably Dean‐​like rant to the Democratic organization MoveOn, he said that the reason Americans reject his vision of climate‐​Armageddon has more to do with what he called “a massive and well‐​organized campaign of disinformation” on the part of me and my few friends, than it does with the thermometer. 



When it comes to disinformation about climate change, Al’s got competition in the principal beneficiary of Howard Dean’s rhetorical largesse, John Kerry, who looks to me like a cinch for the Democratic nomination. On May 17, 2000, Kerry said:





I’m offering a night of free beer to the first journalist who can come up with a picture of John Kerry wearing a coat in November (and expect to have to pay off within one minute of this column’s publication). But what about that whopper about northern New Hampshire’s ponds?



One lesson in climate hype that Gore never learned (and which may have cost him the presidency) is that people can look up facts pretty quickly now. Gore lost normally Democratic West Virginia because of his hype on global warming and his resultant vitriol against the coal industry. Miners, who he would have put on unemployment, stayed home or voted for Bush. Now Gore’s venting about planetary heating in howling blizzards.



So should Kerry beware. There’s lots of data on the Internet, including a study by the U.S. Geological Survey of “ice‐​out” dates on lakes in northern New Hampshire. That’s the day of the year when you can no longer play hockey.



John Kerry is 60 years old, so it’s safe to say he was playing hockey in northern New Hampshire, his home, from the ages of 7 to 17, or 1950 through 1959, near First Connecticut Lake. The average date of ice‐​out for that period was May 1. From 1991–2000, when, according to Kerry, “you are lucky if the ponds freeze,” the average ice‐​out date is later, on May 5. 



A year later, on May 1, 2001, Kerry said, “This summer the North Pole was water for the first time in recorded history,” a story that was originally carried by the _New York Times_ in September 2000. It was retracted three weeks later as a barrage of scientists protested that open water is common at or near the pole at the end of summer. Further, it’s common knowledge in the scientific community that there has been no net change in Arctic temperatures in the last 70 years.



He went on: “In 1995, after a period of unusual warming, a 48 by 22 mile chunk of the Larsen Ice Shelf in Antarctica collapsed.” Disregarding that ice shelves don’t “collapse,” the fact, as accessible as the nearest _Nature_ magazine, is that Antarctica shows a slight cooling trend in recent decades.



Voters need to stay tuned to Kerry on global warming for the Arizona primary on Feb. 3. John McCain, who will do anything to defeat George Bush, has been on a merciless campaign of badgering the president about climate change, including shepherding the first Senate vote to restrict energy use because of global warming, which only failed by eight votes last fall. You can bet Kerry is going to feed off of McCain’s Arizona popularity. He may even entreat him into the Veep slot, claiming to be the ultra‐​centrist and spelling sure defeat for President Bush. 



Anyway, now that he’s the front‐​runner, he’s going to have to watch what he says. Or what he wears. Again, free beer for that picture of him wearing a coat in November.



If Kerry doesn’t check his facts better, he’ll soon be sharing the platform with Al and Howard, trapped in the living hell of the formerly relevant. 
"
"The UK government is being sued for approving a large new gas-fired power plant, overruling the climate change objections of its own planning authority. The plant, being developed by Drax in north Yorkshire, would become the biggest gas power station in Europe and could produce 75% of the UK’s power sector emissions when fully operational, according to the environmental lawyers ClientEarth, who have brought the judicial review.  The planning inspectorate recommended to ministers that the 3.6GW gas plant was to be refused permission because it “would undermine the government’s commitment, as set out in the Climate Change Act 2008, to cut greenhouse emissions” by having “significant adverse effects”. It was the first big project rejected because of the climate crisis. However, Andrea Leadsom, secretary of state for business, energy and industrial strategy, rejected the advice and gave the go-ahead in October. Now ClientEarth has been given permission by the high court to sue ministers, with the case expected to be heard in about two months. The environmental lawyers have previously inflicted three defeats on ministers over their failure to tackle air pollution. “With scientists ringing the alarm bells for decades, we shouldn’t need to take the government to court over its decision,” said Sam Hunter Jones, a lawyer at ClientEarth. “[Leadsom’s] decision is at odds with the government’s own climate change plans. As the planning inspectorate found, if this plant goes ahead the public risks a carbon budget blowout, or a huge stranded asset that would require propping up by the taxpayer, or a combination of the two.” A Drax spokeswoman said the company’s ambition was to be removing, not adding carbon to the atmosphere, by 2030. It would do this by burning wood or plants and then capturing and storing the emissions. She said Drax’s carbon negative ambition could be achieved alongside “new, high efficiency gas power capacity as part of our portfolio” and provide electricity when the wind was not blowing or the sun shining. The UK government’s actions to tackle the climate emergency are under particular scrutiny this year as it will host a vital UN summit in Glasgow in November. The world’s nations must dramatically increase their pledges to cut carbon emissions at the summit to avoid a disastrous 3-4C rise in global temperatures. The government is to bring its environment bill before parliament on Thursday, which it said underlined its commitment to tackling the climate crisis. The Guardian revealed last week that more than 90% of the £2bn in energy deals struck at a UK-Africa investment summit were for fossil fuels. In its planning application, Drax said its proposal for four new gas turbines was warranted to replace its existing two coal-fired units ahead of the government’s proposed phase-out of coal in 2025. It said the new gas plant would be “capable” of having carbon capture technology fitted in the future. In overruling the planning inspectorate, Leadsom argued that the plant’s high carbon emissions were not a reason to block approval under the existing rules. “While the significant adverse impact of the proposed development on the amount of greenhouse gases emitted to atmosphere is acknowledged, the policy set out in the relevant National Policy Statements makes clear that this is not a matter that should displace the presumption in favour of granting consent.” ClientEarth says the government’s latest forecasts estimate the UK will need 6GW of new gas generation up to 2035. The UK has already approved more than 15GW of large-scale gas plants, it said, so approving Drax’s project would take this to three times the government’s estimates. The environmental lawyers argued the combination of the project’s large scale, level of carbon emissions and long operating life made it a significant threat to the UK’s carbon targets. The planning inspectorate also concluded that wind and solar power would cut payers’ bills, while the proposed gas plant would not. “Both [Drax] and [National Grid] confirmed that it is the production of renewable plants that will deliver cheaper energy.”"
"

Have you ever wondered why the vast majority of plants and trees have green
leaves and not some other color?
It’s always been a bit of a mystery why plants absorb red and blue light, but
reflect green, allow us to see the leaves as green. It seems inefficient of
nature when the sun emits the peak energy of its visible spectrum in the
yellow-green areas. A new theory offers one possible answer: that the first
chlorophyll-utilizing microbes evolved to

exploit the red-and-blue light that older green-absorbing microbes didn’t use,
eventually out-competing them through

greater efficiency and the rise of oxygen.
If that were the case, plant life long ago may have had purple leaves to
catch both the red and blue portions of the spectrum. For those whom don’t know
this, RED + BLUE =
MAGENTA (purple)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea6e7304d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

OK, _The New York Times per se_ has not weighed in with harsh criticism, but Prof. Hal Varian of U. Cal. Berkeley, a contributor for the NYT’s excellent “Economic Perspectives” column, weighs in today with a nice summary of the problematic assumptions made by Sir Nicholas Stern in the oft‐​quoted Stern Review on the Economics of Climate Change. For those who don’t recall, Stern argued that it makes sense to spend 1 percent of the world’s GDP to reduce greenhouse gas emissions because the costs associated with those emissions might total anywhere between 5–20% of global GDP some time down the road.



Regular readers here will notice that Prof. Varian’s arguments closely mirror those I made earlier on this page (for the curious, here and here, with a minor correction to the latter here).



So it’s not just me folks .…. 
"
"

Well I just finished Day1 at the conference at UCAR (University Corporation for Atmospheric Research) put together by Dr. Roger Pielke, and sponsored by the National Science Foundation titled: Detecting the Atmospheric Response to the Changing Face of the Earth: A Focus on Human-Caused Regional Climate Forcings, Land-Cover/Land-Use Change, and Data Monitoring.
The day started off bright and early with the shuttle to the NCAR headquarters, shown above. It’s a unique place, at over 6000 feet up right next to the “flatirons“. Once there, we learned that the conference had been moved to downtown Boulder (somebody forgot to tell the shuttle driver). So we had to wait for the shuttle to return. A new one arrived, and we piled in. Then we sat there and waited because others were coming. As we waited in the sun, someone remarked, “It’s getting hot in the van, open your window” to which I remarked “well, with all these windows, it’s a simple greenhouse experiment”. That brought a chuckle, then “no, really, open he window”. So 10 minutes later, we were on our way in a van that holds 12, we had 7.
The driver informed us he had two stops to make to pickup additional people. We added three at the first stop, and at the second stop, at the invitation of the driver (I don’t mind if you don’t ) we added 6 more people, for a total of 16, all crammed into a van that holds 12.  After that exercise I quipped: “well in addition to our earlier greenhouse experiment, now we are adding population growth in an urban setting” which drew a big laugh – inside joke for climate science, you had to be there.
At the conference we had a busy day, lots of papers on land use changes, urbanization studies, rainfall studies, and one statistical study which really caught my eye because I had lunch with the presenter and he gave me the real inside scoop on the “adjustments” process used to turn raw temperature data into “usable” data. More on that later.
I felt a bit out of place at first, because I’d been away from the scientific community for awhile, and this was the first presentation of this type (mine comes tomorrow) in about 25 years. So I was a bit nervous. That soon faded, as people whom I’ve never met saw my name tag, came up and introduced themselves, and said things like “I’ve been following your work, I’m really looking forward to seeing what you’ve found” “after what I’ve seen on your website, I’m beginning to think the surface temperature record is hopeless, and we should focus elsewhere”. So I started feeling a bit more confident. I didn’t see anybody packing rotten tomatoes, and everyone was very nice today, so I’m hoping for the best tomorrow.
Of course Roger Pelke Sr. was a most gracious host, as was his assistant, Dallas, and it was a comfortable and easy day thanks to their efforts.
Later I’ll have a short summary of some of the papers presented today.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea4c6bd7e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The African continent is a “blind spot” for coverage of the humanitarian crises that are being fuelled by the climate emergency, according to a new analysis [pdf]. Madagascar’s chronic food crisis, where 2.6 million people were affected by drought in 2019, came top of the list of 10 of the most under-reported crises last year, Care International’s annual survey found.  Others included Zambia, a country on the frontline of the climate emergency, with 2.3 million struggling to eat due to drought, and Kenya, which received only 20% of expected rainfall in 2019, and where 1.1 million people were hungry amid both floods and drought. Last year, climate activism led by Swedish teenager Greta Thunberg dominated headlines in the northern hemisphere, but the suffering of millions of people in food poverty caused by global heating in the south was not being covered, according to the research. Nine of the 10 countries in which at least one million people were affected by natural or man-made disasters to receive the least media attention were in Africa, where temperatures are rising at twice the global average, according to the Intergovernmental Panel on Climate Change. “We’re seeing increasing linkages between the effects of man-made climate change and the longevity and complexity of humanitarian crises,” said Sally Austin, international head of emergency operations at Care. “From Madagascar to Lake Chad to North Korea, the majority of crises ranked in our report are partly a consequence of declining natural resources, increasing extreme weather events and global warming more broadly.” “What the report does is to highlight those 10 countries which received the least amount of media coverage. Is this because people aren’t interested in reading about it? Should we be thinking: ‘Is this good enough?’” North Korea and Eritrea, both highly secretive states where press freedom is limited and reporting is restricted, were also on the list. “The increased public attention for the global climate crisis is encouraging, but we must ensure that the conversation is not limited to the global north and much-needed transformations there,” Austin said. The countries with most media coverage of humanitarian crisis were Syria and Yemen and the Democratic Republic of Congo, all countries with ongoing conflict. For its fourth annual survey, Care used the Meltwater group to monitor and analyse 2.4 million online sources, in English, French, German, Spanish and Arabic. A list of 40 humanitarian crises in which a million people were affected was monitored from January 2019 until 15 November. The other countries included the Central African Republic, which was ranked second after Madagascar, due to ongoing conflict; Burundi, where instability is causing displacement and 1.7 million people are hungry; and Burkina Faso, where a quarter of the population, 5.2 million, are affected by escalation of violence. Also among the areas listed were Ethiopia, one of the world’s most drought-prone countries, where 7.9 million people are suffering a cycle of disaster, hunger and displacement, and the Lake Chad basin, where 10 million people are in need due to conflict, displacement and hunger, partly due to the lake’s shrinking.  The report found a correlation between media coverage and funding received: three of the 10 most under-reported crises in the report are also on the UN’s 2019 list of most underfunded emergencies."
"
Share this...FacebookTwitterStefan Rahmstorf on the IPCC modelling breakdown: Reason to breathe a sigh of relief, new climate models are far too sensitive.
By Die kalte Sonne
(Translated by P. Gosselin)
DER SPIEGEL provides a regular platform for the controversial climate scientist Stefan Rahmstorf. On 12 May 2020 he was allowed to:
Stronger temperature rise: Why the climate models are running hot
A guest article by Stefan Rahmstorf
New calculations have alarmed the scientific community – they suggest the earth could be more sensitive to greenhouse gases. Will global warming be stronger than previously thought?
Here the quick reader will suspect one of the usual Rahmstorf climate alarm pieces. And this is exactly how the beginning of the article reads. However, it deals with a tricky topic that will certainly hit the Potsdam scientists quite hard to the stomach.
Huge mishap
In the course of the preparation of the 6th Climate Status Report, the IPCC has again run a large number of climate models. This time, however, a huge mishap has occurred. Several of the models have delivered far too much warming, which is not compatible with the measured data of the last decades. This fundamentally casts the models into question. They suggest that the warming effect of CO2 is far too high. A scandal that should actually cast everything into question.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Rahmstorf plays it dumb at the beginning of the article, luring his readers into the alarm trap. Will everything get much worse than expected? This is the typical Rahmstorf narrative.
“Models are crap”
But if you can make it to the end of the article, you will be surprised. Rahmstorf actually admits quietly that the models are crap, running way too hot.
In reality it’s all not so bad. Rahmstorf writes literally in his article:
The comparative study by researchers from the University of Exeter now shows that in particular the warming since 1975 – i.e. most of the modern global warming – is clearly too strong in the sensitive models. More recent analyses by ETH Zurich, for which more models have already been evaluated, confirm this conclusion. This is a reason to breathe a sigh of relief: there is currently some evidence that these models are not better than the old ones, but are simply too sensitive.“
Did SPIEGEL force its guest author to write this article? Was this a prerequisite for him to continue writing there? A balanced presentation with a fair evaluation of all opinions represented in science has never really been Rahmstorf’s strength.
Obvious failure
Or was it a flight to the front because the modelling failure was all too obvious and Rahmstorf feared complete professional isolation? It’s hard to say.

Stefan Rahmstorf must have struggled for several months before deigning to admit this mishap. This certainly could not have been easy for him.
By the way, here in the blog we have already reported on the topic several times: “The sun in February 2020, science against doom and gloom” and “The sun in November 2019 , when models exaggerate” and “The sun in December 2019, advances in climate science“.


		jQuery(document).ready(function(){
			jQuery('#dd_a5e15193542096fb84730e25e385a8c3').on('change', function() {
			  jQuery('#amount_a5e15193542096fb84730e25e385a8c3').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000

Share this...FacebookTwitter "
"

This picture comes to me via www.surfacestations.org courtesy of Dr. Roger Pielke Sr. of the University of Colorado.
It is the US Historical Climatological Network (USHCN) Station of Record for Hopkinsville, KY. The NOAA provided Max/Min Temperature Sensor is located at the observers home. The nearby air conditioner is just 10 feet from the temperature sensor. Then there’s the chimney. The contribution of the portable BBQ grill to the temperature record is unknown.
The MMTS temperature sensor wasn’t always mounted on the tower next to the house, it used to be in the yard, but the observer made some “improvements” over time. Note that published NOAA/NWS siting standards require a 100 foot distance from buildings.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea5a35352',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

_The_ _Washington Post_ writes about how President Obama became obsessed with grabbing our complex energy systems by the scruff of the neck and shaking them into something more appealing to Ivy League planners. I was struck by this vignette: 



But even before the late‐​night session in July, Obama had begun to educate himself about energy and climate and to use those issues to define himself as a politician, say people who have advised him. He read a three‐​part New Yorker series on climate change, for instance, and mentioned it in three speeches.



It’s great that he read a three‐​part series in the New Yorker. But has the president ever actually read anything by a climate change skeptic? Actually, a better term would be “a climate change moderate.” Leading “skeptic” Patrick J. Michaels, for instance, of Cato and the University of Virginia, isn’t skeptical about the reality of global warming. His summary article in the Cato Handbook for Policymakers begins: 



Global warming is indeed real, and human activity has been a contributor since 1975.



But he also notes that climate change is complex, and its policy implications are at best unclear. “Although there are many different legislative proposals for substantial reductions in carbon dioxide emissions, there is no operational or tested suite of technologies that can accomplish the goals of such legislation.” The flawed computer models on which activists rely cannot reliably predict the future course of world temperatures. The apocalyptic visions that dominate the media are not based on sound science. The best guess is that over the next century there will be very slight warming, without serious implications for our environment our society. Michaels’s closing appeal to members of Congress would also apply to President Obama and his advisers: 



Members of Congress need to ask difficult questions about global warming.   
  
  
Does the most recent science and climate data argue for precipitous action? (No.) Is there a suite of technologies that can dramatically cut emissions by, say, 2050? (No.) Would such actions take away capital, in a futile attempt to stop warming, that would best be invested in the future? (Yes.) Finally, do we not have the responsibility to communicate this information to our citizens, despite disconnections between perceptions of climate change and climate reality? The answer is surely yes. If not the U.S. Congress, then whom? If not now, when? After we have committed to expensive policies _that do not work_ in response to a misperception of global warming?



Please, President Obama — in addition to the lyrical magazine articles on the apocalyptic vision that you read, please read at least one article by a moderate and widely published climatologist before rushing into disastrously expensive policies.
"
"The amount of renewable, low-carbon, energy the UK produces is increasing, but it is very different to traditional types of power. It can’t just be turned on when wanted. As a result, the capacity market scheme – essentially a programme of subsidies – was set up to help provide backup power when the supply of renewable energy the UK produces is outpaced by demand.  Until recently this was the flagship mechanism for helping the UK meet its climate change reduction targets while maintaining a secure electricity supply. However, on November 15 the European Court of Justice ruled that the European Commission had failed to properly investigate the scheme, rendering it unlawful. Despite its huge implications, this news was buried beneath blanket coverage of prime minister Theresa May’s Brexit deal, which came out at the same time.  Participants in the capacity market scheme – predominately fossil fuel-based power suppliers – were paid per megawatt (MW) for the capacity they offered. In order to offset the risk of investing in less predictable supply, fixed monthly payments were made to these suppliers. The first contracts were agreed at the end of 2014 and their requirement to start providing capacity started last winter.  Funding of up to £2.6 billion a year was set aside in 2012 by the UK government, as it was deemed the most appropriate mechanism for helping meet demand in an increasingly supply-led system.  However, in 2014 Tempus Energy raised objections about the scheme to the European Court of Justice. Its stance was that it favoured the use of fossil fuels rather than considering other mechanisms, such as flexible price incentives for industry and domestic users, which involve reducing costs when there is an abundance of wind and solar, and increasing the cost when there is not. Smart metering is one way in which this can be done on a domestic scale. This can also happen on an industrial level with incentives to increase or reduce demand based on supply levels, for example, Demand Turn Up or Firm Frequency Response. In essence Tempus argued that the scheme made it harder for it and other demand management companies to compete with the well established fossil fuel-based power generators. Under EU state aid rules, member states are obliged to consider alternative ways of meeting market demand for power before subsidising polluting generators. They also require any capacity boosting measures to be designed in a way that provides “adequate incentives” for operators of new, cleaner technologies.  Tempus data shows that the capacity market had predominantly supported fossil fuel providers. Therefore, the UK had effectively been state funding fossil fuel power generators to be on standby to produce power when, and if, there was a shortfall in the electricity being produced. The concern is that by using the scheme to predominantly fund fossil fuel provision, Britain will continue to rely on such technologies into the future, reducing the ability to cut climate change targets and uphold agreements.  So on November 15, the ECJ ruled in favour of Tempus and stated that the European Commission was wrong to clear the scheme for state aid approval in 2014 without looking into it in more detail. As a result the market has been suspended indefinitely.  The decision means the UK government cannot now issue capacity market payments to energy firms. In addition, it cannot hold any further auctions, including the upcoming auctions scheduled to run in January and February 2019 to secure additional power capacity for upcoming winters.    Britain will, most probably, be able to keep the lights on, but it might come at a higher financial cost in the short term. Energy policy in the UK is incredibly complex. There are numerous incentives, taxes and subsidies all with the aim of reducing cost, reducing carbon and meeting green house reduction targets and producing a sustainable supply.  A review of the cost of energy in 2017, by the Department for Business, Energy & Industrial Strategy, outlined the UK’s energy policy to be lacking in terms of consistency and ability to meet the needs of the country. It stated that, “energy policy, regulation and market design are not fit for the purposes of the emerging low carbon energy market”.  It’s hard to move from a dispatchable system to a more supply-led system. All of the UK’s costing models and energy policies are essentially based on the ability to burn something to provide power, and the economics have developed from the traditional supply-and-demand approach. What Britain now needs to do is move from traditional demand based policies to those which include the whole energy system. The capacity market attempted to do part of this by favouring fossil fuels as a mechanism to cope with short term shortfalls in supply. The UK needs to be doing better than this. It must adopt a truly systems-based approach to dealing with its increasing renewable supply.  This ruling, while problematic in the short term, is a long term opportunity to develop a more dynamic and flexible energy system. The UK must invest in demand management programmes rather than only investing in generating assets. Britain needs to embrace a truly low carbon system with active demand management systems alongside renewable technology – enabling it to phase out its reliance on fossil fuels."
"Curious Kids is a series for children of all ages, where The Conversation asks experts to answer questions from kids. All questions are welcome: find out how to enter at the bottom of this article.  Why do leaves change colour in autumn? – Isaac, age eight, Guildford, UK Hi Isaac, this is a really interesting question and something that lots of people wonder about when the seasons change. In the autumn, lots of plants (especially trees) throw away their leaves.  These are great for jumping in, but why do some plants do this? It seems like a waste. But actually, by dropping their leaves they are saving their nutrients for the next summer.  For plants to grow, they need sunlight, nutrients and water. The nutrients and water come from the soil. The sunlight is captured by the leaves.  To capture the sunlight, the leaves use a chemical called chlorophyll, which is what makes leaves green. Chlorophyll turns sunlight into food, which the trees need to grow, through a process called photosynthesis.  In summer, plants do lots of photosynthesis, because they get lots of light and because it is warm. The food they make is sugar, which they use to grow new leaves, flowers and seeds. In winter, things are less comfortable. The days get shorter, it gets colder and there is less sunshine. For you, this is not a problem – if you are cold, you can put a coat on. But plants can’t do this. And when it gets really cold, and freezes, their leaves can be damaged. If you want to see what freezing does to different leaves, there is an experiment you can try at home. Take some different leaves and put them in your freezer, or the ice box in your fridge. Leave them for a day to get really cold, then take them out again. Put them on a plate so you don’t make a mess, then just wait for them to warm up (this will take a while).   Some leaves are really tough and don’t mind being frozen. If you take a holly leaf, it will look just the same after you freeze it as it did before. But if you try this with a soft leaf, such as lettuce, you will see something different.  For plants with leaves that don’t like to be frozen, winter is a bad time. Their leaves are all going to be destroyed in the cold weather. If this happens, they will also lose a lot of good things which are in the leaf, especially the nutrients they get from the soil.  They use the nutrients to make chlorophyll and they don’t want to lose them when the leaves freeze. So instead, they break down the chlorophyll to get the nutrients out and store them in their roots, which are protected from the cold. As the plants break down the chlorophyll, the green colour disappears from their leaves. What is left behind is other chemicals which you normally cannot see. The most important of these are called carotenoids, which are what makes carrots orange.  Depending on which chemicals are found in the leaf, they can turn different shades of yellow or orange or even red. These chemicals do not have any nutrients in them, so the plant does not bother to break them down, it leaves them in the leaves.   Once all the chlorophyll is taken out, the leaf dies. As it dries out, the leaf starts to look brown and becomes crispy. At this stage, it falls off the tree. In the spring, as the days get longer and the weather gets warmer, the tree uses the nutrients and food that it has stored in its roots to make new leaves, ready for the summer when it can do lots of photosynthesis again. Hello, curious kids! Have you got a question you’d like an expert to answer? Ask an adult to send your question to us. You can: * Email your question to curiouskids@theconversation.com 

* Tell us on Twitter by tagging @ConversationUK with the hashtag #curiouskids, or

* Message us on Facebook. Please tell us your name, age and which town or city you live in. You can send an audio recording of your question too, if you want. Send as many questions as you like! We won’t be able to answer every question, but we will do our best. More Curious Kids articles, written by academic experts: Why do flies vomit on their food? – Lili, age ten, Adelaide, Australia If you have lots of the thing you’re allergic to, does your body get used to it? – Karen and Dawn, Manchester, UK Why do we need food? – Milo, age five, Cowes, Australia"
"The ancient city of Petra is famous for its spectacular ravines which have been the backdrop to Hollywood movies and countless tourist brochures. However, nearly 4,000 visitors  to the Jordanian ruins narrowly avoided being swept away recently when intense rainstorms turned the dry channels into raging torrents.  Further north, near the capital Amman, 13 people died in similar flash floods later the same day as rainwater inundated farmland along the dry ephemeral,  river channels – referred to as wadis – sweeping away roads and bridges. These recent events follow an even worse loss of life in October when more than 20 Jordanian schoolchildren were killed by flash floods in a wadi near the Dead Sea, leading to governmental resignations and widespread outcry. This is not a problem limited to Jordan. In the last year alone, flash floods have killed people in the US, Italy, Israel and elsewhere. The frequency, spread and scale of these tragedies begs the question: why do so many people, in so many places, seem so unprepared? We do not lack an understanding of flash flooding in arid or semi-arid regions. Intermittent, high volume flows are common in deserts, where infrequent but intense rain falls onto dry, steep hillsides. These drain down dry channel networks, eroding, transporting and depositing large quantities of sediment, ranging in size from microscopic silt particles to enormous boulders. Flash floods are the norm, not the exception in drylands.  The latest events in Jordan centre on a 2,000km² area, referred to as the Wala watershed, just south of Amman. The region has been impacted by the arrival of refugees from the Syrian civil war and earlier migration from Iraq, just to the east, in the 1990s and 2000s.  Wadi Wala was also dammed in 2002 with the aim of pumping stored water into underground aquifers to secure water supplies for the growing populations in the capital, and expanding agriculture and industry. But the drainage network transports so much sediment that the 9m cubic metre reservoir is rapidly filling up. Work is already under way to raise the dam to increase the reservoir capacity.  Agricultural intensification, uncontrolled development and population growth are blamed by some for the recent human costs of the floods. Others have claimed that increasingly intense rainstorms are to blame. Indeed, both factors have significant implications, not just for flood hazards, but also for the management of Jordan’s scarce and increasingly stressed water resources. We argued in a 2016 paper that detailed hydrological modelling of catchments, despite the lack of high quality data in Jordan, is the only way to untangle the complex interactions between human development – urbanisation, agricultural intensification – and increasing extremes of rainfall under climate change. Our latest modelling suggests that intensifying agriculture is playing a role in modifying the hydrology of the area. However, a far more significant effect 
on the volume of water discharged, the ferocity of floods, and the amount of sediment transported is the management and coordination, or lack thereof, of water throughout the region.  A UN FAO report endorsed several small measures, from soil drilling and earth bunds to retain water at the field scale, up to check dams and small scale flood storage distributed along smaller tributaries, to slow the flow and prevent surges accumulating downstream. However, modelling and modest infrastructure projects alone will not solve the problem. Communication and education of public and authorities, informed by the outcomes of said modelling is the real way to achieve the level of cooperation required to reduced the scale of these floods.   Unfortunately, flash flooding – going from long periods of no water at all to sudden, brief, extreme hazard – plays to the same negative psychological conditioning as other intermittent environmental risks. Local peoples are reluctant to invest in protection against a threat which is rarely manifest.  Studies from around the globe highlight the importance of cultural memory and language in determining our resilience and preparedness in respect of flooding. In Chile, subtle changes in the sense of words used locally to describe intermittent or ephemeral channels are seen to correlate with variations in the engineering of nearby infrastructure and the organisation of development. In Sheffield, UK, recent work has identified the importance of cultural memory in determining flood resilience among businesses and districts affected by major floods in 2007. In Jordan, many commentators have cynically noted the contrast in fatalities between the international tourists at Petra, protected by a hi-tech flood warning system, and the rural local population in Wala. Away from the tourist hotspots, what is needed is not sirens and evacuation plans but education and integrated watershed management. Deaths in flash floods and dwindling water resources are in fact both symptoms of a fundamental disconnect between human development and natural systems, in some of the most stressed and precarious environments on Earth."
"
Share this...FacebookTwitterIn her latest panic attack, teenage Swedish climate activist Greta Thunberg – citing the Guardian –  once again appeared to be proclaiming the end of the world was a step closer when she tweeted Antarctica has set a new record high temperature:

20,7°C on Seymour Island off Antarctica… https://t.co/OiIdlQIl6A
— Greta Thunberg (@GretaThunberg) February 13, 2020

Two new warm records
According to the Guardian, “The 20.75C logged by Brazilian scientists at Seymour Island on 9 February was almost a full degree higher than the previous record of 19.8C, taken on Signy Island in January 1982.”
That reading, the Guardian reports, follows the February 6 record of 18.3C recorded at the Argentinian research station, Esperanza measured.
As is the case with most alarmists, every warm single datapoint anomaly gets uncritically accepted with open arms as solid evidence of man-made global warming while cold trends get dismissed or downgraded as “natural variability”.
Seymour Island has been cooling for over a quarter century
So we have two recent warm records set at and near the Antarctic peninsula over the past week or so and that means the region there is heating up, alarmists like Greta and the Guardian want us to believe. But what are the real TRENDS there? Do the 2 recent warm records mean the region is heating up.
Looking at official data from NASA, it turns out that warming isn’t true. And because climate is always changing, the temperature in the region in question has also not remained completely steady. The only possibility left? COOLING.
Seymour Island, also known as Marambio Island is an island in the chain of 16 major islands around the tip of the Graham Land on the Antarctic Peninsula. What follows is a plot of the mean annual temperature measured at Seymour Island – based on NASA data – going back to the time all the global warming predictions began in earnest:


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Contrary to that implied by The Guardian and Greta, the island has in fact cooled a bit during the period, despite the warm spike of 2016.
13 of 13 Antarctic Peninsula/island stations cooling
Next we look at the Antarctic Peninsula, which global warming alarmists also like to have us believe is teetering on the brink of meltdown. Not long ago Japanese climate blogger Kirye posted a chart showing the mean annual temperatures of 13 stations located there – going back two decades.
For alarmists, the results turn out to be terribly inconvenient. The following map shows the location of the stations:

The following chart shows the plots of the mean annual temperature of the 13 stations, using NASA Version 4 unadjusted data:

13 of 13 Antarctic Peninsula and nearby island stations show cooling over the past 21 years. There hasn’t been any warming there so far this century. Data source: NASA GISS, Version 4 unadjusted. 
Natural ocean cycles
Buried near the end of the Guardian article is mention of the real reason behind Antarctic temperature trends:
Scientists on the Brazilian Antarctic programme say this appears to be influenced by shifts in ocean currents and El Niño events: “We have climatic changes in the atmosphere, which is closely related to changes in permafrost and the ocean. The whole thing is very interrelated.”
Indeed it is. Very likely in ways the climate alarmists prefer not to mention.
Share this...FacebookTwitter "
"The world’s tropical oceans are suffering turbulent times. Dire predictions of yet more disastrous coral bleaching episodes have been released, placing the very future of wonders like the Great Barrier Reef in danger. Without global action to prevent runaway climate change, we as individuals are largely powerless to stop such loss. With a heavy heart we are now at a marine conservation crossroads. All paths look precarious at best. Do we continue to chase the possibility of developing a means to save and restore our coral reefs as the global climate becomes more and more inhospitable? Or do we follow a new path? One that takes stock of what can be saved, and develops viable solutions that maximise the potential of the oceans.  In Australia, the government has granted AUS$444m to the Great Barrier Reef foundation to find an innovative solution to the problems faced by its namesake coral reef. Proposals for reef shading, spreading of lime (to reduce seawater acidity), and “coral IVF” are all being investigated. In addition, major donor funding totalling US$86m has been given to the 50 Reefs programme aimed at saving some of the world’s most resilient coral reefs from climate decimation. But these aren’t solutions to the problem, they are merely sticking plasters. Despite lots of diverse work being carried out to save them it’s looking increasingly likely that many coral reefs are going to be lost  and we need to start addressing the problems that this will cause. The loss of the world’s coral reefs will result in hundreds of millions of people needing alternative supplies of food as these fisheries habitats rapidly decline in productivity. We are already seeing movement away from coral reef fisheries. In eastern Indonesia, research has found fishers are increasingly dependent on alternative habitats such as seagrass meadows and mangrove forests, instead of degraded and damaged coral reef habitats. By moving away from coral reefs, this highly intense fishing is being focused onto smaller areas. This decimates fish stocks even further, destroying the ecological balance of the tropical seascape. Much commendable conservation and research work is being done to make coral reefs more resilient to climate change. But their global functioning in support of fisheries is collapsing now, and we need current, real world solutions to this as well as blue sky innovation. In our newly published article we argue that there is an urgent need to take heed of the warnings of a future widespread decline in the productivity of coral reef fisheries, and broaden the focus of tropical marine conservation.  Burying our heads in the sand as fisheries move and their negative impact is concentrated elsewhere can no longer be an option for marine conservation. The world needs to look at how conservation efforts can maximise fisheries resources and sustainability throughout the tropical marine seascape. This is not about moving away from coral reef conservation, it’s about taking a much more holistic view of tropical marine conservation. We need to now think about how we can ensure people can continue to sustain themselves into the future. All too often tropical conservation ignores anything that isn’t coral. A rapidly changing climate means that such tunnel vision conservation is no longer viable. Looking to other components of this seascape, practicable conservation opportunities do exist to develop sustainable ways to respond to increased resource use, such as fishing gear management. The problems faced by seagrasses, for example, are immediately related to catchment management and coastal development rather than global climate change. These and other drivers are largely manageable, and threats can be reduced. Yet they are largely off the marine conservation radar. Although habitats like seagrass meadows are sensitive to a changing climate too, predicted scenarios suggest a much brighter future for them than for coral reef systems. We have global evidence of the value of seagrass ecosystems in supporting fisheries production and helping to sequester carbon dioxide from our atmosphere. Targeted action now could restore and protect them in to the future. Coral reefs rightly have had a lot of attention but we believe that now is the time for global conservation efforts to look beyond the reefs and focus on other vital areas of ocean conservation too."
"Hundreds of Amazon employees defied corporate policy to publicly criticize the company for failing to meet its “moral responsibility” in the climate crisis. More than 340 tech workers at Amazon used the hashtag #AMZNSpeakOut in public statements that condemn the company for not taking sufficient action on the climate crisis.  The action comes in direct opposition to an Amazon policy barring employees from speaking about the company’s business without prior approval from management. That policy was introduced after employees vowed to participate in the global climate strikes of September 2019. “Every person who shared a statement had to decide for themselves that whatever the consequences, they needed to stand up for what they felt was right,” Victoria Liang, a software development engineer at Amazon who joined the public action, said. “The climate crisis is just that urgent. We just couldn’t be silenced by these policies on issues of such moral weight.” Employees at Amazon have increasingly criticized the company in recent years for its contracts with large oil and gas firms. In spring 2019, more than 8,700 employees signed an open letter to the CEO, Jeff Bezos, urging him to take bolder action on climate change. The presidential candidates Bernie Sanders and Elizabeth Warren have also offered support of employees for speaking out. The employee activism is part of a broader trend in the tech industry of employee walkouts and protests against corporate policies. Google workers staged internal protests over sexual harassment policies in 2018 that continued into 2019 and gig workers at Instacart and Uber have organized strikes to fight for better pay and benefits. In June 2019, workers at the online furnishings retailer Wayfair walked off the job to oppose the company’s contracts with detention centers for immigrants. An Amazon spokeswoman said the company was aware of the employee actions. She added that the external communication policy had been updated in spring 2019 but was not directed at any one group of employees. Amazon has pledged to reach net-zero carbon by 2040 and 100% renewable energy by 2030. “While all employees are welcome to engage constructively with any of the many teams inside Amazon that work on sustainability and other topics, we do enforce our external communications policy and will not allow employees to publicly disparage or misrepresent the company or the hard work of their colleagues who are developing solutions to these hard problems,” she said. In January, at least three employees said they were threatened with termination for speaking publicly about environmental issues, stoking further protests against the new policy. “I’m proud to work at Amazon, but policies that silence employees who are challenging us to do better runs counter to our own leadership principles,” said Nolan Woodle, an associate contracts manager at Amazon. “When there is an issue of such importance, we have to be able to talk about it. Silencing employees is simply not the right approach.” Meanwhile, other companies appear to be responding to employee unrest. In January 2020, Microsoft announced it would be “carbon negative” by the end of the decade after a number of global actions orchestrated by employees. Amazon’s big tech counterpart Google also appears to be making public responses to the organizing. On the job platform LinkedIn on Monday, someone listing himself as a hiring manager at Google left a post that appeared to attempt to recruit defectors from Amazon to the company. “I recognize your courage and bravery to speak out about climate change,” he wrote, linking to a post on how Google is “working to battle climate change”. A Google spokeswoman said the company was not aware of the post and that it was made by a recruiter acting on his own initiative. The company did not respond to accusations that Google, too, has fired employees over organizing."
"

I found the full page color advertisement on page 5E of the Sunday Enterprise Record quite interesting.
It lists a number of environmental reasons why the M&T Baldwin Gravel mine would be a good thing, not the least of which is the reduction of the number of truck miles traveled in Butte County due to trucking in building gravel from outside the county, and the reduction in gasoline burned and GHG’s avoided helping “Global Warming”.
And then there’s the angle that this mine pit will fill with water, and create an animal habitat just like the Teichert Ponds have done when it was used as a borrow pit to construct Highway 99 overpasses. There we have a clear example of how a lowly gravel mine got turned into a nature habitat, and there was no help or “kickstart” to nature as the M&T operators are proposing for their pits destined to be ponds.

It will be interesting to see how opponents argue against the project with these environmental assets it offers.
Here’s how Chico Creek Nature Center described the Teichert Ponds for a walking tour they sponsored of them:

April 8, Sunday – Teichert Pond/Birding By Ear – Trip co-leaders: Scott Huber and Dawn Garcia. Time: TBD. Chico’s Hidden Wetland – the view from Rte 99 is enticing; a large pond surrounded by tules and ringed with willows and oaks. Trip leader, Scott Huber, will direct you through the maze of streets that lead to the heart of Teichert Pond(s). Once in, you\’ll delight in the diversity of avian life found in this \’secret wetland\’ just blocks from downtown Chico. Co-leader Dawn Garcia, an expert at identifying local bird species by ear, will point out audio clues for ID\’ing species seen and perhaps some that are only heard! Expect at least three woodpecker species, a number of flycatchers, numerous sparrow species, a few raptors (possibly a Great Horned Owl), at least three warbler species, some ducks, geese and shorebirds and with any luck, some surprise migrants! Consider picking up one of the great “birding by ear” CD sets to prepare you for this trip: Bird Songs of California (Keller – Cornell Lab of Ornithology) or Western Birding by Ear (Peterson Field Guides). 
So what’s all the fuss about over this gravel mine?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea687dd57',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

_The_ Current Wisdom _is a series of monthly articles in which Patrick J. Michaels, director of the Center for the Study of Science, reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press._   
  
Could President Obama have picked a worse time to announce his Climate Action Plan?   
  
Global warming has been stuck in neutral for more than a decade and a half, scientists are increasingly suggesting that future climate change projections are overblown, and now, arguably the greatest threat from global warming—a large and rapid sea level rise (SLR)—has been shown overly lurid (SOL; what did you think I meant?).   
  
You hardly need an “action plan” when there is so little “action” worth responding to.   
  
As I frequently discuss the lack of warming and the decreases in the estimates of future climate change, I’ll focus here on new scientific findings concerning the potential for future sea level rise, interspersing a little travelogue.   
  
Projections of a large sea-level rise this century depend on rapid ice loss from Greenland and/or Antarctica. Yes, as ocean waters warm, they expand, but this expansion-induced rise is pretty well constrained and limited to being about 6 inches plus or minus a couple of inches by century’s end. And the contribution from melting glaciers/ice in other parts of the world (not counting Greenland and Antarctica) is even smaller, maybe 2-4 inches. So that adds up to about 8-12 inches of sea level rise by the year 2100—not much different than that which has already occurred over the past century. This is hardly catastrophic.   




So getting a good handle on the contributions from Antarctica and Greenland is essential if you want to develop a reasonable expectation for the future. Lacking a good handle leads to unreasonable projections.   
  
Here is an example of the latter.   
  
A breathless passage from the book version of Al Gore’s _An Inconvenient Truth_ :   




I flew over Greenland in 2005 and saw for myself the pools of meltwater covering large expanses on top of the ice. …These pools have always been known to occur, but the difference now is that there are many more of them covering a far larger area of ice. …In Greenland, as in the Antarctic Peninsula, this meltwater is now believed to keep sinking all the way down to the bottom, cutting deep crevasses and vertical tunnels that scientists call “moulins.”   
  
When water reaches the bottom of the ice, it lubricates the surface of the bedrock and destabilizes the ice mass, raising fears that the ice mass will slide more quickly towards the ocean.   
  
…If Greenland melted or broke up and slipped into the sea—or if half of Greenland and half of Antarctica melted or broke up and slipped into the sea, sea levels worldwide would increase by between 18 and 20 feet.   
  
Tony Blair’s advisor, David King, is among the scientists who have been warning about potential consequences of large changes in these ice shelves. At a 2004 conference in Berlin, he said: THE MAPS OF THE WORLD WILL HAVE TO BE REDRAWN. [all caps in original]



Gore went on to include page after page of now and then maps of the world’s major cities after a sea level rise of 20 feet (of course, assuming no adaptive measures put in place).   
  
But Gore’s disaster mechanism has been shown to be impotent and ineffective. In fact, a collection of recent papers published in the peer-reviewed scientific literature basically dispels all myths foretelling a large sea level rise this century coming from ice loss on Greenland. Recent research on Antarctica largely does the same.   
  
First off, research by Sarah Shannon and 18 co-authors takes direct aim at Gore’s mechanism in their paper “Enhanced basal lubrication and the contribution of the Greenland ice sheet to future sea-level rise.” Here is what they conclude, in direct opposition to Gore’s claims:   




Although changes in lubrication generate widespread effects on the flow and form of the ice sheet, they do not affect substantial net mass loss; increase in the ice sheet’s contribution to sea-level rise from basal lubrication is projected by all models to be no more than 5 percent of the contribution from surface mass budget forcing alone.



And “no more than 5 percent” turns out to be, by the year 2100, somewhere between 0 and 3 millimeters, or in English units, a tenth of an inch or less. Some disaster. Certainly “18 to 20 feet” is a lot scarier, but it is just plain wrong.   
  
Another new study looks at (among other things) the sea-level rise effect of the acceleration of the discharge rate of those glaciers across Greenland, which directly empty out into the sea. Heiko Goelzer and fellow researchers found that after an initial bump in the contribution to sea level rise as these glaciers retreat, once they draw back to the grounding line—the point where the outlet glaciers stop floating and instead rest on the bedrock—the loss rate slows dramatically. They conclude that the contribution from dynamical changes to the flow rate of outlet glaciers may contribute between 8 to 18 millimeters of sea level rise by the year 2100. That is about a quarter to three-quarters of an inch. Again, not even close to a disaster.   
  
Here’s your climate news scoop of the day: The highest discharge-volume glacier in the entire Northern Hemisphere—Greenland’s Jakobshavn—has grounded, which is really going to put the kibosh on the Greenlandic myth. Here’s a picture I took from my own Greenland sojourn* earlier this summer. It shows the southern end of Jakobshavn glacier, on June 24.   






  
  
_Looking south along the calving front of the Jakobshavn glacier, June 24, 2013. Photo by Patrick Michaels._   
  
You can see that it is grounded over most of its humongous 10-kilometer face. The calved ice drops off in smaller chunks, dramatically reducing the size of the bergs that will eventually float down the spectacular Ilulissat Icefjord.   
  
A small portion of the glacier was perhaps still floating when I was there, right near the north end, as indicated by a reduction in the height of the calving face, as shown in this photo.   






  
  
_Looking north along the calving front of the Jakobshavn glacier, June 24, 2013. Photo by Patrick Michaels._   
  
As a tidewater glacier, Jakobshavn regularly calves some tremendous icebergs that take a couple of years to make their way down the 35-mile fjord, only to ground on the terminal moraine near Ilulissat (and conveniently located in view of the Hotel Arctic’s live webcam, here). Because the glacier has largely grounded, these bergs are not the giants that they once were (although some sizeable icebergs continue to be produced in the early summer as the floating ice tongues established in the winter break up). _Hie thee to Ilulissat! The sooner the better!!_ Presumably some views through the webcam (which was near my room) will convince you!   
  
(The terminal moraine near Ilulissat dates to the end of the Little Ice Age—meaning that the productive fishery at the mouth of the fjord was probably inaccessible. Farther south, such an expansion of ice no doubt covered much of the Viking pastureland, chasing them to places elsewhere (including North America?)).   
  
A third new study examined the direct contribution of changes in the surface mass balance (SMB) of Greenland (that is, total run off from ice melting minus total gains from enhanced snowfall) to future sea level rise (they did not consider ice loss from glacier speed). In their study “Estimating the Greenland ice sheet surface mass balance contribution to future sea level rise using the regional atmospheric climate model MAR,” Xavier Fettweis and colleagues found that declines in the SMB by the year 2100 led to somewhere between 2 centimeters and 13 centimeters of sea level rise, depending of the carbon dioxide emissions scenario used in their model. That’s somewhere between 1 and 5 inches (and these projections are based on climate models which, according to the latest science, overestimate future warming by some 70 percent).   
  
So adding all of these effects up—basal lubrication, glacial dynamics, and enhanced melting—the total global sea level rise by the end of the 21st century originating from Greenland projected by the latest, greatest scientific studies averages out to be maybe 3 to 4 inches. Ho hum.   
  
Like I said, sea level rise disaster scenarios that are dreamed up by Greenland shedding large volumes of ice ( _a la_ Al Gore, Jim Hansen, etc.) are SOL.   
  
  
  
**References:**   
  
Fettweis, X., et al., 2013. Estimating the Greenland ice sheet surface mass balance contribution to future sea level rise using the regional atmospheric climate model MAR. _The Cryosphere_ , **7** , 469-489.   
  
Goelzer, H., et al., 2013. Sensitivity of Greenland ice sheet projections to model formulations, Journal of Glaciology, 59, 733-749, doi:10.3189/2013JoG12J182   
  
Shannon, S., et al., 2013. Enhanced basal lubrication and the contribution of the Greenland ice sheet to future sea-level rise. _Proceedings of the National Academy of Sciences_ , doi:10.1073/pnas.1212647110   
  
  
  
*Get that ticket to Greenland pronto! Travel hint: the shortest route is through Reykjavik on Iceland Air and then on Air Iceland to Ilullisat. Reserve in advance and you can get a Saga Class (business) seat for pretty cheap compared to the Majors (which will take you all the way to Copenhagen and then backtracking on Air Greenland’s A330 to Kangerlussaq (Sondre Stromfjord) and an additional connection to Ilulissat, i.e. $$$$).


"
"

 _Global Science Report is a weekly feature from the Center for the Study of Science, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
Whenever the topic of rising seas comes up, we point out that Antarctica is expected to gain mass through enhanced snowfall in a warmer climate, and therefore its contribution to global sea level rise should be negative—that is, the water locked up in the added snowfall there will act to reduce the level of the globe’s seas. The models used by the Intergovernmental Panel on Climate Change (IPCC) in their 2007 Fourth Assessment Report project the sea level reduction from this mechanism by the end of the 21st century to amount to somewhere between 2 cm and 14 cm (roughly 1 to 6 inches). While this is not a lot, the main point is that Antarctica is not expected to be a contributor to rising seas as the climate warms. Without a large contribution from Antarctica, we will not approach alarmist projections of a meter-plus of sea level rise by century’s end.   
  
Up to now, though, Antarctica has not exactly been with the program.   
  
Instead of gaining mass through increased snowfall, there have been indications that Antarctica is losing ice (contributing to sea level rise) as ice discharge from its coastal glaciers exceeds gains from snow increases (which have been hard to find). One has to wonder whether Antarctica, contrary to expectations, will continue to lose mass and become an important contributor sea level rise, or whether the projected increases in snowfall have just not yet reached a magnitude sufficient to offset the loss from glacial discharge.   
  
Things are starting to change down there.   




The research that has gotten the most attention on the subject of Antarctic mass balance has been based on observations made by the Gravity Recovery And Climate Experiment (GRACE) satellite. This orbiter senses changes in gravity (i.e., mass) which can be caused by increasing snow and ice loads over the continent. One key piece of information which must be factored into the calculations of ice mass change is the change in the underlying geologic formations, which are still rebounding from enormous amounts of ice lost after the end of the last ice age. This geologic motion, known as the glacial isostatic adjustment (GIA), is largely modeled rather than directly observed. Our level of knowledge (or lack thereof) of the true GIA adds a sizable amount of uncertainty to GRACE-based estimates of the ice mass changes over time in Antarctica (and Greenland, the northern hemisphere’s cheap imitation of Antarctica).   
  
In a widely cited finding, Velicogna (2009) reported that Antarctica was losing ice at a rate of about 104 gigatons per year (Gt/yr) during the period 2002–2006, increasing to a loss rate of 246 Gt/yr during 2006–2009 (about 374 Gt of ice are equivalent to 1 mm of sea level). Rignot et al. (2011) also found an acceleration of ice loss there, increasing from a loss of about 209 Gt/yr (in 2003-2007) to about 265 Gt/yr from 2007 to 2010. However, Wu et al. (2010) argued that the GIA model used in these previous studies is incorrect, and that when a more accurate GIA model is incorporated in the GRACE-based ice mass change calculations, Antarctica was only losing about 87 Gt/yr during the period 2002–2008.   




Support for the GRACE-based calculations comes from the general agreement between the GRACE numbers and those calculated from studies of changes in the grounding lines of coastal glaciers and the ice flow across those grounding lines in association with the other aspects of the mass balance. This method is known as the Input-minus-Output Method (IOM). The IOM estimates of the average ice loss from Antarctica over the past several decades (1992–2007) lie somewhere around 136 Gt/yr, in rough agreement with the GRACE-based estimates. However, the IOM is also subject to a lot of uncertainty. An attempt by Zwally and Giovinetto (2011) to reduce the uncertainty and increase the accuracy resulted in an IOM-based estimate of a loss of only 13 Gt/yr over the same 18-yr period and led the researchers to conclude that: 



Although recent reports of large and increasing rates of mass loss with time from GRACE-based studies cite agreement with IOM results, our evaluation does not support that conclusion.



It seems that as the calculations and derivations are improved, the amount of ice mass that Antarctica is supposedly losing gets less and less.   
  
Or perhaps it isn’t losing any mass.   
  
Using a set of observations from a series of satellites that have been in orbit since 1992 and that measure changes in the height of the surface of the ice (ICESat), NASA’s Jay Zwally and colleagues (2012) report that Antarctica is gaining mass. Zwally recently presented his findings to a workshop of the Ice-Sheet Mass Balance and Sea Level expert group of the Scientific Committee on Antarctic Research and the International Arctic Science Committee. According to his abstract, Zwally reported that “During 2003 to 2008, the mass gain of the Antarctic ice sheet from snow accumulation exceeded the mass loss from ice discharge by 49 Gt/yr (2.5% of input), as derived from ICESat laser measurements of elevation change.”   
  
Zwally further added, ""A slow increase in snowfall with climate warming, consistent with model predictions, may be offsetting increased dynamic losses.""   
  
So the ""global warming, leading to increased snowfall, leading to a drawdown of global sea level"" mechanism may be operating after all.   
  
A paper to soon appear in _Geophysical Research Letters_ give us another enticing look at recent snowfall changes in Antarctica. In “Snowfall driven mass change on the East Antarctic ice sheet,” Carmen Boening and colleagues from NASA’s Jet Propulsion Laboratory report that extreme precipitation (snowfall) events in recent years (beginning in 2009) have led to a dramatic gain in the ice mass in the coastal portions of East Antarctica amounting to about 350 Gt in total (Figure 1).   






Figure 1. Timeseries of snow accumulation in coastal East Antarctica (shaded region in inset).   
(Source: Boening et al., 2012)Boening et al. reported that the increase in ice mass in East Antarctica has not completely offset the loss of ice mass during the same time in West Antarctica, but as this comparison is made using GRACE data, it is hard to know just how accurate it is.   
  
Also note that a few years with a lot of snowfall does not mean that a change in the long-term snowfall rate has occurred. Nevertheless, the situation bears careful watching.   
  
Putting everything together, we conclude that many of the claims that Antarctica is rapidly losing ice and increasingly contributing to a rise in global sea levels must now be, at the very least, tempered, if not overturned entirely. Time will certainly tell. And time will also tell just how much we need to worry about future sea level rise. Currently, the answer seems to be “not overly much.”   

"
"

The good news is that the coming years hold out real promise for a new wave of market‐​oriented regulatory reforms here in the United States. Momentum on this front crested in the late 1970s and early ’80s and has been all but defunct for the past couple of decades, but that may well be about to change. The bad news is why: the reason we should be getting our political hopes up is that our economic hopes over the near to medium term are so likely to be dashed.



Let me spell out that bad news a bit. U.S. economic performance in the wake of the Great Recession of 2008-09 has been nothing short of dismal. More than three years passed before real output even returned to its prerecession level, and a return to the prerecession growth trend remains nowhere in sight. The story with employment is even worse: yes, the unemployment rate has gradually subsided, but only because the percentage of Americans actually in the work force has sunk to its lowest level since the late 1970s.



Debate still rages over how much of the economy’s continuing sluggishness reflects short‐​term, cyclical factors — in particular, a shortfall in aggregate demand or deleveraging in response to the financial crisis of 2008. It is becoming increasingly clear, however, that slower growth in output and a weak labor market are now the “new normal.” In other words, the ongoing economic slump is not just a matter of a temporary gap between current and “potential” output. Rather, the economy has suffered a decline in its potential or full‐​employment growth rate.



Consider the long‐​term trends for each of the four major components of economic growth: growth in labor participation, growth in labor skills, growth in investment, and growth in output‐​enhancing innovation. As I explained in my 2013 Cato paper “Why Growth Is Getting Harder,” those trends are now uniformly unfavorable. Average hours worked per capita have fallen since 2000. Growth in so‐​called labor quality (as measured by years of school completed) has slowed considerably. The net domestic investment rate has been trending downward for decades. And total factor productivity (TFP) growth, our best measure of innovation, has slumped again in recent years after an Internet‐​fueled surge between 1996 and 2004.



Consequently, there are strong reasons for believing that growth in the years ahead will fall well short of the long‐​term historical average. Between 1870 and 2010, growth in real (i.e., inflation‐​adjusted) GDP per capita averaged just under 2 percent a year. By contrast, recent long‐​term growth projections by top academic and government economists point to an average annual per capita growth rate in the range of 1.0 to 1.5 percent — a fairly dramatic decline from the historical trend line.



In a welcome bit of irony, such economic pessimism offers solid grounds for political optimism. Here’s the basic logic: there is an inverse relationship between the external conditions for growth, on the one hand, and the incentives for good economic policymaking on the other. When conditions for robust growth are favorable, politicians can indulge in the characteristic vices of their profession — a time horizon bounded by the next election cycle and an overriding focus on dividing the pie rather than making it bigger — and still preside over a thriving economy. When, however, times get tougher, politicians must up their game or else economic performance will suffer. In the latter event, the poll numbers of incumbents start to drop, those of their challengers start to rise, and thus opportunities for policy change improve.



Now, policy change could well proceed in the wrong direction and produce worse results than the status quo. But over the past several decades at least, the general pattern around the world — as documented in the annual _Economic Freedom of the World_ reports and similar sources — has been for economic policies to move toward less government control and greater reliance on market competition. And usually, progress in liberalization has been spurred by disappointing economic performance.



If this general pattern holds in the present case, then a protracted period of sluggish growth should open a window of opportunity for pro‐​market, pro‐​growth reforms here in the United States. In sunnier times, many bad policies widely understood to create obstacles for growth are left undisturbed because the political price to be paid for changing them doesn’t seem worth it: why borrow trouble by attacking policies with powerful defenders if things are going OK anyway? But when the economic climate worsens, politicians come under increasing pressure to do something.



 **OPPORTUNITY FOR REFORM**  
Here then is the challenge for supporters of free markets: how can we best take advantage of this window of opportunity? During good times, we are forced to argue that, even if the overall economy seems to be performing well, it could be doing even better with appropriate policy reforms. Now, by contrast, we can reframe our case with a much more compelling sense of urgency: if we do not make difficult but necessary reforms, the economy will perform much worse than in the past. Since people are typically loss averse (they are more concerned about losing what they have than not getting what they want), the growth slowdown makes our case much stronger and more persuasive. But once we reframe our argument, what policy agenda best fits the new frame?



There is no shortage of possible reforms to include. For evidence I refer you to _Reviving Economic Growth_, a new Cato ebook that I edited. The book is a collection of essays by 51 prominent economists and policy experts, all of whom were asked to offer suggestions for improving the U.S. economy’s long‐​term growth outlook. Although a number of reform ideas come up repeatedly, what is striking about the collection is just how wide‐​ranging and varied the proposals are. Which should not be at all surprising: fiscal and regulatory policies affect the allocation of resources and the climate for innovation along countless different margins, and thus the potential levers for improving overall economic performance are similarly numerous and diverse.



In particular, the major economic policy debates that have dominated Washington and the nation’s attention in recent years — the trajectory and composition of federal spending, the level and structure of taxation, health care policy, regulation of the financial sector, what to do about illegal immigration, and climate change and environmental regulation more generally — have important implications for growth. Yet precisely because these debates have already found the spotlight, they are unlikely to supply policy ideas that can take advantage of the political opportunity that the current growth slowdown affords. On all of these hard‐​fought fronts, battle lines are already clearly drawn and often reflect differences over goals and priorities other than growth. Moreover, opinions are highly polarized along partisan and ideological lines, which in the current political environment is often a recipe for stalemate and gridlock.



What we should be looking for, then, are policy ideas that are not already the subject of high‐​profile, politically polarized debate. America’s growth slowdown is a new problem, and policy responses that address that problem are more likely to gain traction if they are not recycled ideas originally put forward to address other problems. And if a policy idea is already clearly associated with either the left or the right, in today’s highly contentious environment it is all but guaranteed that the other side will fight tooth and nail against it — which makes progress of any kind difficult in the absence of large congressional majorities and unified partisan control of the White House and Congress.



Meanwhile, of course, the items on this new policy agenda need to be effective remedies for slow growth. Since innovation is the ultimate source of long‐​term growth in advanced countries at the technological frontier, we should focus especially on policy reforms that can facilitate the introduction and spread of new ideas. In other words, we should target policy barriers that inhibit entrepreneurship and the reallocation of resources through competition and “creative destruction.”



 **REFORMING REGRESSIVE REGULATION**  
In a new Cato White Paper with the curious title of “Low‐​Hanging Fruit Guarded by Dragons,” I take all these factors into account and propose a pro‐​growth reform agenda that focuses on regulatory policies whose primary effect is to inflate the incomes and wealth of the rich, the powerful, and the well established by shielding them from market competition. To apply a convenient label, let’s call these policies “regressive regulation” — regulatory barriers to entry and competition that work to redistribute income and wealth up the socioeconomic scale.



In the paper I identify four main areas of regressive regulation: excessive monopoly privileges granted under copyright and patent law, restrictions on high‐​skilled immigration, protection of incumbent service providers under occupational licensing, and artificial scarcity created by land‐​use regulation. Space constraints prohibit an in‐​depth discussion of those policies here; for details I refer you to the paper. Here I will simply offer some general observations about why targeting these policies seems to me to be the most promising strategy for reversing the growth slowdown — and for taking advantage of the growth slowdown to revive political momentum for economic freedom.



At first blush, these four policy areas seem completely unrelated. They cover highly disparate subject matters, they are administered at different levels of government, and they feature widely varying forms of regulatory apparatus. Notwithstanding all these obvious differences, there are also deep and important similarities. All the policy areas feature regulations that erect explicit barriers to entry — whether in the economist’s sense of barriers to market entry, or in the literal sense of barriers to geographic entry. Copyright and patent laws and occupational licensing limit who can engage in particular kinds of commercial activity; immigration laws and zoning regulations limit who can enter or do business within a designated geographic area.



All of these entry barriers undermine economic growth by restricting vital inputs to innovation. Excessive copyright and patent protections restrict the recombination of ideas that is the essence of innovation by making some ideas artificially inaccessible. Immigration laws restrict the inflow of highly skilled individuals who are disproportionately entrepreneurial and innovative. Occupational licensing restricts the formation of new businesses, which are frequently the vessels for new products or new production methods. And zoning restricts urban density, a vital catalyst for the innovative recombination of ideas.



Finally, all these policy domains have similar distributional consequences: all of them redistribute income and wealth to the well‐​off and privileged. Copyright and patent laws pinch consumers for the benefit of huge corporations. Immigration laws expose America’s lowest‐​skilled workers to intensifying competition from foreign‐​born workers while shielding high‐​skilled workers from equivalent competitive pressures. Occupational licensing boosts the earnings of protected incumbents by restricting supply, especially in higher‐​income professions. And zoning gives windfall gains to wealthy landowners.



 **LEFT-LIBERTARIAN SYNTHESIS**  
In all likelihood because of these underlying similarities, none of these policy areas have become zones of ideological or partisan conflict. To be sure, proper policy is vigorously debated in all these areas, but the contending sides are not divided along left‐​right or Republican‐ Democratic lines. In striking contrast to the polarization and gridlock that now dominate most national policy debates, opposition to regressive regulatory controls has brought together politicians and policy experts from across the political spectrum. Thus, in the field of intellectual property, Nancy Pelosi (D-CA) joined forces with Darrell Issa (R-CA) and Ron Paul (R-TX) to oppose the Stop Online Piracy Act, a failed legislative effort to toughen criminal penalties for copyright violations. Among policy experts, leading critics of copyright and patent law excesses include progressives Lawrence Lessig and Dean Baker and libertarians Tom Bell and Jerry Brito.



With regard to high‐​skilled immigration, a number of bipartisan reform bills have been introduced in recent years. To take a recent example, in January 2015 a group of six senators, including Orrin Hatch (R-UT), Mark Warner (D-VA), and Marco Rubio (R-FL), introduced the Immigration Innovation Act to boost the numbers of both temporary and permanent visas for highly skilled workers. And among policy experts, scholars from the libertarian Cato Institute and the progressive Center for American Progress supported the most recent comprehensive immigration legislation passed by the Senate in 2013. As to occupational licensing, the Obama administration’s latest budget contains a provision to nudge states toward reform; furthermore, this past summer the administration released an excellent report that makes the case for deregulation in this area.



Meanwhile, in July 2014, Rep. Paul Ryan (RWI) released a widely discussed plan for combating poverty. And in the section on regulatory reform, Ryan singled out occupational licensing laws as prime examples of the “regressive regulations” that too often constrict economic opportunity for the least advantaged. Among policy experts, Alan Krueger of Princeton University, who served as chairman of the Council of Economic Advisers under President Obama, is a leading critic of these regulatory restrictions, while the libertarian Institute for Justice has a long track record of challenging and overturning licensing rules in court.



Zoning is a local issue that has long been thought to have only local consequences, so to date it has not attracted much attention from Washington policymakers. Among policy experts, though, the pattern of support for reform across ideological dividing lines holds here as well. Edward Glaeser, who in addition to teaching at Harvard is affiliated with the libertarian‐​leaning Manhattan Institute, is among the nation’s leading critics of current land‐​use regulation. Another prominent critic is Matthew Yglesias of Vox, who wrote his book, _The Rent Is Too Damn High_ , while he was working for the Center for American Progress.



 **PUBLIC INTEREST VERSUS VESTED INTERESTS**  
It’s not simply the case that one can find policy experts on both sides of the ideological spectrum who support reform of these regressive regulatory policies. More than that, it’s very difficult to find disinterested policy experts anywhere on the spectrum who support the status quo. Certainly, there are strong defenders of both intellectual property protection and zoning, but even in their ranks you will find recognition that current policies are seriously flawed. Thus, the economist Carl Shapiro, a prominent supporter of patents generally, has written, “[While] there is no doubt that the patent system taken as a whole plays an important role in spurring innovation, the general consensus is that the U.S. patent system is out of balance and can be substantially improved.” In similar fashion, the economist William Fischel, who has written sophisticated defenses of zoning, acknowledges that its exclusionary impact has increased since 1970 and that the “social and economic costs” of contemporary land use regulation are “not trivial.” As far as high‐​skilled immigration restrictions and occupational licensing are concerned, it is difficult to find any scholar who has anything nice to say about the current state of either.



This combination of qualities — negative impact on entrepreneurship and innovation, absence of political polarization, and an intellectual consensus in favor of reform — makes regressive regulation an especially inviting target for any campaign to enact pro‐​growth policy reforms. For all who are interested in better long‐​term U.S. economic performance, this is the “low‐​hanging fruit.” Reforming these policies is something that we know will make a positive difference, and by “we” I mean the vast bulk of disinterested experts. Yes, it is true that plucking this fruit won’t be easy, because the interest groups that benefit from the status quo are politically powerful, well organized, and highly motivated. This is the “guarded by dragons” part of the story. But knowing clearly what needs to be done, however difficult it might be, is an advantage that should not be underestimated.



Pursuing an agenda of curbing regressive regulation would allow us to open a new front in the economic policy debate. Unlike the all too‐ familiar policy disputes now ongoing, a campaign against regressive regulation would feature issues new to the national policy spotlight — especially in the case of occupational licensing and zoning, because they occur at the state and local levels and thus are typically ignored by Washington. Meanwhile, the organizing rubric of regressive regulation packages together disparate issues in a novel way and can thereby impart new energy to reform efforts in each of its constituent policy domains. This new front would look very different from the other, ongoing policy debates. Instead of the opposing forces being arrayed along the left‐​right axis, here the contest pits an expert consensus across the political spectrum against the interest groups who profit from existing policy. Instead of yet another left‐​right fight, this time the contest could be framed as a choice between the public interest and vested interests.



The idea of a left‐​right coalition to push deregulation may sound far‐​fetched, but it is not without precedent. Consider the country’s last major episode of pro‐​market regulatory reform in the late 1970s and early 1980s. During that brief period, price‐​and‐​entry regulation of airlines, trucking, and railroads was systematically dismantled; price controls on oil and natural gas were lifted; interest‐​rate caps for checking and savings accounts were removed; and the AT&T monopoly was ended, paving the way for competition in long‐​distance telephony. Those too young to remember can be forgiven for associating all of this with Ronald Reagan, but in fact Democrats and progressives played a major role. Jimmy Carter signed the legislation that deregulated airlines, trucking, railroads, and natural gas. On Capitol Hill, Edward Kennedy led the fight for airline deregulation, ably assisted by his aide Stephen Breyer. Yes, the rise of Chicago‐​school economics and especially the law‐​and‐​economics movement supplied momentum for these sweeping policy changes, but so did the activism of Ralph Nader.



History never repeats itself, but sometimes it rhymes. As in the 1970s, the U.S. economy today is delivering disappointing results. Back then the problem was “stagflation”; today we worry about a “great stagnation.” And once again, the shifting currents of political debate are bringing together unlikely allies with a common interest in reviving prosperity and a common hostility to the entrenched interests that stand in the way. With luck, contemporary reformers can follow their predecessors’ good example.
"
"Evan Flint has his feet up, at last. It is day four of the final Test between England and South Africa and, as chief groundsman, all he can do is watch as Rassie van der Dussen and Dean Elgar grind out the beginnings of a doomed rearguard action. Flint has been at the Wanderers since last spring, lured north after 10 and a half years at Newlands, where he won groundsman of the year during his last two seasons. He had had a lot to cope with. In January 2018 Cape Town officials announced that, after three years of insufficient rain, the city was three months away from Day Zero (running out of water). The visiting India team were told not to shower for more than 90 seconds, the India and South Africa sides gave a combined donation of 100,000 rand to the Gift of the Givers Foundation (disaster relief in Africa), fights broke out over water from the nearby Newlands borehole and club and schools cricket was cancelled halfway through the season. Three weeks before Day Zero, Australia and South Africa played a Test in Cape Town – subsequently overshadowed somewhat by the discovery of sandpaper down Cameron Bancroft’s trousers. Newlands is well served by boreholes but Flint remembers it as a difficult time: “We were in the thick of it but it was an opportunity to show the rest of the world that we were doing our bit, almost like a badge of honour.” He had to water the pitch; it has a very high clay content so otherwise would have cracked, but the outfield was something else. “At one point we were watering it maybe twice a week at most and it was very brown. It looked terrible but the last thing we wanted as Day Zero approached was for people to put on the telly and look at a lush cricket field.” “You know what people are like, they announce water restrictions and nobody believes it, so it was a personal decision to adhere to what the authorities said. We used borehole water but we acknowledged there was a drought on and we were playing our part.” “It made me realise that the grass doesn’t need as much water as we think. It taught me that you are doing the turf a bit of a disservice to spoon-feed it every day. I learned that it is when you water that’s important, rather than how much.” Day Zero was eventually averted but sport in South Africa is under strain in a volatile changing climate. Temperatures in the interior of the country are rising at twice the global level according to the International Panel on Climate Change and southern Africa as a whole is facing unprecedented strain. The UN World Food Programme has warned of a hunger crisis on “a scale we’ve not seen before”, blaming, largely: “The cumulative effects of climate-related natural disasters in the form of recurrent widespread droughts. The region has had only one normal rainy season in the past five years amid cyclones and persistent flooding.” Malawi has also had to contend with an influx of fall armyworms, while further north plagues of locusts currently swarm over Kenya. In South Africa the Eastern Cape is still under severe strain, with many of the new boreholes not producing the desired results because ground water levels are so low, a situation the government has called critical. Level 2 water restrictions were announced in October, even in Johannesburg, but the big difference for Flint as a groundsman is the summer rainfall in Jo’burg – in Cape Town the rain falls, or should fall, in the winter. Nor does he miss the south-easterly wind that hits Cape Town from the spring: “They don’t put that in the brochures.” The Wanderers also has the benefit of wonderful ground irrigation, every drop of rain that falls on the stadium roof or gutters runs into a reservoir at the back of the ground to be stored away. The changing climate – declining rainfall and temperature changes – has made Flint question some of groundskeeping’s most cherished ideals. “Cape Town was an eye-opener,” he says. “We have great water harvesting at the Wanderers because we are an international ground but certainly a lot more could be done in council and club facilities and there should be more research into artificial surfaces because they don’t require the maintenance, the cutting and the fuel, etc. “Of course many municipalities have got greater things to worry about but more education would help. I say this with this beautiful turf in front of me, and professional sportsmen want to play on grass, but artificial surfaces do make sense. I’m being really sacrilegious now but think about golf courses. The enormous amounts of water those things guzzle is extraordinary.” Despite all this, the climate crisis is not something that Flint finds crops up in conversation much, either in cricket or groundskeeping circles. “It’s been a bit disappointing. We meet up once a year as groundsmen and we discussed the Cape Town drought but, apart from that, not really. Once it starts raining, people go back to their own ways. But if you don’t have any water, my goodness, you can’t live.” • This is an extract from the Guardian’s weekly cricket email. The Spin. To subscribe, just visit this page and follow the instructions.   "
"
Share this...FacebookTwitterIn a new paper, atmospheric physicist Dr. Richard Lindzen summarizes the “implausible” claims today’s proponents of dangerous anthropogenic global warming espouse.
Dr. Richard Lindzen retired several years ago, and yet his immense contribution to the atmospheric sciences lives on. His research is still cited about 600 times per year.
Lindzen recently published another scientific paper (Lindzen, 2020) in The European Physical Journal criticizing the current alarmism in climate science.  Here are a few of the highlights.
1. Doubling the atmospheric CO2 concentration from 280 ppm to 560 ppm results in just a 1-2% perturbation to the Earth’s 240 W/m² energy budget. This doubled-CO2 effect has less than 1/5th of the impact that the net cloud effect has. And yet we are asked to accept the “implausible” claim that change in one variable, CO2, is predominatly responsible for altering global temperatures.
2. A causal role for CO2 “cannot be claimed” for the glacial-to-interglacial warming events because CO2 variations follow rather than lead the temperature changes in paleoclimate records and the 100 ppm total increase over thousands of years produce “about 1 W/m²” of total radiative impact.
3. Climate science didn’t used to be alarmist prior to the late 1980s. Scientists were instead sufficiently skeptical about claims of climatically-induced planetary doom. That changed during the years 1988-1994, when climate research centered on CO2 and global warming received a 15-fold increase in funding in the US alone. Suddenly there was a great financial incentive to propel alarming global warming scenarios.
4. Concepts like “polar amplification” are “imaginary”.
“The change in equator-to-pole temperature difference was attributed to some imaginary ‘polar amplification,’ whereby the equator-pole temperature automatically followed the mean temperature. Although the analogy is hardly exact, this is not so different from assuming that flow in a pipe depends on the mean pressure rather than the pressure gradient.”

Image Source: Lindzen, 2020
Share this...FacebookTwitter "
"
Share this...FacebookTwitterGlobal warming should mean that the period of May 11-15 – known as the Ice Saints in Europe – when late spring frosts often occur, would become less frosty over the years. But the opposite has been the case since 1995. 
===============================================
Why have the Ice Saints gotten colder over the past 25 years?
By Die kalte Sonne
Authored by Josef Kowatsch
(Translated and edited by P. Gosselin)
In Germany the five days from  the 11th to 15th May are the so-called Eisheilige (Ice Saints). Farmers used to understand “ice” simply as late spring frost, so these are days with frost.
Our question is how have the Ice Saints behaved at various locations across Germany the last 25 years?
POTSDAM
Let’s start with Potsdam, the capital of Brandenburg. We take the last 25 years as the period under consideration:

Figure 1: The five Ice Saints days in the state capital Potsdam. They are getting much colder. 2020 was the low point of the last 25 May months. On three days there were night frosts.
BAD KREUZNACH
At Bad Kreuznach in the Upper Rhine Valley we show a southern Germany station from from Palatinate, a warm sunny region. The DWD Germany National Weather Service weather station is located north, outside the town.

Figure 2: Bad Kreuznach in the Upper Rhine, the trend line of the present day is even slightly lower for the Ice Saints than in Potsdam.
DRESDEN


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The DWD weather station is located in the Klotzsche suburb, at the airport north of the Saxon state capital.

Figure 3: DWD station Dresden Klotzsche. Ice Saints in the present. The trendline is a little bit lower than Potsdam and the Ice Saints 2020 were among the coldest since 1997.
GOLDBACH
Goldbach near Bischofswerda in eastern Saxony, a small suburb with about 500 inhabitants.

Figure 4: The Ice Saints 2019 were clearly the coldest, source of data: Station manager Dietmar Pscheidt.
SCHNEIFELFORSTHAUS
This DWD weather station is located in the Eifel region, near the Belgian border.

Figure 5: Even in the far west of Germany, the Ice Saints outside the cities have become significantly colder. The average of the five calendar Ice Saints days in 2020 was 4.77°C – the second lowest.
NUREMBERG
The DWD Station Netzstall is located near Nuremberg, well outside the city. The missing value of the year 2000 was interpolated using the neighboring stations at Nuremberg and Nuremberg-Roth.

Figure 6: The village near Nuremberg shows a falling trend line of the five Ice Saints days. 2020 was slightly colder than 2019 and this year was clearly the coldest in the last 25 years.

Share this...FacebookTwitter "
"

Fans of Cato@Liberty may have noticed two new features from the Center for the Study of Science. These are a weekly _Global Science Report_ and a monthly _Current Wisdom_.   
  
  
While the _Wisdom_ has been a monthly feature that can be found under my publications, _World Science Report_ is new and is modelled after my original blog, _Global Climate Report_ , which is the Web’s longest running climate change blog. Our first release was September 11, 1995. The enormous archive at http://​www​.world​cli​matere​port​.com is cross‐​referenced by subject and date, and can provide valuable information on virtually any climate question. We also reserved the right to write in a humorous fashion.   
  
  
As the Center adds new affiliates, you will see much more in the new _World Science Report_ than mere climate.
"
"The UK must recruit more than 100,000 people to fill green energy roles within a decade if the government hopes to meet its binding climate targets, National Grid has warned. A report by the company found that Britain needs to fill 120,000 roles in the green energy industry by 2030 to help develop projects that can cut greenhouse gas emissions to near zero. That number is likely to reach 400,000 by 2050, when the government expects to have developed a clean energy system based on renewable electricity, green heating systems and electric vehicles. The growing need for new recruits to power the UK’s climate targets is expected to emerge as Britain faces a green energy jobs crunch over the next 10 years. The report warned that a fifth of employees in the energy sector are due to retire by 2030. The UK’s energy industry faces stiff competition from other sectors and has a narrow pipeline of young people pursuing Stem (science, technology, engineering and mathematics) qualifications to draw from, it said. Nicola Shaw, the executive director of National Grid, said: “The time is now for the sector to rise to the challenge and overcome the longstanding issues we face in recruiting a diverse workforce with the right skills to deliver on the UK’s ambitions.” The UK’s plan to cut emissions to virtually zero, and offset unavoidable pollution through carbon capture schemes, will require major investments in offshore wind, clean heating schemes, electric vehicles and carbon-capture technology. The energy industry is expected to use its role in tackling the global climate crisis to encourage young graduates into the industry. Research carried out by YouGov has found that people of all ages, from all regions across the UK, are “looking for a job with environmental purpose”. More than eight in 10 women and seven in 10 men have said they are keen to play their part in tackling climate change. Over half of adults are specifically looking to work for an organisation that is helping the UK to achieve its climate goals. The rising need for green energy jobs could bring opportunities for skilled tradespeople, engineers and other specialists “across every region of the country”, the report said. A quarter of the green jobs required will need to be based in the north of the country, according to National Grid. It estimated that more than 21,000 new recruits will be needed to complete energy projects, including an offshore wind farm off the coast of Blyth and the new subsea power cable to Norway from the north-east of England. Meanwhile, the development of carbon capture and storage in the Yorkshire and Humber region is expected to support 17,000 new jobs. Another 28,000 roles will be needed to work on more offshore wind farms off the east of England. In Scotland, green energy workers will be needed to fill more than 48,000 jobs by 2050, with a further 25,000 roles expected in Wales. Kwasi Kwarteng, the minister of state for business, energy and clean growth, said: “Tackling climate change is not only saving the planet, but is significantly boosting our economy. As we work to reduce our emissions to net zero by 2050, the UK has the potential to support 2m green-collar jobs across our world-class renewables sector, among other industries.”"
