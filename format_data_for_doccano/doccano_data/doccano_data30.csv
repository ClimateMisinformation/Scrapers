"
Share this...FacebookTwitter UPDATED LIST:   !! CLICK HERE !!
What follows is only the first orignal list of climate gates, click above for the newest list.
Climate science has produced an entire warehouse full of scandals. In fact the scandals are in such big supply that you can now get them at the House of Climate Science for a dime a dozen. No other science has produced such a huge inventory.
It reminds me of a scene in the cult film Dusk till Dawn by Quentin Tarantino where an unseemly character named Chet makes a famous sales pitch outside a certain locale. I simply substituted the obscene term, for what some consider to be the world’s oldest known commodity, with the different “gates” we’ve seen in climate science.  Here’s the new sales pitch:
Come on in gate lovers!! Here at Sleazy Science we’re slashing gates in half! Give us an offer on our best selection of gates! This is a gate blowup! All right, we got drought-gate, Gore-gate, Sting-gate, Himalaya-gate, ursus-gate, we got Amazon-gate, China-gate, Russia-gate, Dutch-gate, Toad-gate
We got data-deletion-gate, we got Overpecker-gate, Hansen-gate, we got Inconvenient-gate, Africa-gate, boot-gate, Finland-gate, we even got Freedom of Information-gate, Greenpeace-gate, Yamal-gate. Come on you want gates, come on in gate lovers! If we don’t got it you don’t want it! Come on in gate lovers!  

(Warning! Should you get the idea to watch the original Chet speech in Dusk Till Dawn, then first clear the kids and ladies out of the room.)
Anyway, that said, here’s my abridged list of gates in climate science for your future reference. There’s something there for everyone! We now got:
1. Acceleration-gate
2. Africa-gate
3. AIT-gate
4. Amazon-gate
5. Antarctic sea-gate
6. Bangladesh-gate
7. Boot-cleaning manual-gate
8. China-gate and here
9. Climate Camp-gate
10. Climate-gate
11. CRU data deletion-gate
12. Dog-ate it-gate
13. Discernable influence-gate
14. Drought-gate
15. EPA-gate  h/t Climate Depot
16. Five-star WWF-gate
17. Finland-gate
18. Flooded house-gate
19. FOI-gate
20. Fungus-gate
21. Gatekeeping-gate
22. GISS Metar-gate
23. Gore private jet-gate
24. Greenpeace-gate
25. Hansen 1930s hot-gate
26. Hansen stagecraft-gate
27. Himalaya-gate and here
28. Hockey-stick-gate and here and here WCR
29. Hollywood hypocrites-gate and Dave Matthews
30. Hurricane-gate
31. Jesus Paper-gate
32. Kilimanjaro-gate
33. Malaria-gate and here (new!)
34. Meat-gate h/t reader Catalina
35. Mega-mansion-gate
36. Met Office computer-gate
37. NASA/NCDC bad data-gate and here
38. New Zealand-gate
39. NOAA adjustment-gate and here, and here
40. NOAA/GISS data selection-gate and here
41. NYT alarmism-gate and here
42. Overpeck get rid of MWP-gate
43. Oxbourgh-gate and here (bishop hill)
44. Pachauri-gate and here and here
45. Peer-review-gate 1
46. Peer-review-gate 2
47. Persecute and execute-gate
48. Polar bear-gate and here
49. Porn(soft)-gate
50. Rahmstorf smoothing-gate and here and here
51. Revelle-gate
52. Russia-gate and and here video
53. Solar-gate  Spain solar-gate
54. Sting-gate
55. Student dissertation-gate
56. Surface stations-gate and here
57. Toad-gate
58. UNEP-gate
59. UN natural disasters-gate
60. Ursus-gate
61. Windmill-gate and here
62. Wikipedia William Connelly-gate h/t to rechauffementmediatique.org
63. Yamal-gate 
      Come on in gate lovers we got lots of gates coming in!  We even got gates for a penny! Gates for only a penny! Come on in!
Hopefully this list will be helpful for the observers of climate science.
Share this...FacebookTwitter "
"
I’m travelling the next few days, moderation will be spotty. After tomorrow, you may find that comments may go for 12 -18 hours or more without approval.
In the meantime, you can debunk my own “hockey stick”.

click for larger image
Thanks to all who have come by and participated with comments and ideas!
Best Regards,
Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea09b8f84',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
I don’t have a lot of time today, but I found this interesting. Commenter “Basil” has offered this for discussion. So I’m putting this up without comment on my part. See also the decadal trends table below. Have at it folks.

Click for full sized image

1979:01-1992:12
---------------------------------------------
GISS        0.000783764**     (0.094C/decade)
HadCRUT     0.000460122**     (0.055C/decade)
RSS_MSU     0.000498964       (0.060C/decade)
UAH_MSU 	1.71035E-05       (0.002C/decade)
1993:01-2001:12
---------------------------------------------
GISS        0.00174741**      (0.210C/decade)
HadCRUT     0.00147990**      (0.178C/decade)
RSS_MSU     0.00221135**      (0.265C/decade)
UAH_MSU     0.00217023**      (0.260C/decade)
2002:01-2008:1
---------------------------------------------
GISS       -0.00091450       (-0.110C/decade)
HadCRUT    -0.00270338**     (-0.324C/decade)
RSS_MSU    -0.00208111       (-0.250C/decade)
UAH_MSU    -0.00130882       (-0.157C/decade)



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0aae336',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterHere’s another excellent post by Eduardo Zorita at the Klimazwiebel.
In this BBC podcast (takes a minute or so to load), the view of green elitists is that we have casus belli. Thus democracy has to be suspended and common sense authoritarianism has to take over – just for a while, until things are put back in their proper order. The general population is just too stupid to understand it, and is only getting in the way. (Actually, and thankfully, they’re too informed and many people understand precisely what this is about).
“The situation is urgent, the world is going to hell in a handbasket – let us rescue the planet. Trust us,” we are constantly told.
I’m trying to think of a veggie or fruit that’s green outside and brown inside. The closest thing I can think of is a rotten avocado. For me it’s even disturbing that the BBC even gives equal time and weight to the green nutjobs who propose suspending democracy and taking us back to the German Democratic Republic – East Germany, behind the Berlin Wall, for those of you who may have already forgotten. “Trust us” just isn’t good enough. History shows that populations have been burned by this all too often.
The good news is that authoritarianism only works if there’s consent. But there can be no consent unless there is a genuine debate. That’s where the problem lies for the kook warmists. They’ll never win this debate, and they know it. Indeed consent has been massively eroding lately. Their science has been exposed as a hoax. They’ve lost the case and their desperation has caused them to lose any rationality they may have once had.
Update: Bishop Hill has found the perfect food staple to symbolise enviro-leftists: pistachios. Green only on the surface, brown inside, and in total, a nut throughout.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterHave Trump/Barr led the Biden campaign into a lethal trap? Was a legal FISA warrant obtained to surveil campaign?
If you watched the Trump Team’s press conference yesterday like I did, you probably wondered why on earth they held it where they did: Four Seasons Total Landscaping, owned by Marie Siravo, located in an industrial zone next to an adult bookstore and crematorium in the outskirts of Philadelphia!
The media was completely baffled by the location choice. Slate, for example, hints the Trump team was probably so incompetent that they mixed up the business with the Four Seasons hotel in Philadelphia.
But then earlier this morning I stumbled upon a Twitter thread that provided an interesting viewpoint. Normally I’d dismiss such things as a conspiracy theory, yet my gut feeling tells me maybe there’s something behind it. Here’s that Twitter thread. I’ve cut and pasted it fearing that it might disappear.




<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Could federal agents and crime fighting units really be that smart? If this were true, Barr and Trump would be absolute geniuses. We all know Giuliani is a top notch organized crime fighter with half a century of experience. He’s seen it all. Maybe he’s sending a message to the Democrats: We’re miles ahead of you.
Could this business also be the location for ballot fraud operations? Has it been bugged and surveilled by the authorities for weeks? Oh what irony this would be.
Or maybe that location was simply chosen to fool potential protesters and Antifa from showing up and disturbing the press conference.
Anyway, it’s just a really weird place for a press conference. Maybe some Democrats are panicking. One thing is clear: There’s mounting hard evidence of massive voter fraud committed and the Democrat operatives are desperate to keep a tight lid on it.
Thoughts?


		jQuery(document).ready(function(){
			jQuery('#dd_9acea29c387eba69c7b9a05eaf776dc7').on('change', function() {
			  jQuery('#amount_9acea29c387eba69c7b9a05eaf776dc7').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Greenhouse gas (GHG) emissions created by humans are a primary cause of global warming. While carbon dioxide emissions from the burning of fossil fuels and changes in land use make up the largest share of these emissions, non-CO2 greenhouse gases such as methane (CH4) and nitrous oxide (N2O) contribute substantially to overall warming and livestock – and the meat that we eat – is a big contributor.   The Intergovernmental Panel on Climate Change (IPCC) provides guidelines for estimating livestock emissions on a regional level. Direct emissions of methane and nitrous oxide from livestock worldwide has been recently estimated: they represent about 9% of total GHG emissions caused by human activity. The IPCC guidelines offer a relatively simple and robust accounting for estimating GHG emissions produced in each country.  However, producing countries are not the only ones responsible for emissions: the goods and services being produced are exported for consumption in other countries. In other words, the responsibility for emissions from meat lies with consumers as much as producers.  Along with other researchers we have developed alternative accounting systems that re-allocate GHG emissions from producers to consumers. In a study we recently published in Environmental Research Letters, we estimated the total non-CO2 emissions from beef, pork and chicken consumption in 237 countries between 1990 to 2010, allocating emissions embodied in trade to countries on the basis of consumer demand for meat.  These emissions were attributed to consumer countries after first estimating the livestock emissions produced by the digestive process that leads to methane release in animals, along with manure management and manure left on pasture in the origin country. We then used data from the new FAOSTAT database to work out emissions from the import and export of meat. The study highlighted the countries with the largest difference between production and consumption emissions and revealed where fluxes of non-CO2 emissions are greatest.  We found that in 2010, 36.1 Mt (millions of tonnes) of CO2-equivalent emissions were related to meat produced in one country but consumed in a different country. Of this total, 72% was methane and 28% was nitrous oxide. In particular, 26.7Mt of CO2-eq (74%), 7.3 Mt of CO2-eq (20%) and 2.1Mt of CO2-eq (6%) emissions related to beef, pork and chicken respectively that were produced in one country and consumed in another.  Although the emissions embodied in traded beef were greater than those of pork or chicken, the emissions from traded beef grew at a slower rate (4%) between 1990-2010 than those related to pork and chicken (81% and 360%, respectively).  The dominant global fluxes are the export of emissions embodied in meat from Brazil and Argentina to Russia (2.8 and 1.4 Mt of CO2-eq, respectively). Meat exported to Russia embodied 5.2 Mt of CO2-eq emissions; we found that in Russia, 18% of meat-related emissions were traded internationally in 2010. In the same year, emissions embodied in US imports of meat from Canada were equal to emissions embodied in US exports to Mexico: 1.2 Mt of CO2-eq. Australian meat exports to South Korea also embodied substantial emissions: 1.0 Mt of CO2-eq. But Figure b reveals that trade among European countries was quite substantial. In particular, meat exported from France to Italy and Greece embodied 1.4 Mt and 1.2 Mt of CO2-eq emissions, respectively. In addition, Italian imports of meat from Poland, Germany and Netherlands embodied 0.7 Mt, 0.6 Mt, and 0.7 Mt of CO2-eq emissions, respectively. We found that in Italy about 30% of meat-related emissions were traded internationally in 2010. Elsewhere in Europe, meat exported from Ireland to the UK embodied 1.0 Mt of CO2-eq emissions.  Globally, percent changes in traded emissions from 1990 to 2010 vary by region and type of meat. The graphs below show the emissions from production and consumption of beef, pork and chicken in several countries in 2010, as well as the percentage change under consumption-based accounting in the same year (negative percentages indicate net export of embodied emissions and positive percentages indicate net import of embodied emissions). We concluded that long-term growth in the international trade of meat since 1990 means that methane and nitrous oxide emissions from beef, pork and chicken produced in one country are increasingly related to meat consumption in a different country. In particular, we found that large transfers of emissions from meat between North American countries, from South America to Russia, and between European countries. Our findings are important because they quantify the superimposed effects of three important global trends: the growth of international trade; the industrialisation and intensification of meat production; and the increasing consumption of meat.  The overall growth in emissions from the meat trade indicates that the emissions related to increasing consumption of meat are also increasingly disconnected in space from the point of consumption. This spatial disconnect of production and consumption is a challenge for regional or national policies that regulate livestock emissions, because all existing policies neglect any emissions embodied in trade.  The main drivers of meat-related emissions in international trade are the volume and type of meat traded. The trade of beef is the largest source of livestock-related emissions. This is due to the large volume of beef traded internationally and the emission intensity of non-dairy cattle, which is substantially higher than pork and chicken mainly due to greater GHG emissions released during enteric fermentation. Consequently, beef releases more emissions than pork and chicken per ton of meat traded. Dietary preferences are a strong driver of livestock emissions, with beef generally related to substantially more GHG emissions per ton of meat traded than pork and chicken, and much more than vegetables. Therefore, substituting pork, chicken or vegetables for beef in the diet could reduce livestock emissions."
"An exotic parasite is spreading through the world’s honey bees and global warming is making it worse, according to a study that shows it will present increasing problems in North America and Europe. All animals are afflicted by a wide range of pests and parasites. Many are relatively benign. But for the honey bee one gut parasite in particular is a major risk, especially as summers become warmer. Bees are fairly used to parasites. A native single-celled (microsporidian) parasite of the honey bee called Nosema apis has  probably co-existed with its host for millions of years. Back in the 1990s Ingemar Fries, a world expert in honey bee pathology, visited Beijing where he came across a subtly different and novel variant of N. apis in the eastern honey bee. He named it Nosema ceranae.  Fries thought nothing more of his scientific paper describing the species, until the same parasite was found a decade later in western honey bees (the species native to Europe) in Spain, Taiwan and Vietnam. This prompted a team of scientists, including me, to search for the parasite in honey bees across the world. If a recently discovered virus had jumped species and was spreading globally, that was big news for us bee experts. By 2007 we had confirmation: it had indeed gone global, being found across the Americas, Africa and Australasia as well as Europe and its native Asia. This represents a dramatic spread, probably brought on by increasing world trade and the movement of goods, and all within a short space of time – maybe only five years. Worryingly, the exotic N. ceranae is far worse for western honey bees than the native parasite. It is more virulent and it weakens the bees causing them to die while away from their colony – sometimes leading to total colony collapse. In Spain, the parasite has lead to honey bee colony collapse within 18 months of infestation.  And the exotic virus is displacing its native sister species. We staged competitions between the two in a series of laboratory experiments which demonstrated N. ceranae can indeed outcompete its rival. Our findings are published in the latest edition of the journal of the Royal Society. For now the exotic parasite remains a rarity in northern Europe and North America. But it won’t stay this way for long. We used a simple mathematical model to analyse the dynamic between the two species of parasite and we found the exotic N. ceranae’s competitive advantage increases the warmer it gets, perhaps a legacy of its origins in the warmer east Asian climate. We predict that, as the world warms, beekeepers in cooler regions will suffer increasing problems. Though we don’t yet know how to solve the problem of the exotic invader, our research does at least help us to understand why warmer summers may paradoxically lead to great problems for honey bees; forewarned is forearmed.  And it may not only be our honey bees which suffer. Earlier this year we have shown that the exotic microsporidian – unlike the native – is also widespread in bumble bees, so it may cause knock-on problems for our wild pollinators too."
"Something you might have missed amid all the horserace and app-failure coverage of the Iowa caucuses: a deep discussion took place over the past year about the climate crisis and agriculture that could change the way our food system operates. Every leading Democratic campaign now endorses an aggressive approach to conservation that could dramatically reduce greenhouse gases, improve water quality and enhance rural prosperity.  Candidates lined up to tour Matt Russell’s organic farm in central Iowa over the past year to learn about how a diversified cropping system involving livestock can suck carbon out of the air and sequester it in the soil to feed us better. Pete Buttigieg and Elizabeth Warren came up with plans that would restructure farm policy to direct funding away from subsidizing production and toward conservation. Warren would increase funding 15-fold for the Conservation Stewardship Program (CSP), to $45bn. The CSP pays farmers for regenerative agriculture practices – planting soil-saving cover crops, reducing chemical use and tillage, and all the while sequestering carbon in the soil. Export markets for American ag commodities are falling apart. The world has been telling us through markets for years that we are growing about 30% too much corn and soy. Meanwhile, we are killing the Gulf of Mexico with excessive commercial fertilizer, which washes down the Mississippi River. California and Australia burn in part because we are spewing too much nitrogen – as problematic as CO2 for global warming – from our broken agrichemical system. Increasing numbers of midwestern farmers who watched their fields wash away in last spring’s scouring torrents are showing up at field days offered by the Practical Farmers of Iowa, which preaches the gospel of making money on the farm by saving soil and reducing chemical costs. They watch the weather closer than anyone, and they’re ready to look into the old way of doing things – grazing in rotation with a diverse series of carbon-capturing crops – to find a way forward. Candidates started to explore the topic at a rural forum organized in Storm Lake last March by the Iowa Farmers Union. The discussion intensified during the summer as a loose coalition of Iowans, led by the former agriculture secretary Tom Vilsack, pushed candidates to pay farmers for environmental services instead of insuring them for planting in a flood plain. Bernie Sanders is all-in with the Green New Deal. Joe Biden, advised by Vilsack, came up with his own comprehensive plan. Buttigieg is now conversant in how microbial activity in the soil can reverse nitrogen loss to air and surface water. Candidates embraced the idea that renewable energy – wind, solar, hydrogen – can not only ameliorate the climate crisis but also create high-paying technical jobs in rural communities hemorrhaging people. You wouldn’t know it by the non-stop coverage of the percentage fractions separating the leading Democratic campaigns, or whether Sanders insulted Warren, or how Senator Susan Collins equivocated again after lunch. But, as the Amazon shrinks, our quiet revolution in agriculture policy might be the most important story of the news cycle. Markets are telling us they don’t need all that corn. Iowa State University tests of the kernel tell us that soil degradation is eroding protein content. Wheat production in China is falling because of it. General Mills is up on the news, and is urging growers in the Dakotas to go organic because consumers demand it. Kellogg is phasing out glyphosate from its acres. The latest poison from Bayer, dicamba, faces a new wave of class-action lawsuits from angry farmers. In other words, the gig is up on the last 50 years of chemical- and export-driven food production. It hasn’t worked for farmers or rural communities, and they know it. That’s the message from the fallow fields of Iowa covered in snow, and in a few places winter rye holding soil together and nitrogen in its place. There would be a lot more of it if the government would catch on and quit rigging everything for the agrichemical supply chain. The ball is rolling because farmers know Nature is calling the shots. Eventually politics catches up. Climate was a priority for Iowans in this cycle, unlike before. The conversation has changed, and not a moment too soon. Art Cullen is editor of the Storm Lake Times in north-west Iowa, where he won the Pulitzer prize for editorial writing. He is author of the book Storm Lake: Change, Resilience, and Hope from America’s Heartland, recently released in paperback"
"Industrial greenhouse gas emissions in Australia have risen 60% in the past 15 years, putting the country on a path that, if it continues, will lead to it missing the target set at the Paris climate conference. That is the conclusion of an analysis by energy and carbon consultants RepuTex, which examined the rise in industrial carbon pollution – including from oil and gas extraction, mining and large-scale transport – in the period covered by Australia’s 2030 emissions target, starting in 2005.  The resulting report highlights the failure of the Coalition government’s “safeguard mechanism” policy, which was promised to limit carbon pollution rises so cuts paid for by taxpayers through the emissions reduction fund, the main national climate policy, were not just wiped out by increases elsewhere. RepuTex found emissions in sectors covered by the safeguard mechanism had grown from 89m tonnes in 2005 to 142m tonnes in 2019, and were projected to reach 187m tonnes by 2030. If the projection is correct, industrial emissions will have increased 110% over the period in which Australia has promised, as part of the Paris agreement, to cut national emissions by 26-28%. RepuTex’s executive director, Hugh Grossman, said emissions from electricity generation had fallen 9% since 2005, but those gains were being eroded by unchecked carbon pollution in other areas. “While we have seen record levels of investment in renewable energy technology in the electricity sector, the industrial sector has been largely missing from the national emissions reduction challenge,” he said. Grossman said unrestrained industrial emissions remained an issue many MPs did not want to acknowledge or discuss. They were expected to pass those from electricity in 2023 to be the nation’s largest polluting sector. “Ultimately all sectors need to play a part in long-term decarbonisation, to varying degrees, either by reducing emissions or just offsetting emissions growth,” he said. The analysis found the biggest relative surge in emissions since 2005 had been in the oil and gas industry, reflecting the rise of one of the world’s biggest liquefied natural gas (LNG) export industries over the past decade from a low base. LNG emissions were found to be 621% higher than 15 years ago. Other significant increases were from direct combustion at mining sites, venting of fugitive emissions in fossil fuel extraction, and metals, chemicals and minerals processing. The safeguard mechanism started operating in 2016. It sets a pollution limit, known as a baseline, for every industrial facility across the country that emits more than 100,000 tonnes annually. The limit was initially based on either a facility’s historic emissions or an independent forecast of future emissions. Companies that exceeded their baseline were expected to buy carbon credits to offset the additional emissions, or pay a penalty, but in practice this has applied in only some cases. Many facilities have applied for, and been granted, an increased limit. Last week, BHP coal and iron ore mines in Western Australia and Queensland, Alcoa’s Portland aluminium smelter in Victoria and a Boggabri coalmine in New South Wales were each given the green light to emit more. An analysis by RepuTex found the allowed emissions approved by the Clean Energy Regulator increased 32% between 2015 and 2018. Not all companies emitted up to their baseline. Actual emissions under the scheme increased 12% over that time. Recently adjusted national emissions data suggests pollution has stayed flat since 2014 under the Coalition. As RepuTex suggests, there have been decreases from electricity, and agriculture due to the drought, but increases from industry and transport. While the government claims it is meeting and beating climate targets, emissions this year are expected to be only 0.3% lower than 20 years ago. The national target over that time is a 5% cut. In 2016, the then environment minister, Greg Hunt, said the safeguard mechanism would ensure emissions cuts contracted through the emissions reduction fund were not offset by significant increases above business-as-usual levels elsewhere in the economy. The description later changed. A government climate policy document released before last year’s election said the mechanism required Australia’s largest emitters to “measure, report and manage” their emissions. Under changes being introduced this year, all facilities will be moved to limits based not on their total emissions, but on emissions intensity – how much they expect to emit per unit of production. As currently proposed, if companies lift production they will be able to increase carbon pollution without penalty. In theory, these limits per unit of production could be reduced over time to cut emissions and encourage a shift to clean practice - a change Labor proposed without detail before the 2019 election. The Morrison government is divided on climate action and the future of coal-fired power. It has commissioned a review of its climate policies led by the businessman Grant King, and has promised it will this year release a technology investment roadmap, an electric vehicle policy and a long-term emissions strategy. Official projections suggest national emissions will be about 16% less than 2005 levels by 2030 under current policies – not enough to meet Australia’s target of a 26-28% cut without the use of a controversial accounting measure that was criticised by other countries at the UN climate conference in Madrid in December. The government is facing calls from business leaders and the energy industry to consider a climate action bill by the independent MP Zali Steggall that includes a proposal for a target of net zero emissions by 2050, an emissions budget, and assessments every five years of national climate change risk."
"
 
Click for magnified view of the sun showing the most recent spot.
Sunspot 987, 988, and now newly emerging 989 are shown above.
With all being near the equator, they are still a cycle 23 spots. A cycle 24 spot would be at a much higher latitude.
The most recent magnetogram shows them to have the magnetic polarity of cycle 23 spots, in addition to being near the equator.

Cycle 24 remains late. There was one sunspot of high latitude and reversed magnetic polarity on January 4th, 2008, but none have been seen since:

Click for a larger image
UPDATE 2: The solar holographic image shows a potentially large spot on the far side of the sun, we’ll have to wait until it comes around to see what it is. The method is not always perfect.

Darker area is the far side of the sun.
Seismic waves propagating through the sun are used to image potential spots on the far side. Here is a description of how it is done.
UPDATE 3:
It looks as if the spot seen yesterday on the far side of the sun via the holographic technique has disappeared. As I said “The method is not always perfect.”

The two spots above are earthward, 987, and 988.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea04b9a7d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The lord mayor of Sydney, Clover Moore, has demanded the prime minister, Scott Morrison, explain to parliament how his energy minister, Angus Taylor, came to rely on a false document that was used to accuse her of hypocrisy on climate change, after the Australian federal police declined to investigate. The AFP announced on Thursday it would not investigate the doctored document scandal further, saying it was unlikely a commonwealth offence had been committed and there was no evidence that Taylor had been involved in falsifying the document.  The decision has outraged Moore and the opposition, which had initially referred the matter to the NSW police. Moore said she still did not know how Taylor came by a page of the City of Sydney’s annual report, which contained incorrect figures and which showed that the City of Sydney had spent $15m on travel. The real figures, available online, show it was a fraction of that. “He didn’t give me any explanation about how that happened. I still don’t know why the minister did that, why he signed that letter,” she told ABC radio. The incorrect figures were included in a letter Taylor sent to Moore criticising her declaration of a climate emergency and suggesting she curtail her travel instead. The letter was then leaked to the Telegraph, which mocked her declaration publicly. Moore said that Taylor’s apology, made in parliament a month ago, was not sufficient. “Does he not think that climate change is important? That my reputation is not important?” Moore said the prime minister should now investigate the matter and report to parliament, as the matter still raised important questions of ministerial conduct. But she added that she would not be pursuing the matter further and would extend focus on the business of the city. The City of Sydney announced on Thursday that it would bring forward its goal of reaching zero net emissions from 2050 to 2040 – ten years earlier. In a clear criticism of the Morrison government’s record and Taylor, the minister for emissions reduction, she said: “It is incumbent on us as a rich country to do all we can to reduce emissions so we can then put pressure on the large emitters to do their part.” Taylor has welcomed the news that the AFP would not be investigating further and accused Labor of a “track record of using police referrals as a political tool”. “The leader of the opposition [Anthony Albanese] and shadow attorney general [Mark Dreyfus]’s pursuit of this matter is a shameful abuse of their office and a waste of our policing agencies’ time,” he said in a statement. Dreyfus and the shadow climate change minister, Mark Butler, said that “serious questions remain unanswered” about the scandal because “two police investigations have now failed to clarify where Angus Taylor got his dodgy figures from”. Guardian Australia is currently seeking a review of a freedom of information request that sought documents. Taylor’s office refused to release several documents, including one which was described as an email from “an external third party”. It was withheld on the grounds that it would adversely affect the business affairs of that person."
"
Share this...FacebookTwitterFirst at Twitter, aspiring meteorologist Chris Martz posted a chart of tornado activity since 1954. Contrary to what climate alarmists have claimed, tornado frequency has not trended upwards:

Hurricane claim involves “massive error”
While on the topic of violent weather, the Washington Post recently published another alarmism-fraught article on the alleged increasing strength of hurricanes due to global warming and how there’s now a “slower decay of landfalling hurricanes in a warming world.”
However, Dr. Roger Pielke Jr. responded saying that there was a “massive error” in the new Nature paper, which  the Washington Post was citing.
“Says it shows hurricanes decaying slower over land post-landfall (more damaging). But they forgot to remove storms that landfall & then go back over the ocean,” Pielke tweeted.
In the Twitter thread, Dr. Ryan Maue pointed out that some of the hurricanes plotted were not “a typical example” of inland hurricane decay.
No real trend


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Examining global landfall cyclone data since 1970, There’s no observed noteworthy trend: Dr. Ryan Maue also tweeted a chart on tropical cyclone activity over the past 50 years:

Declining in some regions
The often heard claim of more frequent and intense cyclones is not supported by the data. Another paper by Zhao et al from 2018 in fact shows Western North Pacific cyclones becoming less frequent since the start of the century.

Moreover, data from the Japan Meteorological Agency (JMA) show that the number of typhoons formed since 1951 has trended somewhat downward:

So it’s a mystery as to why the Washington Post and Nature would create alarm and make false claims over tropical cyclone activity. That’s science no one should consider, let alone follow.


		jQuery(document).ready(function(){
			jQuery('#dd_baa535913b025a18770bfce7c5c773ee').on('change', function() {
			  jQuery('#amount_baa535913b025a18770bfce7c5c773ee').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Calling cycle 24, calling cycle 24……where are you? 

Image from SOHO, inset added by the author
The SIDC in Belgium just issed an end to their “all quiet alert”
:Issued: 2008 Feb 26 1255 UTC
:Product: documentation at http://www.sidc.be/products/quieta
#——————————————————————–#
# From the SIDC (RWC-Belgium): “ALL QUIET” ALERT                     #
#——————————————————————–#
END OF ALL QUIET ALERT
………………….
The SIDC – RWC Belgium expects solar or geomagnetic activity to
increase. This may end quiet Space Weather conditions.
The first new sunspot in weeks has emerged today. The spot that has emerged is small and on the equator, so it appears that it is a cycle 23 spot rather than one from the cycle 24 that is gave one spot on January 8th, signaling a start of cycle 24, but has given no cycle 24 type spots since.
Based on what we know about the sun, a cycle 24 spot would be reverse polarity to cycle 23 spots and high latitude. The longer cycle 24 continues to delay producing its spots heightens the concern that we may be in for a longer inactive period on the sun, such as a Dalton type minimum.
A thought occurred to me. Given that all of the sunspots seen recently during our solar minimum are very small, I wonder if they could be resolved at all with the primitive equipment available during periods like the Maunder Minimum? Today we have satellites and advanced solar telescopes with hydrogen spectra filters that are available to amateurs, so catching any sunspot, even if small, is now easy. In fact this sunspot was was first noted by an amateur observer, Howard Eskildsen, in Ocala, FL, showing that amateurs still have a role in science.
It makes me wonder if an extended minimum really isn’t an absence of sunspots altogether, but just an absence of larger easily observable sunspots.  It is possible that primitive equipment of the period could not easily resolve smaller sunspots.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0fe56b2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Unprecedented wildfires have recently devastated California, the Amazon, southern Europe, Siberia and Australia. It’s safe to say that we’ve entered the era of the climate-fuelled megafire. But because fire conditions depend on local vegetation, topography and climate, each of these great conflagrations is different. Australia’s bushfires of the last four months have been true megafires, creating their own weather and becoming so vast in their impact that more than half of all Australians have been directly affected by them. As I write, fires continue to burn around Canberra, and though rain has begun to fall in northern New South Wales, 17 are “yet to be contained” according to the fire service. Meanwhile, what is traditionally the worst part of the fire season for Victoria and South Australia is just commencing. Conditions have been so severe that firefighters have often been unable to stop fires joining up, generating massive dry thunderstorms that spread fire with thousands of lightning strikes.  So far the fires have burned an area around the size of England, killed more than 30 people and destroyed about 6,000 buildings. They have left deep psychological scars, and while it seems impossible to shift the government’s disastrous climate policies, the fires will alter the way that Australians view themselves and their country. There’s a British saying that fire is a good servant but a bad master. But in Australia, with its unique vegetation and climate, fire can also be a terrifying predator. Like all good predators, it remains hidden until it’s ready to strike, so even in this fire-plagued year, most Australians have not seen the flames that lurk in the forest, taking life seemingly at random. But many have smelt its stench. Sometimes we wake at night to thickening smoke, and lie there wondering where the beast is prowling. Some mornings we peer out the window and decide that it is not a safe day to be outside. This year fire struck Australia when its guard was down, trapping tens of thousands of people while they were enjoying their summer vacations. The sight of Australians disembarking from warships on naval regulation gangways reminded us that, in this new Australia, the fiery beast that lurks in the forest can make anyone a temporary climate refugee. Increasingly, Australian fire harasses its victims for weeks or months before striking. You prepare for its onslaught, only for the flames to turn aside at the last moment. You prepare the buckets, hoses and other equipment again and again, while fire weakens you with its smoke and debilitating heat. It can also trap you, as if you’re a mouse in a corner. You wait, with fire all around, and all exits cut, your fate in the hands of the wind. If you are lucky enough to survive, you can remain trapped for weeks as you wait for roads to be cleared and made safe. When the predator finally does pounce, it does so with extraordinary fickleness. One house is left standing, while all around are smouldering ashes; one fire truck is picked up by a fire tornado and overturned, while nearby no wind is felt. Why me, both the fortunate and unfortunate ask? The psychological impact of such experiences is profound. I’ve lost a house to bushfire, and successfully defended another. When I look into the eyes of the fire-hunted, I see the same expression I’ve seen in the eyes of returned soldiers or traumatised refugees. Bushfire triggers a primeval fear, one that has been plaguing our ancestors since the first ape was plucked from its family, carried away and consumed by a big cat. Few things matter as much to Australians as their homes and communities. More than 2,000 homes have been destroyed so far by the fires. And even when those affected do have insurance, history shows that most people are underinsured. The fires will doubtless also spark new regulations, which will make rebuilding more expensive. Some people now face the prospect of leaving or remaining homeless in stricken regional economies. All of us are less resilient to future shock – and future shocks there will be. Natural climate variation might have brought a year as hot as 2019 to Australia once every 360 years, but greenhouse gas pollution has increased that frequency to one in eight. And every day the coalfires burn in Australia and elsewhere, we’re shortening those odds – adding to the severity of the fires of tomorrow. Right now, many Australians are simply exhausted. They have been deprived of their summer vacations and beset by debilitating smoke. And as the immediate trauma slowly fades, other impacts will emerge. Around a quarter of Australians are already reporting health effects from inhalation of bushfire smoke, and over the longer term more and more consequences will be felt. Some medical researchers fear that the consequences of smoke exposure from this year’s fires will be felt for generations. Any credible response to such a massive national catastrophe needs to be proportionate to the danger. Instead the government response has been risible. It wants to plant more trees to capture carbon, but also wants more coal burned and more forest cleared, so there will be less to catch fire when the next big fire comes. Its previous climate denialism has been revealed as a catastrophic error, but it remains paralysed in the face of a disaster it helped create. If Australia was being threatened by an external enemy, Scott Morrison’s government would be doing everything within its power to recruit allies. It would put the economy on a war footing, and raise arms. But when it comes to climate-fuelled catastrophes, it claims that there’s nothing it can do: Australians have to remain among the world’s worst greenhouse gas polluters, and keep spoiling international efforts to address the problem. To do otherwise, the government says, would be “economy wrecking”. But guess what? There is a solution – burn more fossil fuels, especially gas. To an increasing number of Australians these lies are nauseating. The words of our prime minister sound like the words of a traitor.  What does one do when immense evil is occurring in one’s country? As far as swift climate action is concerned, all good choices have gone up in smoke, and only difficult, divisive options remain. Australians marched against the Vietnam war, and prevailed. But they also protested against the horrific treatment of asylum seekers, and so far have failed. Now we shall see how communities, businesses and politicians respond to this crisis in “the lucky country”."
"
Share this...FacebookTwitterMelting sea ice is causing sea levels to rise 49 micorons per year (3/16 of an inch over 100 years), according to research published in Geophysical Research Letters. Read more here:  http://www.sciencedaily.com/releases/2010/04/100428142258.htm
Share this...FacebookTwitter "
nan
"The climate science might be gloomy but at least governments seem to be doing something about it. The number of laws passed to address climate change is steadily increasing across the world. By last year 127 countries had renewable energy support policies, for instance. But this is only half the story. Examination of public policy developments in the US, EU and China, the world’s three largest economies by far, has shown that side by side with policy initiatives designed to cut greenhouse gas emissions have come new policies that have the opposite effect: increased emissions.  This is a class of policies that we don’t talk about because it doesn’t have a name. Let’s call them “anti-climate policies”. We are not talking here about the numerous existing policies that perpetuate emissions. Anti-climate policies are new initiatives that increase emissions: steps backwards in the fight against climate change. Their existence means that strengthening climate policies will not be enough to defeat climate change alone; anti-climate policies will need to be tackled as well.  There are lots of anti-climate policies out there – subsidies for householders’ energy bills, support for energy-intensive manufacturing or chemical industries, or building new roads and airports – but three stand out as the most damaging. First, there is the licensing of new fossil fuel-fired power stations, especially in China. Figures from the US Energy Information Administration indicate that between 2000 and 2011 fossil fuel electricity generation capacity rose by 34% in China, 6% in the US, and 15% in the EU-27. Then you have the new and ever higher subsidies for fossil fuels. Numerous new tax breaks for exploration have been introduced in the US, for instance. In the EU new tax breaks have mainly focused on fossil fuel use in energy-intensive industries and transportation, although in the UK tax breaks for exploration have been expanding rapidly. The International Energy Agency reports that in 2011 fossil fuel subsidies worldwide came to US$523 billion, six times the level of support for renewable energy. International trade liberalisation is the other main anti-climate policy. Despite the fact that more trade increases greenhouse gas emissions by expanding economic activity and increasing the use of cross-border transportation services, governments keep signing them. The most recent significant agreement was the one that required China to remove trade barriers in order to join the World Trade Organisation in 2001. Between 2000 and 2010 the EU, US and China concluded new bilateral trade agreements with each other and other countries almost every year. Some progress has already been made. Both the US and UK have moved to introduce emissions limits for new power stations that conventional coal-fired power plants cannot meet, effectively banning new such plants.  Although the projected increase in electricity demand in China is so huge that banning new coal-fired stations altogether would cripple the economy, in 2013 the Chinese government introduced a ban on approvals of new coal-fired power stations in three of the country’s most important industrial regions. This is in addition to a programme that has been closing down small inefficient thermal power stations since 2008.  Efforts to extend blocks on new coal-fired power stations to other countries may strengthen as renewables become more credible and increase their lobbying power.  G20 communiqués announcing agreements to phase out fossil fuel subsidies have not been matched by action. Governments appear to view exploration subsidies, for example, as investments that will bring in more tax once oil and gas fields are in production. Industry lobbying is also much in evidence, especially in the US, judging by trends in political contributions.  What is needed is a spotlight on the hypocrisy of governments that expand fossil fuel subsidies while claiming to care about climate change. We also need to tackle deceits such as David Cameron’s assertion that fracking will reduce emissions by displacing coal with fracked gas. What does he think will happen to the displaced coal? It will be used by someone else. Trade liberalisation continues to be vigorously pursued. Global negotiations launched in 2001 in Doha are aimed at reaching a major multilateral trade-opening agreement, while the US and EU are currently negotiating a bilateral Transatlantic Trade and Investment Partnership. As there is a broad consensus that trade opening boosts economic growth, direct opposition to trade is unlikely to be successful, although the fact that trade opening increases emissions needs to be publicised.  A cannier tactic would be to support the efforts of groups that for other reasons stand to lose by new trade deals, such as US and EU farmers. And the failure so far of the Doha round to reach agreement suggests that such deals can be blocked. Bringing greenhouse gas emissions under control is going to be difficult. To succeed we need to take all relevant factors into account. This means that more attention needs to be paid to anti-climate policies and how they can be countered."
"
Share this...FacebookTwitterHow long have we been hearing this kind of talk from alarmists? How often are we told that we have to make lots of sacrifices and give governments unlimited power – otherwise the earth will be destroyed? The answer is: almost everyday.
High energy taxes, loss of freedom, massive regulation, constant monitoring, surveillance over how we do things will be huge inconveniences; but it’s necessary, and so just suck it up.
Can’t you see all the destruction all over the planet? It’s spreading everywhere, and soon it will be at your doorstep, unless of course you suck it up and give them the power they need.
I ask,  just what kind of person does one have to be to heed that kind of advice? Pretty clueless I think.
 Just suck it up, otherwise the planet is going to be destroyed

I know as a climate-blogger I’m going out on a limb with this. But I got a feeling we have not heard the end of this story by any means. Where’s there’s smoke, there’s fire. My eyes and ears are perked.
I can understand the other climate blogs not wanting to touch this with a 10-foot pole, claiming whatever righteous reason. But I think someone has to observe and report on this. I got a hunch, and I#m going to follow it..
I happen to think Al Gore is not an okay person, and so it would not surprise me if this story turns out to be true. I’m not saying the story and allegations are true. Yellow journalism and the such are not what I draw my conclusions on. That’s for the IPCC to use (before asking us to suck it up).


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The problem is that Al Gore unwittingly revealed a lot about himself and his movement when he made the film AIT. The film was carefully designed to mislead and manipulate its viewers in a mean kind of way. And it was carefully crafted to demonize anyone who refused to fall in line. It uses bully psychology.
Does AIT show any respect for the very element that is needed for science to progress? The answer is of course: none whatsoever!  There was absolutely zero tolerance for sceptical views. Sceptics were ridiculed, mocked and called flat-earthers, and other nasty things.
There are lots of other things in the film that are troublesome, which speak volumes about the persons involved in making it. Of course some involved were just gullible and innocently believed the rubbish.
And let’s not even discuss how Gore lives his life, his business ventures, and so on. These also speak volumes about what kind of guy he is. To me it’s crystal clear. Gore in the film is a pretty bitter and quite peeved chap and he’s capable of a lot of things. 
But we’ll see how the story turns out soon enough. I’m not going to stop blogging about it until the story ends and the (inconvenient?) truth is out.
And my feeling is that the other climate bloggers will be joining in with the commentary before too long. Like it or not, it’s a climate story. But, my hunches could be wrong.
FULL POLICE REPORT HERE FOR YOUR READING PLEASURE!
Update: By yellow journalism, I don’t mean the Examiner, rather the Nation Enquirer, who broke the story, and whose website for whatever reason I can’t access from Germany.
Other Links:
http://www.kptv.com/news/24102273/detail.html
Fox News video

Share this...FacebookTwitter "
"Extravagance is as much a part of awards season as the awards themselves, a lavish procession of parties, campaigning and red carpet opportunities leading to a ceremony that’s rumoured to cost more than $40m. But as this year’s Oscars approaches, there’s been a noticeable shift, from plant-based menus to “repeat tuxedos”, a culture of more trying to grapple with the need for less.  It’s not the first year that the climate crisis has affected Hollywood’s most indulgent period (since 2013, the Oscars ceremony has had a net zero carbon imprint) but, as alarmed discourse about the environment has increased outside the industry, a more pronounced change was needed inside. After best actor favourite Joaquin Phoenix lobbied the Hollywood Foreign Press Association to implement a vegan menu at the Golden Globes, other ceremonies followed suit and on 27 January, the Academy announced that for the first time all pre-show food would be entirely plant-based while the post-show Governors Ball menu would be 70% vegan. “We would have liked to have seen the Oscars commit to a 100% plant-based menu,” said Matt Turner of The Vegan Society. “But it’s an important step in the right direction. People are starting to realise that their individual actions as part of a collective movement can make a tangible difference.” Celebrity caterer Kathleen Schaffer has said that while many of her clients have embraced similar menus, “the omnivore is alive and well in Los Angeles”. Phoenix has also closely aligned himself with Extinction Rebellion, starring in Guardians of Life, a short film made by the activists intended to bring more awareness to the climate crisis. The group also intends to protest near the Hollywood sign on the eve of the Oscars. “Some stars of Hollywood are aware of the scale of the climate crisis, and some have started to take action,” a statement read. “But we do not believe that Hollywood as a whole has taken an acceptable stance on the climate crisis.” The week’s festivities also include gifting suites, produced by third party companies who match stars with high-end products provided by a range of companies. Nathalie Dubois, whose established lounge has previously welcomed guests such as Charlize Theron and Scarlett Johansson, says there’s been an increased attempt to include eco-conscious products and experiences, such as sponsoring coral which will then be replanted. Since 2003, marketing company Distinctive Assets has been providing nominees from the four acting categories and those up for best director with extravagant goody bags. This year’s is estimated to be worth around $215,000, with gifts including a $78,000 fully-inclusive 12-day cruise on a luxury yacht. Distinctive Assets founder Lash Fary, who has been putting the bags together for 18 years, said that eco-consciousness is “certainly” on his mind and so a plant-based meal delivery service and a DIY candle kit that encourages re-use of holders have both been included. While the Academy has released a list of the ways in which they are working towards their goal of becoming carbon neutral, campaigners are still raising the alarm on one of the most damaging components of a season that brings together talent from across the globe: air travel. Anna Hughes, director of the Flight Free UK campaign, believes that more recognition is needed by the Academy and the entertainment industry in general, of the impact that such a “highly polluting” form of travel has on the environment. “In terms of emissions, the savings from providing plant-based food are likely to be overshadowed many times over by air travel to the ceremony,” she said. While it’s difficult to calculate exactly, Hughes estimates that if an attendee is flying to Los Angeles from New York or further in business class or on a private or chartered plane, they will be emitting more carbon than the average Brit does in a whole year. “At the very least, the Academy awards needs to acknowledge this, not ignore it,” she said."
"Air pollution from burning fossil fuels is responsible for more than 4m premature deaths around the world each year and costs the global economy about $8bn a day, according to a study. The report, from Greenpeace Southeast Asia and the Centre for Research on Energy and Clean Air, found that burning gas, coal and oil causes three times the number of deaths as road traffic accidents globally.  Children, especially those living in low-income countries, are particularly affected with an estimated 40,000 dying each year before they reach their fifth birthday because of exposure to particulate pollution from fossil fuels. “Air pollution is a threat to our health and our economies,” said Minwoo Son, clean air campaigner at Greenpeace East Asia. “Every year, air pollution from fossil fuels takes millions of lives, increases our risk of stroke, lung cancer and asthma, and costs us trillions of dollars.” The study, released on Wednesday, analysed global datasets of surface level concentrations of common pollutants PM2.5, ozone and NO2 to calculate the health impact and the subsequent economic cost for 2018. It found: NO2, from petrol and diesel vehicles, power plants and factories, is linked to roughly 4m new cases of asthma in children each year. Approximately 16 million children live with the condition due to exposure to fossil fuel pollution. Tiny particulate pollution – known as PM2.5 – is attributed to roughly 1.8bn days of work absence because of illness each year. China, the US and India are hardest-hit financially by the impact of dirty air with estimated costs of $900bn, $600bn and $150bn each year respectively. The study argues that the solutions to the air pollution crisis are clear – and would also help tackle the climate emergency. It says moving to a clean energy and transport system would have economic as well as health benefits. It cites research published in the US recently by the Environmental Protection Agency that shows every $1 invested under the US Clean Air Act yielded at least $30 in return. Likewise, a weekly car-free day in Bogota, Colombia, yielded up to $4 in health benefits for every $1 invested. “This is a problem that we know how to solve,” said Son. “By transitioning to renewable energy sources, phasing out diesel and petrol cars, and building public transport. We need to take into account the real cost of fossil fuels, not just for our rapidly heating planet, but also for our health.”"
"
See related articles from the Guardian: Billions Wasted On UN Climate Programme and Discredited Strategy
“It looks like between one and two thirds of all the total CDM offsets do not represent actual emission cuts.” — David Victor, Stanford University and co-author of a study examining 3000 UN funded offset programs
This article below was reposted from TriplePundit
 

World’s Largest Carbon Market Facilitates Pollution


An article in the Guardian newspaper reveals that billions worth of ‘clean’ investment on the world’s largest carbon offsets market ends up polluting the environment. The article cites researchers who’ve reviewed the participating companies in the Kyoto Protocol Clean Development Mechanism (CDM). They issued a report which seriously undermines the credibility of the CDM.
The CDM certificates facilitate the funding of clean technology investments by Third World companies that are expanding their operations. Western companies can buy the certificates to offset their own pollution. But it turns out that in reality most of the funds go to coal and oil companies, builders of destructive dams and other enterprises that are not green in the slightest.
The research that revealed the practices is of major importance not least because policymakers are set to review the CDM in the near future as the Kyoto Protocol expires in 2012. CDM credits are the world’s largest offset market, with annual trading last year totalling around EUR40 billion. Most credits are currently traded on the European Trading System (ETS) by European countries and companies but when the US starts to participate, something that’s more or less a given, trading will rise to over EUR 100 billion within two years easily.
The Stanford scholars opened a can of worms. They say that “Much of the market does not reflect actual reductions in emissions, and that trend is poised to get worse.” They researched more than 3,000 projects that had been applying/granted for up to $10bn of credits for the next four years and said that most of the applications should be rejected. If the scheme operated in any way realistically, we’d see a much smaller market, they say cautioning that there’s hardly enough clean air available for the demand that will build up in the near future. That’s rather an important point to consider ahead of next week’s Warner-Lieberman cap and trade bill which proposes US companies are allowed to buy up to 15% of their needed carbon credits from the (successor to the) CDM.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f2ea773',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterOnly a few thousand years ago, when CO2 levels were both stable and low (~265 ppm), the  (1) Arctic had far less ice and more vegetation than it does now and (2) the massive rate of ice melt in Antarctica rendered modern melt rates negligible by comparison.
A new study (Cherezova et al., 2020) reveals that until about 6,500 years ago Bolshevik Island in the Russian High Arctic brimmed with grass, birch and willow trees, and large herbivores grazing on grass year-round. At that time sea levels were rising at rates of 7.9 mm/yr, which is more than 5 times faster than the  global sea level rise trend since 1958 (~1.4 mm/yr, Frederikse et al., 2018).
Today this same High Arctic island is treeless with “very scarce vegetation.” It is locked in sea ice and mean annual temperatures only reach -13°C, which is a “similar climate to the Lateglacial.” Modern climate warming “is not observed” in either the meteorological or ice core data (Tvyordoe Lake) for this region. The ice caps are today about the same size as they were during the peak of the last ice age (~20,000 years ago).

Image Source: Cherezova et al., 2020
Another new study (Jones et al., 2020) reveals that from about 7,500 to 4,500 years ago, when CO2 was about 150 ppm lower than today, Antarctica’s Ross Sea glaciers abruptly lost 220 meters (!) of ice surface height. This ice loss – at times reaching >400 cm per year – occurred throughout the region regardless of the topography. This strongly implies the “overarching external driver” of the glacier retreat was an ocean warming trend.
The authors point out that the ice surface lowering may have “continued below the present-day glacier surface,” only to advance again during the last few hundred years.

Image Source: Jones et al., 2020


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A recent study (Sinclair et al., 2012) indicates the sea surface temperatures in this region (western Ross Sea) have been rapidly cooling (-1.59°C/decade) since 1979, and there has been no net ocean warming since 1882.

Image Source: Sinclair et al., 2012
Still another study from this region (Yokoyama et al., 2016) corroborates a “widespread collapse of the Ross Ice Shelf” between 5,000 and 1,500 years ago. Modern melting rates are the slowest in the last 5,000 years and there was “much warmer water beneath the ice shelf at 5 ka compared with the present.”
These studies strongly imply modern polar climate changes and ice melt rates in both hemispheres are neither unprecedented or unusual. In fact, if there is anything anomalous about today’s polar climates, it’s that they are colder and ice melt is less pronounced than just about any time in the last 8,000 years.


Images Source: Yokoyama et al., 2016


		jQuery(document).ready(function(){
			jQuery('#dd_1b239021c037dc1f44fcc32cd693e702').on('change', function() {
			  jQuery('#amount_1b239021c037dc1f44fcc32cd693e702').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

USHCN-M station at Greensboro, AL
While I was at NCDC, Grant Goodge showed and provided me with a PowerPoint presentation about the plan to update the USHCN manual observing network to USHCN-M or “modernized”. In a nutshell, it is a “light” version of the Climate Reference Network. The summary of benefits goes like this.
More Accurate Data Through:

Redundant Sensors
Near Real Time Diagnostics
Time Resolution of Five Minutes vs The Current Daily
Automated data collection via GOES satellite uplink, eliminating human error of reading and transcription
No adjustments to the data post reception. Time of Observation is now irrelevant.
No more routinely missing data, such as on weekends (fire station at Marysville, CA for example) and thus no need to fill in estimated data using the FILNET adjustment any more.

The station looks much like a Climate Reference Network station, but has some economy considerations, especially in having one aspirated IR screen instead of three, but it contains triple temperature sensors so that issues with instrumentation drift or offset events can easily be spotted in the data stream.

This will be a huge step forward in data quality and quality control.
His PowerPoint presentation is available here at this link: why-modernize-hcn (PPT 9 MB)
Interestingly, it included what appears to be a photo of the rooftop station in Asheville, NC, at the old NCDC (Federal Building) I’m waiting on a  positive ID.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f9014bf',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
NOTE: Please note that part 2 is now online, please see it here.
I recently plotted all four global temperature metrics (GISS, HadCRUT, UAH, RSS) to illustrate the magnitude of the global temperature drop we’ve seen in the last 12 months. At the end of that post, I mentioned that I’d like to get all 4 metrics plotted side-by-side for comparison, rather than individually.
Of course I have more ideas than time these days to collate such things, but sympathetic reader Earle Williams voluntarily came to my rescue by collating them and providing a nice data set for me in an Excel spreadsheet last night.
The biggest problem of course is what to do with 4 different data sets that have different time spans. The simplest answer, at least for a side by side comparison is to set their time scales to be the same. Satellite Microwave Sounder Unit (MSU) data from the University of Alabama, Huntsville (UAH), and Remote Sensing Systems (RSS) of Santa Rosa, CA only go back to 1979. So the January 1979-January 2008 period is what we’ll concentrate on for this exercise as it very nearly makes up a 30 year climate period. Yes, I know some may call this an arbitrary starting point, but it the only possible point that allows comparison between land-ocean data-sets and the satellite data-sets.
Here is the first graph, the raw anomaly data as it was published this month by all the above listed sources:

Here is the source data file for this plot and subsequent plots.
4metrics_temp_anomalies.txt
I also plotted a magnified view to show the detail of the lat 12 months with notations added to illustrate the depth of the anomaly over the past 12 months.

March 2005 to January 2008, magnified view – click for larger image
I was particularly impressed with the agreement of the 4 metrics during the 1998 El Niño year as well as our current 2008 La Niña year.
I also ran a smoothed plot to eliminate some of the noise and to make the trends a bit more visible. For this I used a 1 year (12 month) average.

Again there is good agreement in 1998 and in 2008. Suggesting that all 4 metrics picked up the ENSO event quite well.
The difference between these metrics is of course the source data, but more importantly, two are measured by satellite (UAH, RSS) and two are land-ocean surface temperature measurements (GISS, HadCRUT). I have been critical of the surface temperature measurements due to the number of non-compliant weather stations I’ve discovered in the United States Historical Climatology Network (USHCN) from my www.surfacestations.org project.
One of the first comments from my last post on the 4 global temperature metrics came from Jeff in Seattle who said:
Seems like GISS is the odd man out and should be discarded as an “adjustment”.
Looking at the difference in the 4 times series graphs, differences were apparent, but I didn’t have time to study it more right then. This post today is my follow up to that examination.
Over on Climate Audit, there’s been quite a bit of discussion about the global representivity of the GISS data-set due to all of the adjustments that seem to have been applied to the data at locations that don’t seem to need any adjustments to compensate for things like urban heat islands. Places like Cedarville, CA and Tingo Maria, Peru both illustrate some of the oddities with the adjustment methodology used by NASA GISS. One of the issues being discussed is the application of city nightlights (used as a measure of urbanization near the station) as a proxy for UHI adjustments to be applied to cities in the USA. Some investigation has suggested that the method may not work as well as one might expect. There’s also been the issue of whether of not stations classified as rural are truly rural.
So with all of this discussion, and with this newly collated data-set handed to me today, it gave me an idea. I had never seen a histogram comparison done on all four data-sets simultaneously.
Doing so would show how well the cool and warm anomalies are distributed within the data. If there is a good balance to the distribution, one would expect that the measurement system is doing a good job of capturing the natural variance. If the distribution of the histogram is skewed significantly in either the negative or positive, it would provide clues into what bias issues might remain in the data.
Of course since we have a rising temperature trend since 1979, I would expect all 4 metrics to be more distributed on the positive side of the histogram as a given. But the real test is how well they match. All four metrics correlate well in the time series graphs above, so I would expect some correlation to be present in the histogram as well. The histograms you see below were created from the raw data from 1979-2008. No smoothing or adjustments of any kind were made to the data. The “unadjusted” data in this source data file were used: 4metrics_temp_anomalies.txt
First we have the satellite data-set from UAH:

University of Alabama, Huntsville (UAH) Microwave Sounder Data 1979-2008 – click for larger image
The UAH data above looks well distributed between cool and warm anomaly. A slight warm bias, but to be expected with the positive trend since 1979.
Next we have the satellite data-set from RSS:

Remote Sensing Systems (RSS) Microwave Sounder Data 1979-2008 – click for larger image
At first I was surprised at the agreement between UAH and RSS in the percentages of warm and cool, but then I realized that these data-sets both came from the same instrument on the spacecraft and the only difference is methodology in preparation by the two groups UAH and RSS. So it makes sense that there would be some agreement in the histograms.
Here we have the land-ocean surface data-set from HadCRUT:

Hadley Climate Research Unit Temperature data 1979-2008 – click for larger image
Here, we see a much more lopsided distribution in the histogram. Part of this has to do with the positive trend, but other things like UHI, microsite issues with weather station placement, and adjustments to the temperature records all figure in.
Finally we have the GISS land-ocean surface data-set:

NASA Goddard Institute for Space Studies data 1979-2008 – click for larger image
I was surprised to learn that only 5% of the GISS data-set was on the cool side of zero, while a whopping 95% was on the warm side. Even with a rising temperature trend, this seems excessive.
When the distribution of data is so lopsided, it suggests that there may be problems with it, especially since there appears to be a 50% greater distribution on the cooler side in the HadCRUT data-set.
Interestingly, like with the satellite data sets that use the same sensor on the spacecraft, both GISS and HadCRUT use many of the same temperature stations around the world. There is quite a bit of data source overlap between the two. But, to see such a difference suggests to me that in this case (unlike the satellite data) differences in preparation lead to significant differences in the final data-set.
It also suggests to me that satellite temperature data is a more representative global temperature metric than manually measured land-ocean temperature data-sets because there is a more unified and homogeneous measurement system, less potential bias, no urban heat island issues, no need of maintaining individual temperature stations, fewer final adjustments, and a much faster acquisition of the data.
One of the things that has been pointed out to me by Joe D’Aleo of ICECAP is that GISS uses a different base period than the other data-sets, The next task is to plot these with data adjusted to the same base period. That should come in a day or two.
UPDATE1: I’ve decided to make this a 3 part series, as additional interest has been generated by commenters in looking at the data in more ways. Stay tuned for parts 2 and 3 and we’ll examine this is more detail.
UPDATE2: I had mentioned that I’d be looking at this in more detail in parts 2, and 3. However it appears many have missed seeing that portion of the original post and are saying that I’ve done an incomplete job of presenting all the information. I would agree for part1, but that is what parts 2 and 3 were to be about.
Since I’m currently unable to spend more time to put parts 2 and 3 together due to travel and other obligations, I’m putting the post back on the shelf (archived) to revisit again later when I can do more work on it, including show plots for adjusted base periods.
The post will be restored then along with the next part so that people have the benefit of seeing plots and histograms done on both ways. In part 3 I’ll summarize 1 and 2.
In the meantime, poster Basil has done some work on this of interest which you can see here.
UPDATE3: Part 2 is now online, please see it here.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0ceeee4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"It’s one of the big mysteries in my career as a marine biologist. Something lurking in the seas off Britain has been chomping away at local porpoises and none of the usual suspects fit the bill. Now scientists have finally identified the cuddly culprit. I first became aware of the attacks on porpoises – beakless, smaller relatives of dolphins – as I run the UK shark tagging programme and am often asked to comment on possible sightings. In both 2010 and 2011 I was sent pictures from the Norfolk coast of porpoise carcasses with a considerable amount of tissue bitten away.  The common assumption was that these were due to shark attacks. However none of the photographs of the wounds showed the characteristic punctures caused by the multiple rows of shark teeth, such as displayed by human victims of shark attacks.  The UK isn’t exactly known for its deadly sea beasts. While a number of shark species do live round the coast and in the North Sea, the most common are too small to inflict the bites we were dealing with. Larger sharks capable of removing that amount of tissue are very rare.  Invariably the popular press claims attacks like these are evidence of great white sharks. After the 2011 find near Great Yarmouth, Norfolk, the Daily Mail thought a “giant shark or killer whale could be stalking the coast”, The Sun even said it might be Jaws himself. Given that there are no fully substantiated reports of white sharks in UK waters and that in any case such a shark could quickly dispose of a whole porpoise, this is fanciful.  But at last, we have a more likely explanation. The unfortunate Norfolk porpoises aren’t alone. In fact, huge numbers of harbour porpoises have been washing up on the shores of the North Sea. They shared the same nasty-looking bite marks. A team of Dutch scientists has now identified grey seals as the culprits. Their work, reported in the Royal Society journal, reveals DNA from grey seals has been found in bite marks on porpoise carcasses. The researchers examined 721 dead porpoises in detail. The sheer numbers enabled them to identify the key characteristics of seal bites including substantial loss of skin and blubber, puncture wounds (often repetitive) to the head, tail and flippers, plus series of parallel scratches anywhere on the body left by seal claws.  A flow chart has been produced using these marks to determine if the porpoise was attacked by a seal and if it could have escaped. This will be invaluable to those undertaking autopsies of porpoises in the future.  The majority of confirmed seal attacks are on juveniles. Since many of the porpoise carcasses were found on Dutch bathing and surfing beaches the authors offer up the thought that humans may be at risk. Perhaps this serves us right, as it seems humans may have triggered this change in seal diet. Along with their close relatives dolphins and whales, porpoises often become entangled in fishing gear and some attacks may simply be seals scavenging trapped porpoises. Having got used to the scavenged fare, the authors speculate that seals may have turned to attacking live porpoise. Porpoises have a hard life. Even dolphins attack them. They are victims of the fishing industry and subject to an increasing amount of noise from boats, oil rigs and wind turbines.  Now, possibly triggered by our activities, there is yet another pressure on this species: hungry seals."
"
Share this...FacebookTwitterHans von Storch’s blog brings our attention to an excellent German report by normally green ZDF public television.
The report takes a critical view of Europe’s energy policy and reaches the conclusion that it’s a failure. My last post Billions Of Euros For Nothing Called A Success Story illustrates this beautifully.
The ZDF interviews a leading finance researcher, Professor Dr Kai Konrad, and here’s what the ZDF report says:
– Start clip (German)-, content in English:
After 20 years of conference after conference after conference, a sort of traveling climate circus on a worldwide tour, Copenhagen became the highpoint of absurdity in December of last year – a political and media overkill with the aim of nothing less than to rescue the planet. The conference failed yet again. It all gets down to money.
Professor Dr. Kai Konrad is a distinguished finance researcher at the prestigious Max Planck Institute in Munich and a close advisor to the Federal Ministry of Finance. He and a team of researchers drew up an expert assessment of Germany’s climate policy.
The assessment was so damning that the Ministry quickly removed it from its website.
The assessment took a hard look at the 1st Commandment of climate policy: reduce CO2 emissions, and how a relatively small group of countries decided – unilaterally – to reduce CO2 emissions. The researchers writing the assessment deemed this a grave error. Professor Konrad says:
When a small group of countries sit down and say they want to  do something good for the climate, and reduce their emissions, it has practically no effect on the total amount of emissions worldwide. It means the rest of the world picks up the slack and just emits more.
In effect it means that the countries who cut emissions incur all the costs but no benefits. And the countries that don’t cut emissions, profit. So it’s highly worth it for these so-called “free-riders” who don’t sign on. What has the Kyoto protocol produced?
Since 1990 worldwide CO2 emissions have increased 36% and the few countries that have reduced their emissions have had immense costs, estimated to be $150 billion.
When it comes to CO2 emissions, the European Union is a global power. Especially Germany has been a leader in cutting emissions – already 20% less than 1990. Professor Konrad says:
The fact that Europe is a leader in cutting emissions will only lead to other countries slacking off, and thus the costs are merely shifted from the countries that don’t play along to Europe. So whatever progress Europe makes in cutting emissions just gets lost to countries like USA and China.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




And so the circus goes on. The other countries are happy about the cuts, and the EU carries all the costs. Europe’s Climate Commissar estimates the costs will be:
€500 billion ($620 billion) in the next 10 years.
Germany is the leader in this craziness, and is expected to cut emissions by 40% by 2020. This is to be accomplished by Germany’s EEG Gesetz, or Energy Feed-in Act, which forces power companies to purchase renewable energy at exorbitant prices from anyone who produces them and to deliver them to consumers, who then must pay through the nose. Professor Konrad says (in summary):
From a theoretical point of view, the EEG brings no benefit. It brings nothing because the system of buying CO2 emissions certificates doesn’t work.
All the certificates do is ensure that the CO2 gets produced elsewhere. Professor Konrad:
The Feed-in Act is to be criticised in my view because it is no longer transparent as to what an enormous redistribution it creates and the huge subsidies that flow out of the pockets of consumers and into the hands of those who profit from it.
By the end of the year German consumers will have paid €62 billion ($75 billion) without seeing any CO2 reduction. In Professor Konrad’s and his colleagues’ view:
The policy of avoiding the production of CO2 is a failure, nationally and globally.
As a result, Professor Konrad’s recommendation is to use a different strategy (one that even the earliest and most primitive of man used):
A D A P T A T I O N
The researchers say this policy would be much more successful, and certainly much cheaper than the current CO2 elimination policy.
– End clip –
Now, I wonder if our clever politicians will muster the intelligence that even our early Neanderthal ancestors had millions of years ago, and adopt this strategy?
Don’t hold your breath.
Share this...FacebookTwitter "
"Serious poisoning by plants is very rare in the UK so the death of a gardener in Hampshire after brushing against a deadly flower was extremely unusual. Despite the British countryside’s genteel reputation there are a surprisingly large amount of poisonous plants growing both in the wild and in gardens. Some just cause discomfort, but others have the potential to kill. Here are five to watch out for. Wolfsbane belongs to the plant genus Aconitum, a group of plants which are all poisonous.  The native plant, also called monkshood, has large leaves with rounded lobes and purple hooded flowers. Although it can be found throughout the UK, cases of accidental poisoning are very rare.  Still, people plant it in their gardens, possibly unaware of the potential hazard.   It is one of the most toxic plants that can be found in the UK, the toxins in the plant can cause a slowing of the heart rate which can be fatal and even eating a very small amount can lead to an upset stomach. But its poison can also act through contact with the skin, particularly if there are open wounds.  The roots are thought to be especially poisonous but even so, people have been known to eat the roots and survive so it is very difficult to know how much contact is needed to kill someone.  As with any poisonous plant, the best way to avoid it is to learn to recognise what it looks like.  Once you can recognise it then you can make sure you don’t eat it and only handle it with gloves on. Foxglove grows in woodlands and hedgerows. It is a common garden plant, popular due to its tall purple flowers.  Its large soft leaves grow in a rosette.  If any part of the plant is eaten it causes vomiting and diarrhoea together with other unpleasant symptoms, and just like wolfsbane it can slow the heart down causing heart attacks.  Even contact can cause irritation to the skin.   However, foxglove has saved more lives than it has cost as drugs derived from the plant are used to treat heart conditions. The cuckoo pint (Arum maculatum) or lords and ladies, is found growing in woodlands and hedgerows. Its flowers are poker-shaped surrounded by a green leaf-like hood but it is the bright red and orange berries of this plant that are poisonous.  If eaten, the berries cause irritation in the mouth and throat which leads to swelling and pain and can result in difficulty breathing. It also causes an upset stomach. As its name suggests, deadly nightshade is another poisonous plant. Deadly nightshade is most common in central, southern and eastern England but is also found less commonly in other parts of the UK. It is a shrubby plant with purple bell-shaped flowers and shiny black berries.   In the first instance poisoning results in symptoms including dilated pupils, loss of balance and a rash but it can eventually lead to hallucinations and convulsions. Atrophine, a drug extracted from nightshade, is used in eye examinations to dilate the pupil. It’s even used as a nerve gas antidote. Hemlock isn’t native to the UK but can be found in most areas.  It grows in ditches and riverbanks and in disturbed area such as waste ground and rubbish tips.   Hemlock is a tall green plant with purple spots on its stem and leaves similar to the carrot plant, it has white flowers.  If it is eaten hemlock causes sickness and in severe cases it can kill by paralysing the lungs."
"

 _The Marketplace of Democracy: A Conference on Electoral Competition and American Politics Sponsored by the Cato Institute and the Brookings Institution_.



The decline of political competition and the overwhelming incumbent advantage are a growing concern for voters and experts alike. At a Cato Conference on March 9, cosponsored with the Brookings Institution, political journalist Michael Barone, Michael Munger of Duke University, and Gary Jacobson of the University of California, San Diego, examined the factors that contribute to electoral stagnation and discussed the merits of possible solutions.



 **Michael Barone** : I am an optimist, so I want to make the case that the American marketplace is working pretty well. There are some market imperfections, of course, but all markets tend to have them. Overall, I think the system works to present choices to people, to register their opinions, and to provide a basis for informed governance that is capable of responding to opinion. And it has responded to the opinions of both the people who call for more government and those who call for less government.



The Founders did not want or desire a two‐​party system, but such a system emerged very quickly after the first Congress went into session. Madison argued in _Federalist_ 10 that a large republic could contain the power of faction because a multiplicity of factions is inevitable in a large republic. Yet, a multiplicity of factions also makes decision making very difficult.



If you look at countries whose electoral systems encourage factions, typically through proportional representation, you often find very small and unrepresentative groups at the fulcrum of power. In Israel, the religious parties have often had enormous clout and have been able to frustrate majorities on issues of particular interest to them. For more than 20 years in Germany, the splinter Free Democratic Party, which always struggled to get more than the 5 percent threshold for representation, determined which major party would control the government.



Our system is different. The result has been that we give our voters relatively clear choices between two alternatives and have parties that are at least somewhat responsive to opinion because unresponsiveness could cost them votes. Sometimes those choices have been crisp. Sometimes they have been muddled. But in the last two decades, our parties have become ideologically much more coherent. People who do not like that result complain of bitter partisanship and polarized voting, but we should remind ourselves that partisanship is the natural result of the coherent, clear choices that political scientists say voters should have. The winners of elections then have the ability to put their programs into law.



A multiparty system might allow some voters to support candidates who share more closely all their political views. Libertarians, for example, do not have a viable party. But a multiparty system creates a lot of problems. Just look over the border at Mexico, with its three‐​party system, which has been unable to address what are clearly some of the major issues before that country. In Canada, with its four‐​party system, the balance of power is now held by a party that wants to separate from the rest of the country. That system is a bit bizarre.



A third‐​party candidate could win a U.S. presidential election. Ross Perot and Colin Powell were viable independent candidates in the 1990s. But they were already well‐​known to voters. Our long public nominating process limits the field of potential candidates to people who enter the race already famous and able to independently finance their campaigns.



People often attack campaign financing as a market imperfection because some candidates are able to raise more money than others. That argument was much more effective in the past than it is today. When I first started observing politics in the 1950s and 1960s, it was said that the Democratic Party could not raise as much money as the Republican Party because they represented the working people, whereas the Republicans represented rich people. Today, that imbalance doesn’t really exist. The Democrats spent more than the Republicans in the 2004 presidential cycle. Both parties had plenty of money to do most of the things that they wanted to do to get their message across.



My own view is that Supreme Court jurisprudence on campaign finance is wacky. The reigning law seems to say that James Madison and the Founders passed the First Amendment in order to protect nude dancing, student arm bands, and flag burning, but they certainly did not want to protect this messy, awful stuff called political speech.



It is said that some kinds of candidates cannot get financing under the current system of campaign finance regulation. What we see today is that, with the Internet, every point of view seems to be able to find abundant financing. If you had told me at the beginning of 2003 that Howard Dean—a medical doctor who is obviously an intelligent man but is palpably unqualified to be president, on the basis of temperament or knowledge—would be able to raise rafts of money, I wouldn’t have believed it. But the Internet has made large scale fundraising possible for even lesse rknown candidates.



With a government that channels vast flows of money to and decides issues of moral importance for citizens, people are going to spend more money on campaigns, and they’re going to spend more time and energy on the political process. Incumbents will always be able to raise more than challengers because they’ve proven that they can win elections and garner benefits for their constituents. But as the late mayor Richard J. Daley of Chicago said when asked whether there should be a benefit to winning elections, “Why should the people who backed the losers get the insurance contracts?”



All points of view seem to be represented in our democracy. Even as we bemoan polarization and gridlock and nasty partisan clashes, I think we also should recognize that those things have resulted in higher voter turnout and greater citizen involvement in politics. The Bush campaign attracted something like 1.4 million volunteers. Total turnout in the popular vote in 2004 was up 16 percent over 2000. John Kerry received the third‐​highest number of popular votes in history, and he lost the election.



I think that we are overdue for a change in the political contours of our country. It may happen in this election, but by 2010 we will certainly see some change in the political landscape. And although redistricting and campaign finance regulation can help protect incumbents or other favored candidates, when voters’ opinions change, the advantages for incumbents or candidates in safe districts may be overcome. The ability of our political system to adjust to such changes in the political climate is a sign that our political marketplace is functioning well.



 **Michael Munger** : There is good political competition and bad political competition. The fundamental human problem is to foster the good and block the bad. So, as I argued in my presidential address to the Public Choice Society in 1988, the fundamental human problem comes down to the design and maintenance of institutions that make self‐​interested individual action not inconsistent with the welfare of the community.



One example of a set of institutions that accomplish that reconciliation of selfish individuals and group welfare is the market, Adam Smith’s “invisible hand.” We still can’t accurately predict the exact circumstances or times when markets might work as he described, but it is definitely not always true that self‐​interest leads to the welfare of the community, even in market like settings. Nonetheless, by and large, we know that competition in markets serves the public interest. The question is this: under what circumstances is competition good in politics?



Good political competition is where ambition checks, or at least balances, opposing ambition. When President Bush tried to push through the Dubai Ports World deal, some senators and representatives objected on its merits. But even more objected on the grounds that the president was usurping congressional authority. Our political rules have to create situations in which politicians’ ambitions are opposed, in which attempts by one group or person to grab all power are always frustrated.



Bad political competition is what public choice theorists call rent seeking. In my classes, I ask students to imagine an experiment that I call a “George Mason lottery.” The lottery works as follows: I offer to auction off $100 to the student who bids the most. The catch is that each bidder must put the bid money in an envelope, and I keep all of the bid money no matter who wins. So if you put $30 in an envelope and somebody else puts $31, you lose the prize and your bid. When I play that game I sometimes collect as much as $150. Rent‐​seeking competitions can be quite profitable. In politics, people can make money by running in rent‐​seeking competitions. And they do.



What are all those buildings along K Street? They are nothing more than bids in the political version of a George Mason lottery. The cost of maintaining a D.C. office with a staff and lights and lobbying professionals is the offer to politicians. If someone else bids more and the firm doesn’t get that tax provision or defense bid or road system contract, it doesn’t get its bid back. The money is gone. It is thrown into the maw of bad political competition.



Who benefits from that system? Is it the contractors, all those companies and organizations with offices on K Street? Not really. Playing a rent‐​seeking game like that means those firms spend just about all they expect to win. It is true that some firms get large contracts and big checks, but they would be better off overall if they could avoid playing the game to begin with.



My students ask why anyone would play this sort of game. The answer is that the rules of our political system have created that destructive kind of political competition. When so much government money is available to the highest bidder, playing that lottery begins to look very enticing. The Republican Congress has, to say the least, failed to stem the rising tide of spending on domestic pork‐​barrel projects. Political competition run amok has increased spending nearly across the board.



In a perfectly functioning market system, competition rewards low price and high quality. Such optimal functioning requires either large numbers of producers or lowcost entry and exit. Suppose that Coke and Pepsi not only had all the shelf space for drinks but asked in addition if they could make their own rules outlawing the sale of any other drink unless the seller collected 100,000 signatures on a petition to be allowed to sell cola. The Federal Trade Commission would not look favorably on the request, on the industry.



But in our political system, we have an industry dominated by two firms. Republicans and Democrats hold 99 percent of the market share and have undertaken actions at the state and national levels to make it practically impossible for any other party to enter. How did we come to have such a system, with outside competition for office nearly closed off but with inside competition for access to the public purse organized as a kind of expensive ritual combat, where Congress keeps all the bids?



I believe that the perverse competition in the political system is a direct consequence of the so‐​called progressive reforms. First, reformers systematically hamstrung the ability of political parties to raise funds independent of individual cults of personality. Parties are actually necessary intermediaries. They solve what my colleague John Aldridge calls the collective action and collective choice problems by giving voters a shorthand by which to identify and support candidates whose opinions they share. Campaign finance reform cut out soft money, thus weakening parties’ ability to support new candidates, but doubled the limits on hard‐​money contributions to members of Congress.



Second, progressive campaign finance reform surrounds incumbents with a nearly impenetrable force field of protection. Any equal spending rule or equal contribution rule benefits incumbents, who can live off free media and other publicity. Any rule that restricts contributions or makes them more expensive, such as reporting requirements for contributions, benefits those with intense preferences and deep pockets. So restrictions on contributions ensure that only the most hard‐​core competitors—those along K Street—participate in the political bidding wars.



The hidden problem is that politics actually abhors a vacuum. If real grass‐​roots parties are denied the soft money they need to mobilize people and solve the problem of collective action and collective choice, organized interests will fill that vacuum. Because no individual can influence government, stripping away intermediary organizations of individuals makes the remaining organized groups more powerful.



The problem is not our inability to reform. The problem is precisely the extent to which we have reformed the system. Our reforms killed healthy political competition at the citizen level. And now all real political competition takes places in the offices on K Street. That’s the kind of political competition that is antithetical to the interests of the community.



 **Gary Jacobson** : After falling irregularly for several decades, turnover in elections to the U.S. House of Representatives has reached an all‐​time low. On average in the four most recent elections (1998–2004), a mere 15 of the 435 seats changed party hands, and only 5 incumbents lost to challengers. Since 1994 Republicans have won between 221 and 232 of the 435 House seats, and Democrats, between 204 and 212, by far the most stable partisan balance for any six‐​election period in U.S. history.



The historically low incidence of seat turnover and partisan change during the past decade has revived scholarly concern about the decline in competition for House seats that had been prompted by a similar period of stasis in the 1980s. It is easy to understand why. Turnover is by definition a product of competitive races. If low turnover reflects the disappearance of competitive districts and candidates rather than, say, unusually stable aggregate preferences among voters, then election results have become less responsive to changes in voters’ sentiments.



A competitive election requires that both parties field competent candidates with sufficient financial resources to get their messages out to voters. But the decisions of potential candidates and donors about whether to participate depend on their estimates of the prospects of success. Politically skilled and ambitious politicians do not invest in hopeless efforts; neither do the people and organizations controlling campaign money and other electoral resources. Judgments about the prospects of success are strongly affected by incumbency— thus open seats tend to attract a much larger proportion of high‐​quality candidates who raise much more money than the typical challenger to an incumbent— but incumbency is not the only consideration. The underlying partisan balance in a district and national political conditions also count heavily in their decisions. Thus at least two developments unrelated to incumbency might have contributed to declining levels of competition and partisan turnover in recent years: a decrease in the number of districts where the partisan balance gives the out‐​party some hope of winning, and the absence of the kind of national partisan tides that raise the chances of victory for the favored party.



What is behind the decline in competitive seats? The favorite culprit of many critics, the creation of lopsidedly partisan districts via gerrymandering, is a relatively small part of the story. A more important factor is that voters have grown more reluctant since the 1970s to vote contrary to their party identification or to split their tickets, making it increasingly rare for districts to elect House candidates who do not match the local partisan profile. A more speculative, though related, notion is that partisans have been voting with their feet by opting to live where they find the social—and therefore political—climate congenial, creating separate enclaves preponderantly red or blue. These alternative explanations for the disappearance of competitive districts are not incompatible; indeed, the processes they entail would be mutually reinforcing.



With the decline in the number of seats on which the current party’s hold seems precarious enough to justify a full‐​scale challenge, strategic calculations about running and contributing have led to an increasing concentration of political talent and resources in the diminishing number of potentially competitive districts at the expense of the rest.



This trend is clearest in the shifting patterns of challenges to incumbents. The proportion of challengers who have previously won elective public office—a crude but serviceable measure of candidate quality— has headed downward, most notably among Democrats. But the disappearance of experienced challengers is confined to districts where the challenger’s prospects were already slim because the partisan balance favored the incumbent.



In districts where the partisan balance (indicated by the presidential vote) is favorable to the challenger’s party, the proportion of experienced challengers has grown substantially; evenly balanced districts have seen little change. Incumbents in districts favorable to the challenger’s party have also become much less likely to get a free pass; in the 1970s and 1980s, about 17 percent of incumbents defending unfriendly territory were unopposed by major party candidates; since then, the proportion has fallen to less than 5 percent.



The increase in partisan polarization and consistency has clearly favored the Republican Party, allowing it to profit from a structural advantage it had held for decades but, until recently, had been unable to exploit. For example, in 2000 the Democrat, Al Gore, won the national popular vote by about 540,000 of the 105 million votes cast. Yet the distribution of those votes across current House districts yields 240 in which Bush won more votes than Gore but only 195 in which Gore out polled Bush. The principal reason for this Republican advantage is demographic:



Democrats win the votes of a disproportionate share of minority and other urban voters, who tend to be concentrated in districts with lopsided Democratic majorities. But successful Republican gerrymanders in Florida, Michigan, Ohio, Pennsylvania, and, after 2002, Texas enhanced the party’s advantage, increasing the number of Bush majority districts by 12, from 228 to 240.



If this analysis is on target, feasible solutions to the problem of declining competition for congressional seats are quite limited. Nonpartisan redistricting might create a few more evenly balanced and therefore potentially competitive districts. But because voters are to blame for most of the recent diminution of such districts, unless mapmakers sought deliberately to maximize their number through pro‐​competitive gerrymanders, the effect would probably be modest under the current distribution of partisans and their levels of polarization and party loyalty.



Campaign finance reforms are also unlikely to have much effect on competition. No more than a handful of challengers in recent elections could make a plausible claim that they might have won but for a shortage of funds; no matter how I analyzed the data, I could detect no significant effect of the incumbent’s level of spending on the results of those elections or any others. Of the 15 House incumbents who have lost since 2000, only 4 were outspent by the challenger; on average they outspent the opposition by more than $500,000. Experienced challengers and campaign donors do not ignore potentially competitive districts, and challengers do not lose simply because incumbents spend so much cash; their problem is a shortage of districts where the partisan balance offers some plausible hope. Senate races, too, have almost invariably attracted experienced and well‐​financed candidates whenever the competitive circumstances have warranted.



The one thing that clearly could generate a greater number of competitive races is not subject to legislative tinkering: a strong national tide favoring the Democrats. Such Democratic landslides as those of 1958 and 1974 put substantial numbers of Democrats into Republican‐​leaning seats (in addition to those they already held), thus leaving a larger portion inherently competitive. A pro‐​Democratic national tide would, by definition, shake up partisan habits, at least temporarily, counteracting the Republicans’ structural advantage. But absent major shifts in stable party loyalties that lighten the deepening shades of red and blue in so many districts, the competitive environment is likely to revert to what it has been since 1994 after the tide ebbs.



This article originally appeared in the May/​June 2006 edition of _Cato Policy Report_
"
"
Share this...FacebookTwitterOther blogs have mentioned today a report from the German financial daily, Handelsblatt here, but didn’t provide many details, and so I’ve decided to shine a little more light on the article. It is indeed frightening.
The German government has been generously subsidising renewable energy sources for years now, and it’s going to cost the German consumer a bundle – and soon.
The big price driver is solar energy. Year after year more and more panels are getting installed on German roofs and far surpassing even the most optimistic projections. But that shouldn’t be a surprise because Germany’s Energy Feed in Act (EEG Gesetz) guarantees solar energy system operators a fixed tariff for 20 years, making solar energy systems extremely lucrative for those who have them.
According to the Rhine Westphalia Institute for Business Research (RWI) the net costs of all photovoltaic system installed between 2000 and 2010 add up to a whopping $107 billion, Subsidies for renewable energy are going out of control.
An open letter written by Johannes Lackmann, former director of the German Association of Renewable Energy, caused many to take a closer look. Lackmann warns:
Companies are positioning themselves on the same square as the old industries, who failed to modernise and keep up with the market demands because they came to rely on generous subsidies paid by governments to survive. The EEG Act must not be allowed to be misused as cushion to sleep on.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Indeed the extreme comfort of the EEG subsidies will lead to an additional 9000 MW of solar energy capacity to be mounted on roofs this year alone in Germany. It’s a run-away train that will have serious consequences.
Recall that solar energy generators are guaranteed payment for 20 years. For systems going online this year, an average of almost $0.40/kw-hr is guaranteed by law until 2031. By comparison conventional energy is traded on the EEX  exchange for just over 6 cents. The huge difference is passed on to the German consumer.
According to the RWI the net costs for all photovoltaic systems installed between 2000 and 2010 over their 20 year operating life will add up to $107 billion. According to the Handelsblatt:
This sum is more than one quarter of the entire German annual federal budget. Yet the amount of solar energy as a share of the total energy produced is still puny despite the huge subsidies. It is only 1 percent.
Just the photovoltaic systems installed this year will lead to an additional $33 billion dollars in costs over their 20 year operating life. Electrical energy rates will climb 10% in 2011. Energy giant RWE just announced it will increase rates by 7.3% in August.
Sure Germany has cut back the subsidies – some, but the prices of solar panels and systems have fallen even faster, and so as a result they are even more lucrative. Lackmann thinks reductions in subsidies are long overdue, and the industry will not be doing itself a favour attempting to fight them off.

They don’t provide any incentive for investment in R&D. Leading German solar companies invest less than 2% of their sales turnover in R&D. This is far below what a company like Siemens invests in R&D.

Share this...FacebookTwitter "
"
NOTE: Mike alerted me in comments about this article he wrote along the lines of my story on Color and Temperature: Perception is everything. I thought this would be good to examine again.  This article below is re-posted from John Daly’s website, and was originally published July 7th, 2002. – Anthony

By: Michael Ronayne

In a story titled “Coloring Climate Change” by Nick Schulz, Tech Central Station reported that key documents, in a US government report titled “The National Assessment of the Potential Consequences of Climate Variability and Change“, were “doctored” to distort public perceptions of climate change. The report was published by the United States Global Change Research Program. According to their own web page, the USGCRP coordinates the research of ten Federal departments and agencies with active global change programs and provides liaison with the Executive Office of the President. The budget of the USGCRP in fiscal year 2002 was approximately $1.7 billion US dollars.
The National Assessment report has served as the basis for parts of the 2001 National Academy of Sciences’ report “Climate Change Science: An Analysis of Some Key Questions” prepared for President Bush on the state of climate science and, most recently, for the highly controversial “U.S. Climate Action Report – 2002“, covertly issued by climate alarmists within the Environmental Protection Agency, with the objective of embarrassing the Bush Presidency.
The TCS story displays two graphics, shown below. The graph on the left is the one which was circulated during the public comment period after the original draft was developed. It compares the Canadian Model with the Hadley Model for the lower 48 States for the summer months of June through August, over the next 100 years. The TCS story provides additional background on the two graphs and is highly commended to your attention. Then the disparity between the two models’ future forecasts, cast doubt on the predictive capacity of the Canadian and Hadley models, the USGCRP issued the final report on the right, with the color scale altered to obscure the differences between the two models.











Unfortunately for the USGCRP, the two models show the areas of warming and cooling to be occurring in widely different sections of the United States. The USGCRP’s solution to this conundrum was to alter the temperature color scale by eliminating yellow and green, and extending the color orange into negative temperature ranges as low as -1.0°F, thereby implying warming,  when in fact the models were showing no temperature change or cooling for some localities.






 




Above: When the “Draft” and “Final” copies of the USGCRP graphs are animated, employing a technique used elsewhere on this web site, the amateurish nature of the deception becomes painfully obvious.
Not only was the distorted temperature color scale used to obscure the next 100 years of temperature models, it was also used to change the perception of the United State’s         past climatic history. The page “Overview:  Looking at America’s Climate” contains a graphic titled “Temperature Change” (shown below), which attempts to minimize the significant cooling which occurred in the Southeastern United  States during the 20th Century. This is achieved by coloring even the zero or `no change’ temperatures in light orange, and blending colors in such a way as to make it almost impossible to differentiate anything between about 0° and 5°.  Not even the IPCC has as yet stooped to this level of deception. 

On the same web page, there is another graph titled “Summer Maximum and Winter Minimum Temperature Change” (shown         below), which contains the USGCRP’s final version of the Canadian and Hadley 21st Century Summer and Winter Models, again with a choice of color scheme which blends         everything from 0° to 5° into a deceptive spread of orange.  Even         areas which these models show will not change, are colored in orange.         What other purpose can this peculiar coloring scheme serve but to suggest         future warming         in areas where none is actually predicted by the model?
 
“The National Assessment of the Potential Consequences of Climate Variability and Change” report is comprised of three separate sections which represent themselves as addressing increasing levels of detail. The descriptions are those used by the USGCRP:
1. Overview Report:  Concise, well illustrated summary.
2. Foundation Report: Volume, more detailed than the Overview Report.
3. Background Information:  Learn more about the National Assessment.
The Overview Report is published in both HTML and PDF formats and contains all of the USGCRP graphs and most of the URLs, previously referenced. This report is clearly intended for the media and the general public. Its primary message is one of impending doom, associated with anthropogenic global warming.
I am not sure why the USGCRP expended the effort to create the Foundation Report. It has so many technical flaws, in terms of electronic publishing techniques, that anyone who attempted to read it, would be quickly discouraged from delving into its contents. The report is only published in two PDF formats. Each subsection of the report is comprised of two PDF files, one which is black and white, with extremely low resolution gray scale graphics. The second PDF file contains the color figures and graphs but only the text associated with each figure. As the figures associated with the text report are all but useless, because of the poor quality, the serious reader must have  two PDF files open and switch between both files to comprehend the report. What is interesting is that the PDF file titled “Potential Consequences of Climate Variability and Change“, which contained color figures, shows in Figure 13, the US temperatures using the altered color temperature scale, but in Figure 20, the Global temperatures are displayed using the  original color temperature scale found in the draft report. The only function of the altered color temperature scale is to obscure the differences between the Canadian and Hadley models for the 21st Century United States.  By contrast, the 21st Century Global graphs were not altered in this way.
In the Background Information section, things become interesting. On a deeply buried page at “VEMAP Trend Maps” the original high resolution images, on which the draft graphics were based, can still be found. The individual graphs are: “CGCM1 Maximum Temperature Trend (JJA)” and “HadCM2 Maximum Temperature Trend (JJA)“.
One could engage in endless speculation as to why the USGCRP went to the trouble of altering the first two sections yet failing to alter the third, which contained the most incriminating information. The two most likely explanations are: (1) the Background Information section was overlooked and (2) the USGCRP did not expect anyone to find the original graphs from the Canadian and Hadley Models. Also, on the “VEMAP Trend Maps” page the Canadian and Hadley Models are not compared side-by-side, so the inconsistencies between the models are not as obvious. 
Of course, the USGCRP may not even care if the real results from the Canadian and Hadley Models are found. As long as the media continues to endlessly report only the results from the first two sections, the voices of a few skeptics can be safely ignored. 
Last year in another story, a question was asked for which no reply has been forthcoming:   If the evidence for global warming is that compelling, why is it necessary for those who believe in global warming, to misrepresent data in this manner to support their cause?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e5e5da4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

On May 4, 2012, the Cato Institute honored economist Mao Yushi as the recipient of the 2012 Milton Friedman Prize for Advancing Liberty. One of the most outspoken activists for individual rights and free markets in China, Mao has emerged over the last several decades as an instrumental voice against the nation’s heavy‐​handed, authoritarian grip. Before the presentation of the award, attendees heard a keynote address by Chris Christie, the governor of New Jersey. Governor Christie stressed the significance of communicating the ideas of freedom, and how his experience in office holds lessons for the nation at large.



GOV. CHRIS CHRISTIE: It’s a pleasure to be here this evening — to leave all that’s exciting in New Jersey on a Friday night, come down here to this sleepy little hamlet, and speak before you all.



Back in 2008, I remember Barack Obama talking about the lack of hope around the country. And although he and I always defined the solutions to that problem differently, the environment in which I found myself shortly thereafter was not significantly different.



When I first took office in New Jersey in January 2010, optimism was a hard thing to find. In the eight years before I became governor, our state had raised taxes _115 times_. From 2000 to 2009, New Jersey had — literally — a zero job growth decade. In the four years before I became governor, $70 billion in wealth had left the state — not diminished wealth, _departed wealth_. Our unemployment rate was over 10 percent, with 115,000 private sector jobs lost during the four years of my predecessor.



New Jersey had the highest tax burden in the country, the worst climate for small business, and a bloated state government that contained the most public workers per square mile in the country — yeah, you can laugh unless you live there. And it only got worse. In my second week in office, my state treasurer told me that, in the subsequent five weeks, we had to find $2.2 billion in cuts from money that had already been appropriated.



We essentially had to impound the money back from certain departments just to meet payroll — all in what was the second wealthiest state per capita in America. If you need any greater example of what happens to an economy when a government overtaxes, overspends, overborrows, and overregulates, just visit New Jersey in January 2010.



So what did we do? Thanks to New Jersey’s unique constitutional structure, which allows spending to be cut by executive order, my staff and I sat in a room over the course of three weeks and went over all 2,400 line items in the state budget that I inherited. The result was finally cutting $2.2 billion. And the great thing about operating by executive order was that, at first, I didn’t have to tell anybody.



But, after delivering the news in my first speech before the joint session, you can imagine the reaction from the legislature.



Reporters descended upon the floor as the Democrats began calling me names: Julius Caesar, Napoleon Bonaparte — all of those great leaders of the past that I admire. And I realized something. The way I confronted my first substantial problem in office set the tone for my administration. I made clear from the first day that decades of fiscal irresponsibility were no longer going to be tolerated. As I said on the campaign trail, I was ready to go to Trenton and turn it upside down.



Last year we passed a $2.3 billion tax cut for businesses, with nearly 70,000 new private sector jobs created. We’ve cut spending in every department of our state government, from areas that folks told me were the third rails of politics. Given that I was still upright, I decided to go after public pensions and benefits next. And what happened? For the first time in 10 years, a majority of New Jerseyans recently polled believe the state is back on the right track. On election day in 2009, that number was 19 percent. Today, it’s 53 percent.



The American people are ready to hear the truth. They know our government is out of control. And the only thing they care more about than today is tomorrow — because tomorrow is about our children and grandchildren, and today is just about us.



The bottom line is we took action — we did it with solid principles and strong leadership — putting our state’s interests ahead of partisan ones. We turned Trenton upside down. And in the difficult times that America is in now, the only way to govern is by treating our citizens as adults — by telling them the truth about the depth of our challenges and the difficulty of the solutions.



When we fail to do this, we pay the price as a country many times over. The domestic price is obvious: growth slows, unemployment persists, and we make ourselves even more vulnerable to the unpredictable behavior of rightfully skittish markets.



But there’s also a foreign policy price to pay. To begin with, we diminish our ability to influence the thinking and ultimately the behavior of others. Democracy is the best protector of human dignity, liberty, and freedom — and history shows that mature democracies are less likely to resort to force against both their citizens and their neighbors. Yet, all across the world — in the Middle East and Asia and Africa and Latin America — people are debating their own political and economic futures. They’re looking for inspiration, and we have a stake in the outcome of those debates. There’s no better way to reinforce the likelihood that others in the world will opt for more open societies and marketbased economies than to demonstrate that our own system is working well.



At one time in our history, our greatness was a reflection of our country’s innovation, determination, ingenuity, and the strength of our democratic institutions. When there was a crisis in the world, Americans found a way to come together to help our allies and fight our enemies. When there was a crisis at home, we put aside parochialism and put the greater public interest first.



Today, our ability to effect change has been diminished because of our own inability and unwillingness to effectively deal with our problems. Now, I understand full well that succeeding at home and setting an example is not enough. But it’s a start. And I realize that what I’m calling for requires a lot of our elected officials and our people. I plead guilty to that. But I also plead guilty to being an optimist, because I believe in what this country and its citizens can accomplish _if they understand what’s being asked of them_.



We seem to have forgotten that this is a human business. Day after day, I’ve spent time sitting with colleagues on both sides of the aisle, convincing them of my intentions and letting them know that I don’t believe compromise is a dirty word. There’s always a boulevard between compromising your principles and getting everything you want. You should never compromise your principles.



But you also need to understand that you’re not always going to get everything you want. The job of a leader is to find your way onto the boulevard between the two without driving into the ditch of compromising what you believe. And trust me, if you can do this in New Jersey, you can do it anywhere. That’s where my optimism comes from. See, I’m not looking to be loved. I get plenty of love at home — and when you’re looking for love in this job, that’s when deficits get run up.



However, if you make people understand that you’re willing to say no, but you’re also always willing to listen — that you’re willing to stand hard on principles, but you’re also willing to compromise when those principles won’t be violated — then respect will come. It’s about being consistent. It’s about leading by example. It’s about standing up for the things that we believe in, instead of simply trying to figure out which way the wind’s blowing. There’s no need for varnish anymore. In fact, I don’t think we have the luxury to put it on. Liberty and freedom and the human spirit are the most powerful things in the world — and we need to say that directly to the American people. They’re ready to hear it.



I want to thank the Cato Institute for setting an example of why liberty and freedom are so important to the future greatness of America. But please never forget that it’s not going to come without a fight. We need to fight hard, even harder than we are now because the stakes are too great to do anything less. Only then can we allow the United States to, once again, export hope and liberty and freedom around the world, not just because those values are a part of our past, but because we will be acting to make them a bedrock of our future.



 **MAO YUSHI:** Ladies and gentleman, I bring you all my humble greetings from China. Tonight, we are here together in this “Shining City upon a Hill” to celebrate our common beliefs, our common hopes, and our common commitment to the values that make the Cato Institute so very special. Those Cato values bring us together today, united as common citizens of the world. I like to think of us all as “Cato citizens.” The values of which I speak, of course, are peace, free markets, limited government, and the preservation of individual liberty.



Tonight, we celebrate those timeless values — the common thread that runs through all great civilizations of the world from the beginning of humanity. Regardless of whether your traditions and cultural roots are in Africa, the early settlements along the Yellow River, the Tigris and Euphrates, the Nile, or the valleys and mountain ranges of Meso‐​America, these universal values are our common heritage. They touch each heart and resonate in the basic moral fiber of our souls.



I am personally honored and humbled to be recognized as the recipient of the Milton Friedman Prize for Advancing Liberty. I cannot express enough my grateful appreciation for this recognition and for Cato’s many decades of invaluable guidance on the long road to liberty in China.



Over the last 83 years, I’ve endured many threats and fearful nights, years of deprivation and political persecution. My family and friends, however, provided the love, the loyalty, the dignity, and the moral compass to continue that journey, regardless of the headwinds.



They helped me remember the lessons of our nation’s past heroes and heroines, as well as our moral responsibilities to the future generations. They provided the light in the storm so that we could stay the course. I could not be here today without them.



My family and I are honored tonight, but we realize that we are not here just as sons and daughters of China. We are also here on behalf of three additional constituencies who join us in _absentia_.



First, we stand in the shadows of earlier honorees from Estonia, the United Kingdom, Peru, Venezuela, and Iran. Each represents societies that have traveled on their own Homeric Odyssey, which Cato has appropriately recognized. We honor them individually and we salute their great peoples and cultures.



Secondly, we stand in the foreshadow of Cato honorees yet to emerge. To these future Cato citizens, let them know that we see their courage. We feel their heartbeats, their yearning, and their allegiance to the values of the Cato Institute. In the years to come, let everyone here be our proxy and congratulate them on their journey to a shared better tomorrow.



Thirdly, and most importantly, we receive this honor on behalf of two constituencies in China.



The first is the tens of thousands of grassroots organizations who currently work every day to serve the common citizen of China, who strive to build a better and more humane tomorrow. Countless scholars, workers, peasants, teachers, students, volunteers, and friends struggle against the common enemies of humanity: tyranny, poverty, disease, and war. They are the real honorees of tonight’s prize.



The second constituency we represent is the tens of millions of Chinese over the last century who have sacrificed their lives along the road to overthrow feudal dynasties, defeat warlordism, and defend liberty against foreign colonialism and imperialist invasion. They have proven countless times that freedom is more precious than life itself, and their struggle is also revered here tonight. This award is accepted on behalf of them, with the solemn promise that your torch will be carried by each succeeding generation with the same energy, faith, and devotion you brought to your endeavors. We do this to continue to brighten our country’s future and to deliver the inalienable and universal rights of all human beings to our descendants.



This glow — when combined with the lamps carried by fellow Cato citizens around the world — will become beacons of light from the “Shining City upon a Hill,” spreading Cato values to the dark corners of the world. In your hands, more than mine, will rest the final success and failure and hopes for liberty of our peoples.



Many have sacrificed for China’s people, for her dignity and her liberty. Despite my eight decades — my now weak hearing and failing sight — I still remember the names of those who sacrificed for our country. I see their faces. I hear their voices. I feel their souls. Tonight I speak for those who cannot be here. Their sacrifices have not been in vain, and I thank the Cato Institute for giving their lives renewed meaning.



China is a very old country. She is a noble and wise civilization, with a grand history of fine art, science, medicine, philosophy, exploration, hard work, tolerance, and openness. It is a society based on balance known as the Golden Mean, or the middle way. This theme runs through our great traditions of Daoism, Buddhism, and Confucianism. Our people have always sought balance between the needs of the collective and the individual — and against extreme government.



China understands the power of economic liberalism. Her people know that free markets and liberty nourish each other, acting together as a force for social progress. Our long history is full of examples of people rejecting the arbitrary power of the state, refusing to subordinate the rights of the individual, and recognizing that unchecked collectivism stifles human creativity and productivity.



Systemic checks and balances against unlimited government power, corruption, and improper privileges — mechanisms which include free markets, the rule of law, an independent judiciary, and the shift to a smaller, more accountable government — need to be built and put in place permanently.



These are so important because of China’s horrific past. This generation has personally experienced mass repression through the government apparatus, witnessing the more than 50 million Chinese who died directly as a result of the influence of the rule of man over the rule of law. Both our leaders and our people know what is at stake for China’s future. They are all personally invested in ensuring that Cato’s values — tailored to China’s realities — can help build a strong, prosperous, and harmonious country in the finest traditions of our ancestors.



In modern times, many countries have recalibrated their societies to fundamentally improve them with these balanced values. In the early 1970s, there were about 40 democracies in the world. As the 20th century ended, there were 120 diverse countries with some form of self‐​government crafted by the people — a largely peaceful trend that continues to this day. China needs to learn from these powerfully instructive experiences.



In order to preserve societal harmony and build China’s better tomorrow, the government should further extend liberty, freedom, and free markets — and reestablish the peaceful rise of “good neighbor” policy to preserve regional and world peace. If China’s leaders can implement the essential principles common to every successful society, she can further contribute to the world’s peace, prosperity, and harmony.



I remain optimistic that China’s government will hear her people’s desire to make this vision a reality. If one examines our country objectively, they will see that there is great reason to be hopeful given China’s tremendous progress already. Edmund Burke, the great British parliamentarian, once warned that “a state without the means of some change is without the means of its conservation.” China has been changing and progressing — a process which does not require disruptive instant transformation.



Our country has successfully raised more than 300 million people from poverty — a huge number, even in China. Knowingly or not, accomplishments like this have been achieved by the balanced implementation of Cato values in the context of the Chinese society. China’s successful evolution over her long history has been rooted in the balance between rights and responsibilities — a balance that is in focus now as we approach a transition in leadership at the end of this year.



Those of us in China know that the rights of the individual do not come from the generosity of the state. In fact, we have very limited expectations from government. We ask only that the system lives up to our constitution, abides by our laws, and complies with the international covenants of the world community. The people of China know that the fruits of society are not the sole prerogative of the powerful and privileged few. As the next generation steps forward to battle against tyranny, poverty, disease, and war itself, I believe that our journey into the future will be long, but successful. All big rivers come from small streams. Our efforts in China are but one small stream.



Tonight’s constituencies — from the people of China to the other tributaries inspired by the timeless wisdom of the Cato Institute — will join together as a mother river to nourish the human spirit and wash away the hardships of our imperfect world. Thank you once again for this great honor.
"
"

The Wall Street Journal is reporting that a bunch of venture capitalists are now backing Norway’s Think electric-car company. Their plan is to bring the company’s Think City car to the U.S. in 2009 and build it here as well.
I drive a 2002 Ford Think electric car, the open frame model. I’m pretty happy with it, at 3 cents a mile, and I’ve put about 300 miles on it around town since buying it 3 weeks ago. It has gotten a lot of attention in my hometown of Chico, and people are constantly asking me how much it cost and where could they get one? The town is blessed with many alternate back routes, so I don’t have to travel the main congested roads.
The U.S. version is expected to travel 110 miles on a single charge and kind of resembles Smart’s ForTwo. The company expects the car to be priced under $25,000. It’s looking for a site in the U.S. to build U.S.-spec models because it’s cheaper to build an entire line here than it is to ship from Europe, thanks to the weak dollar. Maybe Michigan politicians should be making some calls to Oslo.
The Think City is already in production in Europe, and the company is rushing to produce 10,000 units this year for sale there. One of the people behind the VC funding says they could sell 30,000 to 50,000 Think City cars in the U.S. See Norway’s Think to Produce, Sell Small Electric Cars in U.S. (from WSJ.com) 
There is another car that Think has in the pipeline, and it is pretty cool looking, see it below:



Its new concept, called “Ox”, looks to be a much more mainstream vehicle than any of the minicars the company sells overseas.
But the name needs to change, because I don’t want my friends teasing me that I’m driving an “Ox car”. I think they were shooting for some spin on “Oxygen” but missed the mark.
Roughly the size of a Scion xB, the front-wheel-drive Ox MPV will have a 60-kW electric motor and a range of 124 miles on a full charge. It can be charged via a normal household outlet. Charging the car to 80% will take just an hour using a special charger, while a full charge will take 12 hours. 
The company is planning to use either sodium or lithium-ion batteries, and there’s a strip of solar cells running down the center of the roof. The Ox is built on an interchangeable platform, so a coupe body style with a larger motor and batteries or a taxicab configuration could also be manufactured.
Unfortunately, the Ox looks to be a true concept, with no firm date on when we could expect to see it on the road. The other unfortunate part is that Think doesn’t have a presence in the U.S. General Electric recently invested $4 million into Think, though, so don’t give up hope of one day seeing the “Ox” on the street. More photos here.


Bring a production version of the Ox with a different name, though, and I’d expect people to line up.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ec9aba2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Now the nights are drawing in and the air is chill, many of us stop going out in the evening and retreat to our front rooms to spend cosy evenings curled up with a book or stretched out in front of the TV. By the start of November, even the most miserly of penny pinchers has typically caved in to the need to put the central heating on. Unfortunately, not everyone is lucky enough to be able to afford such a basic necessity as warming their own home. Traditionally a household has been considered to be “fuel poor” if required to spend more than 10% of their total income on energy to maintain an adequate standard of warmth. According to the government’s Annual Fuel Poverty report, 2m UK households are currently living in fuel poverty: 10.4% of the population. Unsurprisingly, unemployed are worst hit with almost a third recorded as fuel poor. The inability to afford energy bills killed a staggering 10,000 Britons in last year. While the UK’s rate of fuel poverty is unacceptably high, transport poverty should also be given attention. The 2m in fuel poverty is dwarfed by the 21m households that motoring lobby group the RAC reports spend more than 10% of their disposable income (as used to be the measure of fuel poverty) on transportation. According to the RAC, transport is the single biggest cost for the average household, at 14% of disposable income – with the majority of this being spent on owning and running a car. Those without a car make, on average, half the number of journeys. As yet transport poverty has not been widely discussed, little research exists and there is no commonly accepted definition. It may be the case that simply transferring home energy costs to transport outgoings is the most appropriate means for tracking this phenomenon or there may be more sophisticated ways to measure it.  Sustrans, the sustainable transport charity, combines three factors: time taken to access essential services; distance to the nearest bus or train station, and; family income. Using this rationale, they estimate half of the nation’s local authorities contain (mostly rural) areas with a high risk of transport poverty. Research from the Office for National Statistics shows that the UK’s poorest families (the lowest tenth of household incomes) spend 31% of their money on the purchase and operation of a car; a proportion that is rising each year as motoring costs increase.  Transport poverty is most pronounced in rural areas where owning a car is considered a necessity and not a luxury. These are areas of low population density, where jobs and services will typically be located a long way from homes. With generally inadequate public transport and long distances making walking or cycling impractical, cars can be the only option for those living in our villages and hamlets. For those living in rural parts of the UK, transport poverty is already a major issue. Research into commuting habits from the RAC show much more need for cars in rural areas. More than half of urban commuters walk or cycle rather than drive but just 31% of rural dwellers could do this. Public transport is easily available in cities and towns but rare elsewhere. And unfortunately for rural dwellers fuel is more expensive in the countryside, adding to an increased cost of living. There is, then, a rural transport problem that needs to be taken seriously. As fuel poverty begins to move onto the political agenda, we must hope that similar happens for transport poverty.  Our presumption for private vehicle ownership is clearly unsustainable in environmental terms: though more efficient petrol and diesel engines still belch out toxic fumes, while even electric cars largely rely on fossil fuel power stations.  Transport poverty shows that our obsession with owning and running cars is not socially or economically sustainable either. Those who are young, elderly, unemployed or in poorly paid work risk being locked out of ordinary society simply because they cannot afford to buy into the car system. Many of those that have enough money for a car will find it a burden that means they are priced out of other activities. In particular, rural communities risk decimation as people need to move away and what remains is a two tier society: the affluent that enjoy full access to 21st century society and an underclass that are left behind. We need to move beyond the current car system and perhaps rural car clubs with collective ownership and shared vehicles offer a way forward."
"
Share this...FacebookTwitterRudolf Kipp of http://www.science-skeptical.de/blog/deutsche-klimaforscher-starten-neuen-rettungsversuch-fur-ein-globales-klimaabkommen-die-klima-kopfpauschale/002480/  reports on the latest German plan to save the planet: Peak and Trade. This is my summary in English.
Without a doubt the Copenhagen Climate Conference last December was a major flop. A treaty for reducing CO2 emissions could not find the support it needed. The conference was such a disaster that a climate agreement is all but ruled out for 2010 as well. So, what to do?
Policy Shift in Germany
The Potsdamer Institute for Climate Impact Research, which includes alarmists Stefan Rahmstorf and Hans Joachim Schellnhuber, has been the primary advisor for chancellor Angela Merkel, and have arguably lost much clout since the embarrassment that was Copenhagen.
To get things back on track, PIK has come up with a new position paper that outlines a whole new approach that could lead to a breakthrough in climate treaty negotiations. It’s dubbed New Strategies for Reaching the 2°C max Climate Target, a scheme which the PIK calls Peak and Trade. It calls for a so-called per capita climate quota, see here (Abstract written in English). http://www.pik-potsdam.de/research/publications/pikreports/.files/pr116.pdf
The PIK scientists call for allocating a quota of 5 tonnes of CO2 to every inhabitant on earth. Should an earthling exceed the quota, then payment would be made into a fund (a yet to be created World Climate Bank). If the quota is not reached, then money would come out of the fund as a reward.
This of course becomes very attractive for the poorest of countries, and very expensive for industrious nations. For example the average American emits 16.9 tonnes of CO2 – more than three times the quota. Yet, some European countries with high amounts of hydroelectric or nuclear power are well-positioned. For example Sweden and Switzerland emit on average 5.6 tonnes, the French 6.3 tonnes. The financial punishment of Peak & Trade would be mild and thus bearable.
The per capita climate quota is very attractive for developing countries
Before anyone writes this idea off as crazy, take a close look at the numbers and who the potential winners and losers would be. But first, where does the 5.1 tonnes of CO2 per earthling number come from? The authors of the paper simply have taken the projected 2015 CO2 emissions figure of 35 billion tonnes and divided it by the current world population of 6.9 billion. Copenhagen failed in part because of resistance from the developing countries, especially Brazil, China and India. They just didn’t see enough incentives. These countries however are well below the magic 5.1 tonne quota – especially Brazil and India, and so now they have ample reason to enthusiastically support Peak and Trade.  It’s reasonable to think this is the instrument to get them back on board.
The per capita climate quota is also an incentive for population growth (and poverty) 
Poor developing countries now stand to rake in the cash. The more inhabitants and the poorer the standard of living, the more money a country stands to pull in. That could be an irresistable incentive for many countries. The humanitarian problem with this German Meisterwerk of a plan now becomes clear: governments and regimes in poor countries would have no incentive to improve living standards for their citizens, and thus keep CO2 emissions at low levels. The funds earned by falling below the 5.1 tonne quota of course would never be seen by the poor citizens. Most likely these funds would flow into in the hands of corrupt government officials and into anonymous bank accounts in Switzerland.
Readers may think this is some far-fetched idea. It is – and that’s the danger. These things are often hatched by social engineering experimentalists here in Europe and they always seem to grow legs. Don’t underestimate it. Peak and Trade has to be killed.
Share this...FacebookTwitter "
"Wolves, lions and other large carnivores rely on meat for sustenance and there are only so many wild animals to go round. Sometimes, dinner means cow or sheep. Farmers can use guard dogs or protective fencing to deter predators and protect livestock. But lethal methods such as hunting and trapping are also used to control wild carnivore numbers. As a livestock farmer in wolf country, it would be reasonable to assume that killing more predators would result in fewer attacks on your animals. However, a new study by Washington State University has turned this assumption on its head by discovering the opposite: the more wolves that are killed (up to a threshold of 25% of the population), the more the remainder preyed on local sheep and cows. Why is this? The researchers, Robert Wielgus and Kaylie Peebles, point to the nature of the species’ social systems: wolves live in family groups containing a breeding pair (also known as the alpha pair) along with related sub-adults, juveniles and pups. The alphas are the only breeders within the group as they limit reproduction by their subordinates.   Killing one of the alphas disrupts the pack and subordinate wolves, who often outnumber the breeders, are then free to reproduce. This could increase the number of breeding individuals in the area, thereby increasing the population of hungry wolves – maybe farmers who shoot wolves are inadvertently doing more towards conservation than they think! Conversely, as humans are more likely to shoot youngsters than adult breeding wolves, the alphas may be temporarily be in a more favourable situation. There would be less competition for food, fewer clashes with other wolves and less risk of the transmission of disease.  Again, this could result in short-term increases in attacks on livestock. Wolf packs also have an important educational role, as the experienced wolves pass on their knowledge. Killing them impairs this social learning. If the rest of the pack hasn’t learnt the skills necessary to take on bison or elk they may instead turn towards easier pickings on the farm. This same behaviour has been seen in lions and cougars (although has not been documented in many other carnivore species). It is interesting to note that this paradoxical finding is not just found in relation to wolves – lethal control of cougars (or mountain lions) also means the remainder kill more cows and sheep as younger, inexperienced cougars are more likely to attack livestock.  Coyotes also show increased litter sizes and more frequent breeding in populations that were lethally controlled. Culling programmes could have even exacerbated livestock attacks by taking out younger, less predatory coyotes.  Further, state-funded coyote removal campaigns have failed to reduce predation on sheep.  Lynx, too, do not significantly reduce livestock attacks until lethal control dramatically reduces total population numbers. It must be noted that other studies have shown that killing predators can sometimes reduce the numbers of livestock they themselves kill, but this is only temporary,  until new populations of predators establish themselves.   If we would like a world where neither livestock nor predators are killed, we are either going to have to take away all the predators or all the livestock. Clearly neither one of these options is viable so we must aim to reduce preying on farm animals to a tolerable level. Despite proof that changes in livestock husbandry reduces predation, farmers may still not want these creatures living near them as they may feel that the carnivores have “won” or taken over “their” land.   As such, despite scientific evidence showing that predators don’t kill that many cattle anyway, that lethal control usually doesn’t reduce attacks, and that non-lethal methods can almost eliminate attacks, this still may not be enough to sway farmers from their anti-predator mind-sets.   We must therefore start to think outside the box. Much of this conflict between humans and wild predators is not really about protecting livestock, but instead concerns a deeper historic and cultural aversion to wolves, lions and other scary carnivores. This won’t be fixed through simple technical solutions – and we now know it certainly won’t be fixed with a gun."
"

In the May/​June 1983 issue of _Regulation_ magazine, the economist Bruce Yandle set forth a new theory of government intervention. At the time Yandle was executive director of the Federal Trade Commission, and, as an economist, he had recently become interested in the demand for and supply of social regulations. Where exactly do these regulations come from?



As he read about historical efforts to control alcohol by banning Sunday sales, Yandle found the answer in an unlikely coalition. Churchgoing teetotalers endorsed the prohibition on moral grounds, while bootleggers supported the restrictions in order to limit competition. As Yandle discovered more and more examples of this alliance — from environmental policy to interstate trucking — he concluded that “durable social regulation evolves when it is demanded by both of two distinctly different groups.” He referred to these groups as “Bootleggers,” who have an economic interest in the regulation, and “Baptists,” who have a moral argument.



In a new book called _Bootleggers & Baptists_, Yandle — along with Adam Smith, director of the Center for Free Market Studies at Johnson and Wales — revisits an old theory with new perspective. The book explores a political dynamic connecting interest groups who, for very different reasons, spend time and resources seeking government favors. “At its very root, it is a story about the demand for and supply of politically provided pork,” Smith and Yandle write.



The authors begin by surveying the explosive growth in federal regulation during the 1970s and 1980s — when the theory was germinating — presenting an array of Bootlegger and Baptist stories stretching from Magna Carta to today’s energy industry. This thick layer of rules is at the center of the narrative. “When the pace of regulation accelerates,” they write, “Bootleggers and Baptists are sure to barbecue while the political fire pits are hot.”



Smith and Yandle then spend time deconstructing the two groups. They show how the ultimate results of noble‐​minded efforts to effect change in the public interest prove to be neither noble nor in the public interest. Instead, a publicspirited group wraps a selfinterested lobbying effort in a cloak of respectability. As a result, “once Bootleggers and Baptists are locked into a successful coalition, their structural incentives change, making political wealth extraction more attractive than private wealth generation — to society’s detriment.”



Consider, for instance, Obamacare. The implementation of health care reform provided some amount of access to a larger share of the population. Yet, it also cartelized 17 percent of the U.S. economy, guaranteeing an expanded market for the country’s biggest political players. “In each of these efforts, a vast Baptist choir sang the praises of government‐​assisted health care,” the authors write. “But lurking in the background — and sometimes in the back row of the choir — were pharmaceutical, insurance, and other health care Bootleggers ready to expand sales to the regulated sector.” As Smith and Yandle detail in full, the story behind the rise of Obamacare is more complex than it seems — and much more interesting.



In one example after another — from the Troubled Asset Relief Program to climate change — the two economists reveal the interaction of lofty values with narrow selfinterest. In short, politicians who deliver pork to Bootleggers can justify their actions by appealing to higher “Baptist” morality. This phenomenon is driven by forces deeply rooted in our DNA, they explain. What, then, is the endgame? Is this marriage of strange bedfellows here to stay? “We are all at least a little bit Bootlegger, a little bit Baptist,” Smith and Yandle conclude, “which means as long as we remain human, the story of Bootleggers and Baptists will continue.”
"
"Devoted followers of international wrangling on climate change will see much that they recognise in the five-page text emanating from the UN climate talks in Lima. The “parties” (countries) have long accepted the maximum 2°C warming target; that mitigation and adaptation must go hand-in-hand; and that past emissions of developed countries need to be accounted under “common but differentiated responsibilities.” Actual promises of financial assistance to developing nations show little sign of progressing to their US$100bn target, though, and emission pledges so far fall far short of levels consistent with 2°C of warming. The hard-fought document restates a commitment from all countries to raise their level of ambition in order to arrive at the next meeting in Paris in December 2015 with a convincing plan to resolve their differences once and for all. Radical approaches will be needed. Climate change is a particularly tough nut to crack: there is still deep uncertainty over its likely impact; there are sharply differing viewpoints and conflicts of interest; and there is no central authority to implement any agreed solutions. Social planners call these public policy issues that challenge conventional thinking “wicked” problems. Most awkward of all, climate and development are deeply interconnected, driving a deep rift between developed and developing blocs. Changes happening now are a result of the fossil-fuelled development of the richer countries, while issues of public health and energy security in poorer nations can seem remote from long-term climate change – at least until rising seas or diminishing harvests intervene.  The climate policies that animate negotiators each December must be thoroughly embedded in the everyday battle to escape poverty that still afflicts the greater part of humanity. Climate change is for life, not just for Christmas. For the third year running a UN climate conference notes with grave concern the significant gap between countries’ emissions pledges and the levels required to stay within the magic two degrees. However the lamentable lack of action is partly the result of a parallel gap in knowledge as politicians need to know how to reduce emissions without damping growth.  Climate change is pushing climate and socio-economic systems far from their equilibrium states and researchers are struggling to catch up. Traditional economic modelling relies on equilibrium assumptions of perfect markets and full employment, but the scale of transition required breaks many of these assumptions. Green growth means exploiting the resulting opportunities to mobilise underused resources.  Economists can’t solve the whole problem on their own. Neither can climate scientists, lawyers, sociologists, sustainability experts or people who work on the heath impacts of climate change. All these fields and others need to work together to come up with the best possible policies. But if politicians can’t, or won’t, find a solution to climate change then other actors are waiting to step in. What was once the exclusive realm of national negotiators now sees, for instance, groups of leading companies and major cities calling for coordinated action. Such widening of participation is one of three core suggestions to accelerate progress backed by the Yale Climate Dialogue prior to Lima.  As highlighted by Lord Stern, the key to success in Paris has less to do with wrangling over financial transfers between self-interested governments and more to do with steering the trillions of dollars spent annually on infrastructure and energy transitions in developing countries towards clean, low-carbon options.  There are plentiful ideas and initiatives to make this happen, but before a vision of a sustainable future can become an everyday reality, our leaders need solutions from the research community that reflect the true scale of the problem and its wicked, multi-faceted ways."
"Michael Gove has implicitly criticised the US and Brazilian presidents for their scepticism about the climate emergency, as he refused to comment on speculation that he could be put in charge of government preparations for UN climate crisis talks. Gove, a Cabinet Office minister with a wide-ranging government role, used a speech opening an event looking ahead to COP 26 to express strong views on the UK’s likely role at the summit it is hosting in Glasgow in November.  A series of leading voices in the climate crisis have said the UK seems to be floundering in its preparations for the event, with the perceived chaos and lack of focus exemplified by the sudden dismissal of the former energy minister Claire O’Neill as president of the negotiations. O’Neill later said that Boris Johnson had demonstrated a “huge lack of leadership and engagement” over the event, and that he did not understand the issue. David Cameron subsequently turned down an offer from the prime minister to take over, and the spotlight has since turned on Gove, a former environment secretary whose cabinet role was formerly focused on preparing for a possible no-deal Brexit. But asked after his speech to the Green Alliance conference in London if he would become the new president, Gove said: “I am very happy with the job that I have, and I think there are many, many, many talented people who could do the job of COP president better than I could.” Using his speech to call for concerted global action on the climate emergency at the summit, Gove noted the lack of efforts on the issue by President Trump and the Brazilian leader, Jair Bolsonaro. “I shan’t mention any world leaders by name in a critical fashion,” he began. “However, it’s important in the United States and in Brazil that we recognise that there will be people, at the state and at the city level, who can play a vital role in driving change that we all need to see.” Predicting that nonetheless COP 26 would be a success, Gove pointed to what he called “politically, a realisation of the scale of the challenge and the emergency” across the globe. He promised new green policies from the UK in areas such as transport, energy, and housebuilding, and said there was “a moral responsibility to lead here, as the first country in the world to industrialise”. He said: “We have a responsibility on the first in, first out basis to ensure that the country that pioneered the industrial revolution, and thus played the biggest role in powering the change in our climate that hydrocarbon extraction and burning for energy created, we have a responsibility to lead a green industrial revolution as well.” Gove also said the event in Glasgow should be the most transparent thus far held, with a mission to “invite citizens in” and livestream conversations. There was, however, no detail of new policies or initiatives, or any sign of when a new president would be appointed to get a grip on the preparations for the summit. Gove was briefly heckled when answering a question about what, for him, would constitute a successful summit. As he began by talking about “acceptance of the needs to act”, an audience member shouted that acceptance had been reached 20 years ago, and what was needed was action. Gove continued: “Acceptance of the needs to act leads to action that is irreversible, accelerating and increasing.”"
"
For now, we have about 1 year of significant cold phase tendency in the Pacific Decadal Oscillation (PDO), here is the last 108 years of the PDO index, plotted from monthly values:

Click for larger image – source Steven Hare, University of Washington
Compared to the negative magnitudes seen from 1946 to 1977, our current PDO phase shift magnitude is relatively mild. But that could change. Don J. Easterbrook, a retired professor from the Dept. of Geology, Western Washington University, in Bellingham, WA sends this analysis:
la-nina-and-pacific-decadal-oscillation-cool-the-pacific (PDF)
The announcement by NASA’s Jet Propulsion Laboratory that the Pacific Decadal Oscillation (PDO) had shifted to its cool phase (Fig. 1) is right on schedule as predicted by past climate and PDO changes.
Global temperatures peaked in 1998 and have not been exceeded since then. Pacific Ocean temperatures began a cooling phase in 1999 that was briefly interrupted by El Nino and dramatic cooling in 2007-2008 appears to be a continuation of a global cooling trend set up by the PDO cool phase (Fig. 1) as predicted [shown in the figure below].

Thus, we seem to be headed toward several decades of global cooling, rather than the catastrophic global warming predicted by IPCC. 
If we are lucky, this PDO will be a short event. 2-4 years. If we are unlucky, and it is the “full Monty” phase switch at 20-30 years as Easterbrook suggests, we may be in for extended cooler times. This may result in some significant extended worldwide effects, notably on agriculture.
UPDATE! Professor Easterbrook adds in comments:
“The projected warming from ~2040 to ~2070 is NOT driven by CO2, it’s merely a continuation of warm/cool cycles over the past 500 years, long before man-made CO2 could have been a factor. We’ve been warming up from the Little Ice Age at rate of about 1 degree or so per century and the 2040-70 projection is simply a continuation of non-AGW cycles. 
An interesting question is the similarity between what we are seeing now with sun spots and global temperature and the drop into the Little Ice Age from the Medieval Warm Period. Could we be about to repeat that? Only time will tell–We might see a more pronounced cool period like the 1880 to 1910 cool cycle (when many temp records were set) or a milder cooling like the 1945-1977 cool cycle. In any case, the setting up of the cool phase of the PDO seems to suggest cooler times ahead, not the catastrophic warming predicted by IPCC and Al Gore.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f700a29',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterRecently Hollywood director James Cameron participated in a panel discussion together with economist Tom Friedman, actress Sigourney Weaver, and MSNBC’s Joe Scarborough. Cameron called Climate Change ‘As Great As The Threat’ U.S. Faced in World War II. To me, watching some Hollywood stars give advice on saving the planet is like watching lumberjacks talk about how to do heart surgery. In both cases the result is the same: the patient dies.
http://www.cnsnews.com/news/article/64516
Cameron’s humble abode
Climate protection starts at home. According to more than one source. Cameron’s humbe abode has 8,272 square feet and boasts 6 bedrooms, 7 bathrooms, pool, tennis court and inner courtyard fit for Hollywood royalty – perfect to sit around in with like-minded stars and worry about the “dying planet”, all the while the private jet sits on the tarmac getting refuelled and the chauffeured limousine waits outside.
http://realestock.com/joomla/blog/Inside-the-Homes-of-2010s-Oscar-Nominees.html (scroll down)
Making everyone miserable
Perhaps Cameron’s arrogance, hypocrisy and insistence the rest of the working masses sacrifice more while he continues to live high on the hog has something to do with a character disorder.  According to Wikipedia collaborator and author Orson Scott Card calls Cameron, who has been married five times, selfish and cruel. He said working with him was “hell on wheels”.  Card added, “He was very nice to me because I could afford to walk away. But he made everyone around him miserable, and his unkindness did nothing to improve the film in any way”.
Cameron’s real estate visions: community of a half dozen estates
http://www.therealestatebloggers.com/2007/06/03/james-cameron-selling-730-acres-in-malibu-near-pepperdine-university/
Share this...FacebookTwitter "
nan
"Unfortunately, it’s the 15th anniversary. … We want you to help, because in the years to come, we do not want to write another ad that says: “Unfortunately, it’s the 30th anniversary”. (Bhopal Medical Appeal advertisement, 1999) Unfortunately, it is indeed the 30th anniversary of the the Bhopal gas leak and it remains an ongoing disaster of epic proportions. The world’s worst industrial catastrophe was utterly preventable, stemming from serious negligence on the part of chemical manufacturers Union Carbide.  Despite widespread, creative and steadfast activism, the site of the chemical factory that released tons of lethal gases in December 1984 has not been cleaned up. Toxins continue to leak into the soil and groundwater supply up to 3km from the site of the abandoned factory, causing a third generation to be born with serious health problems and disabilities. This is often described as Bhopal’s “second disaster”. Bhopal is back in the news as the anniversary protests and commemorations make for arresting headlines, but campaigners in the city have been working tirelessly to provide healthcare without state support and to lobby for adequate compensation.  In an era of rapid-fire news reports, ceaseless disasters, and short-term memories, the question is how to retain focus on the effects of disasters that last for decades rather than months or years.  It’s an issue we’re grappling with at Leeds as part of a series of events entitled Reframing Disaster. As researchers working on Bhopal and other global catastrophes, we feel this struggle exposes the dark heart of globalisation and corporate abuse. At the same time, we wanted to show how survivors living in the poorest conditions, experiencing ongoing trauma and neglect, have created networks of solidarity, care, and hope. One of the things we’ve tried to do is put pressure on the idea of disaster as an “event” – in Bhopal’s case, something that happened on 2-3 December 1984 – and instead focus on the long-term nature of catastrophe. This is very effectively dramatised through the arts: creative writing, photography, music and film provide us with lasting reflections on the physical and emotional fallout of large-scale disasters. They go far beyond numbers and statistics and offer a basis for understanding the preconditions for recovery and healing. Contrast this with the technocratic focus in disaster research on mitigation, preparedness and response. Without attention to long-term recovery these sorts of accidents will continue to happen. To understand this dynamic, we need to look towards the profound forms of knowledge that the arts can bring to issues like trauma, injustice and exploitation in the wake of disasters. Take the world-renowned Indian photographer Raghu Rai, whose work provides vivid testimony of the disaster’s effects over time. As one of the first photographers on the spot in Bhopal in 1984, Rai documented the immediate aftermath of “that night”, and return visits over the past 30 years have built up a powerful visual archive of the local hardships. He is currently back in the city once again to document the anniversary commemorations. We’ve been fortunate to be able to collaborate with the Bhopal Medical Appeal to bring an exhibition of Rai’s work to Leeds, along with a presentation of Francesca Moore’s Bhopal: Facing 30 photography,movie screenings and performance poetry by Avaes Mohammad, a former chemist who won an Amnesty International award for his poem Bhopal. People are keen to understand why disasters like Bhopal last so long. After all, there are many Bhopals around the world – whether in the shape of ongoing catastrophes such as the radioactive Spanish town Palomares, the nuclear Pacific or Fukushima where the long-term effects are keeping people vulnerable, or in the shape of disasters waiting to happen.  Activism and the arts are at the forefront of exposing how power and privilege impact upon the poorest communities and continue to place them directly in harm’s way. We need to continue challenging the structural inequalities driving the rise and overwhelming scale of global disasters. The injustice still that surrounds Bhopal after 30 years shows exactly why."
"
Share this...FacebookTwitterHighly anomalous terrain (an active volcano), 40 years of cooling temperatures, and a CO2 record that dramatically contrasts with fluctuating values from forests and meadows reaching 600-900 ppm all beg the question: Is Mauna Loa’s CO2 record globally representative?
Mauna Loa is the Earth’s largest land volcano. It has erupted over 3 dozen times since 1843, making this terrestrial landscape extremely unusual relative to the rest of the globe’s terrain. (Forests, in contrast, cover over 30% of the Earth’s  land surface.)
Mauna Loa has been thought to be the world’s best location to monitor global CO2 levels since 1958.
While Mauna Loa CO2 levels show a rise of 338 ppm to 415 ppm since 1980, Mauna Loa temperatures (HCN) show a cooling trend during this same time period. The only warming period in the last 65 years occurred between about 1975 and 1985.

Image Source: oz4caster
Forest CO2 fluctuations
As mentioned above, forests are orders of magnitude more terrestrially representative than the highly anomalous site of the Earth’s largest volcano.
In forests or tree-covered areas, CO2 rises from around 300 ppm in the warmth of the afternoon (~3 p.m.) to over 600 ppm before sunrise (~4 a.m.), when it is cooler (Fennici, 1986, Hamacher et al., 1994). This massive fluctuation occurs daily and CO2 values average out to be far higher than the Mauna Loa record suggests.

Image Source: Fennici, 1986

Image Source: Hamacher et al., 1994
Meadow CO2 fluctuations
In open fields, or meadows, air CO2 can vary between 266 ppm and 1,430 ppm. The average variance is from 280 ppm to 980 ppm 2 meters above the soil (Szaran et al., 2005).
Interestingly, just as in forests, temperature drops of 4 to 5°C are associated with rising levels of CO2.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Image Source: Szaran et al., 2005
CO2 beneath snow and ice
Modern CO2 concentrations beneath snowpack and ice range from 600 to 1800 ppm. These concentrations can fluctuate by as much as 200 ppm within a period of just 4 days (Massman and Frank, 2006).
If this kind of rapid and wide-ranging variability can be observed for modern conditions, our capacity to accurately assess the “global” CO2 concentration for ice and snow thousands of years old becomes all the more suspect.

Image Source: Massman and Frank, 2006
CO2 near cave entrances
Within caves, CO2 levels can reach as high as 30,000 ppm. Even in the open air <1 meter from the entrance to a cave, CO2 levels can reach 11,500 ppm (Cowan et al., 2013).
CO2 levels vary by 10s of 1000s of ppm from one cave to the next in the same geographical region.

Image Source: Cowan et al., 2013
Mauna Loa CO2 is globally representative?
Cave entrances should probably be considered no less terrestrially unusual than the site of the world largest active volcano. And certainly forests and meadows are far more representative of the Earth’s terrestrial landscape than the Mauna Loa site.
And yet it has been decided, via consensus, that the rarified air above a Hawaiian island in the middle of the Pacific correctly monitors the CO2 levels for the entire globe.
Why?


		jQuery(document).ready(function(){
			jQuery('#dd_ec2a615e01c9d5f09bc7e5a514a859b6').on('change', function() {
			  jQuery('#amount_ec2a615e01c9d5f09bc7e5a514a859b6').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

The security of the United States does not require nearly 1,600 nuclear weapons deployed on a triad of systems — bombers, land‐​based intercontinental ballistic missiles (ICBMs), and submarine‐ launched ballistic missiles (SLBMs) — to deliver them. As Cato’s Benjamin H. Friedman, Christopher A. Preble, and Matt Fay calculate in **“The End of Overkill? Reassessing U.S. Nuclear Weapons Policy”** (White Paper), a smaller arsenal deployed entirely on submarines would save roughly $20 billion annually while deterring attacks on the United States and its allies. The triad grew from the military services’ competition to meet the Soviet threat. The public rationale was based upon the notion of a second strike: a diversity of delivery systems ensured the nuclear arsenal’s survival against a Soviet preemptive attack. The more sophisticated rationale was a first strike: deterring Soviet aggression against European allies required the ability to preemptively destroy their nuclear forces. But, as the authors show, “U.S. power today makes the case for the triad more dubious.” No U.S. adversary has the capability to destroy all U.S. ballistic submarines, let alone all three legs, and there would be time to adjust if that changed. In fact, nuclear weapons are essentially irrelevant in actual U.S. wars, which are against insurgents and weak states without nuclear arsenals. Cases where the success of deterrence hinges on the U.S. capability to destroy enemy nuclear forces are far‐​fetched. “Even hawkish policies do not require a triad,” the authors write. At a time when austerity heightens competition for Pentagon resources, service leaders may see nuclear missions “as red‐​headed step‐​children that take from true sons.” That shift would facilitate major reductions in the nuclear arsenal, the elimination of at least one leg of the triad, and substantial savings.



 **Comparing Welfare to Work**  
In 1995 the Cato Institute published a groundbreaking study estimating the value of the full package of welfare benefits available to a typical recipient in each of the 50 states and the District of Columbia. It found that not only did the value of such benefits greatly exceed the poverty level but, because welfare benefits are tax‐​free, their dollar value was greater than the amount of take‐​home income a worker would receive from an entry‐​level job. Since then, many welfare programs have undergone significant change. In their new analysis, **“The Work versus Welfare Trade‐​Off: 2013”** (White Paper) Michael D. Tanner, senior fellow at the Institute, and Cato research assistant Charles Hughes examine the current system in the same manner. “Welfare benefits continue to outpace the income that most recipients can expect to earn from an entry‐​level job, and the balance between welfare and work may actually have grown worse in recent years,” they write. In fact, the current system provides such a high level of benefits that it acts as a disincentive for work. Welfare currently pays more than a minimumwage job in 35 states, even after accounting for the earned income tax credit, and in 13 states it pays more than $15 per hour. “If Congress and state legislatures are serious about reducing welfare dependence and rewarding work,” the authors conclude, “they should consider strengthening welfare work requirements, removing exemptions, and narrowing the definition of work.” Moreover, states should consider ways to shrink the gap between the value of welfare and work by reducing current benefit levels and tightening eligibility requirements.



 **Subsidizing the Risk of Terrorism**  
The terrorist attacks of September 11, 2001, inflicted enormous losses on the insurance industry and businesses. In the wake of these disruptions, the government enacted the Terrorism Risk Insurance Act of 2002 to create a “temporary” federal backstop against catastrophic losses. In effect, this program subsidized private risk with public funds through a cost‐​sharing program for which the government does not receive any compensation. But as Robert J. Rhee, professor of law at the University of Maryland, writes, “if there was some ambiguity about the program’s need before, there is none now.” In **“The Terrorism Risk Insurance Act: Time to End the Corporate Welfare”** (Policy Analysis no. 736), Rhee argues that terrorism risk is not more severe than other insurable risks such as natural catastrophes. A federal backstop stakes public money to protect the insurance industry, and subsidizes the terrorism risk insurance premiums for commercial policyholders. “The private market is capable of underwriting this risk,” he continues. Yet in response to effective lobbying by the insurance industry and business interests, Congress has twice extended the program. The program is now scheduled to sunset at the end of 2014, 12 years after this supposedly temporary program was instituted. Rhee argues that the program should sunset as scheduled in 2014, thus ending this form of corporate welfare. “After the fears of the unknown have subsided,” he concludes, “[the insurance market] can more rationally assess terrorism risk and price it.”



 **Driving Investment Policy**  
No country has been a stronger magnet for foreign direct investment than the United States. Valued at $3.5 trillion, the U.S. stock of inward foreign direct investment accounted for 17 percent of the world total in 2011, more than triple the share of the next largest destination. In **“Reversing Worrisome Trends: How to Attract and Retain Investment in a Competitive Global Economy”** (Policy Analysis no. 735), Daniel J. Ikenson, director of Cato’s Herbert A. Stiefel Center for Trade Policy Studies, notes that as the world’s largest economy, the United States has been able to attract the investment needed to undergird its position atop the global economic value chain. “But the past is not necessarily prologue,” he argues. Indeed, while the U.S. claim to 17 percent of the world’s stock of foreign direct investment is impressive, the share stood at 39 percent as recently as 1999. To a large extent, this trend reflects the emergence of new, viable destinations for investment resulting from inevitable demographic, economic, and political changes. “However, some of the decline is attributable to a deteriorating U.S. investment climate,” Ikenson writes. That environment conspires to deter inward investment and to encourage companies to offshore operations that could otherwise be performed competitively in the United States. Ikenson concludes that a proper accounting of these policies, followed by implementation of reforms to remedy shortcomings, will be necessary if the United States is going to compete effectively for the investment required to fuel economic growth and higher living standards.



 **Against Military Action in Syria**  
In the midst of growing public wariness about large‐​scale foreign interventions, the Obama administration has decided to arm the Syrian rebels. But according to Erica D. Borghard, a PhD candidate in political science at Columbia University, in **“Arms and Influence in Syria: The Pitfalls of Greater U.S. Involvement”** (Policy Analysis no. 734), those who call for increasing the scope of U.S. aid to the Syrian rebels are wrong on all counts. “There is a high risk that the decision to arm the Syrian rebels will drag the United States into a more extensive involvement later,” she writes — and this is the very scenario that the advocates for intervention claim they are trying to avoid. The unique characteristics of alliances between states and armed nonstate groups — in particular “their informal nature and secrecy about the existence of the alliance or its specific provisions” — create conditions for states to become locked into unpalatable obligations. That seems especially likely in this case. The Obama administration, therefore, should not have decided to arm the Syrian rebels. Looking ahead, Borghard writes, it is important for policymakers to understand the nature of alliances between states and armed nonstate groups even after the Syrian conflict is resolved. “Given that Americans are unwilling to support large‐​scale interventions in far‐​flung reaches of the globe, policymakers looking for military solutions to political problems may conclude that arming proxy groups may be an attractive policy choice,” she concludes. They should instead, however, avoid committing to conflicts that don’t threaten core national security interests.
"
"

With the newspapers full of crises, it can be hard to maintain a proper perspective on the progress humanity has made, and to remember that there are individuals striving every day to make the world a better place. In a recent interview, businessman and philanthropist Bill Gates discussed the improving state of humanity, and the work that he is doing through private charity to help those in need. He said,   




I think the idea that people are worried about problems, like climate change or terrorism or these challenges of the future, that’s okay. But boy, they really lose perspective of what’s happened over the last few hundred years. And how science and innovation have been a central factor of that. And I think that’s too bad, because people are lucky to live now. And they should see that progress is actually taking place faster during their lives than at any time in history.



One of the major initiatives of the Gates Foundation, for example, aims to eliminate polio. The data bear out how much progress has already been made towards that end:   






In 1980, about half of all children received the polio vaccine. Today, around 90% of children receive the vaccine, and eradication of the condition is in sight – just as people eradicated smallpox in 1979.   
  
  
Gates is also among the many caring individuals working to eliminate malaria and malnutrition, areas where humanity has already made great strides. Insecticide‐​treated mosquito nets, for example, protect more children from malaria in Sub‐​Saharan Africa:   






Malnutrition among children is also declining. In populous developing regions, such as East Asia and the Pacific, malnutrition affected about 20% of children in 1990. More must be done, but today malnutrition affects fewer than 6% of children in those areas.   






Even one child afflicted by polio, malaria, or malnutrition is too many, but the dramatic improvements the world has made on these fronts should be celebrated. Like Gates, while working to make the world better we must not lose a proper perspective on the progress humankind has already made.
"
"

The 11th biennial fiscal report card on the governors comes at a time when those leaders have struggled with a sluggish recovery, resulting budget deficits, unemployment, and other economic problems in their states. Many reform‐​minded governors elected in 2010 have championed tax reforms and spending restraint to get their states back on track. Other governors have expanded government with old‐​fashioned tax‐​andspend policies. In **“Fiscal Policy Report Card on America’s Governors: 2012”** (White Paper), Chris Edwards, director of tax policy studies at the Cato Institute, graded all of the states’ governors and awarded four of them A’s — Sam Brownback of Kansas, Rick Scott of Florida, Paul LePage of Maine, and Tom Corbett of Pennsylvania. Five governors were awarded F’s — Pat Quinn of Illinois, Dan Malloy of Connecticut, Mark Dayton of Minnesota, Neil Abercrombie of Hawaii, and Chris Gregoire of Washington. Edwards offers short analyses of each governor’s performance in addition to a letter grade. Many states are facing major fiscal problems in coming years. Rising debt and growing health and pension costs threaten tax increases down the road. At the same time, intense global economic competition makes it imperative that states improve their investment climates. To that end, some governors are pursuing broadbased tax reforms, such as cutting income tax rates and reducing property taxes on businesses. This report discusses those trends and examines the fiscal policy actions of each governor in an attempt to make their records more transparent.





**Amtrak’s Insatiable Appetite for Federal Funds**  
When Congress created Amtrak in 1970, passenger‐​rail advocates hoped that it would become an efficient and attractive mode of travel. But, in **“Stopping the Runaway Train: The Case for Privatizing Amtrak”** (Policy Analysis no. 712), Cato senior fellow Randal O’Toole shows that more than 40 years of operations have disappointed. Amtrak has become the highest‐​cost mode of intercity travel and remains an insignificant player in the nation’s transportation system. Nationally, average Amtrak fares are more than twice as much, per passenger mile, as airfares. Despite these high fares, perpassenger‐ mile subsidies to Amtrak are nearly 9 times as much as subsidies to airlines, and more than 20 times as much as those to driving. When fares and subsidies are combined, its costs per passenger mile are nearly four times as great as airline costs. “A close look at the data reveal that Amtrak has failed for two primary reasons,” O’Toole notes. First, passenger trains simply aren’t competitive in most markets. Second, government control of Amtrak has saddled it with numerous inefficiencies, including unsustainably expensive labor contracts. “No amount of reform will overcome the fundamental problem that, so long as Amtrak is politically funded, it will extend service to politically powerful states even if those states provide few riders,” O’Toole continues. The only real solution, he writes, is privatization. “Simple justice to Amtrak’s competitors as well as to taxpayers demands an end to those subsidies,” he concludes.



 **The Transparency Problem**  
“President Obama has committed to making his administration the most open and transparent in history,” the White​house​.gov website declared just minutes after the new president took office on January 20, 2009. Yet, it is now clear that government transparency has not improved materially since the beginning of President Obama’s administration. According to Jim Harper, Cato’s director of information policy studies, in **“Grading the Government’s Data Publication Practices”** (Policy Analysis no. 711), this is not due to lack of interest or effort. “Along with meeting political forces greater than his promises,” Harper writes, “the Obama transparency tailspin was a product of failure to apprehend what transparency is and how it is produced.” Starting from a low transparency baseline, this administration made extravagant promises and put significant effort into the project of government transparency. It has not been a success. House Republicans, who manage a far smaller segment of the government, started from a higher transparency baseline, made modest promises, and have taken limited steps to execute on those promises. President Obama lags behind House Republicans, but both have a long way to go. The solution, Harper argues, is to tackle the low‐​hanging fruit first. Establishing an authoritative list of programs and projects within the basic units of government, for instance, “is like creating a language, a simple but important language computers can use to assist Americans in their oversight of the federal government.” Harper goes on to lay out a variety of good data publication practices, which he insists can help to produce a more accountable, efficient, and responsive government.



 **The Race for Protection**  
The world is awash in trade‐​distorting subsidies. According to Scott Lincicome, a former employee of Cato’s Center for Trade Policy Studies and now an international trade attorney at White & Case, governments have adopted massive “stimulus” packages since the financial crisis of 2008 — handouts that have included taxpayer subsidies for various industries including agriculture, alternative energy, and automobiles. In **“Countervailing Calamity: How to Stop the Global Subsidies Race”** (Policy Analysis no. 710), Lincicome argues that these subsidies have, in turn, “distorted global markets, bred cronyism, and undermined free trade.” They have also encouraged copycat subsidization, which spawned an increase in litigation at the World Trade Organization and led to the frequent imposition of protectionist duties. Trade reform is badly needed. “Unfortunately,” Lincicome writes, “the U.S. government has little credibility on this issue: it is one of the world’s largest subsidizers, funneling billions of dollars annually to chosen industries, causing economic uncertainty, and creating breeding grounds for corruption.” Yet, ironically, with 59 currently active or pending national countervailing duty (CVD) measures affecting over $11 billion of imports, the U.S. government is also one of the most frequent users of antisubsidy disciplines. The only answer here, according to Lincicome, is true reform. By curtailing federal subsidies to favored industries and by changing CVD procedures to ensure that they serve the rule of international trade law — rather than protectionist objectives — “the U.S. government can reduce market distortions, restore some faith in free markets, and lead national and international subsidy reform initiatives,” he concludes.
"
"The past week saw two significant events in European science. You know about the first one: the triumphant Rosetta mission which landed a probe on a comet. But the other event was less publicised, and much less welcome. The European Commission has decided to scrap its chief scientific advisor (CSA) role. The current CSA, Professor Anne Glover, tweeted about the incredible achievement of the European Space Agency at the same time as her post was being axed.  I’m a chief scientific advisor myself, for the UK government’s Food Standards Agency, so I understand the demands of the role and the value of such advice. Professor Glover’s dignity and continued enthusiasm for European science highlights her integrity and professionalism, something she has brought to the role throughout her tenure. The removal of the CSA post was controversial. In the UK much of the media reaction framed it as an attack on science, or a triumph for the green campaigners who wrote to the EC about Glover’s support of genetically modified crops. But this oversimplifies the issues and doesn’t focus on what does now need addressing, in the wake of Juncker’s decision not to have a CSA – what now for science and evidence in European policy? Just days before the decision, I attended an event celebrating 50 years of UK government CSAs. Attendees heard about a battle during World War II between Lord Cherwell and Henry Tizard, both eminent scientists, over the effectiveness of bombing German civilians. Lord Cherwell said it was the best strategy, but Tizard thought Cherwell’s calculations were hopelessly optimistic – and Tizard turned out to be right. The politically favoured Cherwell had Curchill’s ear however, causing Tizard’s advice to be ignored, and Allied bombers were wasted on unproductive missions.  The government realised scientific advice must be politically independent. Eventually, the ad hoc system favoured by Churchill evolved into an official role that helped create clear blue water between science and politics. The position in Europe was created for similar reasons and thus it is strange how those opposed to it suggest the CSA was not fully capturing the breadth of scientific opinion on certain issues (especially GM crops), suggesting the green campaigners thought the role was political and aimed navigating between opposing views. Of course, this is precisely what it is not. CSAs do gather consensus and the certainty of the evidence – but only among scientists and the evidence they have generated. There needs to be a robust way to link the external world of science and scientific networks with the internal world of government and policy, which is why most UK government departments have a CSA. Of course, this is just one possible model and there are some objections – green campaigners fear too much power is concentrated with one person, for instance – but CSAs essentially work. They are an important and useful way to ensure the government looks at things through a scientific lens.   The Food Standards Agency is currently considering the safety of what some term “risky” foods such as raw drinking milk. As the CSA, I can offer the scientific evidence relating to such food: how to manage the risks, whether to regulate or offer information on best practice for producers and consumers. Of course, my authority only extends so far – I’m a chief adviser, not a chief executive. At the 50th anniversary meeting former CSA Lord May put it well: “Science frames a stage for democratic policy making. It must provide the evidence, not the answer”, he explained. “When a problem arises, it’s imperative to group the best available people to get knowledge and emphasise openness and uncertainties.” This, in a nutshell, is what a CSA is for, and it directly counters the arguments used by people who dislike the concept. The European Commission must address all sorts of complex questions from climate change to space exploration or disease control, and this means different bodies of evidence need to be linked by what Geoff Mulgan calls systems thinking – and this means an independent expert. People who draft policy look through many lenses and science is only one, albeit an important one. EC president Jean-Claude Juncker needs to quickly identify how he will have the clearest and most accessible advice to assist him in making decisions shaped and informed by science. He isn’t off to a great start – the commission CSA’s office will be replaced by the European Political Strategy Centre which has no scientific advisory role. Many of the attacks on Juncker’s decision to abolish the post go too far, but I do wonder and worry how he will receive the words of scientific wisdom in a trusted and consensual way. Europe deserves the best possible link between science and government."
"

As the noted social critic H. L. Mencken once declared, “The whole aim of practical politics is to keep the populace alarmed (and hence clamorous to be led to safety) by menacing it with an endless series of hobgoblins, most of them imaginary.”



In _A Dangerous World? Threat Perception and U.S. National Security_ , a new book edited by Christopher A. Preble and John Mueller of the Cato Institute, a number of scholars ask to what degree the United States is threatened, examining not only the multiplicity of supposed dangers, but also the wisdom or folly behind the measures that have been proposed to deal with them. Paul Pillar, a visiting professor at Georgetown University’s Center for Security Studies, observes that “substate” threats — from terrorism to crime — pale in comparison to traditional threats posed by states. He urges readers to reexamine their preconceived notions of what is required to keep the United States both safe and free. Noting a disconnect between the severity of threats and how much alarm they generate, he voices dismay at the tendency to identify monsters abroad and “conceive of America’s place in the world largely as one of confrontation against them.” In the end, he cautions that the capacity of the United States to curb substate conflict is usually very limited. In fact, intervention itself can be counterproductive.



Eugene Gholz, associate professor of political science at the University of Texas at Austin, explores the economic effects of warfare. Although war itself has many highly undesirable effects, he finds that overseas tensions do not necessarily harm the U.S. economy. Daniel Drezner, professor of international politics at Tufts University, finds some merit in the global public good provided by overwhelming U.S. primacy. However, it is not obvious that military power is the primary driver of the benefit, while sustaining it comes at great cost.



Other contributors include Peter Andreas of Brown University on “Transnational Crime as a Security Threat,” Martin Libicki of the RAND Corporation on “Dealing with Cyberattacks,” and Mark G. Stewart of the University of Newcastle on “Climate Change and National Security.” Although the world will never be free from dangers, we should aspire to understand them clearly. By chipping away at the common perception that the world is getting more dangerous each day, the contributors to this volume attempt to tame the tendency to overreact.
"
"Will Boris Johnson please listen to his own father, rather than Jeremy Corbyn’s climate sceptic brother, on the subject of climate change? It may go against the prime minister’s instincts, but it is the best hope for Britain to live up to its responsibilities in a crucial year for our species. Johnson cannot do this on his own. That much was clear this week during the shambolic London launch of the COP 26 UN climate summit, which will take place in Glasgow in November. This will be the most important international conference in five years and as host the UK will play a leading role in deciding whether it ends in success or failure.  The event was overshadowed by the sacking of the UK’s COP 26 president Claire O’Neill, who responded with a scathing takedown of a premier who she described as “not getting” climate change, failing to devote sufficient resources to summit preparations, and being fundamentally untrustworthy. The firing was not all bad news. O’Neill lacked the political heft and diplomatic skill needed to forge a global consensus on this most divisive of issues. If a more senior politician is appointed in her place, it would show the UK is taking preparations more seriously. The fact that the prime minister is fronting the launch demonstrates an investment of political capital that is commensurate with the challenge. He also said many of the right things, most concretely by confirming the UK’s commitment to go net-zero on carbon emissions by 2050. “Of course it’s expensive, of course it’s difficult, it will require thought and change and action, people will say it’s impossible and it can’t be done, and my message to you all this morning is that they are wrong,” he declared. “I hope it will be a defining year of action for our country, and indeed for our planet, on tackling climate change but also on protecting the natural world.” But can he be trusted and can he deliver? On that question, almost all the evidence – from parliamentary voting records and donations from climate sceptics to his newspaper columns and performance as mayor – says no. Although Johnson has repeatedly called for reduced emissions, he scored zero out of 100 in the Guardian’s climate scorecards last year based on his rejection of actions that would achieve this in five key parliamentary votes. This includes opposition to onshore wind subsidies, emissions-based vehicle taxes and carbon capture and storage. He was also shown to have declared donations of £5,000 from Michael Hintze and £25,000 from Terence Mordaunt (via First Corporate Shipping), who fund the climate science sceptic Global Warming Policy Foundation. The Conservative party says the Guardian cherry-picked votes and failed to recognise Johnson’s support for the net zero emissions target. The Guardian stands by its methodology, which can be read here. Other broader data sources confirm the prime minister’s woeful record on parliament. The parliamentary watchdog They Work for You notes that Johnson has “almost always voted against measures to prevent climate change” with one vote in favour, eight against and six absences (the latter largely due to his period as mayor of London). The gulf between words and deeds was also evident when Johnson was foreign minister, when he oversaw a 60% cut in the UK’s team of climate attaches across the world from 165 to 65, just as the UK was supposed to be delivering the Paris agreement. He then tried to hush up what happened, according to the former chief scientist David King, who claimed Johnson also ignored his advice to be more publicly supportive of climate change and misled the public about King’s agreement to chair an inquiry into building an airport in the Thames estuary. While mayor of London, his record was mixed. On the positive side, he initiated a trial of fully electric buses, raised diesel charges, improved cycling networks and encouraged tree planting and zero-emission taxis. But he also held back a report on the impact of air pollution on deprived schools and curtailed a western extension of the congestion charge zone, which could have reduced traffic and improved air quality. His most famous green boast was that he would lie down in front of bulldozers to prevent a third runway being built at Heathrow airport. But was absent during the 2018 vote on this issue, and strongly supported his own pet airport project – a four-runway mega-terminal in the Thames estuary that never got off the ground. Johnson’s newspaper columns provide copious proof that he does not “get” climate change. In 2012, he described wind farms as “white satanic mills”, supported shale gas fracking and urged readers to “ignore doom merchants” who warn of the dangers of emissions. On three occasions, he blithely – and completely wrongly – claimed global heating was primarily caused by solar activity. “Whatever is happening to the weather at the moment, it is nothing to do with the conventional doctrine of climate change,” he wrote in 2015, citing the debunked claims of the climate sceptic Piers Corbyn – the brother of the Labour leader Jeremy Corbyn. He was scoring points against a political opponent by dismissing the consensus view among scientists that the climate crisis is driven by human activity. The prime minister’s father, Stanley Johnson, is, by contrast, a committed environmentalist who has campaigned for faster action on climate change. At an Extinction Rebellion demonstration last year he declared himself proud to wear the badge of “non cooperative crusties”, which was the criticism levelled by his son at protesters. His influence is apparent in Boris Johnson’s endorsement of various wildlife campaigns, which he presents in colonial fashion as the fault of other nations. A quick glance at his backlist of articles reveals attacks on Japan for whaling, China for the slaughter of pangolins, and a belief that African elephants and cheetahs need to be saved by the UK. On the basis of his diplomatic clumsiness, poor grasp of science and voting record, Johnson appears to be the worst man for the job of making COP 26 a success. But, beyond his dad, there are two other reasons to hope that he might yet turn the pig’s ear of preparations into a silk purse of an event. If the democrat in him – and, perhaps more importantly in his adviser, Dominic Cummings – decides strong climate action is the will of the electorate (as polls overwhelmingly suggest), then he could demonstrate this by pushing it through with the same determination as Brexit. And if the patriot in him makes up his mind to make the UK a global leader in solving the greatest challenge of our age, then he will recognise the need to appoint a strong candidate to replace O’Neill. In this week’s speech, he alluded to this in an encouraging recognition of the UK’s responsibility to decarbonise. “I think it’s quite proper that we should, we were the first, after all, to industrialise. Look at historic emissions of the UK, we have a responsibility to our planet to lead in this way and to do this,” he said. Sadly it seems more likely, given his own history and his squabbles with Nicola Sturgeon over the hosting arrangements, that Johnson will put political partisanship first and bow to pressure from the US to downplay climate concerns in return for a trade deal, and from his donors and supporters to weaken EU environmental standards so the UK economy is more competitive after Brexit. Father will not approve. But Corbyn’s brother, at least, will have reason to smile. • This article was amended on 11 February 2020. A previous version said that, as Mayor of London, Boris Johnson had electrified the city’s bus fleet; it now clarifies that he initiated a trial of fully electric buses."
"
Share this...FacebookTwitterBy Kirye
and Pierre Gosselin
As urban expansion continues worldwide, it wouldn’t surprise anyone that cities would see a growing number of hot days as asphalt, concrete, steel and automobiles act as heat sinks that absorb the summer sun’s energy, a phenomenon known as the urban heat island effect (UHI).
Indeed this has been the case for many German cities over the past decades. Though he climate in Europe has changed over the past 30 years, that change is likely due to natural cyclic pattern changes.
Tokyo not seeing hotter summer highs
But even with a stronger UHI and supposed climate warming, Japan’s sprawling megalopolis of Tokyo has not seen an increase in the number of hot days (days where the thermometer climb to 30°C or higher), as the following chart clearly depicts:

Chart shows the number of days each year (May 1st to October 31st) where the temperature rose to 30°C or more in Tokyo. Data source: Japan Meteorological Agency (JMA). 


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Clearly the number of hot days has little to do with CO2 emissions.
Tokyo October not warming
Also we’ve got the mean temperature data for October 2020 for Tokyo:

Data source: JMA
Above the chart shows that the mean October temperature in the city of Tokyo has not risen in almost 3 decades. In fact it has trended downward a bit, though statistically insignificantly.
So if anyone is claiming Tokyo is getting hotter days and hotter in general, then they either don’t know what they are talking about or they are misleading us.


		jQuery(document).ready(function(){
			jQuery('#dd_a5ebd9bed6ff70015a66e5fd318aaf1e').on('change', function() {
			  jQuery('#amount_a5ebd9bed6ff70015a66e5fd318aaf1e').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterMelting mountain snow in the Canadian Mackenzie Mountains has uncovered ancient weapons used by early hunters. In the Canadian Mackenzie Mountains scientists have found weapons up to 2400 years old, reports Tom Andrews of the Prince of Wales Northern Heritage Centre in Yellowknife and his colleagues in a press release from the Arctic Institute of North America.
http://www.arctic.ucalgary.ca/main/documents/media_release_pdfs/Melting%20ice%20reveals%20ancient%20artifacts.pdf
 Scientists suspect that hunters followed herds escaping mosquitoes and heat during the hot summers. Caribou was an important food source.
 The results of their findings have been extraordinary. Andrews and his team have found 2400-year-old spear throwing tools, a 1000-year-old ground squirrel snare, and bows and arrows dating back 850 years. Biologists involved in the project are examining caribou dung for plant remains, insect parts, pollen and caribou parasites. It is very likely that snow and ice today still covers more ancient relicts.
 The findings and their dates of origin undercut warmists’ claims that the Medieval Warm Period did not exist, or was localised in Europe, and that today’s warm period is unprecedented. The age of the found artefacts correspond to the Roman Warm Period and the Medieval Warm Period.  Ancient artefacts recovered in the Alps tell the same story.

H/T: http://klimakatastrophe.wordpress.com/
Share this...FacebookTwitter "
"
NEW An update to this has been made here:
evidence of a lunisolar influence on decadal and bidecadal oscillations in globally averaged temperature trends
NOTE: This essay represents a collaboration over a period of a week via email between myself and Basil Copeland. Basil did the statistical heavy lifting and the majority of writing, while I provided suggestions, reviews, some ideas, editing, and of course this forum. Basil deserves all our thanks for his labor. This is part one of a two part series.  -Anthony

Evidence of a Significant Solar Imprint in Annual Globally Averaged Temperature TrendsBy Basil Copeland and Anthony Watts 
It is very unlikely that the 20th-century warming can be explained by natural causes. The late 20th century has been unusually warm.
 
So begins the IPCC AR4 WG1 response to Frequently Asked Question 9.2 (Can the Warming of the 20th Century be Explained by Natural Variability?).  Chapter 3 of the WG1 report begins:
Global mean surface temperatures have risen by 0.74°C ± 0.18°C when estimated by a linear trend over the last 100 years (1906-2005). The rate of warming over the last 50 years is almost double that over the last 100 years (0.13°C ± 0.03°C vs. 0.07°C ± 0.02°C per decade).
Was the warming of the late 20th century really that unusual?  In recent posts Anthony has noted the substantial anecdotal evidence for a period of unusual warming in the earlier half of the 20th century.  The representation by the IPCC of global trends over the past 100 years seems almost designed to hide the fact that during the early decades of the 20th century, well before the recent acceleration in anthropogenic CO2 emissions beginning in the middle of the 20th century, global temperature increased at rates comparable to the rate of increase at the end of the 20th century.
I recently began looking at the longer term globally averaged temperature series to see what they show with respect to how late 20th century warming compared to warming earlier in the 20th century.  In what follows, I’m presenting just part of the current research I’m currently undertaking.  At times, I may overlook details or a context, or skip some things, for the sake of brevity.  For example, I’m looking at two long-term series of globally averaged annual temperature trends, HadCRUTv3 and GHCN-ERSSTv2.  Most of what I present here will be based on HadCRUTv3, though the principal findings will hold true for GHCN-ERSSTv2.
I began by smoothing the data with a Hodrick-Prescott (HP) filter with lambda=100.  (More on the value of lambda later.) The results are presented in Figure 1.

Figure 1 – click for a larger image
The figure shows the actual data time series, a cyclical pattern in the data that is removed by the HP filter, and a smoothed long term low frequency trend that results from filtering out the short term higher frequency cyclical component. Hodrick-Prescott is designed to distinguish short term cyclical activity from longer term processes.
For those with an electrical engineering background, you could think of it much like a bandpass filter which also has uses in meteorology:
Outside of electronics and signal processing, one example of the use of band-pass filters is in the atmospheric sciences. It is common to band-pass filter recent meteorological data with a period range of, for example, 3 to 10 days, so that only cyclones remain as fluctuations in the data fields.
(Note: For those that wish to try out the HP filter, a freeware Excel plugin exists for it which you can download here)
When applied to globally averaged temperature, it works to extract the longer term trend from variations in temperature that are of short term duration.  It is somewhat like a filter that filters out “noise,” but in this case the short term cyclical variations in the data are not noise, but are themselves oscillations of a shorter term that may have a basis in physical processes.
For example, in Figure 1, in the cyclical component shown at the bottom of the figure, we can clearly see evidence of the 1998 Super El Niño.  While not the current focus, I believe that analysis of the cyclical component may show significant correlations with known shorter term oscillations in globally averaged temperature, and that this may be a fruitful area for further research on the usefulness of Hodrick-Prescott filtering for the study of global or regional variations in temperature.
My original interest was in comparing rates of change between the smoothed series during the 1920’s and 1930’s with the rates of change during the 1980’s and 1990’s.  Without getting into details (ask questions in comments if you have them), using HadCRUTv3 the rate of change during the early part of the 20th century was almost identical to the rate of change at the end of the century. Could there be some sense in which the warming at the end of the 20th century was a repeat of the pattern seen in the earlier part of the century?  Since the rate of increase in greenhouse gas emissions was much lower in the earlier part of the century, what could possibly explain why temperatures increased for so long during that period at a rate comparable to that experienced during the recent warming?
As I examined the data in more detail, I was surprised by what I found.  When working with a smoothed but non-linear “trend” like that shown in Figure 1, we compute the first differences of the series to calculate the average rate of change over any given period of time.  A priori, there was no reason to anticipate a particular pattern in time (or “secular pattern”) to the differenced series.  But I found one, and it was immediately obvious that I was looking at a secular pattern that had peaks closely matching the 22 year Hale solar cycle.  The resulting pattern in the first differences is presented in Figure 2, with annotations showing how the peaks in the pattern correspond to peaks in the 22 year Hale cycle.
Besides the obvious correspondence in the peaks of the first differences in the smoothed series to peaks of the 22 year Hale solar cycle, there is a kind of “sinus rhythm” in the pattern that appears to correspond, roughly, to three Hale cycles, or 66 years.  Beginning in 1876/1870, the rate of change begins a long decline from a peak of about +0.011 (since these are annual rates of change, a decadal equivalent would be 10 times this, or +0.11C/decade) into negative territory where it bottoms out about -0.013, before reversing and climbing back to the next peak in 1896/1893.  A similar sinusoidal pattern, descending down into negative annual rates of change before climbing back to the next peak, is evident from 1896/1893 to 1914/1917.  Then the pattern breaks, and in the third Hale cycle of the triplet, the trough between the 1914/1917 peak and the 1936/1937 peak is very shallow, with annual rates of change never falling below +0.012, let alone into the negative territory seen after the previous two peaks.  This same basic pattern is repeated for the next three cycles: two sinusoidal cycles that descend into negative territory, followed by a third cycle with a shallow trough and rates of change that never descend below +0.012.  The shallow troughs of the cycles from 1914/1917 to 1936/1937, and 1979/1979 to 1997/2000, correspond to the rapid warming of the 1920’s and 1930’s, and then again to the rapid warming of the 1980’s and 1990’s.
While not as well known as the 22 year Hale cycle, or the 11 year Schwabe cycle, there is support in the climate science literature for something on the order of a 66 year climate cycle.  Schlesinger and Ramankutty (1994) found evidence of a 65-70 year climate cycle in a number of temperature records, which they attributed to a 50-88 year cycle in the NAO.  Interestingly, they sought to infer from this that these oscillations were obscuring the effect of AGW.  But that probably misconstrues the significance of the mid 20th century cooling phase.  In any case, the evidence for a climate cycle on the order of 65-70 years extends well into the past.  Kerr (2000) links the AMO to paleoclimate proxies indicating a periodicity on the order of 70 years.  What I think they may be missing is that this longer term cycle shows evidence of being modulated by bidecadal rhythms.  When the AMO is filtered using HP filtering, it shows major peaks in 1926 and 1997, a period of 71 years.  But there are smaller peaks at 1951 and 1979, indicating that shorter periods of 25, 28, and 18 years, or roughly bidecadal oscillations.  There is a growing body of literature pointing to bidecadal periodicity in climate records that point to a solar origin.  See, for instance, Rasporov, et al, (2004).  A 65-70 year climate cycle may simply be a terrestrial driven harmonic of bidecadal rhythms that are solar in origin.
In terms of the underlying rates of change, the warming of the late 20th century appears to be no more “unusual” than the warming during the 1920’s and 1930’s.  Both appear to have their origin in a solar cycle phenomenon in which the sinusoidal pattern in the underlying smoothed trend is modulated so that annual rates of change remain strongly positive for the duration of the third cycle, with the source of this third cycle modulation perhaps related to long term trends in oceanic oscillations.  It is purely speculative, of course, but if this 66 year pattern (3 Hale cycles) repeats itself, we should see a long descent into negative territory where the underlying smoothed trend has a negative rate of change, i.e. a period of cooling like that experienced in the late 1800’s and then again midway through the 20th century.

Figure 2 – click for a larger image
Figure 2 uses a default value of lambda (the parameter that determines how much smoothing results from Hodrick-Prescott filtering) that is 100 times the square of the data frequency, which for annual data would be 100.  This is conventional, and is consistent with the lambda used for quarterly data in the seminal research on this technique by Hodrick and Prescott.  I’m aware, though, of arguments for using a much lower lambda, which would result in much less smoothing.
In Part 2, we will look at the effect of filtering with a lower value of lambda.  The results are interesting, and surprising.
Part 2 is now online here
NEW An update to this has been made here:
evidence of a lunisolar influence on decadal and bidecadal oscillations in globally averaged temperature trends


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0602b82',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"One of the common problems we have in wildlife biology is knowing how many of an endangered species there are in a particular area. For example, how many orangutans are there in a particular national park in Sumatra? Traditionally this problem was solved by biologist doing on-the-ground surveys: literally walking through the forests counting the number of orangutans and their nests (a proxy of their presence). Tough terrain and the difficulty in seeing the animals high up in the rainforest’s canopy makes this very time consuming. To solve this problem Serge Wich, a primate expert at Liverpool’s John Moores University, came up with the idea of using drones to survey for orangutans and their nests. As commercial quality drones were too expensive for day-to-day use in conservation biology projects, he and his colleagues set about building their own drone and developing appropriate software.  Now from their site you can download instructions to build a high-quality drone for around US$2,000. Conservation drones have proved to be a huge success. In just a few hours they are able to survey huge areas of the rainforest ––something in the past that took weeks or months to do by biologists walking on foot and straining their necks as they looked constantly upwards.  This means that we are now able to better track changes in population sizes.   However, all these drones generate many hours of video, which needs to be analysed. It is important that the time gained by the use of drones is not lost through the need to manually observe the video recorded. Thus, the development of video recognition software to count objects such as orangutans and their nests is well underway. Drones can also be used to protect animals.  Kenya announced this year that it would be using drones to assist its national park rangers in the fight against poaching. Using drones it is possible to monitor remote areas of national parks where herds of elephants or groups of rhinos may fall victim to the poacher’s bullet.   Drones are also useful in forests where they can detect the campfires of poachers or illegal loggers and gather video evidence. Poachers can’t just shoot the drones down either – shooting a drone with an AK47, the poacher’s weapon of choice, is difficult as drones are small, speedy (60kph) and fly at several hundred feet. Drones open up all kinds of interesting areas of research for wildlife biologists.  For example, the HD cameras on a drone means you can obtain images of rainforests that would make most spy satellite images look like a highly pixelated 1970s computer screen. Plus due to the low cost of flying the drones it would be possible to constantly monitor the target environment – something that can be done with satellite images, but at a high cost. The possible applications for drones in conservation biology are almost endless. I would like to use them to film and follow secretive species such as the puma from the Brazilian savannahs (cerrado).  They could also be used to pick up data from remote sensing stations in environments where there is no telecommunication signal. For example, images from camera traps could be uploaded to a drone as it circles overhead – reducing the need to walk for days to download such images. This technology is not without its problems; at the moment low-cost drones are not able to stay in the air for much longer than 45 minutes. But advances in battery technology and the use of solar panels means this issue should disappear in the not too distant future."
"Commuting is always a hot topic. This week, I was invited onto a BBC regional radio station to talk about why we find commuting so stressful: a caller had gone on a rant to producers about local roadworks that were making his daily drive hellish.  The irate caller was typical – 91% of UK workers commute and most find it an ordeal. We Brits do love a moan, so what better to complain about? In this light, the chancellor’s Autumn statement announcement of £15 billion for road schemes up and down England promises an invitation for even greater commuter disgruntlement. While George Osborne says his investment will ease congestion and link up major urban areas, the very act of commuting is actually bad for us. By encouraging more commuting, and especially by further deepening the hold of the car system with the presumption for private ownership, we will just see more of the same: a legion of stressed out commuters miserably trudging to work and home again throughout the week. We are commuting further than ever before, an average of more than nine miles, a trend that reflects the pressure to find and hold down a job in times of austerity. This has predictably negative consequences. Recent research into commuting has shown it makes us unhappy and anxious while lowering our sense of self-worth and fundamentally reducing levels of life satisfaction. The commuting that more than 80% of workers tolerate at an average of an hour a day adversely affects both our physical and mental health. Commuting increases incidences of back, joint and neck pain, with two-thirds of drivers blaming their daily travel for such ailments.  As well as suffering from higher levels of insomnia, commuters are less likely to take regular exercise and more likely to forego wholesome home-cooked food in favour of ready meals and take-away.  A study in California found commuting to be the most significant lifestyle factor behind obesity – the amount of miles travelled directly correlating to weight gain. When we are out of shape, our self-image suffers and there is a strong relationship between obesity and poor mental health. Commuting also tends to make us more isolated. Every 10 minutes of commuting is said to reduce social capital – the networks of friends and acquaintances we can develop – by 10%. We have fewer people to turn to unburden ourselves. This is the loneliness of the crowds. While commuting inevitably means being surrounded by others, they are typically strangers at best – or, more likely, rivals to compete with and be antagonised by. Indeed research suggests  a rise in commuting by car has increased social atomisation and supplanted the idea of community with a heightened level of detached individualism.  Commuting is not only implicated in a decline of civic spirit but can even be attributed as a major cause of marriage break-up with those travelling more than three quarters of an hour to get to work 40% more likely to divorce their partner. The lack of control in commuting is another stress factor, as traffic jams and unpredictable weather mean we are constantly on edge. Some of the worst effects can be found in women. While women tend to work shorter hours and commute less, they are unfavourably impacted by the health issues surrounding commuting. It has been suggested that this trend may result from the generally weaker occupational position women experience but it seems more likely to result from their having to take on greater responsibility for day-to-day household tasks such as childcare and housework.  In particular, this can be found in a practice labelled “trip chaining” as women tend to make more interim stops along the route of their commute, picking up children from schools or purchasing goods at the shops meaning that they have less flexibility and are under more pressure to squeeze in extra activities. A recent study found walking or cycling to work improves mental well-being, as well as the obvious physical benefits. Those who get to work under their own steam are able concentrate better and felt under less strain than when travelling by car.  Even opting for public transport made commuters feel better than driving so the message seems to be to get out of the car if you want to feel better.  Of course, the happiest people are actually those who work at home so, with advances in telecommuting and flexi-time, not going into the office at all would be the ideal. But for those who must commute, the government’s pre-election inducement to make this easier to do by car might seem like good news but, really, will only tie drivers into a practice that is slowly killing them."
"

In February 2012 Gen. Martin Dempsey, the chairman of the Joint Chiefs of Staff, declared, “I can’t impress upon you [enough] that in my personal military judgment, formed over 38 years, we are living in the most dangerous time in my lifetime, right now.”



One year later, he upped the ante: “I will personally attest to the fact that [the world is] more dangerous than it has ever been.” But General Dempsey is hardly alone. Dire warnings about our uniquely dangerous world are ubiquitous. Director of National Intelligence James Clapper testified in early 2014 that he had “not experienced a time when we’ve been beset by more crises and threats around the globe.”



Members of Congress agree. Sen. John McCain (R-AZ), born before World War II, explained in July 2014 that the world is “in greater turmoil than at any time in my lifetime.”



Is it? Do we actually live in a uniquely dangerous world? And, if we do not, why do we believe that we do?



In his magisterial study of the decline in violence worldwide, Harvard’s Steven Pinker posits that “we may be living in the most peaceable era in our species’ existence,” even as he concedes that most people don’t believe it.



If our perceptions aren’t entirely accurate, if the world isn’t, in fact, more dangerous than a decade ago, or a century ago, we could blame our 24/7 media. After all, reporters don’t write about the planes that land safely; the 11 o’clock news never leads with the murder that didn’t happen. Likewise, the stories about the personal information not stolen by identity thieves, the wars that aren’t fought, and the trade and commerce that flows uninterrupted, are rarely told. Moreover, we lack perspective. There is little focus on the threats that no longer threaten. Few talk about the dangers no longer looming. It is rare, even, to find people putting today’s threats in context with the recent past. Or the distant past. Few even bother to ponder the question.



However, is not an easy task. From wars between states to wars within them, from crime and terrorism to climate change and cyber‐​mischief, we are beset by a seemingly endless array of threats and dangers. How canone compare them to past threats, especially given that the judgments of what should or should not frighten us are inherently subjective?



It remains true that the only existential threat to the United States comes from a prospective thermonuclear war — the stuff of countless novels and Hollywood films during the dark days of the Cold War. Who is to say that this event, which has never occurred, is, or should be, more frightening than the very real acts of violence that do take place every day? And does society benefit if our fears of very low probability, high‐​impact events (e.g., global thermonuclear war) were merely supplanted by fears of slightly higher probability, low‐​impact ones? Some might say that it is better to be safe than sorry. That we should worry about all potential threats. By this logic, it is better to fear things that aren’t real than to take too lightly those that are.



Perhaps the tendency to take seriously even seemingly modest dangers has been programmed into our DNA, a product of thousands of years of natural selection. Our distant ancestors who correctly perceived a fourlegged creature charging at them from a distance to be a dangerous predator had time to either flee or defend themselves, and thus lived to procreate. By contrast, their threatdeflating neighbors, who believed the approaching beast to be harmless, realized their error too late and were mauled to death.



But while we have learned to take threats seriously, we are also taught to differentiate the real from the imaginary. Fallacious claims of impending danger will erode one’s credibility, to the point that the congenital fearmonger is no longer taken seriously. The parable warns of the dangers of crying “wolf” when there are no wolves, but it doesn’t teach us to stay silent when we see one. In the parable, the wolf eventually does come, and the dishonest boy is eaten. The moral of the story is not that all dangers are inflated, but rather that the phony ones should not be.



In truth, we should be on the lookout for both kinds of errors. The business world punishes both the imprudent optimist as well as the too‐​gloomy pessimist. The financial analyst who rated all tech startups as “strong buys” in 2000 or the housing speculator who bought multiple condominiums in Miami in 2007 could rightly be cast as too optimistic. On the other hand, extreme risk aversion can blind us to possibilities. And excessive fear can be harmful to both our physical health and emotional well‐​being. The National Institute of Mental Health explains that “excessive, irrational fear and dread” are key symptoms for one of several anxiety disorders, which according to one estimate, afflict 18 percent of Americans.



 **FEAR IS THE HEALTH OF THE STATE**  
But there is a political harm as well. Individual liberty is often threatened during periods of heightened fear and anxiety, a fact that informed the very structure of the U.S. government. James Madison, in making the case for restraining the new government’s war‐​making powers, warned the delegates to the Constitutional Convention in Philadelphia: “The means of defence against foreign danger, have been always the instruments of tyranny at home.”



He went on: “Among the Romans it was a standing maxim to excite a war, whenever a revolt was apprehended. Throughout all Europe, the armies kept up under the pretext of defending, have enslaved the people.” A decade later, Madison returned to this theme in a letter to Thomas Jefferson. Madison knew that there was already some demand for a standing military, and that a few would use fear of foreign threats to whip up public sentiment in favor of a more powerful state. Indeed, Madison postulated “a universal truth that the loss of liberty at home is to be charged to provisions against danger real or pretended from abroad.”



Others since then have stumbled upon similar ideas about popular notions of threats, and of how the fear of threats has been used to grow the power of government. For example, the noted writer, social critic and satirist H.L. Mencken declared “the whole aim of practical politics is to keep the populace alarmed (and hence clamorous to be led to safety) by menacing it with an endless series of hobgoblins, most of them imaginary.”



Madison and Mencken’s warnings remain relevant today. Recall how in November 2008 incoming Obama chief of staff Rahm Emanuel called for swift government action to deal with what he said was an urgent threat. “You don’t ever want a crisis to go to waste,” Emanuel explained in an interview, “it’s an opportunity to do important things that you would otherwise avoid.”



While Emanuel was talking about an economic crisis, an increasingly powerful state can be used in many different ways, regardless of whether it was precipitated by fears of foreign or domestic threats. The same sorts of powers that allowed the Justice Department to go after suspected terrorists allowed the IRS to harass suspected tea partiers.



 **NEW TECHNOLOGIES, NEW FEARS**  
Because inaccurate or misleading characterizations of threats pave the way for the growth of government, it is crucial to understand their true nature.



Thus, is the world more dangerous than ever before? In a word, no. Americans, especially, enjoy a measure of security that our ancestors would envy, and that our contemporaries do envy.



This is not to say that there are no dangers in the world today, as the residents of Tel Aviv and Gaza City will attest. Nor can we say that circumstances will not change for the worse in the future. No one would have predicted that a single act of violence in late June 1914 would precipitate a series of events that culminated in the First World War. Could the 21st‐​century successors to Gavrilo Princip deploy cyber‐​weapons to wound, or even kill, their chosen targets? Could they do more than simply kill a single head of state, but do grievous harm to millions? Or could their actions, as Princip’s did, lead to a major war, which in the nuclear era would result in the deaths of hundreds of millions?



The possibilities cannot be ruled out. But, for now, they are only that: possibilities, and unlikely ones at that. Since their inception, nuclear weapons have been the one true weapon of mass destruction. And following the 9/11 attacks, many believed that they would inevitably fall into the hands of terrorists or other nonstate actors inclined to use them. Still others worry that more nation‐​states will acquire them. The fear of proliferation is not new. In either case caution is warranted, but excessive fear is not. Few countries have ever seriously aspired to possess such weapons, and many of those who did eventually gave up. In a few cases, countries actually turned over their weapons entirely. In fact, for nearly every country in the world, nuclear weapons are more trouble than they are worth.



Terrorists and nonstate actors have, so far at least, come to a similar conclusion. Contrary to the apocalyptic predictions immediately after 9/11, al Qaeda and others have relied exclusively on conventional weapons–chiefly bombs and bullets–to terrorize their victims. They seem to be heeding the advice found in a memo on an al Qaeda laptop seized in Pakistan in 2004: “Make use of that which is available … rather than waste valuable time becoming despondent over that which is not within your reach.”



 **NATION-STATES VS. NONSTATES**  
What of the more traditional threats posed by states? While Vladimir Putin seems to be trying to restore Russia to its place at the top of the enemies list, China is the one country with sufficient size and potential wealth to directly challenge the United States in the future. But it is premature, to say the least, to assume that a war between China and the United States is inevitable. To be sure, U.S. treaty commitments to some of China’s neighbors risk drawing the United States into vexing territorial disputes, and China has been developing military capabilities that could significantly raise the costs for the United States if it chose to back its allies’ claims by force. For now, however, all parties have many reasons to try to resolve these claims peacefully — including the fact that China is the leading trading partner throughout the region. Similarly, the United States and China have many reasons to work together to address other problems beyond the Asia Pacific region.



Many of those common dangers emanate from nonstate and substate actors, from terrorists and insurgents to revolutionaries and rebels. And while these threats are real, they pale in comparison to what states used to do to one another on a regular basis.



Terrorism is, in fact, far less dangerous than widely believed. Consider, for example, that a total of 19 Americans have been killed in four separate terrorist incidents carried out by Islamist extremists on American soil since 9/11. For reference, 50 people were killed in just three separate incidents during a 14- month span in 2012 and 2013 (Aurora, Colorado; Newtown, Connecticut; and the Washington Navy Yard). Excluding U.S. military personnel, fewer Americans have been killed by terrorism globally since 2002 than have died from allergic reactions to peanuts (an average of 50–100 per year).



Although a relatively small number of people are killed or injured by terrorism every year, many people worry that new technology will allow nonstate actors to inflict harm in other ways, say, for example, by attacking the Internet, or company or individual computers connected to it. States, too, are known to have used so‐​called cyberweapons, or have aspired to do so. Thus, numerous U.S. officials have warned that cyberattacks are the single greatest threat to national security. Here again, however, some skepticism is in order. Hackers and criminals are adept at exploiting the vulnerabilities in computer networks, but it is extremely difficult to carry out a major attack with far‐​reaching consequences.



Even an attack that managed, somehow, to crash the banking system completely would be unlikely to undermine confidence in the wider economy, let alone trigger a recession. As the RAND Corporation’s Martin Libicki points out, NASDAQ’s three‐​hour shutdown on August 22, 2013, didn’t spark a wave of panic selling. “It would require data corruption (e.g., depositors’ accounts being zeroed out) rather than a temporary disruption,” Libicki explains, “before an attack would likely cause depositors to question whether their deposits are safe.”



The greater threat may come from measures taken to prevent attacks if they significantly impede legitimate transactions in cyberspace. Similarly, poorly conceived or badly executed policies after the fact might cause more harm than the original incident that precipitated them. The attribution problem compounds the risk. The difficulty in tracking possible cyberattacks to their true source, and in ensuring that the punishment or retaliation is directed at the perpetrators, places a high premium on measured, targeted responses. Maintaining that standard will help ensure a safer world.



Another factor that can explain why the world is becoming less, rather than more, dangerous, is evolving social norms. Harvard’s Pinker notes a perceptible shift in public attitudes toward violence, and documents an associated decline in violent crimes of all types. The rate of such crimes, including murder, rape, and assault, are at or near all‐​time lows. Within the United States, for example, the homicide rate (homicides per 100,000 residents) fell by nearly half (49 percent) in the 20‐​year period from 1992 to 2011.



 **SUPPOSED THREATS TO GLOBAL STABILITY AND ECONOMIC PROSPERITY**  
Still others worry not so much about physical security, but rather about our prosperity and way of life. They fear that a war could cripple the international economy, or that the mere threat of war could disrupt global trade and commerce, including the world’s oil supplies. This concern, at least today, is the main justification for the U.S. military’s forward presence around the world, a posture oriented around stopping possible threats before they materialize.



But the patterns of global trade are far more resilient than the pessimists envision. War between major trading partners is highly unlikely, and, even if it were to occur, trade flows between nonbelligerents would not be disrupted, or not for very long. Indeed, Eugene Gholz shows that when countries shift resources to the purchase of military goods, the result is similar to other consumption binges. During wartime, neutral nations may well benefit economically, as they become a safe haven for investment diverted from warzones, and as they are able to buy certain goods at low cost.



While war itself has many horrific effects, the costs that Americans pay to stop all wars are unlikely to be outweighed by the benefits. There are other risks associated with maintaining a forward military posture. Current U.S. strategy encourages other countries to free‐​ride on the security guarantees provided by the U.S. military, imposing an unnecessary — and ultimately counterproductive — burden, on U.S. taxpayers. Exaggerated fears of distant conflicts could even prompt the United States to fight wars that pose no direct threat to U.S. security and to spend too much on the military, which, in turn, weakens the overall U.S. economy.



 **PUTTING TODAY’S THREATS IN PERSPECTIVE**  
Although the world will never be free from dangers, we should aspire to understand them clearly. Maintaining perspective isn’t easy when we are bombarded with images of fighting from Eastern Ukraine or Gaza. But, in many instances, we are today merely seeing what has always existed beyond our field of vision. Tragic, even horrifying, stories of human suffering do not portend that we are living in a more dangerous world. In most respects, we are living longer, better lives. Our chances of suffering a violent or premature death are very low, and still declining. And our prosperity and broader wellbeing are protected by a dynamic and resilient international economy, and by the spread of powerful ideas that have reduced poverty and disease.



A better understanding of what actually threatens us will help us tame our tendency to overreact. An honest assessment of the threat environment — problems that lurk today and on the horizon — will allow us to redirect some of the money that goes to the Pentagon and military contractors back to the taxpayers and private entrepreneurs. And, recalling Madison and Mencken’s warnings about how and why states exaggerate threats to grow their power, a more accurate assessment of the world’s dangers will ultimately help us to preserve our liberty.
"
"BBC Studios has announced a documentary series about the teenage climate activist Greta Thunberg. The new show will follow Thunberg’s “international crusade” against the climate emergency, focusing on her campaign work as well as her “journey into adulthood”. It will also see Thunberg meet with scientists, politicians and businesspeople to explore the evidence around rising global temperatures. In a statement, the executive producer, Rob Liddell, said: “Climate change is probably the most important issue of our lives so it feels timely to make an authoritative series that explores the facts and science behind this complex subject. To be able to do this with Greta is an extraordinary privilege, getting an inside view on what it’s like being a global icon and one of the most famous faces on the planet.” Stockholm-born Thunberg came to prominence after organising a school strike against the climate crisis in 2018, under the banner Fridays for Future. She has since gained global recognition, and has addressed the United Nations, been nominated for the Nobel peace prize in 2019 and 2020, and been named Time magazine’s person of the year 2019. Thunberg’s activism has prompted criticism from seeming climate science deniers, among them Donald Trump, who described her as having an “anger management problem” in December.  The 17-year-old also remains at the forefront of climate activism. Writing in the Guardian last month, she urged world leaders attending the World Economic Forum in Davos to abandon investments in fossil fuels, describing it as “madness”. Thunberg went on to describe the climate crisis as “extremely complicated, and this is an emergency. In an emergency you step out of your comfort zone and make decisions that may not be very comfortable or pleasant. And let’s be clear – there is nothing easy, comfortable or pleasant about the climate and environmental emergency.” The announcement of the BBC series follows news that the US broadcaster Hulu is making a documentary about Thunberg with the working title Greta."
"

Would a liability insurance mandate for firearm owners provide an effective means of gun control? In the latest issue of _Regulation_ , Stephen G. Gillis and Nelson Lund examine this alternative, which rests on the principle of competitive pressure.



Insurance companies would have an incentive to keep premiums for low‐​risk gun owners low, while charging higher premiums to those who are more likely to cause injury to others. “The benefits to public safety would be modest,” the authors ultimately conclude, “but such a regulation would be preferable to many politically popular gun control proposals that would be ineffective, unconstitutional, or both.”



Timothy D. Lytton next considers our inadequate system of food regulation — pointing in particular to misrepresentative food labeling and outbreaks of illness. These problems, he notes, underscore “the shortcomings of government food regulation and the inadequacy of industry self‐​regulation.”



Yet, there is one niche market in the industry that points to an alternative solution. “The success of kosher food certification offers a model of independent, private certification that could improve food safety and labeling,” Lytton writes, “and point the way toward regulatory reform in other areas such as finance and health care.”



Patrick J. Michaels and Paul C. Knappenberger ask why climate change assessments overlook the differences between the models being used and the empirical data. Jagadeesh Gokhale considers a new approach to Social Security disability insurance reform, noting that more of the disabled would return to work if they faced better incentives.



Other contributors include Ike Brannon, who tackles the economics of sports stadiums in “Could Dan Snyder End Publicly Financed Stadiums?” and M. Todd Henderson, who considers ways to improve corporate governance in “Reconceptualizing Corporate Boards.”



The Fall 2013 issue features book reviews on the causes of and response to the financial crisis, what ended history’s great empires from ancient Rome to modern America, and why state‐​led humanitarian efforts typically fall short of their stated goals. It wraps up with editor Peter Van Doren’s survey of recent academic papers, as well as a final word from Tim Rowland on the demise of automotive dealerships.
"
"
This is environmentalism jumping the shark:

Click image above to play the game
I don’t know where to begin, except to say that when we see things like this, we should complain loudly and incessantly. The Australian Broadcasting Corporation has crossed a line beyond science, beyond decency, and beyond rational thought.
This is what you get after pressing “start”:
 
The screen above says: When you’re done, click on the (skull and crossbones) to find out what age you should die at so you don’t use more than your fair share of Earth’s resources!
Hat tip to CallonJim who writes:
This “kids” games at the Australian Broadcasting Corporation. Tell’s kids depending on their magical “carbon footprint” how long they should live?
The actual title is “Professor Schpinkee’s Greenhouse Calculator – find out when you should die!”
The thing I find amazing is the average foot print is 24.6 tonnes of CO2, which calculates out to 9.3 years old! Where it tells the child “YOU SHOULD DIE AT THE AGE 9.3!” Guess what age this kids games is marketed to? That’s right, 9 year olds.
What is most disgusting about this is that ABC ignores their own published Code of Practice
In section 2.12 they talk about content for children:
2.12 Content for Children. In providing enjoyable and enriching content for children, the ABC does not wish to conceal the real world from them. It can be important for the media, especially television, to help children understand and deal with situations which may include violence and danger. Special care should be taken to ensure that content which children are likely to watch or access unsupervised should not be harmful or disturbing to them.
I venture that any child who takes this carbon footprint test “unsupervised” without mommy and daddy around, and who may be old enough to read, but not old enough to understand he/she is being brainwashed by an agenda, would be “disturbed” find they should die at age nine, since just clicking through with default choices gives you that age.
Here is where you can contact the ABC and give them an inbox full of your opinion. This kind of propaganda needs to be removed.
http://www.abc.net.au/contact/contactabc.htm
UPDATE: There is a row developing in the Austrailian press over this.
UPDATE2: The New York Post highlights this site on June 1st with the headline “Enviro Mental Institution“


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9eb64813',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

August, 2005 — Hurricane Katrina blows into the Gulf of Mexico and blasts New Orleans to smithereens. Environmentalists quickly blame the storm on global warming — or at the very least, claim that warming will inevitably lead to more Katrina‐​like hurricanes. Although there is no clear scientific consensus on what impact a warming world might have on the frequency of big Gulf hurricanes, it’s enough to move public opinion significantly on the question of whether federal, state, and local governments ought to do something about climate change.   
  
  
May, 2006 — Al Gore’s _An Inconvenient Truth_ opens in New York and Los Angeles. The companion book becomes the #1 paperback non‐​fiction book on the New York Times bestseller list in July. The movie goes on to become the fourth highest grossing documentary in U.S. history and wins an Academy Award.   
  
  
July, 2007 — _Live Earth_ concerts to save the planet feature 150 top musical acts in 11 cities around the world. While it’s unclear how many people actually watched those concerts, _Live Earth_ set a record for on‐​line entertainment with over 15 million video streams during the live concerts alone.   
  
  
October, 2007 — Al Gore and the Intergovernmental Panel on Climate Change win the Nobel Peace Prize.   
  
  
March, 2008 — The Heartland Institute sponsors a conference in New York City to showcase scientific skepticism about the seriousness of climate change. The event is received with uncharacteristically loud derision by the mainstream media.   
  
  
Now, with all of that in mind, wouldn’t you think that the public would be growing more — not less — worried about climate change? You might, but you would be wrong. According to today’s _Energy & Environment Daily _(subscription required), a new poll conducted by Princeton Survey Research Associates and released by the John Brademas Center for the Study of Congress at New York University finds that Americans are less worried about climate change than they were a couple of years ago.   
  
  
_E &E Daily_ reports that the survey’s margin of error was +/- 3 percent. Here are the highlights:   
  
  
The percentage of Americans who said global warming requires immediate attention declined from 77 in 2006 to 69 percent today.   
  
  
The percentage of Americans who said they were “very worried” about global warming increased from 31 percent in 2006 to 39 percent in 2008. But that’s misleading; everyone gets “more worried” about everything in a presidential election year. What’s striking to me is that the rise in the number of those “very worried” about global warming was less than the rise in the number of those “very worried” about the four other issues surveyed by Brademas Center (Medicare, Social Security, and energy).   
  
  
The declining number of those who said they were “somewhat worried” about global warming more than offset the increase of those who reported being “very worried.”   
  
  
There are several possible explanations for this data. My guess is that it’s a little of each of the following.   
  
  
Explanation #1 – The public has only limited patience for “end of the world” prognostications. If the world isn’t visibly ending from whatever boogey man is said to menace said world, most of us begin to lose interest. We’re all well aware that Earth has been sentenced to doom hundreds of times over by activists of various stripes but has somehow gained a reprieve time and time again.   
  
  
Explanation #2 – The time horizon of most voters is very, very short. Getting people to voluntarily sacrifice for “the grandkids” or whomever is a near‐​impossible task. It would probably take a Katrina‐​a‐​year … and even then, that might not be enough. The mathematical certainty regarding the economic train wreck about to be visited upon “the grandkids” as a consequence of the trillions of dollars of unfunded liabilities for present federal health care and retirement programs does not engender sacrifice. It engenders shrugs and accelerated wealth transfers from the future to the present.   
  
  
Explanation #3 – Global warming, if it plays out as the IPCC suspects, will be a slow‐​moving event. Panic over climate change has to compete with panic over Islamic terrorism, panic over housing markets, panic over globalization, panic over energy prices, panic over immigration, and episodic panic over dozens of other (usually dubious) worries. Simply put, global warming has a hard time competing with all of the other items on the policy agenda.   
  
  
So conservatives, take heart. Enviros, take a valium.
"
"
Share this...FacebookTwitterSuper Pugh
Faster than a speeding bullet…more powerful than a locomotive…able to leap tall buildings in a single bound! Look, it’s a bird! No it’s a plane! It’s Super-Pugh!
When evil threatens the planet Earth, then it’s Super Pugh to the rescue.
Now Super Pugh is in Nepal, where he just completed a 1 km swim across an icy 2″C lake to save the planet from, yes, you guessed it, manmade global warming read here.
One or two summers ago, publicity-monger and Mr Save-The-Earth Lewis Gordon Pugh attempted canoeing to the North Pole to draw attention to the problem of AGW (not to himself). He didn’t make it, probably too much kryptonite up there.
But now, in Nepal, according to his Hero Website, this was a “Swim for Peace. It is a plea to every nation, to do everything it can, to put a stop to climate change.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Who knows, maybe Super Pugh will get the Nobel Peace Prize this year.
How did he get to Nepal? Like everywhere else his expeditions take him, it’s up up and away – in a jet! (Surely he offsets his super carbon footprints).
Action hero Pugh is concerned that AGW is not being taken seriously enough. According to the Daily Planet, er, the Beeb: 
He urged governments to make tackling climate change a priority and said he was disappointed the issue did not feature more prominently in the UK election. 
It’s not easy being an action hero when nobody cares. Finally, here’s a video of Super Pugh in action!
My hero.
Share this...FacebookTwitter "
nan
"
Share this...FacebookTwitterIn search of particles
Envirozealots are now moving against street sweepers, burning firewood and wood floors, claiming they emit dangerous aerosols. Expect microscopic aerosols to become the next environmental catastrophe.  
The Swiss online news magazine Die Weltwoche has a report by journalist Alex Reichmuth called Environmental Protection Ad Absurdum (in German). 
Environmental protection in Switzerland, like much of Europe,  has fallen into the hands of envirozealots. European ministries of environment are increasingly becoming armies of white-gloved snoopers in search of single molecules of contaminants. And the envirowacko journalists are chiming in, of course.  
In Switzerland the latest environmental catastrophe are airborne microscopic aerosols ( now joining biodiversity, ocean acidification, water consumption and climate change). It’s gotten so bad that now even environmental groups are now getting annoyed.
For example since 1988  it has been a tradition for environmental awareness group Alps Initiative to light a bonfire every August to remind people to protect the Alps from air pollution. This year, however, the event has been banned by the local environmental authorities. The reason, reports Reichmuth:  
The bonfire would harm the climate and pollute the air with microscopic aerosols.  
The Alps Initiative reacted:  
This is making a mountain out of a molehill.  


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Indeed it is. But modern environmentalism has always been about making molehills into mountains, hasn’t it? Just look how life-giving CO2 has been made the culprit for the coming global Armageddon.  
Reichmuth serves up more ad absurdum cases.  
Another example: residents in small villages in Graubünden and in Tessin have been discovered to be suffering from microscopic aerosols emitted by homes burning wood in fireplaces in the wintertime. Yes, it’s about time to close up them romantic fireplaces in Swiss chalets.  
Even street sweepers are now deemed a microscopic-aerosol producing problem. A local newspaper wrote:  
When sucking up dirt, dangerous fine particles are emitted into the air by the sweeper’s air exhaust. And depending on the manufacturer, at alarming rates!  
Wait, it gets worse! That beautiful flooring you have in the rooms in your home? It may be emitting fine aerosols that are dangerous to your health too. Reichmuth writes:  
Anyone with wood or wood laminate floors is living dangerously. According to a German study, rooms with smooth floors produce concentrations of microscopic aerosols that are considerably higher than rooms with carpeting. Concentrations on average were even higher than Swiss daily limits. Taking into account all the victims who have died as a result of the aerosols, then we have to call manufacturers of natural wood floors and wood laminate mass murderers.  
Sounds loony, but let it be a warning of what can happen if you don’t stand up and push this movement back. Although Cap & Trade is in a coma in the US, waiting to wake up after the November elections, the EPA is waiting to swoop down and run every aspect of your lives.
Share this...FacebookTwitter "
"

When the Federal Reserve was created in 1913, its powers were limited and the United States was still on the gold standard. Today the Fed has virtually unlimited power and the dollar has no backing. Limited, constitutional government requires a rules‐​based, free‐​market monetary system with a stable‐​valued dollar. “For this reason,” F. A. Hayek wrote in 1960, “all those who wish to stop the drift toward increasing government control should concentrate their effort on monetary policy.”



To that end, the Cato Institute has launched the Center for Monetary and Financial Alternatives. By leveraging the Institute’s reputation for objective research and sound analysis, the Center will make a comprehensive economic, political, and philosophical case for reform by exploring alternative monetary arrangements. “We’ve assembled a group of scholars who will challenge the Federal Reserve in a way it hasn’t been challenged in 100 years,” Cato president John Allison says. Ultimately, the goal is to build the intellectual foundation for a freemarket banking system.



In November, following the announcement of the Center, the Institute held its 32nd Annual Monetary Conference, bringing together leading scholars and advocates for reform in order to examine the case for sound money. The event was directed, as always, by Cato vice president for monetary studies, James A. Dorn.



In his keynote address, James Grant, the founder and editor of _Grant’s Interest Rate Observer_ , declared that the need for sound money is clear and urgent. “Money is as old as the hills, and credit — the promise to pay money — is as old as trust,” he said. “Yet we still search for an answer.” Grant went on to explain that the notion of sound money is neither clear nor urgent to those who own so much of the other kind.



“I will count us victorious when the name of the chairman of the Federal Reserve Board is just as obscure as that of the chairman of the Weights and Measures Division of the Department of Commerce,” he said. “Come to think of it, the monetary millennium will arrive when the dollar reverts to a tangible weight or measure.”



Throughout the day, panelists discussed a wide range of topics — from the bitcoin revolution and the future of cryptocurrencies to the role of gold in a decentralized monetary regime — before considering the path toward fundamental reform. Judy Shelton, codirector of the Atlas Network’s Sound Money Project, explained that money is supposed to be a tool for measuring value, not a means for implementing economic and social policy. “This monetary anti‐​system we have today is anathema to free trade, to the ideals of Bretton Woods,” she said. “If America still believes in the power of free markets and the potential of free people, we need to fix what broke.”



Gerald P. O’Driscoll Jr., senior fellow at the Cato Institute, proposed the formation of a committee for monetary reform. “To get from talk to action, I propose that those committed to actual monetary reform plan to meet regularly,” he said, “not to discuss current policy but to devise a concrete plan for monetary reform.” Norbert Michel, research fellow at the Heritage Foundation, offered several near‐​term solutions — including reversing quantitative easing and removing the Fed’s regulatory role — that would complement any structural changes that came about.



In his luncheon address, Patrick Byrne, the CEO and chairman of Over​stock​.com, said that the vulnerabilities of the current system stem in part from the vulnerabilities of regulators. “They can be captured by the very same people they’re supposed to go after,” he said. Yet the intellectual climate has never been more open to a critical analysis of existing institutions, both here and abroad.
"
"

NEW An update to this has been made here:
evidence of a lunisolar influence on decadal and bidecadal oscillations in globally averaged temperature trends
Part II
By Basil Copeland and Anthony Watts
 
 
In Part I, we presented evidence of a noticeable periodicity in globally averaged temperatures when filtered with Hodrick-Prescott smoothing. Using a default value of lambda of 100, we saw a bidecadal pattern in the rate of change in the smoothed temperature series that appears closely related to 22 year Hale solar cycles. There was also evidence of a longer climate cycle of ~66 years, or three Hale solar cycles, corresponding to slightly higher peaks of cycles 11 to 17 and 17 to 23 shown in Figure 4B. But how much of this is attributable to value of lambda (λ). Here is where lambda (λ) is used in the Hodrick-Prescott filter equation:

The first term of the equation is the sum of the squared deviations dt = yt − τt which penalizes the cyclical component. The second term is a multiple λ of the sum of the squares of the trend component’s second differences. This second term penalizes variations in the growth rate of the trend component. The larger the value of λ, the higher is the penalty. 
For the layman reader, this equation is much like a tunable bandpass filter used in radio communications, where lambda (λ) is the tuning knob used to determine the what band of frequencies are passed and which are excluded. The low frequency component of the HadCRUT surface data (the multidecadal trend) looks almost like a DC signal with a complex AC wave superimposed on it. Tuning the waves with a period we wish to see is the basis for use of this filter in this excercise.
Given an appropriately chosen, positive value of λ, the low frequency trend component will minimize. This can be seen in Figure 2 presented in part I, where the value of lambda was set to 100. 

Figure 2 – click for a larger image
A lower value of lambda would result in much less smoothing. To test the sensitivity of the findings reported in Part I, we refiltered with a lambda of 7. The results are shown in Figures 3 and 4.

Figure 3 – click for a larger image 
As expected, the smoothed trend line, represented by the blue line in the upper panel of Figure 3, is no longer as smooth as the trend in the upper panel of Figure 1 from Part I. And when we look at the first differences of the less smoothed trend line, shown in Figure 4, they too are no longer as smooth as in Figure 2 from Part I. Nevertheless, in Figure 4, the correlation to the 22 year Hale cycle peaks is still there, and we can now see the 11 year Schwabe cycle as well. 


Figure 4 – click for a larger image 
The strong degree of correspondence between the solar cycle peaks and the peak rate of change in the smoothed temperature trend from HadCRUT surface temperature data is seen in Figure 5. 

Figure 5 – click for a larger image
The pattern in Figure 4, while not as eye-catching, perhaps, as the pattern in Figure 2 is still quite revealing. There is a notable tendency for amplitude of the peak rate of change to alternate between even and odd numbered solar cycles, being higher with the odd numbered solar cycles, and lower in even numbered cycles. This is consistent with a known feature of the Hale cycle in which the 22 year cycle is composed of alternating 11 year phases, referred to as parallel and antiparallel phases, with transitions occurring near solar peaks. 
Even cycles lead to an open heliosphere where GCR reaches the earth more easily. Mavromichalaki, et. al. (1997), and Orgutsov, et al. (2003) contend that during solar cycles with positive polarity, the GCR flux is doubled. This strongly implicates Galactic Cosmic Ray (GCR) flux in modulating global temperature trends. The lower peak amplitudes for even solar cycles and the higher peak amplitudes for odd solar cycles shown in Figure 4 appears to directly confirm the kind of influence on terrestrial climate postulated by Svensmark in Influence of Cosmic Rays on Earth’s Climate (1998)From the pattern indicated in Figure 4, the implication is that the “warming” of the late 20th century was not so much warming as it was less cooling than in each preceding solar cycle, perhaps relating to the rise in geomagnetic activity. 
It is thus notable that at the end of the chart, the rate of change after the peak associated with solar cycle 23 is already in the negative range, and is below the troughs of the preceding two solar cycles. Again, it is purely speculative at this point, but the implication is that the underlying rate of change in globally averaged temperature trends is moderating, and that the core rate of change has turned negative.It is important to understand that the smoothed series, and the implied rates of change from the first differences, in figures 2 and 4, even if they could be projected, are not indications of what the global temperature trend will be. 
There is a cyclical component to the change in global temperature that will impose itself over the underlying trend. The cyclical component is probably dominated by terrestrial dynamics, while the smoothed series seems to be evidence of a solar connection. So it is possible for the underlying trend to be declining, or even negative, while actual global temperature increases because of positive cyclical factors. But by design, there is no trend in the cyclical component, so that over time, if the trends indicated in Figures 2 and 4 hold, global warming will moderate, and we may be entering a phase of global cooling.
Some are probably wondering which view of the historical correspondence between globally averaged temperatures and solar cycles is the “correct” one: Figure 2 or 4? 
Such a question misconstrues the role of lambda in filtering the data. Here lambda is somewhat like the magnification factor “X” in a telescope or microscope. A low lambda (less smoothing) allows us to “focus in” on the data, and see something we might miss with a high lambda (more smoothing). A high lambda, precisely because it filters out more, is like a macroscopic view which by filtering out lower level patterns in the data, reveals larger, longer lived processes more clearly. Both approaches yield valuable insights. In Figure 2, we don’t see the influence of the Schwabe cycle, just the Hale cycle. In Figure 4, were it not for what we see in Figure 2, we’d probably miss some similarities between solar cycles 15, 16, and 17 and solar cycles 21, 22, and 23.In either case, we are seeing strong evidence of a solar imprint in the globally averaged temperature trend, when filtered to remove short term periodicities, and then differenced to reveal secular trends in the rate of change in the underlying long term tend in globally averaged temperatures. 
At one level we see clear evidence of bidecadal oscillations associated with the Hale cycle, and which appear to corroborate the role of GCR’s in modulating terrestrial climate. At the other, in figure 4B, we see a longer periodicity on the order of 60 to 70 years, correspondingly closely to three bidecadal oscillations. If this longer pattern holds, we have just come out of the peak of the longer cycle, and can expect globally average temperature trends to moderate, and increased likelihood of a cooling phase similar that experienced during the mid 20th century. 
In Lockwood and Fröhlich 2007 they state: “Our results show that the observed rapid rise in global mean temperatures seen after 1985 cannot be ascribed to solar variability, whichever of the mechanisms is invoked and no matter how much the solar variation is amplified.” . Yet, as Figure 5 demonstrates, there is a strong correlation between the solar cycle peaks and the peak rate of change in the smoothed surface temperature trend.
The periodicity revealed in the data, along with the strong correlation of solar cycles to HadCRUT surface data, suggests that the rapid increase in globally averaged temperatures in the second half of 20th century was not unusual, but part of a ~66 year climate cycle that has a long history of influencing terrestrial climate. While the longer cycle itself may be strongly influenced by long term oceanic oscillations, it is ultimately related to bidecadal oscillations that have an origin in impact of solar activity on terrestrial climate.
 
UPDATE: We have had about half a dozen people replicate from HadCRUT data the signal shown in figure 4 using FFT and traditional filters, and we thank everyone for doing that. We are currently working on a new approach to the correlations shown in figure 5, which can yield different results using alternate statistical methods. A central issue is how to correctly identify the peak of the solar cycle, and we are looking at that more closely. As it stands now, while the Hodrick-Prescott filtering works well and those results in figures 2,3, and 4 have been replicated by others, but the correlation shown in figure 5 is in question when a Rayleigh method is applied, and thus figure 5 is likely incorrect since it does not hold up under that and other statistical tests. There is also an error in the data point for cycle 11. I thank Tamino for pointing these issues out to us. 
We are continuing to look at different methods of demonstrating a correlation. Please watch for future posts on the subject.
NEW An update to this has been made here:
evidence of a lunisolar influence on decadal and bidecadal oscillations in globally averaged temperature trends
References: 
 
Demetrescu, C., and V. Dobrica (2008), Signature of Hale and Gleissberg solar cycles in the geomagnetic activity, Journal of Geophysical Research, 113, A02103, doi:10.1029/2007JA012570.
Hadley Climate Research Unit Temperature (HadCRUT) monthly averaged global temperature data set (description of columns here) 
J. Javaraiah, Indian Institute of Astrophysics, 22 Year Periodicity in the Solar Differential Rotation, Journal of Astrophysics and Astronomy. (2000) 21, 167-170
Katsakina, et al., On periodicities in long term climatic variations near 68° N, 30° E, Advances in Geoscience, August 7, 2007
Kim, Hyeongwoo, Auburn University, “Hodrick-Prescott Filter” March 12, 2004 
M. Lockwood and C. Fröhlich, Recent oppositely directed trends in solar climate forcings and the global mean surface air temperature, Proceedings of the Royal Society of Astronomy doi:10.1098/rspa.2007.1880; 2007, 10th July
Mavromichalaki, et. al. 1997 Simulated effects at neutron monitor energies: evidence for a 22-year cosmic-ray variation, Astronomy and Astrophysics. 330, 764-772 (1998) 
Mavromichalaki H, Belehaki A, Rafios X, et al. Hale-cycle effects in cosmic-ray intensity during the last four cycles ASTROPHYS SPACE SCI 246 (1): 7-14 1997.
Nivaor Rodolfo Rigozo, Solar and climate signal records in tree ring width
from Chile (AD 1587–1994), Planetary and Space Science 55 (2007) 158–164
Ogurtsov, et al., ON THE CONNECTION BETWEEN THE SOLAR CYCLE LENGTH AND TERRESTRIAL CLIMATE, Geophysical Research Abstracts, Vol. 5, 03762, 2003 
Royal Observatory Of Belgium, Solar Influences Data Analysis Center, monthly and monthly smoothed sunspot number. (Description of data here)
Svensmark, Henrik, Danish Metorological Institute, Influence of Cosmic Rays on Earth’s Climate, Physical Review Letters 15th Oct. 98
Wikipedia, Hodrick-Prescott Filter January 20, 2008


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9feb48b0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Electric eels can incapacitate prey by producing a stunning 660-volt zap of electricity, but what’s really shocking is how they use that power. The mechanism of the eel’s attack was a mystery, but an experimental study published in the journal Science reveals the shocking predatory power of these fish. Scientists have discovered that electric eels use shocks to “remote control” their prey, causing involuntary spasms that reveal the target’s location and prevent its escape. Electric eels – Electrophorus electricus, technically fish rather than true eels – are one of several species of electrical fish but they are the most powerful, with around 80% of their bodies composed of specialised cells that effectively work as biological batteries. They breathe air and surface regularly to do so. A special mucous membrane in their mouths can absorb oxygen from the air, helping them to survive during the dry season. Electric eels live in muddy freshwater in and around the Amazon and Orinoco rivers and their vision is poor. To sense their environment, eels and other electrical fish therefore send out low-voltage pulses of power which they use like a radar. Electrical eels, however, have evolved a high-voltage charge to defend themselves and to capture prey.  We know how the charges work from the eel’s side of things – they start their attack when the battery cells discharge a high-frequency burst of high-voltage pulses. At 660v, these pulses have several times the power of standard mains electricity. However, until now little was known about what the eel’s electrical zaps actually did to their prey. Recent experiments have demonstrated that electric eels have evolved a precise remote control mechanism that takes advantage of their prey’s nervous system. An eel’s electrical discharges can efficiently induce muscle contractions in fish, as they mimic the electrical pulses that the fish’s own neurons send to stimulate muscle movement. The eel effectively hijacks the target’s neural pathways. Escape is prevented by further high-frequency volleys of electricity. These trigger massive, involuntary, whole-body muscle contractions in the prey. This shocking predatory strike ensures detected prey can be immobilised and seized. What’s more, there’s no safe hiding place from this electrical predator. It can emit bursts of charges that cause a massive involuntary twitch in concealed prey (think of how your arm jerks if you brush against an electric cattle-fence).  These charges ask the question: “Anything alive and tasty out there?” The induced response provides the answer, and it also reveals the prey’s location. Nature is yet to come up with a response that allow animals to fight back against the electrical discharges of electric eels. In South America electric eels are known as poraquê – the one who puts you to sleep, and their oil is used to treat rheumatism, osteoporosis, and insect and snake bites. Early attempts to understand electricity made use of electric eels. The great electrical pioneer Michael Faraday, for instance, spent a year experimenting on a less potent relative species that he had shipped over from Guyana specially.  Electric eels can also provide some festive cheer and get people all charged up for Christmas. Aquariums including ones in Japan, the US and Canada have been shocking visitors with their Christmas displays – using electric eels to power the illuminations on their Christmas trees."
"

Big business has too much power in Washington, according to 90 percent of Americans in a December 2005 poll.



Every week, headlines reveal some scandal involving politicians, lobbyists, corporate cash, and allegations of bribes. CEOs get face time with senators, cabinet secretaries, and presidents. Lawmakers and bureaucrats take laps through the revolving door between government and corporate lobbying. Whatever goes on behind closed doors between the CEOs and the senators can’t be good or the doors would not be closed.



Just what is big business doing with all this influence? There are many assumptions about big business’s agenda in Washington. In 2003 one author asserted, “When corporations lobby governments, their usual goal is to avoid regulation.”



That statement reflects the conventional wisdom that government action protects ordinary people by restraining big business, which, in turn, wants to be left alone. Historian Arthur Schlesinger articulated a similar point: “Liberalism in America [the progression of the welfare state and government intervention in the economy] has been ordinarily the movement on the part of the other sections of society to restrain the power of the business community.” The facts point in an entirely different direction:



 **The Big Myth**



The myth is widespread and deeply rooted that big business and big government are rivals—that big business wants small government.



A 1935 _Chicago Daily Tribune_ column argued that voting against Franklin D. Roosevelt was voting for big business. “Led by the President,” the columnist wrote, “New Dealers have accepted the challenge, confident the people will repudiate organized business and give the Roosevelt program a new lease on life.” However, three days earlier, the president of the Chamber of Commerce and a group of other business leaders met with FDR to support expanding the New Deal.



Almost 70 years later _New York Times_ columnist Paul Krugman assailed the George W. Bush administration: “The new guys in town are knee‐​jerk conservatives; they view too much government as the root of all evil, believe that what’s good for big business is always good for America and think that the answer to every problem is to cut taxes and allow more pollution.” At the same time, “big business” just across the river in Virginia was ramping up its campaign for a tax increase, and Enron was lobbying Bush’s closest advisers to support the Kyoto Protocol on climate change.



Months later, when Enron collapsed, writers attributed the company’s corruption and obscene profits to “anarchic capitalism” and asserted that “the Enron scandal makes it clear that the unfettered free market does not work.” In fact, Enron thrived in a world of complex regulations and begged for government handouts at every turn.



When commentators do notice business looking for more federal regulation, they mark it up as an aberration.



When a _Washington Post_ reporter noted in 1987 that airlines were asking Congress for help, she commented, “Last month, when the airline industry found itself pursued by state regulators seeking to police airline advertising, it looked for help in an unlikely place—Washington.” In truth, airline executives had been behind federal regulation of their industry for decades and had aggressively opposed deregulation.



In fact, for the past century and more big business has often relied on big government for support.



 **The History of Big Business Is the History of Big Government**



As the federal government has progressively become larger over the decades, every significant introduction of government regulation, taxation, and spending has been to the benefit of some big business. Start with perhaps the most misunderstood period of government intervention, the Progressive Era from the late 19th century until the beginning of World War I.



President Theodore Roosevelt is usually depicted as the hero of this episode in American history, and his “trust busting” as the central action of the plot. The history books teach that Teddy empowered the federal government and the White House in a crusade to curb the big business excesses of the “Gilded Age.”



A close study of Roosevelt’s legacy and that of Progressive legislation and regulation, however, yields a far different understanding and shows that the experience with meat—big business calling in big government for protection—was a recurring theme. Roosevelt expanded Washington’s power often with the aim and the effect of helping the fattest of the fat cats.



Today’s history books credit muckraking novelist Upton Sinclair with the reforms in meatpacking. Sinclair, however, deflected the praise. “The Federal inspection of meat was, historically, established at the packers’ request,” he wrote in a 1906 magazine article. “It is maintained and paid for by the people of the United States for the benefit of the packers.”



Gabriel Kolko, historian of the era, concurs. “The reality of the matter, of course, is that the big packers were warm friends of regulation, especially when it primarily affected their innumerable small competitors.” Sure enough, Thomas E. Wilson, speaking for the same big packers Sinclair had targeted, testified to a congressional committee that summer, “We are now and have always been in favor of the extension of the inspection, also of the adoption of the sanitary regulations that will insure the very best possible conditions.” Small packers, it turned out, would feel the regulatory burden more than large packers would.



Consider the story of one of the most famous “trusts” in American folklore: U.S. Steel.



In the 1880s and 1890s, rapid steel mergers created the mammoth U.S. Steel out of what had been 138 steel companies. In the early years of the new century, however, U.S. Steel saw its profits falling. That insecurity brought about a momentous meeting.



On November 21, 1907, in New York’s posh Waldorf‐​Astoria, 49 chiefs of the leading steel companies met for dinner. The host was U.S. Steel chairman Judge Elbert Gary. The gathering, the first of the “Gary Dinners,” hoped to yield “gentlemen’s agreements” against cutting steel prices. At the second meeting, a few weeks later, “every manufacturer present gave the opinion that no necessity or reason exists for the reduction of prices at the present time,” Gary reported.



The big guys were meeting openly— with Teddy Roosevelt’s Justice Department officials present, in fact—to set prices.



But it did not work. “By May, 1908,” Kolko writes, “breaks again began appearing in the united steel front.” Some manufacturers were undercutting the agreement by dropping prices. “After June, 1908, the Gary agreement was nominal rather than real. Smaller steel companies began cutting prices.” U.S. Steel lost market share during this time, which Kolko blames on “its technological conservatism and its lack of flexible leadership.” In fact, according to Kolko, “U.S. Steel never had any particular technological advantage, as was often true of the largest firm in other industries.”



In this way, the free market acts as an equalizer. While economies of scale allow corporate giants more flexible financing and can drive down costs, massive size usually also creates inertia and inflexibility. U.S. Steel saw itself as a vulnerable giant threatened by the boisterous free market, and Gary’s failed efforts at rationalizing the industry left only one line of defense. “Having failed in the realm of economics,” Kolko writes, “the efforts of the United States Steel group were to be shifted to politics.”



Sure enough, on February 15, 1909, steel magnate Andrew Carnegie wrote a letter to the _New York Times_ favoring “government control” of the steel industry. Two years later, Gary echoed this sentiment before a congressional committee: “I believe we must come to enforced publicity and governmental control… even as to prices.”



When it came to railroad regulation by the Interstate Commerce Commission, the railroads themselves were among the leading advocates. The editors of the _Wall Street Journal_ wondered at this development and editorialized on December 28, 1904:



Nothing is more noteworthy than the fact that President Roosevelt’s recommendation recommendation in favor of government regulation of railroad rates and[Corporation] Commissioner [James R.] Garfield’s recommendation in favor of federal control of interstate companies have met with so much favor among managers of railroad and industrial companies.



Once again, big business favored government curbs on business, and once again, journalists were surprised.



To cast it in the analogy of Baptists and Bootleggers, the muckrakers such as Sinclair were the “Baptists,” holding up altruistic moral reasons for government control, and the big meatpackers, railroads, and steel companies were the “Bootleggers,” trying to get rich from government restrictions on their business. Roosevelt was allied to the “bootleggers,” the big meatpackers in this case. To get federal regulation, he found Sinclair a handy temporary ally. Roosevelt had little good to say about Sinclair and his ilk; he called Sinclair a “crackpot.”



This preponderance of evidence drove Kolko, no knee‐​jerk opponent of government intervention, to conclude, “The dominant fact of American political life at the beginning of [the 20th] century was that big business led the struggle for the federal regulation of the economy.” With World War I around the corner, this “dominant fact” was not about to change.



The men who gathered at the Department of War on December 6, 1916, struck a startling contrast. Labor leader Samuel Gompers sat at the table with President Woodrow Wilson and five members of his cabinet.



Joining Gompers and those Democratic politicians were Daniel Willard, president of the Baltimore and Ohio Railroad; Howard Coffin, president of Hudson Motor Corporation; Wall Street financier Bernard Baruch; Julius Rosenwald, president of Sears, Roebuck; and a few others. This extraordinary gathering was the first meeting of the Council of National Defense, formed by Congress and President Wilson as a means for organizing “the whole industrial mechanism… in the most effective way.”



The businessmen at this 1916 meeting had dreams for the CND that went far beyond America’s imminent involvement in the Great War, both in breadth and in duration. “It is our hope,” Coffin had written in a letter to the DuPonts days before the meeting, “that we may lay the foundation for that closely knit structure, industrial, civil, and military, which every thinking American has come to realize is vital to the future life of this country, in peace and in commerce, no less than in possible war.”



The CND, after beginning the project of government control over industry, handed much of its responsibility to the new War Industries Board (WIB) by July of 1917. That coalition of industry and government leaders increasingly took control of all aspects of the economy. War Industries Board member and historian Grosvenor Clarkson stated that the WIB strived for “concentration of commerce, industry, and all the powers of government.” Clarkson exulted that “the War Industries Board extended its antennae into the innermost recesses of industry.… Never was there such an approach to omniscience in the business affairs of a continent.”



Business’s aims in the WIB were much higher than government contracts, and certainly business did not lobby for laissez faire. As Clarkson puts it, “Business willed its own domination, forged its bonds, and policed its own subjection.” Business, in effect, shouted to Washington, “Regulate me!” Business called on government to control workers’ hours and wages as well as the details of production.



A decade later Herbert Hoover practiced more of the same. Hoover’s record was one not of leaving big business alone but of making government an active member of the team. As commerce secretary in the 1920s, he helped form cartels in many U.S. industries, including coffee and rubber. In the name of conservation, Hoover “worked in collaboration with a growing majority of the oil industry in behalf of restrictions on oil production,” according to economic historian Murray Rothbard.



In the White House (where history books portray him as a callous and clueless practitioner of laissez faire), Hoover reacted to the onset of the Great Depression by pressuring big business to lead the way on a wage freeze, preventing the drop in pay that earlier depressions had brought about. Henry Ford, Pierre DuPont, Julius Rosenwald, General Motors president Alfred Sloan, Standard Oil president Walter Teagle, and General Electric president Owen D. Young all embraced the policy of keeping wages high as the economy went south.



Hoover praised their cooperation as an “advance in the whole conception of the relationship of business to public welfare… a far cry from the arbitrary and dog‐​eat‐​dog attitude of… the business world of some thirty or forty years ago.”



Before FDR, Hoover got the ball rolling for the New Deal with his Reconstruction Finance Corporation. The RFC extended government loans to banks and railroads. The RFC’s chairman was Eugene Meyer, also chairman of the Federal Reserve. Meyer’s brother‐​in‐​law was George Blumenthal, an officer of J.P. Morgan & Co., which had heavy railroad holdings.



 **The New Deal and Beyond**



After the groundwork laid by the Progressives, Wilson, and Hoover, the alliance of big business and big government continued throughout the 20th century.



“The greatest trick the devil ever pulled,” said Kaiser Soze in the film _The Usual Suspects_ , “was convincing the world he didn’t exist.” In a similar way, big business and big government prosper from the perception that they are rivals instead of partners (in plunder). The history of big business is one of cooperation with big government. Most noteworthy expansions of government power are to the liking of, and at the request of, big business.



If this sounds like an attack on big business, it is not intended to be. It is an attack on certain practices of big business. When business plays by the crooked rules of politics, average citizens get ripped off. The blame lies with those who wrote the rules. In the parlance of hip‐​hop, “don’t hate the player, hate the game.”



This article originally appeared in the July/​August 2006 edition of _Cato Policy Report_



<em>Tim Carney is the author of The Big Ripoff: How Big Business and Big Government Steal Your Money.</em>
"
"

 _Thanks to a last‐​minute “patch,” 23 million Americans were saved from paying an average of $2,000 in additional taxes under the Alternative Minimum Tax in 2007. But the debate over AMT, which is poised to strike again in 2008, continues. On December 6, 2007, on the eve of the AMT patch, Rep. Paul Ryan (R-WI) spoke at a Cato Capitol Hill Briefing on his proposal to repeal AMT and overhaul the current income tax code with a simplified, two‐​rate plan. He was joined by Cato senior fellow Daniel J. Mitchell and Chris Edwards, director of tax policy._



 **REP. PAUL RYAN:** This is about more than just the Alternative Minimum Tax or what kind of tax policy we ought to have. The AMT debate we are in right now is the beginning of an enormous fight we are going to have in this country. We are talking about whether we sanction an everhigher trajectory of federal spending. Fundamentally, we are talking about how big our government is going to get.



The AMT is a federal income tax that is imposed on top of the existing income tax system. In 1969, AMT was passed to go after 155 rich people who were using deductions and loopholes to avoid paying any taxes. And while subsequent tax reform closed those loopholes, the AMT remained. Most critically, the AMT was never tied to inflation, so that today the AMT is targeting an ever‐​increasing fraction of the middle class.



About 20 million Americans were subject to AMT in 2006; 23 million in 2007. Their estimated increased tax liability was about $2,000 per person. According to the Congressional Budget Office, by 2010, if nothing is changed, one in five taxpayers will have AMT liability. Nearly every married taxpayer with income between $100,000 and $500,000 will owe the alternative tax.



So the AMT represents an enormous tax hike on the middle class. Going forward, it will represent an even larger tax increase. That is a major reason it must be repealed. But more centrally, the AMT would massively expand government revenue, which would in turn allow increased government outlays, increased government involvement in the economy, and increased government control over our lives. Meanwhile, many of the proposals to reform AMT come with additional tax hikes that would also mean continued government growth.



Federal revenues as a share of GDP have been about 18.5 percent historically. How much money has the federal government taken out of the U.S. economy, U.S. income, U.S. productivity? About 18.5 percent on average for the past 40 years. The AMT puts a new tax system on top of the current one, bringing us to a historically unprecedented level of taxation in the not so distant future. Of course, most people in Washington think that that’s fine.



That’s why the debate until recently has not just been about getting rid of AMT. It has been about how to replace the supposed “lost revenue.” Congressional Democrats don’t like the AMT because it targets mainly the middle class. Although they want to repeal it, they want to replace it with another revenue machine. For instance, Rep. Charles Rangel (D-NY), House Ways and Means Committee chairman, introduced a major piece of tax legislation in October that, while repealing AMT, would “offset” it through a host of new taxes on high‐​income households and on the private equity industry.



If you want to see what the future of taxation will look like under a Democratic president and Democratic Congress, look no further than Charlie Rangel’s tax bill. It is what he believes in. It is his philosophy. It puts the top federal marginal tax rate in this country at 44.2 percent. That’s the rate small businesses will pay. Meanwhile it raises the rate paid by private equity, venture capitalists, and hedge fund managers from 15 percent to 35 percent.



Now that is what you have to do to the tax code to replace the revenue from an AMT repeal. But as a conservative, I believe we shouldn’t replace that revenue. Let’s agree to keep government where it is. A lot of us could make a good argument for cutting taxes to below where they are now. But let’s at least agree to keep government at about 18.5 percent of GDP, after which we can focus on cutting spending, in particular on entitlement programs.



Because if we buy into this notion that we should have an ever‐​higher revenue baseline, we will take more freedom away from individuals, raise taxes, and make ourselves much less internationally competitive. And it will also lull us into a false sense of having a balanced budget or even a small surplus.



Along with Rep. Jeb Hensarling (R-TX), Rep. Michele Bachmann (R-MN), and Rep. John Campbell (R-CA), I’ve introduced the Taxpayer Choice Act, a bill that would not only eliminate the AMT and the massive tax hike that would come from its automatic expansion. It would also establish a highly simplified alternative to the current income tax system that individuals could choose. Under the current tax system, you fill out an income tax form and an AMT form, and you are obligated to go by whichever is the higher figure. By contrast, our bill gives people the choice of whether they want to pay taxes under the regular income tax or a much simpler and transparent tax system.



The plan woud raise approximately the same amount of revenue that we raise today under the current tax code. It also spreads the income tax burden basically the same as it does today. For those who are concerned about distributional tables, at the recent historical average of 18.5 percent, this is what we call distributionally neutral and revenue neutral.



Now, I want you to think about all the tax expenditure lobbyists who come to get members of Congress to promise not to touch their pet preference in the tax code. From a political perspective, it’s going to be hard to get members of Congress to vote against particular deductions or exemptions given the influence these lobbyists have. It will be much easier to get members of Congress to vote for a clean bill, one that puts that decision in the hands of individual taxpayers.



What would the effect of my plan be on those taxpayers? If they already have their affairs arranged to deal with the exemptions and deductions in the current code, they may opt to continue filing under the current system. But if they prefer a simplified tax form, one with two rates of 10 and 25 percent and little more than that to worry about, then they can opt for that. At the heart of this is a pro‐​growth, profamily profamily, pro‐​entrepreneurial tax system. We’re putting a stake in the ground and saying we don’t want government to grow beyond its current size. We do not accept this Washington doctrine — this Washington dogma — that we have to keep growing government at this ever higher rate.



If my three kids, who are three, four, and five years old, want to have this government for them when they are my age, they will have to pay twice the level of taxation that we have today. Take today’s government, add no new programs to it, take none away, and look ahead 40 years to when my three children will be approximately my age. At that point, they will have to pay 40 percent of GDP in taxes to the federal government just to keep it afloat. This is basically due to entitlement spending.



You can’t have a free and prosperous America with levels of taxation like that. You can’t have an internationally competitive country that can compete with China and India with levels of taxation like that. Yet that is the path we are on right now. And the left is trying to make it worse by proposing new entitlements on top of the ones we have already today.



Let’s recognize the path we are on right now and let’s put out an alternative that is bold but doable to prevent that from happening, so that we can preserve the American legacy: leaving your kids and the next generation with a country and a standard of living that is better than what you have now. That is what this is all about. That is what we hope to achieve.



 **CHRIS EDWARDS:** There is no doubt that tax reform has been stuck in a rut for a while. This year, Congress has been more focused on raising taxes than doing anything about tax reform. A flat tax hasn’t been championed in over a decade when Steve Forbes and Dick Armey did so.



One alternative to our current system is the national sales tax. One version of this, the FairTax has lately been endorsed by Arkansas governor Mike Huckabee to much press and praise. A national sales tax would in principle replace all current federal income with a single national retail sales tax, levied once at the point of purchase of new goods and services. The income tax, the payroll tax, the Medicare tax, capital gains tax, estate taxes, and even the AMT would go in favor of this national sales tax. But in my judgment it’s too dangerous in today’s political climate to even think about moving ahead with the idea of a national sales tax. If a sales tax started moving through Congress, there is no doubt in my mind it would end up being an add‐​on tax to the income tax system, which would be a disaster.



Rep. Charles Rangel (D-NY) has his own problematic proposal to reform the tax code. On the plus side, his bill would abolish AMT. It would also cut corporate tax rates, an area where the U.S. woefully lags behind the rest of the world. But it would replace this “lost revenue” with new tax hikes. In effect, this would amount to a trillion dollar tax hike, because the everexpanding AMT represents a new, additional tax on top of the current system. Congress should consider the pro‐​growth elements of Rangel’s package such as the corporate rate cut, without imposing new taxes on individuals and businesses.



Paul Ryan’s plan is by far the best of the bunch. It is a very credible, very pro‐​growth proposal, a way of moving ahead with tax reform, and a big step toward a Dick Armey or Steve Forbes flat tax.



Let me just give you a couple of things that I think are interesting about the Taxpayer Choice Act. I’m all for a flat tax. A flat tax would be optimal in terms of efficiency and fairness, in my view. But unfortunately, the current static revenue estimation methods up here on Capitol Hill provide easy fodder for opponents of a flat tax, who claim the flat tax is unfair.



So to move ahead with tax reform, I think a good idea is to enact essentially a flat tax but with two rates. The Taxpayer Choice Act has two tax rates, one at 10 and one at 25 percent. Those aren’t picked out of the air. If you look at people at the very top of the income distribution, they pay an effective rate of about 25 percent. That is to say, their total taxes divided by income comes to about 25 percent.



If you look at the broad middle class, people making from about $50,000 to $100,000, they have an effective tax rate currently of about 10 percent. This plan hits the same sort of distribution, in a static sense, as the current tax code.



Some folks looking at the details might criticize dropping the top rate from 35 to 25 percent. They might claim that it is a giveaway to the rich. But, again, the effective rate of those at the top of the distribution is 25 percent currently.



What’s interesting about the current tax codes is that the 25 percent tax rate starts at a very low income level. If you’re single and you earn an adjusted gross income of $40,000, you start getting hit by the high 25 percent tax rate. Under Ryan’s plan, that 25 percent tax rate doesn’t start until about $66,000. So there is a big chunk of people in the middle who would have a sharp marginal tax rate cut under the plan.



I think that the Taxpayer Choice Act is an excellent plan. Admittedly, one of the reasons why I think so is that I introduced something similar a few years ago in a February 2005 Cato Tax & Budget Bulletin, “A Proposal for a ‘Dual‐​Rate Income Tax.’ ” One thing that I included in my plan was a sharp corporate tax rate cut as well. If I were to add one thing to Ryan’s plan it would be to lower the corporate rate 35 percent down to 25 percent — at the least.



There has been a lot of discussion this year about corporate tax rate cuts. As mentioned before, even Rangel’s proposal includes one. Bear in mind that in Europe right now the average corporate tax rate is just 24 percent. At 35 percent, the United States has the second highest corporate tax rate in the world. And yet despite this, we have fairly low corporate revenues. Indeed, according to my analysis, we are in the Laffer curve range for the corporate tax rate, where cutting the rate down to 25 percent would mean no revenue loss for government at all.



A corporate tax cut is long overdue. We should add a corporate rate cut to the Paul Ryan tax plan, after which we would have a real winner for businesses and, frankly, for the government, which would probably get more revenue.



 **DANIEL J. MITCHELL:** What is good tax policy? Rates should be low. You shouldn’t double tax. There should be no special loopholes. It’s that simple.



Why have a low rate? Because that’s the price on productive behavior. Politicians understand this, whether they admit it or not. For instance, they institute higher cigarette taxes to diminish smoking. While I may not think that is government’s job, they get an A+ for economics. The higher the tax on something, the less you get of it. But I get frustrated by the fact that they don’t apply this same lesson to work, saving, investment, and entrepreneurship.



Meanwhile, lots of empirical data shows that once you get tax rates at 20 percent or below, people aren’t really going to worry about evasion and avoidance; they are going to focus on being productive. That’s another reason to keep rates low.



Now, why should income only be taxed one time? Because even if you have low tax rates, if you cycle income through the tax code more than once your effective tax rate can be very high.



Every economic theory agrees that capital formation via saving and investment is the key to long‐​run growth. Even radical socialists who believe government should do the saving and investing agree on this point. But in America there are four different layers of taxes that a single dollar of income may be hit with: the capital gains tax, the corporate income tax, personal income tax, and the estate tax.



So even if you get all those rates down to 20 percent, by the time the IRS gets four different bites at the apple, your effective tax rate can be very, very high. The government should not punish the very thing that everyone agrees is critical to long‐​run growth. Why should the tax code be neutral? Because the government should not be in the business of picking winners and losers. Issues of fairness aside, this leads to the misallocation of resources.



If you do everything right, you wind up with a postcard‐​size tax form. And even if you do a few compromises with it, like Congressman Ryan does, you can just have a bigger postcard. But if you go to the IRS Web site and you go to “Forms and Publications,” there are more than 1,100 different forms and publications you can download. Wouldn’t a postcard‐​size form be better?



Now, let me bring it back to some of the things that are relevant to policy work on Capitol Hill. Some people make the interesting argument that the AMT is like a flat tax. After all, it doesn’t have many of the exemptions and deductions of our current tax code. Meanwhile, it taxes income at, alternately, 26 or 28 percent depending on income, which is pretty close to a single tax rate.



But a flat tax isn’t just about having one rate. It’s also getting rid of double taxation. And the only thing similar between the tax base of an AMT and the tax base of a flat tax is you get rid of state and local tax deductions. That’s actually privately one of the reasons I’m amused by the AMT. You have all these high‐​tax states, like California and New York, complaining about it.



Now, what about Mr. Ryan’s plan? It’s not a flat tax either. It too has two rates. But marginal tax rates are going down. Productive behavior is not being excessively penalized. Government will be prevented from growing as it would under an allencompassing AMT. It represents progress.
"
"

 _The Current Wisdom_ is a series of monthly posts in which Senior Fellow Patrick J. Michaels reviews interesting items on global warming in the scientific literature that may not have received the media attention that they deserved, or have been misinterpreted in the popular press.



 _The Current Wisdom_ only comments on science appearing in the refereed, peer‐​reviewed literature, or that has been peer‐​screened prior to presentation at a scientific congress.



Bet you haven’t seen this one on TV: A newer, more sophisticated climate model has lost more than 25% of its predicted warming! You can bet that if it had predicted that much more warming it would have made the local paper.



The change resulted from a more realistic simulation of the way clouds work, resulting in a major reduction in the model’s “climate sensitivity,” which is the amount of warming predicted for a doubling of the concentration of atmospheric carbon dioxide over what it was prior to the industrial revolution.



Prior to the modern era, atmospheric carbon dioxide concentrations, as measured in air trapped in ice in the high latitudes (which can be dated year‐​by‐​year) was pretty constant, around 280 parts per million (ppm). No wonder CO2 is called a “trace gas” — there really is not much of it around.



The current concentration is pushing about 390 ppm, an increase of about 40% in 250 years. This is a pretty good indicator of the amount of “forcing” or warming pressure that we are exerting on the atmosphere. Yes, there are other global warming gases going up, like the chlorofluorocarbons (refrigerants now banned by treaty), but the modern climate religion is that these are pretty much being cancelled by reflective “aerosol” compounds that go in the air along with the combustion of fossil fuels, mainly coal.



Most projections have carbon dioxide doubling to a nominal 600 ppm somewhere in the second half of this century, absent no major technological changes (which history tells us is a very shaky assumption). But the “sensitivity” is not reached as soon as we hit the doubling, thanks to the fact that it takes a lot of time to warm the ocean (like it takes a lot of time to warm up a big pot of water with a small burner).



So the “sensitivity” is much closer to the temperature rise that a model projects about 100 years from now — assuming (again, shakily) that we ultimately switch to power sources that don’t release dreaded CO2 into the atmosphere somewhere around the time its concentration doubles.



The bottom line is that lower sensitivity means less future warming as a result of anthropogenic greenhouse gas emissions. So our advice… keep on working on the models, eventually, they may actually arrive at something close puny rate of warming that is being observed



At any rate, improvements to the Japanese‐​developed Model for Interdisciplinary Research on Climate (MIROC) are the topic of a new paper by Masahiro Watanabe and colleagues in the current issue of the _Journal of Climate_. This modeling group has been working on a new version of their model (MIROC5) to be used in the upcoming 5th Assessment Report of the United Nations’ Intergovernmental Panel on Climate Change, due in late 2013. Two incarnations of the previous version (MIROC3.2) were included in the IPCC’s 4th Assessment Report (2007) and contribute to the IPCC “consensus” of global warming projections.



The high resolution version (MIROC3.2(hires)) was quite a doozy — responsible for far and away the greatest projected global temperature rise (see Figure 1). And the medium resolution model (MIROC3.2(medres)) is among the Top 5 warmest models. Together, the two MIROC models undoubtedly act to increase the overall model ensemble mean warming projection and expand the top end of the “likely” range of temperature rise.



FIGURE 1





Global temperature projections under the “midrange” scenario for greenhouse‐​gas emissions produced by the IPCC’s collection of climate models. The MIROC high resolution model (MIROC3.2(hires)) is clearly the hottest one, and the medium range one isn’t very far behind.



The reason that the MIROC3.2 versions produce so much warming is that their sensitivity is very high, with the high‐​resolution at 4.3°C (7.7°F) and the medium‐​resolution at 4.0°C (7.2°F). These sensitivities are very near the high end of the distribution of climate sensitivities from the IPCC’s collection of models (see Figure 2).



FIGURE 2





Equilibrium climate sensitivities of the models used in the IPCC AR4 (with the exception of the MIROC5). The MIROC3.2 sensitivities are highlighted in red and lie near the upper und of the collection of model sensitivities. The new, improved, MIROC5, which was not included in the IPCC AR4, is highlighted in magenta, and lies near the low end of the model climate sensitivities (data from IPCC Fourth Assessment Report, Table 8.2 and Watanabe et al., 2010).



Note that the highest sensitivity is not necessarily in the hottest model, as observed warming is dependent upon how the model deals with the slowness of the oceans to warm.



The situation is vastly different in the new MIROC5 model. Watanabe _et al_. report that the climate sensitivity is now 2.6°C (4.7°F) — more than 25% less than in the previous version on the model.[1] If the MIROC5 had been included in the IPCC’s AR4 collection of models, its climate sensitivity of 2.6°C would have been found near the low end of the distribution (see Figure 2), rather than pushing the high extreme as MIROC3.2 did.



And to what do we owe this large decline in the modeled climate sensitivity? According to Watanabe _et al._ , a vastly improved handling of cloud processes involving “a prognostic treatment for the cloud water and ice mixing ratio, as well as the cloud fraction, considering both warm and cold rain processes.” In fact, the improved cloud scheme — which produces clouds which compare more favorably with satellite observations — projects that under a warming climate low altitude clouds _become a negative feedback_ rather than acting as positive feedback as the old version of the model projected.[2] Instead of enhancing the CO2‐​induced warming, low clouds are now projected to retard it.



Here is how Watanabe _et al_. describe their results:



A new version of the global climate model MIROC was developed for better simulation of the mean climate, variability, and climate change due to anthropogenic radiative forcing… .



MIROC5 reveals an equilibrium climate sensitivity of 2.6K, which is 1K lower than that in MIROC3.2(medres).… This is probably because in the two versions, the response of low clouds to an increasing concentration of CO2 is opposite; that is, low clouds decrease (increase) at low latitudes in MIROC3.2(medres) (MIROC5).[3]



Is the new MIROC model perfect? Certainly not. But is it better than the old one? It seems quite likely. And the net result of the model improvements is that the climate sensitivity and therefore the warming projections (and resultant impacts) have been significantly lowered. And much of this lowering comes as the handling of cloud processes — still among the most uncertain of climate processes — is improved upon. No doubt such improvements will continue into the future as both our scientific understanding and our computational abilities increase.



Will this lead to an even greater reduction in climate sensitivity and projected temperature rise? There are many folks out there (including this author) that believe this is a very distinct possibility, given that observed warming in recent decades is clearly beneath the average predicted by climate models. Stay tuned!



 **References:**



Intergovernmental Panel on Climate Change, 2007. Fourth Assessment Report, Working Group 1 report [4], available at http://​www​.ipcc​.ch.  
Watanabe, M., et al., 2010. Improved climate simulation by MIROC5: Mean states, variability, and climate sensitivity. _Journal of Climate_ , **23** , 6312–6335.  
[1] Watanabe et al. report that the sensitivity of MIROC3.2 (medres) is 3.6°C (6.5°), which is less that what was reported in the 2007 IPCC report. So 25% is likely a conservative estimate of the reduction in warming.  
[2] Whether enhanced cloudiness enhances or cancels carbon‐​dioxide warming is one of the core issues in the climate debate, and is clearly not “settled” science.  
[3] Degrees Kelvin (K) are the same as degrees Celsius (C) when looking at relative, rather than absolute temperatures.
"
"Westminster’s energy strategy to “keep the lights on” by relying on new nuclear build is looking increasingly like a recipe for economic ruin and political disarray. George Osborne, the chancellor, confirmed in this week’s Autumn Statement a co-operation agreement with a Franco-Japanese consortium to build a new plant at Moorfield in Cumbria as part of his national infrastructure plan.  There is already such an agreement in place for another plant at Wylfa Newydd in Wales, and of course a full deal agreed with the Franco-Chinese project to build Hinkley Point C in Somerset – the first new station in the UK in a generation. Yet that latter project’s huge estimated cost increase illustrates exactly what is wrong with nuclear – and why global sentiment has swung against it as the real costs become clearer.  Westminster’s claim that Hinkley Point C would cost £16 billion has been countered by experts at the EU who have placed the cost at nearer £25 billion (and note the original estimate was £10 billion). The deal involves paying twice the current price for electricity, with UK taxpayers and electricity consumers locked into a binding contract for an extraordinary 35 years.  The European Commission raised concerns that Westminster had breached state aid rules in the subsidies being offered to finance the project. Energy secretary Ed Davey’s huge sigh of relief in October, when the EC controversially gave the green light for the project, may be premature: it will be challenged by the Austrian government in the EU courts. Even if these obstacles can be surmounted, the financial risks to these kinds of projects are simply huge. Severe delays to new-build stations at Olkiluoto in Finland and Flamanville in France demonstrate a systemic problem. The Level-7 nuclear incidents at Fukushima and Chernobyl are evidence of the safety issues that are forever present – and then there is the insoluble problem of nuclear waste and the astronomical eventual decommissioning costs.  According to government figures, the waste problem at Sellafield alone hit nearly £68 billion — and that was almost two years ago. Little wonder that there is both local opposition to Hinkley Point and a fair amount of concern about nuclear power across the British public.     Areva’s failing financial performance must also be of significant concern to the project. Its share price plummeted on November 19 and is yet to recover amid a slew of profit warnings and multi-billion-euro debts.    To blame for the company’s predicament are its exposure to the French and Finnish nuclear projects, Japan’s reluctance to resume its nuclear programme and reticence in other countries, not least Germany. If this leads to a restructuring at the company, it brings into question the future of Hinkley Point. While the project is being led by EDF, Areva is providing its European Pressurised Reactor technology and has a 10% equity stake in it. Without the company’s injection of billions of pounds and its technological know-how, the project has to be in jeopardy.  Another issue is who should provide these projects. With Areva and EDF both under French state control, critics have said that the project amounts to the UK treasury writing a “blank cheque” to the French government. The same could be said of China General Nuclear Corporation and China National Nuclear Corporation, who came onboard last year.  EDF is also reputedly planning to hand over an additional and significant financial stake in Hinkley Point to other foreign corporations. Saudi Electric is reportedly in talks, while the Qataris have confirmed an interest too. British involvement in the project has been non-existent since the UK’s Centrica left a cavernous hole in the project by ceasing involvement in early 2013. Should de facto control of such an important element of our national electricity security be placed in the hands of foreign corporations?  Taken together, these sets of very deep concerns mean that nuclear can only be an option of last resort. To the astonishment of many, the Telegraph recently reported that the former UK chief scientist and nuclear “salesman”, has arrived at the same conclusion. Given this analysis, the Scottish government would appear more than justified in using its extensive planning powers to reject new nuclear build. In light of this and the fragility of future fracking prospects, Westminster would be wise to rethink its national energy policy and give more and not less support to onshore and offshore wind and other marine renewables.  To get an alternative viewpoint on nuclear power, now read this piece."
"In a shallow pool amid a mossy landscape is a trap, a tiny triggered vacuum that sucks in unexpected prey at great speed, absorbs what it needs, then ejects the empty husk of its victim. If you’ve sunk and splashed your way through a peat bog in summer, you may have caught a glimpse of the plant’s more alluring feature, the showy yellow flowers that wave above the water. Bladderworts are free-floating aquatic plants that sink back in winter to tight buds, washed along in the currents of wilder weather. They are not alone in their bizarre eating habits. There are sundews whose hundreds of pin-shaped tentacles wrap their sticky digestive juices around their prey, and butterworts, which possess the strongest glue in nature to trap hapless insects wandering over them, among the heathers and layers of sphagnum moss that make up peatland.  Peat bogs are incredible ancient landscapes; layers of dead plants, only partially decomposed due to the anaerobic conditions, turn into a spongy black soil. A single bog can be more than 1,000 years old, and it takes that time again to develop a deposit of a metre or so of new peat. It’s renewable, but not on a timescale we understand. With nutrients in short supply, everything here grows very slowly; it takes sphagnum moss a quarter of a century to grow 2.5cm. Those carnivorous plants grab at any nutrients they can to stay alive, hence trapping insects to feed themselves. Peatlands are considered the most efficient carbon sinks on Earth. The plants that grow in them capture the carbon released by the peat, maintaining an equilibrium that we cannot afford to lose. Extracted and degraded peat bogs do the opposite: they release a lot of carbon dioxide. It goes without saying that we can’t afford to destroy them while the world burns. About 10 years ago, the Department for Environment, Food and Rural Affairs (Defra) promised that it would phase out peat use in amateur horticulture by 2020. This was a voluntary scheme, and it’s predictably failed. Walk into nearly any garden centre and there’s plenty of peat on offer, from composts to growing media for houseplants, bedding plants and so on. Peat use in horticulture is by no means the only issue: there’s burning peat for fuel, and the vast quantities used in mushroom growing. But this is the bit you can influence. Stop buying peat-based compost (the ingredients are on the back), buy plants from the many good folk who already grow peat-free, and ask your local garden centre to commit to being #peatfree. You have power – use it to preserve peat."
"Mysterious, seemingly coordinated, drones have appeared in the past month over a number of nuclear power stations in France. We don’t know what these flights are for or who is behind them. But perhaps the most crucial question they raise is whether this now widespread technology poses a threat to nuclear facilities. Drone flights were first reported over at least 13 nuclear facilities in October.  The flights have taken place mostly at night, involving drones of different sizes and capability, from smaller models that would need to be operated within the immediate vicinity to larger ones around two meters in size, which could be controlled from kilometres away. Flights have been carried out both in isolation and concurrently, with drones flown simultaneously over nuclear facilities hundreds of miles apart. It is difficult to assess the risk posed by the recent drone flights as at this point it is unclear who is behind them and crucially what their intentions and capabilities are.  The fights are in breach of the 2.5km no-fly zones, which protect the air space around French nuclear power plants. However, nuclear operator EDF has been quick to down-play the potential threat noting, “these objects are not capable of damaging anything if they fall, nor is any object they might drop.”  Since the 1980s, French nuclear regulation has considered the risk of light aircraft crash, dictating that certain nuclear power plants had to be able to withstand a collision by an aircraft of 5.7 tonnes (the size of a small private jet). Following 9/11 the French authorities will undoubtedly have revisited the issue. However drones themselves are relatively light and slow: a nuclear plant should easily brush off the impact of a crash.  More worryingly, drones could be used to carry explosives for detonation close to the reactor or other sensitive parts of a nuclear site, although there have been no reports to date that these drones have been carrying a malicious cargo.  Although this may seem alarming, the use of explosives to cause a radiological release or disruptive plant operations will have been assessed by nuclear operators when designing onsite security systems.  A typical reactor design includes a high strength steel pressure vessel at least a foot thick to contain the reaction and a concrete containment structure several feet thick. While this is mostly in place for safety reasons, it also brings benefits in terms of security. That said this specific attack route – involving drones as a delivery vehicle – may not have been considered as the wide availability of this technology is a recent phenomenon. If so France and other countries will no doubt be in the process of updating their Design Basis Threat, a restricted document that outlines the capabilities and intentions of potential adversaries and serves as a guide against which physical protection systems are designed and evaluated. Drones could also have other malicious uses. When mounted with small cameras, they could be used to conduct reconnaissance or to test security provisions before carrying out a follow-up attack by other means. Or they could be potentially used to drop equipment onsite to help out a malicious insider. A recent case in Belgium involving the sabotage of non-nuclear systems of a power station by an employee highlights that insiders can pose a real threat.   The malicious use of drones – although unlikely – is certainly a threat worthy of consideration. However, the actors and intentions behind the French cases still remains a mystery.  A group of three model aeroplane enthusiasts were arrested and questioned by French authorities in early November. They were allegedly about to launch a basic drone (costing around €100) in the vicinity of a power plant. However, following their arrest the mysterious flights have continued, with reports suggesting those arrested were either copycats or just pursuing their hobby in an unfortunate location. More likely than a malicious group being behind the flights is that they are the work of anti-nuclear pressure groups. There have been a number of incidents in recent years involving protesters breaking in to nuclear facilities to highlight inadequate security and nuclear “risks”. These include a break in by more than 60 people earlier this year at a facility in France, a break in at a Swiss plant in March, and three break ins to Swedish nuclear plants over two years up to 2012.  In 2012 a Greenpeace activist flew into the secure area surrounding a French reactor using a motorised paraglider.  A spokesperson for Greenpeace – which has denied any connection to the recent drone flights – stated at the time that “we wanted to illustrate an external danger, like a fall of an aeroplane” onto nuclear facilities. Perhaps the use of drones represents the work of another group of activists seeking to highlight the risk that readily available drone-technology could pose to nuclear facilities.   The drone flights, as with other breaches of security  involving terrorists, spies or protesters, can have important implications for the nuclear industry. While a radiation release caused by a malicious act at a nuclear facility is unlikely, incursions and other breaches of security can impact in other ways. Events leading to outage or plant shutdown can be hugely costly. The sabotage at the Belgian power plant, for example, along with the unrelated shutdown of two other reactors is costing the operator €40m a month.  More broadly, the reputation of operators, regulators and the whole industry is at stake when security weaknesses are highlighted. It is for this reason that – whoever is behind the mysterious flights – it is in the interests of the nuclear industry and the French authorities to get to the bottom of the issue as soon as possible."
"A series of detailed maps have laid bare the scale of possible forest fires, floods, droughts and deluges that Europe could face by the end of the century without urgent action to adapt to and confront global heating. An average one-metre rise in sea levels by the end of the century – without any flood prevention action – would mean 90% of the surface of Hull would be under water, according to the European Environment Agency.  English cities including Norwich, Margate, Southend-on-Sea, Runcorn and Blackpool could also experience flooding covering more than 40% of the urban area. Across the North Sea, Dutch cities including the Hague, Rotterdam and Leiden were predicted to face severe floods from an average one metre sea-level rise, which is forecast if emissions rise enough to cause an increase in global temperature of 4C–6C above pre-industrial levels. The model does not account for the Netherlands’ extensive flood-prevention measures, although many other countries have not taken such action. Meanwhile, large areas of Spain, Portugal and France would be grappling with desertification, with the worst-affected zones experiencing a two and half-fold increase in droughts under the worst-case scenario. Hotter summers increase the risk of forest fires, which hit record levels in Sweden in 2018. If emissions exceed 4C, France, southern Germany, the Balkans and the Arctic Circle could experience a greatly increased fire risk. However, the absolute fire danger would remain highest in southern European countries, which are already prone to blazes. Further north, winters are becoming wetter. Failure to limit global heating below 2C could mean a swath of central and eastern Europe, from Bratislava in the west to Yaroslavl in the east, will be in line for sharp increase in “heavy rain events” during autumn and winter by the end of the century. In some areas of central and eastern Europe there is predicted to be a 35% increase in heavy rain events, meaning torrential downpours would be more frequent. While the climate data has been published before, this is the first time the EU-agency has presented it using detailed maps on one site. Users can zoom in on small areas, for example, to discover that one-third of the London borough of Hammersmith and Fulham could be exposed to flooding by 2071. The Copenhagen-based agency hopes the maps will reach decision-makers in governments and EU institutions, who would not usually read a lengthy EEA report on the impact of the climate emergency. “It’s very urgent and we need to act now,” said Blaž Kurnik, an EEA expert in climate change impacts and adaptation. Even if countries succeed in restricting global temperature rise, existing CO2 in the atmosphere would still have an impact, he said. “The number of extreme events and sea level rise will still continue to increase for the next decades to a century,” Kurnik said. “Sea level rise, especially, can be problematic, because it is still increasing because of past emissions and the current concentration of greenhouse gases.” The agency wants governments to focus on adapting to unavoidable global heating. “Adaptation is crucial in the next decades of the century. Even if we are able to increase the temperature by 2C, adaptation is crucial for the next decades.” The EEA has concluded it is possible to limit the rise in global temperatures to 2C above pre-industrial levels, as long as greenhouse gas concentrations peak during the next 15 to 29 years. Meeting a more demanding 1.5C limit requires concentrations to peak in the next three to 13 years. Under both scenarios, there is a 50% chance of overshooting the temperature. • This article was amended on 10 February and 14 May 2020. An earlier version said that the EEA concluded “it is possible to keep global temperatures 2C below pre-industrial levels, as long as emissions peak during the next 15 to 29 years”. That meant to say greenhouse gas concentrations, not emissions; and the 2C referred to a rise in temperature above pre-industrial levels, not the temperature below pre-industrial levels. This article was further amended because an earlier version omitted “enough to cause an increase in global temperature of” from the sentence: “… an average one metre sea-level rise, which is forecast if emissions rise enough to cause an increase in global temperature of 4C–6C above pre-industrial levels”. This has been corrected."
"
My good friend Jim Goodridge, former state climatologist for California, came to visit yesterday to offer some help on my upcoming trip, as well as to talk shop a bit about the state of affairs on climate change.
He had previously authored a paper that I had hoped to present on his behalf at ICCC, but unfortunately it got excluded from the schedule by an omission. Yesterday he decided to rework that paper to bring out it’s strongest point.
One of the best and simplest ways of seeing the solar connection is to look at accumulated departure. Here is Jim’s essay on the subject:
Solar – Global Warming Connection
Jim Goodridge
State Climatologist (Retired)
jdgoodridge – (at) – sbcglobal dot net
March 22, 2008
Solar irradiance has been monitored from satellites for three sunspot cycles. The sunspot numbers and solar irradiance were shown to be highly correlated. Since sunspot numbers have been increasing since 1935 the irradiance must also be increasing.
The sun was once considered to be constant in its output, hence the term “Solar Constant”. Recent observations suggest that the sun is a variable star. Observations of solar irradiance have been made with great precision from orbiting satellites since about 1978. These observations are from Wikipeda: http://en.wikipedia.org/wiki/Solar_variation
They clearly indicate that the solar irradiance varies with the historic sunspot numbers:

Click for a larger graph

Click for a larger graph:
Using this relationship, 307 years of solar irradiance is easily inferred.
Sunspot numbers since 1700 were plotted as accumulated departure from average in order to compare them with weather variables. The sunspot number index indicates a declining trend for the 1700 to 1935 period and an increase from 1935 to 2008. The eleven-year cycle is clearly visible.

An increase in sunspot activity, and by inference, irradiance since 1935 is plainly indicated.
Moderators note: And I want to also call attention to these graphs, which shows the change in solar irradiance since 1611 and Geomagnetic activity over the last 150 years:


Clearly, solar geomagnetic activity has been on the rise. There will be more interesting posts on sunpots coming in the next week or two, stay tuned -Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0810d7e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe June SEARCH report of September sea ice outlook here shows the predictions of 16 research institutes for 2010:
According to the report:
 A quick calculation (leaving out the outliers of 3.2 million in 2009 and 1.0 million in 2010) shows an average prediction of 4.72 in June 2009, and 5.05 million this year.
On average their predictions for 2010 are 300,000 km² more than last year’s average prediction. This is probably due to the embarassment they had to endure last year, when ice melt was far less than all the institutes had predicted. Last year sea ice area bottomed out at 5.4 million km², see below:
As you can see from the above chart, they all fell short. This year they probably thought twice about making headline-grabbing claims like they did last year.
Share this...FacebookTwitter "
"If you live in a poorly insulated home, and many of us do, you could spend thousands this winter on energy bills. But our ancestors had many ways to keep snug at little or no cost. Now, thanks to modern infrared cameras and advances in environmental physics, we can understand how these methods work and measure how effective they are. The key to understanding how to keep warm is the fact you lose more heat by radiation to your surroundings than you do by convection to the air. This is why your house feels so cold when you get back from a winter break, even after you’ve turned on the central heating; though the air quickly warms up, the walls take far longer to do so and may continue to make you shiver for up to a day.  In the same way, in poorly insulated houses the inside of the external walls can be several degrees colder than the air and the internal walls, making you feel chilly.  Fortunately, there are five simple ways to overcome this and minimise your energy bills. During the day, your windows let in more radiant energy than gets out; sunlight can enter through the glass, but the window is opaque to the infrared radiation trying to escape. At night, however, single-glazed windows can get extremely cold – in my Victorian house which we try and keep at a room temperature of 20°C, an infrared camera showed internal window temperatures of as low as 7°C on a frosty night.  Even double-glazed windows aren’t great insulators and can fall to around 14°C. This results in energy losses of 50-100 watts per square metre, equivalent to running an old-fashioned light bulb.  The best way to prevent this heat loss is to close your curtains and lower your blinds immediately after dusk. They provide an extra barrier to radiant heat loss, add insulation and reduce draughts. My cheap blinds raise the internal surface temperature to 16°C and thick curtains raise it virtually to room temperature, minimising heat loss and making the room feel cosier. Solid brick or stone walls are better insulators than glass, but they still get cold and let out lots of heat. In my house the external walls fell to 16-17°C, 3-4°C cooler than the air in the room, even though they were made of 50cm thick sandstone.  Fortunately you can significantly reduce energy losses by covering them with picture or mirrors. Even a simple poster adds an extra layer of insulting air, raising internal surface temperatures by around 1°C and cutting lost energy by a quarter. Framed pictures or mirrors are better, if more expensive. Not being a Russian oligarch or a medieval baron I don’t have any carpets or tapestries to hang on my walls, but these would be even more effective.  Best of all are bookshelves. My partner is an avid collector and her old books make superb insulators. The spines of the volumes in our book-lined study are raised almost to room temperature, making it snug and warm. Thermally at least, printed books are far superior to their electronic counterparts. Doors can let in draughts, and being thin and sometimes glazed can be very poor insulators, falling to 10-15°C on cold nights. Covering your door and the surrounding wall with a thick lined door curtain can eliminate pretty much all the heat loss. Even if you can’t reduce all the heat loss from your outer walls you can still shield yourself from the cold. Our ancestors used to draw up wooden screens behind themselves and huddle up to the fire. Being at room temperature, the screens kept their backs warm, while radiation from the fire heated up their front. You could do the same, and you could even protect your face from the damaging effects of a roaring fire by using miniature fire screens, just like Georgian ladies. How warm you feel in a room depends on where you are, even though air temperature is the same throughout. You will feel warmer if you position yourself closer to the inside of the house because the cold external walls are further away. So try and place your furniture next to an internal wall.  If your desk is up against an external wall so you can look out of the window your legs will tend to get cold, though you can reduce this effect by leaning a cardboard sheet against the wall. If the head of your bed is next to a cold external wall you will be prone to getting a stiff neck, though you can counter this somewhat by using a solid headboard. The best solution, of course, is a four-poster bed, but most bedrooms just aren’t big enough. So knowing something about how heat moves can help you brave the cold winter. My experience has also shown that investigating the thermal properties of your house with an infrared camera will keep your kids amused for hours."
"
This thread debates the Miskolczi semi-transparent atmosphere model. 
The link with the easiest introduction to the subject is http://hps.elte.hu/zagoni/Proofs_of_the_Miskolczi_theory.htm


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ea1d30e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

What explains the enormous differences in income per capita that exist across the world today? The question has been posed many times over. The gaps in prosperity that surround us in the modern age are much wider than those that motivated Adam Smith to write _The Wealth of Nations_ in 1776, which of course is where the modern discipline of economics began.



Whereas back then the gap between the richest and poorest nations was 4- or 5- fold, today it is over 40‐​fold. Why is it, then, that certain nations are distinguished from others in terms of wealth and poverty, health and sickness, food and famine? Theories abound. If you turn to the popular media — or even some respectable journals such as Science and Nature — you will most likely come across articles that argue that geographic factors are what explain these differences. Climate, soil quality, disease, and the environment have all been put forth as the determining elements of prosperity. Yet, when you look at the evidence, these geographic factors don’t seem to be all that important. The same countries that are very rich today were once poorer than others with the same soil quality, for instance.



An even more popular explanation is the importance of cultural factors. You will hear, for example, that it is the difference between Catholics and Protestants (as Max Weber argued), or perhaps between Christians and Muslims and Judeo‐​Christians that leads to economic differences. Others have focused on Asian versus non‐​Asian values, or differing social attitudes toward work. The significance of cultural factors is a popular explanation for the differences that exist between North America and the Iberian cultures of Latin America, as well.



Popular among academics and journalists is the notion that “enlightened leadership” is what matters — meaning that either leaders or their advisers have the right ideas about what drives prosperity. It’s no surprise that this has some appeal to economists, who, of course, are in the business of developing the best micro‐ and macroeconomic policies — ones believed to be so critical to a nation’s ultimate success.



However, once again, these all seem to have relatively little explanatory power. Remember that it was only four decades ago that many scholars were talking about the deleterious effects of Confucian values — the same cultural traits that are now touted as the foundation upon which Chinese growth has been built. And while economic policies that condemn nations to poverty abound, it will soon become clear that those policies are not adopted by mistake. They are adopted by design. It is not in the ignorance of leaders, in other words, that we should look for the causes of poverty. It is in their incentives. Let me explain.



 **INSTITUTIONS: INCLUSIVE VERSUS EXTRACTIVE**  
Our theory rests upon the nature of institutions — meaning the rules, both formal and informal, that govern our economic and political life. It should not come as any surprise that there are certain sets of economic institutions — property rights, enforcement of contracts, and so on — that create incentives for investment and innovation. Those institutions that create a level playing field through which a nation can best deploy its talents are referred to as “inclusive economic institutions.”



Inclusive economic institutions, however, are the exception rather than the rule. That holds true throughout history as well as around the world today. Instead, many nations today and in the past operate under extractive institutions, which do not create property rights, generate law and order, create secure contract environments, or reward innovation. They certainly do not create a level playing field, and therefore they do not encourage sustained economic growth.



As I have already mentioned, however, these extractive institutions do not develop by mistake. They are designed by the politically powerful to extract resources from the mass of society for the benefit of the few. Such institutions are in turn sustained by extractive political institutions, which concentrate power and opportunity in the hands of an elite. This elite, in essence, designs, maintains, and benefits from these extractive institutions.



So the question is: Why do these extractive institutions emerge and persist? This is where politics enters into the equation. When extractive political institutions concentrate power in the hands of the few, those groups that monopolize political power can maintain these institutions in spite of the fact that they fail to create incentives for economic growth. Let me offer an example.



 **CASE STUDY: SOUTH AMERICA**  
There is no better laboratory that demonstrates how extractive institutions emerge and persist than the New World. The Americas provide a brilliant example for understanding how different institutions form, how they become supported within different political frameworks, and how that, in turn, leads to huge economic divergences.



The economic and political institutions in the New World have been largely shaped by their colonization experience starting at the beginning of the 16th century. While the tales of Francisco Pizarro and Hernán Cortés are quite familiar, I’d like to start with Juan Díaz de Solís — a Spaniard who in 1516 initiated the colonization of the southern cone of South America, in what is today Argentina and Uruguay. Under de Solís’s leadership, three ships and a crew of 70 men founded the city of Buenos Aires, meaning “good airs.” Argentina and Uruguay have very fertile lands, with a climate that would later become the basis of nearly a century of very high income per capita because of the productivity of these areas.



The colonization of these areas itself, however, was a total failure — and the reason was that the Spaniards arrived with a given model of colonization. This model was to find gold and silver and, perhaps most importantly, to capture and enslave the Indians so that they could work for them. Unfortunately, from the colonists’ point of view, the native populations of the area, known as the Charrúas and the Querandí, consisted of small bands of mobile huntergatherers.



Their sparse population density made it difficult for the Spaniards to capture them. They also did not have an established hierarchy, which made it difficult to coerce them into working. Instead, the Indians fought back — capturing de Solís and clubbing him to death before he could make it into the history books as one of the famous conquistadors. For those that remained, there were not enough Indians to act as workhorses, and one by one the Spaniards began to die as starvation set in.



The rest of the crew moved up the perimeter to what is now known as Asunción, Paraguay. There the conquistadors encountered another band of Indians, who on the surface looked similar to the Charrúas and the Querandí. The Guaraní, however, were a little different. They were more densely settled and already sedentary. They had also established a hierarchical society with an elite class of princes and princesses, while the rest of the population worked for the benefit of the elite.



The conquistadors immediately took over this hierarchy, setting themselves up as the elite. Some of them married the princesses. They put the Guaraní to work producing food, and ultimately the remainder of de Solís’s original crew led a successful colonization effort that survived for many centuries to come.



The institutions established among the Guaraní were the same types of institutions that were established throughout other parts of Latin America: forced labor institutions with land grants for the elite Spaniards. The Indians were forced to work for whatever wages the elites would pay them. They were under constant coercive pressure — forced not only to work but also to buy what the elites offered up for sale. It is no surprise that these economic institutions did not promote economic growth. Yet it’s also no surprise that the political institutions underpinning this system persisted — establishing and continuously recreating a ruling class of elites that did not encourage economic development in Latin America.



Yet, the question still remains: Could it have been geography, culture, or enlightened leadership — rather than institutional factors — that played a critical role in the distinct fates of the two teams of explorers?



 **CASE STUDY: NORTH AMERICA**  
Roughly a thousand miles north, at the beginning of the 17th century, the model of the Virginia Company — made up of the elite captains and aristocrats who were sent to North America — was actually remarkably similar to the model of the conquistadors. The Virginia Company also wanted gold. They also thought that they would be able to capture the Indians and put them to work. But unfortunately for them, the situation they encountered was also quite similar to what the conquistadors witnessed in Argentina and Uruguay.



The joint stock companies found a sparsely populated, very mobile band of Indians who were, once again, unwilling to work in order to provide food for the settlers. The settlers therefore went through a period of starvation. However, while the Spaniards had the option of moving up north, the captains of the Virginia Company did not have this option. No such civilization existed.



They therefore came up with a second strategy. Without the ability to enslave the Indians and put them to work, they decided to import their own lower strata of society, which they brought to the New World under a system of indentured servitude. To give you a sense of this, let me quote directly from the laws of the Jamestown colony, promulgated by the governor Sir Thomas Gates and his deputy Sir Thomas Dale:



No man or woman shall run away from the colony to the Indians upon pain of death. Anyone who robs a garden, public or private or a vineyard or who steals ears of corn shall be punished with death. No member of the colony will sell or give any commodity of this country to a captain, mariner, master, or sailor to transport out of the colony or for his own private use upon pain of death.



Two things become immediately apparent in reading these laws. First, contrary to the image that English colonies sometimes garner, the Jamestown colony that the Virginia Company was chartered to establish was not a happy, consensual place. Pretty much anything the settlers could do would be punished by death. Second, the company encountered real problems that were cause for concern — namely, that it was extraordinarily difficult to prevent the settlers they brought to form the lower strata of society from running away or engaging in outside trade. The Virginia Company therefore fought to enforce this system for a few more years, but in the end they decided that there was no practical way to inject this lower stratum into their society.



Finally, they devised a third strategy — a very radical one in which the only option left was to offer economic incentives to the settlers. This led to what is known as the headright system, which was established in Jamestown in 1618. In essence, each settler was given a legal grant of land, which they were then required to work in exchange for secure property rights to that plot. But there was still one problem. How could the settlers be sure that they had secure rights to that property, particularly in an environment in which a stolen ear of corn was punishable by death?



The very next year, in order to make these economic incentives credible, the General Assembly offered the settlers political rights as well. This, in effect, allowed them to advance above the lower strata of society, to a position in which they would be making their own decisions through more inclusive political institutions.



 **LESSONS**  
These historical examples illustrate several important lessons. The first is that there is clear positive feedback between inclusive economic and political institutions. Inclusive economic institutions are not only more conducive to economic growth than extractive ones. They are also supported by, and support, inclusive political institutions, which distribute political power widely, while still achieving some amount of political centralization so as to establish law and order, the foundations of secure property rights, and an inclusive market economy.



Second, this example illustrates that none of the alternative theories has much explanatory power. The large disparities in prosperity that exist around us today formed mostly in the 19th and early 20th centuries. But why did they form? The examples we have considered give us several insights.



It wasn’t geography that caused the divergence between South and North America. If anything, much of South America had higher agricultural productivity, supporting a greater population density, at the time of colonization. But South America ended up poorer than North America. This reversal cannot be accounted for by the impact of geographic factors. It wasn’t some sort of culture either. In fact, it’s remarkable how similar the objectives and chosen methods of the Spanish and English colonialists were. Even if their religion and culture were different, they were after the same thing and they had the same way of going about getting it. But the conditions on the ground meant that the Spanish could achieve their goals and the English could not. And the divergence wasn’t related to enlightened leadership. If anything, the Spanish leaders were more successful because they could achieve what they wanted. The Virginia Company, Sir Thomas Dale, and Sir Thomas Gates could not.



Instead, the root cause of the divergence between South and North America is in the different economic and political institutions that developed in these territories. Because the Spanish were successful in setting up extractive institutions to enrich themselves and their king, the long‐​run economic development of most of their empire was hampered. Because the English failed in setting up similar extractive institutions — and instead inclusive institutions started developing there — the United States would be much better placed to take advantage of new technologies and economic opportunities come the 19th century.



The history of the Americas is illustrative because it shows how the trajectory of institutions and economic development depends on whether elites bent on setting up extractive institutions succeed or fail. But the Americas are not fully representative of the rest of the world. In many other parts of the world, extractive institutions are not so much imposed from the outside, but are created by domestic elites. The crucial part of the story, therefore — which _Why Nations Fail_ tries to explain in detail — is the process of institutional change.



 **CONCLUSION**  
A key lesson of the framework we present in Why Nations Fail is the importance of politics. Of course, it is economic institutions that determine economic incentives and the resulting allocation of resources, investment, and innovation. But it is politics that shapes how economic institutions work and how they have evolved. Most societies suffering under extractive economic institutions do so because political power is concentrated in the hands of an elite ruling under extractive political institutions.



The recent events in the Middle East and North Africa also highlight the role of politics. The Arab Spring has shaken not only Tunisia, where it started, but Egypt, Libya, Yemen, Bahrain, and Syria, even if the governments in the latter two countries are still holding onto power. The roots of discontent in these countries are economic and social, but those are in turn shaped by political factors. The general population has been repressed and excluded from political power for generations. The protesters in Tahrir Square in Egypt understood this and this is why they demanded not just handouts or concessions from the existing regime, but fundamental political change.



This all implies a simple but critical conclusion: You can’t succeed economically if you don’t get your politics right. And that’s where the difficulty lies, because there is no formula for getting politics right. This is illustrated, for example, by the challenges lying ahead for the Middle East and North Africa — in particular, Egypt and Tunisia. Do we expect democracy or extremism to triumph in Egypt? Have the events in Tahrir Square changed the nature of politics irrevocably or will a similar economic and political structure reemerge under a different guise? Have they opened the way to a new authoritarian regime under the auspices of the Muslim Brotherhood? Central though these questions are for understanding the economic trajectory of the region, unequivocal answers are not possible. It’s only the details of politics and how the contingent path of history will play out that will determine how successful politically and thus economically these nations will be.
"
"We know the world is warming and that, unless things drastically change, we will keep emitting more carbon. We know the two are linked. But exactly how much warmer will it become as we emit more carbon? It’s one of the most important questions we all face. It lies at the heart of the Intergovernmental Panel on Climate Change’s 5th report on climate science, which appeared in 2013. The lead author of the report, Professor Tom Stocker, identified the most important finding as the analysis of the link between cumulative carbon emissions and global mean surface warming. This question of how much warmer it will get as we emit carbon is usually only understood using highly complex climate models, including many physical, chemical and biological processes. These climate models are like extensions of weather forecast models, but projecting over the next century, rather than the next week. The climate projections are calculated for various “emissions scenarios” – defined rates of carbon emissions for every year, ranging from a best case scenario where emissions are reduced to a worst case where nothing is done and emissions keep increasing. Analysis of the climate models found a deceptively simple result: climate warming depends almost linearly on how much carbon we emit, rather than the details of the particular emission scenario. This is because the most important factor behind global warming is the total amount of carbon emitted since the pre-industrial age, not the details of the rate of carbon emitted in every particular year.  You can see this response on the graph above for how temperature has increased  versus carbon emissions since 1870. The different coloured lines show different emission scenarios with the precise timing of how much carbon is emitted over the next 100 years. However the red, orange, light and dark blue lines for the different emissions all nearly lie on top of each other in the graph. Therefore, to know how warm it will be in, say, 2100 all you need to know is the total amount of carbon emitted up to then, rather than the details of the emission scenario. In order to understand why climate warming links so simply to how much carbon we emit, we have gone back to basics, drawing on simple climate theory to understand the climate response. Our research is published in the latest edition of the journal Nature Geoscience. We derived a relatively simple equation, using global heat and carbon budgets to connect climate warming to how much carbon has been emitted since the pre-industrial era. The equation includes crucially two competing factors: how the ocean takes up heat and how the climate system takes up carbon from the atmosphere. We found that the simple link between warming and carbon emissions emerges due to how the ocean takes up heat and carbon. There is a reduction in surface warming over time from the way the ocean soaks up carbon dioxide, which is almost cancelled by the increase in surface warming from the way the ocean soaks up heat. This link means that whatever warming we experience, it will stay around for many centuries after carbon emissions are stopped. We found from our simple theory that we will experience around 1°C of warming for every million-million tonnes of carbon emitted, with an uncertainty of around half a degree. This warming range from our simple theory agrees well with earlier findings from complex climate models, thus highlighting the crucial role of the ocean in connecting warming to emissions. But our results could be amplified by other climate forcing, such as release of other greenhouse gases, including methane from direct emissions, marine hydrates or permafrost."
"
Share this...FacebookTwitterLike in many countries in Europe, politcal parties in Germany, whether right or left, are big boosters of re-engineering society in order to save it from the fantasized self-inflicted climate catastrophe. People who speak up face risk feeling the wrath of the many climate-doctrine-following drones and zombies. And as the level of absurdity reaches intolerable levels, people are indeed speaking  up.
One such person is mayor Hans-Martin Moll of the town of Zell am Harmersbach in Germany. He has written a letter addressed to Tanya Gönner, Minister of Environment in the state of Baden Wuerttemberg and a member of the conservative CDU party.  The European Institute For Climate and Energy (EIKE) features Moll’s letter here in German.
Mr Moll, who is also a CDU member,  has become very concerned about the CDU’s aimless drift, led by Angela Merkel, in the direction of “green illusions” over the last years. Chancellor Merkel is advised by alarmists like Hans Joachim Schellnhuber and Stefan Rahmstorf.
What takes the cake for Mr Moll is Tanya Gönner’s declaration that Germany’s EEG Act is a complete success, and that it ought to be continued. The EEG Act forces power companies to buy renewable energy from anyone who produces it at fixed, exhorbitant prices that are guaranteed for years. (More info on the EEG Act here). Moll writes:
Producing power with coal or nuclear reactors costs between 2.5 and 4 cents per kwh. The EEG forces the consumers and the economy to pay 43 cents per kwh for photovoltaic power, or about 15 times more than the reliable, steady supply, conventional power.
And to make this hugely subsidised power of any use, billions of euros more are needed for expanding the power grid, for adding necessary over-capacity, and for “imaginary storage technologies”, which are physcially and geographically completely illusionary.
 
You call this a success story? I call it a political swindle of the citizens. Only in a communist centrally planned economy has such a thing ever been done.
Consumers and the economy had to fork out already 12 billion euros in 2009 for a completely useless and ideological nonsense. This EEG Act which you call a success story will cost hundreds of billions of euros.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Moll does not mince any words. Indeed the amount of CO2 that Germany may save by 2020 will be offset by Chinese economic growth within just a few months. It amounts to nothing. But it is a very expensive nothng.  CO2 reductions in Germany will have zero impact on the climate, assuming that added CO2 has a noticeable impact on climate. Moll writes:
With this kind of politics, the only thing that is sustainable is the harm done to the consumer, the economy and the jobs for our future generations.
This swindle must not only be reduced, it has to be eliminated completely. The same is true for wind energy.
Energy policy is going precisely in the Green parties’ direction. Their target is not the environment, rather it is the dismantling of industry.
Mr Moll concludes with:
I do hope the conservative CDU party will wake up soon, recognise this huge error, and that it will endeavour to pursue a real energy policy that is based on natural science and common sense.
I couldn’t agree more with Mr Moll. As people start speaking up, other people will start listening.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterUlli Kulke of the German online Die Welt national newspaper has written a piece: How Sceptics Are To Be Converted. He reports on the recent Global Media Forum held by German public broadcaster Deutsche Welle dubbed “The Heat Is On – Climate Change and the Media”, see here for background and here. According to Kulke the real objective of the forum:
The media are to warn the public of the dangers of climate change even more effectively and powerfully than before, and of course to make it even more clear that it’s the fault of man.
One well-attended workshop was: How To Deal With Climate Scepticism. Its own stated objective:
This workshop aims to point out what journalists must know about climate change policy, whom to trust and when to question their own professional procedures.
and warned:
Falling back on a “neutral” journalistic position can mean playing into the hands of the skeptics at the expense of the basis of life.
According to the workshop’s moderator, Bernhard Pötter of the newspaper Tageszeitung,
For journalists, climate change is the most important topic of the 21st century.
The “How To Deal With Climate Scepticism” workshop was designed to provide assistance to frustrated editors, authors and other journalists on how to best deal with the unwanted confrontation with a climate sceptic.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Oreskes’s Propaganda
One notable speaker at the workshop was Naomi Oreskes, who, according to Kulke, requests journalists eliminate the use of the word “scepticism” from their reporting. Kulke reports on Oreskes:
‘Scepticism” is too positive, and is indeed even a virtue in science. It’s better to use the word “contrarian’, which one can translate as ‘adversary’ or ‘dissenter’, says Oreskes. “Also it’s a no-no to use the term climate debate’.
‘It’s no wonder,’ complained Oreskes, ‘that people think science is still debating climate change when everywhere in newspapers one reads about a ‘debate’. Debate has long been in the history books. Climate change is a scientifically proven fact.’ It’s important for journalists to stress that the debate is over.
Ulli Kulke wonders what newspapers Oreskes could be possibly reading out in California, which would lead her to conclude the press is playing down climate change. Kulke writes:
In the years leading up to and after the last IPCC assessment report in 2007, the press and television reported daily on the coming end of the world in America and Europe.
But this has changed over the last half-year. Inconsistencies, cover-ups, big blunders and, most of all, exaggerations by climate scientists have been exposed. Some have admitted their errors. Even plots by scientists against their sceptic colleagues came to light. As a result the media have toned down their alarmism a little. And one even gets the impression that, since Climategate, journalistic principles have made a comeback. But some people have got a problem with that.
Like Oreskes.
Much to her chagrin, parts of the German press, such as Ulli Kulke, are not ready to abandon the principles of journalism. That’s good news.
Expect scepticism contrarianism to grow in Europe.
Share this...FacebookTwitter "
"We live in a time when our climate is warming more rapidly than ever before.  Rising temperature and associated changes in weather are driving shifts in the distributions of species on Earth.  Some are thriving in these new climate conditions and have even moved into new regions that were historically inhospitable. One concern for us humans is how harmful species – diseases or pests – are responding to a changing climate. An example is the malaria parasite, which is transmitted to humans by tropical mosquitos.  In the western highlands of Colombia, malaria is slowly creeping upwards in elevation as the climate warms. But these diseases don’t just directly harm humans. Some species of wildlife are being infected by diseases that are thriving in a warmer climate? Should we be concerned? In the spring of 2008 I first started noticing that large ochre sea stars (or starfish) known as Pisaster were literally turning to ooze on the western coast of Vancouver Island – both on beaches and in experimental aquariums.   It was a gruesome sight. Symptoms of wasting started with lesions or bulging of the tissues, and in some cases, the arms even dropped off.  I tried anti-fungal and bacteria agents, and even tea tree oil, yet the only way that I found to control the disease was to move infected animals to cooler temperatures. What struck me at the time was how rapidly wasting spread among my animals, with death the ultimate outcome, all in the span of few days. In particular, I found that outbreaks occurred when animals were exposed to temperatures that were warmer than they were used to.  In a scientific study where I reported these results with my collaborators, I suggested that this “sea star wasting disease” could be triggered by changing climate conditions and even lead to large-scale outbreaks. Starting in early summer of 2013 starfish starting dying in massive numbers. Dozens of sea star species were affected and millions of individuals were wiped out along the entire Pacific coast of North America – an exceptional event that captured international media attention and puzzled scientists.   A new study published in the Proceedings of the National Academy of Science has exposed the culprit. The authors present compelling evidence that a virus (Parvoviridae) is responsible for the wasting disease – they were able to infect healthy sea stars with the virus, which then leads to wasting symptoms.  They also found that the virus is common in the sea star’s environment, and is even associated with urchins, distant cousins of sea stars.  But perhaps more intriguing, the authors identify the virus in museum specimens dating back to 1942.   This paints the picture of a virus that can spread outside its host, that is not particularly picky about which host it infects, and that has been living with sea stars for over seven decades without causing the large-scale mass mortality of the past two years.   While implicating a virus as the likely cause of wasting disease is certainly a commendable achievement, we still don’t know why such a large number of sea stars over such a wide geographic area have succumbed to this disease. Has the virus changed to be more virulent, or deadly? Perhaps something in the environment has shifted, increasing the prevalence of this disease agent or its spread within wild populations. Is this disease outbreak somehow related to climate change? While we can’t answer these questions now, the scope and scale of the wasting outbreak in sea stars is unprecedented and suggests a need to pay attention to wildlife diseases – especially because many species exposed to warmer temperatures than they are adapted to are weakened and become more vulnerable to disease.   It is important to take heed of the speed and aggressiveness with which such a disease can spread and dramatically alter ecosystems as well as the potential to impact humans more directly.  Marine diseases can infect species that we depend upon for food or other resources, or even cause illness in humans such as has been observed with the Vibrio virus, which is spreading north through Europe.   Increasing outbreaks of wasting disease in sea stars will probably lead to dramatic changes in our shorelines.  Sea stars feed on algae and other marine invertebrates, helping shape the ecosystem in which they live by what they eat.   This role for sea stars was made famous by Pisaster, the original keystone species, so named because it feeds on mussels along the shoreline, preventing the mussels from taking over and creating habitats for many different species.  Populations of these sea stars have vanished in the past year and we are left to wonder about the ecological outcomes. Knowing what agent is responsible for this disease may lead to strategies to mitigate its further spread, and hopefully, to reverse the decline of starfish in the northeast Pacific. As well, scientists and managers may be able to learn how best to both detect and respond to future marine diseases."
"The circular economy is typically seen as the progressive alternative to our wasteful linear economy, where raw materials are used to make the products that feed today’s rampant consumerist hunger, which are then thrown away. The idea of the circular economy only took off in the 1980s, but this doesn’t mean that the practices at the core of a circular economy, such as repairing, recycling, refurbishing, or repurposing, are equally novel. All of these strategies have the aim of keeping materials in use – whether as objects or as their raw components – for as long as possible. And all are hardly revolutionary. The repurposing of objects and materials may be as old as tool use itself. In Palaeolithic times, smaller flint tools were made from old hand-axes. People in the Neolithic period had no problem reusing standing stones to construct their tombs, such as seen in Locmariaquer in France. Even ceramics, made from clay and therefore available in abundance, were frequently recycled. Old pottery was often ground down to powder and used in the clay for new pots. On Minoan Crete, this ceramic powder, known as grog, was also used to manufacture the mudbricks from which houses were built.   At the Bronze Age site in Hungary where I excavate, spindle whorls made from broken pot fragments turn up regularly. Large stones at this site pose an interpretative dilemma because of their continuous reuse and repurposing, from grindstone to anvil and doorstep to wall support. In fact, up until the 20th century, repair, reuse, and repurposing were common ways of dealing with material culture. The dominance of the wasteful linear economy is a real historical anomaly in terms of resource use. But we should be careful not to fall into the trap of the “noble savage”. Our ancestors were no ecological saints. They polluted their surroundings through mining, burned down entire forests, and they too created massive amounts of waste. Just look at Monte Testaccio, a large artificial hill in Rome made up entirely out of broken amphorae.  When things are in abundance, people easily accept a wasteful and exploitative attitude. But for most of the past, most things were not in abundance, and so a core practice of a circular economy was adopted. This did not happen due to ideological motivation, but out of necessity.  Archaeologists typically don’t use the terminology of the circular economy, and describe the above examples simply, as reuse. This might partly explain why the deep roots of core practices of the circular economy are not discussed more widely. The same is also true of recycling.   When one adopts a very broad definition of recycling (thinking of it, for example, as the use of previously discarded artefacts), the origins of this practice can be traced all the way back to the Palaeolithic period. But let’s focus here on the understanding of recycling as is employed today. This is a practice in which waste (used objects) is completely converted, becoming the raw material of new products. This practice of complete transformation also entered the repertoire of human behaviour far earlier than you may think. It became the core practice of an economy as long ago as the Bronze Age. From about 2500BC, prehistoric people started to combine copper and tin on a regular basis, making metal known as bronze. The mass adoption of this artificial material caused significant shifts. Societies reoriented themselves economically because making bronze meant moving materials over long distances. Connecting sources with end users led to an intensification of trade. For these reasons, the Bronze Age is considered to be a formative epoch in the formation of Europe, in which we witness the emergence of pan-European exchange networks and large-scale trade. Bronze also made people think in new ways. The process of metalworking differs markedly from other, earlier, crafts. Wood and stone carving involve the removal of material, which is why they are known as reductive technologies. Basketry, weaving, and pottery, meanwhile, are additive technologies. Bronze is different in that it is a transformative technology. The raw material is melted down to a liquid state and poured into a mould. Moulds were the very first blueprints, documenting the design of an object to be produced – and reproduced. This may not sound very exciting to us now but for the prehistoric people involved this must have a been a groundbreaking way of working materials.  Just imagine, if your stone axe broke, you could repurpose the pieces, but you would not be able to remake that axe. In contrast, if your bronze axe broke, you could remelt it and produce the same axe with the same quality, again. Recycling, as a core economic practice, was invented in the Bronze Age.   Bronze was not the first metal to be used in such a way; the origins of metal use start with pure copper being hammered into shape. But it is only at the beginning of the Bronze Age that recycling starts to take place on a large scale.  From the Middle Bronze Age onwards, all over Europe, bronze was being recycled. We know this because archaeologists have analysed the metal composition of hundreds of objects, showing the depletion of certain elements, as a result of frequent recycling.  In addition, “old” metal was traded. A shipwreck discovered off the coast of Dover carried a large amount of French bronze objects dated to 1100BC, destined to be recycled in the UK.  As a political term, we might want to keep the circular economy in the present, but the practices that are part of it have long been part of human existence. In this respect, the Bronze Age could be seen as the first example of a circular economy in practice. Bronze was a main material of this period, and its economy revolved around recycling. Recognise this, and we start seeing that it is not the circular economy that is novel. Rather, it is the linear, and wasteful economy that is the anomaly. The beauty of this is that we can put the past to good use. The core values of a circular economy are rooted in our past and in this manner, they can help shape and inspire a modern craftsmanship that fundamentally should revolve around sustainability and durability."
"Few things have loomed larger in Brexit imaginary than the stupendous trade deals the UK will get as soon as it frees itself from the thicket of European Union regulation. The government hopes to hash out deals with both the EU and the United States by the end of the year, when the transition period ends and Britain is no longer bound to Europe’s rules. It’s an ambitious goal, but not impossible. President Trump is “bullish” on a UK deal, the American ambassador Woody Johnson recently said, adding that Trump, like the prime minister, wants to “get it done”. They make it sound so easy. And it’s true that Donald Trump has so far favoured quick and dirty deals with individual countries over complex, multinational agreements. But trade negotiations will also mean facing the world’s largest economy at the bargaining table alone, with little leverage and a ticking clock. The US will push aggressively against anything that blocks American companies from doing business here, and there are few things on which the two countries are more divided than climate policy.  The UK has ambitious plans to decarbonise by 2050, while the US has little policy and no targets to speak of. So the government will struggle to come to an agreement without sacrificing its already fragile climate commitments. Trade deals are increasingly about the alignment (a very EU-ey word) of standards and regulations to allow foreign goods and services into our borders. And it won’t surprise anyone to learn that alignment with the US rarely leads to stronger regulation; instead, trade agreements often act as a powerful and undemocratic tool to erode or supersede existing protections. The common example is the fear of poorly checked, chlorine-washed American chicken being sold in the UK. But the gulf between the US and UK on climate is even larger than on food standards. Documents from a 2018 meeting, leaked in November, set out a harsh starting position: the US will not discuss the climate crisis or include the term in any deal, which means it won’t consider existing climate policies legitimate grounds for opposition to any of its demands. This suggests the American negotiators will push to weaken existing low-carbon standards, or worse – given the UK has many climate targets but little actual concrete policy as yet – lock in clauses favourable to the fossil fuel industry that can’t be easily overturned by domestic legislation. This would effectively head off climate policy before it can be written. A similar thing played out before negotiations when the UK was part of the EU: in the now stalled Transatlantic Trade and Investment Partnership (TTIP), the US pushed for wording that would prevent initiatives to help renewables compete with fossil fuels. That demand is likely to be repeated, with the American fossil fuel industry keen to get fracked gas as well as oil from tar sands into the UK. The government’s recent fracking ban could also come under pressure from US companies looking to operate here. In practically all the sectors identified by the committee on climate change as requiring decarbonisation policies – agriculture, power generation, transport – there will be a push to lock in market mechanisms, enshrine “competition”, and head off attempts at regulation. Perhaps most worryingly, the US has also indicated it will push for any agreement to include an investor-state dispute settlement – a controversial mechanism that gives foreign companies access to a supranational tribunal where they can sue entire countries within the bounds of the trade agreement but outside their own legal system. These have been used by companies to recoup “lost profits” when countries try to introduce environmental legislation: for instance, the Canadian province of Quebec was sued by an oil company under the North American Free Trade Agreement after it banned fracking in 2011. With an ISDS, a bad deal for climate would not only harm existing standards, it could also prevent climate policies being made in the future, either because they contradict the agreement or because they could open the government to trade disputes. Climate policy wouldn’t cover a product quite as viscerally unappealing as chlorinated chicken, or an institution as beloved as the NHS, both of which have focused public anger on the usually arcane process of trade negotiation. The risk with climate is that a trade deal would involve death by a thousand cuts for regulation, with changes across the entire economy – none big enough to contest on their own. It doesn’t have to work this way: the EU is currently proposing a “carbon border tax” that would force trading partners to comprehensively consider the climate impact of all their products and services, an example of how trade policy can be used to strengthen global climate efforts. But Liz Truss, the minister in charge of negotiations, and Dominic Raab, the foreign secretary, are deregulators of the highest order. Both were co-authors of the 2012 book Britannia Unchained, which criticised the UK for a “bloated state, high taxes and excessive regulation”. They maintain that the government is committed to its existing climate targets, but it’s unlikely they’ll actually back the protections we need in order to meet them. • Stephen Buranyi is a writer specialising in science and the environment"
"
Share this...FacebookTwitterThe online German Der Spiegel reported yesterday here that the EU Commission wants to accelerate cuts in CO2 emissions, but industry and government officials are saying “no!”. With economies gripped by hardship and overall growing public scepticism (see here), calls for even more draconian measures to curb CO2 emissions are ringing hollow.
German Minister of Economics Rainer Bruederle says it’s time to take a break from efforts to protect the climate:
It accomplishes nothing for environmental protection when Europe goes it alone and jobs are sent to other regions of the world.
Werner Schnappauf, Director of the German Association of Industry adds:
As long as there is no international and legally binding climate protection treaty,  industry rejects increasing the climate reduction target from 20% to 30%. There are only disadvantages for both the climate and economy if Europe rushes and goes it alone.
      The EU wants to ratchet up the target from 20% to 30% less CO2 emissions than 1990 by 2020. Other leading German government officials think they can both appease the climate gods by making human sacrifices at the Altar of Climate, and at the same time boost the economy. German Minister of the Environment Norbert Roettgen and other EU environment ministers have said they want to go ahead and require the 30% target, with or without an international treaty.
It’s good for the environment and also for the incentive to innovate, from which the German industry would greatly profit.
      According to the EU Commission, a CO2 reduction of 30% by 2020 would lead to a 0.54% drop in GDP. Can’t these people think of ways to make our lives easier for a change?
Share this...FacebookTwitter "
"
Share this...FacebookTwitterI must admit I’m in a bit of shock reading this stuff, especially full police report about Al Gore’s alleged behaviour. I’m not going to draw any conclusion right now. It’s all too stunning, if it’s true.
The other climate blogs have not written a word about it so far. Maybe they want to be extra careful, which is understandable. But then again, Drudge plasters the story as its big headline for the day. Matt Drudge has been in business for years, and surely he’s done his homework. Maybe it’s to encourage other victims to speak out, if there are any.
I read the entire police report and it is shocking – really. Will other women come forward? Is it all a hoax? We’re not talking about Mike Tyson or Kobe Bryant here. We’re talking about the former VP of the USA and the prophet of AGW.
Incredible.
UPDATE; Germany’s top tabloid Bild reports here (German).
Share this...FacebookTwitter "
"

A couple of comments have mentioned the global “turn off your lights” night. Lubos Motl at the Reference Frame has a suggestion
Earth Hour: turn your lights on at 8 p.m.
Tonight, at 8 p.m. local time, you should turn on all the light bulbs you have for 60 minutes (it will only cost you 3 cents per light bulb in average for the whole hour) to fight global obscurantism. You should look how many lights are on around. Every light bulb you see will be a sign of the audacity of hope, as Jeremiah Wright would say.”15 years ago, I would have done this. Now, I plan to turn all my lights on as my silent form of protest against the likes of Gore and his Enron like carbon credit scheme. I’m going to “Watts Up” my house!
If you want to learn about the event, here is the web page:
http://www2.earthhourus.org/
Of course if you are simply interested in saving money and using less electricity (something I’m for, especially here in California since the state has hamstrung itself for future power generation) then get one of these:

I have several. They work great. And, buying one via this link sends some help back to me for keeping my www.surfacestations.org effort running.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea021377a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Since this blog has main focused on air temperature measurement, and has not done any discussion of manual measurement techniques of Sea Surface Temperature measurements, I thought it would be good to first review some of the instrumentation used.
Sea Surface Temperature Measurement Instruments:
Standard Thermometer

Measures: Temperature in degrees, typically used in the bucket thermometer
Operates: At any depth by cable or line or by hand
Notes: Mercury in original thermometer has been replaced in many standard
thermometers by less toxic materials

Bucket Thermometer

Measures: Water temperature near the surface
Operates: At the surface by hand or line
Notes: Typically lowered about 1 meter into the water, left there for one
minute, and then retrieved deck side for reading.

Reversing Thermometer (for Nansen Bottle)

Measures: Water temperature at a specific predetermined depth
Operates: Only when turned 180 degrees (the mercury breaks in the special loop
and will not get back together until reset) Temperature at depth can be recorded
with a 180 degree flip (as is done with the Nansen Bottle) and there will be no
change on the way up.
Notes: A Nansen bottle
is a device for obtaining samples of seawater at a specific depth. It was
designed in 1910 by the early explorer and oceanographer Fridtjof Nansen.
The bottle, a metal or plastic cylinder, is lowered on a cable into the sea, and
when it has reached the required depth, a brass weight called a “messenger” is
dropped down the cable. When the weight reaches the bottle, the impact tips the
bottle upside down and trips a spring-loaded valve at the end, trapping the
water sample inside. The bottle and sample are then retrieved by hauling in the
cable.

Bathythermograph (BT)

Measures: Water temperature over a range of depth
Operates: Over any depth with a cable or line by hand or with a hydraulic winch
Notes: This model records the information inside and is retrieved however there
are expendable models (XBTs) that free fall on a copper line and transmit the
temperature and depth information through the copper wire before dropping to the
bottom


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ef4fb93',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The Supreme Court’s decision in _King v. Burwell_ upheld President Obama’s massive power grab, allowing him to tax, borrow, and spend $700 billion without congressional approval. This establishes a precedent that could let any president modify, amend, or suspend any enacted law at his or her whim.



As it stands, Obamacare will continue to disrupt coverage for sick Americans until Congress repeals it and replaces it with reforms that make health care better, more affordable, and more secure. Despite the ruling, Obamacare remains unpopular with the American public and the battle to set in place a health care system that works for all Americans is far from over. At a Capitol Hill Briefing in July, Michael Cannon, director of health policy studies at Cato, and Ilya Shapiro, a senior fellow in constitutional studies at the Institute, came together to discuss the impact of _King v. Burwell_ on health care reform, the separation of powers, and the rule of law.



 **MICHAEL CANNON** : The ink wasn’t yet dry on the ruling in _King v. Burwell_ before supporters declared that the debate over repealing Obamacare is over. Well, that debate has been declared over so many times at this point that I’ve lost count. But I believe that Obamacare supporters dodged a bullet with this ruling. The way the Affordable Care Act (ACA) was written, approved by both chambers of Congress, and signed into law by the president, it gave states the power to block major provisions of the law. States can block the subsidies that are supposed to flow through the health insurance exchanges. They can block the employer mandate and, to a large extent, the individual mandate. The law as written gave states these powers.



Obamacare supporters dodged a bullet because 34 or 38 states — depending on how you count — did not establish health insurance exchanges, which they needed to do in order for those provisions to take effect. They effectively exercised the vetoes that Congress gave them over portions of the ACA. And eliminating those subsidies would have revealed the full cost of this coverage to enrollees in Health‐ Care​.gov. That is what the Obama administration and its supporters fear. (I do not consider them ACA supporters, by the way, because they do not really support that law as written.) Nevertheless, the repeal debate is not over and there are a lot of reasons why. First, more than six years after the first draft of Obamacare was introduced in the House, it remains unpopular. In fact, it is as unpopular now as it was when it was enacted more than five years ago. And this is a year and a half into implementation, a year and a half after people have been receiving the benefits under this law, as rewritten by the Obama administration and the Supreme Court. And a lot of the costs of the law have not even taken effect yet: the Cadillac tax, some premium hikes on the horizon, the fact that some of the temporary programs designed to mitigate adverse selection will expire. This law has been unpopular for six solid years, and you know what? Unpopular laws, in a democracy, are always up for repeal. Second, the Obamacare repeal debate is going to keep going on because Obamacare hurts the sick. Yes, it does insure more people by throwing lots and lots of money at health insurance. But it also threw millions out of their health care plans in 2013 — including cancer patients and others with severe illnesses — leaving them with inferior coverage. It threatened to throw millions more out of their plans again until the Supreme Court amended the law in this latest ruling. And it is going to continue to threaten coverage for cancer patients and others as long as that law remains on the books.



It is not just opponents of the law that are noticing this. If you look at the January 29 issue of the _New England Journal of Medicine_ , the lead article is about how Obamacare is pushing insurers into a race to the bottom by jettisoning coverage for HIV patients. There are other studies that have found the same thing happening with other high‐​cost chronic conditions, like mental illness, diabetes, rheumatoid arthritis, and cancer. This has happened in other markets with Obamacarelike bans on discrimination on the basis of preexisting conditions. There are a number of examples where community rating rules have encouraged insurers to avoid, mistreat, and ultimately dump the sick. And it does not matter that it is only happening with a few insurers now. The _New England Journal of Medicine_ article said that a quarter of the insurance plans that they studied displayed these characteristics. But eventually all insurers will have to follow suit. Supporters of the law will likely blame this on greedy insurance companies. But the truth is that it is Obamacare that forces these companies into this race to the bottom, even when insurers are not trying to discriminate against the sick.



Another reason the repeal debate will continue is because Obamacare, and the way it has been implemented, demeans voters. And they feel it. They sense that it demeans them. The way that the Obamacare’s architects designed, sold, enacted, and implemented this law has been an ongoing string of insults to the intelligence, the compassion, the dignity, and the sense of fair play of Americans who oppose this policy. Obamacare’s architects have lied to the public. They have called voters stupid. They have called opponents evil. President Obama and the Supreme Court have rewritten the ACA now in so many ways that it disempowered and disenfranchised Obamacare opponents.



Finally, this debate is going to continue because there are ways to provide more secure health coverage to the sick. No one wants the pre‐​Obamacare world where government had already been making health insurance less secure. But in spite of the tax exclusion for employer‐​sponsored insurance and all the other things that the government has done to cripple private health insurance markets, they were still innovating to develop products that made health insurance more secure.



One example is guaranteed renewability. Another example is preexisting condition insurance, an innovation that was happening right underneath Congress’s nose as they were debating the Affordable Care Act. It was first introduced in late 2008, and UnitedHealthcare was getting regulatory approvals in early 2009. What was this product? It was basically an insurance product where, at a cost of just 20 percent of what you would pay for health insurance, you can buy the option to purchase that health insurance plan at any time, no matter how sick you get. Even if you develop a preexisting condition, you pay the same rate as everyone else. This was available in 2008. There were further innovations that markets were likely to develop that encouraged insurers to compete to cover the sick rather than to avoid them.



But Obamacare destroyed these innovations. When you think about it, if Obamacare were such an improvement over the status quo ante as it existed before the law, then why do 65 percent of Health​Care​.gov enrollees want the freedom to purchase their pre‐​Obamacare plans? Obviously something is amiss there. Unfortunately, these innovations are not coming back until we get rid of Obamacare, specifically its community rating price controls, and that’s why the debate is going to keep happening.



Now, what should Congress do in the wake of _King v. Burwell_? It should stay focused on what it has always been focused on with regard to Obamacare — which is repealing it. We are not going to get lower costs and more secure health insurance for the sick until this law is off the books. Repeal is actually more important after _King v. Burwell_. With that ruling, the president and the Supreme Court just created entitlements and imposed taxes on 70 million Americans — employers and individuals — that no Congress ever approved and from which Congress specifically exempted those 70 million employers and individuals. The fact that tens of millions of Americans are currently subject to taxes that Congress never approved makes it all the more important that Congress repeal Obamacare. _King v. Burwell_ shows that what we are living under right now is an illegitimate law.



I like to say that the Affordable Care Act is imperfect. But, gosh, it is a lot better than what we’ve got.



 **ILYA SHAPIRO** : Look, I’m a simple constitutional lawyer. Essentially everything I know about health care I’ve learned through litigating two cases, _NFIB v. Sebelius_ three years ago and now _King v. Burwell_. Originally, when we were planning this forum, we thought the case could go either way. It was really a 50/50 toss‐​up, and I thought I could add some value by explaining the nuanced rules of decision: for instance, what does this mean for health care or other types of regulatory policy going forward? Given the Court’s opinion, however, all I can say is that there’s really very little law, as it were, in the majority opinion. It’s as if the whole opinion just said “Affirmed.” The reasons don’t matter because they don’t make any sense whatsoever. Words have whatever meaning the writer wants them to have — which is a far cry from the following judicial opinion: “It is not our job to protect the people from the consequences of their political choices.” Now who wrote that? Is that from Justice Antonin Scalia’s dissent here in _King v. Burwell_? No. Is it from Justice Anthony Kennedy’s (combined with Scalia’s) dissent in _NFIB v. Sebelius_? No. It was Chief Justice John Roberts in the majority opinion in _NFIB_.



What’s going on here is an unholy confluence of liberal judicial activism and conservative judicial passivism that has found a perfect home with John Roberts. It’s not that the Court is liberal. Nor is John Roberts “evolving,” as so many justices have in the past. What’s going on here is that, well, Obamacare is special. As Scalia pointed out in his dissent, all the normal rules of constitutional — and now statutory — interpretation go out the window when it comes to the Affordable Care Act. Mind you, it wasn’t a matter of enforcing the text of the Affordable Care Act. It was rewriting the text in a different way to do what John Roberts thought in his infinite wisdom would cause the least disruption in public policy or the health care system. And it shows why we don’t want judges making these kinds of extra‐​legal determinations, because again this is not a liberal decision. In _NFIB_ , the individualmandate case, Justice Ruth Bader Ginsburg’s partial concurrence/​partial dissent said that there are no constitutional limits on federal power. That’s the liberal position.



Similarly here, the liberal position would have been to say that the IRS gets to do whatever it wants, applying what is known as _Chevron_ deference. _Chevron_ is a legal doctrine named after a case from more than 30 years ago, which says that when a law is ambiguous, courts are to defer to an agency’s interpretation of that law, unless that agency is being arbitrary and capricious. So even if courts might disagree with the agency’s determination, as long as it’s not completely crazy they’ll defer to it. In other words, A, B, C, D, and E are all somewhat plausible interpretations. One might be better; one might be worse. But as long as the agency doesn’t go for X, Y, or Z — which are all completely out of left field — they’ll be okay. Roberts specifically said in _King v. Burwell_ said that that was not what he was doing. This was not an administrative‐ law, agency‐​deference case. And that was backed up in June in the _Michigan v. EPA_ case, where Justice Clarence Thomas wrote a concurring opinion on the need to narrow _Chevron_. Justice Scalia’s majority opinion there said that the agency’s determination was indeed unreasonable and therefore sent it back to the drawing board.



But that doesn’t mean, I don’t think, that in some future instance _Chevron_ is going to be narrowed. I don’t know if the Court has the five votes necessary to do that. Some people are saying the fact that _Chevron_ wasn’t expanded is the silver lining in _King v. Burwell_. It sort of is. But again, this is kind of a sui generis opinion, good for the Affordable Care Act only. I’m sure some lower‐​court judges will use it to buttress some future fanciful statutory interpretations — to say that A is equivalent to Not‐​A, as it is here — that “exchange established by the state” means “exchange not established by the state.” Possibly. But really the way that it’s written, John Roberts’s goal in _King_ , as it was in _NFIB_ , was to achieve a certain result without really changing legal doctrine, without expanding federal power.



Let me extrapolate from that. Looking forward, there’s a lesson we can draw to avoid that unholy alliance of liberal activism — rewriting the law — with conservative passivisim — restraining and bending over backwards to let Congress do whatever it wants. The way we avoid that is to learn the lessons of history.



In the late 1930s and early 1940s, the Supreme Court started going off the rails and eviscerating the doctrine of limited and enumerated powers. It began eviscerating federalism and bifurcating (even trifurcating) our rights, such that some rights are more equal than others. Then, from the 1950s to the 1970s, when the Warren and Burger Courts continued these “evolving” notions of what the Constitution means, the conservative response to that sort of activism was not: “You’re wrong. Here’s the correct theory of constitutional or statutory interpretation.” Instead and alas, the response was: “Why are you not deferring to the political branches? You are unelected judges. You should be restrained. You should be deferring. You should be sitting on your hands.” And that’s why we have what we have now.



The answer is to appoint judges who are actually committed to judging. We should be fighting for judges who have a proven track record of saying what the Constitution actually is — of engaging with the law — rather than trying to bend over backwards and defer to agencies or to Congress. The judicial branch is a branch of the federal government for a reason. It’s there to check and balance the others.



John Roberts’s background was too smooth in many ways. He checked all the right boxes and excelled at the legal craft, but he never identified as an originalist or movement conservative. It’s clear that he’s a Republican. But it’s never been clear that he was committed to any particular theory of judicial interpretation. So congratulations to him, but future Republican administrations will have to be more careful about what kind of conservative they want on the Supreme Court: someone who focuses on restraint (a judicial mode) or someone who has a particular substantive theory of jurisprudence.



Those of you, especially here on Capitol Hill, who are going to the barricades to fight for a proper judiciary, make sure you’re fighting for the right people so that it’s worth the effort. Someone who has displayed loyalty to generic conservatism or some kind of “Red Team, Blue Team” fight is not enough. We need judges who actually are willing to make the difficult “balls and strikes” calls — as John Roberts said at his confirmation hearings — rather than kick the plate a little bit, squint, and call it a strike because Congress “intended” it to be a strike. This is not going to change overnight. It’s not a matter of winning the White House. It’s a matter of picking the right judges and understanding the climate of ideas such that the proper judicial philosophy isn’t being conservative or minimalist or incrementalist or restrained. It’s about judging in a particular way and applying the standard tools of statutory and constitutional interpretation regardless of where the political chips may fall.
"
"It is often said that humans have caused the Earth to warm at an unprecedented rate. However researchers have discovered another period, some 55m years ago, when massive volcanic eruptions pumped so much carbon into the atmosphere that the planet warmed at what geologists would think of as breakneck speed.  The good news is that most plants and animals survived the warm spell. The planet has experienced several mass extinctions – and this wasn’t one of them. But there’s a catch: even after carbon levels returned to their previous levels, the climate took 200,000 years to return to normal. Geologists have a name for this earlier period of sudden warming: the Palaeocene-Eocene Thermal Maximum. The PETM, as we’ll call it, occurred 55.5–55.3 million years ago. According to new research published in the journal Nature Geoscience it involved global warming of between 5 and 8°C over a period of 200,000 years. The massive carbon injections responsible for the PETM probably originated in volcanic eruptions in the North Atlantic and the burning of organic-rich rocks through which lava passed, which further triggered the melting of frozen methane stored at the bottom of the deepest oceans. There are obvious analogies between the PETM and the present-day situation, even despite the lack of fossil-fuel burning humans 55m years ago. The study shows the PETM was caused by annual carbon emissions of at least 900 million tons (900 MT) over the 200,000 years. That is ten times less than the 9500 MT carbon humans are releasing into the atmosphere every year. Surely cause for concern?  However, it is a little misleading to suggest emissions of just 10% of current levels resulted in warming of 5°C or more. It is possible to zoom out too far, even when assessing climate change. Considering that CO2 only sticks around in the atmosphere for 1,000 years at most, to achieve as much as 8°C warming the bulk of PETM carbon must have been delivered to the atmosphere in a very short time, during which the long-term average was greatly exceeded.  The researchers identify two such “pulses”, each lasting no more than 1,500 years. Volcanic eruptions are not predictable – and certainly not consistent – but they would fit the profile of these “pulses”. It is these massive, rapid injections of carbon into the atmosphere that overwhelmed Earth’s otherwise extremely efficient oceanic carbon sink: it takes time to deal with such blows.  The study suggests that it took 200,000 years before Earth returned to normal (probably hindered by volcanic eruptions) – a duration that suggests Earth will not recover from its current stresses any time soon. An intriguing aspect of the PETM warming is that there was no mass extinction. Perhaps ecosystems were resilient having evolved in the aftermath of the great extinction of the dinosaurs some 10m years earlier. Perhaps the warming didn’t last long enough. Perhaps warming just made things “nice”.  But global warming hasn’t usually been particularly good news for the planet’s inhabitants. When massive volcanic eruptions in present-day Siberia generated a comparable increase in global temperature 250m years ago, it caused the greatest crisis in Earth’s history. Around 95% of the planet’s species were wiped out in what is known as the Permian-Triassic extinction, or the Great Dying. Earth was definitely not “nice” and remained inhospitably hot for millions of years. In comparison, the PETM looks like a tea party. Is humankind off the hook? After all, we are only emitting a bit of carbon each year, nothing like the massive doses administered by enormous eruptions in geological history. No. The excellent high-resolution palaeoclimate records now emerging indicate that global warming has precedent in the rock record, and it always takes Earth a long time to recover.  The impact of 5-8°C global warming today is hard to define. It is beyond the worst-case scenarios of most climate models – and yet it is not beyond the bounds of possibility. Let’s do some rough calculations – and simplify things by talking about CO2 alone: today, Earth’s atmosphere contains at least 3000 gigatons of CO2. Humans inject a further 30000 megatons every year (1% of the mass in the atmosphere). Volcanoes add only a fraction of the human contribution.  By contrast, the total CO2 release from the Siberian Traps and the rocks burned by its lava is estimated at 30,000 to 100,000 gigatons. That’s ten to thirty times the total amount of carbon currently in the atmosphere. At current rates it will take humans just 1000 - 3000 years to produce this amount (it took the Siberian Traps much longer) and by the year 3014 Earth could be facing another catastrophe. Does that sound a long time away? For a geologist, these are incredibly short timescales. Our saving grace appears to be that we are coming out of an ice-age with an atmosphere relatively low in CO2, and that we are going to run out of fossil fuels before the above scenario can happen. However, geologists have shown that massive injections of carbon to the atmosphere can change the climate quite rapidly, and we are already well along the road. We are now seeing major changes to our weather and time will tell if this is the manifestation of longer term climate change.  Undoubtedly Earth’s climate will change as we continue to emit. Our generation might not see the impact of that change, but we need to decide what we want for the future of Earth. It is time to learn from Earth’s past – episodes such as the PETM, and the Permian-Triassic – as we look to its future."
"
Share this...FacebookTwitterDr Oleg Pokrovsky has kindly taken the time to provide further information on his recent remarks, which have been widely quoted. We thank him for doing so. He writes as follows and includes a link to a ppt. presentation (see below):
Dear Colleagues,
Thank you for discussion of my conclusions presented at recent IPY conference
occurred in AARI (St.Petersburg, Russia).
My vision of future climate is based on comprehensive analysis of climate index series analysis, which permits to reveal fundamental quasi-periodical oscillations in most components of climate system:
-Solar activity
-SST of ocean (AMO and PDO)
-Surface air temperature
– Surface irradiance
-Precipitations
-Ice extent in Russian Arctic Seas
I found that that those are in strong coherence when inter-annual climate noise was removed in each of them
My motivation might be illustrated by a set of figures presented at recent Arctic Frontiers Conference (Tromso, Norway)
http://www.arctic-frontiers.com/index.php?option=com_docman&task=doc_download&gid=242&Itemid=155
Please keep your comments focused on the contents Dr Pokrovsky’s presentation.
Share this...FacebookTwitter "
"

 _Thanks to a last‐​minute “patch,” 23 million Americans were saved from paying an average of $2,000 in additional taxes under the Alternative Minimum Tax in 2007. But the debate over AMT, which is poised to strike again in 2008, continues. On December 6, 2007, on the eve of the AMT patch, Rep. Paul Ryan (R-WI) spoke at a Cato Capitol Hill Briefing on his proposal to repeal AMT and overhaul the current income tax code with a simplified, two‐​rate plan. He was joined by Cato senior fellow Daniel J. Mitchell and Chris Edwards, director of tax policy._



 **REP. PAUL RYAN:** This is about more than just the Alternative Minimum Tax or what kind of tax policy we ought to have. The AMT debate we are in right now is the beginning of an enormous fight we are going to have in this country. We are talking about whether we sanction an everhigher trajectory of federal spending. Fundamentally, we are talking about how big our government is going to get.



The AMT is a federal income tax that is imposed on top of the existing income tax system. In 1969, AMT was passed to go after 155 rich people who were using deductions and loopholes to avoid paying any taxes. And while subsequent tax reform closed those loopholes, the AMT remained. Most critically, the AMT was never tied to inflation, so that today the AMT is targeting an ever‐​increasing fraction of the middle class.



About 20 million Americans were subject to AMT in 2006; 23 million in 2007. Their estimated increased tax liability was about $2,000 per person. According to the Congressional Budget Office, by 2010, if nothing is changed, one in five taxpayers will have AMT liability. Nearly every married taxpayer with income between $100,000 and $500,000 will owe the alternative tax.



So the AMT represents an enormous tax hike on the middle class. Going forward, it will represent an even larger tax increase. That is a major reason it must be repealed. But more centrally, the AMT would massively expand government revenue, which would in turn allow increased government outlays, increased government involvement in the economy, and increased government control over our lives. Meanwhile, many of the proposals to reform AMT come with additional tax hikes that would also mean continued government growth.



Federal revenues as a share of GDP have been about 18.5 percent historically. How much money has the federal government taken out of the U.S. economy, U.S. income, U.S. productivity? About 18.5 percent on average for the past 40 years. The AMT puts a new tax system on top of the current one, bringing us to a historically unprecedented level of taxation in the not so distant future. Of course, most people in Washington think that that’s fine.



That’s why the debate until recently has not just been about getting rid of AMT. It has been about how to replace the supposed “lost revenue.” Congressional Democrats don’t like the AMT because it targets mainly the middle class. Although they want to repeal it, they want to replace it with another revenue machine. For instance, Rep. Charles Rangel (D-NY), House Ways and Means Committee chairman, introduced a major piece of tax legislation in October that, while repealing AMT, would “offset” it through a host of new taxes on high‐​income households and on the private equity industry.



If you want to see what the future of taxation will look like under a Democratic president and Democratic Congress, look no further than Charlie Rangel’s tax bill. It is what he believes in. It is his philosophy. It puts the top federal marginal tax rate in this country at 44.2 percent. That’s the rate small businesses will pay. Meanwhile it raises the rate paid by private equity, venture capitalists, and hedge fund managers from 15 percent to 35 percent.



Now that is what you have to do to the tax code to replace the revenue from an AMT repeal. But as a conservative, I believe we shouldn’t replace that revenue. Let’s agree to keep government where it is. A lot of us could make a good argument for cutting taxes to below where they are now. But let’s at least agree to keep government at about 18.5 percent of GDP, after which we can focus on cutting spending, in particular on entitlement programs.



Because if we buy into this notion that we should have an ever‐​higher revenue baseline, we will take more freedom away from individuals, raise taxes, and make ourselves much less internationally competitive. And it will also lull us into a false sense of having a balanced budget or even a small surplus.



Along with Rep. Jeb Hensarling (R-TX), Rep. Michele Bachmann (R-MN), and Rep. John Campbell (R-CA), I’ve introduced the Taxpayer Choice Act, a bill that would not only eliminate the AMT and the massive tax hike that would come from its automatic expansion. It would also establish a highly simplified alternative to the current income tax system that individuals could choose. Under the current tax system, you fill out an income tax form and an AMT form, and you are obligated to go by whichever is the higher figure. By contrast, our bill gives people the choice of whether they want to pay taxes under the regular income tax or a much simpler and transparent tax system.



The plan woud raise approximately the same amount of revenue that we raise today under the current tax code. It also spreads the income tax burden basically the same as it does today. For those who are concerned about distributional tables, at the recent historical average of 18.5 percent, this is what we call distributionally neutral and revenue neutral.



Now, I want you to think about all the tax expenditure lobbyists who come to get members of Congress to promise not to touch their pet preference in the tax code. From a political perspective, it’s going to be hard to get members of Congress to vote against particular deductions or exemptions given the influence these lobbyists have. It will be much easier to get members of Congress to vote for a clean bill, one that puts that decision in the hands of individual taxpayers.



What would the effect of my plan be on those taxpayers? If they already have their affairs arranged to deal with the exemptions and deductions in the current code, they may opt to continue filing under the current system. But if they prefer a simplified tax form, one with two rates of 10 and 25 percent and little more than that to worry about, then they can opt for that. At the heart of this is a pro‐​growth, profamily profamily, pro‐​entrepreneurial tax system. We’re putting a stake in the ground and saying we don’t want government to grow beyond its current size. We do not accept this Washington doctrine — this Washington dogma — that we have to keep growing government at this ever higher rate.



If my three kids, who are three, four, and five years old, want to have this government for them when they are my age, they will have to pay twice the level of taxation that we have today. Take today’s government, add no new programs to it, take none away, and look ahead 40 years to when my three children will be approximately my age. At that point, they will have to pay 40 percent of GDP in taxes to the federal government just to keep it afloat. This is basically due to entitlement spending.



You can’t have a free and prosperous America with levels of taxation like that. You can’t have an internationally competitive country that can compete with China and India with levels of taxation like that. Yet that is the path we are on right now. And the left is trying to make it worse by proposing new entitlements on top of the ones we have already today.



Let’s recognize the path we are on right now and let’s put out an alternative that is bold but doable to prevent that from happening, so that we can preserve the American legacy: leaving your kids and the next generation with a country and a standard of living that is better than what you have now. That is what this is all about. That is what we hope to achieve.



 **CHRIS EDWARDS:** There is no doubt that tax reform has been stuck in a rut for a while. This year, Congress has been more focused on raising taxes than doing anything about tax reform. A flat tax hasn’t been championed in over a decade when Steve Forbes and Dick Armey did so.



One alternative to our current system is the national sales tax. One version of this, the FairTax has lately been endorsed by Arkansas governor Mike Huckabee to much press and praise. A national sales tax would in principle replace all current federal income with a single national retail sales tax, levied once at the point of purchase of new goods and services. The income tax, the payroll tax, the Medicare tax, capital gains tax, estate taxes, and even the AMT would go in favor of this national sales tax. But in my judgment it’s too dangerous in today’s political climate to even think about moving ahead with the idea of a national sales tax. If a sales tax started moving through Congress, there is no doubt in my mind it would end up being an add‐​on tax to the income tax system, which would be a disaster.



Rep. Charles Rangel (D-NY) has his own problematic proposal to reform the tax code. On the plus side, his bill would abolish AMT. It would also cut corporate tax rates, an area where the U.S. woefully lags behind the rest of the world. But it would replace this “lost revenue” with new tax hikes. In effect, this would amount to a trillion dollar tax hike, because the everexpanding AMT represents a new, additional tax on top of the current system. Congress should consider the pro‐​growth elements of Rangel’s package such as the corporate rate cut, without imposing new taxes on individuals and businesses.



Paul Ryan’s plan is by far the best of the bunch. It is a very credible, very pro‐​growth proposal, a way of moving ahead with tax reform, and a big step toward a Dick Armey or Steve Forbes flat tax.



Let me just give you a couple of things that I think are interesting about the Taxpayer Choice Act. I’m all for a flat tax. A flat tax would be optimal in terms of efficiency and fairness, in my view. But unfortunately, the current static revenue estimation methods up here on Capitol Hill provide easy fodder for opponents of a flat tax, who claim the flat tax is unfair.



So to move ahead with tax reform, I think a good idea is to enact essentially a flat tax but with two rates. The Taxpayer Choice Act has two tax rates, one at 10 and one at 25 percent. Those aren’t picked out of the air. If you look at people at the very top of the income distribution, they pay an effective rate of about 25 percent. That is to say, their total taxes divided by income comes to about 25 percent.



If you look at the broad middle class, people making from about $50,000 to $100,000, they have an effective tax rate currently of about 10 percent. This plan hits the same sort of distribution, in a static sense, as the current tax code.



Some folks looking at the details might criticize dropping the top rate from 35 to 25 percent. They might claim that it is a giveaway to the rich. But, again, the effective rate of those at the top of the distribution is 25 percent currently.



What’s interesting about the current tax codes is that the 25 percent tax rate starts at a very low income level. If you’re single and you earn an adjusted gross income of $40,000, you start getting hit by the high 25 percent tax rate. Under Ryan’s plan, that 25 percent tax rate doesn’t start until about $66,000. So there is a big chunk of people in the middle who would have a sharp marginal tax rate cut under the plan.



I think that the Taxpayer Choice Act is an excellent plan. Admittedly, one of the reasons why I think so is that I introduced something similar a few years ago in a February 2005 Cato Tax & Budget Bulletin, “A Proposal for a ‘Dual‐​Rate Income Tax.’ ” One thing that I included in my plan was a sharp corporate tax rate cut as well. If I were to add one thing to Ryan’s plan it would be to lower the corporate rate 35 percent down to 25 percent — at the least.



There has been a lot of discussion this year about corporate tax rate cuts. As mentioned before, even Rangel’s proposal includes one. Bear in mind that in Europe right now the average corporate tax rate is just 24 percent. At 35 percent, the United States has the second highest corporate tax rate in the world. And yet despite this, we have fairly low corporate revenues. Indeed, according to my analysis, we are in the Laffer curve range for the corporate tax rate, where cutting the rate down to 25 percent would mean no revenue loss for government at all.



A corporate tax cut is long overdue. We should add a corporate rate cut to the Paul Ryan tax plan, after which we would have a real winner for businesses and, frankly, for the government, which would probably get more revenue.



 **DANIEL J. MITCHELL:** What is good tax policy? Rates should be low. You shouldn’t double tax. There should be no special loopholes. It’s that simple.



Why have a low rate? Because that’s the price on productive behavior. Politicians understand this, whether they admit it or not. For instance, they institute higher cigarette taxes to diminish smoking. While I may not think that is government’s job, they get an A+ for economics. The higher the tax on something, the less you get of it. But I get frustrated by the fact that they don’t apply this same lesson to work, saving, investment, and entrepreneurship.



Meanwhile, lots of empirical data shows that once you get tax rates at 20 percent or below, people aren’t really going to worry about evasion and avoidance; they are going to focus on being productive. That’s another reason to keep rates low.



Now, why should income only be taxed one time? Because even if you have low tax rates, if you cycle income through the tax code more than once your effective tax rate can be very high.



Every economic theory agrees that capital formation via saving and investment is the key to long‐​run growth. Even radical socialists who believe government should do the saving and investing agree on this point. But in America there are four different layers of taxes that a single dollar of income may be hit with: the capital gains tax, the corporate income tax, personal income tax, and the estate tax.



So even if you get all those rates down to 20 percent, by the time the IRS gets four different bites at the apple, your effective tax rate can be very, very high. The government should not punish the very thing that everyone agrees is critical to long‐​run growth. Why should the tax code be neutral? Because the government should not be in the business of picking winners and losers. Issues of fairness aside, this leads to the misallocation of resources.



If you do everything right, you wind up with a postcard‐​size tax form. And even if you do a few compromises with it, like Congressman Ryan does, you can just have a bigger postcard. But if you go to the IRS Web site and you go to “Forms and Publications,” there are more than 1,100 different forms and publications you can download. Wouldn’t a postcard‐​size form be better?



Now, let me bring it back to some of the things that are relevant to policy work on Capitol Hill. Some people make the interesting argument that the AMT is like a flat tax. After all, it doesn’t have many of the exemptions and deductions of our current tax code. Meanwhile, it taxes income at, alternately, 26 or 28 percent depending on income, which is pretty close to a single tax rate.



But a flat tax isn’t just about having one rate. It’s also getting rid of double taxation. And the only thing similar between the tax base of an AMT and the tax base of a flat tax is you get rid of state and local tax deductions. That’s actually privately one of the reasons I’m amused by the AMT. You have all these high‐​tax states, like California and New York, complaining about it.



Now, what about Mr. Ryan’s plan? It’s not a flat tax either. It too has two rates. But marginal tax rates are going down. Productive behavior is not being excessively penalized. Government will be prevented from growing as it would under an allencompassing AMT. It represents progress.
"
"A volcanic eruption in Iceland caused massive disruption throughout Europe in 2010. A huge ash cloud grounded more than 100,000 flights and delayed 10m passengers, costing the aviation industry more than £2 billion. This wasn’t a freak event. New evidence shows such ash clouds are more common than we thought, and they can even cross the Atlantic from volcanic hot-spots in North America. We need to be wary as another major ash cloud could arrive at any time. In fact, the ash has barely settled from Alaska’s latest major eruption. Given volcanoes erupt all the time it seems odd that the Iceland incident came as such a shock. Perhaps there is a failure to appreciate that volcanic eruptions often occur in cycles with busy periods followed by intervals of relative quiet during which time these events pass out of social memory. Looking back through history one can see that 2010 was by no means unique. The Icelandic volcanoes Katla and Hekla, for example, produced large ash plumes in 1947 and 1918, but both were modest by comparison with the massive Asjka eruption of 1875 which blanketed much of Scandinavia in ash. We should remember that intercontinental plane travel has only existed for around 50 years, with budget airlines allowing mass air travel only within the past few decades. Flying has changed from being the reserve of the wealthy to a regular travel expectation for the majority.  The industry was lucky to evolve in what was a relatively quiet period between major ash producing eruptions in Iceland. A few years ago we were involved in a project to reconstruct past environmental changes along North America’s east coast. We found a number of ash layers throughout the sediments covering the past several thousand years.  By analysing the elements in the ash’s glass particles we are able to obtain a chemical “fingerprint” unique to that ash layer. These “fingerprints” can then be compared with samples from elsewhere. When an ash layer is identified, it provides a means of joining and aligning the environmental histories of different areas where it occurs. They are very precise time markers in the sediment because they are deposited over a very short period of time (days to weeks). The majority of the dozen or so ash layers we found during this study were from well-known eruptions in North American volcanic regions such as the Aleutian Islands off Alaska or the Cascade Mountains near Portland. One layer however stood out. It presented us with a puzzle: we had found a chemical match between an ash layer from Alaska and a layer which occurs throughout Europe, which was always presumed to come from Iceland. Using the ages of the eruptions was no help as they both occurred at approximately the same time. In North America, we know this as the White River Ash, which erupted from Bona-Churchill massif in Alaska. The European layer is called the AD860B (named after the approximate date of the layer).   We suspected both derived from the same eruption. But this would imply that ash would be capable of travelling from Alaska, over North America, and out across the Atlantic to Europe – a  total distance of 7,000km. One might expect this of past mega-eruptions such as Toba on Sumatra which blasted ash as far as Lake Malawi in eastern Africa around 75,000 years ago.  However, the White River Ash was by no means a mega-sized event. Although it was large – approximately ten times larger than the 1990 eruption of Pinatubo – it was also half the size of the 1815 eruption of Tambora. In the long run we could expect an eruption the size of White River somewhere in the world every 100-200 years.  We collected samples of both the White River Ash and AD860B from both sides of the Atlantic and re-examined them in detail: there were no appreciable differences between the Alaskan and European ash deposits. As an added bonus the ash has also been found deep in the Greenland ice. This allowed us to count the annual ice layers as one would for tree rings to obtain a new age for the eruption of around AD 847.  It is unlikely that we stumbled upon the only time North American ash that made it to Europe, and we fully expect more such layers will be found to correspond with the many large eruptions that have occurred in North America. If it happened at least once before, we need to be aware of the risk that it will happen again. The White River Ash/AD860B layer covered a third of the globe’s circumference at approximately 60°N. This coincides with a number of trans-Atlantic flight paths and would pose an obvious hazard when any of North America’s plentiful volcanoes have a White River Ash-type eruption. Findings such as ours should provide additionally useful data for the airline industry when calculating the risk likelihood associated with future volcanic eruptions and how to improve resilience against them."
"
Share this...FacebookTwitterA few days ago I wrote a post about how Reuters wasted no time blaming the outbreak of a fungus disease in the U.S. Northwest and British Columbia on climate change: http://pgosselin.wordpress.com/2010/04/23/were-to-blame/.
Of course this was just another example of wreckless media speculation and wishful thinking by those who simply can’t wait for the coming manmade climate catastrophe. They really are pretty desparate. The claim has already been completely debunked at World Climate Report: http://www.worldclimatereport.com/.
Share this...FacebookTwitter "
"It was late October when Adrian Sparks caught sight of the first smoke rising from the hilly horizon. Within days the haze evolved into drift smoke, which grew thicker as the mountain behind the Mount Pleasant winery in the Hunter Valley caught fire. “It was full on,” Adrian says. “There was smoke all through November and December. A clear day would still be hazy. At its worst, some days our eyes would sting. We’d be coughing. You’d have to stay inside with the doors shut and the air conditioning going. It was like an apocalypse..”  Though the winery suffered no fire damage, the blanket of smoke that was its legacy has caused nightmares for it and the broader Hunter Valley wine industry, thanks to what is known as “smoke taint”. Within the Hunter at least, the taint is forcing growers to confront the possibility that an entire year’s harvest will be dumped, with some vineyards choosing not to produce a 2020 vintage at all. The phenomenon occurs when smoke binds to the skin of grapes, ruining the taste of wine made from the fruit. For an industry where perception equals success, the reputational damage caused by selling a vintage affected by smoke taint can be lethal.  Sparks had seen the effects of smoke on wine grapes before. Years earlier he encountered the problem while working as a winemaker in the Yarra Valley, around the time of the Black Saturday bushfires. “It wasn’t as bad in 2009,” he says. “This is first time ever I’ve seen a company pull the pin on a vintage. I’ve been with the company for 20-odd years and I’ve never pulled the pin on an entire vintage.” On 14 January the winery that ordinarily produces 30,000 cases of wine in a year decided it wouldn’t take the risk and scrapped its 2020 vintage entirely. Mount Pleasant wasn’t alone. While vineyards further away from the fires escaped the worst, among the first to speak publicly about the issue was Bruce Tyrrell. Tyrrell’s Wines – the family have operated in the Hunter Valley since 1858 – ordinarily harvests 1,200 tonnes of grapes but this year lost 80% of its crop. “We didn’t have any immediate fire, we just had the smoke hanging around,” Tyrrell says. “We made the decision early, we weren’t going to take the risk with the brand. If a sommelier at a restaurant in New York opens a bottle of ours in 2030 and that wine has smoke taint, I’ve lost a whole lot of work. “We’ve worked too long, too hard to build the reputation to get where we are to let it go in five minutes. We’ve been here for 160 years and I’d like to see the family here in another 160 years.” Brokenwood Wines, Meerea Park Wines and Davis Wine Group have all made the similar difficult decisions about their harvests with some being left unpicked. Christina Tulloch, the chief executive of Tulloch Wines and president of the Hunter Valley Wine and Tourism Association, says the full economic cost is yet to be known. The community is already hurting after the bushfires cost it $42m in lost tourism. “That’s the economic loss based purely on visitation – people visiting cellar doors,” she says. “It is still too early to put a figure on the loss in production as we would normally be in the middle of vintage. We are hearing reports of between 50 to 90% of crop loss due to smoke taint. “Overall, we’re saying the loss will be more likely to be around 80 to 90% in reduction of tonnage that is brought into wineries in the 2020 year.” A similar story is playing out elsewhere. When bushfires tore through the Adelaide Hills before Christmas, a third of the wine-producing region was hit hard, as were grape growers on Kangaroo Island off the coast. Those who weren’t directly affected by the fires themselves watched the smoke linger over their fruit. Anita Poddar, of Wine Australia, still says she is hopeful the worst may be avoided. Out of 64 wine-producing regions which make up the $6.25bn industry, just 1% have been affected by the fires. With authorities still assessing the direct and indirect damage, there is a chance some regions may escape unharmed. “At this stage it is still too early to tell what the exact situation is,” Poddar says. “We started doing research on smoke taint in 2003. What happens is when there’s fresh, heavy smoke, it lands on the outside of the grape, and specific compounds get into the skin, not the flesh. It’s a one-season thing – next season the vines are fine.” With most winemakers now focused on the short-term work of getting through the year, few are asking whether this might represent a new normal. Alisdair Tulloch, of Keith Tulloch Wine, says the threat posed by bushfires and smoke taint are indicative of a larger problem. “You need to pull the camera back further than the bushfires themselves and the way they are influenced by climate change to look at the broader picture of grape growing in general,” he says. “Grape growing has been showing the fingerprints of climate change since the 1980s when the harvesting dates began to move forward. “Last year was both the hottest and driest year on record, according to the Bureau of Meteorology, which confirms that this drought is anything but normal.” Like chocolate and coffee crops elsewhere, wine grapes the world over require specific microclimates, while growers need predictable weather to make production decisions. Thanks to the climate emergency this is changing right across the globe. Within Australia, seasons are arriving earlier and weather has grown unpredictable. Rain falls in large quantities or not at all, while drier conditions are making it harder to grow certain varieties. These effects were predicted in a landmark study on the impact of climate change on the Australian wine industry published in 2011. A team at Australia’s scientific research agency CSIRO forecast temperatures would rise between 0.3C and 1.7C by 2030. Events have since borne out the predictions, with shorter winters and earlier harvests as fruit ripens earlier. All of which has underscored the need to act, according to Alisdair Tulloch, who says problems affecting viticulture are true for all agricultural sectors. “Our small family vineyard has been carbon neutral since 2017,” Tulloch says. “We’ve been certified as carbon neutral by the Australian government since March 2019. “Meanwhile, other industries are free to pollute and pump as much of these greenhouse gases into the atmosphere, while agriculture and the wine industry is picking up the bill. “If we can do it, why can’t they?” "
"
Guest post by John Goetz

I keep an active watch of the news for progress being made in the areas of renewable and alternative energy sources. One area that has caught my eye is algal fuel (biofuel produced by algae). One company that has been in the news lately is Sapphire Energy, which claims to be able to produce ASTM compliant 91-octane biogasoline. Sapphire Energy says their technology “requires only sunlight, CO2 and non-potable water – and can be produced at massive scale on non-arable land”.
I am not trying to pick on any one solution or Sapphire Energy in particular. I simply wondered how massive a scale of CO2 and non-arable land is needed to make a noticeable dent in our gasoline demand.
First, how much CO2 do we need? The IPCC guidelines for calculating emissions require that an oxidation factor of 0.99 be applied to gasoline’s carbon content to account for a small portion of the fuel that is not oxidized into CO2. To calculate the CO2 emissions from a gallon of fuel, the carbon emissions are multiplied by the ratio of the molecular weight of CO2 to the molecular weight of carbon, or 44/12. Thus, the IPCC says the CO2 emissions from a gallon of gasoline = 2,421 grams x 0.99 x (44/12) = 8,788 grams = 8.8 kg/gallon = 19.4 pounds/gallon.
Now let’s assume Sapphire Energy simply reverses the process and consumes the CO2 to produce gasoline. In other words, we take 19.4 pounds of CO2 out of the atmosphere for every gallon of gasoline we produce. This seems like is a nice “carbon neutral” process.
What is the cubic volume of atmosphere required to make 1 gallon of gas? Let’s assume for the moment an efficiency factor of 100%, meaning our process will consume 100% of the atmospheric CO2 it is fed. This is unrealistic, but it is unrealistic on the “optimistic” side. According to the EPA, one cubic meter of CO2 gas weighs 0.2294 lbs. At an atmospheric concentration level of 385ppm, one cubic meter of atmosphere contains 0.000088319 lbs of CO2. Thus, 19.4 / .000088319 = 219658 cubic meters (yes, I am ignoring the atmospheric density gradient as one moves from the ground upward, but hang with me). This equates to roughly 4553 gallons of gasoline per cubic kilometer of air.
According to the US Energy Information Administration, US gasoline consumption is currently averaging (4-week rolling) 9.027 million barrels of gasoline per day, or about 379 million gallons (42 gallons per barrel). Thus, to completely replace US gasoline consumption, Sapphire Energy would need to “scrub”, at 100% efficiency, just over 83000 cubic kilometers of air per day. Certainly there is plenty of air available – this volume represents less than 0.02% of the volume of air in the first 1 km of atmosphere. Nevertheless, it is an enormous  amount to process each day.
Of course, Sapphire Energy’s near-term goals are much more modest. As CEO Jason Pyle told Biomass Magazine, “the company is currently deploying a three-year pilot process with the goal of opening a 153 MMgy (10,000 barrel per day) production facility by 2011 at a site yet to be determined.” Using my fuzzy math above, that equates to a minimum of 92 cubic kilometers of air a day. Still seems like a lot.
So where will all of the CO2 come from?
Presumably the answer is coal-fired power plants. But let’s see if that makes sense. According to Science Daily, the top twelve CO2-emitting power plants in the US have total emissions of 236.8 million tons annually, or 1.3 billion pounds per day. Now, if that can be converted completely to gasoline, it would amount to 67 million gallons per day, or roughly 1/6 of the daily gasoline consumption.
(Science Daily refers to the twelve as the “dirty dozen,” which I found somewhat humorous given that CO2 is colorless and odorless, and is presumably needed to sustain some forms of life. But then again, so is dirt.)
Sounds great, except that a lot of land is needed to grow all that algae. According to Wikipedia, between 5,000 and 20,000 gallons of biodiesel can be produced per acre from algae per year. Assume for the moment that biogasoline can be produced at the same rate per acre. If we attempted to produce 67 million gallons of gasoline from our “dirty-dozen” every day, we would need between 1.2M and 4.9M acres of land to do this on. The low-end of the scale puts the area needed at more than that of Rhode Island. The high-end adds in Connecticut.
I kind of doubt there is that much land around each of the dirty dozen facilities. This means the gas would have to be sent by pipeline to a giant algae field. Given our ability to pipe oil and natural gas all over the place, sending CO2 across the country via pipeline is probably doable. There may also be plenty of unused or abandoned land (think abandoned oil fields) available to produce the gasoline. Nevertheless, the production scale and transportation logistics required to make this a viable alternative do indeed look massive.
So while the technology holds promise at the micro-scale, it remains to be seen what can actually be done at a scale that matters.
Talk among yourselves.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e6e362c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterWe recall last year how projections of new lows in Arctic sea ice extents were boldly made, and eagerly publicised by the catastrophe-obsessed  media. It made for good headlines, but they were all wrong. See here June Outlook Report.

This year Germany’s Alfred Wegener Institute AWI here and the KlimaCampus of the University of  Hamburg are taking another shot at it, along with a dozen or so other fortune tellers. The AWI press release states:
The projection of the KlimaCampus of the University of Hamburg is 4.7 million sq km, which is more negative than the 5.2 million sq km made by the AWI scientists. Yet both research centres do not exclude a record low of 4.3 million set in 2007 being reached.
Decisive factors like ice thickness and how the rest of the summer develops are unknown and do not allow for accurate projections. Yet the AWI is projecting a sea level that is almost a million sq km over 2007. They know that the ice in the central Arctic is thick, but probably don’t want to say it publicly. Indeed, most of the sea ice fortune tellers are projecting 2010 to finish higher than 2008.
Both teams used different methods for their projections. Prof. Rüdiger Gerdes and his team at the AWI worked out a model together with the scientific companies OASys and FastOpt that uses oceanic drift buoys and satellite data for measuring the movement of ice. The projection will be revised each month during the summer. Dr. Gerdes says:
Currently we calculate with 80% probability that the sea ice extent will be between 4.7 million and 5.7 million sq km in September. The projections will become more precise as summer progresses.
Meanwhile the KlimaCampus-Team of Prof. Lars Kaleschke takes satellite photos of the Arctic sea ice for each day of 2010 and compares them to the same day of each year from 2003 to 2009.
The number and size of the ice-free areas are indicators for subsequent ice developments. These dark spots store more solar energy in early summer and thus enhance ice melt during the polar summer, as the sun does not disappear until September.
Share this...FacebookTwitter "
"Like many young people, Joe Brindle, 17, is scared for the future because of the climate crisis. He is, he says, “angry about the injustice that is allowing the most vulnerable people in the world to suffer from the actions of the richest and most powerful”. So Brindle, who is preparing for his A-levels in Devizes, Wiltshire, decided to do something. On top of his studies, he founded a campaign group, Teach the Future, which has spent the last few months formulating legislation entitled the climate emergency education bill. The latest version has just arrived in his inbox: it has been written by a professional parliamentary draftsperson, paid for by crowdfunding. “We didn’t want our demands to be half met, so we thought we’d show them exactly what we want,” says Brindle. Hiring an experienced drafter was a nifty move to quash any notion that young people’s ideas are unworthy of serious consideration. Brindle hopes the bill will be taken forward by the government, “or it could be a private members bill”.  On 26 February Brindle and his fellow campaigners will gather in parliament’s biggest committee room to launch their bill, sponsored by Nadia Whittome, the UK’s youngest MP. Greta Thunberg, the climate activist, in her blistering speech to the United Nations last September, said that as young people begin to understand adults’ betrayal of the planet, “the eyes of all future generations” will be watching. Now young people in the UK are demanding that the government address the climate emergency through radical reform of what – and how – pupils learn. But is the government listening? Brindle founded Teach the Future to campaign for a sustainable education system, after being inspired by the global school strikes that began in 2018. Its six demands are uncompromising, he says. “We feel the education system is wasting our time, because we’re facing the biggest issue of our time, and our education isn’t even touching on it.” As well as the proposed new act, Teach the Future is calling for a government review into how the education system is preparing students for the climate emergency and the ecological crisis. It wants teacher training to assess a minimum standard of knowledge about climate change and its impact, and a national fund to help young people’s voices be heard. It calls for all new state-funded educational buildings to have a zero-carbon footprint from 2022, with the entire education sector becoming net-zero by 2030, and a youth climate endowment fund to support young people’s projects and ideas. Last week the prime minister said the UK would lead the world in cutting carbon emissions, but was accused by Claire O’Neill, a former energy minister, of a “huge lack of leadership and engagement”. So, can our education system – and the Department for Education – step up?  Many teachers have their own ideas. In Leeds, Matt Carmichael, a long-time environmental campaigner, and English and drama teacher, who has spoken at events run by the Extinction Rebellion Educators group (XR Educators), is dedicating up to three days a week unpaid to develop an action plan for schools. This will not only mean changing what is taught, Carmichael says, but recognising that our education system itself has an impact on the planet through carbon emissions, building projects and procurement practices. Carmichael’s five action points include changing how buildings are heated; preparing schools for extreme weather; and addressing the mental health implications for students of fully comprehending the climate emergency. “What happens if our temperature levels are smashed by five degrees as happened in France? Then there’s flooding, and high winds. I don’t think schools would have a clue how to make children safe. We’re going to have to think about it.” While some schools are adapting their curriculum content, many others are “burying their heads in the sand as they always did”, he says. Everyone is operating “in the most astonishing radio silence” from the Department for Education, he adds. “There’s no guidance, no staff training and no accountability structure, so schools have no idea what the government thinks they are responsible for doing about climate change. It seems business as usual is fine by the DfE, but that’s not acceptable to a lot of students, parents and teachers.” The DfE says it understands the importance of students learning about climate change and “relevant topics are included in the national curriculum for both primary and secondary schools”. However, when asked if there were plans for new guidance, a spokesperson could offer nothing specific. Frances Bowman, a sixth form art and design teacher in London and a member of XR Educators, says children need to be supported through the “fear, guilt, anger and sadness” many feel. Sadness, she says is the one that students keep bringing up, “because there’s so much being lost. And the kids feel that more acutely, I sense, than the adults do.” Bowman says Extinction Rebellion has inspired her to reassess her responsibilities as an educator. “I feel quite cautious about the protest part of it, but there is an urgency to it,” she says – and the curriculum needs to take account of that. “The climate crisis means things are going to change, and yet we teach as if it’s all going to just go on as it is.” Some organisations are creating resources. A free “climate curriculum”, launched by the social enterprise ThoughtBox Education, and a climate learning week, created by London’s City and Islington college and the University and College Union, are two such projects. But a more comprehensive, deeper and radical approach is urgently needed, says Dr d’Reen Struthers, lecturer at the Institute of Education at University College London. She is campaigning for new thinking about the ethos in schools. “It means rethinking our content-heavy curriculum of information pupils need to regurgitate, and instead helping them learn how to question the insidious agendas that are all about money being made, which have led to this ecological crisis.” Brindle’s plan, too, calls for a fundamental change. “Some people have been pushing a natural history GCSE as the solution to climate education, but I think this furthers us from the solution,” he says. “Rather than pushing this aside so that only a handful of students learn about it, we should be making it a key aspect of all parts of education. “We don’t just want future ecologists to understand sustainability. We want bankers, builders and everyone else to consider it in everything they do.”"
"

Thomas Friedman of the _New York Times_ has a column today provocatively titled “Why I Am Pro‐​Life.” Of course he doesn’t mean that he wants the government to protect life in utero. Instead he turns to a standard Democratic theme: How can you say you’re “pro‐​life” and oppose welfare, environmental regulation, and every other government program? Friedman doesn’t miss a beat: “common‐​sense gun control…the Environmental Protection Agency, which ensures clean air and clean water, prevents childhood asthma, preserves biodiversity and combats climate change that could disrupt every life on the planet.… programs like Head Start that provide basic education, health and nutrition for the most disadvantaged children.…”   
  
  
But then he takes it a breathtaking step further: 



the most “pro‐​life” politician in America is New York City Mayor Michael Bloomberg. While he supports a woman’s right to choose, he has also used his position to promote a whole set of policies that enhance everyone’s quality of life — from his ban on smoking in bars and city parks to reduce cancer, to his ban on the sale in New York City of giant sugary drinks to combat obesity and diabetes, to his requirement for posting calorie counts on menus in chain restaurants, to his push to reinstate the expired federal ban on assault weapons and other forms of common‐​sense gun control, to his support for early childhood education, to his support for mitigating disruptive climate change.



Thomas Friedman’s vision of “pro‐​life” policies is, in every case, a network of bans and mandates forcing us to live our lives in ways that are pleasing to him and Mayor Bloomberg. No “life, liberty, and the pursuit of happiness” for him. No, his pro‐​life vision is Ira Levin’s dystopia in _This Perfect Day_ , a world in which the state takes care of our every need.   
  
  
When Hayek, in his essay “Why I Am Not a Conservative,” wrote about “the party of life,” he described it as “the party that favors free growth and spontaneous evolution.” Not Tom Friedman’s party! And certainly also not the party that seeks to ban drugs, gay marriage, and the discussion of evolution in science class. In her book _The Future and Its Enemies_, Virginia Postrel wrote at length about “the party of life,” and she didn’t have in mind Friedman’s crabbed view of a government that “protects life” by snuffing out liberty.   
  
  
Some years ago I wrote a column titled “Pro‐​Life,” and I too had the Hayekian, not the Bloomberg‐​Friedman, view of life and liberty in mind. But long before that, as usual, Alexis de Tocqueville, in “What Sort of Despotism Democratic Nations Have to Fear,” warned us that one day Thomas Friedman and Michael Bloomberg would come for our liberties: 



Above this race of men stands an immense and tutelary power, which takes upon itself alone to secure their gratifications and to watch over their fate. That power is absolute, minute, regular, provident, and mild. It would be like the authority of a parent if, like that authority, its object was to prepare men for manhood; but it seeks, on the contrary, to keep them in perpetual childhood: it is well content that the people should rejoice, provided they think of nothing but rejoicing. For their happiness such a government willingly labors, but it chooses to be the sole agent and the only arbiter of that happiness; it provides for their security, foresees and supplies their necessities, facilitates their pleasures, manages their principal concerns, directs their industry, regulates the descent of property, and subdivides their inheritances: what remains, but to spare them all the care of thinking and all the trouble of living?   
  
  
Thus it every day renders the exercise of the free agency of man less useful and less frequent; it circumscribes the will within a narrower range and gradually robs a man of all the uses of himself. The principle of equality has prepared men for these things;it has predisposed men to endure them and often to look on them as benefits.   
  
  
After having thus successively taken each member of the community in its powerful grasp and fashioned him at will, the supreme power then extends its arm over the whole community. It covers the surface of society with a network of small complicated rules, minute and uniform, through which the most original minds and the most energetic characters cannot penetrate, to rise above the crowd. The will of man is not shattered, but softened, bent, and guided; men are seldom forced by it to act, but they are constantly restrained from acting. Such a power does not destroy, but it prevents existence; it does not tyrannize, but it compresses, enervates, extinguishes, and stupefies a people, till each nation is reduced to nothing better than a flock of timid and industrious animals, of which the government is the shepherd.
"
"
Share this...FacebookTwitterThe German version of RIA Novosti reports that Russia hopes to gain more precise weather forecasts, new findings on global warming and improved exploration of new oil and gas reserves from its planned, new Arktika Satellite system. http://de.rian.ru/science/20100429/126119398.html
The Arktika System, which is made up of 5 satellites,  is a whole new instrument that will deliver absolutely new data on climate change says Alexander Bedrizki, Climate Appointee of the Russian President.  The project will allow continous observation of the Arctic 24 hours per day and be able to measure water temperature and ice thickness. The project will also have economic value because the Arctic holds huge oil and gas reserves. The project will also enable commerical flights to pass over the Arctic.
Alexander Frolow Director of the Russian Weather Service hopes to generate more accurate weather forecasts and to better assess events such as the recent Iceland volcano eruption which was above 60° north latitude.  Current satellite systems were not able to accurately track the cloud of ash from afar.
Russian aerospace company Lawotschkin will begin work on the project this year. Two communications satellites, two waether satellites and a radar satellite for measring ice  and exploring natural resources will be developed and launched into space.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterYet another Russian scientist believes the Arctic is set for cooling and thus increasing sea ice, this reported in the German version of the Russian online news RIA NOVOSTI (see links below). Scientist Vladimir Sokolov says:
The warming that occurred in the Arctic has swung back to cooling and sea ice that melted over the past years is recovering.
http://de.rian.ru/science/20100427/126088756.html, The English version is here: http://en.rian.ru/russia/20100427/158771845.html
Arctic sea ice reached a historic minimum in 2007 when it shrank to 4.28 million sq km. But the trend now appears to have reversed. According to the weather observation administration Roshydromet, it has grown by a fifth reaching 5.2 million square km in 2009, Sokolov said at the Petersburg Research Insttitute for the Arctic and Antarctic on Tuesday.
Sokolov calls predictions of continued shrinking Arctic ice “incorrect”.
He says the cooling is due to the polar night and the associated missing sunlight, and this as a result will lead to ice formation. Some scientists are warning that politicians and corporations who promise lucrative oil and gas projects in the Arctic may have made dramatic miscalculations. The researchers say that no warming will take place, instead cooling will impact the earth over the next decades.
Share this...FacebookTwitter "
"

Today’s question at “ _Politico_ Arena”:   
  
  
**“Have the greens failed?”**   
  
  
My response:   
  
  
If the greens have failed, it’s not for lack of trying. For years now, in everything from pre‐​school programs to “educational” ads aimed at adults, they’ve been “greenwashing” our brains. In September the _Wall Street Journal_ reported that the EPA was focusing on children: “Partnering with the Parent Teacher Organization, the agency earlier this month launched a cross‐​country tour of 6,000 schools to teach students about climate change and energy efficiency.”   
  
  
Yet for all that effort, the public isn’t buying. As _Politico_ notes this morning: “The Pew Research Center found that by last January, global warming ‘ranked at the bottom of the public’s list of policy priorities for the president and Congress this year.’ ” And “Independent voters and Republicans ranked it last on a list of 20 priorities, while Democrats ranked it 16th.” Meanwhile, “other polling suggests Americans are growing more skeptical of the science behind climate change, with those who blame human activity for global warming — 36 percent — falling 11 percentage points this year, according to Pew.” And that was _before_ “Climategate” came to light.   
  
  
At bottom, the greens face three basic problems. First, by no means is the science of global warming “settled” — if anything, the fraud Climategate surfaced has settled _that_ question. Second, even if global warming were a settled science, the contribution of human activity is anything but certain. And finally, most important, even if the answers to those two questions were clear, the costs — or benefits — of global warming are unknown, _but the costs of the proposals promoted by the greens are astronomical._   
  
  
So how do they respond to all of this? _Politico_ cites Greenpeace executive director Phil Radford: “ ‘Obama’s problem is not his position on the climate issue but, rather, his will,’ says Radford. ‘The question is how much the president will lead.’ Americans have ‘overlearned’ the lessons of Kyoto, where President Bill Clinton agreed to a treaty that he never submitted for ratification because it faced near‐​unanimous rejection in the Senate, Radford said. ‘They’re using that as a reason to hide behind Congress instead of to lead Congress.’ ”   
  
  
There you have it. It’s all a matter of will — indeed, of belief. The president needs simply to will this through, the people (and Congress) be damned. We, the anointed, know what’s right, what needs to be done. Is it any wonder that the greens are failing, at least where the people can still be heard?
"
"
Share this...FacebookTwitterReconstructions of past temperatures show much colder periods with higher CO2 levels or as-warm or warmer periods with much lower CO2 levels.
A new study (Paus, 2020) indicates modern July temperatures center around 7.5 to 8°C in the Scandes Mountains (Norway). Today’s CO2 atmospheric concentration has reached 410 ppm. 
During the latter stages of the last ice age (19,000 to 17,000 years ago), Late Glacial (LG) CO2 fluttered near 200 ppm. But with the discovery of temperature-sensitive tree species in the area it can be affirmed that July temperatures were also “at least 7-8°C” in the Scandes at this time. Despite more than a doubling of CO2, there has been no consequent summer temperature increase in this remote location.

Image Source: Paus, 2020
In another new study from Eastern Europe, Blagoveshchenskaya (2020) has determined January temperatures were almost 11°C warmer than the Little Ice Age (700 to 300 yrs ago) and 4°C warmer than today from about 6,000 to 4,500 years ago.
CO2 levels were 270 ppm when this region was 11°C warmer and 275 ppm when 11°C colder. It is 4°C colder today, at 410 ppm, than it was when CO2 was 270 ppm.
So, once again, reconstructions of Holocene temperatures do not support the narrative that CO2 and temperature changes are correlated.

Image Source: Blagoveshchenskaya, 2020


		jQuery(document).ready(function(){
			jQuery('#dd_674ed8c89aead8cfe23d2bf9d8a55622').on('change', function() {
			  jQuery('#amount_674ed8c89aead8cfe23d2bf9d8a55622').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

I am Paul C. Knappenberger, Assistant Director of the Center for the Study of Science at the Cato Institute, a nonprofit, non‐​partisan public policy research institute located here in Washington DC, and Cato is my sole source of employment income. Before I begin my testimony, I would like to make clear that my comments are solely my own and do not represent any official position of the Cato Institute.



For the past 25 years, I've conducted research on topics of climate and climate change including hurricanes, heat-related mortality, and temperature trends as well as worked to quantify the projections of human-caused climate change.



This last topic, specifically how it relates to the proposed Keystone XL pipeline, will be the subject of my testimony.



When I refer to climate change in these remarks, I am specifically referring to that climate change which may occur as a result of human emissions of greenhouse gases, primarily carbon dioxide. Climate change may (and does) occur from other influences as well, both human and natural. But the primary concern raised over the Keystone XL pipeline involves the carbon dioxide emissions resulting from the burning of the oil that the pipeline will carry. So it is the potential climate change from these emissions that will be focus of my testimony.



In its Draft Environmental Impacts Statement (DEIS), the State Department has done a good job in quantifying the extra emissions that result from the extraction, transportation, refining, and eventual end use of the oil which will be transported by the Keystone XL pipeline. They find, and I think that there is broad agreement on this point, that a barrel of oil produced from the Canadian tar sands has about a 17 percent carbon dioxide emissions premium compared to the average barrel of oil finding its way into the U.S. market.



The emissions premium primarily arises from the relatively energy-intensive manner in which tar sands oil is currently extracted. In the DEIS, the State Department points out that this emissions premium may well shrink over time as new extraction methodologies are developed, as extraction in other regions, such as Saudi Arabia, becomes more energy intensive, or depending on the type of oil that is ultimately displaced by the oil carried by the Keystone XL pipeline.



The disagreement between the State Department, the Environmental Protection Agency, and several environmental groups, involves how many new carbon dioxide emissions this current 17 percent per barrel premium results in when applied to the 830,000 barrels of oil that the Keystone XL pipeline will carry each day when operating at full capacity.



The State Department concludes that the demand for the tar sands oil is great enough that it will come to market whether or not the Keystone XL pipeline is ever built. It thus finds very few additional carbon dioxide emissions resulting from the pipeline project—somewhere in the range of an additional 0.1 to 5.3 million metric tons of carbon dioxide emissions per year over the case where the pipeline is not built.



The EPA contends that the State Department is too quick to come to such a conclusion. The EPA suggests that without the pipeline, much of that oil will remain in the ground. Therefore, if the pipeline were to be built, oil would be produced from the tar sands to meet its capacity. While this won't result in more oil being used in the U.S., it will result in a 17 percent carbon dioxide emissions premium applied to the 830,000 barrels per day of delivered oil. The EPA cites an extra 18.7 million metric tons of carbon dioxide emissions per year over the situation of no pipeline.



Several environmental organizations take the view that while the Keystone XL pipeline may not increase the amount of oil used in the U.S., the oil that it displaces from the U.S. market will be consumed by other countries as the global demand for oil continues to grow. Thus, they calculate the full emissions from the 830,000 barrels per day plus the 17 percent emissions premium, and arrive at an additional 181 million metric tons of carbon dioxide emissions per year resulting from the existence of the Keystone XL pipeline.In terms of carbon dioxide emissions, these differences may appear large and contentious, and, in fact, much of the protestation involving the Keystone XL pipeline focuses on these emissions numbers.



But, these protests are largely misplaced.



It is very important to keep in mind that the end game is climate change and the potential need of climate change mitigation. _Carbon dioxide emissions are not climate change_. They influence climate change, but they are not a measure of it.



Therefore, before any type of assessment as to the potential climate impact of the Keystone XL pipeline can be made, it is essential to translate the additional carbon dioxide emissions that may result from it into climate units—such as the global average temperature. In other words, how much global warming will the Keystone XL pipeline produce?



Isn't that what everyone wants to know?



Why is it then, that such numbers are never given?



It is not as if there is no good way of calculating them—that is precisely what climate models are designed to do. These complex computer programs emulate the earth's climate system and allow researchers to change various influences upon it—such as adding additional carbon dioxide emissions—and seeing what the end effect is. These climate models produce the projections of future climate change from human activities that we are all familiar with using precisely this methodology.



General circulation climate models are very complex and computational expensive to run (both in time and money) and as a result have not been used to generate the global temperature effects of the Keystone XL pipeline.



However, in lieu of running a full climate model, climate model emulators have been developed which can run on a desktop computer. One such program is MAGICC, the Model for the Assessment of Greenhouse-gas Induced Climate Change. MAGICC is a climate model simulator developed by scientists at the U.S. National Center for Atmospheric Research under funding by the U.S. Environmental Protection Agency and other organizations.



MAGICC is itself a collection of simple gas-cycle, climate, and ice-melt models that is designed to produce an output that emulates the output one gets from much more complex climate models. MAGICC can produce in seconds, on a personal computer, results that complex climate models take weeks to produce running on the world's fastest supercomputers. MAGICC doesn't provide the breadth of output or level of detail that fully resolved climate models do, but instead simulates the general, broader aspects of climate change such as the global average temperature.



Moreover, MAGICC was developed, according to MAGICC's website, ""to compare the global-mean temperature and sea level implications of two different emissions scenarios."" So, using MAGICC to compare the climate change that is projected to result from the different Keystone XL pipeline carbon dioxide emissions scenarios fits precisely into the program's designed purpose.



Using MAGICC, I (and anyone else) can calculate the potential impact of the Keystone XL pipeline on the global average temperature based on the various carbon dioxide emissions estimates, and produce results very similar to ones that would be achieved by using a full climate model. In the base case, I run MAGICC using a mid-range, business-as-usual future emissions scenario as defined by the IPCC (SRES A1B). To examine the climate change impact using the EPA's Keystone XL carbon dioxide emissions scenario, I add 18.7 million metric tons per year to the global carbon dioxide mission total each year beginning in the year 2010 and continuing through the year 2100. To assess impact of the emissions scenario preferred by some environmental organizations, I add 181 million metric tons of additional carbon dioxide emissions to the global total beginning in 2010 and extending through 2100.



When running MAGICC as described, I find that no matter how the additional carbon dioxide emissions are calculated, the Keystone XL pipeline has an exceedingly and inconsequentially small impact on projected the course of global temperature.



In the case of the State Department's analysis, as there are very few additional carbon dioxide emissions, there is essentially no associated change in the global climate. The change in global average temperature resulting from the EPA's additional 18.7 million metric tons of carbon dioxide emissions per year from the Keystone XL pipeline, would be about 0.00001°C per year—that is one one-hundred thousandths of a degree. The 181 million metric tons per year from the assumption that all Keystone XL oil is additional oil in the global supply would result in about 0.0001°C of annual warming—one ten-thousandths of a degree.



In other words, if the Keystone XL pipeline were to operate at full capacity until the end of this century, it would, worst case, raise the global average surface temperature by about 1/100th of a degree Celsius. So after nearly 100 years of full operation, _the Keystone XL's impact on the climate would be inconsequential and unmeasurable_.



And even these tiny numbers are probably overestimates. In calculating them, I used the MAGICC default value for the magnitude of the earth's equilibrium climate sensitivity. A value, 3°C, that was based on the assessment of the equilibrium climate sensitivity given by the Intergovernmental Panel on Climate Change (IPCC). The equilibrium climate sensitivity is the amount that the earth's surface temperature will rise from a doubling of the pre-industrial atmospheric concentration of carbon dioxide. As such, it is probably the most important factor in determining whether or not we need to ""do something"" to attempt to mitigate future climate change. The lower the climate sensitivity, the less the temperature rise from human carbon dioxide emissions, and the lower the urgency to try to reduce them. If the sensitivity is low enough, carbon dioxide emissions confer a net benefit.



And despite common claims that the ""science is settled"" when it comes to global warming, we are still learning more and more about the earth complex climate system—and the more we learn, the less responsive it seems that the earth's average temperature is to human carbon dioxide emissions.



For example, the observed lack of statistically significant temperature rise over the past 16 years (and counting), is strong indication that climate models have a tendency to overestimate the amount of warming resulting from human greenhouse gas emissions (Figure 1).





Figure 1. Current (ending in December 2012) trends in three observed global surface temperature records of length 5 to 15 years (colored lines) set against the probability (gay lines) derived from the complete collection of climate model runs used in the IPCC Fourth Assessment Report under the SRES A1B emissions scenario (Knappenberger and Michaels., 2013).



I was involved in research that we published more than a decade ago pointing out that global temperatures were not rising as fast as climate model expectations (Michaels et al., 2002), and increasingly, there is a growing acknowledgement of this fact.





Figure 2. Climate sensitivity estimates from new research published since 2010 (colored, compared with the range given in the Intergovernmental Panel on Climate Change (IPCC) Fourth Assessment Report (AR4) (black). The arrows indicate the 5 to 95% confidence bounds for each estimate along with the best estimate (median of each probability density function; or the mean of multiple estimates; colored vertical line). Ring et al. (2012) present four estimates of the climate sensitivity and the red box encompasses those estimates. The right-hand side of the IPCC AR4 range is dotted to indicate that the IPCC does not actually state the value for the upper 95% confidence bound of their estimate and the left-hand arrow only extends to the 10% lower bound as the 5% lower bound is not given. The light grey vertical bar is the mean of the 14 best estimates from the new findings. The IPCC's ""best estimate"" (3.0°C) is 50% greater than the mean of recent estimates (2.0°C).



Over the past three years, a collection of findings in the peer-reviewed scientific literature has suggested that the IPCC's best estimate of the equilibrium climate sensitivity is likely too high by nearly 50 percent. Instead of the IPCC's 3.0°C, the new findings are indicating a value close to 2.0°C (see Figure 2).



Rerunning the MAGICC climate model simulator with an equilibrium climate sensitivity setting of 2.0°C, instead of the 3.0°C default value, drops the calculated warming impact from the Keystone XL pipeline by about 30 percent.



It is this information, not the information on carbon dioxide emissions that is required to properly assess the climate change aspect of the environmental impact of the Keystone XL pipeline.



In these terms, the difference between the State Department's Environmental Impact Statement and those of its critics all but vanish.



No matter whose carbon dioxide emissions estimate is used to calculate it, the climate impact of the oil carried by the Keystone XL pipeline is too small to measure or carry any physical significance.  
In deciding the fate of the Keystone XL pipeline, it is important not to let symbolism cloud these facts.



 **References:**



Aldrin, M., et al., 2012. Bayesian estimation of climate sensitivity based on a simple climate model fitted to observations of hemispheric temperature and global ocean heat content. _Environmetrics_ , doi: 10.1002/env.2140.



Annan, J.D., and J.C Hargreaves, 2011. On the generation and interpretation of probabilistic estimates of climate sensitivity. _Climatic Change_ , **104** , 324-436.



Hargreaves, J.C., et al., 2012. Can the Last Glacial Maximum constrain climate sensitivity? _Geophysical Research Letters_ , **39** , L24702, doi: 10.1029/2012GL053872



Intergovernmental Panel on Climate Change, 2007. Climate Change 2007: _The Physical Science Basis. Contribution of Working Group I to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change_. Solomon, S., et al. (eds). Cambridge University Press, Cambridge, 996pp.



Knappenberger, P.C., and P.J. Michaels, 2013. Policy Implications of Climate Models on the Verge of Failure. American Geophysical Union Science Policy Conference. Washington, DC, June 24-26, 2013, submitted.



Lewis, N. 2013. An objective Bayesian, improved approach for applying optimal fingerprint techniques to estimate climate sensitivity. Journal of Climate, doi: 10.1175/JCLI-D-12-00473.1.



Lindzen, R.S., and Y-S. Choi, 2011. On the observational determination of climate sensitivity and its implications. _Asia-Pacific Journal of Atmospheric Science_ , **47** , 377-390.



Michaels, P.J., Knappenberger, P.C., Frauenfeld, O.W., and R.E. Davis. 2002. Revised 21st century temperature projections, _Climate Research_ , **23** , 1-9.



Ring, M.J., et al., 2012. Causes of the global warming observed since the 19th century. _Atmospheric and Climate Sciences_ , **2** , 401-415, doi: 10.4236/acs.2012.24035.



Schmittner, A., et al. 2011. Climate sensitivity estimated from temperature reconstructions of the Last Glacial Maximum. _Science_ , **334** , 1385-1388, doi: 10.1126/science.1203513.



Wigley, T.M.L., et al. MAGICC/SCENGEN v5.3. Model for the Assessment of Greenhouse-gas Induced Climate Change/A Regional Climate Scenario Generator. http://www.cgd.ucar.edu/cas/wigley/magicc/



van Hateren, J.H., 2012. A fractal climate response function can simulate global average temperature trends of the modern era and the past millennium. _Climate Dynamics_ , doi: 10.1007/s00382-012-1375-3.
"
"

Last year, Chairman of the Joint Chiefs of Staff Gen. Martin Dempsey contended that “we are living in the most dangerous time in my lifetime, right now.” This year, he was more assertive, stating that the world is “more dangerous than it has ever been.”



Is this accurate? At “Dangerous World? Threat Perception and U.S. National Security,” a Cato Institute Conference held in October, experts on international security assessed the supposed dangers to American security, examining the most frequently referenced threats, including wars between nations and civil wars within nations.



Historically, states have posed the greatest threats to international security. The first two panels discussed whether this is still the case, exploring the dangers not only from traditional nation‐​states but also from sub‐​state actors.



“The U.S. government has overreacted to terrorism relative to its direct physical costs,” Max Abrahms, assistant professor at Northeastern University, said. But the policy community has also inflated the risk that terrorism would spread throughout the world. “Just as the direct costs of terrorism have been overstated, so too has the political value.”



With a lack of credible state rivals since the end of the Cold War, fears have arisen in response to less traditional dangers, including cyberwar, climate change, and general instability. Mark G. Stewart, director of the Centre for Infrastructure Performance and Reliability at the University of Newcastle, subjected the worst‐​case scenarios of global warming to cost‐​benefit analyses. “My answer is that the impact of climate change on national security is manageable,” he concluded. “Change is going to be gradual—not abrupt—and there will be plenty of time to adapt.”



Given that many of these threats have been inflated, the question remains whether the global order depends on a single power enforcing the rules. In the final panel, scholars considered whether the United States must prevent general lawlessness in order to maintain our relative prosperity. Eugene Gholz, associate professor of political science at the University of Texas at Austin, challenged this thesis by focusing on how costly it is to fight wars.



“The claim that the global economy would become unhinged if the United States was not providing primacy and tamping down conflict around the world is just not true,” he concluded. In the end, many of those scholars that disagreed with the Institute’s positions nevertheless praised its scholarship—even on issues as divisive as foreign policy. “Cato scholars are very strong and clear advocates of the view that the U.S. should retrench,” said Stephen Brooks, associate professor of government at Dartmouth College. “In my view, this comprehensive version of retrenchment … is the one which is most interesting and most compelling as an alternative to the current U.S. grand strategy.”
"
"For a month or two every summer, beaches across the south-eastern Mediterranean might be packed but the inviting seas remain suspiciously empty. It’s jellyfish season, and the “nomads” are out in force. Bathers avoid the sea while the swarm is offshore; their stings are painful and may last weeks, and one is likely to feel a burning sensation even without touching the creature.  Yet the nomad jellyfish (Rhopilema nomadica) is but one of more than 350 marine invasive alien species that have traversed the Suez Canal from the Red Sea into the Mediterranean. People long dreamed of joining the two seas, and these ideas gained force as Europe’s maritime trade with the East became increasingly profitable. Indeed, one of Napoleon’s aims in his Egyptian campaign was to “…cut the Isthmus of Suez … for the French Republic”. In 1854 the French vice-consul to Egypt, Ferdinand de Lesseps, was given the go-ahead. Private investors in France put up half the capital needed and much of the rest was invested by Egypt’s ruler, Mohammed Said, who also supplied the bulk of the workforce – 20,000 conscripted fellahin (peasants) and prisoners.  That canal, completed in 1869, was 8m deep, 58-90m wide and 160km long. The canal was deepened and widened several times until it reached its present size: 24m deep and 313m wide. The Suez Canal is now one of the primary pathways for invasions by alien marine species globally – and its impact on the Mediterranean has been particularly harmful.  The invasion was foretold. Even before the original Suez Canal was excavated Leon Vaillant, a French zoologist, argued that the canal would allow organisms to “emigrate”. He called for a survey of the pre-canal ecosystem – what today would be considered a “baseline study”. That never took place. Today, massive jellyfish swarms clog intake pipes at desalination and power plants. They spoil fishing harvests and nets, stinging tourists and causing millions of euros in economic damage to the marine tourism industry.  The silver-cheeked toadfish (Lagocephalus sceleratus), a poisonous pufferfish from the Indo-Pacific Ocean, has become highly abundant in fishing catches right across the Mediterranean over the past decade and has now made it as far as Crimea. It poses severe health hazards: between 2005 and 2008, 13 people were treated for poisoning in Israel alone and hospitalisations occurred in Egypt, Lebanon, Cyprus, Turkey and Greece.  Alien invasive rabbitfish have created extensive barren areas along rocky shores. They’re algae-eaters with few competitors. Their overgrazing has dramatically reduced the habitat available for other creatures, with knock-on effects right through the food web. In August this year Egypt announced plans to further enlarge the Suez Canal, doubling its capacity by creating a new waterway parallel to the current channel. This should mean more invasions from the Red Sea. The Mediterranean is a sensitive and unusual environment – a sea full of unique wildlife. Therefore risky projects of this magnitude must, at minimum, be subject to a transparent and scientifically sound Environmental Impact Assessment (EIA) and risk analysis.  This in turn should lead to control and mitigation measures. One of the possible choices may be the introduction of locks and a salinity barrier – the original Suez Canal passed through a highly salty artificial lake which kept some organisms from crossing. Yet the publicly available information on the enlargement of the canal makes no reference to an EIA.  Marine biologists are apprehensive, and a group of 18 of us have expressed our concerns in the journal Biological Invasions. We fear an increased influx of additional marine invasions in the Mediterranean Sea.  We recognise Suez enlargement is inevitable, as global trade and shipping are vital to society. Yet the existing international agreements also urge for sustainable shipping that minimises unwanted impacts and long term consequences. There are means available to limit the introduction of invasive species, which can be carried out at the early stages of a project but which become increasingly expensive as the project progresses.  The ecological and economic cost of inaction may be substantial, potentially leading to irreparable, large-scale ecosystem damage."
"Is solar power the technology of the future? It is certainly the fastest-growing energy generation technology in the UK. By the early 2020s, according to a new report, it will be cost-competitive with gas and coal power. If so, the goal of having unsubsidised renewable energy is in sight.  The report, by Berlin-based think tank Thema1, concludes that this is possible without radical technology improvements or similar step changes. This somewhat disagrees with similar studies, which tend to point to the next big thing as being just around the corner.  There are lots of exciting developments in the laboratories but to make a real difference they need time – more than the 10-year time frame in Thema1’s forecasts, so their report is right not to factor them in. The majority of new technologies focus on the photovoltaic (PV) module itself, promising higher power output per unit (by using graphene or nanotechnologies) or much reduced production costs (using novel materials like organic solar cells).  Higher rates of converting light into electricity (“efficiencies”) are always welcome in new PV devices, but their viability depends on the production costs. It is possible today to produce cells that can convert up to 46% of the sun’s power into electricity, but costs render these commercially unfeasible. The incumbent technology, wafer-based silicon PV modules, converts about 22% of sunlight at a fraction of the cost.   On the other hand, there is a lot of excitement around technologies such as organic solar cells that are less efficient but have much reduced costs. But this approach tends to shift the balance of costs from the module to the other system components such as mounting structures and can make the system more expensive. To be commercially viable, these devices need a minimum efficiency of about 10%-12%. This recently led to the demise of virtually all thin-film silicon manufacturers, for example, which struggled to get the double-digit efficiencies in cost-effective production times.  Market shares of regular and thin-film PV The reality is that the road from laboratory cell to a full-size module is surprisingly difficult and slow. This can be seen when looking at current polysilicon thin-film technologies and how long it took them to come to their current competitive position. There is no reason to believe that other technologies will be much luckier.   Having said this, the Thema1 report is right to say that PV can achieve the costs required to survive without subsidies without any step change in technology. All it needs is the political will. If governments offer sufficient subsidies in the short term, solar will cut costs just by doing things better. This was the underlying idea of solar subsidies all around the world in recent years. Yet Thema1 suggests that all we now need to do is incrementally reduce these subsidies and by 2020 will have learned how to do things at the market price. This is not completely impossible, but there are some major caveats.  The reductions to UK subsidies of recent years are in fact one of the biggest issues in the industry at present. There were step cuts in funding that incentivised developers to rush through solar projects before cut-off dates, which resulted in installation gluts. This has been detrimental for the quality of installations, resulting in higher operation and maintenance costs and thus higher energy costs.  Governments might argue that subsidy reduction has happened each year and is therefore foreseeable, However, this ignores the fact that these “cliffs” result in a rushed building phase to meet the deadlines. Reductions typically occur in April. This means most building happens in the first quarter of the year, when the weather affects ground conditions and can drive up costs. Changing this hard funding cliff to a softer decline and shifting the timing to later in the year may actually make a noticeable difference in system costs.   UK renewables subsidies 2014/15 The cost of connections is another major issue in the UK, especially with larger developments. The connection cost is sometimes nearly as expensive as the system itself – clearly rendering the investment impossible. This may be down to weaknesses in the grid and should be addressed on a national scale. All new technologies for producing electricity have required major grid investment, so saying such moves are too expensive for solar is a bit of a smoke screen.  The most mentioned solution to making solar more competitive is to make it possible to store the electricity to get around the problem that the amount of solar energy varies during days and seasons. But this is potentially not required in the medium term. One reason is that people make the mistake of looking at technologies in isolation. There have been studies in Germany that indicate that this variability can be offset by using wind and solar together, for example. One would need to look at the combinations for the UK to see if this is true in this country as well. It is also worth pointing out that subsidies are paid to renewable electricity irrespective of the time of generation. If they were somewhat redistributed to include a timing element, it could be a way of cutting the price of PV energy without improving the technology. You can also maximise the amount of generation by shifting the orientation of the panels.  In short, the factor that has the power to make or break solar power is the political support. Combine such changes with the fact that PV still does have amazing cost-saving potential through technological progress and you have a future that could still be very sunny indeed. It is no exaggeration to say that incentive-free solar really could be on the horizon."
"For several years now climatologists have puzzled over an apparent conundrum: why is Antarctic sea ice continuing to expand, albeit at the relatively slow rate of about one to two percent per decade, while Arctic sea ice has been declining rapidly (by some 13% per decade in late summer)? Just a few weeks ago the Antarctic saw a third consecutive record year of sea ice coverage. The two previous records were set in 2012 and 2013.  To help get to the bottom of this mystery, one team of scientists have enlisted an underwater robot to help measure the thickness of the ice. Their vehicle, known as SeaBED, has an upwards looking sonar which maps the underside of ice floes and provides novel, highly-detailed three-dimensional maps of Antarctic sea ice. The researchers present their findings in the journal Nature Geoscience. Measuring a total of ten ice floes covering more than 500,000 square metres, they found mean ice thicknesses of 1.4 to 5.5 metres. In some places the ice was up to 16 metres thick. This is much thicker than has been gauged by previous more limited field-based (mainly ship-based) measurements, possibly because ships tend to avoid the areas of thicker sea ice, so there may very well be a sampling selection bias.  Satellites would ideally be able to assess ice thickness over a much wider area. However, although they have had some success in the Arctic, at the other end of the world satellites are severely hampered by our poor knowledge of how much snow there is on top of any given area of Antarctic sea ice. The researchers report that the ice they measured was in its first year of growth. This is important because it is the multi-year sea ice (ice that survives more than one summer melt season) which is more susceptible to thickness growth through deformation and ridging.  Previous estimates of mean thickness for first-year Antarctic sea-ice – which date back to at least 1986 – suggest it is no more than around a metre thick on average. It has long been recognised however that much thicker multi-year ice floes exist – especially near the coast and the Antarctic Peninsula, where sea-ice ridges can be the size of a house. Although the new study is important, especially from an innovation/technological point of view, I would like to see this kind of analysis repeated across a range of different areas, and if possible seasons and years. At the end of winter around 20 million square kilometres of sea around the Antarctic is covered by ice – an area larger than Russia. The surveyed zone is tiny in comparison. If the results are confirmed by future work, they suggest Antarctic sea-ice may be more resilient towards climate warming than has previously been appreciated.  Also, changes in the sea ice will in turn affect land-based glacial ice and free-floating ice-shelves if the sea-ice suddenly gets removed (or is thicker than realised). This is especially important in regions next to the strongly warming Antarctic Peninsula where some ice shelves have dramatically broken away into the ocean. However, it will be some time yet before we know answers to the crucial question of what has caused recent sea-ice growth in Antarctica: is it changes in ocean currents, maybe related to an increase in fresher, colder sub-surface meltwaters running off from the great continental ice sheets? Nevertheless thicker Antarctic sea-ice cover has profound implications. Ice thickness closely controls the exchange of energy between the ocean beneath and the air above – without ice cover, too much heat will leave the oceans and join the atmosphere. Although ice is a very effective insulator, as soon as it reaches a few tens of centimetres thickness, equally important are holes in the ice cover. These holes, knows as leads and polynyas (leads are long rectilinear channels in the ice, while “polynya” comes from the Russian from “open” and is a larger, lake-like opening) act as natural vents or chimneys, releasing hundreds of watts  of heat per square metre into the overlying atmosphere.  Any changes in the distribution of ice thickness can dramatically affect the points where these features form and their persistence. Thus it is fair to say that Antarctic sea ice thickness plays a pivotal role in what we call sea ice-climate feedbacks, where levels of ice cover are strongly linked to ongoing global climate change and vice versa. It is therefore crucial for scientists modelling sea ice behaviour to have a good knowledge of thickness distribution to feed their models. This study represents a significant step forward."
"
Share this...FacebookTwitterIn my recent post here I wrote about a ZDF story on an Expert Assessment Report, led by Prof. Dr. Kai Konrad of the Max Planck Institute and a team of finance researchers, on Europe’s and Germany’s climate policy. The report is titled:
Climate Policy Between Emissions Prevention and Adaptation
Expert Assessment By The Scientific Advisory Board Of The Federal Ministry of Finance
Note: The report itself is not a product of the Max Planck Institute, as some have mistakenly believed. The lead author is Dr. Kai Konrad of the Max Plank Institute, who is also vice chairman of the Finance Ministry’s Scientific Advisory Board, the actual producer of the assessment report. The members of the Scientific Advisory Board participating in the expert assessment are listed below at the end of this post.
You’ll recall the assessment report was so damning that the Finance Ministry took it down from its website. When you read the following summary and conclusion you’ll see how it completely contradicts the government’s current policy, which is to prevent CO2 emissions and to subsidise alternative energy. This is a finding that was embarrassing for the government.
Note that the authors of the assessment report take the position that CO2 is bad for the climate, i.e. the more CO2 that is produced, the worse the climate will become. They are finance experts after all, and not climate experts – obviously.
I’ve translated the all-important Part 4, Summary and Conclusion (bold print is my emphasis), which is as follows:
4. Summary and Conclusion
Economic and political action on global warming can be categorised under two kinds of measures: 1) measures that aim to slow down global warming (prevention) and 2) measures that aim to react to global warming (adaptation).
With adaptation measures, the beneficiary and the cost-bearer are the same. Decisions concerning many adaptation measures can thus be decided by the private economy. In the cases where this is not possible, the extent of adaptation measures can be handled by the local, regional or national politics.
But when it comes to measures for preventing CO2 emissions, the circle of beneficiary and the cost bearer splits apart. A meaningful reduction in emissions through uncoordinated, single country initiatives cannot be achieved. Effective emissions reduction with respect to global climate protection can be accomplished only through global coordination. In the past, global coordination has proven to be difficult and hardly successful. Despite various international attempts and considerable use of resources on the part of some countries, a worldwide climate policy has not been reached.
The theory of international public good offers an economic explanation as to why the international climate policy has not reached its ambitious goals up to now. That’s why suspicions that the current efforts will not lead to any success are being confirmed.
This assessment yields the following results:


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




• The uncoordinated, single-country go-it-alone approach leads to unachievable emissions reductions. Many polluters hardly participate in avoiding emissions. It has to be expected that only the more populated, economically strongest, environmentally aware and climatically threatened countries will make any notable efforts to undertake emissions reductions.
• Efforts by single countries to act as a leader in climate protection and to influence climate policy by imposing emissions reductions on itself can cause other countries to slack off in their own climate-policy efforts rather than intensifying them. As a result, taking a leadership role in climate policy leads to, as a rule, higher costs in that country without assuring any decisive improvement in the global climate.
• Special efforts and leadership initiatives made by individual countries also do not necessarily improve the situation for a global climate agreement, but rather can actually imperil an agreement. Diminishment of remaining benefits arising from worldwide climate agreements make the realisation of an agreement more improbable.
• Also unfavourable are agreements among groups nations of the international community of nations. Such agreements greatly burden  the participating countries economically, and serve to benefit the countries that do not participate. Despite the high costs, the positive climate effects of such group-nation agreements can end up being very small. Moreover, coalitions of nations can actually worsen the chances of an international worldwide climate treaty.
However, in no way do these arguments speak against continuing international negotiations. Effective international climate agreements are urgently needed. The arguments listed above do, however, speak against going it alone nationally, taking a leadership role, in preventing CO2 emissions.
When it comes to implementing measures for adaptation to climate change, there are no problems like those listed above. Measures for adapting to climate change do not have the problems that measures for prevention have. Adapting to climatically related environmental changes do not have the “free-rider” problem, where one incurs the costs and the other reaps the benefits. The circle of beneficiary and cost-bearer are mutual when it comes to adaptation measures. The strategy of adaptation thus offers opportunities for a unilateral, cost-effective national climate policy in a wide variety of impact areas (e.g. against flooding or storm damage). At the same time, such a policy augments the chances of an international emissions limitation.
• The adaptation strategy leads to an immediate climate cost reduction in one’s own country, independent of  international agreements.
• If a country invests in national adaptation measures, it also improves its bargaining strength in negotiations for a climate treaty.
• When all countries take up adaptation strategies, it results in – when compared to an ideal, worldwide combination of both instruments – a strain that in the end favours adaptation instead of prevention. The economic-political result would be worse than the one from a non-existing prosperity-maximizing world government, but better than the result that would arise from foregoing an adaptation strategy.
• Without adaptation measures, more prevention measures would have to be undertaken due to reasons of precaution and in view of the uncertainty of climate impacts from irreversible CO2 emissions. Adaptation buys governments time to more precisely research climate impacts.
The way for some especially motivated industrial countries to use comprehensive unilateral early contributions and subsides for alternative energy is misguided with regards to a timely, binding and adequately scaled climate policy. Even worse, it is to be feared that this policy not only has been and is very expensive for Germany and Europe, but also that it is an obstacle to reaching an effective worldwide climate policy. In view of the fact that emissions reduction is an internationally public good and in view of strategy effects, the Advisory Board recommends options for adaptation to climate change be examined and pursued more vigorously by single countries than in the past. The strategy of adaptation does not only ensure immediate adaptation to climate change, but also increases the chances for an effective international agreement to reducing emissions.
Directory of members of the Scientific Advisory Board at the Federal Ministry of Finance
Prof. Dr. Clemens Fuest (chairman)
Prof. Dr. Kai A. Konrad (vice chairman)
Prof. Dr. Dieter Brümmerhoff
Prof. Dr. Thiess Büttner
Prof. Dr. Werner Ehrlicher
Prof. Dr. Lars P. Feld
Prof. Dr. Lutz Fischer
Prof. Dr. Heinz Grossekettler
Prof. Dr. Günter Hedtkamp
Prof. Dr. Klaus-Dirk Henke
Prof. Dr. Johanna Hey
Prof. Dr. Bernd Friedrich Huber
Prof. Dr. Wolfgang Kitterer
Prof. Dr. Gerold Krause-Junk
Prof. Dr. Alois Oberhauser
Prof. Dr. Rolf Peffekoven
Prof. Dr. Dieter Pohmer
Prof. Dr. Helga Pollak
Prof. Dr. Wolfram F. Richter
Prof. Dr. Ulrich Schreiber
Prof. Dr. Hartmut Söhn
Prof. Dr. Christoph Spengel
Prof. Dr. Klaus Stern
Prof. Dr. Marcel Thum
Prof. Dr. Alfons Weichenrieder
Prof. Dr. Dietmar Wellisch
Prof. Dr. Wolfgang Wiegard
Prof. Dr. Berthold Wigger
Prof. Dr. Horst Zimmermann
Share this...FacebookTwitter "
"
Share this...FacebookTwitterEarly in 2011, NTZ readers and I entered a climate bet with Rob Honeycutt and climate warming dogmatist Dana Nuccitelli. The bet, which I dubbed the Honeycutt Climate Bet for Charity, was whether globally the 2011-2020 decade would be warmer or cooler than the previous 2001-2010 decade.
Myself and and a number of NTZ followers that bet the 2011-2020 decade would see no warming, or even cooling. Conversely Messieurs Honeycutt and Nuccitelli claimed global warming would continue, due to manmade greenhouse gas emissions, of course. To decide the winner of the bet, it was agreed that the RSS and UAH datasets would be used.
Now we are at the end of 2020, and the data suffices to declare a winner. The following chart is the latest UAH from Dr. Roy Spencer:

Chart: Dr. Roy Spencer.
As we can see, before 2016 the global mean temperature had been cooling since 1998, thus establishing what came to be known as “the hiatus”. The early half of the 2011-2020 decade had been running a bit cooler than the previous decade.
But then came the monster 2015/16 El Nino, a natural event occurring at the equatorial Pacific and it saved the warmists from losing the bet:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




That ENSO event was the strongest in memory, and thus drove global surface temperatures to a new level. As we can see from the UAH chart above, the overall current decade indeed did become warmer than the previous 2001-2010 decade. I haven’t crunched the numbers, but I think no one will dispute it.
Congratulations to the warmists on winning the bet, all thanks to the natural factors that coolists keep arguing in favor of.
Of course, the bet winners will boast and insist it’s all due to human activity. But it isn’t. Such a claim is Dominion-voting-machine science. The one deciding factor was the powerful mid-decade El Nino event, which is natural and has little to do with people burning fossil fuels.
Personally the results do not change my skeptic view of CO2’s role in climate at all. In fact the results only reinforce my view because it’s crystal clear that the 2016- 2020 warming was due to the El Nino, a natural factor, and not CO2.
200 euros for SOS Kinderdorf e.V. 
Those participating in the bet of course must honor their bets and pay the pledged amounts to a charity helping needy children. I myself will be paying to SOS Kinderdorf e.V., a Germany-based charity set up to aid needy kids, 200 euros (ca. 230 USD). I’ll be posting proof of payment in the days ahead, as soon as the bank statement comes in).
I’ll also be sending an e-mail to the other bet participants and asking them to pay up. Hopefully they are all still with us.
I don’t ever lose my climate bets. But as you can see, there’s always a first time.


		jQuery(document).ready(function(){
			jQuery('#dd_ec8e161b5077dbcad2b1a7613e27689f').on('change', function() {
			  jQuery('#amount_ec8e161b5077dbcad2b1a7613e27689f').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

Super Typhoon Haiyan left a path of tremendous devastation as it traversed the Philippines. Virtually all Category 5 hurricanes that make landfall do the same, with the number of casualties modulated by poverty, preparation, and preparedness (which are two different concepts).



While they are not infrequent over the open ocean, Category 5 storms don’t hit land very often. There’s some evidence that hurricanes of this intensity are unstable, and the eyewall structure required to maintain such winds often undergoes a natural change that reduces them to Category 4, or even three.





The best defense against Category 5 storms is resilient infrastructure and preparedness—characteristics surely better achieved through a free‐​market, than global governance.



But some of them do devastate a coastline before this occurs. That includes the current record‐​holding storm for (estimated) maximum wind, 1969 Hurricane Camille, which one of us (Michaels) rode out in an oceanfront laboratory not far from the eastern eyewall. Haiyan may well beat Camille’s records, but we won’t know until damage surveys are completed.



If Category 5 landfalls were more common we’d probably be more cautions as to how and where (and if) we decided to construct coastal communities. It’s a fact that buldings can be hardened to withstand some pretty severe winds. Hong Kong is going to get a Category 5 one day, and the high‐​rise building codes there are extremely stringent. It’s a matter of cost and benefit, like so many other things.



Nowadays, in the aftermath of every weather‐​related disaster, proponents of restricting fossil fuel use in the name of halting climate change are quick to place the blame for the tragedy on human‐​caused climate change (i.e., industrialized nations like the U.S.). The calls to “do something” amplify.



This is happening right now in Warsaw, at the latest (19th) in a long string of U.N.-organized Climate Change Conferences aimed at getting countries to agree to some sort of action aimed at mitigating climate change.



On the conference’s opening day, an envoy form the Philippines, Yeb Sano, gave an emotional address to the delegates in which he vowed to stop eating until something was accomplished.



“I will now commence a voluntary fasting for the climate. This means I will voluntarily refrain from eating food during this (conference) until a meaningful outcome is in sight.”



Adding,



“We can fix this. We can stop this madness. Right now, right here.”



Sano got a tear-filled standing ovation.



While the outpouring of sympathy was certainly deserved, an outpouring of action on climate change is certainly not. A story from the _Associated Press_ covering the events at the conference summed up the science on anthropogenic climate change and tropical cyclones pretty accurately:



Scientists say single weather events cannot conclusively be linked to global warming. Also, the link between man-made warming and hurricane activity is unclear, though rising sea levels are expected to make low-lying nations more vulnerable to storm surges.



In other words, limitations, even strict ones, on anthropogenic emissions of carbon dioxide and other greenhouse gases—the very thing that Sano seeks—will have no detectable (at least based on our current scientific understanding) impact on the characteristics of future tropical cyclones, such as Haiyan, or Sandy, or Katrina, or any other infamous storm. And as for sea level rise, projections are far more lurid than observations.



The hard numbers (from Ryan Maue’s excellent compilation) show that global tropical cyclone activity for the last 40+ years—during the time of decent observations and the time with the greatest potential human impact from greenhouse gas emissions—while showing decadal ups and downs, show little overall change. In fact, global cyclone activity has been below average for the past 5 years.





_Figure 1. Global cyclone activity as measured by the ACE ACE -2.14% (accumulated cyclone energy) index since 1972 (top). The Northern hemisphere ACE index is denoted by the lower (black) points, and the Southern Hemisphere ACE is the area in between the two curves (from Ryan Maue)._



The science on tropical cyclones is complicated and ultimately unclear in terms of the influence of greenhouse gas emissions, but is quite clear when it comes to the influence of demographics and wealth vs. climate change—the former grossly dominates the latter when it comes to future tropical cyclone disasters. So, no matter what this year’s U.N. climate confab does (forecast: nothing significant), it will not result in any meaningful changes to damages from future tropical cyclones.



Category 5 storms like Haiyan, Andrew, and Camille will always pose a threat to coastal communities (and beyond) in tropical cyclone-prone areas of the globe. The best defense against them is resilient infrastructure and preparedness—characteristics surely better achieved through a free-market, than global governance. But no matter what actions are taken, more Category 5 monster storms are coming. When they arrive, the news ought to focus on where they hit, not _that_ they hit.
"
"
You may recall the previous post where Basil Copeland and I looked at correlations between HadCRUT global temperature anomaly and sunspot numbers. This is similar, but looks at the Pacific Decadal Oscillation (PDO) and uses the same Hodrick-Prescott (HPT) filter as before on the HadCRUT global temperature anomaly data and the PDO Index.

click for a larger image –
NOTE: the purple line is a monthly warming rate, to get decadal values, multiply by 120
This graphic provides some context to what may be happening with the PDO. In the upper panel we’ve plotted the PDO (in red), a smoothed PDO (in light blue), and our analysis of the bidecadal variation in warming rates.
From the PDO data itself, it is just too soon to be able to tell whether the current cool phase is just one of the shorter cycles, or whether it is the beginning of a longer term cycle like we saw back in the 1950’s and 1960’s. It is tempting, when looking at the warming rate cycles, to believe that we’ve just come out of a 60-66 year “Kerr” climate cycle, and are on the cusp of a cool phase like we see for the 1950’s and 1960’s.
But if you look closely at the end of the purple curve for our warming rate cycle, it seems to be about ready to turn back up. Now we do not want to put too much stock in the end values of a series that has been smoothed with HP filtering. So it could still be on a downward trend.
Then, to make it all the more interesting, we have solar cycle 23 lingering on. Considering that also, confidence is higher that we will continue to see a relative respite in the rate of warming and that we’re not likely to see our warming rate cycle jump back to where it was during solar cycles 22-23. But whether we see a full blown interlude between two strong warming trends, like we saw during the 1950’s and 1960’s, remains to be seen.
In other words, as we saw with Easterbrook’s analysis, we can be reasonably confident in projecting at least no further warming for a while. For that to happen, the purple warming rate curve must not only turn back upwards, it must rise into the region of positive values, and continue to rise for several years. If solar cycle 24 turns out to be a weak solar cycle, and there are historical precedents for cycle length suggesting it is likely to be weak, that probably isn’t happening.
I’ll have more on solar cycles 23 and 24 coming up in the next day or so.
So, in summary; probably no net warming for awhile, and maybe a period of extended cooling as in the mid 20th century. It all depends on whether this current PDO shift is a short term or longer term event such as we saw in the mid 20th century.
This is inline with the article in today’s UK Telegraph, saying: 
“Global warming will stop until at least 2015 because of natural variations in the climate, scientists have said. Researchers studying long-term changes in sea temperatures said they now expect a “lull” for up to a decade while natural variations in climate cancel out the increases caused by man-made greenhouse gas emissions. 
The average temperature of the sea around Europe and North America is expected to cool slightly over the decade while the tropical Pacific remains unchanged. This would mean that the 0.3°C global average temperature rise which has been predicted for the next decade by the UN’s Intergovernmental Panel on Climate Change may not happen, according to the paper published in the scientific journal Nature.”
There’s a similar article in Yahoo News.
The paper by Keenlyside et al entitled “Advancing decadal-scale climate prediction in the North Atlantic sector” from the Nature website


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f4de349',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
The Dr. Roger Pielke Sr. weblog today includes a letter from Dr. Joanne Simpson, recently retired.  He calls her “among the most preeminent scientists of the last 100 years”. It seems that she really spoke her mind on the subject of climate models and the problems of the changing measurement environment around climate monitoring stations.
The full letter is here on that weblog.
Excerpt: 

Since I am no longer affiliated with any organization nor receive any funding, I can speak quite frankly. […] The main basis of the claim that man’s release of greenhouse gases is the cause of the warming is based almost entirely upon climate models. 
We all know the frailty of models concerning the air-surface system. We only need to watch the weather forecasts. […] The term “global warming” itself is very vague. Where and what scales of response are measurable? One distinguished scientist has shown that many aspects of climate change are regional, some of the most harmful caused by changes in human land use. 
No one seems to have properly factored in population growth and land use, particularly in tropical and coastal areas. 
[…] But as a scientist I remain skeptical. I decided to keep quiet in this controversy until I had a positive contribution to make. […] Both sides (of climate debate) are now hurling personal epithets at each other, a very bad development in Earth sciences. 

I agree, enough of this sniping. 
Witness the cordial exchange I have with Atmoz, a graduate student at the University of Arizona in Tucson. We see things differently, each of us has made some good analyses and each of us has made some mistakes, but we don’t insult each other over it.
Though I do wish he and others would remove the cloaks of anonymity. Science has never been advanced by an anonymous person, there’s always a real person with a name at the center of discovery and progress.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0e96a5c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
The week was productive, 21 USHCN stations visited, 20 surveyed, one dropped due to access problems (Southport, NC which turned out to be at an Army Depot). My trip odometer said 1828 miles when I turned in the car in Nashville tonight.
Here is the map of my travels this week:

Click for an interactive map
The highlight of the week was of course my 2 day visit to NCDC and the survey of the new CRN station west of Asheville. Another fun moment in the trip came when I visited the Lewisburg, TN Agricultural Experiment Station. It was quite a pretty setting for a station:

While I was doing the survey, and looking for the MMTS which wasn’t near the Stevenson Screen but was indicated by the NCDC equipment log, a farm cat came by to say hello. He was quite the talker. He gave me the grand tour and followed me while I was looking around.

I asked him: “hey Kitty, have ya seen Hansen’s Bulldog around” ? He answered simply “meow” and then took off to the cattle barn. I kid you not.
Interesting thing about this trip, I identified two stations that have undergone undocumented station moves in the last year, which look like good test cases for detecting undocumented changes points via the new USHCN2 methodology. More on that later.
Footnote: While this is a lot of miles, it’s nothing compared to the mileage that Don Kostuch, Eric Gamberg, Russ Steele, and others have put in over the life of this project. I wish to thank them too.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9fbaff9e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Please click the picture then continue reading.
This is the city. Los Angeles, California. I study weather stations here. I carry a thermometer. My name’s Anthony. The story you are about to see is true; the names have been changed to protect the innocent.
The day was Monday, March 24th, four days after the vernal equinox. It started out like any other day, with a bad cup of coffee and a stack of reports on scumbags you normally wouldn’t give the time of day to. But then, just as I was about to down that last gulp of coffee, a tip came in on the email hotline. It was Goetz, and his side kick Foutch.  They said there has been a heist of a weather station on the southeast side. It had been moved, and then it was dumped mysteriously on the campus of USC.
9:15AM Goetz and Foutch told me they had picked up the trail of the weather station the night before. They knew it had been bagged, and that some g-men were hopping mad about it. The g-men had written a report on the crime. In it, they claimed that because of the heist, which had been orchestrated by some other g-men at NOAA, the great City of Los Angeles had been denied it’s due: A new rainfall record year of 2004-2005. Worse than that, the temperature of the city was going down.
I’d heard about this station. It was ugly, it was dirty, it was perched on a rooftop, and it was on the wrong side of town, out by the City Department of Water and Power, just south of the Santa Ana freeway. It hung out with utility trucks and those little red street racers the punks around here drive. There was only one single photo of it. It wasn’t the kind of pristine weather station you’d take home to introduce to your mother.
10:05 AM I knew this was going to be a tough case to crack without hard as nails proof, so I decided to setup surveillance. I called in a favor from a chopper pilot named Barney that I used to share a beat with. I asked him to get aerial photos, lots of them. He asked why. I told him it was because nobody would believe that a City of Los Angeles official weather station had been on a rooftop of a parking garage and now was a shell of it’s former self sitting over at the USC campus.
I told him that when they dumped it in a cool park at USC, they killed the heart and soul of the city’s temperature record with it. And worse, they not only moved the station, but they replaced the man who had sweated and toiled on the rooftop in the hot LA smog and sun to get that weather data with one of those sissy robot contraptions. They call it an ASOS, and it has a sleek look about it, but it could never do a man’s job.
12:01 PM So Barney sets me up with the aerial surveillance from this morning. He sends the photos. I took them down to the lunch counter of the corner drugstore to develop them on my laptop. I had a cup of coffee while I did that. It cost 25 cents, and included pie.
The first aerial photo was a little fuzzy, it was hard to make out the station:

Click for a live link
But I found it, and marked it with an arrow. It wasn’t a pretty sight, right in the middle of acres of blacktop and automobiles. I kept reminding myself I’d seen worse, like in Tucson, and down the street from that Ace hardware store parking lot in Lampasas, Texas. But still, it ate at me.
12:15 PM I finished the pie, and asked for refill on the coffee. The waitress looked at the first photo and just shook her head. Barney had made several passes from several angles, and he snapped one good photo of it that hit me between the eyes like the butt end of a .38 special. There it was, our beloved City of Angels Weather Station. It made me sick just to look at it. What kind of people would do something like this?

Click for larger image
But that wasn’t all, Barney got a picture from another angle out of the archives, and it showed the station even closer looking east. It was even uglier than the other photo. Just thinking about the albedo of the parking lot in the hot LA summer made my skin crawl.

1:05 PM Barney said he had other photos, but he couldn’t get them to me now. So he put them in a file, on something called a web server. And gave me something called a link. He said any citizen of our fair city who wanted to see the terrible place where they put the City of Angels weather station could click the link and look at the photos from all angles. Good man that Barney.
The photos were good, but not good enough. I knew that these photos would eventually be seen by Judge Rabett. Rabett has told us before that pictures don’t matter in his court, so I knew this wouldn’t be enough. I had to prove the connection to the single photo taken by the g-men for their report.
2:15 PM I had figured out a way to show that the single picture taken by the g-men in their report matched the aerial photos Barney took. To do that, I used the photo lab. The guy there is named Gimp, he walks with a limp from an old command line of fire injury. But he does good work. With Gimp’s help I was able to match the camera angle of the single land photo taken by the g-men with one of the aerial photos:

Click for a larger image
3:03 PM I’d finished up the aerial surveillance work of the original scene of the crime, but I still had to get photos of the place where the body of the weather station had been dumped in the park. All I had to go on was the single photo of the park taken by the g-men for their report. It sure looked like a nice cool park and final resting place. It had a little wrought iron fence around it and reminded me of a cemetery – a cemetery where the weather goes to die. It looked good, too good. I had a hunch it wasn’t as good as it looked.
3:05 PM I called up Barney, and asked about the aerial photos where they dumped the weather station; he said he had it covered. He said to check the file he left on the webserver for the street address where the park was.
3:15 PM I was running out of time, I had to get this wrapped up today. The webserver was slow, some punks were using it for a joyride. But I finally managed to open the file. and get the street address. It was out on South Vermont Avenue.
3:30 PM The aerial photos of the campus of USC where they had dumped the weather station proved my hunch was right. The picture the g-men took made it look like a perfect little park-like setting but in reality, it was just another cruddy location surrounded by acres of concrete and asphalt. The place where they dumped the station was only a few yards from the street:

Click for a live link

Source: https://www.bing.com/maps?v=2&cp=pp3hv95484k5&style=o&lvl=2&tilt=-90&dir=0&alt=-1000&scene=6986505&encType=1
The little bit of grass and the fact that it was closer to the beach made it a little cooler. The tennis courts probably didn’t help either.
Barney also left links for the close up aerial surveillance photos he’d done. When I pulled up the one looking West, it hit me. I knew why they had dumped the weather station there. There was a parking garage just across the street. It must have felt like home.

click for a larger image

4:00 PM It was getting late, I had figured out where the original crime had occurred, and where they dumped the body of the weather station. Now all I had to do was find it’s data and I was ready to close this case.
4:15 PM I found the data in a webserver called GISTEMP. Somebody had already plotted it. Sure enough, there it was, the smoking gun. The temperature had dropped about 1.5°C when they pulled this caper in 1999. The continuity of the record had been ruined and there was now a big step function in the data that hadn’t been removed by the g-men at NCDC.
No wonder the g-men who wrote the original report were so hopping mad about it.
Since I couldn’t undo the plot, I called in Gimp again. With his help I was able to separate the time-line into red and blue segments to show where in the time-line the data had been taken from:

Click to see original graph.
5:00 PM Quitting time. I had wrapped up this investigation into the sordid story of crime against temperature in the City of Angels and gotten all the documentation together to present for the court of public opinion. I’m feeling good, I’ve served the public interest. Thats’ my job. I think I’m going to go blow another quarter on pie and coffee.
9:30 AM Tuesday Foutch reports that he’s located the entire history of the station, which can be viewed here:
http://mrcc.sws.uiuc.edu/FORTS/histories/CA_Los_Angeles_Conner.pdf
The story you have just seen is true; the names were changed to protect the incompetent.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea071e2ea',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Its all quiet on the solar front. Too quiet. It has now been almost 2 and a half months since the last counted cycle 24 sunspot has been seen on April 13th, 2008. There was a tiny cycle 24 “sunspeck” that appeared briefly on May 13th, but according to solar physicist Leif Svalgaard, that one never was assigned a number and did not “count”. It is just barely discernable on this large image from that day.

The sun today: spotless
NASA’s David Hathaway updated his solar cycle prediction page on June 4th. The start of cycle 24 keeps getting pushed forward while the ramp up line starts to look steeper into 2009.

Click for full sized image
The most recent forecast ( June 27th, 2008 ) from the Space Weather Prediction Center says little that would suggest our spotless streak would end any time soon:
Solar Activity Forecast: Solar activity is expected to be very
low. 
Analysis of Solar Active Regions and Activity from 26/2100Z
to 27/2100Z: Solar activity was very low. No flares occurred during
the past 24 hours and the solar disk remains spotless.

 So when will solar cycle 24 really get going? It seems even the best minds of science don’t know for certain. A NOAA press release issued last year in April 2007 calls for Cycle 24 to be up to a year late, but they can’t decide on the intensity of SC24. That argument is ongoing.
Meanwhile the NOAA SEC Solar Cycle Progression Page looks pretty flat in all metrics charted. 
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e4ce774',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

In the early 1990s, Rep. Dick Armey (RTX) proposed a flat tax. He would have junked the Internal Revenue Code and replaced it with a system designed to raise revenue in a much less destructive fashion. The core principles were to tax income at one low rate, to eliminate double taxation of saving and investment, and to wipe out the special preferences, credits, exemptions, deductions, and other loopholes that caused complexity, distortions, and corruption.



The flat tax never made it through Congress, but it’s been adopted by more than a dozen other countries since 1994.



It’s unfortunate that the United States is missing out on the tax reform revolution. Instead of the hundreds of forms demanded by the current tax system, the Armey flat tax would have required just two postcards. Households would have used the individual postcard to pay a 17 percent tax on wages, salary, and pensions, though a generous family‐​based allowance (more than $30,000 for a family of four) meant that there was no tax on the income needed to cover basic expenses.



Taxes on other types of income would have been calculated using the second postcard, which would have been filed by every business regardless of its size or structure. Simply stated, there would have been a 17 percent tax on net income, which would have been calculated by subtracting wages, input costs, and investment expenditures from total receipts.



While the simplicity and low tax rate were obvious selling points, the flat tax also eliminated various forms of double taxation, ending the bias against income that was saved and invested. In other words, the IRS got to tax income only one time. The double tax on dividends would have been completely eliminated. The death tax also was to be wiped out, as was the capital gains tax, and all saving would have received “Roth IRA” treatment.



Another key feature of the flat tax was the repeal of special tax breaks. With the exception of a family‐​based allowance, there would have been no tax preferences. Lawmakers no longer would have been able to swap loopholes for campaign cash. It also would have encouraged businesses to focus on creating value for shareholders and consumers instead of trying to manipulate the tax code. Last but not least, the flat tax would have created a “territorial” system, meaning that the IRS no longer would have been charged with taxing Americans on income earned—and subject to tax—in other jurisdictions.



Proponents correctly argued that a flat tax would improve America’s economic performance and boost competitiveness. And after Republicans first took control of Congress, it appeared that real tax reform was possible. At one point, the debate was about, not whether there should be tax reform, but whether the Internal Revenue Code should be replaced by a flat tax or a national sales tax (which shared the flat tax’s key principles of taxing economic activity only one time and at one low rate).



Notwithstanding this momentum in the mid‐​1990s, there ultimately was no serious legislative effort to reform the tax system. In part, that was because of White House opposition. The Clinton administration rejected reform, largely relying on class‐​warfare arguments that a flat tax would benefit the so‐​called rich. But President Clinton wasn’t the only obstacle. Congressional Democrats were almost universally hostile to tax reform, and a significant number of Republicans were reluctant to support a proposal that was opposed by well‐​connected interest groups.



 **The Flat Tax around the World**



One of the stumbling blocks to tax reform was the absence of “real‐​world” examples. When Armey first proposed his flat tax, the only recognized jurisdiction with a flat tax was Hong Kong. And even though Hong Kong enjoyed rapid economic growth, lawmakers seemed to think that the then–British colony was a special case and that it would be inappropriate to draw any conclusions from it about the desirability of a flat tax in the United States.



Today, much of the world seems to have learned the lessons that members of Congress didn’t. Beginning with Estonia in 1994, a growing number of nations have joined the flat tax club. There are now 17 jurisdictions that have some form of flat tax, and two more nations are about to join the club. As seen in Table 1, most of the new flat tax nations are former Soviet republics or former Soviet bloc nations, perhaps because people who suffered under communism are less susceptible to class‐​warfare rhetoric about “taxing the rich.”





**Flat Tax Lessons**



The flat tax revolution raises three important questions: Why is it happening? What does the future hold? Should American policymakers learn any lessons?



The answer to the first question is a combination of principled leadership, tax competition, and learning by example. Flat tax pioneers such as Mart Laar (prime minister of Estonia), Andrei Illarionov (chief economic adviser to the president in Russia), and Ivan Miklos (finance minister in Slovakia) were motivated at least in part by their understanding of good tax policy and their desire to implement pro‐​growth reforms. But tax competition also has been an important factor, particularly in the recent wave of flat tax reforms. In a global economy, lawmakers increasingly realize that it is important to lower tax rates and reduce discriminatory burdens on saving and investment. A better fiscal climate plays a key role both in luring jobs and capital from other nations and in reducing the incentive for domestic taxpayers to shift economic activity to other nations.



Moreover, politicians are influenced by real‐​world evidence. Nations that have adopted flat tax systems generally have experienced very positive outcomes. Economic growth increases, unemployment drops, and tax compliance improves. Nations such as Estonia and Slovakia are widely viewed as role models since both have engaged in dramatic reform and are reaping enormous economic benefits. Policymakers in other nations see those results and conclude that tax reform is a relatively risk‐​free proposition. That is especially important since international bureaucracies such as the International Monetary Fund usually try to discourage governments from lowering tax rates and adopting pro‐​growth reforms.



The answer to the second question is that more nations will probably join the flat tax club. Three nations currently are pursuing tax reform. Albania is on the verge of adopting a low‐​rate flat tax, as is East Timor (though the IMF predictably is pushing for a needlessly high tax rate). A 15 percent flat tax has been proposed in the Czech Republic, though the political outlook is unclear because the government does not have an absolute majority in parliament.



It is also worth noting that countries with flat taxes are now competing to lower their tax rates. Estonia’s rate already is down from 26 percent to 22 percent, and it will drop to 18 percent by 2011. The new prime minister’s party, meanwhile, wants the rate eventually to settle at 12 percent. Lithuania’s flat rate also has been reduced, falling from 33 percent to 27 percent, and is scheduled to fall to 24 percent next year. Macedonia’s rate is scheduled to drop to 10 percent next year, and Montenegro’s flat tax rate will fall to 9 percent in 2010—giving it the lowest flat tax rate in the world (though one could argue that places like the Cayman Islands and the Bahamas have flat taxes with rates of zero).



The continuing shift to flat tax systems and lower rates is rather amusing since an IMF study from last year claimed: “Looking forward, the question is not so much whether more countries will adopt a flat tax as whether those that have will move away from it.” In reality, there is every reason to think that more nations will adopt flat tax systems and that tax competition will play a key role in pushing tax rates even lower.



 **Could It Happen Here?**



For American taxpayers, the key question is whether politicians in Washington are paying attention to the global flat tax revolution and learning the appropriate lessons. There is no clear answer to this question. Policymakers certainly are aware that the flat tax is spreading around the world. Mart Laar, Andrei Illarionov, Ivan Miklos, and other international reformers have spoken several times to American audiences. President Bush has specifically praised the tax reforms in Estonia, Russia, and Slovakia. And groups like the Cato Institute are engaged in ongoing efforts to educate policymakers about the positive benefits of global tax reform.



But it is important also to be realistic about the lessons that can be learned. The United States already is a wealthy economy, so it is very unlikely that a flat tax would generate the stupendous annual growth rates enjoyed by nations such as Estonia and Slovakia. The United States also has a very high rate of tax compliance, so it would be unwise to expect a huge “Laffer Curve” effect of additional tax revenue similar to what nations like Russia experienced.



It is also important to explain to policymakers that not all flat tax systems are created equal. Indeed, none of the world’s flat tax systems is completely consistent with the pure model proposed by Professors Robert Hall and Alvin Rabushka in their book, _The Flat Tax_. Nations such as Russia and Lithuania, for instance, have substantial differences between the tax rates on personal and corporate income (even Hong Kong has a small gap). Serbia’s flat tax applies only to labor income, making it a very tenuous member of the flat tax club. Although information for some nations is incomplete, it appears that all flat tax nations have at least some double taxation of income that is saved and invested (though Estonia, Slovakia, and Hong Kong get pretty close to an ideal system). Moreover, it does not appear that any nation other than Estonia permits immediate expensing of business investment expenditures. (The corporate income tax in Estonia has been abolished, for all intents and purposes, since businesses only have to pay withholding tax on dividend payments.)



Policymakers also should realize that a flat tax is not a silver bullet capable of solving all of a nation’s problems. From a fiscal policy perspective, for instance, the Russian flat tax has been successful. But Russia still has many problems, including a lack of secure property rights and excessive government intervention. Iraq is another example. The U.S. government imposed a flat tax there in 2004, but even the best tax code is unlikely to have much effect in a nation suffering from instability and violence.



With all these caveats, the flat tax revolution nonetheless has bolstered the case for better tax policy, both in America and elsewhere in the world. In particular, there is now more support for lower rates instead of higher rates because of evidence that marginal tax rates have an impact on productive behavior and tax compliance. Among developed nations, the top personal income tax rate is 25 percentage points lower today than it was in 1980. Similarly, the average corporate tax rate in developed nations has dropped by 20 percentage points during the same period. Those reforms are not consequences of the flat tax revolution. Margaret Thatcher and Ronald Reagan started the move toward less punitive tax rates more than 25 years ago. But the flat tax revolution has helped cement those gains and is encouraging additional rate reductions.



Moreover, there is now increased appreciation for reducing the tax bias against income that is saved and invested. Indeed, Sweden and Australia have abolished death taxes, and Denmark and the Netherlands have eliminated wealth taxes. Other nations are lowering taxes on capital income, much as the United States has reduced the double taxation of dividends and capital gains to 15 percent. And although the United States is a clear laggard in the move toward simpler and more neutral tax regimes, the flat tax revolution is helping to teach lawmakers about the benefits of a system that does not penalize or subsidize various behaviors.



The flat tax revolution also suggests that the politics of class warfare is waning. For much of the 20th century, policymakers subscribed to the notion that the tax code should be used to penalize those who contribute most to economic growth. Raising revenue was also a factor, to be sure, but many politicians seem to have been more motivated by the ideological impulse that rich people should be penalized with higher tax rates. If nothing else, the growing community of flat tax nations shows that class‐​warfare objections can be overcome.



 **Building a High‐​Tax Cartel**



Although the flat tax revolution has been impressive, there are still significant hurdles. Most important, international bureaucracies are obstacles to tax reform, both because they are ideologically opposed to the flat tax and because they represent the interests of high‐​tax nations that want tax harmonization rather than tax competition. The Organization for Economic Cooperation and Development, for instance, has a “harmful tax competition” project that seeks to hinder the flow of labor and capital from high‐​tax nations to low‐​tax jurisdictions. The OECD even produced a 1998 report stating that tax competition “may hamper the application of progressive tax rates and the achievement of redistributive goals.” In 2000 the Paris‐​based bureaucracy created a blacklist of low‐​tax jurisdictions, threatening them with financial protectionism if they did not change their domestic laws to discourage capital from nations with oppressive tax regimes.



The OECD has been strongly criticized for seeking to undermine fiscal sovereignty, but its efforts also should be seen as a direct attack on tax reform. Two of the key principles of the flat tax are eliminating double taxation and eliminating territorial taxation. These principles, however, are directly contrary to the OECD’s anti‐​tax competition project—which is primarily focused on enabling high‐​tax nations to track (and tax) flight capital. That necessarily means that the OECD wants countries to double tax income that is saved and invested, and to impose that bad policy on an extraterritorial basis.



The OECD is not alone in the fight. The European Commission also has a number of anti‐​tax‐​competition schemes. The United Nations, too, is involved and even has a proposal for an International Tax Organization. All of those international bureaucracies are asserting the right to dictate “best practices” that would limit the types of tax policy a jurisdiction could adopt. Unfortunately, their definition of best practices is based on what makes life easier for politicians rather than what promotes prosperity.



Fortunately, these efforts to create a global tax cartel have largely been thwarted, and an “OPEC for politicians” is still just a gleam in the eyes of French and German politicians. That means that tax competition is still flourishing, and that means that the flat tax club is likely to get larger rather than smaller.



 _This article originally appeared in the July/​August 2007 edition of_Cato Policy Report.



<em><a href=”/people/daniel-mitchell”>Daniel J. Mitchell</a> is a senior fellow at the Cato Institute.</em>
"
"
Share this...FacebookTwitterBy Kirye
and Pierre Gosselin
Today we look at October mean temperatures for the emerald island country of Ireland, the Scandinavian country of Sweden and Finland.
Global warming alarmists claim that the globe is warming, which intuitively would tell us summers should be getting longer, which in turn would mean the start of fall is getting pushed back. In such a case, September and October temperatures should be warming, but they are not!
Cooling Ireland
First we plot the mean temperature for 7 stations in Ireland for the month of October, for which the Japan Meteorological Agency (JMA) has sufficient data going back 25 years:

Data source: JMA
Seven of 7 stations in Ireland have seen a strong cooling trend for October since 1995.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Nothing unusual happening in Greta’s Sweden
Next we look at the October trends for 6 stations in climate activist Greta Thunberg’s home country of Sweden.

Data Source: JMA. 
Three of 6 stations in Sweden show no warming trend for October. Greta can begin to calm down and stop worrying herself to death about climate doom. As is the case for all the charts shown here, we plot the stations for which the Japan Meteorological Agency (JMA) has sufficient data going back 25 years
Stable climate in Finland
Finally we look at Sweden’s Nordic neighbor of Finland. Here as well there’s little indication of a widespread warming.
Data source: JMA.
Three of 6 stations in Finland in fact showed a modest cooling trend for October over the past quarter century. There’s nothing to be alarmed about.


		jQuery(document).ready(function(){
			jQuery('#dd_8dedc33634186ee6cf8964dc52c4f36c').on('change', function() {
			  jQuery('#amount_8dedc33634186ee6cf8964dc52c4f36c').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Dozens of activists have coated themselves in plaster and are trying to occupy the British Museum overnight in a bid to pressure the institution to cut ties with oil corporation BP. About 60 protesters were taking part in the defiant act of impromptu sculpture making as the museum in London attempted to close its doors at 5pm on Saturday. The action, entitled Monument, is the first of its kind to be conducted by the theatrical protest group BP or not BP? “We are imagining a world in which the British Museum has stopped celebrating those causing the climate crisis and is instead allying itself with those who have, currently are, and will be putting their bodies on the line in the fight for climate justice,” said a plaque describing the protest. The civil disobedience campaign began on Friday night when protesters dressed as ancient Greek warriors smuggled a 13-foot Trojan horse into the museum’s foyer. The protest was in response to an exhibition, Troy: Myth and Reality, which is described as “supported by BP”. Two guards slept inside its belly overnight, despite a downpour at around 4am, to prevent it being removed. The group estimated that as many as 1,500 supporters took part in the action on Saturday. Protesters occupied 11 of the museum’s rooms, which became host to spoken word performances, singalongs and talks by campaigners from West Papua and Senegal. Origami swans were scattered throughout the museum adorned with messages including: “Choose sponsors who care about our future”. Protesters finished the day by tearing apart paper versions of BP’s logo near the museum’s main entrance. “We feel that the museum’s attitudes towards climate change and colonialism are not what they should be in the 21st century,” said the group’s Jess Worth. Suzanne Savage, from Malvern, Worcestershire, was among a gathering of fee-paying British Museum members who unfurled a banner emblazoned with the words “BP must fall”. She said: “I have been a longstanding member of the British Museum. I very much support the art but we do not want this noble institution’s reputation being sullied by sponsorship from a dirty oil company, which is one of the biggest polluters in the world.” Hartwig Fischer, the director of the British Museum, said: “The museum is a public space where people can come to debate and we respect other people’s right to express their views. “We share the concerns for the challenges that we all face together as a result of climate change. We address these issues in an innovative way through significant exhibitions and public programming. “The British Museum offers for millions of people an extraordinary opportunity to engage with the cultures and histories of humankind. Without external support and sponsorship this would not be possible. “Removing this opportunity from the public is not a contribution to solving the climate crisis.” The protest is the latest move in activists’ campaign to end fossil fuel sponsorship of the UK’s leading cultural venues. In October, the Royal Shakespeare Company ditched its sponsorship deal with BP, after a campaign from artists, environmentalists and members of the public."
"Few people are familiar with the pangolin. It is a shy creature, about as big as a medium-sized dog, and its diet consists of ants and termites. Most distinctively, it has armoured plates and will curl into a ball to resist predators. The eight different species of pangolin live across most of Africa, India, southern China and Southeast Asia. Despite their recognition as an endangered species and the supposed protection that brings, pangolin are now the most trafficked wild mammal species in the world. Their scales have been used for millennia in Asian medicine. When roasted, they  are alleged to detoxify and drain pus, relieve palsy, and stimulate lactation. The animals are either traded alive (involving appalling cruelty) or killed and their scales removed to meet culinary and medicinal demand in East and Southeast Asia. In parts of Africa, scales are also used as a kind of traditional medicine known as “muti”, and African pangolins are also being increasingly exploited to satisfy growing demands from Asia. In July 2014 customs officials in Vietnam seized an astonishing 1.4 tonnes of dried pangolin scales from a cargo ship arriving from Sierra Leone. This was just one instance among many. Over the past decade thousands of pangolins and dozens of tonnes of scales have been seized in China and Vietnam. And what gets confiscated is but a fraction of what slips through unnoticed.  Each pangolin yields only about half a kilo of scales; the arithmetic of pending extinction is thus simple for a species that bears only one offspring per year. Logbooks apprehended in 2009 from one trafficking syndicate in the Borneo revealed 22,000 pangolins were killed over a 21 month period. This illegal trade continues unabated; online reports expose an extensive international trade network, with seven countries involved in 15 pangolin trafficking incidents investigated between August and October 2013. This multi-nationalism makes enforcement extremely challenging, with huge borders and vast coastlines to monitor across Asia, necessitating a greater exchange of information and intelligence between national enforcement agencies. International wildlife protection is simply not keeping pace with the resolve and devious methods of illegal traders, such as posting parcels of pangolin scales by mail (last year Beijing Customs uncovered more than one tonne of posted scales). Furthermore, illegal traders exploit corruption at border controls to facilitate the illegal supply chain. This is despite global legislation and national laws (notably in China and Vietnam), which carry severe punitive sentences, aimed at stemming cross-border trade. The illegal trade in fauna and flora (excluding fisheries and timber) is big business, worth somewhere between US$7 and US$23 billion each year according to the UN and Interpol. Of this total, however, the sale of valuable commodities such as ivory, rhino horn and tiger accounted for only around US$75m in 2010, despite  getting the most attention in Asia. Of course ivory poaching is awful, but it is important to recognise the sheer numbers involved when we talk about trade in other species. That 1.4 tonne shipment of pangolin scales represented around 3000 dead animals. Cutting out the supply is one thing, but reducing consumer demand is also essential. Lamentably, even the government-backed Chinese Pharmacopoeia Commission supports the medical value of pangolin scale. Ordinary citizens, often sick or elderly, searching for a traditional remedy are thus mislead and become unwitting participants in this species’ demise.  While other wildlife products such as ivory and rhino horn are prohibitively expensive for most Chinese consumers, and symbolise elite status within society, pangolin is more affordable and easily attainable. Its price has been increasingly recently, yet it is still a price consumers seem willing to pay. Clearly, people in China and across Southeast Asia aren’t going to be priced out of the pangolin market any time soon so it is imperative modern medicine is adopted and that we achieve a step-change in  public understanding of illegal trade and conservation. The exact extent to which this illegal exploitation affects wild pangolin populations remains largely unknown – although it would seem totally unsustainable. Further ecological research is crucial to establish how many pangolins are out there in the wild and how best to protect them. The Scaling up pangolin conservation action plan published by the International Union for Conservation of Nature requires urgent implementation, which means reducing consumer demand, strengthening protections in wild pangolin strongholds, help for communities to move away from poaching, and stronger, better enforced laws prohibiting the pangolin trade."
"

La Scala to stage Gore’s ‘Inconvenient Truth’
MILAN, Italy (AP) — First it was the film and the book. Now the next stop for Al Gore’s “An Inconvenient Truth” is opera.
La Scala officials say the Italian composer Giorgio Battistelli has been commissioned to produce an opera on the international multiformat hit for the 2011 season at the Milan opera house. The composer is currently artistic director of the Arena in Verona.
Bring your marshmallows.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f207d1f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe pitfalls of planned economies…

Power shortages, power surpluses, seas of milk and mountains of butter
By Reinhard Storz
(Translated/ edited by P. Gosselin)
Some of the older generation will still remember the time when the press, radio and television talked about a sea of milk and a mountain of butter. Politicians had meant well and, in order to aid the farmers, decided on adequate prices for milk. This planned economy led to an ever increasing quantity of milk in West Germany. All the talk was about a sea of milk. There were numerous ideas on what to do with the surplus milk. One should give milk to the school children for school breaks etc. Surplus milk was processed into milk powder and butter. The resulting butter was stored in cold stores until there was no room left. Finally, a part of the butter that could not be sold in Germany was sold to the USSR for a fraction of the market price, and another part was ultimately given away to Chile.
Politicians learned back then that this was the wrong way, and so introduced a milk quota more than 30 years ago to promote agriculture. As a result the sea of milk evaporated and the mountain of butter disappeared. Today, due to the planned economy with solar and wind power, we have similar conditions as we did with milk and butter. When the wind blows strongly and the sun shines, coal, gas and nuclear power plants are throttled down. Nevertheless, we still have a surplus of green electricity at times. This cannot be accommodated by consumers in Germany. To get this problem under control one has two options.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




1. You search for customers in neighboring countries. That frequently succeeds. However, they are usually not willing to pay for the surplus electricity. The power is thus sometimes given away, often money has to be paid on top so that the power gets accepted.
2. Wind turbines are turned off if the grid gets overloaded. The operators of the wind parks, however, are still reimbursed for the price of the unproduced electricity. These costs are also passed on to us, the electricity consumers.
But there are also times when the wind does not blow and the sun does not shine. There you could very well use the electricity that was previously available in abundance. A program for even more wind turbines or solar roofs will not help. A tenfold amount of solar surfaces does not supply electricity at night and a tenfold amount of wind turbines has no use during calm periods. In nature, fluctuations are nothing unusual. We remember the story of the Pharaoh and the 7 fat people and the lean years. Surplus food was already stored for bad times thousands of years ago.
To even out the fluctuations in solar and wind power, electricity storage is therefore urgently needed. Politicians to plan it out in such a way that additional wind turbines and solar areas are only approved if they are equipped with appropriate storage capacity. The goal of the Energiewende (transition to green energies) is to replace electricity from coal-fired power plants with electricity from the sun and wind. But it must then also be available around the clock. This is not possible with the sporadically available electricity from wind turbines and solar systems.  This electricity, disparagingly referred to by some people as inferior, fidgety electricity, must be made permanently available by means of electricity storage systems. Only in this way can a transition to green energies succeed.
Therefore, demonstrations should first take place to demand the introduction of sufficient electricity storage. Until this is achieved, there will remain a need for coal-fired power plants. Anyone who wants to abolish coal-fired electricity without a secure and affordable alternative supply, is just sawing off the branch we are all sitting on. But we should not give up hope. Like the politicians who finally got the mountain of butter mountain and the sea of milk under control with the milk quota, they will also bring the necessary power storage facilities on the way as soon as possible.
Pumped storage facilities for such amounts of electricity are likely to be ruled out due to technical and economic reasons. The same is true with regards to compressed air storage, which has a lower efficiency than pumped storage plants. Battery storage is unaffordable at today’s costs. The only option is to produce hydrogen as an energy storage from surplus electricity.


		jQuery(document).ready(function(){
			jQuery('#dd_a4404f563bedf5c83941b0e5d9152640').on('change', function() {
			  jQuery('#amount_a4404f563bedf5c83941b0e5d9152640').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

In September 2005, the Danish newspaper _Jyllands‐​Posten_ printed a dozen cartoons — prompted by recent examples of self‐​censorship by the European media — related to Islam, one of which depicted the Muslim prophet Muhammad with a bomb wrapped in his turban. Their publication quickly spiraled into a violent international uproar, as Muslims around the world erupted in protest and the paper’s culture editor was branded by some as “the Danish Satan.” In _The Tyranny of Silence_ , a book published by the Cato Institute in November, Flemming Rose grapples with the difficult issues surrounding his decision to run those cartoons. At Cato’s 27th Annual Benefactor Summit in Naples, Florida, Rose spoke about the lessons he learned in the process of reconciling the tension between respect for cultural diversity and the protection of democratic freedom.



When I was writing my book back in 2009, I interviewed Salman Rushdie and he said something very important to me. I’d been having difficulty coming to terms with the fact that others were telling my story without, I felt, knowing who I was. Rushdie observed that from childhood, we use storytelling as a way of defining and understanding ourselves. It’s a phenomenon that derives from a language instinct that is universal and innate in human nature. It’s in fact one of the things that makes us different from other creatures. Any attempt to restrict that impulse and put limits on speech therefore isn’t just a violation of our political rights. It’s an act of violence against human nature, an existential assault that turns people into something they are not. What differentiates open and closed societies is the right to tell and retell our own and other people’s stories. In a democracy, no one can claim that exclusive right, be it an oppressive state or a minority.



Rushdie told me that the conflict over the right to tell a certain story was at the center of his own controversy. He said: “This goes back to the question of what sort of society we want. If you wish to live in an open society, it follows that people will talk about things in different ways, and some of them will cause offense and anger. From the moment you begin to talk about limiting and controlling certain expressions, you step into a world where freedom no longer reigns, and from that moment on, you are only discussing what level of un‐​freedom you want to accept. You have already accepted the principle of not being free.”



Rushdie’s words came just at the right time for me. They opened my eyes and helped me define my own project. Even though the Muhammad cartoons were conceived in a Danish and European context, the debate is global. It touches on issues fundamental to any kind of society: freedom of speech and of religion, tolerance and intolerance, immigration and integration, Islam and Europe, majorities and minorities, and globalization, to name but a few. And what I realized is that we are all entitled to tell whatever story we wish.



That insight is very fundamental. It goes to the heart of the relationship between the person who speaks and those who hear — between individuals and communities — and to what extent individuals, groups, and institutions have a right to determine speech limits. In the U.S. constitutional system, there is more focus on the speaker, on the individual. You have the right to autonomy. In much of the rest of the world, including the European Union, it’s the other way around. The community — those on the receiving end — have broad powers to determine what an individual is allowed to say.



And this difference in approach has had farreaching consequences for the concept of tolerance. Originally, tolerance implied one’s ability to bear what one couldn’t stand. Freedom of speech meant freedom for the speech we hate — that’s how Justice Oliver Wendell Holmes put it. But because the receiver of speech has so much discretion in many parts of the world, tolerance has been turned on its head. That’s a very dangerous development.



There are two factors that are driving the challenges of free speech in a globalized world. The first is migration: the fact that people are moving across borders in numbers never before seen in human history Every society is getting more and more diverse in terms of culture, ethnicity, and religion, which means that it’s a lot easier to get offended by what people around you say because we are increasingly exposed to different ways of living. How do we negotiate the right to freedom of expression and freedom of speech in this increasingly multicultural world?



The other factor that is driving this process is the digitization of communication technologies. Now when something is being published in one place, it is immediately published everywhere. And when information travels, context is lost. This creates a huge space for misunderstanding, not to mention outright manipulation. That is something I experienced personally during the cartoon crisis.



But migration and digitization also means that all of us are being impacted by what’s going on outside our own country. You have competing approaches to free speech that are beginning to clash. The disappearance of borders and the spread of technology means that there is a need for universal standards no matter where you live. To a certain extent this goes on within the United Nations. But often things seem to be moving in the opposite direction.



More and more countries are passing laws that fragment and undermine any universal standard, a point stressed by Miklós Haraszti, the former representative on freedom of the media for the Organization for Security and Co‐​operation in Europe. Haraszti writes that “the very notion of an international standard for limits on free speech become obsolete if the fragmentation into separate content‐​oriented, historically based, culturally defined, politically shaped, country‐​specific approaches to speech restriction becomes accepted.”



In other words, no international advocacy for free speech is possible without a shared assumption that only incitement of actual crimes should be illegal. Otherwise offensive speech should be countered by speech, not courts. Unfortunately, this fragmentation of the international standard, to a certain extent, started in Europe with the passing of Holocaust denial laws. And one of the big surprises I experienced writing my book was to find out that the vast majority of these laws were in fact passed after the fall of the Berlin Wall. This indicated to me that they were not passed right after the Holocaust to prevent incitement to violence, but for other reasons. It’s important to note that the horror of the Holocaust serves as the founding narrative legitimizing European integration, and it is the key motivation for hate‐​speech laws on the continent.



The Council of Europe Commissioner for Human Rights has called for all 47 member states to pass laws against Holocaust denial, based on a widely accepted interpretation of what led to the Holocaust. It says that anti‐ Semitic hate speech was the decisive trigger — that evil words beget evil deeds — and that if only the Weimar government had clamped down on verbal persecution of the Jews in the years prior to Hitler’s rise to power, then the Holocaust may never have happened.



In my research, I looked into what actually happened in the Weimar Republic and found that, contrary to what most people think, Germany did have hate‐​speech laws that were applied quite frequently. The assertion that Nazi propaganda played a significant role in mobilizing anti‐​Jewish sentiment is irrefutable. But to claim that the Holocaust could have been prevented if only anti‐​Semitic speech had been banned has little basis in reality. Leading Nazis, including Joseph Goebbels, Theodor Fritsch, and Julius Streicher, were all prosecuted for anti‐​Semitic speech. And rather than deterring them, the many court cases served as effective pubicrelations machinery for the Nazis, affording them a level of attention that they never would have received in a climate of a free and open debate.



In the decade from 1923 to 1933, the Nazi propaganda magazine _Der Stürmer_ — of which Streicher was the executive publisher — was confiscated or had its editors taken to court no fewer than 36 times. The more charges Streicher faced, the more the admiration of his supporters grew. In fact, the courts became an important platform for Streicher’s campaign against the Jews.



Alan Borovoy, general counsel of the Canadian Civil Liberties Foundation, points out that cases were regularly brought against individuals on account of anti‐​Semitic speech in the years leading up to Hitler’s takeover of power in 1933. “Remarkably, pre‐​Hitler Germany had laws very much like the Canadian anti‐​hate law,” he writes. “Moreover, those laws were enforced with some vigour. During the 15 years before Hitler came to power, there were more than 200 prosecutions based on anti‐​Semitic speech… As subsequent history so painfully testifies, this type of legislation proved ineffectual on the one occasion when there was a real argument for it.”



The same can be said about Yugoslavia. Before the 1990s, Yugoslavia had rather tough laws criminalizing incitement to national, racial, or religious hate. In fact, people were being put in jail for telling an ethic joke. Obviously, these laws did little to help prevent the ethnic violence that we saw in the wars following the disintegration of Yugoslavia. Nevertheless, the dominant view in Europe is that too much freedom of expression will destroy the peace. In that sense, the EU is driven by a vision of what I call a benign utopia, one that aims to eliminate hate and create an insult‐​free public space. This became particularly evident in 2012, when it was awarded the Nobel Peace Prize. In receiving the prize, the leaders of the EU made no reference to the close relationship between freedom and peace. Instead they focused on the EU’s efforts to avoid division and create a continent without conflict.



I believe that Europe would do itself a great service if the narrative about the Holocaust was integrated into a broader anti‐​totalitarian framework. Hate speech wasn’t the trigger for mass murder during World War II. It was the clash between two totalitarian powers in the center of Europe — the Nazi regime and the Soviet regime — that was the primary cause. And if that’s the case, it means that the destruction of Jews in Europe was closely connected to the destruction of freedom. Moving forward, it would mean that the struggle against evil doesn’t require less freedom, but in fact, quite the contrary.



It seems there are two available responses to threats against free speech. One option is, basically, “If you accept my taboos, I’ll accept yours.” If one group wants protection against insult, then all groups should be so protected. If denying the Holocaust or the crimes of communism is against the law, then publishing cartoons depicting the Muslim prophet should also be forbidden. But that option can quickly spiral out of control: before we know it, hardly anything may be said.



The second option is to say that, in a democracy, there is no “right not to be offended.” Since we are all different, the challenge is then to formulate minimum constraints on freedom of speech that will allow us to coexist in peace. A society comprising many different cultures should have greater freedom of expression than a society that is significantly more homogeneous.



That premise seems obvious to me, yet the opposite conviction is widely held, and that is where the tyranny of silence lurks. At present, the tendency in Europe is to deal with increasing diversity by constraining freedom of speech, whereas the United States maintains a long tradition of leading off in the other direction. And it appears that the United States will increasingly stand alone with its tradition of upholding near‐​absolute freedom of expression.



My personal view is that the Americans are right. Freedom and tolerance are, to me, two sides of the same coin, and both are under pressure. As noted earlier, the world is undergoing rapid change. Taking offense has never been easier, or indeed more popular: many have developed sensitivity so exquisite that it has become excessive.



It almost tempts one to ask Europe’s welfare states to spend some money, not on “sensitivity training” — learning what not to say — but on insensitivity training: learning how to tolerate. For if freedom and tolerance are to have a chance of surviving in the new world, we all need to develop thicker skin.
"
"A British “poo bus” went into service last week, powered by biomethane energy derived from human waste at a sewage plant.  For those of us who follow these matters – and my academic works include Geographies of Shit: Spatial and temporal variations in attitudes towards human waste – this was an exciting moment, a rare piece of good PR for human waste. After all, most societies strongly associate it with a sense of disgust. Poo threatens the health of around 2.5 billion people … and it smells bad.  Yet it also represents an important resource, used in lots of different ways throughout history. Though the “poo bus” has captured the imagination there are many other uses for human waste. Urine is particularly versatile. In Medieval Europe, it was widely used to clean clothes while the Romans used it for tanning leather and cleaning wool.  It also makes an excellent agricultural fertiliser. Before the 19th century realisation that human waste was a health risk, sewage was routinely transported from British towns to villages for use as manure.  However, most of the health risks can be eliminated if urine (harmless if unpleasant) and feces (full of diseases) are separated at source through some form of urine diversion toilet. Such strategies make sound environmental and economic sense given the urine produced annually by each adult contains enough plant nutrients to grow 250kg of grain, enough to feed them for a year. China has a long history of using such toilets to collect urine for use as a fertiliser. In some regions of Sweden these toilets are now mandatory, improving environmental quality as well as creating significant savings on fertiliser costs for farmers.  Although harvesting biogas from human waste is not a new concept (Assyrians were using it to warm their bath water back in the 10th century BC), the potential to simultaneously manage waste and generate power has attracted increasing attention in recent decades. Modern waste treatment leaves behind sewage sludge that has traditionally been difficult to dispose of. However when the sludge is fed into a large vat, essentially like a stomach, and left to digest (an anaerobic digestion plant) it can produce valuable biogas and nutrient-rich digestate.  Biogas can be used directly as a fuel, cleaned up to create bio-methane or fed through a combined heat and power unit to generate electricity. The digestate can be used as a fertiliser or soil conditioner, helping in the process to reduce methane emissions, enhance plant growth and sequester carbon through photosynthesis. In rural China especially, low-tech biogas sanitation systems play an important role in killing pathogens while providing clean cooking fuel and fertiliser from the digestate.  Sweden and Germany are particularly big anaerobic digestion users. In Germany, sewage plants can sell their excess energy back to the national grid. Attractive tariffs designed to promote renewable energy have even meant many plants have started to “feed” their anaerobic digestion units with purpose-grown energy.  Though the UK’s biogas industry lags behind that of countries like Sweden and Germany, some sewage works are already releasing biogas into the national grid. With each adult producing around 30kg of dried sewage each year, there is lots of growth potential. If all of the UK’s sewage plants adopted this technology, around 350,000 homes could be supplied with gas derived from human waste. The environmental benefits of poo-powered travel are clear: bio-methane produces 95% less CO2 and 80% less nitrous oxide than diesel as well as having no particulate emissions. In the UK, there is enough bio-methane to fuel half the country’s large trucks.  Four years ago engineers developed a VW Beetle fuelled by bio-methane gas generated at the Avonmouth sewage plant near Bristol. This same sewage plant is now powering the “poo bus” and it could do even more. Avonmouth produces around 17m cubic meters of bio-methane each year which, if exported to the grid, could meet the gas needs of 8,300 homes.  But Sweden, again, is a leader here. Their transport policy has prioritised the development of bio-methane for trucks and buses; an initiative that has helped to clean up the air and meet renewable energy targets.  At a smaller and more experimental scale, meanwhile, researchers at the Bristol Robotics Laboratory have succeeded in charging a mobile phone using electricity generated from urine. Using a microbial fuel stack, they have succeeded in taking advantage of the metabolism of live micro-organisms to create electricity from convert organic matter – in this case urine.  Other research teams working on similar “pee conversion” technologies have succeeded in generating electricity, clean water and hydrogen from human waste.  If such technologies can be made to work on a bigger scale, the future for renewable power looks not only bright … but yellow."
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
_\---_   
  
Methane is all the rage. Why? Because 1) it is a powerful greenhouse gas, that molecule for molecule, is some 25 times as potent as carbon dioxide (when it comes to warming the lower atmosphere), 2) it plays a feature role in a climate scare story in which climate change warms the Arctic, releasing methane stored there in the (once) frozen ground, which leads to more warming and more methane release, _ad apocalypse_ , and 3) methane emissions are also be linked to fossil fuel extraction (especially fracking operations). An alarmist trifecta!   
  
Turns out, though, that these favored horses aren’t running as advertised.



While methane is a more powerful greenhouse gas in our atmosphere than carbon dioxide, its lifetime there is much shorter, even as the UN’s Intergovernmental Panel on Climate Change can’t quite say how long the CO2 residence time actually is. This means that it is harder to build-up methane in the atmosphere and that methane releases are more a short-term issue than a long-term one. If the methane releases are addressed, their climate influence is quickly reduced.   
  
This is why methane emissions from fracking operations—mainly through leaks in the wells or in the natural gas delivery systems—really aren’t that big of a deal. If they can be identified, they can be fixed and the climate impact ends. Further, identifying such leaks are in the fracking industry’s best interest, because, in many cases, they represent lost profits. And while the industry says it has good control of the situation, the EPA isn’t so sure and has proposed regulations aimed at reducing methane emissions from new and existing fossil fuel enterprises. The recent scientific literature is somewhat split on who is right. A major paper recently published in _Science_ magazine seemed to finger Asian agriculture as the primary suspect for recent increases in global methane emissions, while a couple of other recent studies seemed to suggest U.S. fracking operations as the cause (we reviewed those findings here).   
  
And as to the runaway positive feedback loop in the Arctic, a new paper basically scratches that pony.   
  
A research team led by University of Colorado’s Colm Sweeney set out to investigate the strength of the positive feedback between methane releases from Arctic soil and temperature (as permafrost thaws, it releases methane). To do this, they examined data on methane concentrations collected from a sampling station in Barrow, Alaska over the period 1986 through 2014. In addition to methane concentration, the dataset also included temperature and wind measurements. They found that when the wind was blowing in from over the ocean, the methane concentration of the air is relatively low, but when the wind blew from the land, methane concentration rose--at least during the summer/fall months, when the ground is free from snow and temperature is above freezing. When the researchers plotted the methane concentration (from winds blowing over land) with daily temperatures, they found a strong relationship. For every 1°C of temperature increase, the methane concentration increased by 5 ± 3.6 ppb (parts per billion)—indicating that higher daily temperatures promoted more soil methane release. However (and here is where things get real interesting), when the researchers plotted the change in methane concentration over the entire 29-yr period of record, despite an overall temperature increase in Barrow of 3.5°C, the average methane concentration increased by only about 4 ppm—yielding a statistically insignificant change of 1.1 ± 1.8 ppm/°C. The Sweeney and colleagues wrote:   




The small temperature response suggests that there are other processes at play in regulating the long-term [methane] emissions in the North Slope besides those observed in the short term.



As for what this means for the methane/temperature feedback loop during a warming climate, the authors summarize [references omitted]:   




The short- and long-term surface air temperature sensitivity based on the 29 years of observed enhancements of CH4 [methane] in air masses coming from the North Slope provides an important basis for estimating the CH4 emission response to changing air temperatures in Arctic tundra. By 2080, autumn (and winter) temperatures in the Arctic are expected to change by an additional 3 to 6°C. Based on the long-term temperature sensitivity estimate made in this study, increases in the average enhancements on the North Slope will be only between -2 and 17 ppb (3 to 6°C x 1.1 ± 1.8 ppb of CH4/°C). Based on the short-term relationship calculated, the enhancements may be as large as 30 ppb. These two estimates translate to a -3 – 45% change in the mean (~65 ppb) CH4 enhancement observed at [Barrow] from July through December. Applying this enhancement to an Arctic-wide natural emissions rate estimate of 19 Tg/yr estimated during the 1990s and implies that tundra-based emissions might increase to as much as 28 Tg/yr by 2080. This amount represents a small increase (1.5%) relative to the global CH4 emissions of 553 Tg/yr that have been estimated based on atmospheric inversions.



In other words, even if the poorly understood long-term processes aren’t sustained, the short term methane/temperature relationship itself doesn’t lead to climate catastrophe.   
  
The favorite thoroughbreds of the methane scare are proving to be little more than a bunch of claimers.   
  
  
  
**Reference:**   
  
Sweeney, C., et al., 2016. No significant increase in long-term CH4 emissions on North Slope of Alaska despite significant increase in air temperature. _Geophysical Research Letters_ , doi: 10.1002/GRL.54541.   
  
__


"
"
Share this...FacebookTwitterA new international agreement to replace the Kyoto environmental protocol will not be signed in 2010, the Russian presidential advisor on climate change has said. Read more:
http://en.rian.ru/Environment/20100426/158743966.html
On April 16, Russian President Medvedev said:
All countries, including developed and developing economies, should reach an agreement, or, if we do not agree on this [the common terms of carbon emissions reduction], Russia will not prolong its participation in the Kyoto agreement – you cannot have it both ways.
http://en.beta.rian.ru/Environment/20100416/158607110.html
Share this...FacebookTwitter "
"
Share this...FacebookTwitterMany of us have made complaints about how the MSM is biased and reluctant to cover topics like Climategate and other scandals. Well take heart! People are waking up to the fact that they are being denied information and facts, and as a result they’re turning their backs on the main stream print media. Sure a part of it has to do with bad economic times (can’t blame that on Bush anymore), but another factor is that the internet offers an alternative. Here are the gory numbers for newspaper circulation in the US. Only the conservative Wall Street Journal has made gains. The rest are bleeding massively.
The Wall Street Journal 2,092,523 +0.5%
USA Today 1,826,622 -13.58%
New York Times 951,063 -8.47%
Los Angeles Times 616,606 -14.74%
Washington Post 578,482 -13.06%
Dallas Morning News 260,659 -21.47%
San Francisco Chronicle 241,330 -22.68%
The Star-Ledger, Newark, N.J. 236,017 -17.79%
Read the complete story here: http://www.editorandpublisher.com/eandp/news/article_display.jsp?vnu_content_id=1004086334
I’ve always encouraged tree-hugging subscribers to heed the advice from newspapers to cut CO2 emissions. And what better way to begin than to scale back the energy-guzzling industry of newspaper production, distribution and newspaper disposal? This can be done by cancelling subscriptions. This also rescues a lot of trees. So hats off to the tree-huggers out there who choose to save a tree by ending a subscription.
Share this...FacebookTwitter "
nan
"

The _Washington Post_ has an article today on the battle over the Keystone XL pipeline. There is a sense of urgency on both sides as the decision on the project is expected to be fast approaching.   
  
  
The _Post_ features arguments from pipeline proponents that the project will provide an economic boost to the state of Nebraska, and from pipeline opponents that the oil carried though it will lead to more carbon dioxide emissions than previously thought, thus upping the impact on global warming and climate change.   
  
  
But the numbers being tossed about don’t tell the whole story.   
  
  
First, a look at the new economic claims. An analysis from the Consumer Energy Alliance concludes that during the two year construction phase of the pipeline, the economic activity in Nebraska will increase by a bit more than $400 million per year—generating directly or indirectly, about 5,500 new jobs. Sounds impressive, but this boost is short‐​lived. After that, for the next 15 years, the economic input drops down to about $67 million/​yr, supporting about 300 jobs. A net positive, but not as much as many proponents claim.   
  
  
The climate claims are even less significant. In its new report, Oil Change International asserts that the current estimates of the well‐​to‐​wheel (WTW) carbon dioxide emissions from oil extracted from the Alberta tar sands have been underestimated. They claim that the State Department failed to fully include carbon dioxide emissions from the burning of the petroleum coke that is produced as a side product of producing oil from the tar sands. This “petcoke” can be burned like coal, and in fact, is cheaper and more energy dense than coal, so it is often preferable. According to Oil Change International, including the petcoke in the calculation would increase the WTW carbon dioxide emissions by about 13 percent.   
  
  
There are several things wrong with the Oil Change International analysis. First is that the State Department actually did include a considerable discussion of the influence of the treatment of petcoke in its assessment. It concluded, just like Oil Change International, that if the petcoke is burned, it increases the total wells‐​to‐​wheels carbon dioxide emissions of Canadian tar sands oil by the same 13 percent. But what the State Department points out, and which Oil Change International plays down, is that the burning of petcoke to produce energy by and large displaces the use of coal for the same purpose. So instead of the total emissions, what is important is the _incremental_ carbon dioxide emissions produced from using petcoke instead of coal. And when that number is used, the WTW emissions increase by less than 1 percent—which is why the State Department concluded that the fate of the petcoke really wasn’t all that significant in the overall WTW emissions calculation.   
  
  
But whether consideration of petcoke increases the WTW carbon dioxide emissions of the tar sands oil by 1%, 13%, or any number in between, really doesn’t matter anyway in terms of its impact of global warming. For as I have shown previously, the global warming potential of the Keystone XL pipeline oil is only about 0.00001°C/yr. Increase that by 13% and you basically get the same environmentally insignificant number. In fact, you’d have to increase it _by several orders of magnitude_ before it is even worth paying attention to.   
  
  
The war over the pipeline will probably rage on until (and even after) a decision is reached in a couple of months. Hopefully, emotion will play a role secondary to facts.
"
"
Share this...FacebookTwitterStar in Easy Rider
Share this...FacebookTwitter "
"What is a healthy room temperature?  On releasing its Cold Weather Plan for 2014, Public Health England has recently revised its recommended minimum levels to keep in good health.  No longer, they say, do living rooms need to be kept at 21°C and bedrooms at 18°C, as used to be advised. Now all rooms can be kept at 18°C with “minimal risk” to the health of “a sedentary person wearing suitable clothing”. It’s a rethink of just three degrees but, as with outdoor climate change, a few degrees of indoor climate change can make a significant difference. Such a review was long overdue. The World Health Organisation advice that standards were based on was more than 30 years old, and the accumulation of evidence over time (although limited) warranted another look. Under 18°C evidence shows that the health  risks increase, particularly for those with cardio-respiratory disease, and for older people who spend a lot of time at home and may not perceive cold temperatures due to physiological changes that occur with ageing. But the evidence that 21°C was a significant threshold, even for those most vulnerable, was found wanting. So good scientific governance.   But it is also a recognition that room temperature can have an indirect impact on health. If people keep their living rooms at 21°C because they feel they ought to, they will spend more on their energy, potentially getting into financial debt and other forms of hardship that have health and well-being implications. Higher room temperatures also more mean more energy demand and more carbon emissions, so the longer term health effects of climate change also need to be factored in. All sound and balanced thinking. Keeping warm and healthy depends on many other factors. Our bodily temperature is also the result of the clothes we wear, what we eat and drink, how active we are, all in a systemic interaction. So Public Health England says that younger people who are healthy and active and able to “layer up” should be fine with lower than 18°C. And overnight certainly the 18°C threshold is less important for healthy people if they have sufficient bedding, clothing and use thermal blankets when necessary. “Layering up” advice is territory where others have come unstuck. In an ongoing analysis of newspaper reporting of energy issues we are finding a pretty universal castigation of any suggestion made by politicians and energy companies that people should wear more clothes or put another blanket on the bed. Who are the prosperous and profiting to tell others how to live?  The messenger matters, clearly.  Public Health England should get an easier ride, but the organisation still concedes, “we know that people have strong feelings about their homes and don’t want to be told what to do in them”. Such are the political sensitivities nowadays around even common sense health advice entering the domestic sphere. The rethink on the “need” for room temperatures could have implications for government policy which currently defines being fuel poor as the energy required keep the main living area at 21°C (along with “required” energy for other purposes such as lighting and hot water).  Should this threshold now shift to 18°C? Maybe, but careful consideration is needed. The guidance notes that older or less healthy people can benefit in medical terms from a “slightly” higher room temperature.  If fuel poverty policy is about protecting the most vulnerable under all circumstances, then 18°C may well not be sufficient. In other respects the advice might not have got the balance quite right. They recommend keeping a bedroom temperature of 18°C overnight for people older than 65 or with pre-existing medical conditions, as this “may be beneficial” to protect their health. The health evidence here is not so strong (hence the “may”), and if taken seriously would mean keeping home heating systems on overnight in winter. Research shows this is far from standard practice for most households, and particularly for people on low incomes, so would imply increased energy costs of the form that elsewhere Public Health England says it is keen to avoid. There are also questions about energy rights, or energy justice more broadly. The new guidance says the 18°C threshold is relevant for everyone. Does it then follow on health grounds – as well as on the grounds of meeting what has become a basic human need – that there should be no longer be any disconnections by electricity and gas suppliers?   This was one of the demands of the Energy Bill of Rights recently launched by Fuel Poverty Action at the House of Commons. There are already regulatory and best practice provisions that limit the circumstances and times of year during which certain categories of households can be disconnected by energy suppliers. But if all people need to be able to achieve 18°C (or near to this) at home to minimise health risks, then shouldn’t the “right to not be cut off” be universally applied?   Since 1999, home water disconnections have been banned on health grounds. If “room temperature” is truly vital to health, maybe it is time we banned energy disconnections as well."
"
Share this...FacebookTwitterH/T: Benny Peiser
This is the final day of the Deutsche Welle’s Global Media Forum, this year’s conference is titled “The Heat is On – Climate Change and the Media”. If any conclusion can be drawn, it is that elite warmists are extremely frustrated. Read here.
Bob Ward:
British journalists don’t know difference between fact and fiction.
Peiser’s GWPF report reads: “But he also concedes that there have been grave mistakes made by researchers”. And Ward called for scientists to handle their findings and knowledge responsibly. Ward goes on to say:
The IPCC is too slow in correcting the faults.
Naomi Oreskes, non-consensus denialist:
The statements from scientists are so greatly disconnected from the media in the USA because the journalists unknowingly and inaccurately repeat what was said.
…so-called climate skeptics are nothing but “contrarians” and can’t be taken seriously because their critique isn’t scientifically based.
Can you hear their teeth gnashing?
Share this...FacebookTwitter "
"”Unprecedented” is the word that keeps being tied to the apocalyptic weather Australia has faced over the past few months. Bushfires have always been a reality in Australia, but never recorded on this scale with such widespread damage. It’s estimated that more than 60,000 sq km have been scorched in New South Wales and Victoria alone. Days of smoke have shrouded Sydney, Canberra and Melbourne. And after the fires, flooding at the weekend in NSW and parts of Queensland left thousands without power and dozens of schools closed on Monday.  While the country is still grappling with the economic reality and human devastation caused by the fires, it’s easy to think the worst of this disaster is over. But unfortunately other extreme weather may yet occur this summer and these will also require safety preparations and rapid responses. Last year was the driest and hottest year on record in Australia. Some parts of the country have had several years of drought in a row. But all droughts end eventually. At the weekend devastating storms swept through eastern NSW, causing flooding, power outages and commuter chaos. The Bureau of Meteorology says 391.6mm of rain fell over Sydney in the past four days, the most since 414.2mm fell from 2 to 5 February 1990. Historically Australian continental-scale droughts are often broken by widespread heavy rain, leading to an increased risk of flooding, including potentially lethal flash floods. The flood risk from the heavy rains is exacerbated by the bare soil and lack of vegetation caused by the drought and by bushfires that destroy forest and grassland. When a decade-long drought ended in 2009, what followed were two extremely wet years with serious flooding. Flooding also brings the risk that ash might contaminate water supplies. The heavy rain falling on bare soil can also lead to serious erosion. The onset of the tropical wet season over northern Australia has been very much delayed, as predicted in the middle of last year by the Bureau of Meteorology. Most of the Australian tropics have had well below average rainfall in the past few months, and some areas had their lowest November-January rainfall. As well, the tropical cyclone season was late, also predicted by the bureau months ago. In recent weeks there has been some cyclone activity and some rain. But the wait is still on for widespread tropical rains and for more cyclones to cross the coast as Damien did at the weekend. Although rain brought by cyclones are often welcome, these systems can also leave serious damage. We are at the riskiest time for heatwaves in southern Australia. The risk usually peaks around the middle to the end of summer. Weather conducive to increased bushfire risk also usually peaks in February for southern states. So although media, community and political attention have focused on the horrendous bushfires we have already suffered, we should not overlook the likelihood of other extreme weather, including cyclones, floods and heatwaves, or think that the bushfire risk is over for the year. It is important to remain vigilant for all weather extremes. Although the fires have tragically claimed many lives, many others have been saved by the improved firefighting and warning systems, supported by improved weather and climate forecast systems to emergency services and the media by the Bureau of Meteorology. Temperature forecasts five days ahead are better than one-day-ahead forecasts were 50 years ago. Rainfall forecasts have also improved dramatically. As well, scientific seasonal climate forecasts of rain, drought and seasonal cyclones – something not even dreamed of 50 years ago – are provided routinely by the bureau. These longer forecasts allowed emergency services to better prepare for this horrible summer, and the detailed shorter-range forecasts of weather conducive to fire spread has helped fire agencies target warnings and provide resources to threatened areas. But one challenge as the summer continues will be the need for continued communication between forecasters, emergency services and the public about predicted extreme weather. For some, “warning fatigue” may set in and announcements of dangerous heatwaves or floods might be ignored. The warnings must continue to be disseminated to at-risk populations and local authorities must strive to ensure they are acted on. And meteorologists must keep up their efforts to improve forecasts on all timescales. Global warming is already lengthening the fire season and making heatwaves more intense, more frequent, and longer. It is also increasing the likelihood of heavy rains and flash floods while simultaneously making droughts worse in some areas. The occurrence of devastating bushfires has been increasing in the past few decades, despite better forecasts and improved firefighting technology and organisation. The intrinsic link between these weather extremes and climate change means we need to address the wider issue: what can we do to slow the rate of global warming. Australians pride themselves on winning against the odds and adapting to extreme weather. How often have you heard a politician say Australia has always been a land of droughts and flooding rains, or that we have always had heatwaves and bushfires? Australian rainfall is indeed more variable than most parts of the world, so the reality is that we do face extreme weather, year after year. But another reality is that daytime temperatures across Australia have increased by more than a degree just in the past 25 years. In the 1990s on average about 5% of the country each year had annual average daytime temperatures in the hottest 10% of historical temperatures. But over the past five years, on average, more than half the country has experienced such hot temperatures each year. From 5% to over 50% in the lifetime of generation Z. Such strong warming must affect not just all the weather extremes – droughts, floods, heatwaves, bushfires, cyclones and storms – but also their impacts on humans, animals and the bush. And there is no reason to expect this warming to slow without concerted political action. So we need an increased focus on how to deal with these amplified risks from climate and weather extremes – how to adapt – at the same time that our politicians lead global action to slow the rate of warming. We don’t have much time. We need to adapt, to manage the unavoidable, and slow greenhouse gas emissions to avoid the unmanageable. • Neville Nicholls is an emeritus professor at Monash University. He spent 35 years with the Bureau of Meteorology, with his research focusing on how and why the climate is changing"
nan
"The mascots for the Rio de Janeiro 2016 Olympics and Paralympics have been announced. The yellow chap above is the Olympic mascot, apparently an amalgam of all Brazilian animals, which surprisingly only includes monkeys, cats and birds. The green and blue fellow, the Paralympic mascot, is supposed to represent all of Brazil’s plant species. I’m not using their names as they don’t have them yet – the mascots are waiting for the results of a public competition.  The name choices on offer are not great.  Two are from Brazilian slang meaning great (Oba and Eba), two are from a native language and have meanings related to dance (Tiba Tuque and Esquidim) and the final two choices are after two of the founders of bossa nova music (Tom and Vincius). It’s a real shame that neither of the mascots has the chance of being given a female name. Over the past year Brazilian conservationists have been lobbying for various different species to be the Olympic mascot. Of course this is motivated by the financial funds likely to be directed towards the winner. Proposed species included primates (muriquis and golden lion tamarins), cats (ocelot) and birds (hummingbirds).  All of these would be appropriate and worthy choices. The golden lion tamarin is one of the true symbols of conservation biology – I learnt about its reintroduction back into the wild as an undergraduate student more than 25 year ago. It is also a natural Carioca: a species that once lived within the boundaries of Rio de Janeiro.   The muriqui, while not a Caricoa, can be found in the mountains which form a backdrop to the city.  Ocelots are medium sized cats, which once lived in present-day Rio.  And hummingbirds are also Cariocas.  All species are threatened with extinction in the region. When you talk about Brazilian wildlife most people think of the Amazon. It may therefore come as a surprise to find out that the Amazon basin is not actually considered a true biodiversity hotspot.  These hotspots, 35 in total around the world, are defined by the large number of species they contain and the high degree of threat from human activities.  Brazil has two: the Cerrado (savannahs) and the Atlantic forest. The latter extends along much of Brazil’s south-eastern coast and once covered the whole of the city of Rio de Janeiro. It is ranked the fourth most important biodiversity hotspot in the world, but it is being chopped down faster than ever. Despite conservation biologists having their own preference for the mascot, everyone agreed these games are an opportunity to highlight the importance of the Atlantic forest to the world.  Biologists were thus dismayed to see two mascots who appear uncannily like Pokémon characters. This has caused some derision among Brazil’s conservationists. For the football World Cup held in Brazil earlier this year the organisers went with Fuleco, a three-banded armadillo – cuddly but endangered. However they didn’t back things up with sufficient conservation support or even make it too clear what Fuleco was supposed to represent. I had hoped that the Rio Olympics would not make the same mistakes, and it seems they haven’t. However they appear to have avoided this simply by inventing cartoon characters.  If a mascot is invented – if there is no “real animal” to preserve – then no-one can complain about lack of support for its conservation. To me this odd hybrid-creature smacks of trying to appease the biologists lobbying for real species.   It should be pointed out that while Pokémon was popular with children its premise as a game was about capturing, collecting and training wild creatures. Given that animal trafficking is a major problem in Brazil, the Pokémon-like mascots are particularly inappropriate. As a father of two small children whose life is plagued by cartoons on the TV I understand the need for these mascots to appeal to kids.  But actually the real species proposed were all very cute.  What I object to more is the unashamed anthropomorphising of the mascots and giving them super powers – the ability to stretch.  Children’s TV series such as Dora the Explorer and Go Diego Go were perfectly entertaining and educational without the lead characters needing to have abilities of the Fantastic Four. I hope that some of the lessons from Fuleco have been learnt, and that the organisers of the Rio Olympics will grasp the opportunity to promote the plight of the Atlantic Forest and its amazing wildlife."
"Like all great pledges, it is just the right side of implausible. The UK aviation sector this week committed to making flying “net zero” by 2050, wiping out its carbon emissions despite taking more than 100 million extra passengers into the air each year. At a celebratory event in London, the bosses of airports, airlines and aircraft manufacturers queued to scrawl their signatures on a giant Net Zero pledge card from the Sustainable Aviation [SA] campaign. Greenpeace described it as greenwash. But aviation figures insist the ambition is genuine. John Holland-Kaye, the Heathrow airport chief executive, says: “I imagine it’s like it is for alcoholics. The first step is to admit we have a problem – and then do something about it. I’m not sure what the 10 points on the AA [Alcoholics Anonymous] programme are, but you can find the equivalent in the SA roadmap.” The steps certainly involve a leap of faith, in future fuels and aircraft technology, and human capacity to endlessly offset emissions. Just how does a growing industry plan to reduce its annual footprint from just over 36m tonnes of CO2 to net zero in three decades’ time? Aircraft such as Boeing’s 787 or Airbus’s A350 already emit significantly less than the older jets they are replacing, through lighter materials and more efficient engines. Sustainable Aviation anticipates that emissions will drop by about 30% on routes operated by Boeing 747 jumbos as they are phased out within a decade. The next iteration of the manufacturers’ short-haul workhorses are supposed to be 10%-15% more efficient – demonstrated with the A320neo superseding A320s, although the disastrous introduction of the grounded Boeing 737 Max has delayed airline plans to replace older 737s. Altogether different types of propulsion will be in operation by the 2030s, manufacturers believe. But while new electric planes could revolutionise regional flights, and hybrid-electric could manage short-haul flying, their contribution to cutting UK emissions – about 65% of which hail from flights of more than 1,000 miles – will be comparatively minor by 2050, SA admits. “We’re not going to be electrifying a London-Singapore A380 for a long time, if ever,” says Paul Stein, chief technology officer of aircraft engine maker Rolls-Royce. “But sustainable fuels can work on the engines we have today, and the concept has been proved.” Creating the necessary volume is the tricky bit. Hopes have been pinned on new schemes, such as the new British Airways-backed plant to create jet fuel from household waste. Stein says Rolls-Royce has been exploring whether a small modular nuclear reactor could be used in a synthetic fuel plant, making hydrocarbons in a way that would reduce CO2 to balance the emissions produced in flight. “Right now we’re in analysis phase – understanding the economics … But what we’re going to see over the next 10 to 20 years is a lot of innovation and energy directed at creating sustainable aviation fuel.” Progress in coordinating European airspace and air traffic control has been slow. But with better use of airspace, and coordinated flight paths, planes could be guided more efficiently from take-off to landing. The UK is in the midst of redesigning its airspace to help create better paths. More efficient operations, such as eliminating stacking, would help aircraft carry and burn less fuel. Environmental groups question the adequacy of offsetting – but the roadmap relies on it to account for more than a third of aviation’s projected emissions. Jonathan Counsell, head of sustainability at British Airways’ owner IAG, says: “We think that we can get to net zero without it – but not by 2050. This is a transitional measure.” The scepticism about offsetting means, Counsell admits, that any schemes have to be high-quality, independently verified and end in actual, additional carbon removal. For now, offsetting means reforestation or restoration of peatland bogs that absorb CO2 from the atmosphere; later, potentially, investment in new carbon capture and storage technologies. In an industry run by accountants, some jiggery-pokery was perhaps inevitable: emissions are “saved” as the 70% rise in passengers by 2050 is, in fact, slightly lower than the Department of Transport’s growth forecast growth. “We will have to pay more to fly,” says Holland-Kaye. Sustainable fuels are more expensive, and the carbon price is expected to rise tenfold, he says. The price of all the offsetting that the industry will rely on will be passed on to consumers through higher fares, constraining demand. So can this work? The industry urgently wants to present an alternative vision to the growing backlash against flying. Stein says: “Flying connects the world – it is a force for good in transporting people and goods, making sure our cultures have great levels of understanding.” As an engineer, he says, he is optimistic that emissions can be brought down sooner than the roadmap pledge: “We have the engineering tools to do it; we need the will, and government help.”"
"

Foreign policy has been a contentious issue for libertarians since September 11, 2001. There have been countless harangues in Washington bars and policy salons over the past five years about libertarianism and the Iraq War, and the topic has been so divisive for libertarians that even Rose and Milton Friedman disagreed. She was in favor and he against, with Rose noting later: “This is the first thing to come along in our lives, of the deep things, that we don’t agree on.We have disagreed on little things … but big issues, this is the first one!”



Why has the war — and post‐​9/​11 foreign policy generally — been so controversial for libertarians? And now, more than six years after 9/11 and more than five years into the war in Iraq, what can libertarian insights tell us about how we got here and what to do next?



To try to answer these questions, we should begin with some libertarian starting points about government and then review the debate over the Iraq war and foreign policy more generally in the wake of 9/11.



Then we can consider where to go from here, and what a counterterrorism policy that paid heed to libertarian insights would look like.



 **Government: Dangerous at Home, Beneficent Abroad?**



Nation‐​states are self‐​interested collective organizations, both at home and abroad. As public choice economists tell us, the first interests the state looks after are the state’s — not the people’s. Quite often, the state’s interests are served by war.



War historically has been the most effective generator of big government. As Bruce D. Porter observed in his book _War and the Rise of the State_ , the nonmilitary sectors of the federal government grew at a faster pace during World War II than they did under the New Deal. War creates the perfect climate for the collectivist mentality, as well as ready‐​made occasions and arguments for expanding the power of the national state.



In the international arena, it is important to note that security — the first‐​order concern of any state — is ultimately contingent on a state’s ability to defend itself. Decisions about national policies are based on how threatening a state views the international environment. Overall, security is scarce, and history tells us that states are competitive and leery of any state that grows too powerful and/​or throws its weight around. The concentration of military power in the hands of one actor in the international system can cause fear, particularly if that state appears intent on overturning the existing balance. It was for that reason that Thomas Jefferson wrote in 1815 of his desire that nations “which are overgrown may not advance beyond safe measures of power, [and] that a salutary balance may be ever maintained among nations.”



In recent years the United States has upset the world’s balance. Countries assess threats on the basis of capabilities and intentions, and the U.S. government at present appears to have enough of both to alarm other governments. Washington spends roughly as much on its military as does the rest of the world combined, and political leaders in both parties argue that we need a military significantly bigger. At the same time, in addition to the attack on Iraq, American leaders have begun to openly discuss their intentions of unraveling the international order. During a June 2007 speech to the Economic Club of New York, Secretary of State Condoleezza Rice argued that “America has always been, and will always be, not a status quo power, but a revolutionary power.” Thus we should not be surprised when we encounter fear and distrust from Berlin to Beijing.



What is most peculiar about this state of affairs is that the United States sits unchallenged atop the international order, with an unparalleled ability to shape it and with any potential peer competitor several decades away. This state of affairs is hugely beneficial to us; imperfect though it is, the United States should be working to _preserve_ , not overturn, the existing international order. But some observers, including a few libertarians, seem to have concluded that the threat from terrorism is so great that the United States must embark on radical social engineering projects abroad to combat it.



 **What Changed after 9/11 — and What Didn’t**



Despite the preeminent position of the United States in the international order, many American political leaders and thinkers — including some libertarians — embraced aggressively interventionist foreign policies after 9/11. The threat of international terrorism, primarily from al Qaeda, was broadened to include the nation‐​state of Iraq. President George W. Bush argued that an effective strategy for fighting terrorism must include regime change in Iraq in order to transform the social and political culture of the Middle East.



Most libertarians questioned those moves. Some embraced them.



Perhaps the most prominent libertarian to advance these ideas has been Randy Barnett, a nonresident senior fellow of the Cato Institute and professor of law at Georgetown University. Barnett published an op‐​ed in the _Wall Street Journal_ in July 2007 criticizing noninterventionist libertarians for failing to understand that “libertarian first principles … tell us little about what constitutes appropriate and effective self‐​defense after an attack.” He argued that libertarians can and should think of the attack against Iraq as appropriate self‐​defense in response to 9/11. Further, he argued that libertarians should favor “a strategy of fomenting democratic regimes in the Middle East.”



Such radical government programs could only be endorsed by a libertarian _in extremis_. But there was never reason to believe Iraq was either responsible for 9/11 or plotting the next one. The Iraqi government was not involved in 9/11, and attacking it devoted scarce resources to the wrong target. The appropriate response to the newly prominent threat of nonstate terrorism was to concern ourselves more with nonstate terrorist groups, which do not have return addresses and frequently cannot be deterred. To lump in states — whose relations with each other were largely unchanged by 9/11 — with such groups is to confuse different types of problems.



Barnett himself wrote in his 1998 book _The Structure of Liberty_ that libertarian conceptions of self defense are limited to _imminent_ attacks, a limitation that Barnett deemed “well‐​founded … because of the enormous knowledge problem that would be confronted if we were to permit selfdefense actions prior to a threat becoming imminent.” Barnett warned readers further that “every erroneous and unjust use of violence threatens to induce resentment, bitterness and the desire on the part of those against whom violence is used to rectify this injustice by responding violently, thereby setting off a cascade of violence.”



One could apply those insights to the war in Iraq. The U.S. government attacked Saddam’s regime in the absence of any imminent threat, and it seems that we indeed induced a significant amount of resentment, bitterness, and desire for vengeance by starting the war. (The debate over whether the intelligence supporting the case for war resulted from governmental incompetence or malfeasance — and neither explanation should confound a libertarian — is irrelevant.)



In his _Wall Street Journal_ article, Barnett admits supporting the war even though he believed that it would go poorly. He concedes that “to a libertarian, any effort at nation building seems to be just another form of central planning which, however well‐​motivated, is fraught with unintended consequences and the danger of blowback” and that he is “disappointed, though hardly shocked, that the war was so badly executed.” A critic of the decision to go to war might then ask why one should support a war you expect to go badly. And given that the objective of the war was a massive social engineering project unprecedented in scope — the destruction and reformation of a regional order — how could libertarians have envisioned it going any other way than poorly?



Indeed, how is it simultaneously possible to oppose government involvement in education or health care on the grounds of the inherent lack of necessary knowledge, but believe that the federal government could invade Iraq and then unravel and reweave the fabric of a thousands‐​year‐​old society whose language we do not speak and whose tribal and confessional allegiances we do not understand? Following the insights of thinkers such as F. A. Hayek, libertarians are deeply skeptical that governments could collect and sort enough data to plan government health care or education effectively. Surely those difficulties are compounded when the goals are even more ambitious and the policies are conducted in foreign countries wracked by sectarian conflicts.



The _Atlantic’s_ Matthew Yglesias observed the debate among libertarians over the war and judged that “the notion that anything even remotely resembling libertarianism could underwrite an effort to conscript huge quantities of resources from the American public and deploy them in an attempt to wholly remake the social and political order in a foreign country is too absurd to merit a rebuttal… . It’s coercion, it’s planning, it’s every non‐​libertarian thing under the sun.”



The policies that libertarian hawks have supported have cost more than half a trillion dollars and four thousand American lives — greater than cost of the 9/11 attacks themselves. (Libertarians also should not ignore the violations of individual rights that occurred in the form of the hundred thousand or so Iraqis who perished as a result of our political science experiment in their country.) Government power, unchecked by prudence or other constraints, can do great harm not only to foreign targets, but also to the very citizens that the government is charged with protecting. To craft an effective response to the terrorist threat, it is necessary to dispassionately assess the nature and scope of the threat.



 **Getting Threat Assessment and Response Right**



The very real problem of terrorism can be handled without massive nation‐​building projects in the Middle East. In fact, the biggest successes in fighting terrorism since 9/11 have been achieved through cooperation with foreign intelligence services and police agencies. Precious few meaningful victories against terrorism, by contrast, can be ascribed to the government’s tinkering with Iraq.



My colleague Benjamin Friedman observes that even in 2001, the flu killed more than 10 times as many Americans as did terrorism. Certainly past performance is no guarantee of future results, and one can conceive of improbable scenarios that would radically expand the destructive capacity of terrorists (their acquisition of a nuclear weapon, say). But to date, the government’s nation‐​building‐​as‐​counterterrorism approach has been more destructive and wasteful than terrorism itself and has done little to diminish the problem. In fact, there is ample evidence that terrorists realize that the best way to inflict harm on America is to trick us into responding in ways that harm ourselves.



Osama bin Laden boasted in 2004 that it is “easy for us to provoke and bait this administration.” Describing his desire to “bleed America to the point of bankruptcy,” bin Laden remarked, “All that we have to do is to send two mujahedeen to the furthest point east to raise a piece of cloth on which is written ‘al Qaeda,’ in order to make generals race there to cause America to suffer human, economic and political losses.”



Instead of allowing ourselves to be goaded into self‐​destructive responses, we should review our diagnosis, our prescription, and our prognosis. In pursuing an accurate diagnosis, we must confront a painful truth that study after study has revealed: U.S. foreign policy plays a significant role in public opinion in the Islamic world — and as a result, represents a big part of our terrorism problem. As a 2006 Government Accountability Office report noted, “U.S. foreign policy is the major root cause behind anti‐​American sentiments among Muslim populations and … this point needs to be better researched, absorbed, and acted upon by government officials.”



The Pentagon’s Defense Science Board was less diplomatic, writing in 2004 that “Muslims do not hate our freedom, but rather, they hate our policies.” Bin Laden himself argued in 2004 that “contrary to what Bush says and claims — that we hate freedom — let him tell us then, why did we not attack Sweden?”



Of course, not every terrorist is motivated by rage at U.S. foreign policy. There are clearly a small number of terrorists who carry out murders for other reasons. It should go without saying that the only viable policy approach toward committed terrorists — no matter their motivation — is to pursue them and capture or kill them in cooperation with foreign intelligence services and, in some cases, with the limited use of American military power. But our strategy should not be solely reactive. There are a vast number of people who may be receptive to bin Ladenism but aren’t yet convinced they should join him. And by far the most effective recruiting tool in al Qaeda’s arsenal is the notion — alarmingly widely accepted in the Muslim world — that America’s actions prove we are out to destroy Islam.



Accordingly, to treat the problem we need to focus more on the question of how we can better affect the marginal terrorist recruit. What makes him or her more or less likely to join the cause? Wouldn’t removing bin Laden’s best recruiting tool be helpful? The other side of the coin is that al Qaeda’s remarkable barbarity has been a public relations disaster in the Islamic world. Very few people — far fewer than support relatively liberal governance — express any desire to be governed by people like al Qaeda. Shibley Telhami, one of the leading pollsters of the Islamic world, testified to Congress in 2005 that al Qaeda’s support in the Arab world stems disproportionately from its opposition to U.S. foreign policy. Of the Arabs in Telhami’s poll expressing support for any of al Qaeda’s aims, only 6 percent supported the group’s objective of creating a Taliban‐​style state.



Al Qaeda can’t sell an affirmative agenda; what it can sell is opposition to U.S. foreign policy. A smart approach to counterterrorism would recognize that fact and avoid providing bin Laden and his comrades with opportunities to pose as the defenders of Islam against a hostile, colossal, anti‐​Islam United States.



Now for the prognosis. It is time to take a deep breath and recognize the strength of our system. Liberal capitalism is the best means for organizing human activity. It provides for the most flourishing, it provides for the most technological innovation, and it has the strength to endure through time. During the Cold War, alarmists warned constantly about the durability of the Soviet system, insisting that it was, in many ways, stronger than our own. They were proved fantastically wrong when the sclerotic Soviet state collapsed in a shambles in 1991. To respond to the band of fanatics we face today with hysteria does not befit a great nation of our size and vitality.



Hollow though it was, Soviet communism was a far more dangerous force than Islamic terrorism. The system that withstood the challenge of communism can similarly survive the threat from Islamic terrorists. As mentioned above, the style of governance that al Qaeda and its affiliates can offer to Muslims around the world is exceedingly unpopular. Earlier in the Bush administration, citizens of Arab countries held surprisingly favorable views of American freedom and the American people, although those figures have declined substantially. What becomes clear from the data, however, is the overwhelmingly negative view of U.S. foreign policy in the Islamic world. Putting our best face forward and emphasizing the positive features of the United States will go a long way to repairing our poor position in the world. As George F. Kennan wrote in his 1993 memoir, the United States must “never lose sight of the principle that the greatest service this country could render to the rest of the world would be to put its own house in order and to make of American civilization an example of decency, humanity and societal success from which others could derive whatever they might find useful to their own purposes.” 



We have lost sight of this principle. But in the months and years to come, we should refocus and take solace in the fact that certain important and basic truths remain unchanged. Our system is strong; bin Laden’s is weak. We are wealthy; al Qaeda is poor. We have greatly influenced the structure of the world order; they can only affect it by provoking reaction. The best thing to do now is to jealously guard our strength, not squander it; to keep and hold our quiet confidence, not panic; and to pursue this new breed of enemy with the prudence and wisdom of a mature nation.



The political scientist Hans Morgenthau wrote in _Politics among Nations_ that “throughout the nation’s history, the national destiny of the United States has been understood in antimilitaristic, libertarian terms.” This fact is linked to the rugged individualism of the American founding and the kernel of libertarianism that lies at the heart of the nation even today. Those who would jettison the antimilitarism would also jettison the libertarianism, compounding the tragedy.



Before his death in 2006, Milton Friedman lamented that his life’s project of limiting government power was “being greatly threatened, unfortunately, by this notion that the U.S. has a mission to promote democracy around the world,” pointing out: “War is a friend of the state… . In time of war, government will take powers and do things that it would not ordinarily do.” It is for precisely that reason that libertarians, more than anyone, should not be friends of war.



 _Justin Logan is associate director of foreign policy studies at the Cato Institute._
"
"“It makes sense” is the first thing to say about the phenomenon being described by psychologists as climate anxiety. Wherever in the world you live, there are very good reasons to feel anxious about the rate of global heating and the lack of adequate action to tackle it by governments, businesses and organisations of all sorts. The predicted consequences are frightening: hotter weather in already inhospitable places, sea-level rises caused by melting ice sheets, and increased disruption of weather systems leading to floods, fires, hurricanes, food and water shortages – with the linked biodiversity crisis another cause for grave concern. Depending on the steps that are taken (or not) over the next decade, a period during which the UN estimates that carbon emissions need to be cut by 7.6% annually if we are to avoid temperature rises above 1.5C, the disruption caused to human societies could be immense. For countries such as Bangladesh, the effects are likely to be devastating.  Given all this, it arguably makes more sense to be anxious than not. And climate anxiety is one way of describing the motivations of every person or organisation that is trying to do something to limit or to mitigate the effects of global heating – whether an individual altering their diet, a charity switching energy supplier, a council setting emissions targets or the Guardian deciding to stop selling advertising space to fossil fuel companies. But, as with all negative emotions, the trick is to distinguish ordinary feelings – what Sigmund Freud famously called “common unhappiness” – from those that are disproportionate, or so intense and prolonged as to be debilitating. While it makes sense to be worried about the climate emergency, becoming overwhelmed is counterproductive. The sound advice from psychologists that actions, however small, can help to alleviate feelings of distress and powerlessness echoes the experiences of activists including Jane Fonda that “the minute you start doing something, the depression goes away”. Not all low moods are readily lifted, however, and warnings of worsening mental health as a result of climate disruptions and hardships should be taken seriously. Already there is cause for concern, with research showing that people who have experienced extreme weather such as floods in the UK are 50% more likely to suffer from problems including depression. Resilience may be a desirable quality, but is much more easily developed by those who are cushioned by income or advantage. Growing demand for psychological support should be met by professionals who are able to distinguish everyday worries from post-traumatic stress or other symptoms. Disasters on the scale of Australia’s recent bush fires and Indonesia’s floods can be expected to produce severe mental as well as physical reactions, particularly in children and other vulnerable groups. In some parts of the world, trauma is already normalised, and when psychologists write of their fear that it could become ubiquitous, policymakers everywhere should take notice. But it’s important to remember that there are reasons to hope, as well as despair. As the environmental scientist Vaclav Smil said last year, “We [humans] are stupid, we are negligent, we are tardy. But on the other hand, we are adaptable, we are smart and even as things are falling apart, we are trying to stitch them together”."
"

This spring the Pew Research Center released its eighth annual report on the state of American journalism. “In some ways, new media and old, slowly and sometimes grudgingly, are coming to resemble each other,” the study says. The traditional platforms of the Fourth Estate are changing, and last year, online news consumption outstripped print media for the first time in terms of both advertising revenue and readership. The tipping point has arrived. The trend line is clear. And the Cato Institute, it seems, has been ahead of the curve.



Since 2005 _Cato Unbound_ has given readers access to a state‐​of‐​the‐​art virtual trading floor in the intellectual marketplace. A unique online magazine, it reflects an appreciation of the way ideas are exchanged in the digital age. Every month one of the world’s leading thinkers presents an essay on a topical issue. A panel of distinguished experts responds, each offering his case before challenging and refining the arguments in an ongoing conversation. Readers are then encouraged to join the dialogue by offering their own thoughts through websites, blogs, and letters to the editor. These contributions are pulled together into an easily accessible forum, creating a media product that is virtually distinct within the digital realm.



Yet _Cato Unbound_ is also designed to avoid the pitfalls of its platform. For starters, the site revolves around the value of debate. All too often, the sheer availability of personalized news today allows readers to give in to confirmation bias — to seek out only the information that reinforces their existing beliefs. The internet, by any measure, caters to the obstinate. At _Cato Unbound_ , however, contributors are forced to confront their critics, and the tendency to selectively ignore the opposition is mitigated.



The site also hinges on the importance of perspective. The current news climate is subject to certain kinds of pressure: readers increasingly look for minute‐​by‐​minute updates. Many sites therefore suffer from a lack of depth by becoming preoccupied with instantaneous delivery. _Cato Unbound_ is different. “We try to step back, take a deep breath, and focus on the larger picture,” Jason Kuznicki, the site’s editor, explains.



In the latest issue, “Targeted Killing and the Rule of Law,” the editors ask whether the executive branch can lawfully kill. Lead essayist Ryan Alford, assistant professor at the Ave Maria School of Law, argues that it cannot. In fact, the “presidential death warrant” is so repugnant to our constitutional tradition, he says, that the Founders didn’t even think it necessary to make an explicit statement about the practice. At the time of the Revolution, British kings hadn’t enjoyed such a power for centuries, and it was thought to be the very antithesis of the rule of law. The distinguished panel of legal and historical experts responding to Alford includes John C. Dehn of the U.S. Military Academy at West Point, Gregory McNeal of Pepperdine University, and Carlton Larson of the University of California at Davis.



Other past issues have included



These monthly conversations have received attention from publications like the _New York Times_ , the _Washington Post_ , and _The Economist_. The site has featured a lineup of prominent contributors, including James M. Buchanan, the Nobel laureate and founder of the public choice school of political economy; Richard H. Thaler, professor of economics and behavioral science at the University of Chicago; James R. Flynn, a pioneer in the study of IQ; Clay Shirky, the renowned social media theorist; and Jorge Castañeda, former foreign minister of Mexico. Over the years, this forum has shown a depth of exchange and an accessible format that few other outlets offer.



An idea can be bound between covers, bound by convention, or bound for the dustbin of history. The ideas of _Cato Unbound_ , we hope, are none of the above.
"
"Australia should legislate a target of net zero emissions by 2050, the chief executive of the Business Council of Australia has said. Appearing on Monday night’s Q&A panel, Jennifer Westacott told the audience “we have to do net zero by 2050”.  Asked if that meant Australia should follow other parliaments such as the United Kingdom and legislate for net zero she said, “I think that would be a start.” “I reckon if we could get the two political parties to agree to that and legislate it, we would have made a massive advance in this country because we would know where we’re going,” Westacott said. “For business that does want to take action in this space, that would at least give us a kind of certainty about where are we heading.” Monday’s panel featured no politicians and was focused on how Australia should transition to a carbon neutral economy. Audience members who asked questions included coalminers from areas including Victoria’s Latrobe Valley. Westacott’s comments come as the independent MP Zali Steggall and crossbench MPs push for bipartisan support for a climate change framework bill aimed at helping the transition to a decarbonised economy. It includes a proposal for a net zero emissions target by 2050, a carbon emissions budget, and assessments every five years of national climate change risk. Steggall is calling for government MPs to be given a conscience vote on the bill, which would only succeed if this was granted. The bill will be introduced as a private member’s bill next month. Westacott told Monday night’s panel what Steggall had proposed was “sensible”. “Make no mistake – and she acknowledges this – the how really matters. We have to get the how right because we’ve got to create those new jobs,” she said. The BCA, which represents Australia’s largest companies, is also saying Australia, if possible, should meet its 2030 emissions targets without relying on carryover credits from the Kyoto period. And it repeated its call for the reintroduction of a carbon price to “drive the transition and incentivise investment in low and no-emissions technology”. It is included in a scoping paper seeking views from members, which include Australia’s major banks and mining companies such as BHP and Rio Tinto, as the group reviews and updates its energy and climate policies. The scoping paper says delivering net zero emissions at the lowest economic and social cost will require emissions cuts across all parts of the economy, including not just electricity, but also industrial processes, mining, transport and agriculture. The statement that Australia should effectively stop releasing carbon pollution within three decades comes as debate over climate action is escalating inside the Morrison government. Moderate Liberals declared on Monday the government should not underwrite a new coal-fired power station, and the trade minister, Simon Birmingham, acknowledged that, by signing the Paris agreement, the Coalition had already agreed to a long-term goal of global net zero emissions. It followed the Coalition announcing a $4m feasibility study into a proposed coal-fired power plant at Collinsville, in north Queensland. Last year the business council joined other groups in representing industry, unions, farmers and investors under the Australian Climate Roundtable banner in calling for policies that could put it on a path to net zero emissions. The group has previously been accused of standing in the way of serious proposals to address climate change. In 2018, it described Labor’s promised 2030 target of a 45% cut in emissions below 2005 levels as “economy wrecking”. The paper says the business council has supported strong action for “over a decade”. It says it supports the Paris agreement, a transition to net-zero emissions by 2050, a market-based carbon price and using technology to drive the change and create jobs and industries that “maintain Australia’s competitiveness”. On the government’s plan to use credits from the Kyoto protocol to meet its target under the Paris agreement, the business council said: “If we can meet our emissions reduction targets without carryover credits then we should.” On Monday night, Westacott said a net zero target had been the BCA’s policy for “a long time” but repeatedly stressed that how Australia got there was what mattered. “The how really matters,” she said. “It matters for regions and we have a responsibility to step out how that will actually occur. “Because just saying stuff and then not being able to show how we’re going to do it over what period of time and what cost and what technology and policy settings, what incentives, that will be another set of empty words that Australians will become cynical about.”"
"Like so many other wine country towns dependent on tourism and out-of-town visitors, the California resort community of Guerneville typically experiences a winter downturn. Business owners know to prepare for it. Restaurant owners scale back seasonal staff. Hotels offer discounted rates.  But this winter, no one was prepared for the aftershocks of the second major wildfire to hit Sonoma county in three seasons – one that prompted widespread mandatory evacuations and panic. No one was prepared for the after-effects of weeks of power shutoffs, a preventative measure during high fire weather. Business owners still reeling from the cancelled bookings and loss of business during peak tourism season in the fall are now struggling more than ever to get through the winter. “There just are not enough people coming into town,” said Larry Boeger, the owner of the Timberline at the River restaurant. “We’re closed during the week. We stopped offering brunch. Last winter, we were open every day.” In a region ruled by an industry where image means everything, Guerneville is experiencing an unforeseen economic impact of climate change. The Kincade fire, which burned more than 77,700 acres in Sonoma county in 2019, was miles away from the community of 4,500 along the Russian River, but officials evacuated the region as a precaution, fearing a repeat of the 2017 Tubbs fire that killed 22 people. The lush green hills and pastural vineyards that define the area remained largely untouched, but some fear the deluge of news drove away prospective visitors who pictured a barren wasteland. “It was a different scenario, but the fact that it was another big fire in Sonoma county, all the imagery of 2017 came to light in people’s heads,” said Claudia Vecchio, the president of Sonoma County Tourism. “Between the level of evacuation, which was widespread, and this ‘once again’ kind of mentality, it’s almost a more difficult perception issue than we had back in 2017.” Data on the number of visitors these past few years have been skewed because evacuations and home destruction have affected local hotel stays. But months after the Kincade fire, bookings are down in Guerneville. Boeger and other business owners are seeking out loans to stay afloat through the winter. “I’m looking out the window right now and there’s nothing but empty parking places,” Boeger said. “It scares people to think that their favorite vacation place isn’t safe any more,” said Megan Perkins, a manager with Russian River Vacation Homes. “They Google the area that they’re coming to visit or coming to book, they’re finding the articles about all the things that have gone wrong and how scary things have been. I think that’s impacting the amount of bookings we’re seeing.” The economic impact of the wildfires has come in waves. In preparing for leaner winter months, Sonoma county business owners know they need to build up a nest egg during the last hurrah of the tourism season, the fall harvest. During this time, Pacific Gas and Electric (PG&E) began cutting the power to large swaths of the state in massive power shutoffs that officials have described as California’s “new normal”. The unpredictability of that time period led to a lot of preemptive cancellations. Boeger lost two wedding parties, which would have brought him up to $12,000 a night. “PG&E planned to turn off the electricity the week before Thanksgiving, so we had Thanksgiving reservations canceling, we had Christmas parties canceling,” Boeger said. “People were calling to say, ‘I’m sorry, if I can’t count on doing my daughter’s wedding dinner here, then we’re going to reschedule somewhere else’.” Perkins’ company realized it was too unsafe to have guests staying in rentals without power if there was a chance of wildfire danger. Then once the fire hit and the mandatory evacuation order came down, they had to get everyone out. “We also personally, everybody who works here, had to evacuate,” she said. “We had to help all our guests get out, cancel anyone who was coming in, and this was usually a very lucrative time because fall is gorgeous in this area. We weren’t available to take bookings for the rest of the year and into the beginning of this year.” Elsewhere in Sonoma county, some felt that the slow season was a bad time to take the temperature of the wildfire’s economic toll. David Howard, the owner of Howling Wine Tours in Healdsburg, says he’s received a fair number of bookings for wedding parties in the critical on-season of summer and fall, and that the winter has not been as slow for him as those in Guerneville.  But after three years of losing business during the peak month for wine tours – in 2018, the smoke from the Camp fire in Paradise choked much of the Bay Area and kept everybody indoors – the problems are still there. “We’ve had three years of crushingly bad numbers,” Howard said. “It’s hit everybody in our business.” Tourism is a $2bn industry in the county, according to Sonoma County Tourism, with one in 10 jobs in hospitality. To help protect this, the organization is working on a new marketing push to encourage long-distance travelers and wedding planners to visit in the spring and early summer – a time when wildfire risk is low and power shutoffs unlikely. “People are wondering if Sonoma is reliable place to visit,” Vecchio said. “We’re telling people to choose to come to Sonoma county, but choose to come when there isn’t that sort of uncertainty.” In the meantime, Guerneville holds out hope in making it through this lean winter. “We’re continuing to message our repeat guests and people who have stayed out here before that it is safe, it’s fun, come stay with us again,” Perkins said."
"
Veteran Meteorologist Joe Bastardi of AccuWeather on Al Gore’s 60 minutes interview:
I am absolutely astounded that someone who refuses to publicly debate anyone on this matter and has no training in the field narrated a movie where frames of nuclear explosions were interspersed in a subliminal way in scenes of droughts and flood, among other major gaffes, can say these things and then have them accepted… by anyone.
See the complete writeup here on the AccuWeather Blog
If you wish to write letters to CBS New regarding the issue, see my post on the same subject here.
(h/t Jim Arndt)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea000efca',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

One of the classic examples of the failure of politicians to communicate with the citizenry is found in a video of Romanian tyrant Nicolae Ceausescu, giving what turned out to be his last speech to the teeming masses gathered in a square in Bucharest.



Oblivious to the mood of the people, Ceausescu is at his bombastic, self‐​important best until he realizes that the chants from the crowd below are not praise, but something rather to the contrary. The look on his face is, as they say in the MasterCard commercials, priceless.



America is a democratic republic, complete with an excellent Constitution that politicians still feel compelled to acknowledge, if not take seriously. So, the growing communications gap between the average American and the average politician, while worrisome, is not irreparable. Solving it should be a high priority for all involved.



The communication problem involves the accelerating realization on the part of many Americans that the essence of America, namely, a respect for the dignity of the individual, which inherently involves the government leaving the individual alone, has been pretty much forgotten by the politicians in Washington, D.C., the state capitals, and city councils around the nation. Which explains why public employees now make on average 30 percent more than their private sector counterparts — and 70 percent more in benefits. The political class seems to believe they have carte blanche to do as they please. They turn a deaf ear to increasingly vocal expressions of frustration by the American people.



Take, for example, a town hall meeting in Washington state last summer in which a young Marine veteran said to six‐​term Rep. Brian Baird, “Now I heard you say tonight about educating our children, indoctrinating our children, whatever you want to call it.” The congressman denied wanting to indoctrinate, but the young father simply responded, “Stay away from my kids.” Virtually all of the 400 or so people in the hall rose as one in loud applause. It was a Ceasusescu moment. The congressman had no clue the people of his district weren’t interested in the federal government concerning itself with the education of their children.



The politicians simply do not get it. The Declaration of Independence says governments are created to secure our rights to life, liberty, and the pursuit of happiness. In other words, to leave us the hell alone. That is what makes for American exceptionalism, despite President Obama’s claim that all nations are exceptional. No, they are not, not in the way America is.



As I write these words, across my desk comes a press release from Bloomberg telling me that 18- term Rep. Henry Waxman wants Congress to ban the use of smokeless tobacco in Major League Baseball dugouts. This is part of our communications problem. Read my lips, Henry: It is none of Congress’s business if baseball players want to use smokeless tobacco (or any other kind of weed, for that matter). And this is the encouraging thing about the Tea Party movement. It is made up of average Americans who are sick to death of politicians regulating, taxing, controlling, and limiting individual choice.



This bipartisan communications problem is also exemplified by a joint press conference held just before the start of the lamentable 111th Congress by Senate Majority Leader Harry Reid and Minority Leader Mitch McConnell. Said Reid, “Sen. McConnell and I believe … that we are going to work in a bipartisan basis … to solve the problems of the American people.” Whoa! See how simple the communications problem is? They think we sent them to Congress to solve our problems when we actually sent them there to see to it that we are left alone to solve our own problems.



Add to that the fact that many, if not most, of our problems have been created by Congress in the first place and we have the basis for a healthy peaceful revolution. Some 85 percent of Americans like their healthcare, so Congress shoves a government‐​mandated system down our throats. Taxes are way too burdensome, so Congress is contemplating a valueadded tax to add to our burden. We spend billions of dollars on wars in the Middle East for no rational reason. Climate change proves to be a wildly exaggerated issue, yet Congress still plans on raising taxes on energy to solve this non‐​existent problem. The list is long, and the frustration grows daily. Talk about a failure to communicate. According to a recent Pew Research Center Poll, 78 percent of Americans don’t trust the federal government. As Ronald Reagan famously put it, “The nine most terrifying words in the English language are: ‘I’m from the government and I’m here to help.’ ”
"
"Labor’s deputy leader has not ruled out the party supporting new coal developments, saying it would be a decision for the markets despite previously declaring it would be a “good thing” if the thermal coal market collapsed. In a somewhat difficult and at times awkward interview with David Speers on the ABC’s Insiders program, Richard Marles struggled to articulate the difference between Labor’s coal policy and the Coalition’s, stating “coal will continue to play a part within our economy for decades to come”, while demanding the government do more on climate policy.  Marles maintained public funds should not be used to subsidise coal and the market should be allowed to make its own decisions, but if a private company decided to push forward with a mine and gained the necessary approvals Marles said Labor would not stand in its way. “This is a matter for the market,” Marles said on the question of whether Labor proposed new coal projects. “… And the normal environmental approvals should apply. “… The normal process should play out … a Labor government is not going to put a cent into subsidising coal-fired power. And that is the practical question as to whether or not it happens.” Although he has maintained no taxpayer funds should be used to prop up the industry, a year ago Marles was more strident about the future of coal. “The global market for thermal coal has collapsed, and at one level that’s a good thing because what that implies is the world is acting in relation to climate change,” he told Sky News in February 2019. The Morrison government is attempting to balance competing forces within its party room by examining a proposal for a new coal-fired power station in north Queensland – a politically turbulent region for both major parties – while maintaining it is serious about reducing emissions. The government announced on Saturday a $4m feasibility study with Shine Energy into the proposed Collinsville coal-fired power plant. In an interview with Sky News, Dan Tehan attempted to suggest the study wasn’t about the government looking to support a new coal-fired plant, but about “providing cleaner, more efficient, lower emissions technology to provide base load funding into northern Queensland”. The move has raised eyebrows given the private sector’s hesitancy to move forward on coal-fired plants, despite pressure from conservative MPs. The recent Nationals leadership wobbles, which led to the Queensland senator and resources champion Matt Canavan’s return to the backbench, has seen those same MPs begin to agitate within the joint party room for the government to do more to support the coal industry, claiming climate change was not an issue in their electorates. ""Coal will continue to play a part within our economy for decades to come,"" @RichardMarlesMP tells @David_Speers on #Insiders #auspol pic.twitter.com/qnTyKCEdNA Following the May election loss, and the party’s backwards slide in Queensland, Labor has attempted to walk a fine line between its support for coal regions, led by the Hunter Valley MP Joel Fitzgibbon, and its push to do more on climate policy. Yes Adam, come to our regions and explain to our coal miners why you believe that, despite growing demand for their relatively efficient & modernising product in Asia, we can’t have both a cleaner Oz economy & a strong coal export industry @InsidersABC @GuardianAus #jobs pic.twitter.com/wcvPCKJx2F Asked to differentiate between Labor’s policy and what the government was doing on Sunday morning, Marles struggled to draw a distinction, instead returning to what he said were the government’s failures. Asked what the government could be doing and wasn’t, Marles said “you could start by having a proper energy policy”, but pressed on what that should include Marles said it was “a matter for them – they’ve been in power for seven years”. Marles settled on the need for a joint parliamentary approach to climate policy, and said the Labor party would once again look at supporting the national energy guarantee, a proposal that ultimately led to Malcolm Turnbull’s downfall as Liberal party leader. “We have been seeking bipartisanship for a long time in relation to this,” Marles said. “But to get bipartisanship, we actually need to have a side that we can talk to. Right now, we’re watching a whole lot of people having a war with each other inside their party room.”"
"New York is one of many cities whose mythical allure claims that the streets are paved with gold. Sadly, you are more likely to be treading on – or at least wading through – the remains of burgers, hot dogs, sweets, cookies, fries and more unmentionable sources of nutrients. Yet in among all that detritus is an awful lot of energy, a resource that could underpin a complex ecosystem. Food webs are a staple of ecology research, but usually explored in rain forests and coral reefs, ponds and savannahs. However a team at North Carolina State University has recently turned its attention to the much more dangerous terrain of Manhattan to find out if the insects living on and under the streets clear up a significant amount of the food litter – and whether the diversity of species makes any difference. Their results are published in the journal Global Change Biology. The precise relationship between resources and the diversity of flora and fauna in ecosystems has been the subject of intense research ever since the coming of the word “biodiversity” in the late 1980s. Ecologists were challenged to explain the role of species: does it matter how many there are, does the number of species affect the way ecosystems work, what do all these species do for us?  All this activity drives the natural ecosystems which keep us alive. Ecosystems are more productive, efficient and resilient the more species they contain, perhaps because different species carry out complementary roles or, however unwittingly, benefit the activities of others.  The North Carolina team set out to test whether the diversity of invertebrate street life affected the removal of food that had suffered “improper disposal” (a charming politeness runs through the whole study) in the parks and elongated traffic islands of Manhattan. To audit the pavement biodiversity, the team collected insects from among the leaf litter, with additional forays into other areas in search of ants. The rate of food clear-up was measured by putting out potato chips, cookies and hot dogs and seeing how much was left the following day.  Some of the food was protected by wire mesh, others not – so that larger creatures such as rats and pigeons could get in too, to allow for their impact. The precise brands of crisp, cookie and hot dog are detailed, each cut up into more appetising chunks.  This is important, allowing experimental replication with street food around the world. For example in Britain the late-night kebab might be a significant bio-geographical variation. The rate of food clear-up was compared to the overall diversity of invertebrates and the precise mix of species. Sadly, nowhere do the team members outline how they explained any of their activity to passing policemen. The speed with which food was removed proved startling. In the first run of the experiment using small chunks of food, 59% was gone within 24 hours. A second run using larger portions resulted in a 32% loss within a day. Whole cookies and chips … gone, chunks of hot dog … vanished.  The insect life on the traffic islands consumed supplies two to three times faster than the inhabitants of the parks. Life in the fast lane perhaps, or maybe the park life was more used to ice creams and sandwiches. In either locality, hot dogs were preferred to the light snacks.  In total the insects from the medians and traffic islands of two long Manhattan streets – Broadway and West St – could remove the equivalent of 600,000 potato chips per year. This could become a standard measure of invertebrate junk food ecosystem services.   The overall conclusion is that our invertebrate neighbours in the city make a notable contribution to the removal of litter. However the food clear-up was not affected by the diversity of species. More important was the presence of one species of ant, the perfectly named pavement ant, Tetramorium caespitum. Two to three times more food was removed where these particular ants were present. There is something particularly pleasing about it being the pavement ant. Not just the name; this ant is not a native New Yorker, but an immigrant from over a century ago, probably coming from Europe to the Big Apple.  Urban wildlife is often rather overlooked as a sorry mix of second-rate left-over habitats and dodgy aliens. However the city represents a whole new habitat, likely to become ever more widespread; a zoopolis, with a distinct and fascinating ecology. Where the streets are paved with last night’s food, these ants have certainly found their niche."
"There are five trillion pieces of plastic in the world’s oceans, weighing a total of 268,000 tonnes. That’s according to a paper  by an international team of scientists who took a substantial amount of data collected across the globe and merged it with ocean circulation models to come up with the staggering figure.  That sounds like a lot of plastic – it is a lot of plastic – but it isn’t distributed quite as most people would imagine. First of all, the way a Pacific Garbage Patch has been reported by some newspapers is largely a myth. No, you can’t see the garbage patch from space – the “garbage” in question is usually many small particles. We are told how these plastic particles now outnumber the tiny creatures at the bottom of the oceanic food web, and it all needs to be cleaned up as a priority. It is true that in some areas plastics do exceed plankton.  But this generally happens in areas with very few nutrients (oligotrophic) and hence low productivity, and which sit at the centre of large currents which focus debris and create these “patches”. The plastics in the ocean can remain as recognisable objects – fishing nets, plastic bottles, bags – but much of it mechanically breaks down to small particles, many a fraction of a millimetre across, which explains the large numbers involved. Though previous studies found plastic isn’t restricted to ocean gyres, the latest paper is one of the first to bring the research together to produce a global estimate. The major gyres of the oceans are found in the north and south Atlantic, the north and south Pacific, and in the Indian Ocean. These vast rotating vortices trap surface material at their centres, concentrating any debris.  However, the gyres aren’t closed off and, in the several years it takes a piece of plastic to complete a circuit, there is a 20-30% chance of it spinning out into other parts of the ocean. This results in the increasing levels of plastic beyond the gyres.   In 2010 I undertook some research in the Arctic Ocean, north of the Svalbard archipelago, the northernmost permanently inhabited place on earth.  As we were interested in monitoring plastics we went equipped with plankton nets to sample the region’s sea.   We didn’t need the nets. On a remote island, more than 1,000 km away from any moderately sized village or town we collected a large box full of plastic debris from one 10 square metre section of beach, which included the box itself. Our finds included plastic bags and bottles from the US, Canada, the UK, France, the Netherlands and Norway.  The ocean currents had transported the debris thousands of miles, and even managed to wrap several bags around the tusks of resident walruses on the island. This is a global problem and the researchers in the new study consider that there are many thousands of tonnes of fine particles unaccounted for which have either entered the food chain or reside in the ocean sediments.  The figure of 268,940 tonnes is quite low to my mind and can easily be put into perspective: if the material was distributed evenly across the ocean then it equates to three particles in every million tonnes of seawater.  The reality is that the plastic is concentrated in the surface layers (or the sea bed sediments) and is globally on the increase, by definition – it is not breaking down and it is still entering the environment.  More importantly there is nothing we can do with what is already there.   I am often asked why don’t we just filter it out of the oceans?  The figures above should tell you why – it is an impossible task, and even if we could we’d take out all of the plankton as well.  All we can do for now is reduce, recycle and monitor. The big question, which still remains largely unanswered, is: what happens to this material once it enters the food chain?"
nan
"
Share this...FacebookTwitterThe southwestern US was nearly a desert from about 9000 to 5000 years ago, when Holocene peaks in aridity, surface temperature, and wildfire rates occurred. Arctic sea ice was at its lowest extent of the Holocene during these years.

Image Source: Lachniet et al., 2020
A new extensively-referenced study (Lachniet et al., 2020) reviewing many dozens of climate records from across western North America has determined there is nothing unprecedented or even unusual about the modern climate for this region.
CO2 checked in at about 265 ppm during the Early and Middle Holocene, but today’s associated >400 ppm CO2 climate is much cooler and wetter, and there is much more Arctic sea ice present today.
During the Early to Middle Holocene (approximately  9 to 5 thousand years ago) this region could be characterized like this:
·2°C warmer, the warmest of the Holocene


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




·150 meters higher tree lines on mountains, indicating greater warmth
·5-17 m lower lake/pond levels
·“complete desiccation” or desert-like conditions in some areas
·peak wildfires
·“a warmer Arctic, reduced sea ice extent” – the lowest extent of the Holocene

Image Source: Lachniet et al., 2020


		jQuery(document).ready(function(){
			jQuery('#dd_c46e1d5aedc7d8b197bc83e2f0539a56').on('change', function() {
			  jQuery('#amount_c46e1d5aedc7d8b197bc83e2f0539a56').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe equatorial Pacific is now showing a distinct band of cool surface water developing. Many forecasters have already written the obituary for the now departing El Nino, which pushed global surface temperatures up by some 0.8°C over the last few months.
Developing La Nina
Source: unisys.com
The newly developing La Nina will probably erase much of that in the months ahead. For example, Accu Weather meteorologist Joe Bastardi predicts cooling over the next year or two. But he does say this year’s Arctic ice melt may challenge the record low set in 2007. One more but – he says that after that the ice will recover over the next 2 years and reach normal levels. So all you obsessed global warmists out there, milk it while you can. It’s your last chance (if Joe’s right).
Read the latest Wrap-Up from the Australian Bureau of Meteorology.

Share this...FacebookTwitter "
"

A new report from the International Energy Agency is sparking headlines across the media. “Global carbon dioxide emissions soared to record high in 2012” proclaimed USA Today; The Weather Channel led “Carbon dioxide emissions rose to record high in 2012”; and the Seattle Post‐​Intelligencer added “The world pumped a record amount of carbon dioxide in the atmosphere in 2012.”   
  
  
The figure below (taken from the IEA summary) provides the rest of the story.   
  
  
It shows a breakdown of the change in carbon dioxide emissions from 2011 to 2012 from various regions of the globe.   






  
  
  
Notice that the U.S. is far and away the leader in reducing carbon dioxide (CO2) emissions, while China primarily is responsible for pushing global CO2 emissions higher. In fact, CO2 emissions growth in China more than offsets all the CO2 savings that we have achieved in the U.S.   
  
  
This will happen for the foreseeable future. Domestic actions to reduce carbon dioxide emissions will not produce a decline in the overall atmospheric carbon dioxide concentration. The best we can hope to achieve is to slow the rate of growth of the atmospheric concentration—an effect that we can only achieve until our emissions are reduced to zero. The resulting climate impact is small and transient.   
  
  
And before anyone goes and getting too uppity about the effectiveness of “green” measures in the U.S., the primary reason for the U.S. emissions decline is the result of new technologies _from the fossil fuel industry_ that are leading to cheap coal being displaced by even cheaper natural gas for the generation of electricity. As luck would have it, the chemistry works out that that burning natural gas produces the same amount of energy for only about half of the CO2 emissions that burning coal does.   
  
  
A new report from the U.S. Energy Information Administration estimates that as a result of these new technologies (e.g., hydraulic fracturing and horizontal drilling), globally, the technologically recoverable reserves of natural gas are nearly 50% greater than prior to their development.   
  
  
Currently, the U.S. is the leader in the deployment of these technologies, and the effects are obvious (as seen in the figure above). If and when more countries start to employ such technologies to recover natural gas, perhaps the growth in global carbon dioxide emissions will begin to slow (as compared to current projections).   
  
  
Considering that possibility, along with the new, lower estimates for how sensitive the global average temperature is to carbon dioxide emissions, and the case for alarming climate change (and a carbon tax to try to mitigate it) is fading fast.
"
"A major change is coming to our skies. From next March, pilots will be able to determine their own routes and plan to fly direct from point to point.  Currently, flights must plan to follow explicitly defined corridors. These are rather like roads in the sky, up to 10 miles wide. Under the new scheme, flights will still be subject to air traffic control from Prestwick Centre in the West of Scotland, but pilots will be considerably more free to plan the specific route that they take.  The move by the UK’s main air-traffic control agency, NATS, to switch to what is known as “route free airspace” will initially apply to an area between Skye and the Isle of Man that carries about 450 planes each day.  Route-free airspace will allow aircraft to take shorter and more direct routes. This is expected to bring multiple benefits including better flight efficiency, greater cost-effectiveness, reduced engine running time, reduced fuel consumption and resulting environmental gains.  How it will work in practice is that the pilot or his airline will determine the best flight path using dedicated software and then submit a flight plan and route before take-off or before entering UK airspace. This can be done online or via several other methods including radio.  Route-free airspace has been made possible by technologies such as Automatic Dependent Surveillance-Broadcast (ADS-B). This makes use of satellite positioning to allow aircraft to broadcast their precise location every few seconds so that pilots of other aircraft are able to plan their routes accordingly.  This means that aircraft can make better use of available space and fly closer together. It removes the need for the tightly restricted routes that are used at present. ADS-B is being fitted to an increasing number of aircraft. It is mandatory in Australia. It will become mandatory for most aircraft in Europe from 2017, and from 2020 in the USA. The idea of pilots planning routes without central coordination might raise safety concerns, but in practice, aircraft will still remain under the supervision of air-traffic control. Our fixed routes were indeed originally designed to maintain safety, but they employ decades-old systems that are based on radar and no longer necessary.  You might also think that fixed routes could lead to greater delays as aircraft manoeuvre around each other, but as routes will be actively monitored and predicted up to 25 minutes ahead of time, such issues are actually reduced. Neither will the routes result in a noise nuisance for people on the ground, since route-free airspace only applies to airspace above 25,000 feet (4.8 miles). NATS plans to extend the trial to the rest of Scotland and parts of the North Sea from 2017 and has a long-term strategy to establish all upper airspace in the UK as route-free (making this work in much of England and Wales will admittedly take rather more planning because the airspace is already very congested). This is being promoted as part of the Single European Sky initiative, which aims to modernise Europe’s air traffic control system. It has the target of a 10% reduction in the effect of aircraft on the environment by 2020, against a background of increasing passenger traffic.  The pan-European air-traffic coordination agency Eurocontrol estimates that while the number of flights will have increased by 50% between 2012 and 2035, deploying route-free airspace over central Europe at night and weekends will reduce flight distance by the equivalent of 1.16m km per year. Route-free airspace has already been implemented in Sweden, Portugal and Ireland. It is planned or partially implemented in the rest of Scandinavia, Italy and central and eastern Europe. It also forms part of the Federal Aviation Authority’s NextGen programme in the United States. However, despite the obvious benefits from going route-free, some argue that rising air-traffic volumes is not something that we will be able to support indefinitely. The biggest potential barriers are probably infrastructure on the ground (especially airports themselves, which consume large areas of land) and rising costs of jet fuel.  Particularly around major cities, airspace is already crowded. While improvements to planning and control could make better use of the space, there is only so much to be done before no further aircraft can fit in the sky with a safe separation distance. And though the answer would presumably be to improve road infrastructure and high-speed rail instead, this has its own issues and controversies. And we can’t just rely on route-free airspace to deal with the environmental issues around aircraft. Eurocontrol’s central forecast is that by 2035, total emissions might have fallen slightly despite the expected rise in air traffic, but that assumes that fuel efficiency keeps improving. This is by no means certain.  Air-traffic control therefore needs to focus on other ways of improving pollution, such as optimising taxi routes at airports and flight sequencing. This stems from the fact that short flights are more polluting than long-distance ones because of the impact of take-off and landing compared to the distance travelled. Indeed, it is interesting to note that in contrast with the liberalisation of air routes, there is a possible move towards more centralised control of taxi routes as automated systems for optimising routes are considered.  For the same reason, it is important that new technologies in computational search and mathematical modelling are fully explored and investigated in the context of ensuring that our air-traffic systems and resources are being used as efficiently as possible. So long as we see these moves towards making large amounts of airspace route free as just one contribution to a wider environmentally-focused modernisation of the aviation industry, we should ultimately reach the right destination."
"

In the early 1990s, Rep. Dick Armey (RTX) proposed a flat tax. He would have junked the Internal Revenue Code and replaced it with a system designed to raise revenue in a much less destructive fashion. The core principles were to tax income at one low rate, to eliminate double taxation of saving and investment, and to wipe out the special preferences, credits, exemptions, deductions, and other loopholes that caused complexity, distortions, and corruption.



The flat tax never made it through Congress, but it’s been adopted by more than a dozen other countries since 1994.



It’s unfortunate that the United States is missing out on the tax reform revolution. Instead of the hundreds of forms demanded by the current tax system, the Armey flat tax would have required just two postcards. Households would have used the individual postcard to pay a 17 percent tax on wages, salary, and pensions, though a generous family‐​based allowance (more than $30,000 for a family of four) meant that there was no tax on the income needed to cover basic expenses.



Taxes on other types of income would have been calculated using the second postcard, which would have been filed by every business regardless of its size or structure. Simply stated, there would have been a 17 percent tax on net income, which would have been calculated by subtracting wages, input costs, and investment expenditures from total receipts.



While the simplicity and low tax rate were obvious selling points, the flat tax also eliminated various forms of double taxation, ending the bias against income that was saved and invested. In other words, the IRS got to tax income only one time. The double tax on dividends would have been completely eliminated. The death tax also was to be wiped out, as was the capital gains tax, and all saving would have received “Roth IRA” treatment.



Another key feature of the flat tax was the repeal of special tax breaks. With the exception of a family‐​based allowance, there would have been no tax preferences. Lawmakers no longer would have been able to swap loopholes for campaign cash. It also would have encouraged businesses to focus on creating value for shareholders and consumers instead of trying to manipulate the tax code. Last but not least, the flat tax would have created a “territorial” system, meaning that the IRS no longer would have been charged with taxing Americans on income earned—and subject to tax—in other jurisdictions.



Proponents correctly argued that a flat tax would improve America’s economic performance and boost competitiveness. And after Republicans first took control of Congress, it appeared that real tax reform was possible. At one point, the debate was about, not whether there should be tax reform, but whether the Internal Revenue Code should be replaced by a flat tax or a national sales tax (which shared the flat tax’s key principles of taxing economic activity only one time and at one low rate).



Notwithstanding this momentum in the mid‐​1990s, there ultimately was no serious legislative effort to reform the tax system. In part, that was because of White House opposition. The Clinton administration rejected reform, largely relying on class‐​warfare arguments that a flat tax would benefit the so‐​called rich. But President Clinton wasn’t the only obstacle. Congressional Democrats were almost universally hostile to tax reform, and a significant number of Republicans were reluctant to support a proposal that was opposed by well‐​connected interest groups.



 **The Flat Tax around the World**



One of the stumbling blocks to tax reform was the absence of “real‐​world” examples. When Armey first proposed his flat tax, the only recognized jurisdiction with a flat tax was Hong Kong. And even though Hong Kong enjoyed rapid economic growth, lawmakers seemed to think that the then–British colony was a special case and that it would be inappropriate to draw any conclusions from it about the desirability of a flat tax in the United States.



Today, much of the world seems to have learned the lessons that members of Congress didn’t. Beginning with Estonia in 1994, a growing number of nations have joined the flat tax club. There are now 17 jurisdictions that have some form of flat tax, and two more nations are about to join the club. As seen in Table 1, most of the new flat tax nations are former Soviet republics or former Soviet bloc nations, perhaps because people who suffered under communism are less susceptible to class‐​warfare rhetoric about “taxing the rich.”





**Flat Tax Lessons**



The flat tax revolution raises three important questions: Why is it happening? What does the future hold? Should American policymakers learn any lessons?



The answer to the first question is a combination of principled leadership, tax competition, and learning by example. Flat tax pioneers such as Mart Laar (prime minister of Estonia), Andrei Illarionov (chief economic adviser to the president in Russia), and Ivan Miklos (finance minister in Slovakia) were motivated at least in part by their understanding of good tax policy and their desire to implement pro‐​growth reforms. But tax competition also has been an important factor, particularly in the recent wave of flat tax reforms. In a global economy, lawmakers increasingly realize that it is important to lower tax rates and reduce discriminatory burdens on saving and investment. A better fiscal climate plays a key role both in luring jobs and capital from other nations and in reducing the incentive for domestic taxpayers to shift economic activity to other nations.



Moreover, politicians are influenced by real‐​world evidence. Nations that have adopted flat tax systems generally have experienced very positive outcomes. Economic growth increases, unemployment drops, and tax compliance improves. Nations such as Estonia and Slovakia are widely viewed as role models since both have engaged in dramatic reform and are reaping enormous economic benefits. Policymakers in other nations see those results and conclude that tax reform is a relatively risk‐​free proposition. That is especially important since international bureaucracies such as the International Monetary Fund usually try to discourage governments from lowering tax rates and adopting pro‐​growth reforms.



The answer to the second question is that more nations will probably join the flat tax club. Three nations currently are pursuing tax reform. Albania is on the verge of adopting a low‐​rate flat tax, as is East Timor (though the IMF predictably is pushing for a needlessly high tax rate). A 15 percent flat tax has been proposed in the Czech Republic, though the political outlook is unclear because the government does not have an absolute majority in parliament.



It is also worth noting that countries with flat taxes are now competing to lower their tax rates. Estonia’s rate already is down from 26 percent to 22 percent, and it will drop to 18 percent by 2011. The new prime minister’s party, meanwhile, wants the rate eventually to settle at 12 percent. Lithuania’s flat rate also has been reduced, falling from 33 percent to 27 percent, and is scheduled to fall to 24 percent next year. Macedonia’s rate is scheduled to drop to 10 percent next year, and Montenegro’s flat tax rate will fall to 9 percent in 2010—giving it the lowest flat tax rate in the world (though one could argue that places like the Cayman Islands and the Bahamas have flat taxes with rates of zero).



The continuing shift to flat tax systems and lower rates is rather amusing since an IMF study from last year claimed: “Looking forward, the question is not so much whether more countries will adopt a flat tax as whether those that have will move away from it.” In reality, there is every reason to think that more nations will adopt flat tax systems and that tax competition will play a key role in pushing tax rates even lower.



 **Could It Happen Here?**



For American taxpayers, the key question is whether politicians in Washington are paying attention to the global flat tax revolution and learning the appropriate lessons. There is no clear answer to this question. Policymakers certainly are aware that the flat tax is spreading around the world. Mart Laar, Andrei Illarionov, Ivan Miklos, and other international reformers have spoken several times to American audiences. President Bush has specifically praised the tax reforms in Estonia, Russia, and Slovakia. And groups like the Cato Institute are engaged in ongoing efforts to educate policymakers about the positive benefits of global tax reform.



But it is important also to be realistic about the lessons that can be learned. The United States already is a wealthy economy, so it is very unlikely that a flat tax would generate the stupendous annual growth rates enjoyed by nations such as Estonia and Slovakia. The United States also has a very high rate of tax compliance, so it would be unwise to expect a huge “Laffer Curve” effect of additional tax revenue similar to what nations like Russia experienced.



It is also important to explain to policymakers that not all flat tax systems are created equal. Indeed, none of the world’s flat tax systems is completely consistent with the pure model proposed by Professors Robert Hall and Alvin Rabushka in their book, _The Flat Tax_. Nations such as Russia and Lithuania, for instance, have substantial differences between the tax rates on personal and corporate income (even Hong Kong has a small gap). Serbia’s flat tax applies only to labor income, making it a very tenuous member of the flat tax club. Although information for some nations is incomplete, it appears that all flat tax nations have at least some double taxation of income that is saved and invested (though Estonia, Slovakia, and Hong Kong get pretty close to an ideal system). Moreover, it does not appear that any nation other than Estonia permits immediate expensing of business investment expenditures. (The corporate income tax in Estonia has been abolished, for all intents and purposes, since businesses only have to pay withholding tax on dividend payments.)



Policymakers also should realize that a flat tax is not a silver bullet capable of solving all of a nation’s problems. From a fiscal policy perspective, for instance, the Russian flat tax has been successful. But Russia still has many problems, including a lack of secure property rights and excessive government intervention. Iraq is another example. The U.S. government imposed a flat tax there in 2004, but even the best tax code is unlikely to have much effect in a nation suffering from instability and violence.



With all these caveats, the flat tax revolution nonetheless has bolstered the case for better tax policy, both in America and elsewhere in the world. In particular, there is now more support for lower rates instead of higher rates because of evidence that marginal tax rates have an impact on productive behavior and tax compliance. Among developed nations, the top personal income tax rate is 25 percentage points lower today than it was in 1980. Similarly, the average corporate tax rate in developed nations has dropped by 20 percentage points during the same period. Those reforms are not consequences of the flat tax revolution. Margaret Thatcher and Ronald Reagan started the move toward less punitive tax rates more than 25 years ago. But the flat tax revolution has helped cement those gains and is encouraging additional rate reductions.



Moreover, there is now increased appreciation for reducing the tax bias against income that is saved and invested. Indeed, Sweden and Australia have abolished death taxes, and Denmark and the Netherlands have eliminated wealth taxes. Other nations are lowering taxes on capital income, much as the United States has reduced the double taxation of dividends and capital gains to 15 percent. And although the United States is a clear laggard in the move toward simpler and more neutral tax regimes, the flat tax revolution is helping to teach lawmakers about the benefits of a system that does not penalize or subsidize various behaviors.



The flat tax revolution also suggests that the politics of class warfare is waning. For much of the 20th century, policymakers subscribed to the notion that the tax code should be used to penalize those who contribute most to economic growth. Raising revenue was also a factor, to be sure, but many politicians seem to have been more motivated by the ideological impulse that rich people should be penalized with higher tax rates. If nothing else, the growing community of flat tax nations shows that class‐​warfare objections can be overcome.



 **Building a High‐​Tax Cartel**



Although the flat tax revolution has been impressive, there are still significant hurdles. Most important, international bureaucracies are obstacles to tax reform, both because they are ideologically opposed to the flat tax and because they represent the interests of high‐​tax nations that want tax harmonization rather than tax competition. The Organization for Economic Cooperation and Development, for instance, has a “harmful tax competition” project that seeks to hinder the flow of labor and capital from high‐​tax nations to low‐​tax jurisdictions. The OECD even produced a 1998 report stating that tax competition “may hamper the application of progressive tax rates and the achievement of redistributive goals.” In 2000 the Paris‐​based bureaucracy created a blacklist of low‐​tax jurisdictions, threatening them with financial protectionism if they did not change their domestic laws to discourage capital from nations with oppressive tax regimes.



The OECD has been strongly criticized for seeking to undermine fiscal sovereignty, but its efforts also should be seen as a direct attack on tax reform. Two of the key principles of the flat tax are eliminating double taxation and eliminating territorial taxation. These principles, however, are directly contrary to the OECD’s anti‐​tax competition project—which is primarily focused on enabling high‐​tax nations to track (and tax) flight capital. That necessarily means that the OECD wants countries to double tax income that is saved and invested, and to impose that bad policy on an extraterritorial basis.



The OECD is not alone in the fight. The European Commission also has a number of anti‐​tax‐​competition schemes. The United Nations, too, is involved and even has a proposal for an International Tax Organization. All of those international bureaucracies are asserting the right to dictate “best practices” that would limit the types of tax policy a jurisdiction could adopt. Unfortunately, their definition of best practices is based on what makes life easier for politicians rather than what promotes prosperity.



Fortunately, these efforts to create a global tax cartel have largely been thwarted, and an “OPEC for politicians” is still just a gleam in the eyes of French and German politicians. That means that tax competition is still flourishing, and that means that the flat tax club is likely to get larger rather than smaller.



 _This article originally appeared in the July/​August 2007 edition of_Cato Policy Report.



<em><a href=”/people/daniel-mitchell”>Daniel J. Mitchell</a> is a senior fellow at the Cato Institute.</em>
"
"

Europe is slowly disarming. For decades the continent could rely on America to fill the gap. No longer. That realization has given France pause. Maybe other European states also will start taking their security responsibilities more seriously.



A new report from the European Union’s Institute for Security Studies acknowledged an unpleasant reality—centuries of the West’s and especially Europe’s “dominance are currently giving way to a more multipolar and less governable world system.” That wouldn’t be such a problem if the change was not combined with diminishing military capabilities, again especially in Europe.





There’s no reason for the U.S. to pull Europe’s chestnuts out of the fire.



Noted ISS: “Failing to act, therefore, means that a mixture of acute budgetary pressures, lack of investment in research development, and widespread reluctance to make the maintenance of effective armed forces a political priority could cause additional reductions in EU military capacity as well as a potential exodus of the defense industry and a loss of technological leadership.” The Europeans are spending ever less, with “the budget cuts carried out so far have been made without any coordination and consultation among allies.” Moreover, European governments are spending unwisely, emphasizing personnel and land‐​based facilities, for instance.



A continuing reduction in capabilities seems likely if not quite inevitable because Europe no longer faces any serious, let alone existential, threats. Russia is a poor replacement for the Soviet Union. It is impossible to build a plausible scenario for Russian troops threatening Warsaw or Berlin, let alone Paris or London. Moscow still might beat up on its neighboring constituent republics, such as Georgia, but the latter actually started their war.



China might become a peer competitor of America, but it has no European ambitions. Balkan instability is no substitute for potential aggression from whomever. North Africa and the Middle East generate continual geopolitical complications, but getting involved usually creates even greater problems. Even a nuclear Iran—an unpleasant prospect, to be sure—seems unlikely to target Europe. About all that’s left for Europe’s militaries are distant nation building, anti‐​pirate sea patrols, and play‐​acting like a _Weltmacht_.



EU leaders still might talk about creating a continental foreign policy and military, and national politicians still might want armed forces capable of doing more than providing an honor guard for foreign dignitaries, but European peoples exhibit little interest in paying the resulting bill. Spending more efficiently and collaborating more extensively would help, but the continent’s ongoing Euro crisis, recession, and heavy indebtedness all encourage further retrenchment. One visiting NATO official told a private, off‐​the‐​record gathering in Washington, “There is no chance for budget increases, not even for keeping spending levels as they are.” Earlier this year Rasmussen declared, “There is a lower limit on how little we can spend on defense.” But what is it?



This is a prescription for eventual European disarmament, but a slight sign of hope is flickering in France. Although modern French presidents don’t look much like reincarnations of Emperor Napoleon, they are not shrinking violets internationally. Both Presidents Nicolas Sarkozy and Francois Hollande had wars they wanted to fight—Libya and Mali, respectively. However, they both found Paris to be unable to fight without assistance, primarily from America.



Europe’s rising enthusiasm for war is ironic. Observed Philip Stephens in the _Financial Times_ : “Europeans have caught the interventionist bug just as the U.S. has shaken it off.”



However, France’s financial difficulties created pressure for additional cuts in military outlays. The Hollande government recently released its defense review, known as the _livre blanc_. Although the government reduced its rapid‐​deployment forces, it “opted to keep France’s air, ground and sea capabilities, while freezing defense budgets over six years,” noted the _Economist_. Outlays will shrink in real terms and as a percentage of GDP, but “dark talk of the loss of 50,000 jobs proved unfounded. The planned yearly cuts will be smaller than under the previous president, Nicolas Sarkozy. France will maintain its capability for expeditionary warfare, and boost special forces.”



One reason for this is Gallic pride, even ego. President Hollande explained: “France’s destiny is to be a global nation and our duty is to guarantee not only our own security but that of our allies and partners.” In doing so, he added, “France wants to maintain its ability to react alone.” How could it be any other way?



Opposition legislators complained that the proposed force was inadequate for such a role. Vincent Desportes, former director of a military school, told the _New York Times_ that the plan “makes France a really minor actor in coalition operations.” However, a budget increase was inconceivable in today’s economic climate.



The second reason is more significant. Paris apparently realized that if it is going to continue to be a “global nation,” it no longer could expect as much help from across the Pond. As the _livre blanc_ delicately put it, Americans will “prove more selective in their overseas engagements.” This led to one conclusion. Noted the _Economist_ : “One arresting element is the recognition that France may have to step up militarily in the Mediterranean and Africa as America pulls back.”



That requires not just sufficient forces but the right forces. Defense Minister Jean‐​Yves Le Drian called some of his nation’s deficiencies “incomprehensible,” requiring Paris to spend more on aerial refueling and other specialties. Said Le Drian, new investments “seem to me inevitable, like intelligence and special forces.”



This may be a seminal moment for European defense policy. Explained Francois Heisbourg of the Foundation for Strategic Research: “Planning to operate in a world where the Americans will be in only a supporting role changes everything. It is essential that we get the right kit to do it.”



Hallelujah!



It long has been obvious that Washington’s promise to protect prosperous and populous allies created a disincentive for them to do more for their own defense. During the Cold War the Europeans routinely violated their promises to hike military expenditures, even in the face of the numerically superior Red Army. Japan hid behind its pacifist constitution and kept military (“self defense”) outlays below 1 percent of GDP. Since the mid‐​1990s South Korea has skimped on its armed‐​forces budgets while providing the North with $10 billion worth of assistance as part of the Sunshine Policy—even as North Korea threatened to turn Seoul into a “lake of fire.”



A lack of capacity did not stop Britain and France from pushing for war with Libya, though they received only limited support from other European states and had to go to Washington for additional assistance. However, American officials have demonstrated far greater reluctance to join the Syrian civil war. As the U.S. further reduces both capabilities and obligations, even Paris realizes that Washington might say no to its next war proposal.



Which means France must do more than it really wished. But Paris apparently will do what it must.



U.S. policymakers should learn from this experience. Instead of bashing the Europeans, insisting that they spend more when they see no compelling reason to do so, Washington should simply shed the burden of Europe’s defense. Inform America’s long‐​time friends and allies that the cheap ride is over. Then let the Europeans decide how much they want to spend to defend what. And allow them to bear the consequences.



The same goes for the Balkans, Mediterranean, Central Asia and Middle East. Whether the issue is Kosovo, Libya, Georgia or Syria, absent a compelling interest for America military action should be up to Brussels, or Paris, London and Berlin. If they decide not to act, no worries. There’s no reason for the U.S. to pull Europe’s chestnuts out of the fire.



There’s still substantial room for security cooperation. And Washington obviously could help the Europeans become militarily self‐​sufficient. But the time for a U.S.-dominated alliance is over.



Economists long have told us that incentives matter. France’s behavior proves that they do. When Paris believed that it could rely on Uncle Sucker, the former did one thing. When the French realized that the Yanks really might not be coming, they did something different. Washington needs to send the same message to the rest of its defense dependents.
"
"
Share this...FacebookTwitterThe prominent German online news magazine  FOCUS  reports that 2010 may set a new NASA high temperature record. The cause of the recent warmth is El Nino. But FOCUS then throws ice-cold water on any warmist dream of an overheating planet, at least for the next few years, and writes that scientists believe: “Womöglich aber sind die warmen Zeiten für unseren Globus bald vorüber”.
In English:
Quite possibly, the warm times for the planet will soon be over.
The FOCUS report looks at three factors, which I present in 3 parts. 
Part 1: La Nina
FOCUS first zeroes in on La Nina, and quotes AccuWeather meteorologist Joe Bastardi:
There are wild cards in the climate system that have changed the previous climate events. Now we’ve got a weak solar cycle and the prospect of increased volcanic activity. Together with a La Nina, it all could be a troublesome triple whammy.
FOCUS also quotes Joe D´Aleo of TV Weather Channel:
We’ll have La Nina conditions before the summer is over, and it will intensify further through the fall and winter. Thus we’ll have cooler temperatures for the next couple of years.
Part 2: Solar Activity
The next big factor is the sun, which has worried a number of scientists over the last couple of years. It refuses to start-up with a new cycle. 2008 had 266 spotless days and 2009 had 261.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




FOCUS writes:
Now some scientists fear the solar slumber could herald in a new Little Ice Age. This period, which extended from the 15th to the 19th century, was characterised by bitter cold winters and cool, wet summers which left grains and crops rotting in the fields.
FOCUS magazine then acknowledges the Maunder and Dalton Minimums, thus indirectly refuting Mann’s version of climate history. The German media is waking up!
FOCUS then quotes Joe D’Aleo:
If the number of spots does not climb over 40 or 50 during the next maximum, which would mean a low level of solar energy, then we have to reckon with much lower temperatures in the coming years.
Part 3: Volcanoes
Volcanoes in Iceland are coming alive. So far the ash clouds have been too small to have any effects on the climate. The real risk, however, is that it may be a foreboding of something much worse to come – the eruption of the mighty neighbouring Katla volcano. Katla has a far more immense chamber of magma. It erupts on average every 70 years and in tandem with Eyjafjalla. The last Katla eruption was in 1918, thus making an eruption overdue.
According to Joe Bastardi:
Katla could be a game-changer. If it erupts and throws ash and sulfur particles into the stratosphere, then the global temperature will plummet.
The triple whammy of La Nina, low solar activity and increased volcanic activity all acting together would certainly put global warming on the back burner for a while. But some scientists, like Prof. Mojib Latif of the University of Kiel, insist that warming will resume once the cooling factors fade off, and that global temperature increases of 5°C by the end of the century cannot be excluded.
In the meantime, get ready for cooling.
Share this...FacebookTwitter "
"Throughout her decade-long modelling career, Arizona Muse has walked catwalks for Chanel, Prada and Tom Ford, and starred in advertising campaigns for Karl Lagerfeld and Louis Vuitton. Now, however, she is starring in a new video for the climate activist group Extinction Rebellion, challenging viewers to “share, rewear, relove your clothing” and “to rebel against your clothes”. Because, she says to camera, “this season’s must-have is the continuation of life on Earth”. The video, which is released today, before London fashion week, features animals on the verge of extinction, as well as ice caps melting and wildfires raging. It is the latest attempt by the fashion wing of Extinction Rebellion to challenge an industry that, as Muse puts it, “is in crisis”.  Every year, global emissions from textile production outweigh the carbon footprint of international flights and shipping combined, according to the Ellen MacArthur Foundation. Between 80 and 100 billion pieces of clothing are made, with £140 million worth of clothing estimated to go into landfill in the UK alone. The fashion industry is also contributing to deforestation and water scarcity. The new video focuses on consumer responsibility, with Muse explaining: “When we are buying clothes we are making choices about the future of the planet. We’re talking about how you and I are all contributing to climate change.”  This week in London, the fashion industry is flying in to show off the designs they have for us next season. But all I can think of is what the seasons of the future might look like. #rebel #rewear #actnow @extinctionrebellion Director: Heydon Prowse Producers: Sarah Creswell & Sara Arnold DOP: Toby Lloyd A post shared by  Arizona Muse (@arizona_muse) on Feb 10, 2020 at 9:20am PST She’s keen not to make the video about shaming, however. Speaking over the phone, Muse readily admits to not having a fully sustainable wardrobe herself, and sometimes takes to Instagram to rank her outfits’ green credentials. Today her jumper (regenerated cashmere), jeans (not organic) and shoes (not vegetable tan leather) score her one out of three. The 31-year-old British-American model, whom Anna Wintour once described as the “new face of American fashion”, is hoping the video reaches “anyone who buys clothes … and anyone who makes clothes. Because clothes are a really big part of our problem today, we are making them in a way that is completely destructive to the environment and societies and people, and we need to seriously rethink this in a very rapid way.” Muse first became interested in sustainable fashion five years ago after realising she knew nothing about where the clothes she was modelling came from or how they were made. In the early days of her sustainable fashion journey, she recalled sitting at fashion events and knowing she had “maybe one or two minutes to talk about sustainability before my seat partner would turn in the other direction”. Now, she says, “people are actually interested”. Educating herself, via documentaries and reports from the Intergovernmental Panel on Climate Change, she became “fascinated by the way the whole supply chain works.” Yet she is not immune to criticism for being a part of the industry that she is calling out. She has been asked previously how she feels about modelling for brands that are not sustainable. Her answer, she says, is simple: “I feel so lucky that I have an access point to these brands. I get to go and meet the designers and CEOs, and guess what I do all day long when I’m on set? I’m chit-chatting about sustainable fashion.” During London fashion week last September, XR staged protests and put on a funeral to grieve for the industry. For many, it seemed a radical move, but there are examples of fashion weeks revolutionising, without that kind of external pressure, in the name of climate crisis. Earlier last year the Swedish Fashion Council announced it would cancel Stockholm fashion week to explore more sustainable options, and last month Copenhagen fashion week announced “radical” new sustainability goals. This time around, XR is planning more rebellion. “XR has an amazing ability to keep the climate crisis looming large in the imagination of the public and to get the right people talking,” says Muse. “I am so hopeful for fashion’s future and I don’t believe that fashion week should be cancelled. But it definitely needs some massive changes. During fashion week, there is an immense pressure on supply chains that leads to more emissions, and there is so much waste – from extravagant set designs to small things like the thousands of old coat hangers that take centuries to break down, giving off terrible toxins and endangering wildlife. We also need to ensure that more is done to highlight the amazing work of those brands making clothes that don’t harm the planet or the people who make them.” Beyond fashion week, Muse is hopeful that the industry can change for the better. “I see greenwashing, but I also see companies hiding behind the big excuse that ‘we can’t move that quickly, we have so many things to consider’. I think they’re considering margins over the health and wellbeing of the planet and society and I’d really like to see that ending.” Across the industry, steps are being taken to try to limit fashion’s detrimental impacts on the planet. For instance, in August last year, 32 companies representing about 150 brands signed the Fashion Pact at the G7 summit promising to cut greenhouse gas emissions to zero by 2050. In September, the luxury conglomerate Kering announced a bid to become carbon neutral. Even so, there have been criticisms of such efforts and Muse is wary of the current focus: “Brands are setting targets – by 2050, we’re going to be carbon neutral, say. I think it would be much more effective to say: ‘Starting right now we’re going to do everything we can to lower our emissions.’” A future target she says, “especially one that doesn’t have a precise action plan of how to get there, a roadmap, actually lets you relax. I don’t think that’s going to get us where we need to go – I think we need to feel urgency.”"
"
Looks like its support is splintering:
Without widespread corporate support, passage of the bill – already a long shot at best – becomes even more unlikely this year. President Bush remains opposed. House Democrats have been slow to act.
From CNN Money.
According to the WSJ even Hillary and McCain are likely to stay away from it. Voting to increase your local energy prices due to a flawed cap and trade carbon tax scheme which will create 5 new government bureaucracies is never a good thing for somebody trying to get elected.
Even with chances of passage dwindling, write your senator to tell them how you feel about it.
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ed9440e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"There are many complex reasons why people decide not to accept the science of climate change. The doubters range from the conspiracy theorist to the sceptical scientist, or from the paid lobbyist to the raving lunatic.   Climate scientists, myself included, and other academics have strived to understand this reluctance. We wonder why so many people are unable to accept a seemingly straight-forward pollution problem. And we struggle to see why climate change debates have inspired such vitriol. These questions are important. In a world increasingly dominated by science and technology, it is essential to understand why people accept certain types of science but not others.    In short, it seems when it comes to climate change, it is not about the science but all about the politics. Back in the late 1980s and early 1990s differing views on climate science were put down to how people viewed nature: was it benign or malevolent? In 1995 leading risk expert John Adams suggested there were four myths of nature, which he represented as a ball on different shaped landscapes.   Different personality types can be matched on to these different views, producing very different opinions about the environment. Climate change deniers would map on to number one, Greenpeace number two, while most scientists would be number three. These views are influenced by an individual’s own belief system, personal agenda (either financial or political), or whatever is expedient to believe at the time.     However, this work on risk perception was ignored by mainstream science because science up to now operates on what is called the knowledge deficit model.  This suggests that people do not accept the science because there is not enough evidence; therefore more needs to be gathered.     Scientists operate in exactly this way, and they assume wrongly the rest of the world is equally rational and logical.  It explains why over the past 35 years a huge amount of work gone into investigating climate change – even though, despite many thousands of pages of IPCC reports, the weight of evidence argument does not seem to work with everyone.    At first failure of the knowledge deficit model was blamed on the fact that people simply did not understand science, perhaps due to a lack of education. This was exacerbated as scientists from the late 1990s onwards started to be drawn into discussions about whether people believed or did not believe in climate change. The use of the word “belief” is important here, as it was a direct jump from the American-led argument between the science of evolution and the belief in creation.  But we know that science is not a belief system. You cannot decide that you believe in penicillin or the principles of flight while at the same time disbelieve humans evolved from apes or that greenhouse gases can cause climate change. This is because science is an expert trust-based system that is underpinned by rational methodology that moves forward by using detailed observation and experimentation to constantly test ideas and theories.  It does not provide us with convenient yes/no answers to complex scientific questions, however much the media portrayal of scientific evidence would like the general public to “believe” this to be true. However, many who deny climate change is an issue are extremely intelligent, eloquent and rational. They would not see the debate as one about belief and they would see themselves above the influence of the media. So if the lack of acceptance of the science of climate change is neither due to a lack of knowledge, nor due to a misunderstanding of science, what is causing it?    Recent work has refocused on understanding people’s perceptions and how they are shared, and as climate denial authority George Marshall suggests these ideas can take on a life of their own, leaving the individual behind.  Colleagues at Yale University developed this further by using the views of nature shown above to define different groups of people and their views on climate change. They found that political views are the main predictor of the acceptance of climate change as a real phenomenon.  This is because climate change challenges the Anglo-American neoliberal view that is held so dear by mainstream economists and politicians. Climate change is a massive pollution issue that shows the markets have failed and it requires governments to act collectively to regulate industry and business.  In stark contrast neoliberalism is about free markets, minimal state intervention, strong property rights and individualism. It also purports to provide a market-based solution via “trickle down” enabling everyone to become wealthier. But calculations suggest to bring the incomes of the very poorest people in the world up to just $1.25 per day would require at least a 15 times increase in global GDP. This means huge increases in consumption, resource use and of course, carbon emissions.   So in many cases the discussion of the science of climate change has nothing to do with the science and is all about the political views of the objectors. Many perceive climate change as a challenge to the very theories that have dominated global economics for the last 35 years, and the lifestyles that it has provided in developed, Anglophone countries.  Hence, is it any wonder that many people prefer climate change denial to having to face the prospect of building a new political (and socio-economic) system, which allows collective action and greater equality?  I am well aware of the abuse I will receive because of this article.  But it is essential for people, including scientists, to recognise that it is the politics and not the science that drives many people to deny climate change.  This does mean, however, that no amount of discussing the “weight of scientific evidence” for climate change will ever change the views of those who are politically or ideologically motivated. Hence I am very sorry but I will not be responding to comments posted concerning the science of climate change but I am happy to engage in discussion on the motivations of denial."
"
Share this...FacebookTwitter


Pachauri CCX advisory board member

EIKE, the European Institute for Climate and Energy, a sponsor of the 4th ICCC in Chicago, has dug up a some information on the Chicago Climate Exchange CCX, where Maurice Strong and a host of other influential leftists are among its board members.

When it comes to cap & trade, i.e. emissions trading, we’re talking serious money here. According to Dr. Richard Sandor, an economist and CCX architect, cap-and-trade represents a $10 trillion per-year market.
Recognizing the enormous profit potential, Al Gore’s Generation Investment Management (GIM)  purchased a 10 percent stake in CCX and became the company’s fifth largest co-owner. Yet all this coziness shouldn’t surprise anyone.
But taking a closer look at the CCX Advisory Board, we find among the likes of Joe Kennedy, Ed Begly, Thomas Lovejoy – Rajendra Pachauri. It’s known that Pachauri has huge investments in carbon markets, but this is ridiculous. Talk about a scam. Yet another gate to add to the long list.
Conflict of interest? Nahhhh
Update: FYI, CCX Directors here and External Advisory Board members here.
Share this...FacebookTwitter "
"
While I was on my week long road trip to survey weather stations and visit the National Climatic Data Center in Asheville, NC last week, I encountered lots of signs. Restaurant signs, road signs, signs from above, you name it. I think I must have passed 100 Bojangles restaurants and/or road signs. Bojangles is a popular southern chicken and biscuits restaurant. One though, really got my attention.

But please read on for the real “mother of all signs” I encountered.

Then there was Cracker Barrel and Waffle House…

Cracker barrel has an interesting marketing slogan on their supply trucks:

Yeah, I drove some of those….
On good advice from my readers, I avoided every one of these I saw:

This place is the southern version of Bob’s Big Boy and Frisch’s, good eats and a great breakfast bar. I didn’t try the wine though.

There were some other restaurant signs that I didn’t quite understand…

And there were some signs that I often wished I had for use when moderating this blog:

Then there were some signs that really spoke to the mission I was on:

And then there were others that I encountered that didn’t have a hint of southern hospitality at all…

I saw a lot of these at gas pumps, and given how I feel about biofuels, I drove to the next stations where I didn’t have to burn food to finish my trip:

I had mentioned that after surveying too many weather stations at sewage treatment plants that I needed a long shower, but when I saw this while I briefly toyed with the idea, I just didn’t see how it would change anything. I’d just be trading one smell for another.

There was one sign though that left a lasting impression on me, and it requires just a little bit of explanation.
When I was driving in Northwestern North Carolina, I went through many small towns and country roads that had small churches, I must have passed 200 during my trip. One thing I noticed is that pastors in these towns tend to try to out do each other with sermon topics on their front signs. I’d drive into a little town, and I’d see one sign advertising salvation, the next would have salvation plus breakfast, the next might have salvation, confession, a quote from the Bible, and a spaghetti feed, while the fourth might just have a zinger that would put all the others to shame.
This was one of those:

Now a caveat, the mountain road I was traveling on when I saw this had no shoulder and there was a semi truck right behind me. I looked for a place to pull over and turn around, and didn’t see one coming up, I couldn’t even pull over to let the truck pass. Five miles later I was still stuck and gave up on the idea.
Today I Google image searched to see if I could find an image where I could recreate the message, and found this neat web site called the Church Sign Generator, the output of which you see above. The sign and message was real, and didn’t look much different from this example except the top had something about Sunday’s service which I didn’t include because I didn’t get a good look at it.
We should all take this message home with us and live it.
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f9cc70b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Just two days after sunspot 983 was reported, it has now disappeared. They just aren’t sticking around like they used to. This is yet another indication of the bottomed out solar minima we are in.
It will be very interesting to see if the cycle 24 predictions by Hathway at NASA for an even stronger cycle will materialize.

Though there does seem to be more discussion of a weak cycle 24 than a strong one as of late. Personally, I think this graph of Average Planetary magnetic index (Ap) is quite telling in the step that occurred in 2005. From the data provided by NOAA’s Space Weather Prediction Center (SWPC) you can see just how little magnetic field activity there has been. I’ve graphed it below:

click for a larger image
What is most interesting about the Geomagnetic Average Planetary Index graph above is what happened around October 2005. Notice the sharp drop in the magnetic index and the continuance at low levels.
From this story on space.com where they talk about the opposing views solar scientists have for cycle 24 they offer some opinions. NOAA Space Environment Center scientist Douglas Biesecker, who chaired the panel, said in a statement:
 […] despite the panel’s division on the Sun cycle’s intensity, all members have a high confidence that the season will begin in March 2008.
We shall soon see if they are correct, March starts this Saturday.
Nature will truly be the final arbiter of this argument.
UPDATE: Jeff C writes
I thought you might find this chart interesting.  Since sunspot cycles overlap and there is no clear start/stop, the “start” of the new solar cycle is usually defined as the smoothed sunspot minimum between cycles (as opposed to the appearance of the first reversed-polarity spot). Although different definitions are sometimes used, this seems to be the most common and accepted variation.
The enclosed chart shows the transition from cycle 22 to cycle 23 back in 1996.  It is interesting how the first new cycle sunspots appeared over a year before the commonly accepted May 1996 start date of the new
cycle.
I’m unsure of the cycle start date definition used by Douglas Biesecker, but if it is the commonly accepted definition, he will be way off.  It will be interesting to see if they claim the appearance of a few reversed cycle sunspots count as a “start”.  If so, then cycle 23 actually started back in March 1995 and is 13 years old.

Click for a larger image


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0b64fad',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
 A new paper published  by the Astronomical Society of Australia titled:
Does a Spin–Orbit Coupling Between the Sun and the Jovian Planets Govern the  Solar Cycle?
contains a warning about earthly climate change not immediately obvious from the abstract:

Based on our claim that changes in the Sun’s equatorial rotation rate are  synchronized with changes in the Sun’s orbital motion about the barycentre, we  propose that the mean period for the Sun’s meridional flow is set by a Synodic  resonance between the flow period (~22.3 yr), the overall 178.7-yr repetition  period for the solar orbital motion, and the 19.86-yr synodic period of Jupiter  and Saturn.
According to an interview with Andrew Bolt, of the Australian Newspaper, Herald Sun, Ian Wilson, one of the authors explained:
It supports the contention that the level of activity on the Sun will  significantly diminish sometime in the next decade and remain low for about 20 –  30 years. On each occasion that the Sun has done this in the past the World’s  mean temperature has dropped by ~ 1 – 2 C. 
###
Hmmm, I’m not sold on this idea. This is a lot like what Dr. Theodor Landscheidt proposes. I have a little bit of trouble understanding how the “mass at a distance” gravitational effects of Jupiter and Saturn could have much effect on the solar dynamo.

I’m sure both my readers, and Dr. Leif Svalgaard, who regularly monitors this blog, will have something to add to provide additional insight. – Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e396d70',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"

As we approach the end of the year, it is appropriate to give you an update on Cato. Fortunately, Cato is doing very well and has positive momentum on all fronts, although there are always opportunities for improvement.



Commentators on both the left and the right are discussing the “libertarian moment.” While it’s obvious we do not have the current president’s ear, libertarian ideas are being taken very seriously, in part as the result of 37 years of Cato scholarship. There is a rapidly rising libertarian student movement where Cato has played a critical role in training future student leaders and providing the intellectual ammunition for students to challenge their left‐​leaning professors.



The Cato brand has also been rising. As a number of our fellow think tanks have become visibly politically partisan, we have maintained a reputation for objectivity in a politically charged environment. Of course, since we have plenty of disagreements with both Republicans and Democrats, this is relatively simple to do.



Cato’s financial position is strong, with our second year of significant increases in contributions. Thank you! However, our budget is tiny compared to the billions of dollars in resources controlled by the statists. We have many opportunities to use additional funding to productively communicate the libertarian message. As we approach the end of the year, I hope you will put Cato high on your list for financial support so we can maintain and accelerate the libertarian momentum.



The list of Cato’s recent accomplishments is impressive. Largely because of a great effort by Cato’s Michael Cannon, the court battle to undo Obamacare continues, with two recent victories in an uphill fight. Cato’s Constitutional Studies team’s record was 10–1 for amicus briefs filed at the Supreme Court this past session and 15–3 for the previous session. This is the result of over 30 years of effort to persuade the Court to limit Congress to its constitutionally enumerated powers.



Since 1999 Cato has been discussing the danger of police militarization. Finally, the public is becoming aware of this issue and its threat to innocent citizens (bring back Barney Fife). The Herbert A. Stiefel Center for Trade Policy Studies has started an invitational dinner program for business and government leaders to discuss trade liberalization and its many benefits. Participants have included Eric Schmidt, executive chairman of Google; Russ Girling, president and CEO of TransCanada Corporation; Greg Page, executive chairman of Cargill; and former U.S. trade representative Susan Schwab.



Harvard’s Jeff Miron, who has joined Cato as director of economic studies, has commissioned a series of economic briefs from top scholars. The scholars are typically not libertarian, but their research supports libertarian policy positions. The willingness of these high‐​level scholars to publish under the Cato brand is very encouraging.



We also took our ideas into the world’s hot spots this year: we cosponsored a conference in Ukraine with approximately 500 policymakers, businessmen, and journalists, as well as a student conference for freedom in Venezuela.



In the 12 months ending in August, Cato’s scholars were mentioned in 5,622 news stories, published 849 opeds, and made 442 major broadcast appearances. We published approximately 100 academically credible articles; held 30 conferences, 38 policy forums, 41 book forums, and 14 Capitol Hill briefings; and testified 18 times before congressional committees. Over 1,300 students participated in various Cato educational programs and our scholars made many presentations to student organizations such as Students for Liberty, Young Americans for Liberty, and the Federalist Society.



We just launched the Cato Center for Monetary and Financial Alternatives, which is the first serious challenge to the Federal Reserve in its 100‐​year history. The quality of scholars and business leaders who have agreed to participate is extraordinary and reflects the increasing concern by even mainstream economists about both the Fed’s monetary and regulatory policies.



The Cato Center for the Study of Science is growing rapidly to take on the increasingly politicized climatechange movement. Climate change is the religion of the Progressives and as their models have continued to fail they have become more irrational in defending their detached‐​from‐​reality positions.



We have launched a campaign to help independent thinkers truly grasp that big government is failing across the board. We are in the process of hiring two civilliberties scholars to take on the NSA, the IRS, and the many other agencies and policies (including the drug war) that are a serious threat to your individual liberties.



We are a voice of reason in an increasingly politicized and irrational environment. Cato is committed to creating a free and prosperous society based on the principles of individual liberty, free markets, limited government, and peace — a noble endeavor. Your support makes our work possible.
"
"
A guest post by David Smith


Recently I completed my tenth survey for Surfacestations.org. These surveys  are fun, almost like treasure hunts where the clues are good but not always  great, thus requiring some ingenuity. Also, the surveyor gets to see areas which  may otherwise never be visited. And, they’re for a good cause.
While I found no “poster-child” poor quality sites I did observe an array of  siting problems. Some thermometers were near the drip-lines of trees, some next  to buildings, one was near a concrete patio, one at a sewage plant, several sat  above poorly-drained soil and so forth.
These conditions are less than ideal, obviously. Perhaps more importantly,  these conditions can change over time. Trees and shrubs grow and die, ground  cover changes, concrete is added (and tends to darken over time), drainage may  improve or deteriorate, fences and other construction are added or removed, and  so forth. Each of these can subtly change the local temperature, a situation  which is especially important if one is looking for changes of a fraction of a  degree.
To what extent do these imperfections affect local temperature?  Well, we  really don’t know (or if anyone knows they’re not talking!).
So, to make a small and imperfect step in that direction, I’m running a few  local experiments. My goal is to examine, at least qualitatively, how local  microclimate factors like trees and concrete affect temperature. As you’ll see,  my methods are too crude to allow fractions of a degree determinations but I  should be able to quantify the magnitudes of the impacts of trees, concrete,  etc. Or at least that is my goal.
First, my instruments:

I’m using several temperature detector/recorders (”USB1″) like the gray  object shown in the photo. These electronic devices measure and log  the temperature to the nearest degree F and allow sampling on various schedules.  I use 30-minute sampling.
Note: Interested readers can buy these at:
http://www.weathershop.com/USB1_temperature_logger.htm
At this point I’m testing the hardware and developing my experimental  plan. But, I have made a few (literally) backyard tests and I’d like to share  one of those. This is to help illustrate the approach and, I hope, stimulate  helpful comments from other readers.
This initial run (sort of a beta test) was made in my backyard. It involved  two extremes. One is near my garage, above a dark-soil flower bed and landscape  bricks. This is near a wooden deck and walkway gravel. This spot gets direct  sunlight about 50% of the day.
The second extreme is deep shade, beneath low-tree (crepe myrtle) cover and  above thick, semi-tropical shrubbery.This is about twenty feet from sunlight. A  photo of the backyard is below, with red boxes marking the two locations:

I also use the temperature readings from an airport/airbase located four  miles west of my house. This airport provides professional-grade open-field  temperature readings which should reasonably approximate regional ambient  conditions.
A representative backyard temperature time series is below:

This shows pretty good agreement between the deep-shade max/min and the local  airport open-field max/min, which frankly surprised me. I’d expected the  deep-shade readings to show less variability (lower highs and higher lows).
More importantly is the contrast between #1 (sunlight and plant beds) and #2  (deep shade). The #1 spot stayed 5 to 10F hotter at midday than #2 (deep  shade) less than 50 feet away (and, as a matter of fact, #1 was 5 to 10 F warmer  than the high-quality nearby airport).
Why does this matter? well, suppose a co-op station had slowly drifted, over  several decades, from open-field conditions to those found at site #1. What  would that do to the apparent trend?  That’s an important question which is at  the heart of the surfacestation effort.
This backyard demonstration involved convoluted conditions. There is little  chance to untangle the relative contributions of so many variables (bricks,  soil, tomato plants, trees, etc). So, my plan is to reduce the number of  variables in the tests such that we might be able to make broad conclusions  about the relative impacts of trees, concrete, drainage and other factors which  may change over time.
This should be fun! Suggestions welcome.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9fa983f4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"



Earlier this week, the _New York Post_ published articles containing information about alleged emails between Hunter Biden, the son of Democratic presidential nominee Joe Biden, and employees at Chinese and Ukrainian energy firms. Twitter and Facebook both took steps to limit the spread of the articles, prompting accusations of “election interference.” Prominent Republican lawmakers took to social media to condemn Twitter’s and Facebook’s decisions. These accusations and condemnations reveal a misunderstanding of policy that could result in dramatic changes to online speech.



According to Twitter, the company restricted access to the _New York Post_ ’s articles because it violated the company’s policies against spreading personal and private information (such as email addresses and phone numbers) and hacked materials. Twitter cited the same policy when it prohibited users from sharing 269GB of leaked police files. Twitter users who click on links to the two _Post_ articles face a click‐​though “this link may be unsafe” warning. The articles in question include such information in images of the leaked emails. Those accusing Twitter of a double standard because the company allows users to share the recent _New York Times_ article based on the president’s leaked tax documents neglect the fact that the _New York Times_ did not publish images of the documents. Although consistent with Twitter’s policies, the decision to block the spread of the _Post_ ’s articles on Twitter absent an explanation or context was criticized by Twitter CEO Jack Dorsey.



According to a Facebook spokesperson, Facebook’s decision to restrict the spread of the _Post_ ’s Hunter Biden articles is “part of [Facebook’s] standard process to reduce the spread of misinformation.” Compared to Twitter’s response, Facebook’s was less clear.



Whatever one thinks about Twitter’s and Facebook’s decisions in this case the decisions were legal and consistent with Section 230 of the Communications Decency Act. Much of the online commentary surrounding restrictions on the _New York Post_ (head over to #Section230 on Twitter to take a look for yourself) makes reference to a non‐​existent “publisher” v. “platform” distinction in the law.



In brief, Section 230 states that interactive computer services (such as Twitter, the _New York Time_ ’s comments section, Amazon, etc.) cannot — with some very limited exceptions — be considered the publisher of the vast majority of third‐​party content. Twitter is not the publisher of your tweets, but it is the publisher of its own content, such as the warning that appears when users click on the two _New York Post_ article links. Section 230 applies to “platforms” and “publishers,” and does not prevent social media sites from fact‐​checking, removing, or limiting access to links.



Some “Big Tech” critics decided not to focus on Section 230 and instead focus on election interference. The conservative outlet The Federalist issued a statement making this claim, as did many others. According to those making the “election interference” claim, the _New York Post_ articles are embarrassing to Joe Biden, and Twitter’s and Facebook’s actions constitute a pro‐​Biden interference in the 2020 presidential election. Conservative pundits are not the only ones making this kind of claim. Senator Joshua Hawley (R-MO) wrote to Dorsey asking him to appear at a hearing titled “Digital Platforms and Election Interference.” Sen. Ted Cruz (R-TX)  wrote to Dorsey accusing Twitter of trying to influence the upcoming election. Later he accused Twitter of election interference and supported the Senate Judiciary Committee issuing a subpoena to Dorsey, which is expected to happen this coming Tuesday.  
  
It is one thing for conservative pundits to accuse a private company of interfering in an election. In today’s political climate it is expected. What should send chills down the spine of everyone who values the freedom of speech and the freedom of association is the sight of two of the most powerful politicians in the country making the same accusation and insisting that Twitter’s CEO appear before a hearing and hand over documents related to how Twitter conducts its business.  
  
To portray how Twitter and Facebook handled the _New York Post_ articles as “election interference” has significant implications. Twitter and Facebook limited access to an article that is potentially embarrassing to a political candidate. If such actions can be considered “election interference,” should every content moderation action by a private company taken against any politician or candidate be considered interference? If _The Wall Street Journal_ rejects an op‐​ed written by the Green Party’s presidential candidate is not that also “election interference”? When a music hall owner decides to allow the Trump campaign, but not the Biden campaign, to host a rally is that not “election interference”?  
  
“Election interference” is a term that ought to mean something useful. Unfortunately, conservative commentators seem intent on warping the term so that it means little more than, “moderating content.”



So‐​called “Big Tech” and content moderation will continue to make headlines next year regardless of who wins the presidential election next month. While conservative commentators and activists are convinced that “Big Tech” is engaged in an anti‐​conservative crusade, they should consider that the political left has its own complaints. Bipartisan anger towards Big Tech could result in Section 230 reform or other legislation that puts the freedom of speech and freedom of association at risk. As lawmakers continue to criticize the most prominent social media companies we should remember that attempts to regulate online speech could have disastrous consequences.
"
"

The purpose of this report is to provide a framework for doing research on the problem of bias in science, especially bias induced by Federal funding of research. In recent years the issue of bias in science has come under increasing scrutiny, including within the scientific community. Much of this scrutiny is focused on the potential for bias induced by the commercial funding of research. However, relatively little attention has been given to the potential role of Federal funding in fostering bias. The research question is clear: does biased funding skew research in a preferred direction, one that supports an agency mission, policy or paradigm?



Federal agencies spend many billion dollars a year on scientific research. Most of this is directly tied to the funding agency mission and existing policies. The issue is whether these financial ties lead to bias in favor of the existing policies, as well as to promoting new policies. Is the government buying science or support?



 **Our working definition of “funding‐​induced bias” is any scientific activity where the prospect of funding influences the result in a way that benefits the funder.**



While the basic concept of funding‐​induced bias is simple, the potential forms that this bias might take are far from simple. Science is a complex social system and funding is a major driver. In order to facilitate research into Federal funding and bias it is necessary to isolate specific kinds of bias. Thus the framework presented here is a taxonomy of funding‐​induced bias.



For the purposes of future research the concept of funding‐​induced bias is analyzed in the following ways:



1) the practices in science where bias can occur,  
2) how agency policy can create bias,  
3) the level at which bias is fostered, and  
4) indicators of bias.



Fifteen different scientific practices are analyzed, ranging from the budgeting and funding for research to the publishing and communication of results. For each of the fifteen practices there is a snapshot of the existing research literature on bias, plus a brief discussion of the directions that new research might take in looking into funding‐​induced bias. The potential for quantifying the extent of bias is also addressed.



In order to provide examples along the way there is a special focus on climate change. Federal policies on climate change and federal funding of climate research are both extensive. The linkage between these policies and research has become a major topic of discussion, including numerous allegations of bias.



The research framework provided here applies to the study of all funding‐​induced bias in science, not just to climate change science. The linkages between Federal policy and federally funded science are extensive and not well understood. Moreover, these linkages have policy implications, especially if they are inducing bias in scientific research. However, policy is not our topic here. Rather we are addressing the needed research that might lead to new policies.



In this report we are mostly concerned with individual types of funding induced bias. But there is an intrinsic sequence to the various biases we have identified and this raises the possibility of cascading amplification. By amplification we mean one biased activity is followed by another, such that the first bias is increased.   
  
A simple, and perhaps common, example of amplification might be when the hype in a press release is exaggerated in a news story. Let’s say the press release overstates the importance of the research result, but with some qualification. The news story then reports the result as a great breakthrough, far more strongly than the press release, ignoring the latter’s qualifications. In this way the original bias has been amplified. 



Cascading amplification when one biased activity is followed by multiple instances of amplification. Using our example, suppose a single biased press release generates many different news stories, which vie with one another for exaggeration. This one‐​to‐​many amplification is properly termed a cascade.



Moreover, there is the possibility of cascading amplification on a very large scale and over multiple biased stages. Here is an example of how it might work.



1) An agency receives biased funding for research from Congress.



2) They issue multiple biased Requests for Proposals (RFPs), and



3) multiple biased projects are selected for each RFP.



4) Many projects produce multiple biased articles, press releases, etc, 



5) many of these articles and releases generate multiple biased news stories, and



6) the resulting amplified bias is communicated to the public on a large scale.



One can see how in this instance a single funding activity, the agency budget, might eventually lead to hundreds or thousands of hyperbolic news stories. This would be a very large scale cascading amplification of funding‐​induced bias.



 _Climate Change Examples_



In the climate change debate there have been allegations of bias at each of the stages described above. Taken together this suggests the possibility that just such a large scale amplifying cascade has occurred or is occurring. Systematic research is needed to determine if this is actually the case. 



The notion of cascading systemic bias, induced by government funding, does not appear to have been studied much. This may be a big gap in research on science. Moreover, if this sort of bias is indeed widespread then there are serious implications for new policies, both at the Federal level and within the scientific community itself.
"
"The unintended consequences of our agricultural food system – polluted air and water, dead zones in coastal seas, soil erosion – have profound implications for human health and the environment. So more sustainable agricultural practices are needed as soon as possible.  Some farmers have turned to less chemically-intensive techniques to reduce the negative impact of agriculture, such as organic farming, which has been shown to outperform conventional farming by many standards of environmental sustainability. The question is whether we can meet these environmental standards and still meet the demand for food, which is predicted to rise substantially in the next 50 years. In our new study, published in Proceedings of the Royal Society B, we found that organic farming systems, when done right, come close to matching the productivity of conventional systems.  Designing a single experiment that could possibly represent the huge variation in crops, weather and soil necessary to get a complete answer is impossible. Instead, we examined the many specific studies that have already been conducted and combined their results – a meta-analysis. We compiled studies from across the globe that compared organic and conventional yields over three decades, representing more than 1,000 comparisons of 52 crop species from 38 countries. This isn’t the first time researchers have attempted to answer this question, but previous studies have had conflicting results. Combining studies carried out by different scientists for different reasons is a big challenge. Depending on what data is included and how it is handled, answers can vary substantially. Many previous studies found organic yields were 8-25% lower than conventional systems. Another study found that organic farming outperformed conventional in developing countries. In revisiting this question, we used the most extensive dataset to date and methods that try to account for the complexity of the data. We found that although organic crop yields are about 19% lower than conventional yields, certain management practises appear to significantly reduce this gap. In fact, planting multiple different crops at the same time (polyculture) and planting a sequence of crops (crop rotation) on an organic farm cut the difference in yield in half. Interestingly, both these practices are based on techniques that mimic natural systems, and have been practised for thousands of years. Our study strongly suggests that we can develop highly productive organic farming methods if we mimic nature by creating ecologically diverse farms that draw strength from natural interactions between species. Crop rotation and polycultures are known to improve soil health and reduce pest pressure. Because these practices add diversity to the landscape they also support biodiversity, so they may improve yields while also protecting the environment. We also found that for some crops such as oats, tomatoes and apples there were no differences in yield between organic and industrial farming at all. The largest yield gaps were found in two cereal crops, wheat and barley. However, since the agricultural Green Revolution in the mid-20th century, improving the yields of cereals grown using conventional, industrial agriculture has received a huge amount of research and funding – far more than organic agriculture. Little wonder, then, that we see a large difference in yields. For example, some seeds are specifically bred to work well in the nutrient-rich, pest-free conditions found in conventional farms due to the heavy use of fertilisers and pesticides, so they may underperform in organic farms. But if we invested in organic agricultural research and development we’d no doubt see a large increase in the yield too. We also found evidence that the yield gap estimate we and others have calculated is likely an overestimate. We found evidence of bias in the studies we compiled, which favoured the reporting of higher conventional yields relative to organic. This can arise for several reasons: the studies can favour specific crops or practices so that the results are unrepresentative, or introduce bias during the selection of results to be published. It’s impossible to know the origins of the bias, but it’s necessary to acknowledge the effect it will have on yield estimates. It’s important to remember that simply growing more food is not enough to address the twin crises of hunger and obesity. Current global food production already greatly exceeds what is needed to feed the world’s population, yet social, political, and economic factors prevent many people from living well-fed, healthy lives. A focus solely on increased yields will not solve the problem of world hunger.  To put the yield gap into context, the world’s food waste alone is 30-40% of food production per year. If food waste were cut by half, this would more than compensate for the difference in yield from converting to organic agriculture, as well as greatly reducing the environmental impact of agriculture."
"
Share this...FacebookTwitterI know I shouldn’t go here, but the temptation is just too strong. I’ll let the readers make up their own minds. Here are some links to read. But do research more.UPDATE 1: Oh! Oh! Drudge has got Gore as the big headline.
1. Serious accusations
2. 3 reasons not to believe the accusations 
3. $540 massage(?) at ritzy hotel
4. Gore assault NYT
5. FULL POLICE REPORT
This eventually will boil down to the question: Can we really trust Al Gore? We saw how far he took the level of propaganda in his AIT film, which was carefully crafted to pull at the heart-strings and to mislead viewers. It was slammed by a British High Court. His jet-setting, mansion-buying lifestyle is in complete contradiction to what he preaches. He constantly ducks debate and keeps his head in the sand.
Not long ago he separated from his wife, indicating possible breach of trust in the relationship. He once claimed to have invented the internet. Just how believable is this guy?
My personal opinion is that Gore is as great a fraud as one will ever find, and he’s living high on the hog because of it. But that’s just my opinion, which is based on the so many words that have come out of his mouth and on his actions.
Indeed this has the potential to be much bigger than Climategate.
“But what does that have to do with the science,” one may ask? Gore is not a scientist, but he is a big messenger who has a message he wants (demands) everybody to believe. And it so happens that he has a huge interest in that message.
Look for the media to build a massive bulwark around Gore, and to come out blasting with everything they’ve got.

Share this...FacebookTwitter "
"The woman who toppled Tony Abbott in Warringah at the last election on a platform of climate change action now has the whole parliament in her sights as she seeks bipartisan support for a climate change framework bill aimed at transitioning Australia to a decarbonised economy. Zali Steggall – along with her fellow crossbenchers Rebekah Sharkie, Helen Haines and Andrew Wilkie – will release the climate change national framework for adaption and mitigation bill on Monday, ahead of its introduction to the parliament in March.  Steggall and the crossbench have begun a conscience vote campaign online and within their communities. They hope to win over enough government MPs to see the bill, which has been modelled on existing legislation in the UK, New Zealand and Ireland, pass in Australia. However, the crossbench faces an uphill battle, with Scott Morrison declaring just last week he would not be “bullied’ into more action on climate change.  Steggall has previously called on the self-styled “modern Liberals” to support the legislation, which she said became imperative following the summer of unprecedented bushfires and resulting hazardous air quality that left communities reeling. With the government’s party room once again at war over climate policy, Steggall said it was time to let individual MPs speak for their communities rather than toe a party line. “The bill will be circulated to all MPs as well as business, environmental and relevant stakeholder groups on Monday,” she said. “It is time to take the party politics out of climate policy. It is a matter of principle that we should all be committed to a safer future. I am urging for a conscience vote when I present the bill on March 23 as a private member’s bill. Now is the time for a rational approach to climate change.” The crossbench group, working with climate action organisations, has already launched petitions calling on MPs to be allowed a conscience vote on the legislation once it is introduced. Without a conscience vote, the bill is doomed to fail, with the government holding the numbers in the lower chamber. Steggall said the events of the summer, on top of the climate impacts Australia was already suffering through, should be enough to prompt MPs to follow their conscience and vote on behalf of their constituents. “This bill is a sensible and bipartisan approach to safeguarding Australia’s future against the impacts of climate change,” she said. “The devastating fires that ripped through Australia over summer; the drought; and our deteriorating air pollution have shown how the impacts of climate change are a real threat to our way of life.” Dave Sharma in Wentworth, Tim Wilson in Goldstein and Jason Falinski in Mackellar, as well as Brisbane’s Trevor Evans and North Sydney MP Trent Zimmerman are being targeted as potential allies. Newcomer Katie Allen, who won the seat of Higgins at the last election, and Bennelong MP John Alexander, who have both urged their government to take more action on climate policy in recent weeks, are also being urged to vote according to their electorate’s wishes. Steggall has previously warned of voter backlash if moderate Liberals ignore their wishes on climate action. “I think they have to be mindful of their electorates feeling disenfranchised if they aren’t voting in accordance with their majority wishes,” Steggall told the Guardian last month. “The Liberal party is the party of the free vote – I am not asking them to do something they have never done before, and I think crossing the floor to vote for a climate act is something they need to do to represent their constituents. “If you choose to ignore the amount of people in your electorate [who want stronger climate action] … you do so at your peril.” Steggall and the crossbench have kept much of the bill under wraps, but have said they are aiming for a statutory long-term target of net zero emissions by 2050, as well as a climate change risk assessment for all sectors. The group wants the government to focus on a national adaption schedule for Australia’s industries, based on what the science has revealed in regards to impacts of climate change. To ensure accountability, the group wants to follow the UK’s lead and establish some sort of climate change authority, which would act independently of the government, and report back on the progress each year. Labor’s deputy leader, Richard Marles, said the opposition was looking to work with the government on a bipartisan climate policy. “We have been seeking bipartisanship for a long time in relation to this,” he told the ABC on Sunday. “But to get bipartisanship, we actually need to have a side that we can talk to. Right now, we’re watching a whole lot of people having a war with each other inside their party room … and that’s preventing the conservatives in this country even coming to the table to have a discussion about this.”"
"All eyes are on Brazil following the re-election of Dilma Rousseff as president after an eventful campaign in which the strongly pro-environment candidate Marina Silva was squarely defeated.  Now, the country’s green credentials are seriously at risk. In a new report in the journal Science, researchers from Brazil and the UK (including myself) highlight the danger of new plans to allow mining and dams in protected areas and indigenous lands.  Congressional debates to approve or reject proposed legislation will decide if Brazil will retain its hard-won status as what The Economist calls “the world leader in reducing environmental degradation”. The new government is at a crossroads: either maintain the integrity and long-term future of its globally significant ecosystems or favour industrial interests by allowing 10% of even strictly protected areas to be mined. While the proposals include mitigation measures (protecting land elsewhere) these are unrealistic and also inadequate because they fail to account for the indirect impacts of mines. Developing mines and hydropower dams in protected areas would represent a reversal for Brazilian law-makers and a body blow to environmental agencies, credited with drastically reducing Amazonian deforestation over the past decade. Mega-projects have mega-impacts and in the Amazon, mining and damming go hand in hand. Mining is energy intensive and is one of the underlying reasons for Brazil developing dozens of large hydropower plants in Amazonia. Hydroelectric dams can harm both society and the environment. For example, I was alarmed to see how severe flooding in Rondônia state this year led to economic paralysis and the spread of water-borne diseases in towns and countryside along the Madeira River. The flooding of the Madeira in both Brazil and upriver in Bolivia was suspected to have been caused by the recently completed Jirau hydropower dam. Under current plans, very few protected areas will remain free from the influence of hydroelectric dams.  Mining projects such as the enormous Carajás iron ore mine in eastern Amazonia, powered by construction of the controversial Tucuruí dam in the 1970s, are only the tip of the iceberg. Mineral extraction in Brazil is poised to expand into what were previously considered no-go areas for industrial development.  Our research found that in the Amazon alone 34,117km2 of strictly protected areas and 281,443km2 of indigenous lands are in areas of registered mining interest. Forget football fields, this is an area larger than the whole of the UK.  The direct impacts of mines and dams are eclipsed by the indirect effects, as thousands of workers follow mega-development projects into protected areas. Rapid population growth in service towns causes urban areas, roads and farmland to expand into surrounding forests. By 2000, Brazil had created the world’s largest protected area network, covering an enormous 2.2m km2 (an area the size of Greenland). These parks have been highly effective. For example, by reducing deforestation rates to only 10-15% of those in surrounding areas, Brazil’s protected areas contribute to mitigating future climate change.  The beneficiaries of climate mitigation range from farmers in the south of Brazil who depend on Amazonia for their rainfall, to the poorest people in developing countries who stand to bear the brunt of global warming, sea-level rise and extreme climatic events.  Brazil’s protected areas go far beyond just saving the forests themselves and support traditional peoples, including rubber-tappers and Brazil-nut harvesters. In addition, indigenous lands provide a safe space to maintain the traditions and cultures of the country’s 305 indigenous ethnic groups, including 69 uncontacted groups. Get-rich-quick mining is not a new threat to Brazil’s unique ecosystems. I have witnessed the decade-long struggle of a strictly protected area, the Jari Ecological Station in Pará, to remove illegal gold-mining from within its borders. However, it is harrowing to now see 71% of the park (an area larger than Greater London) being under official consideration for mining operations. Even if “only” 10% of the park is used for mining, indirect effects will change it for ever.  Relevant federal departments need adequate resources to ensure government decisions are made democratically and with reliable impact assessments. Chronic under-staffing in Brazil’s protected areas means that many lack basic information on baseline environmental conditions and diversity of plants and animals. Buffer areas designed to protect parks from external threats are put at risk, and a lack of staffing and data puts ICMBio (the agency responsible for protecting parks) in a weak position from which to assess the potential impacts of dams or mining on the integrity of a park.  Brazil’s population is growing and increasingly wealthy, which means higher demands for energy and food. Some difficult decisions will have to be made.  However, environmental impact assessments and mitigation measures (in some cases impossible to achieve and in most cases not implemented anyway) surrounding proposed mega-projects have fallen short of international best practice and largely ignore indirect impacts. I hope that Brazil reasserts its status as a leader of green development and does not legislate against her national treasures."
"It has been five years since an earthquake hit the Italian city of L’Aquila leaving 309 people dead. In the aftermath one public official and six earthquake scientists were charged with multiple counts of manslaughter. Each defendant was sentenced to six years in jail.  It is commonly believed the scientists were condemned for failing to predict the earthquake but, in truth, the case was about communicating risks to a vulnerable population. The defendants were accused by the prosecution of giving “inexact, incomplete and contradictory information”. One of L’Aquila’s citizens succinctly articulated the position of many survivors:  We all know that the earthquake could not be predicted, and that evacuation was not an option. All we wanted was clearer information on risks in order to make our choices. The appeal process has been ongoing but as of November 10 charges have been dropped against the scientists involved. Conspicuously, the convicted Italian official had his sentence reduced but still faces two years in prison.  Six days prior to the earthquake scientists met with Bernardo De Bernardinis, then deputy director of Italy’s Civil Protection Agency. A local laboratory technician had been making dubious predictions of an impending large earthquake. Meanwhile, smaller tremors were being experienced in the region.  The meeting was called with intentions to reassure the public. The scientists correctly emphasised to De Bernardinis that the precise timing of major earthquakes could not be known. They were careful not to rule out the possibility of a major earthquake any time.  Following their meeting De Bernardinis publicly stated: “The scientific community tells us there is no danger, because there is an ongoing discharge of energy. The situation looks favourable.” None of the scientists made an effort to correct Bernardinis’s imprecise statements.  Public officials clearly felt pressure to reassure. The overriding wish to calm citizen’s fears created a situation where risks were downplayed and scientific uncertainty was emphasised at the expense of responsible warnings.  There are obvious lessons to be gleaned from L’Aquila: clear science communication has real consequences for public safety. Political officials have a responsibility to communicate risks to the public without sacrificing the science to politically palatable messages.  However we need not endorse the L’Aquila judgements in order to consider alternative circumstances where legal penalties might be appropriately applied.  Take the purposefully organised campaigns of disinformation over lead poisoning, asbestos or tobacco, for instance. Such campaigns, generally orchestrated by vested interest groups, show a reckless disregard for public safety. Now a related campaign seeks to undermine the public’s understanding of climate science. It is easy to feel sympathy for an Italian official who seems to have been motivated to reassure rather than mislead. Despite his failure to convey an accurate but tempered assessment of the dangers it remains true that large earthquakes really are unforeseeable in the short term. I hope his ongoing appeal process is ultimately successful.  But what do we make of politicians who doggedly deny the overwhelming evidence for human-caused global warming after receiving large sums of campaign money from the fossil fuel industry? They too emphasise scientific uncertainty at the expense of responsible warnings.  This is clearly irresponsible and dangerous. The devastation to human life resulting from unchecked climate change is magnitudes greater than the tragedy in L’Aquila and it will get worse still. We may be uncomfortable with the Italian courts, but consider the alternative: is a system which allows politicians to openly receive large sums of money from fossil fuel interests, while dismissing the greatest crisis humanity has yet faced, any better at serving the public’s interests?  We should reconsider political frameworks that allow large sums of money to shape politician’s understanding of climate science if we want to avoid cases like L'Aquila in the future."
"While listening this week to the Guardian Australia editor, Lenore Taylor, on the Full Story podcast speaking with sadness and exasperation about the past 30 years of climate change policy, I thought about good intentions and perfection. We often hear that “the road to hell is paved with good intentions”. It’s one of those sayings that, when you really think about it, is just a squib.  The reality is these good intentions that are paving our way to hell are not so much good as ignorant, and quite often (such as with much of Indigenous policy) outright racist. They are really considered good only by those seeking to excuse the action in the first place. And while good people might occasionally do wrong with actual good intentions, there is a much higher strike rate of people with bad intentions doing bad things. That the road to hell may be paved with good intentions means we should ensure our good intentions are not ignorant or biased; and it sure as heck does not give an excuse to those with bad intent. We certainly have seen a plethora of bad intent with respect to climate-change policy since 1990. Similarly, you could probably fill a long list of quotes by people over the past decade or so who have suggested of climate change policy that “we should not make the perfect the enemy of the good”. And yet rather than this suggesting we need to compromise, what it has come to mean is that we should excuse policy that is bad, because it is not perfect. If we are so worried about the perfect being the enemy of the good, wouldn’t we at least see some evidence of someone in the major parties actually suggesting a policy that could be described as perfect? The science of climate change tells us we need to reduce emissions and the sooner we do it the less the impact will be. And yet rather than see any “perfect” policy with this aim, instead we get supposedly good policy accompanied with caveats – talk of the need for transitional fuels such as gas or that a coalmine is fine – hey, let’s not be perfect! (And please don’t not argue about just how good something has to be before perfect becomes its enemy). So bad has this become that the carbon price instituted by the Gillard government is now considered some perfect policy too far beyond our political grasp. Why are we so lacking in ambition? And given the other side have an ambition fuelled by bad intentions, it might be worthwhile when trying to compromise to ensure negotiation starts from a position rather closer to perfect than just “good”. Because let us be honest: the government is paving the way to climate change hell with bad intentions. The government is replete with climate-change deniers who intend to block and retard any action to reduce emissions. They will do this through obfuscation and outright misinformation – such as the lies about the impact of the ALP’s electric car policy during the last election, or the current lies about the bushfires. On Wednesday, Peter Dutton told ABC’s Afternoon Briefing that “obviously, as we’ve all pointed out, we’re experiencing hotter weather, longer summers, but did the bushfires start in some of these regions because of climate change? No. It started because somebody lit a match. There are 250 people, as I understand it, or more that have been charged with arson. That’s not climate change.” It’s also not the truth. The prime minister has equally betrayed his bad intentions on the issue. He told the media on Thursday that, “I’m not going to allow a confined, narrow debate when it comes to understanding what it means to live in the climate we’re going to live in. It’s not just about emissions reduction. That’s important. But it’s also about resilience and it’s also about adaptation” His intent is clearly to actually narrow the debate by excluding as much as possible any discussion of emissions, and instead to focus on building dams as though that is some saviour for a country affected by climate change and by greater periods of drought. Scott Morrison – and other members of his cabinet – have no good intentions when they suggest Australia is doing well by meeting and beating out Kyoto commitments. Those commitments are frauds. The Howard government purposefully ensured Australia alone could include land use because the Kyoto base year of 1990 involved a spectacular level of land clearing, as does 2005, the base year for our Paris agreement. Saying we will meet and beat our targets is like bragging that you are meeting your target of drinking less beer by comparing how many glasses you drink per day now to what you drank on the day of your 21st birthday party. Last week Morrison told the National Press Club the United States were doing great because of its increase in gas-produced electricity and that “between 2005 and 2017 US emissions fell by about 13 per cent, that’s just a click over what we have achieved, which is 12.8 per cent by the way.” What he failed to note was those drops occurred prior to the election of Trump and its withdrawal from the Paris agreement and that it is not on target to meet what were the US’s Paris targets. His suggestion that Australia is nearly doing just as well is also one of bad faith. Yes, from 2005 to 2017 we reduced our emissions by 12.8%, just below that of the US, but only if we include land use. If we exclude that very dodgy measure, the US’s emissions still fell by 12%, but Australia’s actually rose by 6.2%. We are not doing our bit, and you would only argue we are if your intention was to ensure that good policy is not merely blocked but bad policy is pushed. There has been some talk this week about new Greens leader Adam Bandt’s call for a “Green New Deal” and whether or not Australia should adopt such a US-style political term. To be honest I’m not all that fussed about the marketing, so long as the policy has large ambition. We need some good intentions and we need to aim for perfection. The challenges and forces against action on climate change are large and powerful. As the past 30 years have shown us, being content to argue for a “good” third- or fourth-best policy is no way to win this fight, and neither is allowing those with bad intentions to tell us that what they want is good."
"Some great news at last, as China and the US announce a secretly negotiated deal to reduce their carbon emissions. After years of seeming to get nowhere at all it looks like we have the beginnings of meaningful commitments. If the rest of the world can fall in line with the combined targets of China, the US and EU, and if between us all we can enforce them, we would actually have progress. Not success, but for the first time we would have better-than-nothing global progress on climate change. But just before we all relax, lets get things into perspective. Global emissions have been on a mathematically predictable exponential trajectory for at least 160 years.  Cumulative CO2 emissions (broadly speaking that’s what determines the temperature change) continue to double every 39 years. Nothing that anyone has done to date has succeeded in making even the faintest detectable change in that.  To be blunt, our species has so far not demonstrated any ability whatsoever to influence global emissions growth through deliberate action on climate change. Savings in one place have simply popped up elsewhere. And if we stay on our age-old trajectory we will shoot through the likely threshold of two degrees in the mid-2040s.  By that I mean that by about 2045 we will pass the point at which we will probably experience more than a 2°C rise even if no-one anywhere in the world ever again set fire to any coal, oil or gas. And, roughly speaking, 39 years after that we will crash through the 4°C threshold which humans would be very likely to find extremely unpleasant.  Of course we don’t really know all that much about what level of temperature change will cause us what level of suffering and death. We don’t understand the climate discontinuities that we might trigger, and we don’t know how good we will be at adapting to change and we don’t know how good we will be at preserving world order if things get tough.  The mainstream consensus is that 2°C entails significant risk of something nasty happening while 4°C is probably very nasty indeed. No one knows for sure. What we need is a global constraint on greenhouse gases. And it needs to be rapid enough to  keep temperatures as close to 2°C rise as possible. This much, thankfully, seems to be uncontested these days among people who talk any sense on climate change. So how far do the latest US and China pledges take us? If (and it’s still a big “if”) the world falls quickly in line with the US (27% cuts by 2025), China (peak by 2030 – by which time their emissions could be enormous) and EU (40% cut by 2030) announcements we will come off the exponential curve but still fly through the 2℃ threshold and well beyond. Coming off the curve would be a huge achievement but not nearly enough.  So when I say we might actually stand a chance of getting somewhere, I don’t mean that things are looking rosy. But I do mean this gives me real hope, as big players are talking the right language at last.  All we need now is more of the same – and to make sure the words turn into enforced action. That will be enormously challenging but it is radically more hopeful position than the situation we have been in in which sticky plasters have been proposed, no amount of which could help.  We need the rest of the world to come into the fold with similar commitments, so we get a leak-proof deal on leaving fuel in the ground. Any countries that don’t participate will probably end up growing their emissions to undo efforts made elsewhere, because that is how the system dynamics work to negate piecemeal actions. Binding targets need tightening up for everyone, beyond what is currently on the table, to take us a lot closer to topping out at 2°C. The deal needs enforcing. This is going to be tough, remember that the exponential global emissions curve has proved incredibly resilient to date. All the greenhouse gases need to be properly included in the plan. We need to head off a global dash for biofuels which will undoubtedly be at the expense of feeding the world’s poorest if left to market forces. Some smart and robust agreements are going to be needed on land use for biofuels. While all this is being put in place we can start investing in the technologies we will urgently need – redirecting the money we have been channelling into fossil fuel research and development. To sum up, the announcement is very encouraging. There may still be a long way to go yet and we all need to push hard for next year’s Paris talks to put it all in place – but it is starting to look as if it might actually be worth the effort."
"
Share this...FacebookTwitterWow! Arctic sea ice is still at levels we had back in mid-February. It’s at the highest level for this date in nine years. http://www.ijis.iarc.uaf.edu/en/home/seaice_extent.htm
Normally there’s about a 1 million sq. km decrease from mid Feb to late April. This year – nada!
Share this...FacebookTwitter "
"

Click image for movie – note download is large 2.4MB
A guest post by Michael Ronayne
Note: Mike has created a movie (solar_cycle_23-24_sunspots.gif large (2.4MB) animated GIF) that shows how the cycle 23 forecast has progressed through time. Given that NASA’s David Hathaway recently commented on SpaceWeather that we are still seeing Cycle 23 spots, this seemed like a good time to post Mike’s effort.
The Space Weather Prediction Center (SWPC) at http://www.swpc.noaa.gov/ issues weekly reports on solar activity know as Preliminary Report and Forecast (PRF) of Solar Geophysical Data or “The Weekly”. Generally on the week following the end of the month a monthly summary is issued which includes graphics for the past month.
In the summary is the “ISES Solar Cycle Sunspot Number Progression” graphic which shows past, present and predicted average sunspot numbers by month. SWPC maintains a compressed archive of all weekly PRD reports in PDF format since 1996 which is available here.
Individual weekly reports for 2007 are available here  and current reports for 2008 are available here .
The most current graphic is always here.
All of “The Weekly” reports were inspected to identify the monthly summaries and determine the quality of the “ISES Solar Cycle Sunspot Number Progression” graphic contained therein. It was determined that the graphs prior to April 30, 2003 were in a significantly different format, had quality control problems and skipped months, therefore only graphs from April 30, 2003 to present were used.
Using Adobe Acrobat Professional the “ISES Solar Cycle Sunspot Number Progression” graphics was extracted from each of “The Weekly” PDF reports as oversized TIFF graphics to preserve resolution. The standard publication size for the graphic was 720×550 pixels but the aspect ratio for some of the graphs was not preserved within the PDF document. When the oversized TIFF graphic were resized to 720×550 without preserving the aspect ratio within the PDF the original 720×550 graphic was recovered in all cases. The 720×550 TIFF graphic was then converted to a GIF graphic for use in the animation sequence.
While extracting the “ISES Solar Cycle Sunspot Number Progression” graphs it was found that January 31, 2008 monthly summary had not been generated, a fact which SWPC confirmed in response to an Email inquiry. The February 29, 2008 graphic was hand edited at the pixel level to recreate the missing month and is identified in the animation sequence “proxy200801.gif”. The remaining graphics are all identified by the PRF document number.
The Advanced GIF Animator program was used to create the animation sequence. With the exception of January 31, 2008 all of the frames are prefixed by PRF9999 when 9999 is the document number of the original PDF report from which the graphic was extracted.
When the animated frames were inspected in sequence it was found that there was a discontinuity between July 31, 2006 (PRF1510), August 31, 2004 (PRF1514) and the September 30, 2004 (PRF1520) frames. The causes of the discontinuities were:

Data was retroactively changed on the August 31, 2004 frame.
The August 31, 2004 data point was not plotted on the August 31, 2004 frame.

These three frames were not altered or correct in anyway and are displayed as published. This technique is very good at identifying data discontinuity problems.
Excluding the problems noted above the reconstructed graphic went very well and there was no discernible flicker between frames indicating that the PDF extraction process was near prefect. With the exception of the problem about August 31, 2004 and the missing monthly summary for January 31, 2008 the SWPC product has been amazingly consistent since April 30, 2003.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea00d55cf',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"A big cat is apparently lurking in the countryside near Disneyland, Paris. After reports of a tiger on the loose, 200 police and military officers backed up by a helicopter, and a special “wolfcatcher”, were called in to look for the animal. Local authorities have now confirmed the mystery creature is not actually a tiger. The tiger story was always unlikely. Local zoos and a passing circus have denied knowledge of any animal escapes and the photograph of the “tiger” was unconvincing, typical of such sightings: low light level, on the brow of a hill and no scalable objects within view.   The animal does appear to be a cat, but the person who reported it did not report a tiger but a lynx.  Could this be a case of media exaggeration?  After all lynx are native to France and not a threat to humans, whereas tigers… If it is a lynx and a wild one then this in itself is newsworthy as this species is only, normally, to be found a few hundred kilometres from Paris. If it proves to be a more exotic big cat then its origins are of great interest.  It could be a “pet” from a private collection.  Although, I suspect just as in the UK few people in France are able to meet the legal requirements for maintaining such animals in captivity.  In the UK up until 1976 it was possible to keep animals such as tigers in your back garden, but the Dangerous Wild Animals Act put a stop to this. Since the 1950s there have been reports of big cats on the loose in the UK, a country where badgers count as big and scary. In the 1990s the fabled “Beast of Bodmin Moor” regularly made headlines. And over the years a number of cats such as pumas and lynx have been captured in the UK.   But these animals usually show signs that they were escapees from captivity.  Scientific evidence supporting big cats living wild in the UK has not really been forthcoming.  Most evidence is photographic and suffers from the same problems as the “Paris tiger” image.   Big cats can leave footprints, pugmarks, but they are most likely to be left when the ground is soft, after rain, and this can distort their size as the cat sinks into the mud.  One of the problems with big cats in the UK is that it tends to be investigated by amateurs who want to find a big cat and so their objectivity can be questioned.   A supposed photograph of the Paris tiger’s pugmark is clearly that of a dog: you can see the claws at the end of the toes.  All cats aside from cheetahs retract their claws when walking – something dogs cannot do. I worked at the Edinburgh Zoo for four years back in the mid 1990s and we never had an animal escape of any significance.  By this I mean nothing dangerous.  We did have a few animals go over the wall for a few minutes, but they always returned of their own accord.   The most memorable incident involved a group of cotton-top tamarins. These tiny monkeys could go where they wished within the zoo and one day they decided to visit a neighbouring hospital.  We know this because the hospital rang the zoo saying that one of their patients, who was taking strong pain killing medication, complained that he had seen gremlins at his bedroom window. Zoo animal escapes are rarer than we think.  We need to remember that there are around 10,000 zoos in the world housing about 4m animals and crucially the escape of a zoo animal is big news, especially if it is dangerous.  We live in a world of near misses. Think about crossing the road: everyday cars miss us by seconds but when one hits someone it becomes news.  It is thus that we perceive roads as dangerous and clearly if we crossed them without due care and attention they are dangerous.  Zoos build their animal enclosures with care and attention to avoid animal escapes. Of course there have been cases where zoo animals have escaped and injured or killed people such as in the San Francisco Zoo where a tiger killed a visitor and attacked two others in 2007.  An investigation into the case implicated those attacked in provoking the escape.  There has even been a case where a wild tiger entered Nandankana zoo in India to mate with the resident tigress.  Thus, we cannot assume animals escape because they don’t like their home.   At the Belo Horizonte Zoo in Brazil a female tamandua, a tree-living anteater, used to escape her enclosure into the surrounding wild habitat to find a mate and get pregnant.  Then she would return to the zoo to have her baby with all of the creature comforts that a zoo provides.  She did this several times. I for one will be surprised if the cat near Paris turns out to be a tiger: as a conservation biologist I will be delighted if it proves to be a wild lynx but unsurprised if it turns out to be a domestic cat."
"
Share this...FacebookTwitterLast year Germany’s Potsdam Institute (PIK) boasted that it had a superior El Niño one-year forecasting model, claiming 80% certainty. Today, a year later, its forecast emerges totally wrong and the prestigious institute is left humiliated. 
Hat-tip: Snowfan
In 2019, Germany’s Potsdam Climate Institute (PIK) boasted that it had a superior El Niño forecasting model, claiming one year in advance and with 80% certainty, there would be an El Niño event late in 2020 (upper curve is just an El Niño illustration). But the PIK model forecast flopped totally. The opposite has in fact emerged. Chart source: BOM (with additions).
One year ago, together with researchers of the Justus Liebig University Giessen (JLU), and Bar-Ilan University in Ramat Gan in Israel, Germany’s alarmist yet highly regarded Potsdam Institute for Climate Research (PIK) boldly declared in a press release there would “probably be another ‘El Niño’ by the end of 2020.”
PIK even boasted forecast model superiority
The PIK November 2019 press release bragged that its team of researchers had developed a new, far better model – which they said was capable of forecasting a late 2020 El Niño event a year in advance: “The prediction models commonly used do not yet see any signs of this,” the PIK press release wrote.
The PIK press release then called the early forecasting model approach “groundbreaking”, claiming it was based on a “novel algorithm” developed by its team. Their forecast relied “on a network analysis of air temperatures in the Pacific region and which correctly predicted the last two ‘El Niño’ events more than a year in advance.”
The results were even published in a journal: https://arxiv.org/abs/1910.14642


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“Conventional methods are unable to make a reliable ‘El Niño’ forecast more than six months in advance. With our method, we have roughly doubled the previous warning time,” stressed JLU physicist Armin Bunde, who initiated the development of the algorithm together with his former PhD student Josef Ludescher.
John Schellnhuber: “80% certainty”…”pretty significant”
Prof. Hans-Joachim (John) Schellnhuber, Director Emeritus of PIK, explained: “This clever combination of measured data and mathematics gives us unique insights – and we make these available to the people affected.” He pointed out that, of course, the prediction method did not offer one hundred percent certainty: “The probability of ‘El Niño’ coming in 2020 is around 80 percent. But that’s pretty significant.”
The 20% uncertainty ends up humiliating PIK physicists
Using data from the past and with the help of of their algorithm, the PIK scientists said El Niño events could then be “accurately predicted the year before”.
Today, one year later, in November 2020, we see that the opposite is in fact occurring, see chart above. Now the equatorial Pacific is entering a La Niña event instead of the almost certain El Niño claimed earlier by the now embarrassed PIK researchers.
Can’t even get one climate component over a single year right
The PIK’s “high certainty” forecast misses totally and so underscores the risks and pitfalls of being overconfident when it comes to still poorly understood complex systems.
And if scientists struggle predicting just one single regional component of the entire climate for just one year, then imagine what the reliability of their complete climate system predictions going out decades has to be. GIGO!


		jQuery(document).ready(function(){
			jQuery('#dd_6a44e38d298891ff14bbe070b492c213').on('change', function() {
			  jQuery('#amount_6a44e38d298891ff14bbe070b492c213').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"Since the 1980s, increasingly frequent and intense heatwaves have contributed to more deaths than any other extreme weather event. The fingerprints of extreme events and climate change are widespread in the natural world, where populations are showing stress responses. A common fingerprint of a warmer world is a range shift, where the distribution of a species moves to higher altitudes or migrates toward the poles. A review of several hundred studies found an average shift of 17km poleward, and 11 metres upslope, every decade. However, if temperature changes are too intense or lead species to geographic dead ends, local extinctions occur in the heat. In 2003, 80% of relevant studies found the fingerprints were seen among species, from grasses to trees and molluscs to mammals. Some migrated, some changed colour, some altered their bodies and some shifted their life cycle timings. A recent review of more than 100 studies found 8-50% of all species will be threatened by climate change as a result.  Currently, we have a disturbingly limited knowledge of which biological traits are sensitive to climate change and therefore responsible for local extinctions. However, a potential candidate is male reproduction, because a range of medical and agricultural studies in warm blooded animals have shown that male infertility happens during heat stress. However, until recently this had rarely been explored outside fruit flies in cold blooded animals. This is despite the fact that ectotherms – organisms that rely on heat in their environment to maintain a suitable body temperature – comprise most of biodiversity. Astonishingly, nearly 25% of all species are thought to be a beetle. The red flour beetle (Tribolium castaneum) is a useful ectotherm for large experiments on reproduction, as they can go from egg to adult in a month at 30°C. Females can store male sperm in specialised organs called spermathecae and they only need to keep 4% of a single ejaculate to enable them to produce offspring for up to 150 days. To look at the impact of heatwaves on reproduction, beetles were exposed to either standard control conditions or five-day heatwave temperatures, which were 5°C to 7°C above their preferred temperature. Afterwards, beetles mated and a variety of experiments looked for damage to their reproductive success, sperm form and function, and offspring quality. We found that 42°C heatwave temperatures halved the number of offspring males could produce relative to 30°C, with some males failing to produce any and mature sperm in female storage also experiencing damage from heatwaves. However, the reproductive output of pairs where only the females endured a five-day heatwave event was similar in all temperatures. The decline was likely due to a combination of males becoming worse at mating, less sperm being transferred, less sperm transferred being alive, less sperm being kept in the females’ spermathecae and more sperm being damaged and infertile. Two results were particularly concerning. These beetles, and many cold-blooded animals, can live for years and are likely to see multiple heatwaves. When we exposed males to two heatwave events, ten days apart, their offspring production was less than 1% of that of unheated males.  


      Read more:
      Wildlife winners and losers in Britain's summer heatwave


 This suggests that successive heatwaves can compound the damage of previous ones. The damage to offspring longevity and male fertility was another effect which was compounded over successive generations, and could lead to spiralling population declines. Knowing what aspects of biology higher temperatures could compromise is essential to understanding how climate change affects nature. Hopefully, this new knowledge can help predict which species are most likely to be vulnerable, allowing conservationists to prepare for the trouble ahead."
nan
"

Mount Kilamanjaro – Tanzania, Africa – still snowy. Photo by Neil Modie, January 2008
Last week, I broke the story of a press release issued by NOAA where they publish an opinion smashing any link between hurricanes and global warming saying that “There  is nothing in the U.S. hurricane damage record  that indicates global warming has caused a  significant increase in destruction along our coasts.”
Many readers may recall that Al Gore used hurricanes prominently in An Inconvenient Truth, and mentions hurricane Katrina specifically. Gore claims that increased hurricane activity is caused by global warming.
Last week, when the NOAA press release came out smashing any link between hurricanes and global warming, I wrote to my local newspaper editor, David Little, and said to him “Do you care to bet that AP and Reuters won’t run this story?” He responded: “I hope they do, it seems newsworthy to me.”
Well here is is, 4 days later, not a peep.
A Google search of news stories for “NOAA increased hurricane” (keywords of the press release) reveals a tiny handful of stories about the press release. Could you imagine though if the story said the reverse?  What if NOAA claimed they had established a definitive link between global warming and hurricanes. Oh my, the humanity of it all! Gloom, doom, death, destruction, angst, and demands for action on Kyoto. If it bleeds it leads. Compare to all the stories still circulating about hurricane Katrina and global warming.
Here is another story about a point from Gore’s AIT hit parade; Mount Kilimanjaro. Mr. Gore asserted that the disappearance of snow on Mount Kilimanjaro in East Africa was expressly attributable to global warming; “Within the decade, there will be no more snows of Kilimanjaro.” That was in 2005 in his movie An Inconvenient Truth.
Deforestation seems to be causing Mount Kilimanjaro’s shrinking glacier. Researchers think deforestation of the mountain’s foothills is the most likely culprit. Without the forests’ evapotranspiration of humidity into the air, previously moisture-laden winds blowing across those forests now blow drier. The summit, no longer replenished with water from those winds, started shrinking. Studies show the ice is evaporating through a process called sublimation. You can witness this effect at home, have you ever noticed that ice cubes left in your freezer tend to shrink with time?
Last year, a British Court ruled Gore’s point about Kilimanjaro not to be true.
So when a news story crossed my desk today that said: “Mount Kilimanjaro: On Africa’s roof, still crowned with snow” I had to wonder, will we see this one covered in the main stream media? Or maybe those beacons of truth over at Real Climate will make a note of it?
Don’t hold your breath. But, at least the New York Times travel section covered it. It seems more of a touristy thing to have snow on Kilimanjaro than a scientific issue of truth I suppose.
UPDATE: Kate over at SDA created a collage over time showing the snow of Mt. Kilimanjaro:



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea1177354',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
BUMPED for visibility. Originally published on 6/24. Bumped on 6/28 and again on 6/30
This poll will gauge reader perception to the issue that Dr. Hansen of NASA has recently raised that I cover in my post here. One vote per computer, and please spread this permalink to the poll far and wide to get a good mix of input across the blogosphere.

Click on a dot, then click the little yellow vote icon. Poll closed.
I will run this poll 1 week until next Wednesday at 9AM PST, at which time it will close. The results will be submitted to a member of the U.S. Senate for distribution, NASA’s director, and will also be mailed to Dr. Hansen at NASA GISS.
You can subscribe to the results of this poll by RSS. Simply copy the link below into your RSS reader.
http://polldaddy.com/pollRSS.aspx?id=49940E93EC30ACAF
NOTE: A couple of Pro-Hansen sites have staged a “crash party” for this poll. This has accounted for a huge increase in the votes for the first question overnight. This sometimes happens with online polls when agenda driven activists decide to skew it, which is the biggest weakness of online polls.
Addendum: Some other sites that are not Pro Hansen have also now linked to this poll, so I suppose it is becoming a battle between opposing views now. Agenda driven activists on both sides are at work now. 
Update 7/1 It appears that about 8000 votes were added for question 1 overnight. -Anthony
Update 7/2 9 AM PST Poll is closed, more here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e1960fd',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Question: Why does a major grocery store chain need a “comprehensive policy addressing climate change”?
Answer: They don’t.

The Atlanta Business Chronicle reports that one of the nations oldest and largest grocery firms, Kroger Inc., based in Cincinnati, OH rejected a shareholder proposal which called for the company to develop a comprehensive policy addressing climate change.
Having shopped at many a Kroger store myself, I’m glad I won’t be bombared with climate change messages while I shop. I really don’t need to know what the carbon footprint is on a can of soup or a head of lettuce.
Cincinnati-based Kroger (NYSE: KR) operates more than 2,400 supermarkets and multidepartment stores in 31 states.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e935c4a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Atlassian co-founder Mike Cannon-Brookes has thrown his support behind a climate action bill proposed by the independent Zali Steggall and has urged the major parties to put down the cudgels and support it. And the Australian Energy Council, representing major electricity and gas businesses, said the Steggall bill deserved to be seriously considered as it had the potential to deliver certainty and a path forward for the national economy.  Cannon-Brookes said on Tuesday the Steggall proposal, unveiled this week, was “a smart bill, and the exact type of action we need to change Australia’s international reputation on climate”. The bill includes a proposal for a net zero emissions target by 2050, a carbon emissions budget, and assessments every five years of national climate change risk. The MP has called on the major parties to bring the bill to the floor and allow a conscience vote. Cannon-Brookes said the proposal contained all the elements of a viable settlement to the climate wars. “The legislated 2050 target and five-year increments are precisely what is required, and the bill deserves bipartisan support.” Cannon-Brookes has been vocal in support of climate action in Australia, and has supported independents through Climate 200 – an environmental lobby group also supported by Simon Holmes à Court. But he has not played a hands-on role in drafting the current proposal. Sarah McNamara, the Australian Energy Council’s chief executive, said Steggall’s bill was a considered attempt to find a workable framework and had the potential to move the country beyond the climate policy impasse. “We would encourage that it be carefully considered and calmly assessed,” she said. “It offers the potential for much-needed certainty and a clear path forward not just for the energy industry, but for the Australian economy. For our part we will be consulting with our members in the coming weeks to assess the bill in detail.” It follows a declaration by the Business Council of Australia that Australia should work to achieve net zero emissions by 2050. Independent MP Zali Steggall – along with her fellow crossbenchers Rebekah Sharkie, Helen Haines and Andrew Wilkie – is seeking bipartisan support for a climate change framework bill aimed at transitioning Australia to a decarbonised economy.  This is what's in it. • Aims to limit global warming to well below 2C, pursuing efforts to limit it to 1.5C above pre-industrial temperatures. • Net zero carbon emissions target by 2050. • To achieve the target, the minister creates an emissions budget. • Establishes a Climate Change Commission to prepare a national climate change risk assessment every five years. • The commission is made up of a chair, Australia's chief scientist and five other members – who must have experience in either climate science, business, agriculture, environmental management, energy, transport or regional development. * The assessments cover the risks climate change poses to the economy, society, agriculture, environment and ecology. • In response, the minister creates a national adaptation plan. • The commission provides the minister with yearly adaptation plan progress reports. • Decisions made under the Act must be based on the best available science. • It abolishes the current Climate Change Authority, with the commission to take its place. While the BCA has at times in its history has been riven on climate action, and has actively stymied progress, the organisation’s chief executive Jennifer Westacott told the ABC on Monday night the time had now come to deliver policy certainty. “I reckon if we could get the two political parties to agree to that and legislate it, we would have made a massive advance in this country because we would know where we’re going,” Westacott said. While the BCA in 2018 described Labor’s more ambitious 2030 emissions reduction target as “economy wrecking”, in 2019 it joined other groups in representing industry, unions, farmers and investors under the Australian Climate Roundtable banner in calling for policies that could put it on a path to net zero emissions. On Monday night, Westacott characterised the Steggall proposal as “sensible”. Steggall’s bill will not be brought on for debate unless either the government or Labor supports it reaching the floor of the House. The government has not yet made a decision but it is unlikely to support it. On Tuesday morning, the Labor leader Anthony Albanese said it was highly unlikely the bill would be voted on “because that’s what happens with private member’s bills in the House of Representatives, unless the government agrees to allocate time for the bill, it will not be voted on”. Albanese said the proposal was very well intentioned, and he “respected” Steggall for bringing it forward, but told the ABC “we are unlikely to have a conscience vote on climate change. What we’ll do is support action on climate change.” The Labor leader said the opposition would commit to a long-term emissions reduction target “very soon” and, referencing an internal split within the Coalition about taxpayer backing for new coal plants, said: “I don’t think there is a place for new coal-fired power plants in Australia. Full stop.” On Sunday, Labor’s deputy leader Richard Marles, in a particularly awkward interview, did not rule out the party supporting new coal developments, saying it would be a decision for the markets despite previously declaring it would be a “good thing” if the thermal coal market collapsed."
"Stink bug. As names go it is a PR disaster, each of the words alone hardly endearing, and, in combination, wholly off-putting. Which is a shame because stink bugs have a perky charm, a distinctive style and, for an insect, a surprising concern for their offspring.  The trouble is that a new member of their clan is on its way to the UK: the marmorated stink bug, Hyalomorpha halys. The press is on the case, combining a distrust of anything arriving from the continent to live in the UK with dire warnings of damaged apples and nasty smells. Stink bugs deserve a better name and they have one: shieldbugs. This is the much more widely used name in the UK for the Pentatomidae, Scutelleridae and allied families of the order of insects called the Heteroptera, or true bugs. All of them have glands producing nasty smelling defensive secretions, but they are also united by many much more likeable characteristics. The UK has an enchanting selection of native species, such as the forget-me-not shieldbug, ornate shieldbug and parent shieldbug. A much more enticing set of names, hinting at their often striking form and behaviour.  Most are less than a centimetre long, either neatly oval or akin to a medieval knight’s triangular shield. Some such as the hawthorn and red-legged shieldbugs have sharply angled front corners to their thorax, giving a hint of 1980s shoulder pads.  Many are brightly coloured, black and white or with bold yellow or red.  Even the more conventionally dowdy brown or green species often sport zebra striped antennae and are edged with a black and white chequered border. The marmorated stink bug goes in for this chequered style too: marmorated refers to a marbled-effect pattern. The “marbled shieldbug”. That sounds much better. The marmorated stink bug often feeds on soft fruit which it probes using a long proboscis to suck the juices.  If fruit are attacked by large numbers of bugs the puncture wounds disfigure the crops and they are no longer marketable. Some bugs also have the potential to transmit crop diseases.  On the other hand this group of bugs includes many species that show devoted parental care for their eggs and newly hatched young. The adults stay close to batches of eggs, often squatting over them, to fend off predators. When the young hatch they cluster together much like a proud gaggle of primary school children on their first outing. What provokes the greatest ire, however, is the smell. Glands between the first and second pair of legs release a foul odour if the bugs are attacked. The stink from a marmorated bug is like a nasty version of coriander and very persistent.  Over the past decade the bugs have spread from their original home in China, Japan and Korea to become established as an invasive species in the US, where they are considered a major agricultural pest. They come into houses to hibernate and beleaguered home-owners who unwittingly try to dispose of them are left regretting their actions as the frightened bugs let rip with those armpit stink glands  I’ve had a soft spot for shieldbugs ever since receiving a letter from a member of public wondering if I could identify the strange insect she had found.  Inside the letter, immaculately displayed where the sorting machine had squished it flat, was a birch shieldbug, laid out with a precision to gladden the heart of the most painstaking curator.  While many dead insects shrivel or curl, the shieldbug’s tough design holds its shape, the curves and fins reminiscent of American 1950s automobiles.  The live bugs are even better. They tend to walk slightly high on their front legs lending an inquisitive air. They do not scuttle or jump but instead the legs on either side alternate back and forth much like a wind-up toy. Everything about them is slightly retro, a steampunk insect. The bug is likely to be on its way to the UK. A few stink bugs have been found in passenger luggage from the US but the real invasion threat is cross channel, part of a trend for continental insects to establish in the UK in recent years. Many have gone unremarked outside of specialist insect newsletters but others have attracted wider attention such as the willow emerald and small red eyed damselflies. This summer saw a widespread scatter of the scarce tortoiseshell butterfly, recorded only once before in the UK.  We like damselflies and butterflies. Less lovely is the spread of the bluetongue virus, a livestock disease that seems to have arrived with midges. What all these cases have in common is continental, warmth-loving species expanding their ranges north-westwards across Europe and hopping over to the UK. The marmorated stink bug is a good flyer. It is spreading. Maybe we should start calling it the marbled shieldbug and wait to see if it is quite the nuisance that its press suggests."
nan
"
Share this...FacebookTwitterMerkel No Longer Backs World Climate Treaty
That’s the headline announcing a report in the upcoming issue of Der Spiegel. German Chancellor Angela Merkel is retreating from the objective of reducing global CO2 emissions through a binding global treaty. http://www.spiegel.de/spiegel/vorab/0,1518,691013,00.html. 
…Merkel wants to avoid another debacle for Germany and Europe in the UN climate negotiations.
According to Der Spiegel, the climate conference that began in Petersberg near Bonn on Sunday shifts the focus to climate protection measures that can lead to measurable results without a binding treaty.
Federal Minister of the Environment Norbert Röttgen of Merkel’s CDU party told SPIEGEL of the new approach:
It’s not about giving up on the 2°C target; rather it is about finding new ways to reach it. At Petersburg  we want to create a new level on which we not only want to reach CO2 targets from the top,  but also to start projects from the bottom that lead to measurable results.
This includes protection of forests and more concrete cooperation in the transfer of environmentally friendly technologies.
Make no mistake about it, without the cooperation of Germany any global binding treaty mandating CO2 reductions becomes extremely unlikely, and that sends a clear signal that global Cap & Trade is all but dead. The US Senate can (and should) now kill Cap & Trade for good.
H/t: Rudolf Kipp at http://www.science-skeptical.de/
UPDATE…read the entire Der Spiegel article here in English (h/t Brian H): http://www.spiegel.de/international/germany/0,1518,691194,00.html
Share this...FacebookTwitter "
"
Share this...FacebookTwitterNaomi Orsekes
My how the times have changed. Climategate, and all the other gates surrounding it, have turned things inside-out. The science is far from settled, as many of us have long suspected. The ranks of sceptic scientists are swelling, public opinion has swung; even the Royal Society has adopted a new position on climate science –  by George, there might be more to it than CO2 molecules after all!  The mainstream media is slowly coming around, too.
Yet, others refuse to hear it. 
Here’s a Youtube clip of Naomi Oreskes’ Truth About Denial presentation in 2007. Some of you may have watched it already. That presentation is in two parts.
Part 1: The Truth Part (CO2 drives global warming, there’s a consensus, science is settled).
Part 2: The Denial Part (There’s a disinformation campaign out there, denying it all).
Okay, that was back in 2007. Back then global warming science looked convincing, and so maybe such a position was plausible.
But here’s Oreskes in March 2010  in a presentation called the Merchants of Doubt, which is pretty much the same as her 2007 Truth About Denial. Despite all the new revelations, scandals and shifting scientific viewpoints, Oreskes continues to play the same music.  In the 2010 presentation she continues to ask (paraphrasing):



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




How can there be so much scepticism in the public when there’s consensus among scientists?  Where does all the public doubt come from?
And answers by claiming it all stems from a tiny few merchants of doubt, who she describes as:
…a small but powerful group of people aided and abetted by well-funded think-tanks and a compliant mass media…not for money, but in defense of an ideology of laissez-faire governance, opposition to gevernment regulation in all forms.
Yes, ladies and gentlemen, Oreskes still believes, despite all the new revelations we’ve seen over the last few months, that all the scepticism and denialism out there today is still coming from the same sinister merchants of doubt. You’d think she’d would step back for a minute and re-evaluate her position. No chance.  Instead her reaction is to drive her head yet further into the sand.
Oreskes claims to be a science historian. My question is: Will she wake up and start a new chapter for the science history books? Or will she continue repeating her fairy tales? Don’t hold your CO2 breath.
UPDATE: Yesteday this post appeared at position No. 6 when one googled “Naomi Oreskes’ Denial”. Today it has dropped off to No. 17.
Share this...FacebookTwitter "
"
The survey project continues to move forward, even in these cold and snowy winter months. I’m pleased to announce that we have just passed the 500 mark for surveyed stations. Now with 41.1% of the network surveyed comprising 502 stations surveyed so far, that leaves 719 to go out of 1221 stations nationwide.
Some stations have recently become catalysts for larger investigations, such as the station in Lampasas TX, done by Julie K. Stacy which has brought out questions from a number of other bloggers. This prompted a review of stations previously surveyed, such as Cedarville, CA, which then prompted a larger investigation in the satellite city nightlights methodology used by NASA GISS. A whole new avenue of exploration has now opened up not just for US stations, but worldwide thanks to new features of Google Earth.
You never know where curiosity and serendipity will lead you. Thanks to Atmoz for starting the ball rolling. I also want to thank Barry Wise and Gary Boden, our early volunteers, whose help on this project has been indispensable.
Recently, this project got a significant endorsement from Dr. Roger Pielke of the University of Colorado in Boulder in his weblog. I and all the volunteers appreciate the recognition.
Here is the latest breakdown of USHCN stations that have been surveyed, and their site quality ratings:


We could really use some help this spring and summer in the following states:
Kansas, Nebraska, Arkansas, Alabama, Illinois, Idaho, Kentucky, Tennessee, Missouri, Mississippi, North Dakota,  South Dakota, Oklahoma, Texas.
If you think that you can help with this project by surveying a station near you, please visit the www.surfacestations.org website and sign up. We’ll provide instructions and help on locating stations in need of surveying.
You may also wish to consider signing up for the national flower and foiliage survey to help track climate change which is prominently mentioned on Dr. Roger Pielke’s weblog.  You can double your fun!


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea10c3fc4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Over the past few weeks Clover Hogan has found herself crying during the day and waking up at night gripped by panic. The 20-year-old, who now lives in London, grew up in Queensland, Australia, cheekbyjowl with the country’s wildlife, fishing frogs out of the toilet and dodging snakes hanging from the ceiling. The bushfires ravaging her homeland over the past few weeks have taken their toll. “I’ve found myself bursting into tears … just seeing the absolutely harrowing images of what’s happening in Australia – it is overwhelming and terrifying.”  Hogan said her lowest point came when she heard about the death of half a billion animals incinerated as the fires swept through the bush. “That was the moment where I felt my heart cleave into two pieces. I felt absolutely distraught.” The physical impact of the climate crisis is impossible to ignore, but experts are becoming increasingly concerned about another, less obvious consequence of the escalating emergency – the strain it is putting on people’s mental wellbeing, especially the young. Psychologists warn that the impact can be debilitating for the growing number of people overwhelmed by the scientific reality of ecological breakdown and for those who have lived through traumatic climate events, often on the climate frontline in the global south.. Until two years ago Dr Patrick Kennedy-Williams, a clinical psychologist from Oxford, had spent his career treating common mental health difficulties including anxiety, depression and trauma. Then something new started to happen. Climate scientists and researchers working in Oxford began to approach him asking for help. “These were people who were essentially facing a barrage of negative information and downward trends in their work … and the more they engaged with the issue, the more they realised what needed to be done – and the more they felt that was bigger than their capacity to enact meaningful change,” he said. “The consequences of this can be pretty dire – anxiety, burnout and a sort of professional paralysis.” Kennedy-Williams began to research the topic and realised it was not just scientists and researchers who were suffering. “There is a huge need among parents, for instance, who are asking for support on how to talk to their kids about this.” When Kennedy-Williams began focusing on young people he assumed most would be older teenagers or at least have started secondary school. But he soon discovered worrying levels of environment-related stress and anxiety in much younger children. “What I was most surprised by is how young the awareness and anxiety starts. My own daughter was just six when she came to me and said: ‘Daddy, are we winning the war against climate change?’ and I was just flummoxed by that question in the moment. It really showed me the importance as a parent of being prepared for the conversation, so we can respond in a helpful way.” He says there is no way to completely shield young people from the reality of the climate crisis, and argues that would be counterproductive even if it were possible. Rather, parents should talk to their children about their concerns and help them feel empowered to take action – however small – that can make a difference. A key moment for Kennedy-Williams came with the realisation that tackling “climate anxiety” and tackling the climate crisis were intrinsically linked. “The positive thing from our perspective as psychologists is that we soon realised the cure to climate anxiety is the same as the cure for climate change – action. It is about getting out and doing something that helps. “Record and celebrate the changes you make. Nobody is too small. Make connections with other people and at the same time realise that you are not going to cure this problem on your own. This isn’t all on you and it’s not sustainable to be working on solving climate change 24/7.” This certainly resonates with Hogan, who has set up Force of Nature, an initiative aimed at helping young people realise their potential to create change. Hogan’s group aims to target people aged 11-24 with a crash course in the climate crisis that helps them navigate their anxiety and realise their potential to get involved, take action and make a stand. “This is only the beginning,” said Hogan. “We’re going to see massive, massive widespread climate crisis in every country around the world, so it’s about developing the emotional resilience to carry on, but in a way that ignites really dramatic individual initiative.” Beyond climate anxiety – the fear that the current system is pushing the Earth beyond its ecological limits – experts are also warning of a sharp rise in trauma caused by the experience of climate-related disasters. In the global south, increasingly intense storms, wildfires, droughts and heatwaves have left their mark not just physically but also on the mental wellbeing of millions of people. For Elizabeth Wathuti, a climate activist from Kenya, her experience of climate anxiety is not so much about the future but what is happening now. “People in African countries experience eco-anxiety differently because climate change for us is about the impacts that we are already experiencing now and the possibilities of the situation getting worse,” she said. She works with young people through the Green Generation Initiative she founded and sees the effects of eco-anxiety first-hand. A common worry she hears among students is: “We won’t die of old age, we’ll die from climate change.” Extreme climate events can create poverty, which exacerbates mental health problems, and Wathuti says she has seen stress, depression and alcohol and drug abuse as some of the side-effects of climate anxiety and trauma in her country.  Even in the UK, a recent study by the Environment Agency found that people who experience extreme weather such as storms or flooding are 50% more likely to suffer from mental health problems, including stress and depression, for years afterwards. More than 1,000 clinical psychologists have signed an open letter highlighting the impact of the crisis on people’s wellbeing and predicting “acute trauma on a global scale in response to extreme weather events, forced migration and conflict”. Kaaren Knight, a clinical psychologist who coordinated the letter, said: “The physical impacts related to extreme weather, food shortages and conflict are intertwined with the additional burden of mental health impacts and it is these psychologists are particularly concerned about.”  She added that fear and trauma “significantly reduced psychological wellbeing”, particularly in children. “This is of huge concern to us and needs to be part of the conversation when we talk about climate breakdown.” One of the high-profile signatories of the letter, Prof Mike Wang, the chair of the Association of Clinical Psychologists UK, said: “Inaction and complacency are the privileges of yesterday … Psychologists are ready and willing to help countries protect the health and wellbeing of their citizens given the inevitable social and psychological consequences of climate change.” This rallying of the psychological profession around the climate crisis has led to experts around the world forming groups to research and treat the growing number of people caught up in the unfolding crisis, attempting to help them move from fear and paralysis towards action. But even for those who are following this advice, the scale of the emergency is taking its toll. Kennedy Williams – who has set up his own group, Climate Psychologists, specialising in climate anxiety – said he and his colleagues were not immune from the psychological impacts of the crisis. “This is such a universal thing that [we] have all been through our own set of climate-related grief and despair, and we talk about riding the wave between hope and despair … it is absolutely as real for us as it is for anyone else.” Remember that you do not need to be a climate expert It’s OK to explore learning together. If your child asks a question you can’t answer immediately, respond by saying: “What a great question. Let me look into that so I can answer it properly.” Try to validate, rather than minimise, children’s emotions If children express anxiety, it’s much better to say: “It’s OK to feel worried. Here is what we can do about it,” than to say: “Don’t worry. It’s all fine.” But always try to support this emotion with suggestions for positive action. Negative information hits harder Bad or threatening facts tend to resonate more strongly – and therefore stick in the mind. So try to balance one piece of negative news with three pieces of positive news. Have some examples of good climate-related news ready – for example, successful conservation projects. For younger children, keep it local and tangible Suggest litter picks and school events. For teenagers, encourage them to stay connected at a wider level – help them write to their MP, take part in protests and join local communities and campaigns. Set practical goals as a family and follow through Record and celebrate your climate successes together (even a piece of paper on the fridge door). Reinforce the message that small actions can make a big difference."
"What do Beyonce, Hitler, David Attenborough, Darth Vader and GoldenPalace.com all have in common?  They all have species named after them.  In the case of Beyonce it is an Australian horse fly whose striking golden behind apparently inspired the scientists to give this species the scientific name Scaptia beyonceae. Most species do not have such frivolous scientific names.  Last week a new species of frog was described from New York City.  It has been named Rana kauffeldi, in honour of the American herpetologist Carl Kauffeld who in the 1930s predicted a new species of leopard frog would be found on the east coast of the US. What’s in a name? And why don’t scientists simply number species?  The scientific name is not an arbitrary label, well at least not the first part, which tells us the genus of the species.  From knowing this we can start to understand the evolutionary relatedness between species.  For example, chimpanzees and bonobos both come from the genus Pan; whereas humans are from Homo.  Thus, as a scientist I know that chimpanzees and bonobos are closer to each other than they are to humans.  Scientific names according to the naming rules for species must be unique and show evolutionary relatedness; that is, relate to the importance of common ancestor species. The reason we need scientific names and not just common names is to permit scientists to precisely identify the species they are investigating.  Returning to our New Yorker Rana kauffeldi, there are 15 species of leopard frog and in many countries common names are generic or vary by region.   I have spent many years studying titi monkeys in Brazil of which there are more than 20 species, but in Minas Gerais where I study them they are all referred to in Portuguese as guigó.  Once I was giving a talk at the University of São Paulo in Brazil about my titi monkey research and I noticed a look of puzzlement on the audience’s faces until I showed a slide of my study animal when the audience collectively shouted out sauá. If I had use the monkey’s scientific name I would have avoided ten minutes of bemused expressions. The second part of a scientific name is chosen by whoever first described the species in a scientific journal, and this is where opportunity lies.  Many species names refer to physical characteristics, such as Artibeus hirsutus for the hairy fruit eating bat, its geographic location Ovis canadensis for bighorn sheep or after an appropriate eminent scientist such as Rhinoderma darwinii for Darwin’s frog.  A successful taxonomist may sell the rights to a person or a company to award a species its name. The Golden Palace titi monkey (Callicebus aureipalatii) is a case in point.  This online gambling site in 2004 paid US$650,000 to name the species and the funds were used for the conservation of the monkey’s habitat in Bolivia.  There are now websites where you can bid to name species.  Some people have criticised this approach to raising funds for species conservation as being vulgar and too commercial, liking it to how sponsorship has taken over sport. When I was a child there was “The FA Cup”, these days it is the “The FA Cup with Budweiser” – a change that has generated a lot of money for football.  Yet such sporting examples are ephemeral: the FA Cup will not always belong to the same sponsor. Scientific names are permanent however, and can only be changed in accordance with the rules of the International Code of Zoological Nomenclature.  Thus, while it may be extremely unpalatable to think there is a beetle, which in 1937 was named  Anophthalmus hitleri – the rules do not permit a name change. There has been alarm that some companies, which do considerable environmental damage, might use species naming as a form of greenwashing.  What I would like to suggest here is that rather than paying a one-off fee to name a species companies would need to pay into an environmental endowment fund. Thus, the impact of their funds would be positive for the environment in the long-term. Each year approximately 15,000 new species are given a formal scientific name, creating lots of sponsorship opportunities.  Of course companies will prefer to sponsor charismatic species such as monkeys, dolphins or parrots.  While such species aren’t as common as new insects, there should be enough to go around.  In Brazil, a new primate species is discovered on average once a year.   Since funds would be to protect the sponsored species’ habitat this will result in the protection of the non-cute species in that habitat.  Thus, sponsored animals would become what we conservation biologists call umbrella species, inadvertently sheltering others in their habitat."
"

The recession of 2007–2009 knocked the wind out of state government budgets. Yet, as revenues have risen steadily in recent years, some governors have pursued reforms to reduce tax burdens on families and make their states more competitive. Other governors have used rising revenues to expand programs. In their biennial survey, **“Fiscal Policy Report Card on America’s Governors 2014” (White Paper)** , Nicole Kaeding, a Cato budget analyst, and Chris Edwards, director of tax policy studies at the Institute, use statistical data to grade the governors on their taxing and spending records. “Reading the report card and other works by the institute may change some minds,” according to Forbes​.com. “But more importantly, it broadens the debate over the role of fiscal policy in particular and government more generally.” Four governors were awarded an “A” on this report card: Pat McCrory of North Carolina, Sam Brownback of Kansas, Paul Le‐ Page of Maine, and Mike Pence of Indiana. Eight governors were awarded an “F”: Mark Dayton of Minnesota, John Kitzhaber of Oregon, Jack Markell of Delaware, Jay Inslee of Washington, Pat Quinn of Illinois, Deval Patrick of Massachusetts, John Hickenlooper of Colorado, and Jerry Brown of California. “With the economy currently growing, governors and legislatures are having few problems balancing their budgets in the short run, but the states face major budget challenges down the road,” the authors write. At the same time, global economic competition is making it imperative that states improve their investment climates.



 **IS PRESCHOOL EFFECTIVE?**  
Demands for universal preschool programs have now become commonplace, reinforced by President Obama’s call for “highquality preschool for all” in 2013. Yet as David J. Armor, professor emeritus at George Mason University, points out in **“The Evidence on Universal Preschool” (Policy Analysis no. 760)** , any program that could cost state and federal taxpayers $50 billion per year warrants a closer look at the evidence on its effectiveness. This paper reviews the major evaluations of preschool programs, including both traditional programs such as Head Start and those considered high quality. As it turns out, these evaluations do not paint a generally positive picture. “The most methodologically rigorous evaluations find that the academic benefits of preschool programs are quite modest, and these gains fade after children enter elementary school,” Armor writes. This is the case for Head Start, Early Head Start, and also for the “high‐​quality” Tennessee preschool program. Two other high‐​quality programs have been evaluated using a rigorous experimental design, and have been shown to have significant academic and social benefits, including long‐​term benefits. These are the Abecedarian and Perry Preschool programs. However, the groups studied were very small, they came from single communities several decades ago, and both programs were far more intensive than the programs being contemplated today. Armor concludes, “Before policymakers consider huge expenditures to expand preschool, especially by making it universal, much more research is needed to demonstrate true effectiveness.”



 **GOOD INTENTIONS, IMPOVERISHED RESULTS**  
Over the last half century, federal and state governments have spent more than $19 trillion fighting poverty. But what have we really accomplished? In **“War on Poverty Turns 50: Are We Winning Yet?” (Policy Analysis no. 761)** , Michael Tanner, a Cato senior fellow, and Charles Hughes, a research associate at the Institute, argue that, although far from conclusive, the evidence suggests that we have successfully reduced many of the deprivations of material poverty. However, these efforts were more successful among socioeconomically stable groups such as the elderly than low‐​income groups facing other social problems. “Moreover, other factors like the passage of the Civil Rights Act, the expansion of economic opportunities to African Americans and women, increased private charity, and general economic growth may all have played a role in whatever poverty reduction occurred,” the authors write. Nevertheless, even if the War on Poverty achieved some initial success, the programs it spawned have long since reached a point of diminishing returns. In recent years we have spent more and more money on more and more programs, while realizing few, if any, additional gains. We may have made the lives of the poor less uncomfortable, but we have failed to truly lift people out of poverty. This should serve as an object lesson for policymakers today. “Good intentions are not enough,” Tanner and Hughes conclude.



 **WORK DISINCENTIVES**  
The Social Security Disability Insurance (SSDI) program faces imminent insolvency. Annual expenditures totaled $143 billion in 2013, but program receipts amounted to $111 billion—a shortfall that is projected to continue indefinitely. In **“SSDI Reform: Promoting Gainful Employment while Preserving Economic Security”(Policy Analysis no. 762)** , Jagadeesh Gokhale, senior fellow at the Cato Institute, points out that, according to the Social Security Trustees, the program’s trust fund will be fully depleted in 2016, compelling either a large benefit cut or a large tax hike. Neither option will be politically popular. Regardless of the program’s insolvency, SSDI creates substantial work disincentives, causing many with medical impairments who could work to withdraw from the labor force and apply for SSDI. Gokhale advocates a change in the structure of SSDI’s benefit payments to those admitted to the program. Shifting benefits at the margin toward paying beneficiaries to work rather than to remain out of the work force would encourage beneficiaries with residual capacities to return to work. “That shift would serve as a backstop to reduce the economic loss from wrongful allowances of applicants into SSDI,” Gokhale writes. “Such a switch in benefit design can be accomplished without compromising benefit eligibility for those who cannot work.” In this analysis, he explains how to implement such a change to SSDI’s benefit structure and the advantages that would accrue from it.



 **DISTORTING TRADE**  
The use of antidumping measures to protect certain domestic industries may be the most widely abused trade policy instrument worldwide,” writes K. William Watson, trade policy analyst at the Cato Institute. In **“Will Nonmarket Economy Methodology Go Quietly into the Night? U.S. Antidumping Policy toward China after 2016” (Policy Analysis no. 763)** , Watson argues that U.S. authorities reserve their most punitive and abusive practices for goods from China. In those cases, the United States sets antidumping duties using what is called nonmarket economy (NME) methodology. The practice gives license to the U.S. Department of Commerce to ignore Chinese producers’ cost and price data and to turn, instead, to estimates for those data that are punitive and unrealistic. Current WTO rules permit the United States to maintain this discriminatory approach, but that condition will expire in December 2016. Absent a major change in the mindset of U.S. trade officials with respect to Chinese treatment in antidumping proceedings, it is unlikely that the United States will bring its policy into compliance. Watson presents some of the alternative scenarios that might unfold as the expiration date approaches. “The policy that would best serve a strong U.S. trade agenda and the American public is to end NME treatment of China by no later than December 2016,” he concludes. Nondiscriminatory treatment of Chinese imports would bring U.S. trade policy into compliance with WTO rules while reducing the distorting effect of antidumping measures on the U.S. economy.
"
nan
"

 **Lecture at the Friedrich Naumann Foundation.**



Introduction



Trade negotiators, policy analysts, media and others interested in the Doha Round of multilateral trade talks have been asking the same question since the end of the ministerial meeting in Hong Kong in December: where do we go from here? The question implies, of course, that the Doha Round is in serious trouble. Well, that may very well be true.



To find a literal answer, though, one is advised to look to the “Hong Kong Declaration,” which is a statement of recommitment by the ministers to the goal of reaching a comprehensive Doha Round agreement by the end of 2006. The Declaration provides the usual diplomatic platitudes about the nobility of the efforts undertaken and the virtuousness of the goals being pursued. But the document also provides some concrete guideposts to success, from which the enormity of the task at hand can be inferred.



The goal is to complete the negotiations by the end of this year. By April 30, “modalities” (framework and formulae) for the agricultural and non‐​agricultural market access (NAMA) negotiations must be accomplished, and by July 31 the actual numbers to plug into those formulae must be agreed. Meanwhile, all requests for services liberalization are to be made by the end of February, and corresponding offers are to be tabled by July 31.



While no interim deadlines were set for several other items on the Doha Agenda, including the important rules negotiations (which cover the contentious issue of antidumping reform), all of these negotiations will have to produce outcomes that, when considered together, enable 150 trade ministers to agree to the single undertaking that will be known as the Doha Agreement. And all that within 10 months!



WTO Director General Pascal Lamy has been out pounding the pavement, meeting with delegations far and wide, offering encouragement to keep at the negotiations. He says the negotiations are about 60 percent complete, and that the impending deadlines will help “focus minds.” How he calculates the 60 percent figure is a bit mysterious, since most of the contentious decisions have thus far been deferred. While there have been fruitful meetings in the various negotiating committees since Hong Kong, the reports coming out of those meetings show how much still needs to be done.



Meanwhile, EU Trade Commissioner Peter Mandelson and U.S. Trade Representative Rob Portman have been on the diplomatic trail, attempting to convince developing countries that liberalization in their industrial and services industries are in their own interests. That is most certainly true. However, the truth about trade was one of the first victims of the Doha Round. Accordingly, skepticism abounds among trade policymakers and experts in Washington, Brussels, Geneva, and elsewhere regarding prospects for an ambitious outcome to the Doha Development Round. I share that skepticism.



The concept of a Doha‐​lite, which means a far less ambitious agreement than envisioned when the Round commenced, will have to be embraced. It may be the only way to avert failure of the Round, and the residual damage that could cause the WTO.



Why Are We Stuck?



The question of where do we go from here requires an assessment of why we are at this impasse in the first place. There is plenty of blame to go around.



The most obvious answer is agriculture. For almost four and a half years, the emphasis of the negotiations has been on agricultural. Yet little progress has been registered. Rich country farm supports and agricultural tariffs are egregious and should be dismantled, not only because of their adverse impact on poor countries, but because they constitute a waste of limited resources. Taxpayers in the United States and Europe should not be forced to subsidize their well‐​to‐​do farmers, particularly when government budgets have grown out of control. Farm reform is a matter of domestic fiscal necessity more than anything else.



In the months leading up to the Hong Kong ministerial, there was a flurry of activity in the agricultural negotiations. The United States and Europe submitted fairly comprehensive proposals and counter‐​proposals in an effort to inject some momentum into the discussions before Hong Kong. But any momentum initially created soon subsided after several members concluded that the European proposal was far less ambitious than was the U.S. proposal. Without European willingness to go further–to at least the level of reform reflected on paper in the U.S. proposal–there would be little room for substantive progress in Hong Kong.



If nothing else, Hong Kong constituted a public relations victory for the United States. One of the great failings of the United States in the Doha Round was its willingness to appear in lock step with Europe on the agriculture agenda at the ministerial meeting in Cancun in 2003. The appearance to the developing countries that the rich countries were working to scuttle meaningful reform inspired the creation of the G-20, and ultimately the collapse of the talks in Cancun.



In my view, the only way to avert a similar disaster in Hong Kong was for a bold agricultural proposal to be on the table. The fact that big differences were observed between the U.S. and European proposal sent an important signal to the developing countries that the rich countries were no longer in lock step. Europe has taken this development on the chin.



Europe was left isolated as the primary villain in the agriculture saga after Hong Kong, and Peter Mandelson has looked nothing but defensive since then. Mandelson has come under a great deal of criticism for his efforts to dismiss U.S. proposals as disingenuous or simply too ambitious to be practicable–and I largely agree that his rhetoric and tactics have been less than diplomatic–but I also agree with Mandelson’s proposition that Europe should go no further on agriculture unless and until it sees movement on NAMA and Services. After all, this is supposed to be a single undertaking where everything is on the table before an agreement can be reached. The problem is that the developing countries (Brazil and India, in particular) don’t see it that way. And it is they who will determine the Doha Round’s fate.



There is a larger context for understanding why progress in the Doha Round has been scant.



First, in the years between 1995 and 2001 (when Doha was launched), there was a lingering sense of betrayal among some developing countries, a perception that the Uruguay Round was a big success for the rich countries, and gave little to the developing countries. Considering that the most significant “concession” from the rich to the developing countries was the Agreement on Textiles and Clothing (ATC), there is a basis for understanding the sense of betrayal.



The ATC was an agreement to end the decades‐​old quota system, known as the Multifibre Arrangement, which allowed restraints by the United States, Europe, Norway, and Canada on imports of textiles and clothing from almost every developing country. The ATC specified a 10‐​year phase out of the quotas in four stages. At each stage, a minimum percentage of products subject to quota in 1994 were to be liberalized from quota, and the growth rates in remaining quotas were to be accelerated.



But while the United States and Europe may have adhered to the letter of the Agreement, each certainly violated its spirit. The United States chose to liberalize from quota in the first stage (1995) products such as tents, parachutes, awnings, sails, and other products that had never been subject to quota in the first place! Most meaningful liberalization was deferred until the final two stages in 2002 and 2005. In fact, approximately 80 percent of the products subject to U.S. quota in 1994 remained under quota until the final day, January 1, 2005.



Europe was guilty of “backloading” its liberalization too, but to a lesser degree. Instead, Europe used the special safeguard mechanism to curtail import growth after quotas were removed, oftentimes bringing cases with the flimsiest of evidence. Many developing countries harbor the somewhat justified belief that they were double‐​crossed by the rich countries in the Uruguay Round. Their seemingly unbending negotiating postures this Round reflect the lessons of the past.



Then, the terrorist attacks hit America in September 2001, which started to focus attention on what might be the root causes of such violent upheaval. Economic stagnation in the developing world was identified as one of the important causes.



Two months after the attacks, in an effort to show solidarity among the world’s leaders and with a virtuous sense of purpose to start tackling the economic problems in the developing world, the Doha Round was launched and dubbed the “Doha Development Agenda.”



But the development emphasis of the round reflects factors beyond the desire to address economic stagnation and to redress perceived and real grievances of the past. It also reflects the reality of the WTO’s composition. Since the WTO was established in 1995, its membership has grown by 25 percent, and each new member is a developing country. The goal of liberalizing trade by cutting tariffs, which dominated the GATT agenda for most of the post‐​war period, has been transformed into an agenda of development‐​oriented goals, which have not always been hospitable to trade liberalization.



There are now 150 members in the organization with disparate levels of economic development, different negotiating priorities, and asymmetric negotiating resources, attempting (presumably) to reach consensus on a diversity of issues. Add to the mix, the emergence of the anti‐​globalization movement and all the NGOs it has spawned proliferating sometimes good, but usually bad advice to the developing countries. The idea that rich country trade barriers are a primary cause of poor country poverty and that poor country barriers are justified and should not be negotiated away not only stokes the flames of an already pronounced (and somewhat justified) sense of victimization among developing countries, but it also provides the wrong prescription. Furthermore, the bickering between Europe and the United States over the question of who does more for the poor countries lends further credibility to the “victim” position successfully staked out by the developing countries. Why should they offer any market openings when the rich countries tit‐​for‐​tat exercise just might excuse any liberalization from the poor countries?



All of these factors considered together have conspired to create a situation where the developing countries feel that they shouldn’t have to do much in the way of opening up their own markets.



On top of these misguided beliefs, the developing countries have an ace in the hole to back up an uncompromising negotiating position: Brazil’s successful complaints in the WTO against the U.S. cotton program and the EU sugar program. Brazil believes it has already achieved some of the cuts in agricultural subsidies that are being negotiated in the Doha Round. Brazil feels that a large chunk of the reforms being offered by the EU and the US are not concessions at all–that the reforms already have to be made or else, Brazil and other countries can litigate them in dispute settlement with precedence to back them up. The United States and Europe know they are vulnerable on this.



There are yet other important reasons for the Doha impasse. The emergence of China may be the most critical. Many members are scared of the implications of China’s growth, including Europe, which will be imposing new antidumping duties on footwear very soon, and the United States, where Congress is threatening some very provocative, reactionary legislation this election year. But if Europe and America worry about import competition from China, think about how every developing country must feel. China does or can produce almost anything the developing countries can produce. Thus, there is an aversion or even unwillingness among some countries to agree to tariff cuts in the Doha Round because they are afraid of Chinese competition.



Still another reason for Doha’s roadblock: the proliferation of bilateral and regional trade agreements. While these types of trade agreements are not necessarily mutually exclusive with multilateral agreements, the danger is that they can become so.



In 2002, then-U.S. Trade Representative Robert Zoellick announced a policy that he described as “competitive liberalization,” which meant that the United States would pursue bilateral, regional and multilateral agreements simultaneously. The rationale behind the policy was that trade agreements–particularly multilateral ones–can take a long time to materialize, and possibly might not materialize at all. To insulate U.S. trade policy goals from failure due to the limited ambition of others, and to avoid putting all eggs in one basket, Zoellick announced that the U.S. would pursue several alternatives at the same time.



The Free Trade Area of the Americas was to be the main thrust of U.S. liberalization efforts outside of the Doha Round. Beyond the stated goal of providing options for U.S. trade policy, “competitive liberalization” had the strategic benefit of showing the rest of the world that the United States had viable alternatives outside of Doha. The not so subtle message of competitive liberalization was that within the Doha negotiations the United States should no be pushed too hard for concessions and that U.S. demands should be taken seriously.



But the policy took a major blow when it became apparent that the FTAA was going nowhere, primarily because of resistance from Brazil, which was insisting on the same reforms being demanded in the WTO–agricultural and antidumping reform. So, the United States moved to isolate Brazil by concluding its bilateral agreement with Chile, and then announcing negotiations with Central America and several Andean countries. While these negotiations were underway, the political and economic climate in Latin America began to change for the worse and support for the FTAA all but totally dissipated. The United States no longer had a viable alternative to use as leverage for its Doha agenda.



Meanwhile, other countries, particularly in Asia and the Pacific, embarked on bilateral and regional discussion as well. In many regards, several Asian countries have more to show for their efforts than does the United States. There should be little question that prospects like an ASEAN plus China union or an Australia‐​China free trade agreement or a U.S.-Korea free trade agreement undercut at least some of the enthusiasm for a multilateral deal. It also stretches limited negotiating resources, perhaps too thin.



A final, but also very significant explanation for the lack of progress in Doha is that there might not be sufficient interest in a deal from the developing countries. One picture that remains indelibly in my mind is that of several developing country trade delegations and various NGOs, upon learning of the collapse of the talks in Cancun, jubilantly embracing, dancing, and slapping hands in the lobby of the Cancun convention center. I couldn’t quite understand why they should be so happy. At that point, there was fear that the whole round might be dead–a round that, if concluded, would bring many benefits to these poor countries. There reactions, I thought, were antithetical to what they should have been feeling.



The point that this drove home for me was that some developing country negotiators and their governments get a lot of political mileage back home when they are seen standing up to the rich countries. Reaching an agreement would eliminate that stage, and could probably subject them to criticism that they got duped again. Furthermore, I have to believe that some developing country leaders would rather have a deadlock on Doha so that they can continue to blame the rich countries for their woes. Eliminating agricultural subsidies and tariffs, which are only a small part of the broad problems facing developing countries, could expose the domestic problems caused (or not resolved) through their own errors of commission or omission.



There are thus plenty of explanations for the Doha Round’s stasis.



Failure is not an Option



Failure to reach a Doha Agreement by the end of the year could be more severe than simply missing the opportunity to expand trade this go around. In fact, there would likely not be another go around for years to come.



Failure would produce an immediate round of finger pointing, as countries position themselves to deflect blame. This will hasten antagonisms between countries that will have spent 5 years in vain trying to work through difficult issues. It could produce conclusions that there is little real interest in trade liberalization, which could harden perceptions of victimization and distrust. Domestic constituencies that opposed trade liberalization in the first place will be energized by the turn of events, and their views could win favor among a broader cross‐​section of their populations.



Brazil and others would likely prepare more WTO challenges of U.S. and European agricultural policies. In the United States, where Congress has been outspoken and critical of WTO rulings, more adverse rulings would not have a welcome reception. At a time when U.S. congressional antipathy toward trade is rising, it is possible that there would be more calls than usual to ignore WTO findings. Simultaneously, there would be calls for the United States to bring more cases against China (in particular).



If those unfriendly, even hostile sentiments begin to take root, particularly in the absence of an ongoing trade negotiating round, questions regarding the efficacy of the existing rules and the legitimacy of the WTO itself might not be far behind. Doha failure could lead to an erosion of respect for the rules and institutions that have helped expand international trade and investment and have contributed significantly to the economic growth and rising living standards experienced throughout the world over the past 60 years.



A weakened (or merely the perception of a weakened) rules‐​based system of trade could invite a resurgence of protectionism, as countries recoil from previously‐​made commitments. And with international trade and investment flows increasing rapidly on a account of the emergence of China, India, and other formerly smallish economies, politically expedient protectionist policies might prove tempting, as countries grapple with the question of how best to respond to dramatically changing economic circumstances. Fidelity to the rules and institutions will be needed more than ever at a time when temptation to dispense with them is heightening.



Doha’s failure could lead to an increased parceling of the world economy as countries turn more aggressively toward bilateral and regional agreements. While there has been much scholarly debate about the efficacy of bilateral and regional agreements, much of their intellectual support derives from the belief that they are complementary to multilateral deals, and not a substitute for them. Broad, nondiscriminatory trade liberalization under homogenous rules is generally more conducive to producing gains from trade than are discriminatory agreements between subgroups, which could be trade diverting. The so‐​called spaghetti bowl of rules raises the cost of compliance as well.



Another problem with bilateral and regional agreements is that agricultural and antidumping reform would likely be immune from liberalization–as they have been in the past. Furthermore, developing countries tend to be excluded for these types of arrangements, as richer countries tend to cherry pick their prospective partners.



Thus, Doha failure is not a viable option.



Where do we go from Here?



Efforts must be undertaken to ensure that Doha doesn’t fail (which does not mean that an ambitious outcome is necessary). Brazil, India and other large developing countries are in the driver’s seat, but they are on the verge of overplaying their hand. They, and the other developing countries (G20 and G90, alike), would be hurt more from a Doha collapse than would the rich countries. More pressure has to be put on these bigger developing countries to show greater willingness to reduce applied industrial tariffs, not just bound rates.



Developing countries need to be disabused of the belief that it is their right, and in their interest, to do nothing toward reducing their own tariffs. Unless they can show that their economies are opening and that their rules are transparent and that their country is a good place to do business, they are going to get crushed as globalization advances. In this era of just in time, hub and spoke world supply chains, countries are competing with each other for international investment. Investment flows to regions where there is greater certainty in the business and political environment. And where there are fewer frictions and lower costs of doing business. Protectionist policies are anathema to a business‐​friendly environment. Without that environment, the investment won’t come. Without investment, you fall farther behind.



All that being said about how doing more, much more, is in the developing countries own interest, the onus remains on the rich countries to get a deal done. Sustained economic growth in the developing world is an objective shared by countries rich and poor. This objective transcends economics too. It is a matter of profound foreign policy and security policy interest for the United States and Europe, as well.



These geopolitical aspects of the Doha Round need to be trumpeted by Peter Mandelson and Rob Portman, as they start to downplay expectations that a Doha Agreement will bring huge short‐​term benefits to their exporters. The offensive agenda of broadly opening developing country agricultural, non‐​agricultural, and services markets needs to be downgraded. But there are still important benefits to tout.



First of all, a Doha failure, as I argued earlier, would be worse, far worse, for rich country exporters than a deal that only shows gains on paper for developing countries. A deal that benefits the developing countries disproportionately would improve prospects for U.S. and European exporters by giving their prospective developing country customers greater opportunity to earn foreign exchange. This will increase demand for imports, which could inspire greater sales for American and European businesses. Meanwhile, access of rich country producers to cheaper imports will help lower their own costs of production, which could create opportunities for selling at lower prices and thus competing more effectively in developing countries.



Furthermore, liberalization of rich country markets without any rigid demands that developing countries follow suit could inspire what Jagdish Bhagwati calls “sequential reciprocity.” Without the external pressure of negotiations, countries have in many cases come to the realization that reform and trade liberalization was in their interest. India, China, Mexico, Chile, New Zealand, Australia, Singapore, and Hong Kong, to name a few, have all unilaterally liberalized their trade regimes at one point or another without the external pressure that negotiations bring to bear.



As countries grapple with their own policies to find out how best to compete in this dynamic and increasingly linked world economy, perhaps it is better for them to come to their own conclusions at their own paces.



Certainly, it is important that Lamy, Mandelson, and Portman continue to apply some pressure to the G-20 to do their part in offering enough in the way of NAMA and services liberalization so that a plausible, face‐​saving deal can be accomplished. But they shouldn’t push too hard. It could backfire. If developing countries are compelled to accept a level of barrier reduction with which they are not comfortable, then they will be more apt to blame any domestic discontent associated with adjustment on the rich countries for forcing the deal on them. That could inspire a difficult backlash against trade, its institutions, and the countries that advocate it.



The best hope for Doha is an agreement that compels the rich countries to eliminate distorting farm programs and to eliminate or substantially reduce tariffs on products important to the developing countries. Those outcomes are necessary regardless of the other components of the deal. Negotiators should be sure, then, to understand that “Doha Lite” is far preferable to Doha failure.



Thank you.
"
"
There has been a lot of discussion lately about the accuracy of measuring Sea Surface Temperatures prompted by a new study from Phil Jones from the University of East Anglia and Director of UEA’s Climatic Research Unit. The measurement issue for sea surface temperatures that Dr. Jones is studying was recently showcased in an article in the UK Independent.
I’m going to present the article here first, and then we’ll talk about how sea surface temperatures have been measured, and what sorts of issues the changes between cloth buckets, metal buckets, and engine inlets actually entails.
At first glance, I see this issue raised by Phil Jones as not being well thought through, and ignoring the measurement environment actuality, instead focusing on the change in bucket types as being “absolute”. I think it has a lot of grey area, and a lot of potential errors that haven’t been considered. I’ll cover those in the next part, but for now please read the article and let me know what you think.
Case against climate change discredited by study
By Steve Connor, Science Editor
Thursday,  29 May 2008

A difference in the way British and American ships measured the temperature  of the ocean during the 1940s may explain why the world appeared to undergo a  period of sudden cooling immediately after the Second World War.
 Scientists believe they can now explain an anomaly in the global temperature  record for the twentieth century, which has been used by climate change skeptics  to undermine the link between rising temperatures and increases in atmospheric  carbon dioxide.
The record for sea-surface temperatures shows a sudden fall after 1945, which  appeared to go against the general trend for rising global average temperatures  during the past century.
Skeptics have argued it supports the idea that rising temperatures have more  to do with increased solar activity – sunspots – than increasing levels of  man-made carbon dioxide exacerbating the greenhouse effect.
However, an international team of scientists has investigated the raw data  from the period. They found a sudden increase from 1945 onwards in the  proportion of global measurements taken by British ships relative to American  ships.
The scientists point out that the British measurements were taken by throwing  canvas buckets over the side and hauling water up to the deck for temperatures  to be measured by immersing a thermometer for several minutes, which would  result in a slightly cooler record because of evaporation from the bucket.
The preferred American method was to take the temperature of the water sucked  in by intake pipes to cool the ships’ engines. Those records would be slightly  warmer than the actual temperature of the sea because of the heat from the ship,  the scientists said.
Taking into account the difference in the way of measuring sea-surface  temperatures, and the sudden increase in the proportion of British ships taking  the measurements after the war, the result was an artificial lowering of the  global average temperature by about 0.2C, said Professor Phil Jones of the  University of East Anglia in Norwich.
“It occurred in the period of the 1940s when the number of observations of  sea-surface temperature were markedly fewer than either before or after that  period and most of the measurements were made by British and American ships.  This made the apparent anomaly more pronounced,” Professor Jones said.
The study, published in the journal Nature, found that the global average  temperatures in the late 1940s stayed roughly the same rather than falling.  David Thompson of Colorado State University, the team’s leader, said a drop was,  in effect, an artifact rather than a real observation.
“I was surprised to see the drop so clearly in the filtered data, and working  in partnership with others, realized it couldn’t be natural,” Dr Thompson  said.
Although the initial drop was significant, it did not last. By the 1960s,  many other nations began taking ship-borne measurements of ocean temperature, minimizing the discrepancy.
Professor Jones said that the study lends support to the idea that a period  of global cooling occurred later during the mid-twentieth century as a result of  sulphate aerosols being released during the 1950s with the rise of industrial  output. These sulphates tended to cut sunlight, counteracting global warming  caused by rising carbon dioxide.
“This finding supports the sulphates argument, because it was bit hard to  explain how they could cause the period of cooling from 1945, when industrial  production was still relatively low,” Professor Jones said.
A similar problem could be occurring now with the move from ship-borne  measurements to those from unmanned buoys, which tend to produce slightly lower  records. This could explain why global average temperatures in recent years have leveled off.

FYI: According to the American Meteorological Society:
bucket thermometer—A water-temperature thermometer provided with an  insulated container around the bulb.




It is lowered into the sea on a line until it has had time to reach the temperature of the surface water, then withdrawn and  read. The insulated water surrounding the bulb preserves the water reading and  is also available as a salinity sample. 






			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f033b91',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Mining and heavy industry companies, including BHP and Alcoa, have again been allowed to lift their greenhouse gas emissions without penalty under a climate change policy that the Australian government promised would prevent national pollution increasing. Under changes posted online on Thursday, BHP coal and iron ore mines in Western Australia and Queensland, Alcoa’s Portland aluminium smelter in Victoria and a Boggabri coalmine in New South Wales were each given the green light to emit more under the scheme known as the “safeguard mechanism”.  Of those made public, the allowed increases ranged between 3% to 33% above previous emissions limits. Not all the increases were published. Two of three BHP mines moved from annual to multi-year emissions limits, which means they promised to emit less over the next two years to make up for excess emissions last year. The increases were signed off despite the safeguard mechanism’s promise to limit emissions from big polluters to ensure they do not just cancel out cuts paid for by taxpayers through the Coalition’s main climate policy, the emissions reduction fund. Under the scheme, every industrial facility across the country that emits more than 100,000 tonnes a year was set a pollution limit, known as a baseline, based on either its historic emissions or an independent forecast of future emissions. Under changes being introduced this year, all facilities will be moved to limits based not on their total emissions, but on how much they expect to emit per unit of production. The Australian Conservation Foundation found increases approved over the past two years alone allowed more than 7m tonnes of potential extra emissions each year – about 1.3% of annual national carbon pollution. The increases allowed under Thursday’s changes, including those from a new Jemena gas pipeline between Tennant Creek and Mount Isa, are at least 236,658 tonnes a year. Bret Harper, director of research with energy and carbon consultants RepuTex, said it showed the safeguard mechanism was a “pretty ineffective policy”. “The bottom line is there is no accountability for any emissions increases that do occur,” he said. Emissions from electricity generation have reduced and the drought has triggered a drop from agriculture, but cuts in those sectors have been effectively cancelled out by increases, mostly from big industry. Emissions have increased from the liquified natural gas (LNG) industry, mining sector and transport, in particular. Emma Herd, chief executive of the Investor Group on Climate Change, said national climate policies did not send a strong enough market signal to unlock the private investment needed to put Australia on a path to zero emissions. She said it should take the opportunity to strengthen the safeguards mechanism and other emissions reduction policies after a review led by businessman Grant King and through the development of a long-term emissions reduction strategy. “Unless we provide tighter emissions pathways to net-zero for large emitters it’s hard to see how we can reduce pollution in line with our overarching commitments under the Paris agreement,” Herd said. The Coalition has changed how it describes the safeguard mechanism over time. In 2016, the then environment minister, Greg Hunt, said it would ensure emissions cuts contracted through the emissions reduction fund were not offset by significant increases above business-as-usual levels elsewhere in the economy. But a government climate policy document released before last year’s election said the mechanism required Australia’s largest emitters to “measure, report and manage” their emissions, not that it would limit pollution. An analysis by RepuTex found the government regulator had approved a 32% increase in how much large industrial facilities were allowed to emit under the scheme. While not every company emitted up to their limit, the most recent data, for 2017-18, showed emissions from large industry were up 12% since 2015. Some companies that have exceeded their limit have been expected to buy carbon credits to offset the additional emissions, or pay a penalty. Over the first two years of the scheme companies paid for cuts equivalent to 707,625 tonnes of emissions. The government denies the scheme is a form of carbon pricing. The minister for emissions reduction, Angus Taylor, did not respond to a request for comment before publication."
"

 **Presented by Daniel T. Griswold at the James and Margaret Tseng Loe Chinese Studies Center Conference, St. Vincent College, PA, on November 6, 2002.**



Let me confess up front that I am not a China expert. But one cannot talk about international trade and globalization for even a few minutes without addressing China. We are all students of China now. Today China has become one of the world’s major trading nations, and it is destined to grow more influential in the years ahead. 



My remarks today will address four aspects of the topic of China and international trade, what we might call “Chairman Dan’s Four Theses”: The Re‐​emergence of China as a Trading Nation; U.S.-China Commercial Relations Today; Answering the Critics of Normal Trade Relations; and Tilling the Soil for Human Rights.



If we were to travel back six or seven centuries, we would enter a world where China was the most advanced economy on earth and the most and dynamic force in Asian trade. China organized a professional Navy in 1232, with treadmill‐​operated paddle‐​wheelers and catapults that launched heavy stones. 



Marco Polo testified to the vigor of China’s international trade during his visits in the late 13th century. The commercial city of Hangzhou had 1 million residents by then, including a merchant class and uprooted refugees. The city embraced relative freedom, change, and travel, and was open to Arab and Hindu learning. The citizens of Hangzhou had a saying: “Vegetables from the east, water from the west, wood from the south, and rice from the north.” 



In those days, Chinese plied the Indian Ocean with fleets of ocean going merchant junks, 100 feet long and 25 feet wide, carrying 120 tons of cargo and 60 crew. Those ships visited Indonesia, Ceylon, and the west coast of India. By the 13th century, the Chinese had developed dry docks and gunpowder bombs–300 years before those were seen in the West. 



Beginning under Emperor Zhu Di, the Chinese launched seven official naval expeditions between 1405 and 1431 to Indonesia, India, Arabia, and East Africa. The expeditions were lead by the eunuch officer Zheng He. These “Treasure ships” were the largest in the world, 400 feet long by 160 feet wide (vs. 85 feet long for the Santa Maria). The ships were multi‐​decked, with nine masts and sails of red silk traversed with laths of bamboo for more durability and precise steering. Each ship carried hundreds of sailors and had 15 or more watertight compartments, and 60 cabins. At 7,800 tons of displacement, they were the largest ships in the world until those of the British Navy after 1800. In all, China built 250 such ships as part of a major shipbuilding program that would have been unimaginable in Europe at the time. 



The Treasure Ships were sent on huge trade missions. The first, in 1405, consisted of a fleet of 317 ships with 28,000 Chinese. What a sight that must have been! On a mission to Hormuz, in the Persian Gulf, Chinese traded porcelains and silks in exchange for sapphires, rubies, oriental topaz, pearls, coral beads, amber, woolens, and carpets, along with lions, leopards, and Arabian horses. But these were not market‐​opening missions, but more diplomatic in nature, a showing of the flag. No attempt was made to establish bases for trade or military objectives. The missions were very costly for the Chinese government and not profitable in a commercial sense. 



The bureaucratic mandarins, who detested commerce, soon prevailed over the rival eunuchs. At its peak in early 1400s, the great Ming navy consisted of 3,500 ships, but the number fell in half by 1440, and rapidly diminished after that. With the death of Zhu Zhanji in 1435, the new emperor recalled the fleets. In 1477, one of leading eunuchs called for writings of Zheng He to stimulate interest in naval expeditions, but the vice president of the ministry of war ordered them destroyed, calling them “deceitful exaggerations of bizarre things far removed from the testimony of people’s eyes and ears.” By 1500 it was a capital crime to build a ship with more than two masts. In 1525 coastal authorities were enjoined to destroy all ocean‐​going vessels, and in 1551 it was declared a crime to set sail in a multi‐​masted ship. 



In 1400, China was in every way superior to West: in technology, living standards, and global influence. But the country became enveloped in a smug self‐​sufficiency, cultural and economic inwardness, a closed and centralized political system, and an anti‐​commercial culture. In the 15th century, China turned its back on the world economy. It even abandoned naval defenses. Its highly educated elite was uninterested in Western technology and military potential. A British mission in 1793 brought 600 cases of presents, including chronometers, telescopes, a planetarium, chemical and metal products. Chinese officials rebuffed the foreigners, asserting that “there is nothing we lack‐​we have never set much store on strange or ingenious objects, nor do we want any more of your country’s manufactures.” 



So for more than 500 years, from the 15th to the 20th century, China’s economy slipped further behind the rest of the world. As late as 1820, the gross domestic product of China was still 30 percent higher than the total GDP of Western Europe and its settlements, but it was only one‐​twelfth the size by 1950. The Chinese economy was not open in the 19th century despite trade treaties and Western encroachment. Its trade was conducted in self‐​contained trade zones with little impact on the rest of China. The share of exports in China’s GDP was only 1.2 percent in 1913 at the height of pre‐​war globalization in the West. The Taiping Rebellion in the mid‐​19th century and World War II, Civil War, and communist convulsions in 20th devastated China’s economy. 



The economic reforms that began in late 1970s reversed 500 years of history. China’s trade with the rest of the world has grown from only $20 billion at the beginning of the reforms to more than $500 billion in 2001. China is now the world’s sixth largest exporter of goods and also the world’s sixth largest importer. In the past decade, China has reduced its average tariff from 43 percent to 15 percent, and those barriers will fall further as it implements the agreement it signed to join to World Trade Organization. So much for China being a closed economy! 



I believe the re‐​emergence of China as a trading nation is one of the most important and far‐​reaching developments in the last, oh, half millennium or so. After 500 years on the sidelines, China has rejoined the global economy.



Since China began to unilaterally open its market, the people of China and the United States have enjoyed a growing and mutually beneficial trade relationship. From practically nothing in 1980, two‐​way U.S.-China trade grew to more than $120 billion in 2001. China today is America’s fourth largest trading partner. In 2001, Americans imported $102 billion worth of goods from China while we exported $19 billion‐​leaving a bilateral trade deficit with China of $83 billion. 



Since 1980, the United States has allowed Chinese products to enter the U.S. market at the same tariff rates applied to our other trading partners. But the extension of so‐​called normal trade relations to China was always conditioned on the president granting a waiver to the Jackson‐​Vanik amendment (a relic of the Cold War that conditions trade with communist countries on their emigration policies). Each year congressional opponents of trade with China would try in vain to override the waiver, and in 2000 Congress made normal trade relations permanent to clear the way for China’s entry into the World Trade Organization. 



So what in the world do we buy from China? It’s a running joke with my kids that we cannot go to the store without buying something–clothing, toys, household goods–made in China. Three‐​quarters of what Americans import from China are toys and other miscellaneous manufactured goods: footwear‐​1 billion pairs of shoes a year‐​furniture, lighting fixtures, office machines, household electronics, electrical appliances, and clothing. Wal‐​Mart alone will import an estimated $12 billion worth of goods from China in 2002. Those goods mean lower prices, more choice, and more real income for American families. 



On a much smaller scale, China buys American‐​made aircraft, telecommunications equipment, scientific instruments, oil seeds and fruits, electrical machinery and appliances, data processing machines, and fertilizers. 



Why do we run such a large bilateral trade deficit with China? We are the world’s number one consumer society and China has become the world’s workshop for consumer goods, so it should be no surprise that we have become China’s best customer. On the other hand, we are the world’s leading high‐​end manufacturer, while China remains a relatively poor country. In sum, we are more willing and able to buy what the people of China make than they are willing or able to buy what we make. 



Despite warnings, the United States is not dangerously “dependent” on trade with China. Our imports to and exports from China remain a small fraction of our total trade. If anything, China is more dependent on trade with the United States than vice versa. Both our imports and exports with China are less than 10 percent of our total trade, but 38 percent of China’s exports go to the United States. If our trade relations were disrupted, by an outbreak of protectionism or a hot or cold war, both countries would suffer economically but China would suffer more. 



And despite the warning that U.S. factories will soon lock up and move to China, American investment in the mainland remains modest. At the end of 2001, American companies owned $7 billion worth of direct manufacturing investment in China. That is less than 2 percent of the total stock of U.S. manufacturing FDI abroad, and far less than the $35 billion in manufacturing investment American companies own in the tiny Netherlands, population 15 million. Annual outflows of manufacturing investment to China remain a tiny fraction of what American companies invest domestically in the U.S economy, and what the rest of the world invests in China. 



Criticism of U.S. trade with China takes two basic forms: that our trade with China, and by this the critics invariably mean what we import from China, threatens our national security, and that it threatens our economy. 



Let’s examine the national security argument first. The most legitimate concern about trade and national security is what we export to China. The U.S. government wields extensive powers to block exports to China of sensitive military and so‐​called dual‐​use technology‐​and the government should use that power when necessary. We should not be selling cutting‐​edge military technology to China that could then be sold to our enemies or turned against us in any way. But that is not what really bothers the critics of trade with China. What they object to are imports from China. They believe in a simple, what I would call a simplistic, formula that says: When we buy goods from China, China becomes richer, and the richer China becomes, the more it can fund its military to threaten American security. 



That was the conclusion this summer of the U.S.-China Security Review Commission. The commission was established by Congress in 2000 when it approved permanent normal trade relations. In its first annual report, the commission warns that, through our trade and investment ties with China, “we are strengthening a country that could challenge us economically, politically, and militarily.” 



“If China becomes rich but not free,” the commission warns, “the United States may face a wealthy, powerful nation that could be hostile toward our democratic values, to us, and in direct competition with us for influence in Asia and beyond.” 



The commission’s national security critique is fundamentally flawed, for at least two major reasons. First, while trade with the United States has been important for China’s development, it has not been the most important factor. Far more important has been China’s own internal liberalization, starting with its farm sector in the late 1970s, and then expanding to the privatization of its state‐​owned sector, repeal of price controls, and the unilateral opening of its economy to foreign competition. If the U.S. market were far less open to Chinese goods than it actually is, China would still have grown rapidly in the last 20 years, although not quite as rapidly as it actually has. 



Second, even if it were possible, through changes in U.S. trade policy, to put the brakes on China’s economic growth, would we even want to? From a humanitarian point of view, a dramatic slowdown in China’s growth would cause hardship for hundreds of millions of families and condemn millions of children to lives of perpetual poverty without hope for further education and upward mobility. And from a foreign policy point of view, a still‐​poor, stagnant, and frustrated China may be more unstable and hostile to American interests than a China that is advancing economically. In fact, a policy of disengagement from China could be self‐​fulfilling, creating the very enemy its proponents claim to be protecting us from. In sum, it would be cynical and foolish to stake our national security on a policy designed to keep 1 billion people isolated and poor. 



The other major criticism of trade with China is that is threatens America’s economy. Here the critics believe in an equally simplistic formula that says: Every widget we import from China means one less widget we make ourselves, which means a weaker U.S. economy and a potentially dangerous dependence on foreign widgets. And here too the argument against trade with China is fundamentally flawed. 



First, the types of goods we import from China are not important for the U.S. military. Recall the list of top imports from China: toys, shoes, clothing, office machines, household appliances and household electronics. American soldiers may be buying those goods at the local Wal‐​Mart or PX, but they are not being procured by the Pentagon. The China Security Commission warns that the U.S. steel industry may be jeopardized by Chinese imports, but the Commerce Department has already investigated the national security impact of steel imports and found no connection. 



Second, imports from China do not weaken the U.S. economy, cause unemployment, or threaten our industrial base. Imports strengthen our economy by raising real wages for families, providing lower‐​cost inputs for business, and spurring innovation and higher productivity through competition. Like technology, trade does cause certain industries to decline, thus eliminating some jobs, but it also creates new opportunities for wealth and job creation. In an economy with a reasonably flexible labor market, jobs eliminated by technology and trade will be fully offset by the creation of new jobs. 



A blatant example of overblown rhetoric about the trade deficit and jobs occurred on the eve of the vote on permanent normal trade relations in May 2000, during a segment of the NewsHour with Jim Lehrer on PBS. In summing up why the House should reject permanent normal trade relations with China in a vote the next day, AFL-CIO executive Richard Trumka asserted:



No one is saying isolate China. That’s the smoke screen they blow out because they don’t have the facts. Look, we have a $70 billion trade deficit with China. The U.S. International Trade Commission came out with a study yesterday [Monday, May 22] saying, if you give them permanent NTR status, two things will happen: We’ll lose one million jobs, and the trade deficit will increase.



Trumka’s sweeping claim offers a textbook example of how opponents of trade liberalization abuse trade deficit figures to serve their agenda. In fact, the U.S. International Trade Commission had issued no such study that week on trade with China. The commission’s most recent study on the impact of China PNTR had been released in August 1999, almost a year earlier, and it contained no estimate of job gains or losses. 



The actual source of the figure of one million jobs lost was a paper released the week before by the Economic Policy Institute, a union‐​aligned, non‐​profit organization. The EPI had used numbers from the 1999 USITC study to extrapolate an estimate of future bilateral trade deficits with China. It then crunched the hypothetical trade deficit numbers to estimate a total loss of almost 900,000 jobs during the next decade if Congress were to approve PNTR with China. But the EPI estimate of job losses was based on three whoppingly false assumptions. 



One serious error of the EPI study was to misapply the USITC’s estimates for the growth in China trade. The USITC study only offered a one‐​year, static estimate of the impact of Chinese tariff liberalization on the U.S. trade deficit. The ITC study didn’t even attempt to estimate the number of American jobs that would be created or eliminated by the further opening of the Chinese market. 



The EPI’s second crucial error was then to assume that rising imports from China automatically mean lost jobs in the U.S. economy. But rising imports need not and typically do not translate into a net loss of jobs. In fact, the growth of real goods imports and manufacturing output tend to be positively correlated. That is, as manufacturing output rises in the United States so too do imports of goods, adjusted for price changes. As with so many other economic indicators, the same economic expansion that spurs manufacturing output also attracts more imports and enlarges the trade deficit. 



Trade critics such as EPI wrongly assume that every import from China displaces domestic production, eliminating jobs in the economy. In reality, much of what we import from China, such as toys, shoes, and clothing, substitutes for imports from other low‐​wage producers. Another sizeable portion of our imports consists of intermediate inputs, which are then assembled into U.S.-made products by American manufacturers. That helps to explain why there is no correlation between rising manufacturing imports from China and falling manufacturing output. 



A third critical error of the EPI study was to consider the bilateral trade balance with China in isolation. While a change in trade policy can affect a particular bilateral deficit, the increased bilateral deficit tends to be offset by changes in other bilateral balances. The ITC study confirms this. The USITC estimated that China’s lower tariffs would cause America’s overall trade deficit to shrink slightly. Although America’s bilateral deficit with China would increase within the USITC’s limited model, our trade balance with other countries would “improve” enough to more than offset the increased deficit with China. The USITC estimated that America’s total exports would growth by $1.9 billion while imports would grow by $1.1 billion, decreasing the overall U.S. trade deficit by $0.8 billion. If you believe EPI’s own faulty methodology, the smaller overall U.S. trade deficit caused by China’s lower tariffs should lead to an increase in U.S. jobs, not a decrease. 



Trade with China is about more than jobs and incomes. Around the world, trade and the development it has spurred have created a more hospitable climate for civil and political freedoms. The economic openness of globalization allows citizens greater access to technology and ideas through fax machines, satellite dishes, mobile telephones, Internet access, and face‐​to‐​face meetings with people from other countries. Rising incomes and economic freedom help to nurture a more educated and politically aware middle class. People who are economically free over time come to want and expect to exercise political and civil liberty as well. Catholic social thinker Michael Novak identified this as the “Wedge Theory”:



Capitalist practices, runs the theory, bring contact with the ideas and practices of the free societies, generate the economic growth that gives political confidence to a rising middle class, and raise up successful business leaders who come to represent a political alternative to military or party leaders. In short, capitalist firms wedge a democratic camel’s nose under the authoritarian tent.



The interplay of economic openness and political and civil freedom is admittedly complex, and the question of causation remains unsettled, but the two phenomena are clearly linked in the real world. In the past 25 years, as an expanding share of the world has turned away from centralized economic controls and toward a more open global market, political and civil freedoms have also spread. Since 1975, the share of the world’s governments classified by Freedom House as democracies has risen sharply, especially since the late 1980s when globalization began to gather steam. Many of those new democracies are low‐ and middle‐​income countries that have simultaneously liberalized and opened their economies. 



When we compare countries according to their economic openness and their degree of political and civil freedom, the connection becomes even more evident. People who live in countries relatively open to international trade and investment are far more likely to enjoy full political and civil liberties than those who live in countries that are relatively closed. Among the top two quintiles of nations ranked according to their economic openness, 90 percent are rated “Free” by Freedom House and not a single one is rated “Not Free.” In the bottom quintile of openness (i.e. those with the most closed economies), fewer than 20 percent are rated “Free” and more than half are rated “Not Free.” In other words, countries that maintain a relatively open economy are more than four times more likely to be free of political and civil oppression than countries that remain closed. 



Recent decades have witnessed dramatic examples of how economic freedom and openness till the soil for civil and political reform. Twenty years ago, both South Korea and Taiwan were military dictatorships without free elections or full civil liberties. Today, thanks in part to economic growth and globalization, both are thriving democracies where citizens enjoy the full range of civil liberties and where opposition parties have won elections against long‐​time ruling parties. In Mexico, more than a decade of economic and trade reforms helped lay the foundation for the historic July 2, 2000, election of the opposition candidate Vicente Fox, ending 71 years of one‐​party rule by the PRI. Internal economic reforms and the North American Free Trade Agreement helped to undermine the dominance of the PRI over Mexican political life. Alejandro Junco, publisher of the opposition newspaper Reforma, noted after the PRI’s historic defeat, “As the years have passed, and with international mechanisms like NAFTA, the government doesn’t control the newsprint, they don’t have the monopoly on telecommunications, there’s a consciousness among citizens that the president can’t control everybody.” 



While genuine political reform has been absent so far in China, and dissent is still brutally suppressed, economic reform and globalization give reason to hope for political reforms. After two decades of reform and rapid growth, an expanding middle class is experiencing for the first time the independence of home ownership, travel abroad, and cooperation with others in economic enterprise free of government control. The number of telephone lines, mobile phones, and Internet users has risen exponentially in the past decade. Tens of thousands of Chinese students are studying abroad each year. 



China’s economic reforms have opened the door for religious witnessing. More than 100 Western missionary organizations are active in China. Those organizations have distributed millions of Chinese language Bibles in China. Thousands of Christian workers who are tent‐​making as English teachers and in other occupations are able to minister to the growing body of believers in China. All this would have been unthinkable 25 years ago when China was still isolated from the global economy. 



All this must be good news for individual freedom in China, and a growing problem for the government. A recent study by the Chinese Communist Party’s influential Central Organization Department noted with concern that “as the economic standing of the affluent stratum has increased, so too has its desire for greater political standing.” The study concluded that such a development would have a “profound impact on social and political life” in China. 



Globalization and economic development do not guarantee political reform in China or anywhere else, but the track record of economic engagement is far more promising than the failed record of sanctions and economic isolation. Four decades of an almost total U.S. embargo against Cuba have yet to soften Fidel Castro’s totalitarian rule. Sanctions against Burma (a.k.a. Myanmar) have only worsened the condition of the very people we are trying to help without bringing any progress toward democracy and freedom. The folly of imposing trade sanctions in the name of promoting human rights abroad is that it deprives people in the target countries of the technological tools and economic opportunities that can help to free them from tyranny. 



For the past two decades, globalization, human rights and democracy have been marching forward together, haltingly, not always and everywhere in step, but in a way that unmistakably shows they are interconnected. By encouraging more trade and market liberalization in China, we not only help to raise growth rates and incomes, promote higher standards, and feed, clothe and house the poor; we also spread political and civil freedoms. 



President Bush, in The National Security Strategy of the United States of America document released in 2002, wrote that, “Chinese leaders are discovering that economic freedom is the only source of national wealth. In time, they will find that social and political freedom is the only source of national greatness.” Opponents of trade with China see the rising incomes and falling poverty of hundreds of millions of people as a threat to our security and well‐​being. Instead, we should see China’s rising prosperity as an immediate blessing for mankind. And we should understand that trade offers the best hope that China will one day join the community of nations that are free and democratic just as it now seeks to join those that are open and prosperous.
"
"There are at least 268,000 tonnes of plastic floating around in the oceans, according to new research by a global team of scientists.  The world generates 288m tonnes of plastic worldwide each year, just a little more than the annual vegetable crop, yet using current methods only 0.1% of it is found at sea. The new research illustrates as much as anything, how little we know about the fate of plastic waste in the ocean once we have thrown it “away”. Most obviously, this discarded plastic exists as the unsightly debris we see washed ashore on our beaches. These large chunks of plastic are bad news for sea creatures which aren’t used to them. Turtles, for instance, consume plastic bags, mistaking them for jellyfish. In Hawaii’s outer islands the Laysan albatross feeds material skimmed from the sea surface to its chicks. Although adults can regurgitate ingested plastic, their chicks cannot. Young albatrosses are often found dead with stomachs full of bottle tops, lighters and other plastic debris, having starved to death.   But these big, visible impacts may just be the tip of the iceberg. Smaller plastic chunks less than 2.5mm across – broken down bits of larger debris – are ubiquitous in zooplankton samples from the eastern Pacific. In some regions of the central Pacific there is now six times as much plankton-sized plastic are there is plankton. Plankton-eating birds, fish and whales have a tough time telling the two apart, often mistaking this plastic – especially tan coloured particles – for krill. However, even this doesn’t quite tell the whole story. For technical reasons Eriksen and his team weren’t able to consider the very smallest particles – but these may be the most harmful of all. We’re talking here about tiny lumps of 0.5mm across or considerably less, usually invisible to the naked eye, which often originate in cosmetics or drugs containing nanoparticles or microbeads. Such nanoparticles matter as they are similar size to the smallest forms of plankton (pico and nano plankton) which are the most abundant plankton group and biggest contributors in terms of biomass and contribution to primary production. There’s a lot going on when you zoom right in. We don’t yet know precisely how plastic nanoparticles interact with marine fauna but we do know that they can be absorbed at the level of individual cells. And what’s worse is they’re very efficient carriers of organic molecules such as estradiol, the drug used for birth control and IVF that finds it way through our sewage system into the sea.  Indeed, this efficiency is one of the reasons nanoparticles are being explored for drug delivery – they’re a great way to get the right medicine absorbed into the right cells.  Therefore it isn’t just the plastic itself that should concern us. We need to look at what it’s carrying, as substances clinging to nanoparticles of plastic could badly damage marine ecosystems.  Nasty endocrine disrupting chemicals can be concentrated a million times more than background levels on the surfaces of plastic particles. These can then be ingested by organisms and the chemicals absorbed leading to disruption of the reproductive process – some species such as bivalve mussels have even seen males turned into females. Floating chunks of plastic can also be colonised by organisms including potential bacterial pathogens such as cholera, and marine insect sea skaters which need a hard surface to lay their eggs on – plastic in the sea increases their numbers and range. The fact that floating plastic debris is novel and persists for longer than most natural flotsam could make them ideal vehicles for the introduction of invasive species with potentially devastating consequences. Plastic pollution of the marine environment is the Cinderella of global issues, garnering less attention than its ugly sisters climate change, acidification, fisheries, invasive species or food waste but it has links to them all and merits greater attention by the scientific community."
"
With apologies to Robert Duvall in Apocalypse now-
Kilgore: Smell that? You smell that?
Lance: What?
Kilgore: Sewage, son. Nothing in the world smells like that.
[kneels]
Kilgore: I love the smell of sewage in the morning.  The smell, you know that rotten eggs smell… Smells like… victory. Someday this war’s gonna end…

USHCN at Tullahoma, TN Wastewater Treatment Plant – Visible light

USHCN at Tullahoma, TN Wastewater Treatment Plant – Infra red
You know it seems like every morning this week that I prepare to start my day’s worth of surveys, I find that I’m going to visit another USHCN climate station of record at a sewage treatment plant. And so is the case today, my last day of surveys. I’m gonna take a loooong shower when I get home.
I know you all want to hear more about NCDC and USHCN2, and I’ll get into those details next week, but for now, another sewage treatment plant beckons.
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9fc997a3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Economic interests are set to play an increasingly important role in shaping development in the Arctic. Yet prominent members of the mining industry, familiar with the economic and reputational perils of impatient investment, remain cautious. They can – and should – play a pivotal role in guiding responsible industrial activity in the region.  Corporate development of the Arctic appears to be a foregone conclusion and this is reflected by the development of major transnational agreements. For example, with Arctic shipping projected to increase massively in the coming years, the UN Maritime Safety Committee (MSC) will adopt the Polar Code, international guidelines for the safety of ships operating in polar waters.  On the back of such recognition, countries are making longer term economic commitments. China, for instance, has just agreed to purchase oil and gas from the Russian Arctic over the next decades, while at the same time having secured stakes in Russian oil platforms in the region.  The Arctic is rich in oil, gas, and metals such as nickel, copper, gold, uranium, or tungsten. It even has large diamond reserves. Rapidly shrinking sea ice exposes new shipping routes through the Arctic Ocean that will save time and money for companies moving goods from Asia to Europe, while also providing new opportunities for tourism and fishing.  Mining companies have a big opportunity here, and some of the planet’s northernmost mines are already making an impact. Alaska’s Red Dog mine is one of the world’s biggest producers of zinc and lead, whereas Greenland’s Ilimaussaq complex is estimated to meet a quarter of global demand for rare earth elements, critical components in a wide range of electronic devices, over the next 50 years. With Arctic mining in its infancy, many non-Arctic states, including China and the UK, are lining up to invest in future projects.  In spite of these riches, mining companies, particularly the mega-multinationals such as BHP Billiton or Rio Tinto, remain cautious about the Arctic. This is partly because mining companies have had their fingers burnt by high-profile environmental accidents, such as the 2006 lead poisoning of the Australian town of Esperance or the discharge of over two billion tons of untreated mine waste, over nearly three decades, into the Ok Tedi river in Papua New Guinea. Mining accidents have also killed or disabled workers. For example, around 170 miners are killed each year in the South American gold industry. Expensive mining projects built in a hurry have hit the industry’s reputation. Yet these companies can only avoid the Arctic for so long. Climate change coupled with the global decline of ore quality has already made life difficult for extractive industries, which find themselves having to operate in increasingly remote environments where they encounter heightened competition with local inhabitants for water and energy. In addition, most major mining companies are part of the International Council on Mining & Metals (ICMM), founded in 2001 with the specific aim of addressing sustainable development challenges. As members, major mining companies commit to a set of principles designed to maintain sustainable development standards. This arrangement is unique for large-scale industries; for example, the equivalent global oil and gas industry association for environmental and social issues (IPIECA) allows any company to join regardless of environmental performance.  As the Arctic opens up for business, mining has taken an important step towards a leadership position in private-sector environmental stewardship. At the recent Arctic Circle Assembly in Iceland, ICMM president Anthony Hodge urged other industries to follow mining’s lead in endorsing full sustainability perspectives in the Arctic, taking into account the well-being of the region’s people and the broader natural environment.  Admiral Robert Papp, the newly appointed US Special Representative for the Arctic, believes social media has completely changed how the US government interacts with remote communities in Alaska, increasing pressure for accountability by establishing a transparent two-way dialogue. It’s a lot harder to put a zinc mine next to a village in the middle of nowhere if the residents are able to tweet their concerns. Modern communication has also pushed industry transparency and disclosure policies to the top of the agenda. This bottom-up approach to whistleblowing has contributed to rapidly improved standards of accountability among extractive companies, suggested within the broader Global Reporting Initiative (GRI) and the more recent industry-specific Initiative for Responsible Mining Assurance (IRMA). Operating in the Arctic is challenging, whatever industry you’re in. But mining firms have already made many of the mistakes, and learnt many of the lessons, that lie ahead of the oil, gas and shipping industries. Before these mistakes are repeated by others, mining representatives must step up to facilitate the sensible economic development of Earth’s northernmost latitudes."
"
One of the strangest things I’ve learned in the past year about the US Historical Climatological Network is the propensity for placement of weather stations at sewage treatment plants.
The reason of course has to do with putting a thermometer at a facility that is staffed 7 days a week. That thermomter must be manually read once a day and the readings transcribed into a logbook. Waste Water Treatment Plants (WWTP’s) fit that requirement (as they have an operator on duty, often 24/7) but they themselves are their own mini islands of waste heat and humidity, especially in winter and overnight. Yet, a significant portion of the US climate data comes from these locations.
Some have grassy areas where a climate monitoring station could be placed, such as the one in Morrison, IL, and you’d think they would place it there, away from the sewage tanks. Unfortunately, no.

Click for a larger image, additional photos available here at surfacestations.org
My sincere thanks to volunteer surveyor Scott Finegan for these photos.
The Stevenson Screen housing the thermometer is about 5 feet away from each tank, while the concrete building in the background is some 50 feet away. You’d think that they could have placed the station a little further away. Again, as we’ve seen time and time again, the placement is not often about the best location, it is about convenience for the observer.
The GISS graph of temperature over the station history shows a fairly strong warming trend from about 1980 to the present. The question is, how much of that is from increased throughput of the sewage treatment plant responding to population growth, and how much of it is climate change?

Click for source graph from NASA GISS
According to NCDC’s Multi Metadata System database, this station has been at this location since at least 1948, even though a lat/lon accuracy update makes it appear to have been relocated in 1997, it has in fact been at this location all that time.
A nearby station, 25km away, Clinton, IA, is also in the GISS database and shows less of a trend during the same period:

Separating a climate change signal from the waste heat (and increasing effluent volume of the WWTP due to population growth) may not be a simple matter to disentangle. Since each WWTP has different conditions, coming up with a blanket correction would not be easy. Therefore, since the USA is highly oversampled spatially with weather stations that report daily data which can used for climate, it would be prudent in my opinion, to remove stations like this from the climatic database since the data produced by USHCN stations at WWTP’s may not be truly representative of climate.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f3e77b5',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe German transformation to green energies will fail due to wind power
By Prof. Fritz Vahrenholt, Die kalte Sonne

Not going to work, says German energy expert Prof. Fritz Vahrenholt
(German text translated/edited/subheadings by P. Gosselin)
The goals of the German transition to green energies are simple in terms of energy policy:
1. phase-out nuclear energy by 2022,
2. phase-out coal by 2035,
3. phase-out oil and gas in parallel and completely by 2050.
The energy needed for electricity, heat, mobility and industrial processes in climate-neutral Germany will then have to be supplied by wind and solar energy and a few percent by hydropower and biomass. This is at least according to the plans of the German government, which are supported by all major social players.
Is this realistic?
Today, wind and photovoltaics supply slightly less than 30% of the 600 terawatt hours of electricity (1 terawatt hour Twh is 1 billion kilowatt hours Kwh). Today 126 Twh is supplied by wind energy and 46 Twh by photovoltaics. For 600 TWh, the same mix would need 439 Twh of wind and 161 Twh of solar. For the sake of simplicity, let us assume that this amount of electricity should be generated by the largest wind turbines, namely 5 megawatt turbines positioned 1000 m apart. With an annual efficiency of 25%, a turbine produces an average of 5 MW x 0.25 x 8760 (hours) = 10,950 Mwh = 0.01095 Twh. For 439 Twh we would need 40,000 such turbines. To accomplish this, an area of 200 km x 200 km (40,000 sq km) would be required.
Too much unneeded surplus power
But we still would not reach the goal. Wind energy is produced when the wind blows, and not necessarily when the consumer needs it. With a power supply in Germany based solely on volatile sources, 36% of the electricity generated annually can be consumed directly (source: Dr. Ahlborn). The rest is surplus electricity that has to be stored. For economic reasons, storage in hydrogen alone is the best option here. For this purpose, a gigantic number of electrolysis plants would have to be installed.
Huge area required for electricity from wind turbines
However, it is completely uneconomical to dimension the capacity according to the extreme peaks of strong wind events. Therefore, about 12% of the wind energy has to be regulated. This now leaves 52% of the electricity generated that can be stored in hydrogen. Electrolysis of hydrogen, storage/methanization and conversion back into electricity leaves only 15.6% of the 52%. The entire conversion chain generates a loss of 2/3 of the electricity used. 36% plus 15.6% result in about 50% of the generated wind power being usable. Thus, we need twice as many turbines. The area for the 80,000 wind turbines becomes 80,000 km², which corresponds to an area of 283 km x 283 km (80,089 sq. km).
Now add the demand from transport and heating…
But we remain very far from the finish line. Up to now we have only covered the electricity demand with 2 x 439 Twh, but without supplying the demand from transport and heating. Also with demand from transport (today 600 Twh) and heat (today 1200 Twh) we have storage and conversion losses when the necessary electricity is generated by wind and solar. Here we only consider wind for this, because with photovoltaics, the annual efficiency of 10% full load hours is significantly lower and the land consumption is many times higher. This makes our calculation extremely conservative.
Devastating lack of efficiency


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Assuming that the transport sector can actually be powered by battery vehicles, which is justifiably doubtful, converting cargo transport, maritime transport or air cargo transport over to electricity is already adventurous. Instead, synthetic fuels would have to be used.
And here as well the electricity calculation is devastating. As Dr. Detlef Ahlborn was able to show, the Frankfurt airport alone consumes 14.7 million liters of kerosene per day (before Corona), which comes out 4.3 million tons annually. 4.3 million tons of kerosene correspond to an energy value of 47 Twh. If one wanted to synthesize kerosene from electricity with the help of hydrogen (assumed efficiency 50%), 100 Twh of electricity would be needed. Just for the Frankfurt airport alone, this comes out to being as much as the German wind energy industry currently produces (126 Twh).
Minimum 900 Twh for heating and transport
Next we conservatively assume that all passenger transport also can be powered with electricity and that only a quarter of the amount of the 600 Twh of energy consumed today (since electric cars are more efficient by this factor) is needed. However, we also want to drive a car when there is no wind, and as explained above, most of this electricity has to be put through the chain of hydrogen, storage, and re-electrification, thus doubling the input electricity to 300 Twh.
We further assume that the current demand of 1200 Twh for heating can be reduced to a quarter through electrification (heat pump) so that here too, due to the necessary intermediate storage of wind power via hydrogen, the necessary doubling of wind energy leads to 600 Twh. If synthetic gas from wind power, hydrogen, is used directly, the yield is even worse because the efficiency of the heat pump is not applicable. Transport and heat therefore in the best case lead to a wind power demand of 900 Twh. This results in an area requirement of another 80,000 km², thus we are up to 160,000 km² of area needed by wind turbines (approx. half the area of Germany).
Another 600 Twh for heavy industry
But we still haven’t reached the ultimate target because the most difficult part is still unsolved. Emissions from the steel, chemical and cement industries (10% of CO2 emissions) require 600 Twh, according to industry estimates (www.in4climate.nrw). This is easy to understand if one remembers the above example of Frankfurt Airport. And plastics, pharmaceuticals, insulating materials, paints, varnishes, adhesives, detergents and cleaning agents may then only be produced using CO2 plus hydrogen. The replacement of industrial CO2 emissions thus leads to a further 55,000 km² area for wind turbines, so now we are up to 215,000 km² – much more than half of Germany’s total area.
2/3 of Germany would end up plastered with wind turbines
Two thirds of Germany would now be outfitted with 200-meter tall rotating wind turbines at a distance of 1000m, no matter if there is a city, a river or a highway, a forest, a lake or a nature reserve.
Can we and policymakers imagine such a Germany?
Environmental catastrophe, obstinate policymakers
If you wish to know which effects wind power plants in large numbers have on the extinction of birds of prey, bats, the decline of insects already today, then read it in our book Unerwuenschte Wahrheiten (Unwanted Truths). There you’ll find the hidden fact that wind farms lead to a considerable warming in their area of influence of about 0.5 ° Celsius because the rotating blades compensate for the strong temperature gradient at night and shovel warmer air back to the ground. Numerous studies have shown that the soil in the windparks has dried up considerably.
10-fold higher electricity prices
But politicians refuse to discuss the environmental incompatibility of a massive expansion of wind power plants. Recently the German Bundestag decided that the so-called legal, suspensive effect of objection and action for rescission is no longer applicable to lawsuits against turbines taller than 50 meters. In this way, Germany can be now turned into a single giant wind park without all the annoying objection.
It is almost superfluous to point out that we are talking about astronomical costs. Electrolysis and power-to-gas plants cannot be operated free of charge.
From today’s point of view, one has to expect a tenfold higher electricity price. Any person can imagine the consequences for jobs and prosperity.
Prof. Fritz Vahrenholt


		jQuery(document).ready(function(){
			jQuery('#dd_1a0c8b531431dcb96168154efc24df57').on('change', function() {
			  jQuery('#amount_1a0c8b531431dcb96168154efc24df57').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

Last summer, progressive legal scholar Simon Lazarus offered a commentary on the shifting landscape of the U.S. legal establishment.



“For decades, and as recently as Barack Obama’s first year in the White House, libertarians were marginalized within the conservative pantheon,” he wrote in the _New Republic_. “Now they rival, and in important areas threaten to displace social conservatives and big‐​government conservatives.”



This “upheaval” — which Lazarus concluded was “potentially seismic” — has had a discernible impact on more than just the conservative movement. Since taking office in 2009, President Obama has suffered a string of losses before the Supreme Court, racking up 20 unanimous high court defeats over the last five‐​and‐​a‐​half years. “The fact that his track record is as bad as it is in the Supreme Court,” Sen. Mike Lee (R-UT) recently said, “is yet another indication of the fact that we’ve got a president who is playing fast and loose with the Constitution.”



The Cato Institute has been at the center of this reversal. In another successful term at the marble palace at One First Street NE, the Center for Constitutional Studies went 10–1 in cases where it filed amicus briefs. This is on the heels of the Institute’s 15–3 record last term. Notably, the solicitor general’s office most recently went 11–9 on the year. “Perhaps the government would be better served following our lead on constitutional interpretation, advocating positions that reinforce our founding document’s role in securing and protecting individual liberty,” Ilya Shapiro, senior fellow at the Institute, wrote in response. Cato was the only organization in the country to file on the winning side of this term’s three highest‐​profile 5–4 cases.



The year’s most highly anticipated case was _Burwell v. Hobby Lobby_ , in which a familyowned business filed suit challenging the Affordable Care Act’s contraceptive mandate, citing religious objections. The Court ultimately sided with Hobby Lobby. After the ruling, Cato’s vice president for legal affairs Roger Pilon identified the core issue in the case. “Religious liberty is treated today as an ‘exception’ to the general power of government to rule — captured, indeed, in the very title of the statute on which the Hobby Lobby decision rests: the Religious Freedom Restoration Act,” he wrote on Cato’s blog. “That Congress had to act to try to restore religious freedom — to carve out a space for it in a world of ubiquitous, omnipresent government — speaks volumes.”



In _Harris v. Quinn_ , the Court ruled that government does not have the power to force public employees to associate with a labor union. At issue was an Illinois law claiming that home‐​care workers were public employees, ostensibly for one purpose: collective bargaining. The forced unionization of homebased workers has spread to nearly a dozen states, providing a substantial number of new workers — and dues — to the labor movement. “[This] decision will slow, and perhaps eventually end, that flow of funds, as workers decide they can represent their own interests and would prefer to keep their earnings for themselves and their families,” wrote Cato adjunct scholar Andrew M. Grossman.



Finally, the Court issued its latest blockbuster ruling on campaign finance with _McCutcheon v. FEC_ , striking down the “aggregate” contribution limits on how much money any one person can contribute to election campaigns. As Chief Justice John Roberts wrote for the majority, “If the First Amendment protects flag burning, funeral protests, and Nazi parades — despite the profound offense such spectacles cause — it surely protects political campaign speech despite popular opposition.” Shapiro, for his part, put a finer point on the decision. “In a truly free society, people should be able to give whatever they want to whomever they choose, including candidates for public office,” he wrote.



In one sense, these developments indicate the weight of Cato’s work today. But that impact is the cumulative effect of more than 30 years of intellectual debate. Over that period, the Institute’s mission has been to change the climate of ideas to one more conducive to a government of delegated, enumerated, and limited powers. Since 1989 the Center for Constitutional Studies has been a critical institution in that pursuit. As such, we may find that we are now approaching the Court’s libertarian moment.
"
"A cat is, of course, a cat. Lions are cats too, as are leopards, lynxes and so on – the “Felidae” family contains 41 species in total. But what about other closely related species such as hyenas or mongooses? These animals are not in the cat family: they are cat-like “Feliformia”, but are in their own separate families. So why are some species grouped together in the same families and others separated into different families? It might surprise you to learn that there is no general answer to this question, despite the fact that we now know a lot about evolutionary relationships for groups like mammals. Science has moved on and so should the way we classify life on earth. The science of “taxonomy” categorises species (such as Homo sapiens, in the case of humans) into broader groups such as orders (for example primates) or kingdoms (for example Animalia). Current approaches date back to 18th century Swedish biologist Carl Linnaeus. Linnaeus saw all living things as creations of god and sorted them into hierarchical groups according to how similar or different he perceived them to be. Evolution hadn’t even been theorised in Linnaeus’s lifetime. These days, we have a huge amount of DNA and fossil data to map out how, and when, one species branched out from another. Modern taxonomists therefore aim to base their decisions on evolutionary relationships, but the process remains subjective and there has been no attempt to standardise practises across all species on earth. Taxonomic groups such as birds and mammals represent “classes” under current classification systems, which are then subdivided into orders, families and genera. Our research uses the latest evolutionary trees for birds and mammals to demonstrate that current taxonomic classifications are highly inconsistent. [ ](http://www.onezoom.org/tetrapods.htm?view=1&signs=1&common=1&polytomy=3&ltype=2&hltype=2&font=helvetica&colour=1&init=1&taxa=Felidae&url=https://theconversation.com/evolutionary-evidence-shows-its-time-to-revise-how-we-classify-life-on-earth-33161&name=The%20Conversation&logo=http://emilynicholson.files.wordpress.com/2013/05/conversation-full-logo-cbaac7752ab98f2473e3fd769fa885a6.png&text=link%20to%20The%20Conversation) To resolve this issue, we can use evolutionary trees directly in order to consistently create taxonomic ranks. We applied a technique known as “temporal banding” to the bird and mammal trees, producing new classifications that reduce amount of evolutionary divergence within groups to a minimum. Under these new schemes, 70% of bird groups and 61% of mammal groups need to be revised. Biologists have generally determined the major taxonomic orders fairly consistently – we found that the big groups, such as parrots, hummingbirds & swifts, rabbits & hares, opossums and so on, have been made in a fairly constant manner. But classification can zoom in much further than this – there are 372 species of parrot, for instance, grouped into 86 genera. These more specific groupings are sometimes not much better than if they had been defined at random. Our study considered relationships within taxonomic groups that scientists use on a daily basis. This isn’t just a debate for scientists though, as these classifications have an important impact on what species we choose to study and how we communicate our observations of the natural world.  The New Zealand rockwren (Xenicus gilviventris) provides an excellent example of this. These are fairly unique species, not closely related to other species of wren, and are of conservation concern. When we classified bird species in a consistent manner, New Zealand rockwrens became their own taxonomic order, highlighting their evolutionary uniqueness to everyone. In another example, the dog family (Canidae) and the cat family (Felidae) currently have similar numbers of species but, under our standardised system, the cat family is expanded to include civets, hyenas, mongooses, fossas, and other relatives. As a result the new cat family contains four times more species than the dog family, which remained unchanged.  Since these new families are defined on a consistent basis, they tell us something about the evolution of these groups: cats have diversified far more than dogs over a similar time period. An example from the birds sees the owls, which are currently in the order Strigiformes, split in to two new orders: barn owls and true owls. These two groups are too distantly related to be lumped together. Such grouping by evolutionary divergence is controversial and many taxonomists will still feel that classifications should be focused on physical characteristics – what we call morphological similarity. However, this focus on what animals look like just adds inconsistency. A classification system based on morphology makes sense in theory, but in practise it leads to a high level of subjectivity. It is hard to imagine an objective approach based on morphology that could be applied across the entirety of life on earth. How could someone evaluate the physical difference between a bacterium and an animal? We are currently undergoing a revolution in DNA technology and our understanding of the tree of life is improving quickly. Our study demonstrates an approach that can consistently incorporate this information into the way we classify and view the natural world."
"

Big business has too much power in Washington, according to 90 percent of Americans in a December 2005 poll.



Every week, headlines reveal some scandal involving politicians, lobbyists, corporate cash, and allegations of bribes. CEOs get face time with senators, cabinet secretaries, and presidents. Lawmakers and bureaucrats take laps through the revolving door between government and corporate lobbying. Whatever goes on behind closed doors between the CEOs and the senators can’t be good or the doors would not be closed.



Just what is big business doing with all this influence? There are many assumptions about big business’s agenda in Washington. In 2003 one author asserted, “When corporations lobby governments, their usual goal is to avoid regulation.”



That statement reflects the conventional wisdom that government action protects ordinary people by restraining big business, which, in turn, wants to be left alone. Historian Arthur Schlesinger articulated a similar point: “Liberalism in America [the progression of the welfare state and government intervention in the economy] has been ordinarily the movement on the part of the other sections of society to restrain the power of the business community.” The facts point in an entirely different direction:



 **The Big Myth**



The myth is widespread and deeply rooted that big business and big government are rivals—that big business wants small government.



A 1935 _Chicago Daily Tribune_ column argued that voting against Franklin D. Roosevelt was voting for big business. “Led by the President,” the columnist wrote, “New Dealers have accepted the challenge, confident the people will repudiate organized business and give the Roosevelt program a new lease on life.” However, three days earlier, the president of the Chamber of Commerce and a group of other business leaders met with FDR to support expanding the New Deal.



Almost 70 years later _New York Times_ columnist Paul Krugman assailed the George W. Bush administration: “The new guys in town are knee‐​jerk conservatives; they view too much government as the root of all evil, believe that what’s good for big business is always good for America and think that the answer to every problem is to cut taxes and allow more pollution.” At the same time, “big business” just across the river in Virginia was ramping up its campaign for a tax increase, and Enron was lobbying Bush’s closest advisers to support the Kyoto Protocol on climate change.



Months later, when Enron collapsed, writers attributed the company’s corruption and obscene profits to “anarchic capitalism” and asserted that “the Enron scandal makes it clear that the unfettered free market does not work.” In fact, Enron thrived in a world of complex regulations and begged for government handouts at every turn.



When commentators do notice business looking for more federal regulation, they mark it up as an aberration.



When a _Washington Post_ reporter noted in 1987 that airlines were asking Congress for help, she commented, “Last month, when the airline industry found itself pursued by state regulators seeking to police airline advertising, it looked for help in an unlikely place—Washington.” In truth, airline executives had been behind federal regulation of their industry for decades and had aggressively opposed deregulation.



In fact, for the past century and more big business has often relied on big government for support.



 **The History of Big Business Is the History of Big Government**



As the federal government has progressively become larger over the decades, every significant introduction of government regulation, taxation, and spending has been to the benefit of some big business. Start with perhaps the most misunderstood period of government intervention, the Progressive Era from the late 19th century until the beginning of World War I.



President Theodore Roosevelt is usually depicted as the hero of this episode in American history, and his “trust busting” as the central action of the plot. The history books teach that Teddy empowered the federal government and the White House in a crusade to curb the big business excesses of the “Gilded Age.”



A close study of Roosevelt’s legacy and that of Progressive legislation and regulation, however, yields a far different understanding and shows that the experience with meat—big business calling in big government for protection—was a recurring theme. Roosevelt expanded Washington’s power often with the aim and the effect of helping the fattest of the fat cats.



Today’s history books credit muckraking novelist Upton Sinclair with the reforms in meatpacking. Sinclair, however, deflected the praise. “The Federal inspection of meat was, historically, established at the packers’ request,” he wrote in a 1906 magazine article. “It is maintained and paid for by the people of the United States for the benefit of the packers.”



Gabriel Kolko, historian of the era, concurs. “The reality of the matter, of course, is that the big packers were warm friends of regulation, especially when it primarily affected their innumerable small competitors.” Sure enough, Thomas E. Wilson, speaking for the same big packers Sinclair had targeted, testified to a congressional committee that summer, “We are now and have always been in favor of the extension of the inspection, also of the adoption of the sanitary regulations that will insure the very best possible conditions.” Small packers, it turned out, would feel the regulatory burden more than large packers would.



Consider the story of one of the most famous “trusts” in American folklore: U.S. Steel.



In the 1880s and 1890s, rapid steel mergers created the mammoth U.S. Steel out of what had been 138 steel companies. In the early years of the new century, however, U.S. Steel saw its profits falling. That insecurity brought about a momentous meeting.



On November 21, 1907, in New York’s posh Waldorf‐​Astoria, 49 chiefs of the leading steel companies met for dinner. The host was U.S. Steel chairman Judge Elbert Gary. The gathering, the first of the “Gary Dinners,” hoped to yield “gentlemen’s agreements” against cutting steel prices. At the second meeting, a few weeks later, “every manufacturer present gave the opinion that no necessity or reason exists for the reduction of prices at the present time,” Gary reported.



The big guys were meeting openly— with Teddy Roosevelt’s Justice Department officials present, in fact—to set prices.



But it did not work. “By May, 1908,” Kolko writes, “breaks again began appearing in the united steel front.” Some manufacturers were undercutting the agreement by dropping prices. “After June, 1908, the Gary agreement was nominal rather than real. Smaller steel companies began cutting prices.” U.S. Steel lost market share during this time, which Kolko blames on “its technological conservatism and its lack of flexible leadership.” In fact, according to Kolko, “U.S. Steel never had any particular technological advantage, as was often true of the largest firm in other industries.”



In this way, the free market acts as an equalizer. While economies of scale allow corporate giants more flexible financing and can drive down costs, massive size usually also creates inertia and inflexibility. U.S. Steel saw itself as a vulnerable giant threatened by the boisterous free market, and Gary’s failed efforts at rationalizing the industry left only one line of defense. “Having failed in the realm of economics,” Kolko writes, “the efforts of the United States Steel group were to be shifted to politics.”



Sure enough, on February 15, 1909, steel magnate Andrew Carnegie wrote a letter to the _New York Times_ favoring “government control” of the steel industry. Two years later, Gary echoed this sentiment before a congressional committee: “I believe we must come to enforced publicity and governmental control… even as to prices.”



When it came to railroad regulation by the Interstate Commerce Commission, the railroads themselves were among the leading advocates. The editors of the _Wall Street Journal_ wondered at this development and editorialized on December 28, 1904:



Nothing is more noteworthy than the fact that President Roosevelt’s recommendation recommendation in favor of government regulation of railroad rates and[Corporation] Commissioner [James R.] Garfield’s recommendation in favor of federal control of interstate companies have met with so much favor among managers of railroad and industrial companies.



Once again, big business favored government curbs on business, and once again, journalists were surprised.



To cast it in the analogy of Baptists and Bootleggers, the muckrakers such as Sinclair were the “Baptists,” holding up altruistic moral reasons for government control, and the big meatpackers, railroads, and steel companies were the “Bootleggers,” trying to get rich from government restrictions on their business. Roosevelt was allied to the “bootleggers,” the big meatpackers in this case. To get federal regulation, he found Sinclair a handy temporary ally. Roosevelt had little good to say about Sinclair and his ilk; he called Sinclair a “crackpot.”



This preponderance of evidence drove Kolko, no knee‐​jerk opponent of government intervention, to conclude, “The dominant fact of American political life at the beginning of [the 20th] century was that big business led the struggle for the federal regulation of the economy.” With World War I around the corner, this “dominant fact” was not about to change.



The men who gathered at the Department of War on December 6, 1916, struck a startling contrast. Labor leader Samuel Gompers sat at the table with President Woodrow Wilson and five members of his cabinet.



Joining Gompers and those Democratic politicians were Daniel Willard, president of the Baltimore and Ohio Railroad; Howard Coffin, president of Hudson Motor Corporation; Wall Street financier Bernard Baruch; Julius Rosenwald, president of Sears, Roebuck; and a few others. This extraordinary gathering was the first meeting of the Council of National Defense, formed by Congress and President Wilson as a means for organizing “the whole industrial mechanism… in the most effective way.”



The businessmen at this 1916 meeting had dreams for the CND that went far beyond America’s imminent involvement in the Great War, both in breadth and in duration. “It is our hope,” Coffin had written in a letter to the DuPonts days before the meeting, “that we may lay the foundation for that closely knit structure, industrial, civil, and military, which every thinking American has come to realize is vital to the future life of this country, in peace and in commerce, no less than in possible war.”



The CND, after beginning the project of government control over industry, handed much of its responsibility to the new War Industries Board (WIB) by July of 1917. That coalition of industry and government leaders increasingly took control of all aspects of the economy. War Industries Board member and historian Grosvenor Clarkson stated that the WIB strived for “concentration of commerce, industry, and all the powers of government.” Clarkson exulted that “the War Industries Board extended its antennae into the innermost recesses of industry.… Never was there such an approach to omniscience in the business affairs of a continent.”



Business’s aims in the WIB were much higher than government contracts, and certainly business did not lobby for laissez faire. As Clarkson puts it, “Business willed its own domination, forged its bonds, and policed its own subjection.” Business, in effect, shouted to Washington, “Regulate me!” Business called on government to control workers’ hours and wages as well as the details of production.



A decade later Herbert Hoover practiced more of the same. Hoover’s record was one not of leaving big business alone but of making government an active member of the team. As commerce secretary in the 1920s, he helped form cartels in many U.S. industries, including coffee and rubber. In the name of conservation, Hoover “worked in collaboration with a growing majority of the oil industry in behalf of restrictions on oil production,” according to economic historian Murray Rothbard.



In the White House (where history books portray him as a callous and clueless practitioner of laissez faire), Hoover reacted to the onset of the Great Depression by pressuring big business to lead the way on a wage freeze, preventing the drop in pay that earlier depressions had brought about. Henry Ford, Pierre DuPont, Julius Rosenwald, General Motors president Alfred Sloan, Standard Oil president Walter Teagle, and General Electric president Owen D. Young all embraced the policy of keeping wages high as the economy went south.



Hoover praised their cooperation as an “advance in the whole conception of the relationship of business to public welfare… a far cry from the arbitrary and dog‐​eat‐​dog attitude of… the business world of some thirty or forty years ago.”



Before FDR, Hoover got the ball rolling for the New Deal with his Reconstruction Finance Corporation. The RFC extended government loans to banks and railroads. The RFC’s chairman was Eugene Meyer, also chairman of the Federal Reserve. Meyer’s brother‐​in‐​law was George Blumenthal, an officer of J.P. Morgan & Co., which had heavy railroad holdings.



 **The New Deal and Beyond**



After the groundwork laid by the Progressives, Wilson, and Hoover, the alliance of big business and big government continued throughout the 20th century.



“The greatest trick the devil ever pulled,” said Kaiser Soze in the film _The Usual Suspects_ , “was convincing the world he didn’t exist.” In a similar way, big business and big government prosper from the perception that they are rivals instead of partners (in plunder). The history of big business is one of cooperation with big government. Most noteworthy expansions of government power are to the liking of, and at the request of, big business.



If this sounds like an attack on big business, it is not intended to be. It is an attack on certain practices of big business. When business plays by the crooked rules of politics, average citizens get ripped off. The blame lies with those who wrote the rules. In the parlance of hip‐​hop, “don’t hate the player, hate the game.”



This article originally appeared in the July/​August 2006 edition of _Cato Policy Report_



<em>Tim Carney is the author of The Big Ripoff: How Big Business and Big Government Steal Your Money.</em>
"
"

I haven’t been following the debate over Sen. Dodd’s financial overhaul closely enough to have an opinion on the overall package, but Mike Masnick flags one aspect of the legislation that seems really troubling. Bob Litan explains:   




Under existing law, startup companies can raise money easily and quickly from “accredited investors” — individuals with substantial wealth or income. There is no need for the companies or the investors to gain approval from any state or regulatory official.   
  
  
All of this would change if Section 926 of the Dodd bill is included in any final reform legislation. That section would require, for the first time, companies seeking angel investment to make a filing with the Securities and Exchange Commission, which would have 120 days to review it. This would both raise the cost of seeking angels and delay the ability of companies to benefit from their funding.   
  
  
The negative impact of the SEC filing requirement would be aggravated by the proposed doubling of the net worth or income thresholds required for investors to be “accredited.” 



It’s hard to overstate how important a favorable regulatory climate is to the success of startups. Some of the most important startups have been founded by 20‐​somethings without the resources to hire lawyers or navigate regulatory bureaucracies. And startups frequently find themselves within weeks of insolvency before they have a big breakthrough. Having a crucial round of funding delayed by four months can be the difference between success and failure. If this description of the bill is accurate (and I have no reason to doubt that it is), this provision would be very bad for the future of high‐​tech innovation in the United States.
"
"
I found this over on Jerry Pournelle’s Chaos Manor. It seemed fitting given the discussion as of late. 
Some say the world will end in fire,
Some say in ice.
From what I’ve tasted of desire
I hold with those who favor fire.
But if it had to perish twice,
I think I know enough of hate
To say that for destruction ice
Is also great
And would suffice.
– Robert Frost


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0dde577',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
As I mentioned in my post here about one of the satellite data sets (RSS) that showed a marked cooling globally in 2008, La Niña and PDO seem to be drivers of this change. Here is Joe D’Aleo’s take on it below. – Anthony
By Joseph D’Aleo, CCM ICECAP
Evidence is growing this La Niña will be a longer term event. Most similar important La Niñas are often multi year events (1949-1951,1954-1956, 1961-63, 1970-1972, 1973-1976, 1998-2001). Though the easternmost Pacific near South America has warmed at the surface as the seasonal weakening of the tropical easterlies led to weakened upwelling, it is still cold beneath. Below you can see the latest depth-section of ocean temperatures (top) and anomalies (bottom). Temperature are in degree Celsius. Note the large reservoir of subsurface anomalously cold water (up to 4 degrees C) in the eastern tropical Pacific at 50 to 100 meters.

 Also see the latest CPC depicted ocean heat content in the tropical Pacific. This shows the heat content remains at near maximum deficit levels.
 
These suggest as the easterlies increase again, cooling will return to the east Pacific and La Niña will persist at least well into 2008. The Pacific Decadal Oscillation (PDO) has dropped strongly negative (latest value from NCEP is -1.54 STD). This decline may represent another Great Pacific Climate Shift as the PDO warm and cold phases tend last 25 to 30 years and the last change , to a warm Pacific, occurred in 1976. See more in this pdf here.  If indeed the PDO shift is the real deal, we might expect more La Niñas and fewer weaker El Niños over the next few decades with a net tendency for cooling. Add to that a quieter sun and eventually a cooling Atlantic, and you have a recipe for global cooling.
However, this has its own drawbacks, La Ninas bring more drought and summer heat waves, landfalling hurricanes, large tornado outbreaks, spring floods, winter snows and cold outbreaks than their more famous counterpart, El Niño, which has dominated during the warm PDO era. A while back, Stan Changnon did an interesting analysis which I reported on recently here that suggests the era we have gone through since the late 1970s with dominant El Niños was unusually benign with more benefits than damages and will be looked on as the golden era, a modern climate optimum. Even if all this is correct, you might expect the media and enviro-alarmists ‘evidence’ we are affecting our climate to morph from warming and ice melt to the climate extremes characteristic of La Niñas.
See full pdf here. 
<!– –>


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea0c25407',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Two weeks of international climate negotiations in Lima, Peru, are over, with an agreement pulled out of the bag at the eleventh hour. While Lima has been seen by many as a mere curtain-raiser to talks in Paris in a year’s time, when a new deal needs to reached to replace the Kyoto protocol, it will have an impact beyond this. Lima has reinforced the familiar battle ground between the developed and developing world, and it has seen the re-emergence of a key concept: climate justice. The idea of equity is at the heart of this – the question of how to ensure any UN-backed emissions deal is fair and that those countries that caused the problem do the most to clean it up. This had largely been ignored at previous summits but at Lima it was once again a big talking point. “If equity is in, we are out.” Those were the reported words of Todd Stern, the US chief negotiator, on the eve of the last day of Durban talks back in 2011, when the foundations for a new global agreement were laid. Stern was reacting to the clamour from developing countries that rich, developed nations should take the lead in making emission cuts under the principle of “common but differentiated responsibility and capability”, given their historical responsibility for climate change and their enhanced technological capabilities. While some observers were alarmed by Stern’s position, his words were a fair, if vulgar, rendition of the mind-set that is quite pervasive among many developed countries. Rich nations tend to prefer to wave aside or at least make light their moral responsibility to tackle climate change, while appealing for concerted action by “all parties”.  Pragmatism, realism, and “we are in this together” are some of the phrases used by developed countries as they try to duck their responsibility and cajole developing nations to instead step up their own climate actions. It was to this effect that many Western countries lined up behind the US in Durban.  Eventually all references to equity, justice and common but differentiated responsibility were expunged from the text. It was a short-lived victory.  Events in Lima over the past two weeks have overwhelmingly demonstrated the utter futility of developed countries’ schemes to diminish issues of equity and justice, let alone sidestep them altogether.  In virtually all the key issues and categories under discussion – countries’ mitigation contributions, states’ adaptation commitments, the remit of the loss and damage, and climate finance, among others – equity and differentiation have stood out as sticking points. For example, the G77 group of developing countries said that the principle of equity must guide all negotiations and long-term actions. Showing their heightened distrust in the progress, developing countries even requested that texts should be displayed on the big screen in real time while negotiating to enhance transparency. The harshest word for developed countries, however, came from Bolivian president Evo Morales, who referred to industrialised nations that have appropriated more than their own fair share of global atmospheric space as “thieves” that must be made to pay back what they have stolen. Of course, none of this implies that developing countries should be given an easy ride in negotiating the 2015 climate agreement, or that there are easy approaches to finding a “just” climate agreement. Climate justice is a deeply contested concept, open to multiple interpretations, recommending diverse and sometimes conflicting policy. For example, there are plausible justice-based arguments for allocating carbon emissions quota on individual (per capita) and on national (per country) basis.  However it appears that the Stern approach to international climate politics, seemingly without morality, is beginning to lose ground. If Lima has taught us anything, it is that humanity badly needs a dose of international respect if we are to avoid climate chaos. The brazen scheme to expunge equity from previous climate agreements by the US and its backers only served to further erode the mutual trust sorely needed to make compromises. Morality might be a dirty word in some states’ foreign policy handbooks. But call it what you like, the world needs to find its guiding principles quickly, and developing countries want rich nations to pay for what they’ve broken."
"
Share this...FacebookTwitterThe German Readers Edition reports that 3 leading scientists, among them alarmist Stefan Rahmstorf, are calling on Rajendra Pachauri to step down as Chairman of the IPCC because of management errors and the recent attacks on the IPCC and climate science.According to Stefan Rahmstorf’s blog Klimalounge:
I’m not calling for an end of Pachauri, but I could certainly imagine a better Chairman because in my view, among other reasons, he reacted in an unfortunate manner with respect to the media attacks on the IPCC. The role of the Chairman is not to decide the contents of the report (he should not get involved with our work). Rather he ought to well represent the IPCC externally.
Calls for Pachauri’s resignation are nothing new. In February director emeritus of the Max Planck Institute for Meteorology in Hamburg, Prof. Hartmut Graßl, told the Frankfurter Rundschau newspaper that Pachauri should clear the table and leave the job in other hands.
Hans von Storch, director of the GKSS coastal research center in Geesthacht, Germany, said the IPCC director was a burden because he permitted sloppiness in the reviews and checks of the 2007 climate report.
Readers Edition quotes the current issue of zeo2 titled: The zeo2 Climate Summit, which states:
“Pachauri should throw in the towel.”
 
Share this...FacebookTwitter "
"

 **Lecture at the Friedrich Naumann Foundation.**



Introduction



Trade negotiators, policy analysts, media and others interested in the Doha Round of multilateral trade talks have been asking the same question since the end of the ministerial meeting in Hong Kong in December: where do we go from here? The question implies, of course, that the Doha Round is in serious trouble. Well, that may very well be true.



To find a literal answer, though, one is advised to look to the “Hong Kong Declaration,” which is a statement of recommitment by the ministers to the goal of reaching a comprehensive Doha Round agreement by the end of 2006. The Declaration provides the usual diplomatic platitudes about the nobility of the efforts undertaken and the virtuousness of the goals being pursued. But the document also provides some concrete guideposts to success, from which the enormity of the task at hand can be inferred.



The goal is to complete the negotiations by the end of this year. By April 30, “modalities” (framework and formulae) for the agricultural and non‐​agricultural market access (NAMA) negotiations must be accomplished, and by July 31 the actual numbers to plug into those formulae must be agreed. Meanwhile, all requests for services liberalization are to be made by the end of February, and corresponding offers are to be tabled by July 31.



While no interim deadlines were set for several other items on the Doha Agenda, including the important rules negotiations (which cover the contentious issue of antidumping reform), all of these negotiations will have to produce outcomes that, when considered together, enable 150 trade ministers to agree to the single undertaking that will be known as the Doha Agreement. And all that within 10 months!



WTO Director General Pascal Lamy has been out pounding the pavement, meeting with delegations far and wide, offering encouragement to keep at the negotiations. He says the negotiations are about 60 percent complete, and that the impending deadlines will help “focus minds.” How he calculates the 60 percent figure is a bit mysterious, since most of the contentious decisions have thus far been deferred. While there have been fruitful meetings in the various negotiating committees since Hong Kong, the reports coming out of those meetings show how much still needs to be done.



Meanwhile, EU Trade Commissioner Peter Mandelson and U.S. Trade Representative Rob Portman have been on the diplomatic trail, attempting to convince developing countries that liberalization in their industrial and services industries are in their own interests. That is most certainly true. However, the truth about trade was one of the first victims of the Doha Round. Accordingly, skepticism abounds among trade policymakers and experts in Washington, Brussels, Geneva, and elsewhere regarding prospects for an ambitious outcome to the Doha Development Round. I share that skepticism.



The concept of a Doha‐​lite, which means a far less ambitious agreement than envisioned when the Round commenced, will have to be embraced. It may be the only way to avert failure of the Round, and the residual damage that could cause the WTO.



Why Are We Stuck?



The question of where do we go from here requires an assessment of why we are at this impasse in the first place. There is plenty of blame to go around.



The most obvious answer is agriculture. For almost four and a half years, the emphasis of the negotiations has been on agricultural. Yet little progress has been registered. Rich country farm supports and agricultural tariffs are egregious and should be dismantled, not only because of their adverse impact on poor countries, but because they constitute a waste of limited resources. Taxpayers in the United States and Europe should not be forced to subsidize their well‐​to‐​do farmers, particularly when government budgets have grown out of control. Farm reform is a matter of domestic fiscal necessity more than anything else.



In the months leading up to the Hong Kong ministerial, there was a flurry of activity in the agricultural negotiations. The United States and Europe submitted fairly comprehensive proposals and counter‐​proposals in an effort to inject some momentum into the discussions before Hong Kong. But any momentum initially created soon subsided after several members concluded that the European proposal was far less ambitious than was the U.S. proposal. Without European willingness to go further–to at least the level of reform reflected on paper in the U.S. proposal–there would be little room for substantive progress in Hong Kong.



If nothing else, Hong Kong constituted a public relations victory for the United States. One of the great failings of the United States in the Doha Round was its willingness to appear in lock step with Europe on the agriculture agenda at the ministerial meeting in Cancun in 2003. The appearance to the developing countries that the rich countries were working to scuttle meaningful reform inspired the creation of the G-20, and ultimately the collapse of the talks in Cancun.



In my view, the only way to avert a similar disaster in Hong Kong was for a bold agricultural proposal to be on the table. The fact that big differences were observed between the U.S. and European proposal sent an important signal to the developing countries that the rich countries were no longer in lock step. Europe has taken this development on the chin.



Europe was left isolated as the primary villain in the agriculture saga after Hong Kong, and Peter Mandelson has looked nothing but defensive since then. Mandelson has come under a great deal of criticism for his efforts to dismiss U.S. proposals as disingenuous or simply too ambitious to be practicable–and I largely agree that his rhetoric and tactics have been less than diplomatic–but I also agree with Mandelson’s proposition that Europe should go no further on agriculture unless and until it sees movement on NAMA and Services. After all, this is supposed to be a single undertaking where everything is on the table before an agreement can be reached. The problem is that the developing countries (Brazil and India, in particular) don’t see it that way. And it is they who will determine the Doha Round’s fate.



There is a larger context for understanding why progress in the Doha Round has been scant.



First, in the years between 1995 and 2001 (when Doha was launched), there was a lingering sense of betrayal among some developing countries, a perception that the Uruguay Round was a big success for the rich countries, and gave little to the developing countries. Considering that the most significant “concession” from the rich to the developing countries was the Agreement on Textiles and Clothing (ATC), there is a basis for understanding the sense of betrayal.



The ATC was an agreement to end the decades‐​old quota system, known as the Multifibre Arrangement, which allowed restraints by the United States, Europe, Norway, and Canada on imports of textiles and clothing from almost every developing country. The ATC specified a 10‐​year phase out of the quotas in four stages. At each stage, a minimum percentage of products subject to quota in 1994 were to be liberalized from quota, and the growth rates in remaining quotas were to be accelerated.



But while the United States and Europe may have adhered to the letter of the Agreement, each certainly violated its spirit. The United States chose to liberalize from quota in the first stage (1995) products such as tents, parachutes, awnings, sails, and other products that had never been subject to quota in the first place! Most meaningful liberalization was deferred until the final two stages in 2002 and 2005. In fact, approximately 80 percent of the products subject to U.S. quota in 1994 remained under quota until the final day, January 1, 2005.



Europe was guilty of “backloading” its liberalization too, but to a lesser degree. Instead, Europe used the special safeguard mechanism to curtail import growth after quotas were removed, oftentimes bringing cases with the flimsiest of evidence. Many developing countries harbor the somewhat justified belief that they were double‐​crossed by the rich countries in the Uruguay Round. Their seemingly unbending negotiating postures this Round reflect the lessons of the past.



Then, the terrorist attacks hit America in September 2001, which started to focus attention on what might be the root causes of such violent upheaval. Economic stagnation in the developing world was identified as one of the important causes.



Two months after the attacks, in an effort to show solidarity among the world’s leaders and with a virtuous sense of purpose to start tackling the economic problems in the developing world, the Doha Round was launched and dubbed the “Doha Development Agenda.”



But the development emphasis of the round reflects factors beyond the desire to address economic stagnation and to redress perceived and real grievances of the past. It also reflects the reality of the WTO’s composition. Since the WTO was established in 1995, its membership has grown by 25 percent, and each new member is a developing country. The goal of liberalizing trade by cutting tariffs, which dominated the GATT agenda for most of the post‐​war period, has been transformed into an agenda of development‐​oriented goals, which have not always been hospitable to trade liberalization.



There are now 150 members in the organization with disparate levels of economic development, different negotiating priorities, and asymmetric negotiating resources, attempting (presumably) to reach consensus on a diversity of issues. Add to the mix, the emergence of the anti‐​globalization movement and all the NGOs it has spawned proliferating sometimes good, but usually bad advice to the developing countries. The idea that rich country trade barriers are a primary cause of poor country poverty and that poor country barriers are justified and should not be negotiated away not only stokes the flames of an already pronounced (and somewhat justified) sense of victimization among developing countries, but it also provides the wrong prescription. Furthermore, the bickering between Europe and the United States over the question of who does more for the poor countries lends further credibility to the “victim” position successfully staked out by the developing countries. Why should they offer any market openings when the rich countries tit‐​for‐​tat exercise just might excuse any liberalization from the poor countries?



All of these factors considered together have conspired to create a situation where the developing countries feel that they shouldn’t have to do much in the way of opening up their own markets.



On top of these misguided beliefs, the developing countries have an ace in the hole to back up an uncompromising negotiating position: Brazil’s successful complaints in the WTO against the U.S. cotton program and the EU sugar program. Brazil believes it has already achieved some of the cuts in agricultural subsidies that are being negotiated in the Doha Round. Brazil feels that a large chunk of the reforms being offered by the EU and the US are not concessions at all–that the reforms already have to be made or else, Brazil and other countries can litigate them in dispute settlement with precedence to back them up. The United States and Europe know they are vulnerable on this.



There are yet other important reasons for the Doha impasse. The emergence of China may be the most critical. Many members are scared of the implications of China’s growth, including Europe, which will be imposing new antidumping duties on footwear very soon, and the United States, where Congress is threatening some very provocative, reactionary legislation this election year. But if Europe and America worry about import competition from China, think about how every developing country must feel. China does or can produce almost anything the developing countries can produce. Thus, there is an aversion or even unwillingness among some countries to agree to tariff cuts in the Doha Round because they are afraid of Chinese competition.



Still another reason for Doha’s roadblock: the proliferation of bilateral and regional trade agreements. While these types of trade agreements are not necessarily mutually exclusive with multilateral agreements, the danger is that they can become so.



In 2002, then-U.S. Trade Representative Robert Zoellick announced a policy that he described as “competitive liberalization,” which meant that the United States would pursue bilateral, regional and multilateral agreements simultaneously. The rationale behind the policy was that trade agreements–particularly multilateral ones–can take a long time to materialize, and possibly might not materialize at all. To insulate U.S. trade policy goals from failure due to the limited ambition of others, and to avoid putting all eggs in one basket, Zoellick announced that the U.S. would pursue several alternatives at the same time.



The Free Trade Area of the Americas was to be the main thrust of U.S. liberalization efforts outside of the Doha Round. Beyond the stated goal of providing options for U.S. trade policy, “competitive liberalization” had the strategic benefit of showing the rest of the world that the United States had viable alternatives outside of Doha. The not so subtle message of competitive liberalization was that within the Doha negotiations the United States should no be pushed too hard for concessions and that U.S. demands should be taken seriously.



But the policy took a major blow when it became apparent that the FTAA was going nowhere, primarily because of resistance from Brazil, which was insisting on the same reforms being demanded in the WTO–agricultural and antidumping reform. So, the United States moved to isolate Brazil by concluding its bilateral agreement with Chile, and then announcing negotiations with Central America and several Andean countries. While these negotiations were underway, the political and economic climate in Latin America began to change for the worse and support for the FTAA all but totally dissipated. The United States no longer had a viable alternative to use as leverage for its Doha agenda.



Meanwhile, other countries, particularly in Asia and the Pacific, embarked on bilateral and regional discussion as well. In many regards, several Asian countries have more to show for their efforts than does the United States. There should be little question that prospects like an ASEAN plus China union or an Australia‐​China free trade agreement or a U.S.-Korea free trade agreement undercut at least some of the enthusiasm for a multilateral deal. It also stretches limited negotiating resources, perhaps too thin.



A final, but also very significant explanation for the lack of progress in Doha is that there might not be sufficient interest in a deal from the developing countries. One picture that remains indelibly in my mind is that of several developing country trade delegations and various NGOs, upon learning of the collapse of the talks in Cancun, jubilantly embracing, dancing, and slapping hands in the lobby of the Cancun convention center. I couldn’t quite understand why they should be so happy. At that point, there was fear that the whole round might be dead–a round that, if concluded, would bring many benefits to these poor countries. There reactions, I thought, were antithetical to what they should have been feeling.



The point that this drove home for me was that some developing country negotiators and their governments get a lot of political mileage back home when they are seen standing up to the rich countries. Reaching an agreement would eliminate that stage, and could probably subject them to criticism that they got duped again. Furthermore, I have to believe that some developing country leaders would rather have a deadlock on Doha so that they can continue to blame the rich countries for their woes. Eliminating agricultural subsidies and tariffs, which are only a small part of the broad problems facing developing countries, could expose the domestic problems caused (or not resolved) through their own errors of commission or omission.



There are thus plenty of explanations for the Doha Round’s stasis.



Failure is not an Option



Failure to reach a Doha Agreement by the end of the year could be more severe than simply missing the opportunity to expand trade this go around. In fact, there would likely not be another go around for years to come.



Failure would produce an immediate round of finger pointing, as countries position themselves to deflect blame. This will hasten antagonisms between countries that will have spent 5 years in vain trying to work through difficult issues. It could produce conclusions that there is little real interest in trade liberalization, which could harden perceptions of victimization and distrust. Domestic constituencies that opposed trade liberalization in the first place will be energized by the turn of events, and their views could win favor among a broader cross‐​section of their populations.



Brazil and others would likely prepare more WTO challenges of U.S. and European agricultural policies. In the United States, where Congress has been outspoken and critical of WTO rulings, more adverse rulings would not have a welcome reception. At a time when U.S. congressional antipathy toward trade is rising, it is possible that there would be more calls than usual to ignore WTO findings. Simultaneously, there would be calls for the United States to bring more cases against China (in particular).



If those unfriendly, even hostile sentiments begin to take root, particularly in the absence of an ongoing trade negotiating round, questions regarding the efficacy of the existing rules and the legitimacy of the WTO itself might not be far behind. Doha failure could lead to an erosion of respect for the rules and institutions that have helped expand international trade and investment and have contributed significantly to the economic growth and rising living standards experienced throughout the world over the past 60 years.



A weakened (or merely the perception of a weakened) rules‐​based system of trade could invite a resurgence of protectionism, as countries recoil from previously‐​made commitments. And with international trade and investment flows increasing rapidly on a account of the emergence of China, India, and other formerly smallish economies, politically expedient protectionist policies might prove tempting, as countries grapple with the question of how best to respond to dramatically changing economic circumstances. Fidelity to the rules and institutions will be needed more than ever at a time when temptation to dispense with them is heightening.



Doha’s failure could lead to an increased parceling of the world economy as countries turn more aggressively toward bilateral and regional agreements. While there has been much scholarly debate about the efficacy of bilateral and regional agreements, much of their intellectual support derives from the belief that they are complementary to multilateral deals, and not a substitute for them. Broad, nondiscriminatory trade liberalization under homogenous rules is generally more conducive to producing gains from trade than are discriminatory agreements between subgroups, which could be trade diverting. The so‐​called spaghetti bowl of rules raises the cost of compliance as well.



Another problem with bilateral and regional agreements is that agricultural and antidumping reform would likely be immune from liberalization–as they have been in the past. Furthermore, developing countries tend to be excluded for these types of arrangements, as richer countries tend to cherry pick their prospective partners.



Thus, Doha failure is not a viable option.



Where do we go from Here?



Efforts must be undertaken to ensure that Doha doesn’t fail (which does not mean that an ambitious outcome is necessary). Brazil, India and other large developing countries are in the driver’s seat, but they are on the verge of overplaying their hand. They, and the other developing countries (G20 and G90, alike), would be hurt more from a Doha collapse than would the rich countries. More pressure has to be put on these bigger developing countries to show greater willingness to reduce applied industrial tariffs, not just bound rates.



Developing countries need to be disabused of the belief that it is their right, and in their interest, to do nothing toward reducing their own tariffs. Unless they can show that their economies are opening and that their rules are transparent and that their country is a good place to do business, they are going to get crushed as globalization advances. In this era of just in time, hub and spoke world supply chains, countries are competing with each other for international investment. Investment flows to regions where there is greater certainty in the business and political environment. And where there are fewer frictions and lower costs of doing business. Protectionist policies are anathema to a business‐​friendly environment. Without that environment, the investment won’t come. Without investment, you fall farther behind.



All that being said about how doing more, much more, is in the developing countries own interest, the onus remains on the rich countries to get a deal done. Sustained economic growth in the developing world is an objective shared by countries rich and poor. This objective transcends economics too. It is a matter of profound foreign policy and security policy interest for the United States and Europe, as well.



These geopolitical aspects of the Doha Round need to be trumpeted by Peter Mandelson and Rob Portman, as they start to downplay expectations that a Doha Agreement will bring huge short‐​term benefits to their exporters. The offensive agenda of broadly opening developing country agricultural, non‐​agricultural, and services markets needs to be downgraded. But there are still important benefits to tout.



First of all, a Doha failure, as I argued earlier, would be worse, far worse, for rich country exporters than a deal that only shows gains on paper for developing countries. A deal that benefits the developing countries disproportionately would improve prospects for U.S. and European exporters by giving their prospective developing country customers greater opportunity to earn foreign exchange. This will increase demand for imports, which could inspire greater sales for American and European businesses. Meanwhile, access of rich country producers to cheaper imports will help lower their own costs of production, which could create opportunities for selling at lower prices and thus competing more effectively in developing countries.



Furthermore, liberalization of rich country markets without any rigid demands that developing countries follow suit could inspire what Jagdish Bhagwati calls “sequential reciprocity.” Without the external pressure of negotiations, countries have in many cases come to the realization that reform and trade liberalization was in their interest. India, China, Mexico, Chile, New Zealand, Australia, Singapore, and Hong Kong, to name a few, have all unilaterally liberalized their trade regimes at one point or another without the external pressure that negotiations bring to bear.



As countries grapple with their own policies to find out how best to compete in this dynamic and increasingly linked world economy, perhaps it is better for them to come to their own conclusions at their own paces.



Certainly, it is important that Lamy, Mandelson, and Portman continue to apply some pressure to the G-20 to do their part in offering enough in the way of NAMA and services liberalization so that a plausible, face‐​saving deal can be accomplished. But they shouldn’t push too hard. It could backfire. If developing countries are compelled to accept a level of barrier reduction with which they are not comfortable, then they will be more apt to blame any domestic discontent associated with adjustment on the rich countries for forcing the deal on them. That could inspire a difficult backlash against trade, its institutions, and the countries that advocate it.



The best hope for Doha is an agreement that compels the rich countries to eliminate distorting farm programs and to eliminate or substantially reduce tariffs on products important to the developing countries. Those outcomes are necessary regardless of the other components of the deal. Negotiators should be sure, then, to understand that “Doha Lite” is far preferable to Doha failure.



Thank you.
"
"
One of the great things about our current state of technology is the nearly instant reporting we can get from remote sensing platforms. Thanks to  Dr. Roy Spencer & Dr. Danny Braswell, GHCC at the University of Alabama, Hunsville, we can watch global temperatures of the lower troposphere in near real-time at this page:
http://discover.itsc.uah.edu/amsutemps/
According to UAH: Daily averaged temperatures of the Earth are measured by the AMSU flying on the NOAA-15 satellite. The satellite passes over most points on the Earth twice per day, at about 7:30 am and 7:30 pm local time. The AMSU measures the average temperature of the atmosphere in different layers from the surface up to about 135,000 feet or 41 kilometers. During global warming, the atmosphere near the surface is supposed to warm at least as fast as the surface warms, while the upper layers are supposed to cool much faster than the surface warms.
But as I understand it, the lower troposphere is supposed to be closely coupled to CO2 induced forcings. As we’ve seen from comparison to surface data sets such as HadCRUT, the UAH MSU lower troposphere tracks fairly well with surface temps.
You can learn more about how the Advanced Microwave Sounder Unit on NOAA-15 works and what coverage it has here at my post on it the instrument.
According to the UAH data For 2008, we are averaging about .4 to .5 degrees C cooler than last year. See the graph and click it for a larger one:

Click for larger graph
This tracks with some of the anecdotal eveidence we’ve been seeing in the weather in the northen hemisphere this spring, with late snowfalls, late frosts, and below normal temperatures. The northern latitude areas such as Canada have been very slow to have a spring season.
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9ee7768e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"An experiment in liberalising power markets has been underway in the UK since the 1980s and three phases can be identified. The first ran from around 1989 to 1999, beginning with the privatisation of the generating industry and grid and ending by giving customers the freedom to shop around for their supplier.   When liberalisation proper started in 1999 the system was well supplied – even oversupplied – with generating capacity. So the next stage was one of cut-throat competition for market share. This led to a collapse in the wholesale price of power. Another decade on and new problems emerged as power plants approached the end of their lives. Competitive markets are efficient at getting best value out of existing infrastructure but much less so at deciding when and how it should be replaced – especially when there is uncertainty about how much business coal and gas-fired plants will secure in an era where there are green energy targets to meet.  The problem is that renewable power is not reliable enough to supply all demand, and is often least reliable in the middle of winter when most power is needed – the UK needs about three times as much electricity on a late January afternoon as on an early July morning. This means that to have security of supply, someone needs to build coal/gas/nuclear plants that offer enough reliable capacity to meet peak demand while knowing that for much of the year they won’t have a market at all.  On top of this, the economics of building new carbon-free power are very different from carbon-emitting power. Coal and gas-fired plants tend to be quicker to build and have relatively low capital costs – certainly in the case of the combined cycle gas turbine (CCGT) – but they are expensive to run. Markets prefer this: investors get their money back quite quickly and if the gas price surges, consumers have little choice but to pay the higher power prices that result.   By contrast nuclear power and renewables are expensive and slower to build though they use little or no fuel. Private investors find their capital tied up for longer periods of time without a cash flow. If the project runs over cost and time, as has been seen recently with a number of offshore wind farms, CCGT companies can pick up the business by having a new plant up and running in a couple of years. So governments in the UK and elsewhere have faced real challenges. Do they stick to the market mantra, knowing that to do so they will need to transfer large portions of the risk associated with nuclear and renewables onto the consumer to prevent all investment going into CCGT? Or do they unequivocally renationalise the responsibility for plant mix (while still supporting a competitive market in operation)? So far the answer has been the former. To persuade companies to build renewables, they are not expected to bear the costs of the hugely expanded grid necessary to support their output. When there is too much wind or solar being produced, threatening melting the wires or blowing electronic equipment, renewable generators also get paid to shut their plants down, a benefit not extended to any other players. On top of these enormous hidden subsidies, they also get guaranteed wholesale prices through the “Contracts for Difference” system – a subsidy from which nuclear benefit too.   Crunch point may be coming. Mitigating climate change is coming under threat from an alliance of Big Green and Big Sceptic. Both broadly agree that global warming is probably happening (Big Sceptic less enthusiastically than Big Green but there are very few who do not accept that carbon dioxide is a greenhouse gas). But in practice both argue that the costs of mitigating it are too high –- Big Sceptic focuses on the financial costs, while Big Green frets about nuclear’s environmental costs. (The complete lack of any criticism of increased greenhouse gas emissions in Japan and Germany as they shun nuclear power for purely political reasons is highly illuminating.)  If the fight against greenhouse gas emissions is abandoned under this twin attack from Nigel Lawson and the Greens, all bets are off. And don’t expect an explicit decision so much as a failure to change at the rate needed to meet the very long-term carbon reduction targets. A fracking revolution in Europe like the US one could then reduce dependence on Russia and Iran enough to make a second dash for gas (dwarfing the first) look acceptable.  On the other hand, if the concerns about security of supply and carbon emissions persist then nuclear power is in effect the only source which is both reliable and low carbon. (Two others come close – large dam hydro, which is not quite secure, and biofuels, which are not quite low carbon.)   This means that at least as far as the irreducible 20,000MW of power demand that exists throughout the year is concerned, nuclear is the obvious choice on economic grounds when all costs are concerned. It retains considerable support among British people – even more than before Fukushima, as people realise that in unimaginably stressful circumstance even 1970s nuclear technology did not release enough material to cause detectable health problems. The inevitable waste legacy from the experimental days in the decade or two after fission was discovered will be expensive to resolve but new plants have learned those lessons and volumes of waste will be much lower. Two questions remain though – can the industry deliver to time and cost, and will the government abandon its attempts to persuade investors to carry out public policy at private sector rates of return and instead resume responsibility for the plant mix, allowing it to be carried out at public sector rates of return and slashing the cost to consumers? A yes to both would revolutionise nuclear power’s prospects and return it to its position as the only major technology that can be brought on line quickly. At this uncertain stage, the UK government’s deal over Hinkley Point C and its preliminary agreements over two other new nuclear builds are the only sensible course of action.   To get an alternative viewpoint on nuclear power, now read this piece."
"In recent years, there seems to have been a rise in the extreme weather all over the world from terrible flooding in Bangladesh and Pakistan, the record cold snap in North America, to one of the wettest winters on record in the UK. Extreme events are very difficult to tackle, and in some cases there is little we can do, other than increase our preparedness and our recovery response. However there is one thing we can do in response to smaller scale, more common events such as flooding from intensive rain showers. As winter closes in on, it’s worth looking at some of the ways we can better manage excess water. People have tackled floods for centuries, but modern urban development has thrown up a set of new challenges. The more we develop the landscape, the more rainwater stays on the surface rather than sinking into the soil. That means water gets onto the roads and into drains more quickly, bypassing some aspects of the natural hydrological cycle. Rooftop plants (including green-roofs and roofgardens) along with rainwater collectors and rain gardens can help slow things down and spread the impact of heavy rain out over a longer period. The idea is to replace some of the trees, grass, hollows and wetlands that have been lost to concrete, and so mimic a more natural flow of water. One approach that aims to manage rainwater more naturally is known as Sustainable Urban Drainage Systems (SUDS). SUDS are particularly useful in helping to manage small but frequent floods from rainfall, just as the un-urbanised landscape would. The system has three main aims: to catch and slow down the flow of water; to improve the quality of water by capturing and treating the pollutants it contains; and to benefit the local community by providing a green space that people can enjoy and where wildlife can flourish. Managing water where it lands – at its source – is one of the most effective ways of reducing runoff. We can do this by creating green roofs and raingardens, for example, as well as by designing other mechanisms that slow down the flow of rainwater from our roads, roofs and driveways. These features are often linked to ponds, wetlands and temporary storage areas (called basins) which can hold and treat more water from a larger area. Individual roof gardens, water collectors or raingardens may seem small-scale, but if many of these features are installed in an urban area and they also link up to a larger pond or wetland they can have a huge impact on slowing the flow, cleaning water of some of the pollutants it carries and also providing habitat, attractive settings and helping to recharge our streams and groundwater supplies in a more natural way. Sustainable urban drainage is already being used to great effect in Portland, Oregon, where 3,500 trees have saved the city US$63m in pipe replacement, and in Malmö Sweden, where 6km of water channels and ten retention pools helped an area that previously experienced chronic flooding. In Dunfermline, Scotland, sustainable drainage is being added to a greenfield pre-development. Sometimes it rains so heavily that no sensible drainage system could handle the flow. Where serious and sustained flooding is caused by unprecedented rain – as happened in the UK last winter – it has to be said that these small-scale systems are less effective. Sustainable drainage measures would probably have helped initially, but no form of water management could have accommodated rainfall on that scale.  Record-breaking rainfalls combined with high tides and high water tables presented us with a perfect combination of conditions that any traditional engineering would have been hard pressed to tackle. In these cases, we need to think about how to be more resilient to floods – how we can be more prepared, reduce the impact of the flood, and recover more quickly. Researchers in the UK have been working on this for some time now, and in some cases such approaches have become law – in Scotland, for instance, all new developments since 2006 must have sustainable drainage.  SUDS can deliver sustainable solutions to our urban water management problems. They can give us healthier urban catchments, more livable neighbourhoods, and cleaner rivers and streams. And who doesn’t love a rooftop garden?"
nan
"





There will be a story featuring Al Gore and his climate views on CBS 60 minutes this weekend.  Normally I don’t pay much heed to this program, but Gore is publicly calling those who question the science “…almost like the ones who still believe that the moon landing was staged in a movie lot in Arizona and those who believe the world is flat…”.
To me, a person who has at one time been fully engaged in the belief that CO2 was indeed the root cause of the global warming problem, I find Gore’s statements insulting. In 1990 after hearing what James Hansen and others had to say, I helped to arrange a national education campaign for TV meteorologists nationwide (ironically with CBS’s help) on the value of planting trees to combat the CO2 issue. I later changed my thinking when I learned more about the science involved and found it to be lacking.

I’ve never made a call to action on media reporting before on this blog, but this cannot go unchallenged.
The press release from CBS on the upcoming story on Gore is below. You can visit the CBS website here and post comments:
http://www.cbsnews.com/stories/2008/03/27/60minutes/main3974389.shtml
See the video clip here
But let’s also let the producer, Richard Bonin,  know (via their communications contact) what you think about it, as I did when Scott Pelley aired a whole hour long special telling us Antarctica was melting. They did no follow up.

Kevin Tedesco KEV@cbsnews.com
Director, CBS News Communications (”60 Minutes”)



That email is listed on the CBS website, so it is fair to send comments to it. In fact, here is a contact list they have on their website where you can comment about this story. I feel it is important to respond and to spread the word to others. While I have not seen the video segment, let us hope that it has some semblance of balance, because the press release certainly does not.






			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea031f862',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"A dramatically-named “weather bomb” exploded across the UK in the past week, bringing winds gusting up to 144mph on outlying islands. But despite the cool name these “bombs” are more common than you might think. The UK’s forecasting agency the Met Office defines a weather bomb as an intense low-pressure system with an atmospheric pressure in its core that drops at least 24 millibars in 24 hours. Their charts indeed show an impressive low-pressure system tracking slowly eastwards between southern Greenland and Iceland during 8 and 9 December.  The more precise meteorological term is “explosive cyclogenesis”. It occurs when a low-pressure system is subject to sudden, explosive intensification, typically after an injection of cold, dry air from the stratosphere. This doesn’t normally happen: the stratosphere lies above an altitude of 10 to 15 kilometres and the air up there tends to stay there. This air from above bumps into the warmer, wetter air below which has the effect of reinvigorating and spinning up the low-pressure system. As air in the centre of the system rises ever more rapidly, the ground-level air pressure drops sharply. More air rushes in to fill the space, meaning very strong winds. Bombs are likely to be triggered by changes in narrow, fast-flowing currents of air high above the Earth’s surface known as jet streams. The North Atlantic polar front jet stream is a key player in an ever-present but varying meteorological battleground where warm, moist tropical air masses meet cold polar air from further north. This week the jet stream has been very vigorous with core speeds of up to 200 mph, about twice as fast as normal. These “weather bombs” are not unusual features of the UK’s autumns and winters. The British Isles do after all lie in the path of strong prevailing westerly winds and are often near the confluence of tropical and polar air masses.  Take last winter, for example. In the UK this was one of the stormiest seasons on records going back for well over a century, after a series of more than half a dozen major storm systems ran across the country in quick succession. Indeed during one of these storms Stornoway on the Isle of Lewis in Scotland’s Outer Hebrides saw one of the lowest sea-level air pressures on record.  Then there was the October 1987 storm that caused 18 deaths, £300m worth of damage (at 1987 prices), felled 15 million trees and was responsible for the first major blackout in London since the Blitz. That storm was caused by another weather bomb, in the Bay of Biscay off the west coast of France. The barometric pressure change of 25.5 millibars in three hours – indicative of exceptionally strong winds – recorded at a navy base on the south coast of England was easily the greatest on record for the British Isles. But 1987 wasn’t the UK’s worst storm ever. The November 1703 storm, famous today for the author Daniel Defoe’s pioneering journalistic account, left an unprecedented trail of destruction in its wake. It arrived with minimal warning and killed somewhere in the region of 10,000 people.  So the recent low-pressure system – the bomb – has certainly led to some spectacularly high waves and wind speeds but it is not an untypical intense winter low pressure system; given its position in the extreme northern Atlantic around a favoured spot where cyclones form, it does not seem very unusual from a climatological perspective.  Famously, the Met Office largely failed to forecast the 1987 storm but these days it is much better at making short-term weather predictions of up to a week ahead. Provided we listen to the Met’s severe weather warning system  I don’t think we should be unduly scared by “weather bombs”. In a maritime climate they are just a normal part of the autumn and winter weather experience.  Of course climate change could mean more such bombs in future – after all, it seems to be making many weather extremes more common. However, these storms are formed and controlled by the polar jet stream and models of the climate over the next century aren’t too clear on what will happen to the jet stream. Even reconstructions of past storms based on wind speed and pressure records show lots of fluctuations but no clear, consistent trend. Unlike steadily rising global temperatures, we can’t say for sure that these storms will increase. The IPCC cites “substantial uncertainty and thus low confidence” in projecting changes in North Atlantic storms. We simply don’t yet know whether global warming will necessarily mean more storms and “weather bombs” over the British Isles."
"Mushroom, pineapple and algae: it sounds like the topping for a rather unusual pizza. In fact, they could be the crucial ingredients in the wardrobe of the future as growing numbers of designers try to create fashion that doesn’t harm the environment. Examine a garment’s care label and you may find that it was made out of pineapple stalks or cactus leaves, or a tote bag was woven with thread made from banana trees. From mushroom leather to algae T-shirts, the search is on for alternative materials with smaller carbon footprints. And the latest result are carbon-negative clothes made with algae that absorb carbon dioxide from the air.  “Fashion is part of the problem but it’s also part of the solution,” said Nina Marenzi, founder and director of the Sustainable Angle, a not-for-profit organisation which promotes green textiles at its annual Future Fabric Expo. “We begin with materials and making them sustainable, and if fashion supply chains can change, then we start to address that.” The New York designer Charlotte McCurdy has made a see-through bioplastic mac using algae – specifically algae powder used in vegan food products. She worked with glass casters to find a way to heat the algae and cool it in a controlled fashion to make it transparent. The material is carbon-negative because the algae draw carbon out of the atmosphere, meaning the coat acts as a carbon sink. “Follow the carbon – where did it come from?” she said. “Has it come from carbon taken out of the atmosphere millions of years ago and put in the ground? We talk a lot about what happens to materials after we use them, but not where they come from in the first place.” Post Carbon Lab is using the same principle with another algae prototype – clothes that photosynthesise. The start-up in London has created photosynthesis coating, a layer of living algae on the fabric of garments that absorb carbon dioxide and emit oxygen, turning the carbon into sugar. One large T-shirt – nearly a square metre of material – generates about as much oxygen as a six-year-old oak tree, according to the co-founder Dian-Jen Lin. The start-up has been working with designers and industry to translate its photosynthesis coating into a marketable product, and Lin said it could be used in shoes, backpacks, curtains, pillow cases, umbrellas and building canopies. The care instructions were rather different to normal clothes, she said. Wearing algae was not without its perils. “You can’t put it into your dark wardrobe. It needs light and carbon dioxide, so you have to put it in a well-ventilated area, like the back of your chair.” Washing machines would harm the algae, so “it’s handwash only – you have to be a bit careful. I wouldn’t recommend this coating for your underwear but maybe for a windbreaker or a jacket.” Lin and her co-founder Hannes Hulstaert are testing the limits of the coating, which she says can be applied to almost any garments, either as a full coating or a print. “But it might change colour if it’s really upset, if it didn’t like the light or temperature,” Lin said. “Most of the organisms are in the green shade. In the healthy state they are dark brownish green, orangeish green. When it’s unhappy it might turn yellow, orange, brown, purple or white or transparent.” However, it seems remarkably resilient. “We’ve had samples for three years which have come back to life,” Lin said. Other textiles include Piñatex, made from pineapple leaves and used by Hugo Boss and H&M, and Mycotex, a substance grown from mushrooms. Cactus is the next plant-based leather to emerge, the creation of Desserto, a Mexican company that makes leather from leaves. The challenges facing the fashion industry in its quest to become greener are huge. The UK throws about 300,000 tonnes of clothes into landfill each year, and some studies suggest global textile production creates 1.2bn tonnes of carbon dioxide a year – more than airlines and shipping combined."
nan
"India Block makes many valuable points in her critique of the disastrous standard of accelerated development in UK cities (Who wants cities of ugly new-builds?, Journal, 5 February), but I was exasperated by her failure to mention the non-negotiable bottom line for rejection of this model, especially given the juxtaposition of her article with Steve Bell’s cartoon above it: the climate emergency. As we know, new construction is responsible for 40% of carbon emissions, and operational building emissions account for 28%; speculative development and large-scale “urban regeneration” is complicit in the catastrophic trajectory of global heating and the collapse of ecosystems.  Architects have been developing innovative and radical approaches to address the burgeoning environmental crisis for decades – mostly ignored by unenlightened clients pleading poverty. More recently, schools of architecture in universities, and also cities across the world, are declaring a climate emergency, taking the lead where national governments and powerful corporations have failed to. We have to completely shift our understanding of the measures that are needed to tackle the crisis we face over the next decade, among which the tired concept of “sustainability” seems scarcely serviceable. It is time for city leadership and society as a whole to turn its back on new development as a principle of urban growth and commit to the model of the ecologically “smart” city based on husbanding and reuse of existing resources: the gold standard “world-class city”. For these new-builds are not just ugly on the surface, they are destroying the ecosystem that sustains us on this planet.Dr Clare MelhuishDirector, UCL Urban Laboratory; principal research fellow, Bartlett Faculty of the Built Environment, University College London"
"

Is it possible to address environmental problems, such as pollution, without resorting to the traditional regulatory approach? In the new issue of the _Cato Journal_ , economists Geoffrey Black, D. Allen Dalton, Samia Islam, and Aaron Batteen offer one prominent example of allowing the market to work. By examining the New York City Watershed Memorandum of Agreement (MOA), the authors demonstrate how large‐​scale externalities can be successfully internalized with minimal state intervention.



In 1997, New York City entered into an agreement in which it assigned the area’s watershed communities the property rights to continue developing, despite the fact that some of those activities degraded the city’s drinking water. This assignment placed the burden of water quality on the city. Once responsibility was established, the government opted to buy lands that were contributing to water quality degradation, instead of building a multibillion‐​dollar filtration system. In turn, the residents and landowners upstream were compensated for development restrictions incurred from the agreement. In short, the New York City Watershed MOA is the first of its kind. “The negotiations forged a new method for dealing with externalities showing how … solutions could be facilitated and used in wide‐​reaching economic conflicts,” the authors write.



In September 2012, seven weeks before the presidential election, a Congressional Research Service (CRS) study claimed that there is no evidence that changes in top marginal tax rates have had any impact on U.S. economic growth since World War II. The mainstream media, politicians, and political groups favoring higher taxes on the wealthy widely cited the study as evidence against Mitt Romney’s economic program and in favor of President Obama’s plan to raise top marginal rates. In “Marginal Tax Rates and U.S. Growth,” economists Jason E. Taylor and Jerry L. Taylor revisit the CRS analysis and pinpoint its fatal flaw. “Our results are consistent with what economists have long understood: that a tradeoff exists between income redistribution and economic growth,” they conclude.



Despite pronounced differences in medical financing arrangements, the United States and other countries throughout the Organization for Economic Cooperation and Development (OECD) have witnessed a tremendous growth in health care costs over the last several decades. In “The Medical Care Cost Ratchet,” scholars Andrew Foy, Christopher Sciamanna, Mark Kozak, and Edward J. Filippone explain that health care spending increases over time as new technologies that confer only modest clinical benefits are incorporated into the traditional standard of care. They argue, furthermore, that encouraging individuals to economize on nonemergent health care decisions would help bend the cost curve over time. “Reform efforts,” they conclude, “should focus on rejuvenating market forces that have been systematically suppressed.”



Forecasts of future economic activity underlie any budget revenue projection. However, public choice models of political decisionmaking suggest that government agencies such as the Congressional Budget Office (CBO) and Office of Management and Budget (OMB) face pressures that are likely to result in systematically biased forecasts— whereby, for instance, rosy growth forecasts are rewarded and underforecasted growth penalized. In his article, economist Robert Krol finds that while the CBO, consistent with the private‐​sector forecast, has a downward bias, the OMB estimates indicate a significant upward bias—which is “interpreted to mean executive branch political pressure influences the forecast.” Other contributors include Thomas L. Hogan and William J. Luther on “The Explicit Costs of Government Deposit Insurance,” Paul H. Rubin on “Pathological Altruism and Pathological Regulation,” and Paul Ballonoff with “A Fresh Look at Climate Change.”



The Winter 2014 issue also features reviews of books on the importance of Ayn Rand’s ideas, a theoretical framework for understanding the financial crisis, and the famous bet between Julian Simon and Paul Ehrlich.
"
"

 _The Marketplace of Democracy: A Conference on Electoral Competition and American Politics Sponsored by the Cato Institute and the Brookings Institution_.



The decline of political competition and the overwhelming incumbent advantage are a growing concern for voters and experts alike. At a Cato Conference on March 9, cosponsored with the Brookings Institution, political journalist Michael Barone, Michael Munger of Duke University, and Gary Jacobson of the University of California, San Diego, examined the factors that contribute to electoral stagnation and discussed the merits of possible solutions.



 **Michael Barone** : I am an optimist, so I want to make the case that the American marketplace is working pretty well. There are some market imperfections, of course, but all markets tend to have them. Overall, I think the system works to present choices to people, to register their opinions, and to provide a basis for informed governance that is capable of responding to opinion. And it has responded to the opinions of both the people who call for more government and those who call for less government.



The Founders did not want or desire a two‐​party system, but such a system emerged very quickly after the first Congress went into session. Madison argued in _Federalist_ 10 that a large republic could contain the power of faction because a multiplicity of factions is inevitable in a large republic. Yet, a multiplicity of factions also makes decision making very difficult.



If you look at countries whose electoral systems encourage factions, typically through proportional representation, you often find very small and unrepresentative groups at the fulcrum of power. In Israel, the religious parties have often had enormous clout and have been able to frustrate majorities on issues of particular interest to them. For more than 20 years in Germany, the splinter Free Democratic Party, which always struggled to get more than the 5 percent threshold for representation, determined which major party would control the government.



Our system is different. The result has been that we give our voters relatively clear choices between two alternatives and have parties that are at least somewhat responsive to opinion because unresponsiveness could cost them votes. Sometimes those choices have been crisp. Sometimes they have been muddled. But in the last two decades, our parties have become ideologically much more coherent. People who do not like that result complain of bitter partisanship and polarized voting, but we should remind ourselves that partisanship is the natural result of the coherent, clear choices that political scientists say voters should have. The winners of elections then have the ability to put their programs into law.



A multiparty system might allow some voters to support candidates who share more closely all their political views. Libertarians, for example, do not have a viable party. But a multiparty system creates a lot of problems. Just look over the border at Mexico, with its three‐​party system, which has been unable to address what are clearly some of the major issues before that country. In Canada, with its four‐​party system, the balance of power is now held by a party that wants to separate from the rest of the country. That system is a bit bizarre.



A third‐​party candidate could win a U.S. presidential election. Ross Perot and Colin Powell were viable independent candidates in the 1990s. But they were already well‐​known to voters. Our long public nominating process limits the field of potential candidates to people who enter the race already famous and able to independently finance their campaigns.



People often attack campaign financing as a market imperfection because some candidates are able to raise more money than others. That argument was much more effective in the past than it is today. When I first started observing politics in the 1950s and 1960s, it was said that the Democratic Party could not raise as much money as the Republican Party because they represented the working people, whereas the Republicans represented rich people. Today, that imbalance doesn’t really exist. The Democrats spent more than the Republicans in the 2004 presidential cycle. Both parties had plenty of money to do most of the things that they wanted to do to get their message across.



My own view is that Supreme Court jurisprudence on campaign finance is wacky. The reigning law seems to say that James Madison and the Founders passed the First Amendment in order to protect nude dancing, student arm bands, and flag burning, but they certainly did not want to protect this messy, awful stuff called political speech.



It is said that some kinds of candidates cannot get financing under the current system of campaign finance regulation. What we see today is that, with the Internet, every point of view seems to be able to find abundant financing. If you had told me at the beginning of 2003 that Howard Dean—a medical doctor who is obviously an intelligent man but is palpably unqualified to be president, on the basis of temperament or knowledge—would be able to raise rafts of money, I wouldn’t have believed it. But the Internet has made large scale fundraising possible for even lesse rknown candidates.



With a government that channels vast flows of money to and decides issues of moral importance for citizens, people are going to spend more money on campaigns, and they’re going to spend more time and energy on the political process. Incumbents will always be able to raise more than challengers because they’ve proven that they can win elections and garner benefits for their constituents. But as the late mayor Richard J. Daley of Chicago said when asked whether there should be a benefit to winning elections, “Why should the people who backed the losers get the insurance contracts?”



All points of view seem to be represented in our democracy. Even as we bemoan polarization and gridlock and nasty partisan clashes, I think we also should recognize that those things have resulted in higher voter turnout and greater citizen involvement in politics. The Bush campaign attracted something like 1.4 million volunteers. Total turnout in the popular vote in 2004 was up 16 percent over 2000. John Kerry received the third‐​highest number of popular votes in history, and he lost the election.



I think that we are overdue for a change in the political contours of our country. It may happen in this election, but by 2010 we will certainly see some change in the political landscape. And although redistricting and campaign finance regulation can help protect incumbents or other favored candidates, when voters’ opinions change, the advantages for incumbents or candidates in safe districts may be overcome. The ability of our political system to adjust to such changes in the political climate is a sign that our political marketplace is functioning well.



 **Michael Munger** : There is good political competition and bad political competition. The fundamental human problem is to foster the good and block the bad. So, as I argued in my presidential address to the Public Choice Society in 1988, the fundamental human problem comes down to the design and maintenance of institutions that make self‐​interested individual action not inconsistent with the welfare of the community.



One example of a set of institutions that accomplish that reconciliation of selfish individuals and group welfare is the market, Adam Smith’s “invisible hand.” We still can’t accurately predict the exact circumstances or times when markets might work as he described, but it is definitely not always true that self‐​interest leads to the welfare of the community, even in market like settings. Nonetheless, by and large, we know that competition in markets serves the public interest. The question is this: under what circumstances is competition good in politics?



Good political competition is where ambition checks, or at least balances, opposing ambition. When President Bush tried to push through the Dubai Ports World deal, some senators and representatives objected on its merits. But even more objected on the grounds that the president was usurping congressional authority. Our political rules have to create situations in which politicians’ ambitions are opposed, in which attempts by one group or person to grab all power are always frustrated.



Bad political competition is what public choice theorists call rent seeking. In my classes, I ask students to imagine an experiment that I call a “George Mason lottery.” The lottery works as follows: I offer to auction off $100 to the student who bids the most. The catch is that each bidder must put the bid money in an envelope, and I keep all of the bid money no matter who wins. So if you put $30 in an envelope and somebody else puts $31, you lose the prize and your bid. When I play that game I sometimes collect as much as $150. Rent‐​seeking competitions can be quite profitable. In politics, people can make money by running in rent‐​seeking competitions. And they do.



What are all those buildings along K Street? They are nothing more than bids in the political version of a George Mason lottery. The cost of maintaining a D.C. office with a staff and lights and lobbying professionals is the offer to politicians. If someone else bids more and the firm doesn’t get that tax provision or defense bid or road system contract, it doesn’t get its bid back. The money is gone. It is thrown into the maw of bad political competition.



Who benefits from that system? Is it the contractors, all those companies and organizations with offices on K Street? Not really. Playing a rent‐​seeking game like that means those firms spend just about all they expect to win. It is true that some firms get large contracts and big checks, but they would be better off overall if they could avoid playing the game to begin with.



My students ask why anyone would play this sort of game. The answer is that the rules of our political system have created that destructive kind of political competition. When so much government money is available to the highest bidder, playing that lottery begins to look very enticing. The Republican Congress has, to say the least, failed to stem the rising tide of spending on domestic pork‐​barrel projects. Political competition run amok has increased spending nearly across the board.



In a perfectly functioning market system, competition rewards low price and high quality. Such optimal functioning requires either large numbers of producers or lowcost entry and exit. Suppose that Coke and Pepsi not only had all the shelf space for drinks but asked in addition if they could make their own rules outlawing the sale of any other drink unless the seller collected 100,000 signatures on a petition to be allowed to sell cola. The Federal Trade Commission would not look favorably on the request, on the industry.



But in our political system, we have an industry dominated by two firms. Republicans and Democrats hold 99 percent of the market share and have undertaken actions at the state and national levels to make it practically impossible for any other party to enter. How did we come to have such a system, with outside competition for office nearly closed off but with inside competition for access to the public purse organized as a kind of expensive ritual combat, where Congress keeps all the bids?



I believe that the perverse competition in the political system is a direct consequence of the so‐​called progressive reforms. First, reformers systematically hamstrung the ability of political parties to raise funds independent of individual cults of personality. Parties are actually necessary intermediaries. They solve what my colleague John Aldridge calls the collective action and collective choice problems by giving voters a shorthand by which to identify and support candidates whose opinions they share. Campaign finance reform cut out soft money, thus weakening parties’ ability to support new candidates, but doubled the limits on hard‐​money contributions to members of Congress.



Second, progressive campaign finance reform surrounds incumbents with a nearly impenetrable force field of protection. Any equal spending rule or equal contribution rule benefits incumbents, who can live off free media and other publicity. Any rule that restricts contributions or makes them more expensive, such as reporting requirements for contributions, benefits those with intense preferences and deep pockets. So restrictions on contributions ensure that only the most hard‐​core competitors—those along K Street—participate in the political bidding wars.



The hidden problem is that politics actually abhors a vacuum. If real grass‐​roots parties are denied the soft money they need to mobilize people and solve the problem of collective action and collective choice, organized interests will fill that vacuum. Because no individual can influence government, stripping away intermediary organizations of individuals makes the remaining organized groups more powerful.



The problem is not our inability to reform. The problem is precisely the extent to which we have reformed the system. Our reforms killed healthy political competition at the citizen level. And now all real political competition takes places in the offices on K Street. That’s the kind of political competition that is antithetical to the interests of the community.



 **Gary Jacobson** : After falling irregularly for several decades, turnover in elections to the U.S. House of Representatives has reached an all‐​time low. On average in the four most recent elections (1998–2004), a mere 15 of the 435 seats changed party hands, and only 5 incumbents lost to challengers. Since 1994 Republicans have won between 221 and 232 of the 435 House seats, and Democrats, between 204 and 212, by far the most stable partisan balance for any six‐​election period in U.S. history.



The historically low incidence of seat turnover and partisan change during the past decade has revived scholarly concern about the decline in competition for House seats that had been prompted by a similar period of stasis in the 1980s. It is easy to understand why. Turnover is by definition a product of competitive races. If low turnover reflects the disappearance of competitive districts and candidates rather than, say, unusually stable aggregate preferences among voters, then election results have become less responsive to changes in voters’ sentiments.



A competitive election requires that both parties field competent candidates with sufficient financial resources to get their messages out to voters. But the decisions of potential candidates and donors about whether to participate depend on their estimates of the prospects of success. Politically skilled and ambitious politicians do not invest in hopeless efforts; neither do the people and organizations controlling campaign money and other electoral resources. Judgments about the prospects of success are strongly affected by incumbency— thus open seats tend to attract a much larger proportion of high‐​quality candidates who raise much more money than the typical challenger to an incumbent— but incumbency is not the only consideration. The underlying partisan balance in a district and national political conditions also count heavily in their decisions. Thus at least two developments unrelated to incumbency might have contributed to declining levels of competition and partisan turnover in recent years: a decrease in the number of districts where the partisan balance gives the out‐​party some hope of winning, and the absence of the kind of national partisan tides that raise the chances of victory for the favored party.



What is behind the decline in competitive seats? The favorite culprit of many critics, the creation of lopsidedly partisan districts via gerrymandering, is a relatively small part of the story. A more important factor is that voters have grown more reluctant since the 1970s to vote contrary to their party identification or to split their tickets, making it increasingly rare for districts to elect House candidates who do not match the local partisan profile. A more speculative, though related, notion is that partisans have been voting with their feet by opting to live where they find the social—and therefore political—climate congenial, creating separate enclaves preponderantly red or blue. These alternative explanations for the disappearance of competitive districts are not incompatible; indeed, the processes they entail would be mutually reinforcing.



With the decline in the number of seats on which the current party’s hold seems precarious enough to justify a full‐​scale challenge, strategic calculations about running and contributing have led to an increasing concentration of political talent and resources in the diminishing number of potentially competitive districts at the expense of the rest.



This trend is clearest in the shifting patterns of challenges to incumbents. The proportion of challengers who have previously won elective public office—a crude but serviceable measure of candidate quality— has headed downward, most notably among Democrats. But the disappearance of experienced challengers is confined to districts where the challenger’s prospects were already slim because the partisan balance favored the incumbent.



In districts where the partisan balance (indicated by the presidential vote) is favorable to the challenger’s party, the proportion of experienced challengers has grown substantially; evenly balanced districts have seen little change. Incumbents in districts favorable to the challenger’s party have also become much less likely to get a free pass; in the 1970s and 1980s, about 17 percent of incumbents defending unfriendly territory were unopposed by major party candidates; since then, the proportion has fallen to less than 5 percent.



The increase in partisan polarization and consistency has clearly favored the Republican Party, allowing it to profit from a structural advantage it had held for decades but, until recently, had been unable to exploit. For example, in 2000 the Democrat, Al Gore, won the national popular vote by about 540,000 of the 105 million votes cast. Yet the distribution of those votes across current House districts yields 240 in which Bush won more votes than Gore but only 195 in which Gore out polled Bush. The principal reason for this Republican advantage is demographic:



Democrats win the votes of a disproportionate share of minority and other urban voters, who tend to be concentrated in districts with lopsided Democratic majorities. But successful Republican gerrymanders in Florida, Michigan, Ohio, Pennsylvania, and, after 2002, Texas enhanced the party’s advantage, increasing the number of Bush majority districts by 12, from 228 to 240.



If this analysis is on target, feasible solutions to the problem of declining competition for congressional seats are quite limited. Nonpartisan redistricting might create a few more evenly balanced and therefore potentially competitive districts. But because voters are to blame for most of the recent diminution of such districts, unless mapmakers sought deliberately to maximize their number through pro‐​competitive gerrymanders, the effect would probably be modest under the current distribution of partisans and their levels of polarization and party loyalty.



Campaign finance reforms are also unlikely to have much effect on competition. No more than a handful of challengers in recent elections could make a plausible claim that they might have won but for a shortage of funds; no matter how I analyzed the data, I could detect no significant effect of the incumbent’s level of spending on the results of those elections or any others. Of the 15 House incumbents who have lost since 2000, only 4 were outspent by the challenger; on average they outspent the opposition by more than $500,000. Experienced challengers and campaign donors do not ignore potentially competitive districts, and challengers do not lose simply because incumbents spend so much cash; their problem is a shortage of districts where the partisan balance offers some plausible hope. Senate races, too, have almost invariably attracted experienced and well‐​financed candidates whenever the competitive circumstances have warranted.



The one thing that clearly could generate a greater number of competitive races is not subject to legislative tinkering: a strong national tide favoring the Democrats. Such Democratic landslides as those of 1958 and 1974 put substantial numbers of Democrats into Republican‐​leaning seats (in addition to those they already held), thus leaving a larger portion inherently competitive. A pro‐​Democratic national tide would, by definition, shake up partisan habits, at least temporarily, counteracting the Republicans’ structural advantage. But absent major shifts in stable party loyalties that lighten the deepening shades of red and blue in so many districts, the competitive environment is likely to revert to what it has been since 1994 after the tide ebbs.



This article originally appeared in the May/​June 2006 edition of _Cato Policy Report_
"
"

The “SnoMote Remote Controlled Weather Station”
At first, I though this must be a joke. But, it is not. They call it “an autonomous robot designed by Georgia Tech to gather scientific data in ice environments.” It started life as the Ski-Doo® RC Snowmobile which is 28″ long, and runs for 30 minutes on a charge.
But in a press release from Georgia Tech on May 27th, seen below, it is clear that this is real, true, fully federally funded NASA science project. You can buy one here from Hammacher Schlemmer for $79.95 Ooops, sold out, looks like Georgia Tech bought them out.
My question is, when one of these gets stuck in a crack or crevasse, or simply runs out of power prematurely, do they just leave it there for the polar bears to play with or do they send the lowliest science intern out on the ice to fetch it back, lest it remain to pollute the sea and/or sea ice with it’s Lead or Nickel Cadmium rechargeable batteries?
UAV’s have already been used in the arctic.

Robots go where scientists fear to tread








SnoMote, an autonomous robot designed by Georgia Tech to gather scientific data in ice environments.
Click here for more information.





ATLANTA ( May 27, 2008 ) — Scientists are diligently working to understand how and why the world’s ice shelves are melting. While most of the data they need (temperatures, wind speed, humidity, radiation) can be obtained by satellite, it isn’t as accurate as good old-fashioned, on-site measurement and static ground-based weather stations don’t allow scientists to collect info from as many locations as they’d like.
And unfortunately, the locations in question are volatile ice sheets, possibly cracking, shifting and filling with water — not exactly a safe environment for scientists.
To help scientists collect the more detailed data they need without risking scientists’ safety, researchers at the Georgia Institute of Technology, working with Pennsylvania State University, have created specially designed robots called SnoMotes to traverse these potentially dangerous ice environments. The SnoMotes work as a team, autonomously collaborating among themselves to cover all the necessary ground to gather assigned scientific measurements. Data gathered by the Snomotes could give scientists a better understanding of the important dynamics that influence the stability of ice sheets.








Ayanna Howard, an associate professor in the School of Electrical and Computer Engineering at Georgia Tech, with a SnoMote, a robot designed to gather scientific data in ice environments.
Click here for more information.





“In order to say with certainty how climate change affects the world’s ice, scientists need accurate data points to validate their climate models,” said Ayanna Howard, lead on the project and an associate professor in the School of Electrical and Computer Engineering at Georgia Tech. “Our goal was to create rovers that could gather more accurate data to help scientists create better climate models. It’s definitely science-driven robotics.”
Howard unveiled the SnoMotes at the IEEE International Conference on Robotics and Automation (ICRA) in Pasadena on May 23. The SnoMotes will also be part of an exhibit at the Chicago Museum of Science and Industry in June. The research was funded by a grant from NASA’s Advanced Information Systems Technology (AIST) Program.
Howard, who previously worked with rovers at NASA’s Jet Propulsion Laboratory, is working with Magnus Egerstedt, an associate professor in the School of Electrical and Computer Engineering, and Derrick Lampkin, an assistant professor in the Department of Geography at Penn State who studies ice sheets and how changes in climate contribute to changes in these large ice masses. Lampkin currently takes ice sheet measurements with satellite data and ground-based weather stations, but would prefer to use the more accurate data possible with the simultaneous ground measurements that efficient rovers can provide.
“The changing mass of Greenland and Antarctica represents the largest unknown in predictions of global sea-level rise over the coming decades. Given the substantial impact these structures can have on future sea levels, improved monitoring of the ice sheet mass balance is of vital concern,” Lampkin said. “We’re developing a scale-adaptable, autonomous, mobile climate monitoring network capable of capturing a range of vital meteorological measurements that will be employed to augment the existing network and capture multi-scale processes under-sampled by current, stationary systems.”








Ayanna Howard, an associate professor in the School of Electrical and Computer Engineering at Georgia Tech, with a SnoMote, a robot designed to gather scientific data in ice environments.
Click here for more information.





The SnoMotes are autonomous robots and are not remote-controlled. They use cameras and sensors to navigate their environment. Though current prototype models don’t include a full range of sensors, the robots will eventually be equipped with all the sensors and instruments needed to take measurements specified by the scientist.
While Howard’s team works on versatile robots with the mobility and Artificial Intelligence (A.I.) skills to complete missions, Lampkin’s team will be creating a sensor package for later versions of Howard’s rovers.
Here’s how the SnoMotes will work when they’re ready for their glacial missions: The scientist will select a location for investigation and decide on a safe “base camp” from which to release the SnoMotes. The SnoMotes will then be programmed with their assigned coverage area and requested measurements. The researcher will monitor the SnoMotes’ progress and even reassign locations and data collection remotely from the camp as necessary.
When Howard’s research team first set out to build a rover designed to capture environmental data from the field, it took a few tries to come up with an effectively hearty design. The group’s first rover was delicate and ineffective. But after an initial failure, they decided to move on to something designed for consistent abuse — a toy. Instead of building yet another expensive prototype, Howard instead opted to start with a sturdy kit snowmobile, already primed for snow conditions and designed for heavy use by a child.
Howard’s group then installed a camera and all necessary computing and sensor equipment inside the 2-foot-long, 1-foot-wide snowmobile. The result was a sturdy but inexpensive rover.
By using existing kits and adding a few extras like sensors, circuits, A.I. and a camera, the team was able to create an expendable rover that wouldn’t break a research team’s bank if it were lost during an experiment, Howard said. Similar rovers under development at other universities are much more expensive, and the cost of sending several units to canvas an area would likely be cost-prohibitive for most researchers, she added.
The first phase of the project is focused primarily on testing the mobility and communications capabilities of the SnoMote rovers. Later versions of the rovers will include a more developed sensor package and larger rovers.
The team has created three working SnoMote models so far, but as many SnoMotes as necessary can work together on a mission, Howard said.
The SnoMote represents two key innovations in rovers: a new method of location and work allocation communication between robots and maneuvering in ice conditions.
Once placed on site, the robots place themselves at strategic locations to make sure all the assigned ground is covered. Howard and her team are testing two different methods that allow the robots to decide amongst themselves which positions they will take to get all the necessary measurements.
The first is an “auction” system that lets the robots “bid” on a desired location, based on their proximity to the location (as they move) and how well their instruments are working or whether they have the necessary instrument (one may have a damaged wind sensor or another may have low battery power).
The second method is more mathematical, fixing the robots to certain positions in a net of sorts that is then stretched to fit the targeted location. Magnus Egerstedt is working with Howard on this work allocation method.
In addition to location assignments, another key innovation of the SnoMote is its ability to find its way in snow conditions. While most rovers can use rocks or other landmarks to guide their movement, snow conditions present an added challenge by restricting topography and color (everything is white) from its guidance systems.
For snow conditions, one of Howard’s students discovered that the lines formed by snow banks could serve as markers to help the SnoMote track distance traveled, speed and direction. The SnoMote could also navigate via GPS if snow bank visuals aren’t available.
While the SnoMotes are expected to pass their first real field test in Alaska next month, a heartier, more cold-resistant version will be needed for the Antarctic and other well below zero climates, Howard said. These new rovers would include a heater to keep circuitry warm enough to function and sturdy plastic exterior that wouldn’t become brittle in extreme cold.
###


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f135bf6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Notice all the horrendous news about our environment? That’s a sure sign that the UN is about to throw another mega‐​gabfest where global leaders will shake their heads and shake down the U.S. for monies that Congress will wisely refuse to fork over.



Two weekends from now, the UN is holding its “Rio+20 Earth Summit,” the largest meeting in the history of an organization that pretty much does nothing but stage meetings. The 1992 Rio Summit produced the Framework Convention on Climate Change, which was the basis for the completely failed Kyoto Protocol on global warming. It also spawned Agenda 21, a document which outlined in great detail its plans to punish and pillage producer nations and transmit their wealth to the world’s great kleptocracies.



Rio 1992 was also the basis for 19 annual “Conferences of the Parties” to the Framework Convention, all of which succeeded in doing exactly nothing measurable about climate change. The most famous of these, after Kyoto in 1997, was in Copenhagen in December, 2009.



President Obama flew there, fresh with an “Endangerment Finding” from carbon dioxide hot off of his EPA’s presses. Because it was obvious that the Senate wouldn’t touch cap‐​and‐​trade, he needed something credible in order to goad the world into a new treaty to replace the dead Kyoto agreement. Despite being treated pretty roughly by Brazil, South Africa, China and India, he declared victory — with no specifics — as the meeting drew to a close. Obama couldn’t answer many questions, though, as he had to hightail it back to Washington to beat the first of that winter’s three blizzards. He didn’t, and the image of Air Force One landing in a blinding snowstorm will forever be the icon of the Copenhagen fiasco.



The great “success” of Copenhagen was an agreement that all the participants would submit plans detailing how they would reduce their dreaded greenhouse gas emissions in six weeks. Two weeks before that deadline, Yvo de Boer, Executive Secretary of the Framework Convention, announced that, never mind, we didn’t mean it, we don’t need your silly plans, and then he resigned.



Rio+20 is intended to go beyond all this. Failure is not an option, it is guaranteed.



While the agenda has yet to be finalized at this late date, it’s more of the same hand‐​wringing gloom and doom followed by more of the same outstretched hands. Not surprisingly, the same fault lines that have continually plagued the UN’s are emerging. Poor nations want our money. Europe agrees with this but they’re fresh out. Our Congress wants to be re‐​elected and won’t cooperate. India and China plead for special treatment.



But the list is longer than ever. In addition to climate change, we now have to remediate biodiversity loss, poverty, acid oceans (no such thing), poverty, “unsustainable consumption” (honest!), poverty, the right to food, poverty, and the “right to an adequate standard of living.” If much of this sounds like the wish list of your indolent teenager, that’s about right.



My academic pals are doing their level best to flog for the UN. Just this week, and, according to the _Christian Science Monitor_ , “timed for the Rio meeting,” _Nature_ published a remarkable screed by a team of twenty scientists forecasting the end of the world as we know it (literally) caused largely by increasing human population.



(Hint: a policy‐​driven piece authored by more than ten people, accompanied by a breathless press release, and published before a UN summit is known as a “petition.”)



If this sounds anything like the Club of Rome’s sophomoric 1972 “Limits to Growth,” it is. That forecast of the end of the world as we knew it by 2000 obviously failed, using the advanced methodology of the day (harmonic analysis and multiple regressions). The new paper by Anthony Barnosky uses a “fold bifurcation with hysteresis.” That’s impressive to all the UN delegates, most of whom avoided math and science in order to boss around mathematicians and scientists.



Actually, it really means a lagged discontinuous function, something you can find in honors Algebra II.



The 1972 and 2012 ends‐​of‐​the‐​world are simply the same shtick with the same tactics and objectives, namely abuse of authority to give authority to a global bureaucracy. Between then and now there have been literally dozens of such silly screeds. They obviously didn’t or won’t work, just like the 1992 Earth Summit and Rio+20.



If these people were serious about greenhouse gases and hot air, they would meet online. But they are not, not after 20 consecutive failures.
"
"
Share this...FacebookTwitterAccording to MSNBC, the Katla volcano in Iceland is about to blow her top – hat tip Joe Bastardi Joe Bastardi blog. Katla is the big sis of Eyjafjallajökull;I mean the really big sis. And according to an initial research paper by the University College of London Institute for Risk and Disaster Reduction:
Analysis of the seismic energy released around Katla over the last decade or so is interpreted as providing evidence of a rising … intrusive magma body on the western flank of the volcano.
and
 We conclude that given the high frequency of Katla activity, an eruption in the short term is a strong possibility.
A Katla eruption could be an order of magnitude greater than Eyjafjallajökull and possibly emit significant quantities of ash and sulphur particles into the stratosphere. Read the part about volcanoes at FOCUS warming will end. If that happens, then it’s a game-changer.
And look for a lot of uneasy alarmists to use it as a back door out of an increasingly embarrassing situation.
Share this...FacebookTwitter "
"
La Nina and Pacific Decadal Oscillation Cool the Pacific 

Click here to view full image (228 kb) 

 “The shift in the PDO can have significant implications for global climate, affecting Pacific and Atlantic hurricane activity, droughts and flooding around the Pacific basin, the productivity of marine ecosystems, and global land temperature patterns. ” – NASA JPL

       
A cool-water anomaly known as La Niña occupied the tropical Pacific Ocean throughout 2007 and early 2008. In April 2008, scientists at NASA’s Jet Propulsion Laboratory announced that while the La Niña was weakening, the Pacific Decadal Oscillation—a larger-scale, slower-cycling ocean pattern—had shifted to its cool phase. 
This image shows the sea surface temperature anomaly in the Pacific Ocean from April 14–21, 2008. The anomaly compares the recent temperatures measured by the Advanced Microwave Scanning Radiometer for EOS (AMSR-E) on NASA’s Aqua satellite with an average of data collected by the NOAA Pathfinder satellites from 1985–1997. Places where the Pacific was cooler than normal are blue, places where temperatures were average are white, and places where the ocean was warmer than normal are red.
The cool water anomaly in the center of the image shows the lingering effect of the year-old La Niña. However, the much broader area of cooler-than-average water off the coast of North America from Alaska (top center) to the equator is a classic feature of the cool phase of the Pacific Decadal Oscillation (PDO). The cool waters wrap in a horseshoe shape around a core of warmer-than-average water. (In the warm phase, the pattern is reversed).
See the entire story here:
http://earthobservatory.nasa.gov/Newsroom/NewImages/images.php3?img_id=18012
See the PRESS RELEASE from JPL here:
http://www.jpl.nasa.gov/news/news.cfm?release=2008-066
Look out California agriculture. The wine industry, fruits and nut growers will be hit with a shorter growing season and more threats of frost, among other things.
Recently in Nevada County, much of their grape crop was wiped out. From The Union in Nevada County (h/t Russ Steele)
Nevada County’s agricultural commissioner will seek disaster relief from the state after tens of thousands of dollars worth of crops were ruined from last week’s freezing temperatures.
Orchard trees, wine grapes and pastures were hardest hit, Pylman said. The commissioner is compiling a report of damages that he will send to the state Office of Emergency Services in coming weeks.
“Growers don’t have anything to harvest. That’s a disaster in my mind,” Pylman said.
 
In Paradise, CA, Noble Orchards reports damage to their Apple crop from recent colder weather, as well as reports of issue with vineyards in the Paradise ridge area suffering from frost damage recently.
Here is a short history of PDO phase shifts:
In 1905, PDO switched to a warm phase.
In 1946, PDO switched to a cool phase.
In 1977, PDO switched to a warm phase.
California agriculture has ridden a wave of success on that PDO warm phase since 1977, experiencing unprecedented growth. Now that PDO is shifting to a cooler phase, areas that supported crops during the warm phase may no longer be able to do so.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f7d202f',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
I’ve spent a lot of time on this blog showing how badly maintained and situated the stations in the USHCN network are. And rightly so, the majority of them have issues. But, finding the good ones is actually more important, because they are the ones that hold the true unpolluted temperature signal. Unfortunately, the “good ones” are few and far between.
But when one comes along that is a real gem, it deserves to be highlighted. I present the USHCN climate station of record for Tucumcari New Mexico, COOP ID # 299156, located at the Agricultural Experiment station about 3 miles outside of the edge of town.
I “had” (he just moved to St. Louis) a nephew who lived in Tucumcari, and he just happened to be friends with the director of the experiment farm. Before my nephew left they both helped me get this survey done.

Click picture for additional images
Surfacestations.org image gallery link
This station has several advantages:

Length of continuous record – going back to at least 1946 at this location, possibly to 1905 but NCDC MMS metadata stops at 1946.
Length of continuous instrumentation – using mercury max/min thermometers
Length of continuous data record – there doesn’t appear to be any missing years
Lack of encroachment – 3 miles from the northeast edge of town, little development, little UHI. Tucumcari is well off the beaten path of development. Population actually declined 12% in recent years.
Good siting – the station rates a CRN2 due to distant trees and sun angle, and one small asphalt road 70 meters away.

See the station survey report here (PDF) You can also make out the station on Google Earth using this link. After opening Google Earth, zoom in and the fenced outline and screen will be visible.
Eyeballing, you can see that the temperature data trend for Tucumcari is slightly positive over the last century, about 0.5°C, but there is a “bump” in 2000, which brings it to about 0.9°C. This same bump appears in neighboring stations such as in San Jon (33km away) and in Boys Ranch (135km away). There is nothing in the metadata location or equipment record to suggest a reason for the bump. So, either the bump is naturally occurring, or there is something we don’t know about that changed in the local environment, or we have another data set splicing error like the GISS Y2K debacle from last year.

Click for larger graph from NASA GISTEMP
I plotted the data provided by GISS (which you can find here) to show the effect of the “bump” at year 2000 on the overall trend:

Click for larger graph
Here is the data plot after the GISS homogeneity adjustment, I’ve hue shifted my saved version to red to help keep the graphs visually separate:

Click for larger graph from NASA GISTEMP
And here is the overlay of the USHCN data from GISTEMP and the data from the GISTEMP homogenization process:

In this case, the GISTEMP homogenization code appears to do what would be reasonably expected; reduce temperatures in the present to account for population growth and UHI. I’ve pointed out more than a few times that the GISTEMP homogenization adjustment often becomes flawed for truly rural sites like this when there are large cities within the 250km up to 1200km (depending on process) adjustment zone that Hansen uses, that have accelerating UHI trends. Due to these cities, often the past of a rural station gets adjusted cooler, resulting in an increased temperature trend, such as what happens at Cedarville, CA. Hopefully we’ll have a detailed analysis of that adjustment from John Goetz soon.
If you look at this list, you’ll see that there are a lot of rural stations within 250km. Tucumcari has the advantage of being truly in the middle of nowhere when it comes to other big cities. The closest big cities are Amarillo and Lubbock, but as I understand the algorithm used, when they are near the edge of the 250 km zone, their weighted value decreases.
In this case though, the GISTEMP homogeneity adjustment doesn’t take Tucumcari’s declining population into account, it only uses nightlights, and while the population may dwindle, town infrastructure usually doesn’t; streetlights counted around the station likely remain.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e2be9d3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"

The Cato Institute is committed to holding seminars throughout the country each year, on the basic premise that important questions of policy and governance should not be confined to Washington. At the Policy Perspectives 2012 event in New York City in March, Steve Forbes, chairman and editor‐​in‐​chief of Forbes Media, focused on the economic headwinds holding the country back. The true source of prosperity, he said, is the free market — and these barriers are standing in the way. Mary Anastasia O’Grady, an editorial board member of the _Wall Street Journal_ , detailed the intellectual roots of underdevelopment in Latin America — and, more importantly, the lessons they hold for economic growth in the United States.



 **Steve Forbes** : We all know, even though the economy is doing better this year, that we are still witnessing a punk performance. It’s like being on a superhighway and getting the automobile to go from 20 miles an hour to 40. It should be going 70 or 75 — and, when no one is looking, 125. But something is inhibiting us. I’d like to take a few moments to touch upon the major headwinds that are pushing us back, at a time when we should be moving forward rapidly.



The first, of course, is monetary policy — possibly the most boring subject in the world. The best way to think about monetary policy is to continue the above metaphor. Let’s say you have an automobile, a magnificent vehicle in its own right. If you don’t have sufficient fuel for that car, you’ll stall. With too much fuel, you’ll flood the engine. Only with the right amount do you have the chance to move forward.



The same is true of an economy, even one that has all of the basic strengths. If you don’t supply enough money to meet the organic needs of the marketplace, you’ll stall. If you print too much money, you get the economic equivalent of a flooded engine. The Federal Reserve has been on a bender recently, printing excess money ever since the early part of the last decade. We never would have had a housing bubble if the Fed had not provided the juice for it. Money, of course, is not created by government. It is generated by individuals who make transactions in the marketplace.



Money is simply a facilitator of those transactions. Before money, we had to barter — which means that if I wanted to sell an ad in _Forbes_ , I might have to accept a herd of goats as payment. If I then wanted to turn around and purchase iPads for our writers, I’d have to be prepared for the Apple storeowner to demand sheep rather than goats, which would then take me to a sheep herder, who may prefer red wine to my white wine … . and so on. Simply put, commerce becomes cumbersome.



With money, however, it’s much easier to create capital, invest in the future, and bring that future into the present. The key, however, is that money has to be stable in value, like ounces in a pound or inches in a foot. Can you imagine what would happen if Washington did to the hour what it does to the dollar? Sixty minutes in an hour one day, 42 minutes the next, 77 the following … you would soon need hedges, derivatives, and futures to figure out how many hours you’re working. If you hire someone for $15 an hour, you’d have to specify if that’s a New York hour, an Illinois hour, or a Bangladesh hour. The key is simplicity.



The way to do this is to re‐​link the dollar to gold. For all its imperfections, the question is: What works better, gold or politicians? Without a stable currency measure, people invest in existing hard assets — which results in capital flight from this country because people are not willing to take risks. Equity prices are supposed to reflect the present value of discounted future income flows. But if you don’t know what the income flows are denominated in, that depresses values today. Stability works. Make money a fixed measure of value. Period.



Another headwind that has been holding us back, which Cato has been at the forefront of, is taxes. What the mandarins in Washington don’t understand is that taxes are a price and a burden. A tax on income is the price you pay for working. A tax on capital gains is the price you pay for taking risks.



A tax on profits is the price you pay for success. And the idea is a simple one: when you lower the price of good things like productive work, risk taking, and success, you get more of them. When you raise the price, you get less. Yet the political class keeps trying to raise taxes to solve their own spending problem.



The current tax code, though, goes way beyond raising revenue. It is ultimately a source of power and manipulation, and the biggest source of lobbying in Washington. Last year, we spent 6.5 billion hours filling out tax forms — the equivalent of three million full‐​time jobs. The code has been changed more than 14,000 times since 1986 and, no matter what your faith, it is beyond redemption. The only thing to do with this monster is drive a stake through its heart, bury it, and hope it never rises. We need to start all over.



I am personally in favor of a flat tax: a single rate, with generous exemptions for adults and children, and no federal income tax on the first $46,000 for a family of four. Beyond that, the rate should be 17 percent with no taxes on savings or death. We should be able to leave this world unmolested by the IRS — or, as the Founders would say, no taxation without respiration. The same rules should apply on the corporate side. If you do that, the dollar is as good as gold.



The next barrier to growth, not surprisingly, is spending. Of course, it’s not just that public spending is wasteful. It’s ultimately a source of power. What the government does is, it takes resources from you, puts it through the sausage factory, takes a cut, and then spits it out to politically anointed recipients. That’s not stimulus; it’s stagnation. One reason the current president doesn’t like achieving real reductions in spending is because he knows, given his background, that it is a source of power.



The more resources you control, the more power you have at the center. John Maynard Keynes, after all, said it doesn’t matter whether you dig a hole and fill it up again — it doesn’t matter where the money goes — as long as you have that power. It’s about power, which is why they love it when the government goes from 20 percent of GDP to 25 percent. The more the better.



Regulations are another form of taxation, a burden on the economy. The cost of complying with regulations each year in this country is $1.75 trillion. The regulatory state, in other words, is bigger than every economy in the world except for the top three or four. We do that to ourselves with regulations.



James Madison, the father of the Constitution, wrote in _Federalist 51_ : “If men were angels, no government would be necessary. If angels were to govern men, neither external nor internal controls on government would be necessary.” We need sensible rules for the road. We need speed limits in school zones, for instance — but this is very different from the government telling you what, where, and when to drive.



C. Northcote Parkinson was a British naval historian and author of the bestseller _Parkinson’s Law_. Back in the 1920s, he noticed that the British Navy was sharply downsized after World War I, when they thought they weren’t going to have any more wars to fight. They had far fewer ships, far fewer crews, and far fewer dock workers. But what Parkinson noticed in particular was that the Admiralty, which ran the navy, was bigger than it was when the war ended! And he concluded that organizations grow — like weeds — until somebody stops them, no matter what may be the work at hand.



The same is true of all organizations. They all, if you leave them alone, lose sight of why they were originally created. But private markets serve as a check on this tendency. If you expand to fill the allotted time, without meeting the needs and wants of the market, you fail. Why, then, is the FDA still testing based on a model from 60 years ago — a period during which there have been countless major advances in medicine? Why are they letting countless cancer patients lead shortened lives by refusing to approve medications? It’s a power play, in a sea of cumbersome rules, with no incentive to streamline the process.



Ronald Reagan was right. He said that if you want to change minds in Washington, the best way to do it is through the heat of public opinion. It is not enough to have a change at the top and get a few new faces on Capitol Hill. Ideas matter — and we need to make the case that free enterprise works. Markets create social trust. Government destroys it. In the real world, even if you lust for money, you don’t get it unless you provide for others — and, without you knowing it, that creates circles of cooperation. Markets force you to look toward the future. That’s what the Cato Institute understands, and that’s the mentality that we must encourage to get others to understand as well.



 **MARY ANASTASIA O’GRADY** : Many of you are no doubt wondering what Latin America could possibly teach the United States — what with our muscular Constitution, open markets, limits on federal power, and independent central bank. (No snickering, please.) I was once like you. But, in the last few years, I have seen a number of frightening parallels between this country and our neighbors to the south. To be clear, those parallels did not begin with this president, but they have certainly become more pronounced under the current administration.



The fashionable explanation for Latin American underdevelopment blames corruption, lack of education, poor infrastructure, and — my personal favorite — a shortage of money. But these things are symptoms of bad policies, which I sum up as the Three Ps of poverty: populism, protectionism, and prohibition. Our challenges are, how do we keep politicians from turning us into government dependents? How do we keep markets open? How do we change drug laws in a way that prevents organized crime from replacing democratic institutions? Yet, I’m increasingly convinced that, just as corruption and poor infrastructure are byproducts of the Three Ps, so are the Three Ps byproducts of something else. The source of our economic troubles — in both Latin America and here — is, I believe, much more fundamental.



Consider two simple observations. First, to borrow a fundamental principle of the Cato Institute, ideas matter. To be more specific, those ideas that prevail in society as legitimate are what matter. And second, without entrepreneurship, it is impossible for a society to achieve prosperity.



Looking beyond the immediate policy challenges in Latin America, it becomes clear that it is the ideas of academia — and of intellectuals more broadly — that have played the most important role in undermining the entrepreneurial spirit in Latin American over the past century. Ideas that are hostile to entrepreneurship are not only part of the popular culture, they are embedded in the basic institutions of these countries.



At their core, these ideas hold that profits are morally suspect and private property is not justified, and it is these ideas that strike directly at the heart of prosperity for hundreds of millions of Latin Americans. How did this happen? As John Maynard Keynes wrote, “The ideas of economists and political philosophers, both when they are right and when they are wrong, are more powerful than is commonly understood. Indeed the world is ruled by little else.



Practical men who believe themselves to be quite exempt from any intellectual influence are usually the slaves of some defunct economist. Madmen in authority” — we won’t mention names — “who hear voices in the air are distilling their frenzy from some academic scribbler of a few years back. I am sure that the power of vested interests is vastly exaggerated compared with the gradual encroachment of ideas.” This is a truism that Latin Americans did not understand until it was too late — and it is how we too will lose if we don’t emphasize the moral case for the market. Latin Americans, of course, have no problem being entrepreneurial.



Immigrants to the United States have a long history of starting their own businesses once they’ve landed. So why don’t they display these same skills at home? I submit to you that it is because the dominant ideas in the region over the last century have been hostile to entrepreneurship.



In a new book entitled _Redeemers: Ideas and Power in Latin America_ , the Mexican historian Enrique Krauze profiles 12 individuals who he believes represent the major political ideas in the region from the middle of the 19th century through the 20th.



He starts with José Martí and ends with Hugo Chávez — and along the way he includes profiles on Eva Perón, Che Guevara, Octavio Paz, Gabriel Garcia Márquez, and Bishop Samuel Ruiz, among others. These individuals, Krauze argues, were the ones who sowed the dominant political ideas over that time period. And those ideas focused on hostility toward individualism. Collectivism, economic equality, and the socialization of risk were the chosen themes of political philosophy — and it was the dissemination of these ideas that molded the norms and values of their respective countries. Not one name listed here, by the way, is an entrepreneur. I should add that Krauze also includes Mario Vargas Llosa in the group. He is not a collectivist but he is the exception to the rule.



The power of ideas was well understood among intellectuals on the left throughout the 20th century. They made it their business to get control of academia, and they succeeded. Take for example Venezuela, where the left got total control of the universities, and in the classroom a new narrative emerged. It gave the moral high ground to the state and denounced the market as immoral. Venezuela is reaping the fruits of that indoctrination today. Millions of Latin American students around the region have been marinated in that same stew. This view — that government redistribution is the source of justice and the market is greedy and wrought with failure — has had a profound effect on the political and economic climate in the region.



Today, the ideas of Che Guevara and Eva Perón have been discredited. Modern socialists — those who reject communism and fascism but favor some other form of collectivism — do not attack private enterprise head on. That would be suicidal because the market has created so much prosperity. They therefore emphasize not the wealth of nations, but the immorality of inequality. This, for socialists, is the soft underbelly of the market.



In societies where the morality of the market is understood, vigorously defended, and imparted to young minds, the ethics of collectivism doesn’t do very well. But Latin America shows what can happen when the market is not defended. Even in a society that has made economic gains by adopting free‐​market policies, if the population is not convinced of the legitimacy of the market, it will attempt to destroy what it has achieved.



Take Chile, where since last year students have been running wild in the streets, making all kinds of demands from their government, and accusing those who don’t give in of immorality. The tragedy is that the country’s establishment — including the president — has not been able to put up a strong defense. This is in Chile, the one place in the region that actually reduced poverty significantly. We should be thankful for scholars like José Piñera for carrying the torch of liberty in Chile. But the fact remains that while Chileans are beneficiaries of the market system, they don’t seem convinced of the morality of private property — and of differing outcomes.



Outside of Chile, things are even worse. In most of the region, the idea that equality is the highest goal was handed down from the ivory towers and enshrined in the constitutions themselves. Latin American constitutions are hundreds of pages long. They have objectives like guaranteed national development, the eradication of poverty, and the protection of cultural heritage. The 1988 Brazilian constitution offers constitutional rights for everything from health to education. It guarantees minimum salaries, yearend bonuses, and vacation pay. The section dedicated to sports specifies that “the government shall encourage leisure as a form of social promotion.”



Of course, who can object when the goal is to make the poor child more equal to the wealthy entrepreneur? The problem with a constitution that guarantees equality of outcome is that it cannot protect individual rights. It gives the government not only the power, but the obligation to use coercion toward that end. The fundamental problem with Latin development is this lack of liberty, which emanates from constitutional mandates that intrude on every aspect of human action.



What I’m describing originates with the intellectual class, of course, but many of these bad ideas in Latin America gained influence because the business class supported them. The 1961 Venezuelan constitution was, by most accounts, a fairly sound document. But factions, as James Madison would have called them, began to pick it apart. The business community played a key role.



Venezuelan journalist Carlos Ball described the process like this: “Many in the business community did not rebel against the growing state intrusion because they saw it was easier to convince one cabinet minister than a market of consumers. I’ll never forget watching Venezuelan businessmen cheering the nationalization of foreign oil companies, not realizing that the politicians would soon come after them with more controls, regulations, and taxes.”



The lesson is that when the state seizes the moral high ground in matters of personal decisions, there is no end to the steps that it will take to restrain liberty in the name of social justice. Our neighbors to the south have demonstrated it. You may think this can’t happen in the United States of America. Unfortunately, I am nowhere near as convinced.
"
nan
"
I was initially concerned that my stats were down this month, then I remembered that April has 30 days and March has 31.

Of course there’s that nice spring weather, and I recall that TV station ratings suffer a drop during the spring since people are digging out from their winter igloos. But even though I broke even, there was a nice surprise at the end of the month, WordPress put me as the top “hawt” post, even if only for awhile:

It was a nice way to end the month, thanks to all my readers for your help in getting me to NCDC @ Asheville and for the continued patronage!


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9f61a903',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterMany news outlets have reported today that humans hunted and killed off the large methane-emitting mammoths 13,000 years ago, thus causing global cooling. This report has been published in Nature by some University of New Mexico scientists. Again man, armed with nothing but spears and arrows, may have been the culprit in significant climate change. Read here for example. Here, the Telegraph reports:
But by 11,500 years ago, around 80% of these big mammals had vanished forever.
Their disappearance, accounting for more than 114 lost species, came within 1,000 years of the arrival of humans in the New World.
So the mammoth killing ended at about 11,500 years ago? Wouldn’t that mean that it should have gotten colder from then on as a result? Let’s take a look at the temperatures over the last 20,000 year or so.

This graphic is taken from: oceanworld.tamu.edu.
At about 11,500 years ago the Ice Age ended! With all those big inefficient herbivores disappearing and the millions of tons of methane along with it, wouldn’t the hypothesis suggest that the Ice Age would have deepened, and not ended? Using AGW logic, less methane means more cooling, which leads to more ice, which then leads to more albedo, more cooling, more ice…you know an irreversible tipping point into a permanent ice age. But the opposite happened!
To me this stinks of more junk science by a journal desperate to rescue a science that’s quickly going the way of the mammoths: to extinction. This will be debunked in a matter of days.
Update: The following graphic from Jeff Masters Weather Underground shows the temperature for the last 100,000 years. Note all the spikes during the period. Why are the Younger Dryas caused by man and all the other dips not? Clearly the graphic shows that climate is always changing, often wildly. Belching mammoths were not the drivers.

Share this...FacebookTwitter "
nan
nan
"

The additional power that is being granted to experts under the Obama administration is indeed striking. The administration has appointed “czars” to bring expertise to bear outside of the traditional cabinet positions. Congress has enacted sweeping legislation in health care and finance, and Democratic leaders have equally ambitious agendas that envision placing greater trust in experts to manage energy and the environment, education and human capital, and transportation and communications infrastructure.



However, equally striking is the failure of such experts. They failed to prevent the financial crisis, they failed to stimulate the economy to create jobs, they have failed in Massachusetts to hold down the cost of health care, and sometimes they have failed to prevent terrorist attacks that instead had to be thwarted by ordinary civilians.



Ironically, whenever government experts fail, their instinctive reaction is to ask for more power and more resources. Instead, we need to step back and recognize that what we are seeing is not the vindication of Keynes, but the vindication of Hayek. That is, decentralized knowledge is becoming increasingly important, and that in turn makes centralized power increasingly anomalous.



 **THE AGE OF THE EXPERT**



Populists often make the mistake of bashing experts, claiming that the “common man” has just as much knowledge as the trained specialist. However, trained professionals really do have superior knowledge in their areas of expertise, and it is dangerous to pretend otherwise.



I have faith in experts. Every time I go to the store, I am showing faith in the experts who design, manufacture, and ship products.



Every time I use the services of an accountant, an attorney, or a dentist, I am showing faith in their expertise. Every time I donate to a charity, I am showing faith in the expertise of the organization to use my contributions effectively.



In fact, I would say that our dependence on experts has never been greater. It might seem romantic to live without experts and instead to rely solely on your own instinct and know‐​how, but such a life would be primitive.



Expertise becomes problematic when it is linked to power. First, it creates a problem for democratic governance. The elected officials who are accountable to voters lack the competence to make well‐​informed decisions. And, the experts to whom legislators cede authority are unelected. The citizens who are affected by the decisions of these experts have no input into their selection, evaluation, or removal.



A second problem with linking expertise to power is that it diminishes the diversity and competitive pressure faced by the experts.



A key difference between experts in the private sector and experts in the government sector is that the latter have monopoly power, ultimately backed by force. The power of government experts is concentrated and unchecked (or at best checked very poorly), whereas the power of experts in the private sector is constrained by competition and checked by choice. Private organizations have to satisfy the needs of their constituents in order to survive. Ultimately, private experts have to respect the dignity of the individual, because the individual has the freedom to ignore the expert.



These problems with linking expertise with power can be illustrated by specific issues. In each case, elected officials want results. They turn to experts who promise results. The experts cannot deliver. So the experts must ask for more power.



 **JOB CREATION**



With the unemployment rate close to 10 percent, there is a cry for the government to “create jobs.” But the issue of job creation illustrates the increasingly decentralized nature of the necessary knowledge.



A job is created when the skills of a worker match the needs of an employer. I like to illustrate this idea using an imaginary game in which you draw from two decks of cards, one of which contains workers and one of which contains occupations. For example, suppose that you drew “Arnold Kling” from the deck of workers and you drew “fisherman” from the deck of occupations. That would not be a good match, because my productivity as a fisherman would be zero.



You could do worse — my marginal product as an oral surgeon would be negative. However, you could do better if you were to draw an occupation card that said “financial modeler” or “economics teacher.” One hundred years ago, if you had played this game, you had a good chance of finding a match just by picking randomly. Most jobs required manual labor, and for most people manual labor was the most productive use of their working hours.



Today’s work force is more highly educated and more differentiated. As a result, the task of creating jobs requires much more knowledge than it did in the past. A New Deal program like the Public Works Administration or the Civilian Conservation Corps would not have much appeal for a recent law school graduate or laid‐​off financial professional.



Production today is more roundabout than it was 50 years ago. Only a minority of the labor force is engaged in activities that directly create output. Instead, a typical worker today is producing what George Mason University economist Garett Jones calls “organizational capital.” This includes management information systems, internal training, marketing communications, risk management, and other functions that make businesses more effective.



When production was less roundabout, there was a tight relationship between output and employment. When a firm needed to produce more stuff, it hired more workers.



Today, additional demand can often be satisfied with little or no additional employment.



Conversely, the decision to hire depends on how management evaluates the potential gain from adding new capabilities against the risks of carrying additional costs. The looser relationship between output and employment is implicit in the phrase “jobless recovery.” So how does the economy create jobs? There is a sense in which nobody knows the answer. In his essay, “I, Pencil,” Leonard Read famously wrote that not a single person on the face of this earth knows how to make a pencil. Pencils emerge from a complex, decentralized process. The same is true of jobs.



What the issue of job creation illustrates is the problem of treating government experts as responsible for a problem that cannot be solved by a single person or a single organization.



Economic activity consists of patterns of trade and specialization. The creation of these patterns is a process too complex and subtle for government experts to be able to manage.



The issue also illustrates the way hubris drives out true expertise. The vast majority of economists would say that we have very little idea how much employment is created by additional government spending. However, the economists who receive the most media attention and who obtain the most powerful positions in Washington are those who claim to have the most precise knowledge of “multipliers.”



 **HEALTH CARE**



Despite the many pages contained in the health care legislation that Congress enacted, the health care system that will result is for the most part to be determined. The design and implementation of health care reform was delegated to unelected bureaucrats, as was done in Massachusetts.



In Massachusetts, the promises of propo‐​nents have proven false, and the predictions of skeptics have been borne out. Costs have not been contained; they have shot up. Emergency room visits have not been curtailed; they have increased. The mandate to purchase health insurance has not removed the problem of adverse selection and moral hazard; instead, thousands of residents have chosen to obtain insurance when sick and drop it when healthy. The officials responsible for administering the Massachusetts health care system are no longer talking about sophisticated ways of making health care more efficient.



Instead, they are turning to the crude tactic of imposing price controls.



Once again, we have legislators putting unrealistic demands on experts. This results in the selection of experts with the greatest hubris, shutting out experts who appreciate the difficulty of the problem. When the selected experts find that their plans go awry, they take out their frustrations by resorting to more authoritarian methods of control.



 **THE SECURITY APPARATUS**



In July 2010, the Washington Post ran a series of stories on the size and complexity of the national security apparatus that has developed in response to the terrorist attacks of September 11, 2001. Yet with all this manpower and budget, we still have incidents like the Christmas bomber, a would‐​be terrorist who was stopped by citizens.



There are an infinite number of potential terrorist threats. In response, one could devise an infinite number of agencies and policies. There is little or no scope for anyone to question the relationship between costs and benefits.



More than 10 years ago, scientist and author David Brin wrote The Transparent Society, a book that anticipated the problems of surveillance and terrorism in the context of technological advance. Brin advocated making surveillance tools accessible to ordinary citizens. As counterintuitive and potentially disturbing as this sounds, Brin argued that it is better than the alternative, which is giving surveillance tools to government experts only. The latter approach threatens liberty without providing security. Unfortunately, that is the approach that the United States government has adopted, and it has grown out of control.



 **ENERGY AND ENVIRONMENT**



The Department of Energy has decided that it has the expertise to select specific energy projects, such as the electric car that is being developed by Fisker Automotive of California, the recipient of a $500 million loan guarantee. In theory, if the economic prospects for this electric car were good enough, venture capitalists would be willing to risk money on its development. Now, with a loan guarantee, private investors enjoy only the potential gains while taxpayers bear the risk. Many citizens who would never have considered investing in this electric car company are now partners in the venture, except that we have only the downside and no upside.



The officials who are putting taxpayer money at risk may or may not have better expertise than venture capitalists who put their partners’ money at risk. What the officials certainly have is more power.



The threat of climate change, like the threat of terrorism, can be characterized in such a way as to justify an unlimited attempt at expert control. Regardless of whether experts really can accurately measure, predict, and explain climate change, some will be tempted to exercise power as if their analysis were precise and certain.



 **FINANCIAL REGULATION**



The financial crisis spawned demands for new regulatory powers. However, the crisis itself clearly resulted from the misuse of regulatory power in the first place. It was government policy that attempted to promote home “ownership” by encouraging lending with little or no money down to speculators and inexperienced borrowers. It was government capital regulations that steered banks toward AAA‐​rated securities, with no need to investigate the true underlying risks. It was the view of leading regulators at the Federal Reserve and the International Monetary Fund in 2005 and 2006 that the financial system had become adept at managing and distributing risk. The regulators were not powerless to stop the risky behavior; instead, they were convinced that they had everything under control.



If the regulatory experts could not prevent the financial crisis of 2008, the most reasonable inference to make is that financial crises cannot be prevented. There is no such thing as a financial system that is “too regulated to fail.” The recent Dodd‐​Frank legislation gives broad new discretionary powers to regulators.



Many of the important rules, such as bank capital regulations, are left up to the experts. The decision to use new authority to break up or take over risky financial institutions is discretionary.



Unfortunately, the resolution of troubled financial institutions requires rules rather than discretion. With discretion, there is a problem of time inconsistency. No matter how loudly the regulators proclaim that they will not bail out failing institutions, history shows that when a crisis comes the officials in charge would rather do a bailout than face the uncertainty associated with shutting an institution down. Large failing banks will only be closed if there are strict rules in place that tie the regulators’ hands to make bailouts impossible.



Discretionary resolution authority is authority that will never be used. Banks and their counterparties know this, and they will behave accordingly.



 **THE KNOWLEDGE-POWER DISCREPANCY**



As Hayek pointed out, knowledge that is important in the economy is dispersed. Consumers understand their own wants and business managers understand their technological opportunities and constraints to a greater degree than they can articulate and to a far greater degree than experts can understand and absorb.



When knowledge is dispersed but power is concentrated, I call this the knowledgepower discrepancy. Such discrepancies can arise in large firms, where CEOs can fail to appreciate the significance of what is known by some of their subordinates. I would view the mistakes made by AIG, BP, Freddie Mac, Fannie Mae, and other well‐​known companies as illustrations of this knowledge‐​power discrepancy in practice.



With government experts, the knowledge‐ power discrepancy is particularly acute.



As we have seen, the expectations placed on government experts tend to be unrealistically high. This selects for experts with unusual hubris. The authority of the state gives government experts a dangerous level of power.



And the absence of market discipline gives any errors that these experts make an opportunity to accumulate and compound almost without limit.



In recent decades, this knowledge‐​power discrepancy has gotten worse. Knowledge has grown more dispersed, while government power has become more concentrated.



The economy today is much more complex than it was just a few decades ago. There are many more types of goods and services.



Consumers who once were conceived as a mass market now have sorted into an everexpanding array of niches. In the 1960s, most households had one television, which was usually tuned to one of just three major networks. Today, some households have many televisions, with each family member watching a different channel. Some people still watch major networks, but many others instead focus on particular interests served by specialty cable channels. Still others watch very little TV at all.



This increased diversity of consumer tastes in a world of tremendous variety makes the problem of aggregating consumer preferences more difficult. It becomes harder for government experts to determine which policies are in consumers’ interests. For example, is a national broadband initiative going to give consumers access to something they have been denied or something that they do not want? The advances of science are leaving us with problems that are more complex. As fewer Americans die of heart ailments or cancer in their fifties and sixties, more of our health care spending goes to treat patients with multiple ailments in their eighties and nineties. Given the complexity of each individual case, it seems odd that health care reformers believe that government can effectively set quality standards for doctors.



In business, performance evaluation of professionals is undertaken by other professionals who are in the same work group, observing their workers directly, and who understand the context in which the professionals are working. Even then, performance evaluation and compensation‐​setting are challenging tasks. In health care, proponents of government “quality management” propose to evaluate the decision‐​making of professionals and adjust their compensation on the basis of long‐​distance reports. Taking into account the knowledge‐​power discrepancy, this notion of quality management from afar is utterly implausible.



Financial transactions have gotten extremely complex. Some critics blame the use of quantitative risk models and derivative securities.



However, removing these tools would not remove financial risk, and in many respects could make it more troublesome.



One consequence of modern finance is that it exacerbates the knowledge‐​power discrepancy.



It is as futile for financial regulators to try to track down all sources of risk as it is for security agencies to try to keep track of all possible terrorist threats.



How can we deal with the knowledgepower discrepancy in government? It would be great if we could solve the problem by increasing the knowledge of government experts. Unfortunately, all experts are fallible.



If anything, expert knowledge has become more difficult for any one individual to obtain and synthesize. Analysts of the scientific process have documented a large increase in collaborative work, including papers with multiple authors and patent filings by groups and organizations. Scientists tend to be older when they make their key discoveries than was the case in the first half of the 20th century.



When he was an executive at Sun Microsystems, Bill Joy said, “No matter who you are, the smartest people work for someone else.” Joy’s Law of Management applies to government at least as much as to business. There is no way to collect all forms of expertise in a single place.



Instead, the way to address the knowledge‐ power discrepancy is to reduce the concentration of power. We should try to resist the temptation to give power to government experts, and instead allow experts in business and nonprofit institutions to grope toward solutions to problems.



 **LIVING IN A COMPLEX WORLD**



To summarize: We live in an increasingly complex world. We depend on experts more than ever. Yet experts are prone to failure, and there are no perfect experts.



Given the complexity of the world, it is tempting to combine expertise with power, by having government delegate power to experts. However, concentration of power makes our society more brittle, because the mistakes made by government experts propagate widely and are difficult to correct.



It is unlikely that we will be able to greatly improve the quality of government experts.



Instead, if we wish to reduce the knowledgepower discrepancy, we need to be willing to allow private‐​sector experts to grope toward solutions to problems, rather than place unwarranted faith in experts backed by the power of the state.
"
"
According to wire reports, temperatures reached their lowest point in 30 years, reaching to -2°C in the capital, Riyadh, and to -6°C in mountainous regions blanketed by snow.  At least 10 people have died in the country as a weather system driven South from Siberia sent temperatures plummeting. Below are some pictures of snow from that region.
  
click for larger images
Apparently its gotten so bad (or they just aren’t prepared to deal with it) that King Saud ordered that government assistance should be given in the affected areas, which witnessed sub-zero temperatures this week.

I had to laugh at the photo above and the caption:  “Saudi Arabians are used to getting stuck in the sand, but snow is a new challenge for many.” It almosts seems Pythonesque.
Meanwhile, many roads were flooded by heavy rains in the nearby country of Dubai, which attracts sun-hungry tourists with its year-round blue skies. Roofs in some luxury hotels and office blocks were leaking water and several schools asked parents to keep their children home on Wednesday. It’s hard to imagine getting a “rain day” in the middle east.

While I’m enjoying pointing out these uncommon phenomena, I’d also point out that even though both the northern and southern hemispheres have both seen some record cold events in the past 6 months, that doesn’t necessarily equate to “climate change”. Still, something seems afoot as we are seeing more and more events like this. Maybe the massive La Niña now stretching across the Pacific ocean has something to do with this.
Oh but wait…there’s more!

Snow was seen yesterday atop Maui’s Mount Haleakala  – see story
Yeah, somethings up.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea124d90b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

It was a term more notable for the remarkable amount of unanimity than its blockbuster decisions. Nevertheless, the 2010–2011 term of the United States Supreme Court provided plenty of judicial fodder to provoke a day’s worth of discussion at the Cato Institute’s Annual Constitution Day Conference. This year marked the 10th, and as always, it coincided with the release of the new _Cato Supreme Court Review_.



The conference, “The Supreme Court: Past and Prologue: A Look at the October 2010 and October 2011 Terms,” featured leading legal experts discussing the most pertinent cases of the last term and what we can expect in the near future from the Supreme Court.



David Post, law professor at Temple University and a member of the _Review_ ’s editorial board, examined _Brown v. Entertainment Merchants Association_ — teasing out the First Amendment’s “doctrinal oddities” in a provocative essay on the so‐​called violent video games dispute. “The case presents a fascinating snapshot of the state of [free speech] in the early years of the 21st century,” Post said, “and contains enough peculiarities … to keep law professors and their students busy for years to come.”



The _Review_ includes additional articles on the First Amendment — by far the highest‐​profile issue of the term — with case analyses on funeral protests, campaign financing, and much more. John Eastman, law professor and former dean at Chapman University, addressed _Bond v. United States_ — “your typical sordid tale of adultery, toxic chemicals, and federalism,” as _Review_ editor‐​in‐​chief Ilya Shapiro described it. The case involved a defendant being brought up, not on charges of assault, but on violating an international chemical‐​weapons treaty. The judicial lesson? “Don’t mess with the husband of someone who works in a chemical lab!” Eastman quipped.



The day closed with the annual B. Kenneth Simon Lecture, during which a distinguished legal scholar presents a paper that will be included in the following year’s _Cato Supreme Court Review_. This September, the Hon. Alex Kozinski — chief judge on the U.S. Court of Appeals for the Ninth Circuit — discussed the influence of new technologies on emerging expectations of privacy (See page 9). While he acknowledged the public’s willingness to trade certain boundaries for convenience, the story is much more complex than that. “I think it’s fair to say that privacy is not dead as an ideal,” he argued.



The last decade has seen a reanimation of many such ideals. In the foreword to the inaugural volume, Roger Pilon, Cato’s vice president for legal affairs, expanded on the purpose of the _Cato Supreme Court Review_. “We will examine the Court’s decisions and upcoming cases in the light cast by the nation’s first principles — liberty and limited government — as articulated in the Declaration of Independence and secured by the Constitution,” he wrote. The mission, modestly laid out, was to play a part in changing the climate of ideas to one more conducive to a constitutional government of delegated, enumerated, and thus limited powers. The Center for Constitutional Studies, established in 1989, has been a critical institution in making that vision a reality.



“In short, the Court cannot roll back Leviathan on its own,” Pilon says, “but it can put a brake on it and chip away at its substance” — or, perhaps, lack thereof.
"
"The UK is showing a “lack of coherence” in its leadership of vital UN climate crisis talks this year and giving the damaging impression that the talks are not a high priority, one of the world’s leading voices on the climate crisis has said. Mary Robinson, a former UN climate envoy and Ireland’s first female president, also said the perception that major British politicians, including the ex-prime minister David Cameron and former foreign secretary William Hague, were unwilling to take on the role of leading the COP 26 summit was damaging.  “It is not helpful that we are getting the impression that in the UK no one wants the job. I mean, come on! The UK asked for this, they pitched for this responsibility, they must carry it forward.” “This needs to be an overarching priority, that needs to come across. I do not see a coherent drive for [the summit] in the UK. “The UK’s handling of COP 26 has not become coherent enough for the UN even to be able to support them,” she added. Robinson, the former UN high commissioner for human rights who chairs the Elders group of former world statespeople, served twice as a UN climate envoy and campaigns for climate justice. Her exasperation reflects a growing disquiet among some leading international experts over the UK government’s leadership of the vital COP 26 summit, which remains without a figurehead after a tumultuous week. Last Friday, the former energy minister Claire O’Neill was abruptly sacked as president of the negotiations, the key role alongside the UN in bringing countries together to agree tougher limits on greenhouse gases. Since then, O’Neill has unleashed vitriolic attacks on Boris Johnson, alleging a lack of interest in the climate crisis, while Johnson’s failure to set out a clear vision of how the UK will get a deal at COP 26 has also been criticised. “It’s a huge pity this is happening,” said Yvo de Boer, the UN’s top climate official from 2006-2010, who began the push for climate action that resulted in the 2015 Paris agreement. “The president of the COP really needs to be the force for stability in this process. You need someone in place who is reaching out to key countries to smooth this process.” The sacking of O’Neill reminded him of “a very bad time in my life”, he said, when during the Copenhagen COP in 2009 the Danish prime minister removed the serving COP president, Denmark’s environment minister Connie Hedegaard, and installed himself to lead the process. That was one of the factors that doomed the Copenhagen summit to an acrimonious and chaotic end. In this case, the sacking has happened sooner – meaning “this is not irreparable”, said de Boer. “But if it goes on, I think it will be damaging.” Johnson must also do more to show the UK’s leadership in cutting emissions, beyond the promise to reach net zero by 2050, he said. “It’s very important that the UK has domestic policies in place that allow them to inspire confidence in other countries,” said de Boer. One long-time observer of COPs, who asked not to be named, said Johnson’s handling of the launch, at which he failed to set out clear stepping stones to a successful outcome for COP 26 or domestic policies to reduce emissions, had hurt his standing among world leaders on the issue. “There was nothing there, it was a mess,” they said. “It made me angry.” However, there is still strong support for the UK’s COP 26 presidency and a willingness among many leading figures to assist Johnson and the government in building the coalition of countries needed. At COP 26, governments are being asked to come forward with tougher targets to cut greenhouse gas emissions, as the current targets are too weak to meet the goal of the Paris agreement in limiting global heating to 2C above pre-industrial levels. “COP 26 needs to send a signal, to give guidance to governments and businesses [to encourage them to cut carbon],” said Fatih Birol, executive director of the International Energy Agency, and one of the most respected voices globally on climate issues. “I am sure that the UK will be a good manager of the COP.” He said the UK had the diplomatic heft to forge the coalition of countries needed, even without a COP president currently. “I know the UK civil servants [working on climate], they have top notch climate experts, some of the best in the world. I have full confidence in them.” But he called for the government to show more urgency. “Building a team needs to happen immediately, and putting out a clear vision for COP 26. You need leadership, the engagement of all the key parties, including developing countries, and all other stakeholders [including business and civil society].” “The prime minister is really critical – his leadership will be very important to a successful outcome,” he added. Chile, which holds the current presidency of the UN climate process, also offered its support. Carolina Schmidt, who presided over last December’s Madrid climate talks, said: “Since the announcement that the UK will take on the presidency of COP 26, we have met representatives of the UK government on many occasions – including since the COP 25 meeting in Madrid. We look forward to continuing our close collaboration with the incoming COP 26 presidency, as well as other key partners, throughout 2020. This is a crucial year of climate action for humanity as a whole. We have a great challenge ahead: the climate crisis is a reality that all countries must tackle together, and we will stand firmly with our international partners to do so.” Richard Kinley, former deputy head of the UN’s climate secretariat, said there was still time for the UK to recover. “Ideally, you would have started earlier, but politicians in the UK were preoccupied [with Brexit],” he said. “It is late but it is not too late for things to begin, but a lot needs to be done fairly quickly to build momentum for these talks. You need to get mobilised.” As well as the lack of a president, there is a widespread sense that the UK has got off to a slow start in the all-out diplomatic push that is needed to bring reluctant countries together. “The UK diplomatic machine really needs to be running at full speed now,” said de Boer. “They need to be going to other capitals, finding out the key sticking points, and what needs to happen to get a high level of ambition. That’s only going to happen if you understand what the red lines are for all parties.” Some developing countries said they were awaiting more details from the UK, including the identity of the new president. “Without a doubt it would be good that sooner rather than later we will know who our primary interlocutor will be for COP 26,” said a representative of one key group of countries. Saleemul Huq, director of the International Centre for Climate Change and Development, added: “Every day that the prime minister does not have a COP 26 president in place is valuable time being lost”"
nan
"A row over lamps is emerging as a first major test of the EU’s commitment to its much-vaunted European Green Deal and the bloc’s target of carbon neutrality by the middle of the century. A debate over the continued use of mercury in fluorescent lighting has split the 27 member states with Germany’s industrial interests being pitted against the environmental concerns of Sweden, according to leaked correspondence.  The European commission is being asked by Germany, the Netherlands, Hungary, Poland and the Czech Republic to continue to allow manufacturers to use mercury in light bulbs despite the potential damage to the environment and human health. The three main companies in the sector, General Electric, Philips and Osram, are major employers, particularly of German and Hungarian workers. But critics say that an exemption granted to the lighting industry in 2011 from a general ban on mercury use under the Restriction of Hazardous Substances directive is no longer justifiable. Sweden, Finland and Bulgaria, among others, say the successful argument nine years ago that there was not a readily available alternative to mercury in the manufacture of fluorescent lamps is defunct. Mercury-free LED light bulbs were said to produce significantly poorer levels of lighting, but it is now claimed that the technology has sufficiently moved on. Brussels’ next move is being billed as a test of its stated commitment to fighting the climate crisis. “Sweden’s main concern is that there are no legal grounds for renewing an exemption for the use in questions,” a letter from the Swedish government to the commission stated on 22 January. “Today there are economically viable substitutes available for most of the mercury-containing light sources.” In 2017, the EU signed the Minamata convention obliging it to reduce the use of toxic materials such as mercury. The Swedish government argued in its letter that Brussels risked being in breach of its legal obligations. “Sweden also doubts that renewing the exemptions for mercury in question would be in line with the goal on climate neutrality and the ambitions on chemicals of the green deal recently launched by the commission”, its government added, citing the 40.9m metric tonnes of CO2 emissions savings that would be delivered by phasing mercury out by September 2021. The European commission president, Ursula von der Leyen, said in December that the premise of the so-called European Green Deal targeting a “climate-neutral continent” by 2050 was that the “old growth model that is based on fossil fuels and pollution is out of date”. She described it as Europe’s “man on the moon moment” as she laid out 50 policies to revamp EU rules and regulations. Discussions among the member states, the commission and stake-holders on mercury-bearing lamps are due to continue next week."
"

**_Study one of first to look at little‐​known success story_**



Report: Drug Decriminalization Works in Portugal 

In 2001, Portugal took the dramatic step of decriminalizing all drugs, including heroin and cocaine. Although it did not receive a lot of attention at the time, Tim Lynch, who directs Cato’s Project on Criminal Justice, decided it would be a good idea to commission a study on the Portuguese policy experiment after it had been given a fair chance to work over several years. In 2007, when Lynch met best‐​selling author and lawyer Glenn Greenwald and discovered that Greenwald was fluent in Portuguese, Lynch’s search for the right author was finally over. Greenwald readily agreed to the idea of traveling to Portugal to interview key lawmakers and health officials. Upon his return, Greenwald began to prepare the most exhaustive study on the Portuguese experiment. 

On April 2, Cato released _Drug Decriminalization in Portugal: Lessons for Creating Fair and Successful Drug Policies_. The study notes that while other states in the European Union have developed various forms of de facto decriminalization — whereby substances perceived to be less serious (such as cannabis) rarely lead to criminal prosecution — Portugal remains the only EU member state with a law explicitly declaring drugs to be “decriminalized.” (Portugal has stopped short of “legalization” because drug dealing remains a criminal offense.) The shift in policy was controversial. Conservatives in Portugal argued that the move to decriminalize would only worsen that country’s drug problems. 

With more than seven years of experience under the decriminalization regime, Greenwald reports that the policy has been quite successful. One of the key findings of the study is that none of the nightmare scenarios predicted by decriminalization opponents — from rampant increases in drug usage among the young to the transformation of Lisbon into a haven for “drug tourists” — has occurred. As a result, Greenwald reports that the political climate in Portugal has changed: there is no longer any serious debate about whether drugs should once again be criminalized. 

Drug policy experts have seven years of relevant empirical information to examine. Those data indicate that decriminalization has had no adverse effect on drug usage rates in Portugal, which, in numerous categories, are now among the lowest in the EU, particularly when compared with states with stringent criminalization regimes. Although post‐​decriminalization usage rates have remained roughly the same or even decreased slightly when compared with other EU states, drug‐​related pathologies — such as sexually transmitted diseases and deaths due to drug usage — have decreased dramatically. Greenwald says drug policy experts in Portugal attribute those positive trends to the enhanced ability of the government to offer treatment programs to its citizens — enhancements made possible, for numerous reasons, by decriminalization. 

Greenwald’s study has garnered plenty of media attention since it was released in April. _Time Magazine_ , the _Wall Street Journal_ , the _Financial Times_ , and the _Scientific American_ are among the numerous publications that have cited the findings of this Cato report.



In 2001, Portugal took the dramatic step of decriminalizing all drugs, including heroin and cocaine. Although it did not receive a lot of attention at the time, Tim Lynch, who directs Cato’s Project on Criminal Justice, decided it would be a good idea to commission a study on the Portuguese policy experiment after it had been given a fair chance to work over several years. In 2007, when Lynch met best‐​selling author and lawyer Glenn Greenwald and discovered that Greenwald was fluent in Portuguese, Lynch’s search for the right author was finally over. Greenwald readily agreed to the idea of traveling to Portugal to interview key lawmakers and health officials. Upon his return, Greenwald began to prepare the most exhaustive study on the Portuguese experiment. 



On April 2, Cato released _Drug Decriminalization in Portugal: Lessons for Creating Fair and Successful Drug Policies_. The study notes that while other states in the European Union have developed various forms of de facto decriminalization — whereby substances perceived to be less serious (such as cannabis) rarely lead to criminal prosecution — Portugal remains the only EU member state with a law explicitly declaring drugs to be “decriminalized.” (Portugal has stopped short of “legalization” because drug dealing remains a criminal offense.) The shift in policy was controversial. Conservatives in Portugal argued that the move to decriminalize would only worsen that country’s drug problems. 



With more than seven years of experience under the decriminalization regime, Greenwald reports that the policy has been quite successful. One of the key findings of the study is that none of the nightmare scenarios predicted by decriminalization opponents — from rampant increases in drug usage among the young to the transformation of Lisbon into a haven for “drug tourists” — has occurred. As a result, Greenwald reports that the political climate in Portugal has changed: there is no longer any serious debate about whether drugs should once again be criminalized. 



Drug policy experts have seven years of relevant empirical information to examine. Those data indicate that decriminalization has had no adverse effect on drug usage rates in Portugal, which, in numerous categories, are now among the lowest in the EU, particularly when compared with states with stringent criminalization regimes. Although post‐​decriminalization usage rates have remained roughly the same or even decreased slightly when compared with other EU states, drug‐​related pathologies — such as sexually transmitted diseases and deaths due to drug usage — have decreased dramatically. Greenwald says drug policy experts in Portugal attribute those positive trends to the enhanced ability of the government to offer treatment programs to its citizens — enhancements made possible, for numerous reasons, by decriminalization. 



Greenwald’s study has garnered plenty of media attention since it was released in April. _Time Magazine_ , the _Wall Street Journal_ , the _Financial Times_ , and the _Scientific American_ are among the numerous publications that have cited the findings of this Cato report.
"
"
Share this...FacebookTwitterData from NASA point to a powerful Pacific La Nina event in the works, and so with it could bring a considerable drop in the mean global surface temperature in 2021. 
According to the latest report issued by the Australian Bureau of Meteorology (BOM), the La Niña conditions continue in the tropical Pacific: “International climate models suggest it is likely to continue at least through February 2021.”

Peak La Niña conditions expected in January, 2021. Chart source: BOM. 
Central and eastern tropical Pacific Ocean sea surface temperatures (SSTs) are at La Niña levels, and remain similar compared to two weeks ago, reports the BOM. “Models continue to suggest some possibility that central and eastern tropical Pacific SSTs could briefly reach levels similar to 2010–12, with the peak most likely in December 2020 or January 2021.”
The BOM uses the ACCESS–S model for generating its forecasts.
NASA: temperature deviation to -3°C


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Mean while Snowfan here reports that NASA prognoses are in fact expecting an “unusually powerful La Niña development, with cooler than normal surface anomalies extending into the summer of 2021 and which could become an extended year-long event, which also occurred in 2010 – 2012.

Source: BOM NASA/GMAO-ENSO-Prognosen
“November 2020 sees an unusually strong La Niña in the equatorial Pacific with temperature deviations down to below -3°C and with unusually long duration until NH summer 2021,” Snowfan writes. “If the current NASA forecast is correct, a multi-year La Niña could develop into 2022, just like in 2010-2012.”
Strong and long La Niña events cool the Earth by several tenths of a degree Celsius with a time lag. The overall global cooling occurring since 2016 will therefore continue at least until 2021 and could even last until 2022.
Fuel for future bush fires
The BOM notes that La Niña events “typically enhance spring rainfall in northern, central and eastern Australia” and that during a La Niña summer, “above average rainfall is also typical for much of eastern Australia, but particularly eastern Queensland”. But such good news comes with a price: Rainfall means more vegetation growth, which in turn will lead to much more fuel ffor uture bush fires when drought conditions return, as they always inevitably do.


		jQuery(document).ready(function(){
			jQuery('#dd_30ad4b406255c528e276dfb072615ea2').on('change', function() {
			  jQuery('#amount_30ad4b406255c528e276dfb072615ea2').val(this.value);
			});
		});
		
Donate - choose an amount5101520501002505001000
Share this...FacebookTwitter "
"

“In December 1919, Carlo ‘Charles’ Ponzi approached a group of friends and acquaintances in Boston with a new investment opportunity,” Cato senior fellow Michael Tanner writes. What followed has become immortalized as one of the most infamous investment scams in history. In **“Social Security, Ponzi Schemes, and the Need for Reform”** (Policy Analysis no. 689), Tanner considers recent calls comparing this fraudulent operation with the current U.S. social insurance program. The two programs, he says, have several similarities. Social Security, for instance, “does not actually save or invest any of a participant’s payments” — relying instead on inflows from future contributors to finance the system. This, in turn, provides “a windfall to the first participants, but declining returns to subsequent joiners” — also similar in operation to a Ponzi scheme. Finally, Social Security is “a system that worked well when demographics were favorable,” yet it’s “facing insolvency as the ratio of recipients to contributors increases.” Despite these similarities, there is in the end one crucial distinction between the two. “Social Security is not a Ponzi scheme,” Tanner concludes, “because Charles Ponzi didn’t have a gun.” As such, the debate over epithets obscures a much deeper issue: Social Security is unable to pay promised benefits with current levels of taxation. “In short,” Tanner writes, “the program is facing insolvency without fundamental reform.”



 **Drug Violence Flaring in Mexico**  
In December 2006, President Felipe CalderÃ³n of Mexico launched a military‐​led offensive against his country’s increasingly violent narcotics trade. In **“Undermining Mexico’s Dangerous Drug Cartels”** (Policy Analysis no. 688), Cato senior fellow Ted Galen Carpenter argues that this campaign is not simply ineffective: “It is a futile, utopian crusade that has produced an array of ugly, bloody side effects,” he writes. Many are now questioning whether Mexico is on its way to becoming a “failed state.” While Carpenter determines that these fears are overblown, he nevertheless acknowledges that “the overall trend is troubling.” By the same token, he notes that the extent of a spillover of violence and corruption into the United States has been limited — yet the possibility of turf battles becoming proxy wars is “a harbinger of deterioration of the security situation on our southern border.” By examining several alternatives to the current approach, Carpenter finds that one stands out above the rest. “The most feasible and effective strategy to counter the mounting turmoil in Mexico is to drastically reduce the potential revenue flows to the trafficking organizations,” he writes. This hinges on abandoning the prohibitionist model in favor of full legalization. “The fire of drug‐​related violence is flaring to an alarming extent in Mexico,” he concludes. Restricting the damage will require swift action, “before that fire consumes our neighbor’s home and threatens our own.”



 **War and the Practice of Politics**  
Article I of the United States Constitution vests the power to “declare War” in Congress, leaving to the executive the power to “repel sudden attacks.” But in the years since the Cold War, the practice of initiating limited conflicts has blurred these constitutional distinctions. In **“Congress Surrenders the War Powers: Libya, the United Nations, and the Constitution”** (Policy Analysis no. 687), John Samples, director of Cato’s Center for Representative Government, examines a number of smaller wars — namely, those in Bosnia, Somalia, Kosovo, Iraq, and most recently, Libya — and reaches several conclusions. First, the president has arrogated “largely unfettered powers” to launch wars that are “half‐​made” — conflicts he feels “are essential to fight and yet beyond constitutional propriety.” Second, while sometimes critical, Congress tends to defer to presidential command when these wars are both brief and popular. Third, Congress’s active “investigations and criticisms can affect the conduct of a limited war but not its inception.” On the other hand, while the public is often skeptical that limited conflicts are worth the cost, their “desire for congressional authorization of such wars goes unfulfilled.” Finally, Samples finds that, in Libya in particular, an incremental transfer of these powers to international institutions — also known as weak internationalism — ” contravenes values central to American republicanism.” As such, he concludes, “law becomes over time a function of, not a constraint on, the practice of politics.”



 **The Ivory Tower’s Burden**  
“If you follow higher education — or just live near a college or university — you’ve probably heard the complaint: government keeps axing higher education funding,” writes Neal McCluskey, associate director of Cato’s Center for Educational Freedom, in **“How Much Ivory Does This Tower Need? What We Spend on, and Get from, Higher Education”** (Policy Analysis no. 686). The problem is that there is little evidence to support this claim. While most analysts rely on public funding as a share of overall school revenues, McCluskey examines the burden of postsecondary education borne by taxpayers — the most direct measure of public support — and one that is “typically ignored in anecdote‐​driven media stories.” What do these numbers suggest? “No matter how you slice it, the burden of funding the Ivory Tower has grown heavier on the backs of taxpaying citizens,” he writes. In fact, the burden on the individual taxpayer has risen from $426 in 1995 to $532 in 2010, a 25‐​percent increase. But this is only part of the higher‐​education story. The real question is whether human capital has expanded along with this increased investment. McCluskey finds that the increased flow of dollars has “underwritten poor academic results, rampant price inflation, and considerable college inefficiencies.” “The money taken from taxpayers,” he concludes, “to ‘invest’ in higher education has been on the rise, and it appears to be hurting both taxpayers individually and society as a whole.”



 **Malpractice Caps Hurt Patients**  
Supporters of capping court awards for medical malpractice argue that such caps will make health care more affordable. But is this necessarily the case? In **“Could Mandatory Caps on Medical Malpractice Damages Harm Consumers?”** (Policy Analysis no. 685), economist Shirley Svorny of California State University, an adjunct scholar at the Cato Institute, says that it may not be so simple. In reviewing the structure of the medical liability insurance industry, Svorny begins by offering a key insight. “The decades‐​old conventional wisdom holds that medical malpractice insurers rarely adjust premiums to reflect an individual physician’s risk,” she writes. This assumption, however, is misplaced. As Svorny illustrates, the industry has developed a complex, “interdependent system of physician evaluation, penalties, and oversight” — all of which is based upon the threat of legal liability for negligence. Patients, in turn, derive protections from this oversight. In short, she writes, “the evidence presented here shows that physicians pay the price for putting patients at risk.” Svorny draws on interviews with underwriters and brokers, published sources, and an extensive analysis of state insurance company rate filings to make her case — showing that premiums “act as signals that steer physicians toward higher‐​quality care.” As such, the implication is clear. “Capping court awards, all else equal, will reduce the resources allocated to medical professional liability underwriting and oversight,” she argues, “and make many patients worse off.” The study generated a lively online discussion at the Manhattan Institute’s PointofLaw website.



 **Lessons from Deepwater Horizon**  
On April 20, 2010, an explosion on the Deepwater Horizon offshore drilling unit led to the largest accidental oil spill in the history of the petroleum industry. What lessons have emerged in the year since the well has been declared “effectively dead”? Richard L. Gordon, professor emeritus of mineral economics at Pennsylvania State University and an adjunct scholar at the Cato Institute, argues in **“The Gulf Oil Spill: Lessons for Public Policy”** (Policy Analysis no. 684) that the resulting political backlash uncovers longstanding issues with the attempt to regulate commercial activities. “The underlying problem is a mythology that holds that public lands are precious resources needing careful government management,” he writes. This isn’t the case. By examining the political response — particularly the Waxman‐​Markey bill — he underscores the real issue. “The failure was in fact due to the impotence of the very policy initiatives that the Obama administration wishes to expand,” he writes. Gordon carefully deconstructs the “tangential campaigns” against foreign oil imports, oil consumption, and climate change — making it clear that “the only thing these concerns have in common is their invalidity.” The ideal solution, he contends, is privatization of federal lands. In the interim, Gordon demonstrates that the Gulf oil spill reflects the problems associated not only with our command‐​andcontrol energy strategy, but with government oversight in general. “The real lesson of the oil spill,” he concludes, “is the familiar point that bad policies beget bad consequences.”
"
"A mystery bidder recently paid US$50,000 to name Dallas zoo’s latest baby giraffe.  As we humans know, your name significantly affects your life.  It has been shown to influence your career choice, a phenomenon called nominative determinism (think of Sarah Blizzard the weather presenter), or whether you will get that job interview in the first place.  Because without even meeting you, people form opinions about you based on your name.  My wife likes to remind me she married me because she will be forever Young. We can’t yet conclusively say whether other species of animals have names. Of course, many can be identified by sight or smell, but these are not something the animal actively emits to announce its presence. Until now dolphins are the only other known species with an auditory label. They have what we call signature whistles, which they use to identify themselves and seem to function to co-ordinate the movement of their group.   It’s easy to see why zoos name their attractions – celebrity animals such as Jumbo the elephant who lived in London zoo in the 1860s have become legend. But what of us scientists, who are supposed to ignore such matters in favour of objective reality, should we name our subjects? In most experiments, scientists must identify their subjects as individuals.  Technically, we should identify our animals in a neutral manner; for example, by using numbers to avoid inducing bias into our experiments.  But what about lucky number 7, jinxed number 13, or 666 the number of the beast? Zoologists studying mammals in the field or in captivity may have numbers for their subjects but they often have “pet” names for their animals. It would seem humans cannot resist giving names to certain types of animals.   Over the years I have set up or been involved with a number of research projects where we have had to identify our animals. I have always started out with the good intention of using neutral identifiers, but my postgraduate students/fieldworkers – usually the ones who really spend time with the animals – are the name-givers. Mammals are by far the most frequently named group of animals.  Others just tend to get stuck with a number unless the species is particularly large or an individual has a notable trait such as One-Eye the fish. Primates are virtually always given names. Some wild primates have become famous and it is hard to imagine this could happen if they did not have a name.  Anyone who is fascinated by chimpanzees knows of Flo studied by Jane Goodall who had her obituary published in the Sunday Times.  Another of Goodall’s subjects was called Satan and she was criticised for this name, as it could bias her interpretation of his behaviour. Many primatologists name their animals using the first letter in a name to indicate relatedness.  Thus, my first habituated to humans group of titi monkeys was known as group D and contained Desbotado (faded), Diana and Diego.  When I asked my postgraduate students why they chose the name Diana, not a common name in Portuguese, they said, “because she looks like a princess”. I always tell my postgrads to savour their field time because all too quickly they could end up stuck at a desk like me.  However, my situation is lightened by my primatology students with their stories from the field, which have an almost soap opera-like quality to them – of who did what to whom.  And of course without names none of this would make sense. Giving an animal a name helps people to relate to them.   But the danger of anthropomorphism – ascribing human qualities to animals – rears its ugly head. Can we objectively evaluate the behaviour of a monkey that we think looks like a princess? Or a naughty individual we have affectionately called Asbo? Animal subjects are frequently named by human observers in honour of their academic mentors. Swinging around the rainforests of Brazil is a northern muriqui called Robert Young.  Such anthropomorphising of certain species is difficult to stop. From a scientific perspective we need to be aware of it and take steps to eliminate any biases which it might induce in our research.   For example, we could have one set of people who record the behaviour of the animals with a video camera and a second set of people, who do not know the animals, who collect their behaviour from the video.  This is what we scientists call a blind experimental design. Naming your experimental subjects is a double-edged sword: it seems to spark fascination in researchers but it may cloud their judgement of their subjects’ behaviour."
"
Recently I had some of my readers comment that they thought that The Weather Channel and USA Today (which uses TWC graphics) temperature maps seemed to look “hotter”. They suspected that the colors had changed. I tend to watch such things since my own company (IntelliWeather) produces similar maps.
I searched Google images for some saved older TWC maps, but found none. So I can’t be absolutely sure they have or have not changed.  But looking at the color scheme, nothing sticks out in my recollection of the temperature map colors.
But I decided that it would be an interesting exercise to compare USA national temperature maps from the commonly used services today. I saved national CURRENT temperature isotherms/gradient maps from around 03Z (11PM Eastern Time) tonight. All were generated within about an hour of each other.
What I found was surprising. Here they are in alphabetical order:
Intellicast: (probably the ugliest national temp map I’ve ever seen)



IntelliWeather:

NOAA-NWS:

Unisys:

Weather Central:

Weather Channel:

WeatherForYou:


Weather Underground:

A couple of notes on the graphics: The Weather Channel does not show their color key, nor does IntelliCast. From experience it appears the with the exception of the IntelliWeather map, all maps have fixed color schemes. The IntelliWeather map uses a sliding scale of color based on the max and min temps presented in the data. Also, I tried to include AccuWeather, but could not locate a current national temperature map from that company. They had everything else but that.
UPDATE: I decided that even though AccuWeather did not have a CURRENT temperature map, the color and color key on their HIGH TEMPERATURE FORECAST map would suffice for this comparison, since it a similar range of temperatures presented, from (50’s to 90’s) so here it is:

Note the color scale and where the perceived “cooler” colors start on the AccuWeather map.
So what do you think?
Is it just me or does there appear to be a warm bias in the color temperature presentation of the majority of providers shown here? Just an FYI, I designed my color scheme for the IntelliWeather Map in 2001, well before I started blogging, so please no suggestions that I skewed this comparison with my own map color scheme.
Along those lines, I’ll point out that the color choices are usually done either by a meteorologist, or a graphic artist/programmer or both. Usually the color scheme is the result of the input from a couple people. In my case, myself and my graphic artist made the choice. In places like TWC or AccuWeather, the choice may be made initially by one or two then approved by a larger group.
The point I’m trying to make is that each map represents the color and temperature perception of the presenting organization, as I don’t know of any “standard” for map colors used for air temperature presentation. Having said that, somebody will probably put one in front of me that I’ve never known about. 😉


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9e8352aa',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"As all good Monty Python fans know, water technologies feature large in the legacy of benefits left by Roman civilisation. But while aqueducts, sewers and baths retain an obvious presence in the landscape and in the archaeological record, the Romans’ largest and most important water achievement may have been “virtual”. The Romans developed networks of trade and food supply that enabled them to escape local water constraints, in a way that is explained in a new study in the journal Hydrology and Earth System Sciences. Fertile regions such as southern Spain or Italy’s Po valley would grow lots of food and ship it back to Rome or to the drier outposts of the Empire.  Embedded within this is a what geographers call a virtual water trade – an indirect way of shifting this precious resource from wetter, less populated areas to those regions with more people or a less consistent climate. The map below shows this in action. The amount of virtual water imports (a) and exports (b) in different parts of the Empire are illustrated by the size of the circles. The numbers express this in tonnes of grain. Rome is by far the largest water importer, followed by Alexandria and Memphis in Egypt, and Ephesus and Antioch in modern-day Turkey. Spain and Egypt were the biggest exporters. All ships lead to Rome The paper’s primary author, Brian Dermody at the University of Utrecht, suggests this sophisticated water economy ultimately contributed to its own downfall as it enabled urban populations to boom beyond sustainable levels.  Does this sound uncomfortably familiar? In the next 30 years we are facing a critical combination of inter-related stresses on the core resources that keep our civilisation running. As it happens, the Romans gave us a word for that too – the “food-water-energy nexus” (from the Latin nectere, to bind together).  So are we doomed to the same fate as the Romans? As its name suggests, the nexus recognises that different resources are intimately interconnected. We need water for drinking, washing and for industry; but we also need it to grow food, and around 70% of global fresh water supply is used for farming. As populations grow and become more wealthy, demand for food will increase, placing pressure on domestic water supply and industrial output. Economic growth and technological developments increase energy use, driving additional demand for water in power station cooling and other uses in energy generation. The rise in shale gas extraction provides a stark illustration of this: irrespective of the many other ethical and political issues surrounding fracking, it is its thirst for water (used to force the hydrocarbons out of the rocks) that may prove the key limitation. After all, 38% of the world’s shale gas resources are located in areas of extremely high water stress or arid conditions. In the UK, plans for fracking in major regions such as the Severn catchment could place untenable pressure on water use for farming and domestic supply. In all this complexity, the mega-issue of climate change arguably plays only an aggravating role. Intensive farming is degrading soil, its primary resource base, up to 100 times faster than the rates at which it is formed. Non-renewable fuel and mineral resources are becoming increasingly scarce and more costly to recover. And renewable, drinkable water supplies are under often extreme threat.  Solving climate change will not in itself solve the problems of the food-water-energy nexus; in fact it should be apparent that our effective response to climate change is deeply entwined with a sustainable untangling of the nexus. Like the Romans, the “modern” response to the emergent limitations of the food-water-energy nexus was economic. Global trade through the 20th century allowed us to circumvent local or regional resource limitations, stimulating unprecedented population growth along with rising wealth and living standards.  Many countries could not hope to maintain their current consumption of food and resources if they were forced become self-reliant on resources available within their territory – in the current economic and technological conditions, at least.  This makes the global economy vulnerable to regional problems. Look at this year’s escalation of tension between Russia and the West, for instance. Sanctions imposed by both sides have affected international trade in wheat and other crops, leading to supply shortages or gluts in some places and the destabilisation of farming economies and farmer incomes in others – and has raised the threat of disruption to transnational energy supplies. Again, the challenges of the nexus – and our vulnerability to those changes – transcend the background threat of climate change. So, faced with challenges which appear strikingly similar, what can our postmodern, self-aware civilisation do to avoid the fate of the Romans? We cannot stop the nexus any more than we can prevent the climate change that will result from our current levels of greenhouse gas emission. Business as usual is clearly not an option. In the absence of a magic bullet (or something much worse, an environmental disaster or collapse), resilience is the key.  One advantage we have over the Romans is information; we can learn from precedent. We can see what is over the horizon and make a judgement on how it may impact our lives and livelihoods. The challenge, unfortunately, remains how to stimulate people and politicians to change in response to those dangers. However human nature means we are as ready to listen to soothsayers as scientists and, in that respect, we and the Romans are still very much the same."
"

Click image for a live interactive view of the National Climatic Data Center in Asheville, NC
Today started off terrible. I slipped in the bathtub last night at the hotel, and strained a back muscle and was so sore that just getting dressed and into the car was a chore. As a result, I was late getting to NCDC this morning. I’ve been popping Aleves today. Fortunately, they had slack built in so the day got started cheerfully with a review of the new Climate Reference Network with the principal scientists. It was a super meeting and I took many notes, I’ll have much to share later.
Next came a briefing on “Climate Science” from Tom Peterson, but I’m afraid I stole his thunder a little bit when I announced that I had already seen his presentation, which included an analysis of the Marysville USHCN Station.  See the powerpoint he presented here:aapg-san-antonio-peterson
Then came a personal tour of the Asheville CRN station by Dr. Bruce Baker. In addition to taking visible light photos, I also took matching IR photos from many angles. Bruce and his team were quite impressed with the IR camera I use, and he says he plans to buy a couple in use for siting surveys. He also plans to post the IR photos I took today on the CRN site to show how well the design and siting is free of IR influences.

I’ll have much more on all of this but I still have 8 more stations to survey plus an unexpected customer detour service call Friday to WDNN-TV in Dalton, GA which has some trouble with our weather display system there. So stay tuned for more details on the visit and questions that were asked and answered.
But the big news came with Dr. Baker providing me with a press release (new today) to post here for you all to see. CRN is getting completed and USHCN modernization is starting:
NOAA today announced it will install the last nine of the 114 stations as part of its new, high-tech climate monitoring network. The stations track national average changes in temperature and precipitation trends. The U.S. Climate Reference Network (CRN) is on schedule to activate these final stations by the end of the summer.
NOAA also is modernizing 1,000 stations in the Historical Climatology Network (HCN), a regional system of ground-based observing sites that collect climate, weather and water measurements. NOAA’s goal is to have both networks work in tandem to feed consistently accurate, high-quality data to scientists studying climate trends.
See the full press release here:
 press_release_042408_climatereferencenetwork
What this means: No more adjusted data, the raw data from CRN and from HCN-M is the real data and will be pristine, assuming the network is maintained. No more torturous gyrations of FILNET, SHAP, and TOBS. The downside is that a track record needs to be built up, the older data is also going to be revised with USHCN2 algorithms soon, and I’ll touch on that later.
One thing that Debra Braun said to me today in the meeting hit home: “our funding had been cut for the last two years, and we were unable to move forward until this year”. This made me think that perhaps some of the focus the surfacestations.org project brought to illuminating the deplorable condition of the network may have helped a little bit in convincing some legislators that it was time to get serious about allocating funding to complete the CRN and fix the USHCN. A little public embarrassment of the USHCN provided by all of us that have contributed to surfacestations.org may have helped. I’d sure like to think so.
I want to extend my heartfelt thanks to Dr. Baker, Debra Braun, Grant Goodge, and the entire CRN science team, plus Jeff Arnfield, and Steven Del Greco for answering all my questions and taking such careful time with me. Additionally I wish to thank Dr. Karl, and Assistant Director Sharon LeDuc for hearing my concerns  and offering ideas.
Everyone there at NCDC made me feel welcome and appreciated.
Most importantly, I want to thank you, my loyal readers and volunteers, because without your help, the trip and presentation I made would not be possible.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9fd87aba',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The recent confidence in shale gas was likely premature, according to several new reports published in the US. In particular a study from the University of Texas claims the US boom will tail off by 2020 and not keep going to 2040 as previous less thorough analyses have predicted. To anyone who has been closely following the industry in recent years, this difference in predictions will not be surprising, of course.  In 2013 the US Energy Information Administration (EIA) already noted that Norway’s assessment of its shale gas potential went from 83 trillion cubic feet (tcf) (2011) to zero (2013) due to results obtained from test wells in the alum shale, and how Poland’s estimates went from 44tcf to 9tcf due to stricter application of requirements for successful shale formations. But at that time the EIA did not comment as strongly or publicly on similar concerns about the accuracy of the US shale data.  Likewise concern about overestimates of shale potential is becoming louder in Britain, which is at a much earlier stage in terms of shale gas exploration but has a similarly enthusiastic government. Last month scientists from the UK Energy Research Council suggested that promises by ministers about greater energy security and lower energy prices through shale gas were premature and unlikely to be deliverable. Additionally there are concerns over whether investment in shale gas is still profitable, while numerous potentially costly environmental concerns have not yet been dispelled either.  What then are the reasons for these unreliable calculations? And why do governments promote shale gas with such conviction when it is surrounded by such uncertainty? One possible factor behind inaccurate judgement of shale gas potential is that both official organisations like the EIA and industry specialists rarely release the data behind their forecasts.  The terminology is also surely to blame: there are resources, and then there are reserves. While this is clear to experts, the distinction is not made consistently in the media. This prompted a recent note by the UK Parliamentary Office for Science and Technology urging the government to address the variance.  Experts distinguish between total resources, potentially/technically recoverable resources, and reserves. The reserves is the amount that is economically recoverable, and is normally a fraction of the total resources. Even then, the North American experience demonstrates that well productivity is highly variable.  When UK prime minister David Cameron announced that Britain was to go “all out for shale,” there was no more detailed information available for the UK than the size of the total resource. Reserves could still be anywhere between substantial and zero.  One cannot help but be reminded of the nuclear energy discussion from the 1980s. Less than a month before the Chernobyl accident, The Economist described the technology as being “as safe as a chocolate factory” – and has since castigated itself for its remark. Today, shale gas is lauded to be both an economical solution and environmentally amenable –- before there is ample evidence for either claim.  Given election cycles, politics is naturally drawn to short-term solutions. The fact that shale has brought US gas prices to lower levels than were probably imaginable a decade ago makes it attractive to this mindset. Lobbying interests will be part of the picture too – it does not surprise that countries with major petroleum companies would push the shale gas/oil agenda.  The idea of tapping into a new petroleum resource also sits well with the underlying and largely uncontested objective of faster and continued economic growth. Equally, the idea of technological innovation fixing future problems and therefore allowing the status quo to continue is too good to pass up.  There are many things about alternatives such as renewables or conserving energy that are less comfortable or popular. When you read recent UK energy strategy White Papers, despite much talk of climate change prevention and a need for renewable energy, growth and revenue are more linked to fossil fuels and the traditional energy industry. Shale gas fits the bill.  I don’t categorically deny the possibility of continued revenue and profitability in the shale gas business. My point is that it should be treated as just that: a possibility, not a promise. In an era where the need for more transparency is a statement that appears in nearly all policy proposals, energy politics should be no different. What is needed is clear and factual communication between experts, policymakers and the public about the opportunities and drawbacks. Environmental costs should also be part of those considerations. As these latest US reports remind us, this has to include an accurate account of what is known about it and what uncertainties remain."
"
Share this...FacebookTwitter      While world governments bedwet over a fantasized climate catastrophe taking place 100 years out, mankind could be facing a potential catastrophic food shortage. A worthhile read (see link below).
A fungus threatens 20% of the world's food supply.
      The disease is Ug99, a virulent strain of black stem rust fungus (Puccinia graminis), discovered in Uganda in 1999, threatens the world’s wheat supply. Read the scary details here: http://www.wired.com/magazine/2010/02/ff_ug99_fungus/all/1
      Wheat provides 20% of all calories consumed by humans. According to Nobel laureate Norman Borlaug, father of the Green Revolution:
This thing has immense potential for social and human destruction.
      According to wired.com, the fungus attacks the stem of the wheat plant, causing it to wither and die.
Stem rust is the polio of agriculture, a plague that was brought under control nearly half a century ago as part of the celebrated Green Revolution. After years of trial and error, scientists managed to breed wheat that contained genes capable of repelling the assaults of Puccinia graminis, the formal name of the fungus.
But now it’s clear: The triumph didn’t last.
      The new fungus has spread from Africa and into the Middle East. It would only take a  traveller with a single spore on his shirt to transport it to the USA and Canada.
The pathogen makes its presence known to humans through crimson pustules on the plant’s stems and leaves. When those pustules burst, millions of spores flare out in search of fresh hosts.
      It goes to show that nature has a bag full of nasty tricks, and there’s nothing you can do to stop her. All you can do is adapt, hopefully quickly enough. But if you waste your time trying to appease her, and don’t invest your resources wisely in adapting, you’ll get eliminated.
Share this...FacebookTwitter "
"It’s early January and freezing cold in New York when I meet Jenny Offill to talk about her new novel, Weather – an innocuous title for something that feels less innocuous every day. A couple of weeks earlier, the temperature was warm and spring-like. These fluctuations in the weather, and the warming trends they reveal, are increasingly unsettling reminders of the climate crisis, and they form the backbone of Offill’s latest novel, the follow-up to 2014’s bestselling Dept. of Speculation. Weather follows Lizzie, a university librarian, who responds to the emails sent in to “Hell or High Water”, a climate-focused podcast hosted by her former academic mentor. The job opens Lizzie’s eyes to the crisis and the myriad ways different people respond to it, from “dreary” environmentalists obsessed with composting toilets to “end-timers” eager to embrace the Rapture. Amid a growing sense of her own responsibility to the planet and fear for the future, Lizzie struggles to balance her responsibilities as a wife, mother, sister, daughter and friend.  “I became interested in why I wasn’t more interested,” Offill says, considering the question of why she chose to focus a novel around a subject many people find too vast and frightening to contemplate. In other words, she was curious how it was possible to be intellectually aware of an unfolding disaster without feeling emotionally connected or moved to action. The novel follows Lizzie as she moves from “a state of twilight knowing” to a more conscious awareness of the crisis. At its core, the story asks: what happens after we start to pay attention? When writers and artists have tackled the climate crisis, they’ve tended to come at it obliquely, through imagined dystopias. Even the “zombie apocalypse” that Offill says crops up on doomsday-prepper websites is somehow more comprehensible than the ravages of climate breakdown. “‘Apocalypse’ is one of those words, like ‘fascism’, that immediately feels like an overstatement,” Offill says. “But it was hard to see those pictures coming out of Australia in the last few weeks, or hear that a billion animals may have died, and not feel like that is what’s happening.” Offill’s novel might best be called “pre-apocalyptic”. There are no large-scale emergencies, no heroic survivors; there is no overt horror. Instead, the novel belongs to the everyday world of contemporary gentrifying Brooklyn. As with Dept. of Speculation, the narrative is crafted from short, resonant passages, rarely longer than three or four sentences, that read like diary jottings, conversation fragments, jokes, quotations, trivia or poetry. Their lightness is deceptive. In the gaps between, there’s “the ambient dread of feeling something is coming or happening, but that you can’t understand its dimensions,” she says. That dread is increased in the wake of the 2016 election, which features, indirectly, in the novel’s plot. “I don’t think they’re unrelated,” Offill says, of climate anxieties and Trumpian politics. “I think the spectre of climate change is leading to some of this tightening of us-against-them, lifeboat-ethics feelings.” Strikingly, it’s the election that elicits a grim, panicked response from Lizzie’s husband, Ben, who has been unmoved by her climate worries; he wonders, fleetingly, if they ought to get a gun. Offill is fascinated by the way that these distances can open up, suddenly or slowly, between people within the intimate confines of a family. In Dept. of Speculation, the protagonist finds herself isolated by the intense experience of early motherhood, depression, and the struggle to complete her second novel. The contours of her life reflect Offill’s own, also a married writer with one daughter who has supported herself by teaching and ghostwriting, and who took 15 years to publish the follow-up to her 1999 debut novel, Last Things. But she has resisted the “autofiction” label, which she thinks is applied too readily to women writers, minimising the craft that goes into their fiction. In the course of writing Dept. of Speculation, Offill jettisoned another, more conventionally structured novel, experimented with writing poetry, taught fiction, and eventually hit on the style, stripped to the bone, that would make the novel so powerful – so “joyously demanding”, as Roxane Gay put it in her review for the New York Times. Despite Offill’s belief that the book’s fragmented form would mainly excite other writers – Ocean Vuong, Sheila Heti and Jia Tolentino are among her fans – it found an unexpectedly wide audience, drawn to its offbeat yet honest depiction of the clash between motherhood and creativity. Dept. of Speculation’s unnamed protagonist was a writer who declared her intention to become an “art monster”, the kind of artist who makes a ruthless priority of her creative life. Yet she finds it impossible. The domestic and emotional worlds of motherhood and marriage – not to mention the pressure to make a living – exert a far stronger pull than she anticipated. That character and Lizzie are not the same person, Offill stresses, but she sees a continuity between them in the struggle to balance their domestic roles with their larger ambitions. “To me, the through-line is caretaking,” she says – the work women do to care for children and other family members, often without pay or even the recognition that it is work at all, leading to a kind of burnout that makes it difficult to pay attention to the wider world. “That exhaustion was something I wanted to channel through Lizzie.” It’s no accident that Sylvia, the host of the podcast that draws Lizzie towards an awareness of the climate emergency, is a superstar academic with no children. Her approach to the crisis is shaped by her social class and the freedom of movement it brings. When Lizzie accompanies Sylvia to a conference in Silicon Valley, the well-heeled attendees want to know where the safest place will be to escape the apocalypse – not for themselves, they insist, but for their children. Of course, as Lizzie is well aware, most people don’t have the freedom to choose where to go if their family is in danger. “There’s a lot of people in the world right now who have terrifyingly little control over how to keep their children safe,” Offill observes. When Lizzie worries about the future of her young son, Sylvia’s only advice is to become really rich. Lizzie herself is neither a climate refugee nor a tech titan, but rather “smack in the middle”, Offill says. She has enough support and security to be able to think about the world beyond her immediate situation, but not enough to protect everyone she loves. And like many women entering middle age, her responsibilities keep extending beyond her husband and son to her brother, a recovering addict, her religious mother, and her brother’s wife and baby. She gives too much of herself to them, she chides herself, listening to their problems and offering her advice for hours on end (her husband drily, or sourly, points out that if only she were a real shrink, they’d be rich). Her impoverished mother drives around town giving socks to the homeless, which gives Offill the opportunity to show us a different kind of caring and social responsibility at work. “They’ve done all these studies to show that, proportionally, the people who make the least money give the most to charity,” she points out. “I think what Bezos just gave to charity” – he announced that Amazon would donate $690,000 (£527,000) to the Australian wildfire relief fund – “was something like the equivalent of if I gave one 18th of a penny in terms of proportion of my income.” These bits of information pepper Offill’s conversation as they do the book, connecting ideas and hinting at the research underpinning its allusive style. But while the studies she consulted provided useful reference material, Offill says that her reading began as a way to understand her own reaction to the climate crisis, or lack of it. One book that she cites as formative is criminologist Stanley Cohen’s States of Denial: Knowing About Atrocities and Suffering (“so, big sell at a party”). It was in Cohen’s work that she found the idea of “twilight knowing”, a state hovering between denial and knowledge, as a name for where Lizzie finds herself at the beginning of the book. Her reading widened to climate science, mythology, psychology and sociology, in an effort to understand how people have responded to disasters at different points in history. To confront a looming apocalypse through reading, she admits, was “deeply silly, and also the only thing I knew to do”. By making her protagonist a university librarian, Offill was able to create a character who shared her own instinct to look for answers in books, and to indulge in imagining “a little bit of an unlived life”, she says. Yet Lizzie’s job involves far more human than literary interaction, with an array of lost souls including a pale adjunct professor (“I worry he is selling his plasma again,” Lizzie says) and an eternal graduate student. Offill, who has taught at several different colleges, is familiar with this highly educated but precarious class, who have “social capital and not so much other capital”. It certainly doesn’t escape Lizzie’s notice that her skills and knowledge would be of vanishingly little practical use in a doomsday scenario. Yet libraries also represent, for Offill, a bastion of hope. “As the social fabric of our society has continued to unravel, libraries have become one of the last places where you don’t have to buy anything and yet you are welcome,” she says. “It’s like a tiny little vision of utopia.” She mentions the current interest in experimenting with libraries of tools and household appliances as evidence of this utopian instinct at work. It’s surprising, given the subject matter, how much fun Weather is, both to read and discuss, and also how darkly funny. “I’m always trying to figure out when you can puncture some of the self-righteousness,” Offill says. Even where it seems warranted, relentless seriousness about serious subjects can keep people from facing and fighting them. “They feel undone by the earnestness, by the sense that you have to have your own house completely together before you’re able to join a bigger movement.” Lizzie’s tone is often earnest, sometimes self-righteous, but usually a blend of deadpan and terrified.  One joke is something called the “obligatory note of hope”. In the story, it’s a sardonic label for what Sylvia feels she has to include in every article and speech about the climate crisis, if she wants to keep her audience. After the end of the book, the phrase reappears as a URL, which Offill is excited to explain will direct readers to a website that reaches beyond the novel to offer some pathways to activism (right now, it leads to a landing page with a picture of a goat in an abandoned library). The site highlights the work of three environment-focused organisations – the Sunrise Movement, the Transition movement (which emphasises grassroots local initiatives) and Extinction Rebellion – as well as sharing stories of people who have worked for change. A further section, “Tips for Trying Times”, sounds the most distinctively Offill-like, taking the research that didn’t make it into the book and turning it into a resource that is both practical and encouraging. It includes “what people did when their movements didn’t seem like they were making any headway” and “what does the Swedish government say to do if crisis or war comes?” Offill is adamant that the site is purely a resource, and doesn’t feature any links to buy her books. It’s simply an effort to connect with other people, to stick to a promise she made while writing Weather, that she would undertake more in-person activism. “I no longer felt like I could opt out. I no longer felt like it wasn’t my fight,” she says. That means moving far beyond the individualistic ambitions of the art monster, and the comforts of books and intellectual isolation. “Out of the library, into the streets. But, boy, is it a nerve-racking place to be!” • Weather by Jenny Offill is published by Granta (RRP £12.99). To order a copy go to guardianbookshop.com"
"

More indicators of a colder than normal winter continuing in the northern hemisphere.
From the London Telegraph:
Britain is enduring its most miserable Easter for 25 years as Arctic winds sweep in, bringing snow, hail and sleet.
Easter Sunday temperatures could drop to as low as -3C at night with a band of snow and sleet forecast to move down from the North. The bad weather is most likely to affect the Midlands but snow could even reach London, forecasters said.
From the Sofia news agency:
Bulgaria Meets Vernal Equinox With Snow, Sun Gleams
From This is London:
It’s Bad Friday: Britain braced for worst Easter weather in 25 years as country is battered by gales and sleet.
From the Stars and Stripes:
Snow hits Germany military bases with more possible for Easter.
From CTV.ca
‘Spring’ weather nasty for Eastern Canada
Also from CTV.ca
Six more weeks of winter, top weatherman forecasts
From KDKA-TV:
Snow Advisory In Effect For Parts Of Western Pa
From RedOrbit:
Nebraskans and Iowans heading east for the Easter weekend were experiencing flight delays or snow-covered roads today, and the troubles could continue into Saturday.
From the Detroit Free Press:
Heavy snow across Michigan and points west meant increasing cancellations and delays at Metro Airport today, with things getting worse as snow piled up.
From swissinfo.ch
The Easter break has started with heavy snowfall and strong winds in Switzerland, causing some disruption to traffic.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7ea08f2db6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
