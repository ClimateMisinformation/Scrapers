"

With friends like Jerome R. Corsi, the American free‐​enterprise system is in more trouble than we feared.



Corsi should be on the right side of the battle to defend economic freedom. Before the 2008 election, he wrote a popular book with a great title, _The Obama Nation_ , warning that Barack Obama was not the man we needed in the White House. But in his latest book, _America for Sale: Fighting the New World Order, Surviving a Global Depression, and Preserving USA Sovereignty_ , he sounds a lot more like Obama than like Ronald Reagan, the former president he professes to admire. 



While Corsi takes passing shots at government spending and climate‐​change legislation, his real target is “free trade,” “globalism,” and international agreements such as NAFTA and the World Trade Organization. The villains in his book are trade agreements, trade deficits, growing Chinese foreign‐​currency reserves, and oil imports, all of which threaten to undermine the U.S. dollar, American independence, and the middle class. Orchestrating our decline is a cast of characters from around the globe, both familiar and obscure, working in public and in secret.



Boosted by a friendly Fox News interview with Sean Hannity in October, the book has been selling reasonably well. The people buying it are presumably the same conservative‐​leaning folks who opposed Obama’s election and are now fueling the Tea Party movement. Libertarians and conservatives who value free markets and limited government, however, should keep their distance from this bestselling prophet.



Coming from an author who claims to be a friend of free enterprise, and who earned a Ph.D. in political science from Harvard in 1972, the book is striking in its economic ignorance. It would fail an economics course at a community college. It repeats just about every anti‐​trade cliché that has been uttered by the AFL-CIO, Public Citizen, and Sen. Bernie Sanders (Socialist, Vt.).



The only “invisible hand” to be found in this book is that of an international conspiracy to sell off our national assets, sell out our sovereignty, and abolish the dollar. The “globalists” behind this movement are mostly academics and former officials who don’t run in current circles of power, such as Peter Drucker, Benn Steil, Herbert Grubel, Nouriel Roubini, Robert Mundell, George Soros, Henry Kissinger, Zbigniew Brzezinski, and David Rockefeller. Of course, former officials and academics are putting forward all sorts of ideas every week, some sensible, others far‐​fetched. A string of quotes from secondary players doesn’t prove that an idea is about to be foisted upon us.



Of special interest to Corsi is the plan to construct a NAFTA transportation corridor from Texas through the nation’s midsection. To demonstrate that such a scheme exists, he simply describes various plans to improve infrastructure in North America to accommodate increased freight traffic caused by expanding trade. As evidence that something nefarious is going on, he quotes an official with the Canadian National Railroad, who lets slip that his company is “now positioned to provide shippers with a seamless door‐​to‐​door transportation solution and to ensure the safe and secure flow of goods throughout the North American continent.” Isn’t that exactly what transportation companies should be doing?



Corsi tries to tarnish NAFTA further with arguments that sound like they could have come from Dennis Kucinich. Corsi cites a recent study by the North American Center for Transborder Studies that claims that 40 million jobs were created in the United States, Canada, and Mexico between 1993 (the year NAFTA passed) and 2007. Expressing skepticism, he writes, “Typically, the study failed to articulate the methodology by which these job estimates were derived, nor did it indicate whether the job creation was a net 40 million, after taking into account the jobs lost from NAFTA or other free‐​trade agreements such as those under the World Trade Organization.”



There is nothing mysterious about the methodology. The figure comes from comparing the number of people employed in each country in 2007 to the number employed in 1993. Those numbers are readily available on the Internet from public sources. From 1993 to 2007, employment in the United States grew from 120 to 146 million, in Mexico from 31 to 42 million, and in Canada from 13 to 17 million. Simple subtraction will tell us that the net number of jobs added in the United States after NAFTA was 26 million, in Mexico 11 million, and in Canada 4 million, and simple addition will give us the total of 41 million.



Instead of performing basic arithmetic, Corsi cites the growth in America’s bilateral trade deficits with both Canada and Mexico since NAFTA. He uncritically swallows the formula of the labor‐​union Left that trade deficits by definition mean net job losses, even though this is clearly not the case. This faulty premise leads Corsi to conclude, “What these data suggest is that the net new jobs created under NAFTA in North America are likely being created in Mexico and Canada, not the United States.” In fact, as the real employment numbers show, the United States accounted for the large majority of the net new jobs created in North America since NAFTA.



Corsi often gets even simple facts wrong. Take, for example, this passage:



What is clear is that the United States has been losing manufacturing jobs steadily since the end of World War II. In 1945, at the conclusion of the war, the service industries accounted for only 10 percent of nonfarm employment, compared to 38 percent for manufacturing, according to the U.S. Bureau of Labor Statistics. The crossover point came in 1982, when for the first time services surpassed manufacturing as the largest employer among major industry groups. By 1996, services accounted for 29 percent of nonfarm employment, and manufacturing, at 15 percent, had reduced to being somewhat smaller than retail trade. By 2008, manufacturing was less than 10 percent of nonfarm employment, and service‐​producing employment had risen to approximately 84 percent.



The number of manufacturing jobs was actually rising until the late 1970s, from 15 million in 1960 to a peak of 20 million in 1979. It has indeed been generally declining since then, but not steadily. Net U.S. manufacturing employment actually rose by 700,000 in the first half decade after the passage of NAFTA before resuming its decline in 2000.



Also, if we accept Corsi’s numbers at face value, more than half the nonfarm U.S. labor force shifted to the service sector over the span of a dozen years (from 29 percent in 1996 to 84 percent in 2006) — which would be perhaps the most radical economic transformation in such a short period of any country in history. But according to BLS and Census Bureau data I cite in my own recent book, _Mad about Trade: Why Main Street America Should Embrace Globalization_ , a majority of Americans were working in the service sector by the end of the 1920s. In fact, there has never been a time in our history when manufacturing workers outnumbered service‐​sector workers, and thus there was no “crossover,” much less a revolution on the scale Corsi claims. 



Corsi repeats the union mantra that globalization has caused the loss of high‐​paying manufacturing jobs in exchange for low‐​paying service jobs. He states, without any citation, that globalism means “American workers must exchange manufacturing jobs paying in excess of $35 an hour for service jobs paying $10 to $15 an hour.” In fact, as I document in _Mad about Trade_ , two‐​thirds of the jobs our economy added in the past two decades have been in the service fields of health care, education, and business and professional services — all of which pay higher wages on average than does manufacturing. Despite the nostalgia for manufacturing work, the American middle class today earns its keep in the service sector.



The examples of sloppy scholarship just keep coming. Corsi repeatedly describes the recent economic downturn as “the U.S. recession that officially began in December 2008.” There is no definition of “recession” under which this is true. The National Bureau of Economic Research, the accepted authority on the U.S. business cycle, puts the data a year earlier; that is when employment and industrial output began to fall. 



To date the recession to December of 2008 — the month after Obama’s election — Corsi accepts the informal definition of a recession as two consecutive quarters of negative GDP growth, and then defines the beginning of a recession as the end of the second consecutive quarter. This makes no sense: Under this definition, whenever there are two and only two consecutive quarters of negative growth, a recession begins and ends on the same day. Using the real two‐​consecutive‐​quarters definition — placing the beginning of the recession at the point at which the negative growth started — the recession started in July 2008, not December.



The errors don’t end there. Corsi writes, “Between December 2008, the date the recession officially began, and February 2009, the U.S. lost approximately 4,384,000 jobs.” Actually, the BLS data show a loss of 2.1 million jobs during those three months. The U.S. economy also lost nearly 2.3 million net jobs in the year leading up to December 2008, which Corsi apparently just added in.



There’s more. Corsi writes that “China’s economy, heavily dependent on making cheap goods for the U.S. market, was cast into its own deep recession by the U.S. economic downturn.” In reality, China didn’t experience a recession, much less a deep one. According to the generally accepted figures from China’s National Bureau of Statistics, its economy grew 12 percent in 2007, 9 percent in 2008, and at an annual rate of 7.6 percent through the first three quarters of 2009.



He writes that our soaring trade deficit with China “reflects imports from China growing nearly 250 percent, from $100.1 [b]illion in 2000 to $243.5 [billion] in 2005.” Actually, while the latter number is about 250 percent of the former, the former only grew about 150 percent. He outdoes himself on page 182, declaring that “the U.S. negative trade balance with China in 1985 was under $1 billion; in 2008, the U.S. negative trade balance with China had grown more than 250 percent, to a negative $266 billion.” An increase from 1 to 266 would be an increase not of 266 percent, but of 26,500 percent (or 266 _times_ ).



Granted, many math‐​challenged adults and journalists struggle with percentages, but then again, those same adults do not pose as experts qualified to write books on global trade and finance. And typos and random errors creep into many books, but serious books by serious authors do not contain such widespread, obvious, and systematically biased errors as those teeming in America for Sale.



 _America for Sale_ is not a complete loss. In a chapter titled “The Mortgage Bubble Bursts,” Corsi names many of the right names: a Federal Reserve Board that kept rates too low for too long, the Community Reinvestment Act, Fannie Mae and Freddie Mac, and the abuses of many subprime‐​mortgage lenders. But even this section is not original; other authors have ably made the same case, including my Cato colleague Johan Norberg in his recent book _Financial Fiasco_. And nothing in this section implicates globalism, free trade, China, or the WTO as a cause of the recession.



In the following chapter, he argues, quite reasonably, that Americans could reduce their reliance on imported oil by more aggressively developing domestic sources. He expresses skepticism toward renewable and other alternative energy sources, and describes in some detail potential domestic gas and oil fields that could yield significant amounts of energy. But he never attempts to analyze what a dramatic ramping up of domestic oil and gas production would mean for oil prices, the dollar, and our living standards. For example, a sharp drop in oil imports would mean fewer U.S. dollars flowing into international exchange markets, a stronger dollar, and relatively fewer exports of U.S. manufactured or agricultural goods.



Corsi invokes the name of Ronald Reagan, but his book could not be farther from the spirit of Reagan when it comes to our economic engagement in the world and our future as a nation. As president, Reagan embraced the idea of a North American free‐​trade area and moved it closer to reality by signing a free‐​trade agreement with Canada in 1988. As far back as 1980, Reagan talked about joining the United States with Mexico and other countries of Latin America in a hemispheric free‐​trade zone. Reagan’s able U.S. trade representative, Clayton Yeutter, was instrumental in launching the Uruguay Round in 1986, which led to the founding of the World Trade Organization in 1995. In his farewell address, Reagan shared his vision of America as a shining city on a hill “teeming with people of all kinds living in harmony and peace; a city with free ports that hummed with commerce and creativity. And if there had to be city walls, the walls had doors and the doors were open to anyone with the will and heart to get here.”



If Jerome R. Corsi is now the conservative defender of free enterprise and the America Way, we are in deeper trouble than the Gipper ever imagined.
"
"

Given the recent claims that hurricanes are getting dramatically worse because of global warming, it’s too bad we’ve already exhausted the letter “G” for this hurricane season. “Gasbag” would have been a pretty good moniker for the next storm. 



In case you’ve missed the hype, MIT’s Kerry Emanuel has a paper in the online version of _Nature_ magazine saying that hurricanes are becoming dramatically more powerful as a result of global warming. 



Merely venturing into the discussion of hurricanes and global warming is more dangerous than most tropical cyclones. About Emanuel’s article, William Gray of Colorado State University — the guy who issues the annual hurricane forecast that grabs headlines every summer — told the _Boston Globe,_ “It’s a terrible paper, one of the worst I’ve ever looked at.” 



There’s also nastiness if you say hurricanes aren’t getting worse. A month ago, University of Colorado’s Roger Pielke, Jr., posted a paper that was accepted in the _Bulletin of The American Meteorological Society_ concluding there is little if any sign of global warming in hurricane patterns. In a pre‐​emptive strike, Kevin Trenberth from the federally funded National Center for Atmospheric Research in Boulder, Colorado, told the local newspaper, “I think he [Pielke] should withdraw his article. This is a shameful article.” 



Six months earlier, Christopher Landsea of the National Hurricane Research Laboratory, another federal entity, quit the United Nations’ Intergovernmental Panel on Climate Change. Landsea is probably the world’s most respected hurricane scientist. He was furious that Rajenda Pauchari, director of the panel, condoned Trenberth’s statements that hurricanes were worsening because of global warming. 



What is going on here? Nothing unusual. Behavior like this takes place every day at faculty meetings across academia. But global warming and hurricanes are hot topics right now, so the bickering spills over into the press. 



What is unusual is the especially shoddy nature of the current scientific review process on global warming papers. 



Consider the recent _Nature_ article. If hurricanes had doubled in power in the last few decades as Emanuel claims, the change would be obvious; you wouldn’t need a weatherman to know which way this wind was blowing. All of these feuding scientists would have agreed on the facts long ago. 



Damages caused by doubling the strength of hurricanes would be massive and increasing dramatically. Figures on this are pretty easy to come by, at least in the United States. The insured value of property from Brownsville, Texas to Eastport, Maine — our hurricane prone Atlantic Coast — is greater than a year of our Gross Domestic Product. If hurricanes had actually doubled in power, the losses in the insurance industry would be catastrophic. 



Pielke has studied this, and his work is well known. Hurricanes are causing greater dollar damages because more and more people are building increasingly expensive beachfront monstrosities that have financially appreciated during the recent real‐​estate bubble. Account for these and there is no significant change in hurricane expenses along our coast. Illinois climatologist Stanley Changnon has also studied this for non‐​hurricane weather damage over the entire country with similar results. 



Pielke told me that, “analysis of hurricane damage over the past century shows no trend in hurricane destructiveness, once the data are adjusted to account for the dramatic growth along the nation’s coasts.” 



You would think that reviewers of Emanuel’s paper at _Nature_ would have thought to ask whether, in fact, there was evidence for increasingly powerful storms. 



But they didn’t. There is just no incentive in the scientific community to kill the remarkably fertile global warming goose, a beast that feeds on public fears. 



The federal outlay on climate research is now $4.2 billion per year, roughly the same amount given to the National Cancer Institute. The climate research community sees a grave threat when research shows there’s no threat from the climate. So papers that hawk climate disaster get superficial reviews and uncritical headlines, while those that argue otherwise are “shameful.”
"
"

Today the _Washington Post_ has a big story on efforts by the coal industry to get public schools to teach positive things about — you guessed it — coal. The impetus for the article is no doubt a recent kerfuffle over education mega‐​publisher Scholastic sending schools free copies of the industry‐​funded lesson plan “The United States of Energy.” Many parents and environmentalists were upset over businesses putting stealthy moves on kids, and Scholastic eventually promised to cease publication of the plan.   
  
  
Loaded curricula designed to coerce specific sympathies from children, however, hardly come just from industry, as the _Post_ story notes. Indeed, as I write in the new Cato book _Climate Coup: Global Warming’s Invasion of Our Government and Our Lives_ , much of the curricular material put out at least on climate change is decidedly alarmist in![](http://wac.0873.edgecastcdn.net/800873/cato/store/sites/default/files/imagecache/product/climate_coup_cover_130.jpg) nature, and is funded by you, the taxpayer. In other words, lots of people are trying to use the schools to push their biases on your kids, which is an especially dangerous thing considering how unsettled, uncertain, and multi‐​sided so many issues are.   
  
  
In light of the huge question marks that exist in almost all subjects that schools address, the best education system is the one that is most decentralized, in which ideas can compete rather than having one (very likely flawed) conclusion imposed as orthodoxy. And it would be a system in which no level of government — either district, state, or federal — would decide what view is correct, or what should be taught based on the existence of some supposed consensus, as if “consensus” were synonymous with “absolute truth.” What is truth should not be decided by who has the best lobbyists or most political weight, nor should children be forced to learn what government simply deems to be best.   
  
  
Of course, there are some people who will decide that they are so correct about something that it would be abusive not to have government force children to learn it. If their conclusion is so compelling and obvious, however, no coercion should be necessary to get people to teach it to their children — it should be overwhelmingly clear. More importantly, if there is controversy, efforts to impose a singular view are likely to fail not just with the children of unbelievers, but for many of the children whose parents share the view. As significant anecdotal evidence over the teaching of human origins has stongly suggested — and new empirical work has substantiated — when public schools are confronted with controversial issues, they tend to avoid them altogether rather than teach any side. In other words, efforts at compulsion don’t just fail, they hurt everyone.   
  
  
Educational freedom, then, is the only solution to the curricular problem. If you want full power to avoid the imposition of unwanted materials on your children, you must be able to choose schools. And if you want to ensure that your kids get the instruction you think every child should have, everyone else must have that ability, too.
"
"

It has been said that when the United States sneezes, the world catches a cold. While this metaphor still prevails, the world is now heavily vested in China’s well being as well.



A decade of near double‐​digit annual economic growth, tens of billions of dollars of annual investment inflows, an industrial complex that churns out an ever‐​increasing share of the world’s consumption, and the implications these developments have had on everything from raw materials prices to shipping rates have thrust China into the spotlight in 2004.



But success and the capacity to influence events bring responsibility. Thus, how China manages its economic policies, including its trade relations with the United States, Europe, and its Asian neighbors, will impact profoundly the direction of the world economy and institutions linked to it, such as the World Trade Organization.



China is the pretty girl with whom everyone wants to dance. The countries of ASEAN, New Zealand, and Australia have been tripping over themselves to get a date — for a free trade agreement. Argentina, Brazil, and Chile are giddy about commitments from the Chinese premiere for direct investment and big ticket purchases. American, European, and Asian companies continue unabated the process of setting up manufacturing operations in China. And, the reality of a growing consumer class — the prospects of which are made all the more alluring by new infrastructure projects in the country’s interior — has producers throughout the world dreaming of the possibilities.



In large measure, China owes the rapid pace of its economic progress to its World Trade Organization accession. Joining the WTO leveled the playing field for Chinese exporters, who until then often faced steeper trade barriers than those applied to standing WTO members. Facing equivalent trade barriers, Chinese exporters have been able to capitalize on their wage and other logistical advantages to capture growing shares of foreign markets.



Perhaps more importantly, WTO membership forced China to engage in a process of liberalizing its own rules on investment, foreign ownership, tariffs, and other barriers to trade. While much remains to be accomplished in these areas, WTO membership and the initial steps taken by China to honor its commitments have provided confidence to foreign investors, business partners, and importers that China would not be subject to whimsical and unpredictable changes in the business climate. Thus, the investment and purchase orders have poured in.



Going forward, it is important that China continue to recognize and honor this linkage. It may be tempting for China to be smitten by the acclaim and succumb to the fallacy that bilateral or regional trade agreements are viable alternatives to the WTO. But ASEAN’s countries are seeking to link their fortunes to China’s because under the WTO the latter has become an investment magnet — largely at the expense of the former. 



China is seeking copper and iron ore and other raw materials in South America because it wants to continue large infrastructure projects and it needs to feed its industrial machine. These developments are attributable to the willingness of Americans and others to buy Chinese‐​made products, again outcomes inspired by the WTO.



But Americans can continue to consume Chinese products only if the reserves are recycled. If China intends to diversify its portfolio by foregoing American bonds and securities in favor of purchasing hard assets in other countries — a perfectly legitimate choice — it will have to purchase more American services, technology, bio‐​technology, entertainment, and pharmaceutical products. This will require further liberalization of rules that disadvantage foreign providers, and substantially better enforcement of intellectual property protections.



The Bush administration has been a fairly decent steward of trade relations with China. It has overruled imposing sanctions under the China safeguard law on each of four occasions that the U.S. International Trade Commission recommended that prescription. It declined to initiate an investigation into what U.S. unions were calling unfair labor practices in China, and it dismissed a similar investigation into currency undervaluation. 



On matters of antidumping and textile safeguards, the Bush administration has acted less commendably. But its actions on these fronts reflect a prominent anti‐​trade — indeed, anti‐​China — strain in certain U.S. business and policy circles that will only grow worse if China does not, at a minimum, show greater progress on intellectual property rights enforcement.



Multilateral trade rules embodied in the WTO have been the infrastructure used by China — and indeed most of the world — to create wealth and opportunity over the past decade. Should China lose sight of its importance, the world may catch more than a cold. 
"
"**England enters a tougher version of its three tier system of restrictions on Wednesday, as a four-week lockdown ends.**
Northern Ireland has a two-week circuit-breaker lockdown, while Wales is banning the sale of alcohol in pubs, cafes and restaurants from Friday. Scotland has its own five-tier system.
Across the UK, some restrictions will be relaxed over Christmas, to allow three households to form a ""Christmas bubble"".
From just after midnight on Wednesday 2 December, areas will be placed in one of three tiers: medium, high and very high.
About 99% of England has been placed into the high and very high coronavirus risk category - tiers two and three.
The placing of areas in each tier will be reviewed every 14 days, with the first review on 16 December.
**Areas in tier two**
**Tier two (high) rules**
**Areas in tier three**
**Tier three (very high) rules**
Additional restrictions apply:
**Areas in tier one**
Only three areas have been placed in the lowest tier:
**Tier one (medium) rules**
Areas in the lowest tier will have some restrictions relaxed:
There are exceptions in all tiers for childcare and support bubbles. More details of the plan are here.
The new coronavirus tier restrictions will mean 55 million people will be banned from mixing with other households indoors. The decision about which tier to place an area in is based on:
Lockdown restrictions in Wales were eased on 9 November.
**The current rules say:**
People who you don't live with still cannot come into your home socially, unless you are in an extended household (bubble) with them. Tradespeople can enter your home to carry out work.
However, from **Friday 4 December:**
Read Wales' official guidance.
Northern Ireland started a two-week circuit-breaker lockdown from 00:01 GMT on Friday 27 November.
Read Northern Ireland's official guidance.
Each area of Scotland has been placed in one of five tiers.
Eleven local authority areas in west and central Scotland have recently moved from level three to level four, affecting two million people.
First Minister Nicola Sturgeon told MSPs the level four measures would be lifted at 18:00 GMT on Friday 11 December.
**Areas in level zero**
No areas have been placed in the lowest tier.
**Level zero (nearly normal) rules**
**Areas in level one**
**Level one (medium) rules**
Additional restrictions apply:
**Areas in level two**
**Level two (high) rules**
Additional restrictions apply:
**Areas in level three**
**Level three (very high) rules**
Additional restrictions apply:
**Areas in level four**
**Level four (lockdown) rules**
Additional restrictions apply:
Schools stay open in all levels, and here must also be no non-essential travel between Scotland the rest of the UK.
**Do you meet other people for exercise? Have you been out walking during the November lockdown? You can share your experiences by emailing**haveyoursay@bbc.co.uk **.**
Please include a contact number if you are willing to speak to a BBC journalist. You can also get in touch in the following ways:"
"We had driven the land cruiser for half a day across the seemingly endless reaches of the Great Basin, the vast, near-waterless region of valleys and plains that stretches out from Nevada in the western US and into neighbouring states. A mile away on the side of a mountain I spotted through binoculars the object of my quest: a small band of wild horses ambling slowly downhill, the alpha mare in the lead with her brood stallion bringing up the rear. Locating wild horses in the Great Basin has occupied me for years.  For all the time and effort, success still relies more on luck and persistence than skill. Yet as often as I’ve seen them, they never fail to inspire. Human fascination with wild horses, known as mustangs in the US, is as undeniable as it is inexplicable.  Perhaps anthropologist Elizabeth Atwood Lawrence had it right when she said: “Of all animals perhaps the horse is uniquely suited to represent the conquest of the wild – the extension of culture into nature.”   With only two humans per square mile it is by far the least densely populated part of America, but it is currently home to some 50,000 free-roaming wild horses and burros under the care and protection of the Bureau of Land Management (BLM), a US government agency.   The animals are scattered throughout the desert typically surviving in small family bands. They have no single coat color, and can appear as bay, brown, black, sorrel, chestnut, white, buckskin, gray, palomino, pinto, and blue, red and strawberry roans.  The breeds are mixtures of everything from draft horses and thoroughbreds to everyday grade horses. They are largely the descendants of strays from western cattle ranching, mining and the military. Only a small number are direct descendants of the horses introduced by the Spanish conquistadors and passed through Native American hands – true living history. Wild horses may provide inspiration to some – see the sports car and fighter plane that bear their name. But to others such as the ranchers who rely upon the federal lands for their livelihood and for whose cattle the horses compete for scarce feed and water, they are a curse on the land.  With such divergence between wild horse admirers and their haters, it is small wonder wild horses rank among the most challenging of public land issues. Cattlemen just prior to and after World War II, in an attempt to clear their grazing allotments of these unwanted animals, hired agents known as “mustangers”, whose job was to remove the wild horses by any means possible.  Frightened horses were pursued with motorised vehicles until exhausted, lassoed, tied down, and hauled to slaughter houses. Western ranchers removed these animals from their federal grazing allotments at such an alarming rate that by the late 1960s they numbered fewer than 18,000.   The struggle to save what remained of the West’s wild horses began with a secretary named Velma Bronn Johnston.  The particular event that brought this woman into the fray occurred not on the open range, but rather on the streets of downtown Reno, Nevada.  One day in 1950, while driving to work, she happened alongside a cattle truck filled with wild horses headed for slaughter and saw blood dripping from underneath it onto the city street.   The incident so disturbed Johnson that she soon dedicated her life to seeking protection for her “wild ones” as she called them.  Along the way, she even acquired the nickname “Wild Horse Annie”, first given by her enemies as a sign of contempt, but then worn as a badge of honour.  Johnson’s journey would eventually carry her far beyond her enemies, from Reno to Washington, DC and the Halls of Congress.   In 1971 Congress passed, and President Richard Nixon signed into law, the Wild Free-Roaming Horses and Burros Act.  The product of more than a decade of tireless effort on the part of Johnson and her colleague, Congressman Walter Baring of Nevada, the law was intended to protect the few remaining mustangs and burros on public lands as “living symbols of the pioneer spirit of the West”. The rangeland these animals now occupy, alongside cattle, wildlife and human hikers, is owned and managed by the federal government.  And it is the fierce human competition over these “multiple-use lands” that rests at the heart of the wild horse controversy.  Extreme positions have all but ruled-out compromise thus contributing to a serious state of policy gridlock. The single greatest problem facing the BLM is the excessive reproduction of wild horses.  Each year their numbers increase by about 20%, doubling their population every five years.  Round-ups and public adoptions are held, but these are not sufficient to offset the natural increase.  As a result, some 47,000 animals are currently in long-term holding facilities waiting out their lives far from the free roaming life originally envisioned by Wild Horse Annie.  The BLM has set a target population of about 27,000 for the free roaming animals.  As previously noted, the current population is nearly double that.   While it’s possible, extinction seems an unlikely fate for the wild horses.  The challenge is more one of proper management, especially with respect to controlling overpopulation. In this regard, the BLM is experimenting with fertility control.  One can only hope that the disputing factions can come together with BLM and workout an amicable solution. "
"We are in the middle of one of the biggest experiments in human history. At its core is the homogenisation of global food systems, which increasingly must deliver the same products to an expanding population (in all senses) across the world.  I now live in Kajang, in the Klang Valley around Kuala Lumpur, Malaysia. This area typifies many fast emerging economies where increasing wealth and aspirations lead to an appetite for global brands – to buy and to eat.  Within a few kilometres of my house I can purchase the same fast-food as in New York, London or Sydney.  The first McDonald’s in Kuala Lumpur opened in 1982. Now, there are more than 250 restaurants in Malaysia, with 42% of the local fast-food market in the Klang Valley. It is hard to imagine that when the McDonald brothers opened their first branch in California in 1940, they would initiate a global phenomenon whereby 70m customers in 118 countries would consume an estimated 1% of the food eaten every day on the planet in a McDonald’s outlet.  Kajang actually claims to be the home of satay. However, it seems inconceivable that a local “mamak” stall owner could ever sell satay on virtually every street corner around the world. McDonald’s now serves 144m “happy meals” in Malaysian outlets each year. Presumably, this saves 144m bored Malaysians from staring into their bowls of curry mee, satay and Roti Canai. The homogenisation of global food systems means that any fast-food outlet must depend on a long, complex and increasingly vulnerable supply chain to source products whose ingredients are derived from a tiny range of plant and animal species. While there are an estimated 30,000 edible plant species, just three (wheat, rice and maize) now account for more than 60% of the calories consumed by 7 billion people across the world. If we disturb the supply chains or the productivity of these major crops we are in trouble – wherever we live. Precisely because of their global significance and the consequences of their failure, virtually all our agricultural research, funding and promotion focuses exclusively on squeezing more out of these major crops grown as monocultures. As the climate changes, our increasing reliance on a few major crops will jeopardise food security. The recent IPCC (2014) report predicts that, without adaptation, temperature increases of above about 1o C from pre-industrial levels will negatively affect yields on the major crops in both tropical and temperate regions for the rest of the century.   These impacts need to be seen in the context of crop demand, which is predicted to increase by about 14% per decade until 2050. In a recent study in Nature, an international team of scientists found that iron and zinc concentrations were substantially reduced in wheat, rice, soybean and pea crops grown under the CO2 levels expected by 2050. In other words, climate change will reduce both the yield and the nutritional content of the world’s major crops – leaving many hungry and malnourished. While we might modify the characteristics and management of major crops sufficiently to yield under the lower range of temperature increases, we are unlikely to succeed at higher temperatures. So what should we do for agriculture in hotter, drier climates? A good start would be to explore the many hundreds of underutilised crops that have survived, yielded and fed people for millennia despite, not because of, agricultural science. For example, bambara groundnut (Vigna subterranea) is a highly nutritious, drought-tolerant African food legume. However, during Africa’s colonial period it was increasingly displaced by the oil-rich peanut, grown for its cash and export potential. Bambara – “the groundnut of the women” – has survived more through its own resilience and the tenacity of the communities that have cultivated it than the contribution of agricultural scientists to its improvement or extension agencies to its expansion. Our entire food system is in a precarious state, propped up by a narrow elite range of major crops backed by global research and advocacy.  Meanwhile everything else, including the underutilised and ignored crops that could sustain us in the future, is increasingly starved of resources.  Without urgent, serious and comparative research on crops that can yield in hotter, volatile climates of the future, the global food system will increasingly depend on only a few crops. Future generations will not thank us for allowing the rest to wither away."
"
Guest post by David W. Schnare, Esq. Ph.D.
When Phil Jones suggested that if folks didn’t like his surface temperature reconstructions, then perhaps they should do their own, he was right.  The SPPI analysis of rural versus urban trends demonstrates the nature of the overall problem.  It does not, however, go into sufficient detail.  A close examination of the data suggests three areas needing address.  Two involve the adjustments made by NCDC (NOAA) and by GISS (NASA).  Each made their own adjustments and typically these are serial, the GISS done on top of the NCDC.  The third problem is organic to the raw data and has been highlighted by Anthony Watts in his Surface Stations project.  That involves the “micro-climate” biases in the raw data.
As Watts points out, while there are far too many biased weather station locations, there remain some properly sited ones.  Examination of the data representing those stations provides a clean basis by which to demonstrate the peculiarities in the adjustments made by NCDC and GISS.
One such station is Dale Enterprise, Virginia.  The Weather Bureau has reported raw observations and summary monthly and annual data from this station since 1891 through the present, a 119 year record.  From 1892 to 2008, there are only 9 months of missing data during this 1,404 month period, a missing data rate of less than 0.64 percent.  The analysis below interpolates for this missing data by using an average of the 10 years surrounding the missing value, rather than basing any back-filling from other sites.  This correction method minimizes the inherent uncertainties associated with other sites for which there is not micro-climate guarantee of unbiased data.
The site itself is in a field on a farm, well away from buildings or hard surfaces.  The original thermometer remains at the site as a back-up to the electronic temperature sensor that was installed in 1994.

The Dale Enterprise station site is situated in the rolling hills east of the Shenandoah Valley, more than a mile from the nearest suburban style subdivision and over three miles from the center of the nearest “urban” development, Harrisonburg, Virginia, a town of 44,000 population.

Other than the shift to an electronic sensor in 1994, and the need to fill in the 9 months of missing reports, there is no reason to adjust the raw temperature data as reported by the Weather Bureau.
Here is a plot of the raw data from the Dale Enterprise station.

There may be a step-wise drop in reported temperature in the post-1994 period.  Virginia does not provide other rural stations that operated electronic sensors over a meaningful period before and after the equipment change at Dale Enterprise, nor is there publicly available data comparing the thermometer and electronic sensor data for this station.  Comparison with urban stations introduces a potentially large warm bias over the 20 year period from 1984 to 2004.  This is especially true in Virginia as most such urban sites are typically at airports where aircraft equipment in use and the pace of operations changed dramatically over this period.
Notably, neither NCDC nor GISS adjusts for this equipment change.  Thus, any bias due to the 1994 equipment change remains in the record for the original data as well as the NCDC and GISS adjusted data.
The NCDC adjustment
Although many have focused on the changes GISS made from the NCDC data, the NCDC “homogenization” is equally interesting, and as shown in this example, far more difficult to understand.
NCDC takes the originally reported data and adjusts it into a data set that becomes a part of the United States Historical Climatology Network (USHCN).  Most researchers, including GISS and the East Anglia University Climate Research Center (CRU) begin with the USHCN data set.   Figure 2 documents the changes NCDC made to the original observations and suggests why, perhaps, one ought begin with the original data.

The red line in the graph shows the changes made in the original data.  Considering the location of the Dale Enterprise station and the lack of micro-climate bias, one has to wonder why NCDC would make any adjustment whatever.  The shape of the red delta line indicates these are not adjustments made for purposes of correcting missing data, or for any obvious other bias.  Indeed, with the exception of 1998 and 1999, NCDC adjusts the original data in every year!  [Note, when a 62 year old Ph.D. scientist uses an exclamation point, their statement is rather to be taken with some extraordinary attention.]
This graphic makes clear the need to “push the reset button” on the USHCN.  Based on this station, alone, one can argue the USHCN data set is inappropriate for use as a starting point for other investigators, and fails to earn the self-applied moniker as a “high quality data set.”
The GISS Adjustment
GISS states that their adjustments reflect corrections for the urban heat island bias in station records.  In theory, they adjust stations based on the night time luminosity of the area within which the station is located.  This broad-brush approach appears to have failed with regard to the Dale Enterprise station.  There is no credible basis for adjusting station data with no micro-climate bias conditions and located on a farm more than a mile from the nearest suburban community, more than three miles from a town and more than 80 miles from a population center of greater than 50,000, the standard definition of a city.  Harrisonburg, the nearest town, has a single large industrial operation, a quarry, and is home to a medium sized (but hard drinking) university (James Madison University).  Without question, the students at JMU have never learned to turn the lights out at night.  Based on personal experience, I’m not sure most of them even go to bed at night.  This raises the potential for a luminosity error we might call the “hard drinking, hard partying, college kids” bias.  Whether it is possible to correct for that in the luminosity calculations I leave to others.  In any case, the lay out of the town is traditional small town America, dominated by single family homes and two and three story buildings.  The true urban core of the town is approximately six square blocks and other than the grain tower, there are fewer than ten buildings taller than five stories.  Even within this “urban core” there are numerous parks.  The rest of the town is quarter-acre and half-acre residential, except for the University, which has copious previous open ground (for when the student union and the bars are closed).
Despite the lack of a basis for suggesting the Dale Enterprise weather station is biased by urban heat island conditions, GISS has adjusted the station data as shown below.  Note, this is an adjustment to the USHCN data set.   I show this adjustment as it discloses the basic nature of the adjustments, rather than their effect on the actual temperature data.

While only the USHCN and GISS data are plotted, the graph includes the (blue) trend line of the unadjusted actual temperatures.
The GISS adjustments to the USHCN data at Dale Enterprise follow a well recognized pattern.  GISS pulls the early part of the record down and mimics the most recent USHCN records, thus imposing an artificial warming bias.  Comparison of the trend lines is somewhat difficult to see in the graphic.  The trends for the original data, the USHCN data and the GISS data are: 0.24,
-0.32, and 0.43 degrees C. per Century, respectively.
If one presumes the USHCN data reflect a “high quality data set”, then the GISS adjustment does more than produce a faster rate of warming, it actually reverses the sign of the trend of this “high quality” data.  Notably, compared to the true temperature record, the GISS trend doubles the actual observed warming.
This data presentation constitutes only the beginning analysis of Virginia temperature records.  The Center for Environmental Stewardship of the Thomas Jefferson Institute for Public Policy plans to examine the entire data record for rural Virginia in order to identify which rural stations can serve as the basis for estimating long-term temperature trends, whether local or global.  Only a similar effort nationwide can produce a true “high quality” data set upon which the scientific community can rely, whether for use in modeling or to assess the contribution of human activities to climate change.
David W. Schnare, Esq. Ph.D.
Director
Center for Environmental Stewardship
Thomas Jefferson Institute for Public Policy
Springfield Virginia
===================================
UPDATE: readers might be interested in the writeup NOAA did on this station back in 2002 here (PDF, second story). I point this out because initially NCDC tried to block the surfacestations project saying that I would compromise “observer privacy” by taking photos of the stations. Of course I took them to task on it when we found personally descriptive stories like the one referenced above and they relented. – Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8e27da78',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"With the California Democratic primary taking place on Super Tuesday this presidential season, the most populous, delegate-rich state in the US will have more influence than ever over choosing the party’s nominee. That influence will reflect the particular priorities of California Democratic primary voters, who in a December poll named the climate crisis as their highest priority for the next president. In the Golden state, climate priorities are not just a matter of lowering emissions and preventing further catastrophe, but also planning to adapt for the kinds of disasters Californians have come to know all too well. Rising seas lap at communities up and down the Pacific coast, and devastating wildfires since 2017 have killed more than 150 people and destroyed more than 35,000 structures. All the candidates have pledged to end new fossil fuel operations on federal lands, after the Trump administration approved new leases for oil drilling on federal land in California in December. But how else do their plans compare on climate and California?  Joe Biden’s $5tn climate plan leans hard on his record as vice-president and role in passing the 2009 Recovery Act, which made the biggest investment in clean energy in US history. It calls for a new collaborative agency focused on climate and innovation and 100% net zero emissions by 2050. Biden’s plan is unique in blaming urban sprawl for its impact on climate. He says he will work on altering local regulations to allow for denser housing near public transit to cut commute times and decrease the carbon footprint of sprawly areas. Policies like those have proven highly controversial in California, where the legislature has repeatedly rejected statewide zoning reforms that would promote denser, greener development. Biden’s plan is unique in calling for the insurance industry to collaborate on lowering premiums for homeowners and communities that invest in resilience – particularly relevant for Californians who live in high wildfire risk areas and have faced soaring insurance premiums or loss of policies altogether. Since he entered the race late, Mike Bloomberg’s policy proposals have escaped much of the scrutiny that other candidates have weathered. His climate plan pledges 80% clean energy by 2028 and 100% emission-free new passenger vehicles by 2035. Bloomberg has a separate climate resilience plan, pledging to invest in underserved communities, prioritize the most vulnerable and mitigate climate hazards. And in an apparently continuation of the candidate’s focus on California, there’s also a specific wildfire resilience plan, by far the most extensive of any of the candidates’ efforts, which would raise federal funding for fire resilience and management to $10bn, prevent insurers from dropping homeowners in high fire-risk areas and create a new “wildfire corps” with thousands of new jobs, all with the goal of reducing wildfire-caused loss of life and property by 50% by 2024. Pete Buttigieg’s $2tn climate plan pledges to reach net-zero electricity by 2035 and net-zero emissions from industry by 2050. The plan calls for investments in clean technology and climate resilience alike. The plan calls for establishing a National Catastrophic Extreme Weather Insurance program that would cover Americans facing climate-driven disasters and earthquakes – good news for Californians. “The government would also create an exchange for families to purchase catastrophic insurance subsidized by the government depending on income level,” the plan reads. It also references the impacts of drought on California farmers, and disproportionate impacts on immigrant farmworkers specifically. Amy Klobuchar’s climate plan includes a $1tn package of investment for climate research, tax incentives to spur research and development through public-private partnerships and other projects. She says she would bring back fuel economy standards, which have proven a serious point of contention as the Trump administration has weakened the standards for cars and light trucks and has challenged the right of California and other states to follow more stringent standards. Klobuchar wants to “mobilize the heartland” to build for climate resiliency. Her plan would upgrade levees for more frequent and severe floods in the midwest – but there’s no plan for California fires. Bernie Sanders is pitching a $16.3tn public investment to reduce emissions and prepare America for the impacts of climate crisis. His plan includes 100% renewable energy for electricity and transportation by 2030, and full decarbonization by 2050. Sanders’ climate plan includes a pledge to transition America’s investor-owned utility companies to public ownership – a big dig at troubled PG&E. Extensive grants to purchase new electric vehicles would be a boost to California drivers, who are on the road more than the American average. The plan also a long section on firefighting, pledging to invest $18bn in the federal firefighting workforce. Community resiliency funding would go toward preparing for climate impacts, including wildfire evacuation plans, and additional sea walls – which, in California, have actually been known to contribute to coastal erosion. Elizabeth Warren’s $10tn climate plan calls for 100% net-zero emission electricity by 2035, 100% new clean vehicles by 2030, and 100% fossil-fuel-free new buildings by 2028 – the three industries she blames for making consumers think the climate crisis is their own fault for using plastic straws. The plan aims to create more than 10m green jobs. Those investments would prioritize frontline communities and environmental justice. Overall Warren’s plan focuses on addressing the causes of climate change more than its impacts. While there is copious detail on how and why to transition the industries to blame, there’s little to nothing on adaptation and community resiliency against the threats the candidate briefly outlines at the start of her plan."
"
Share this...FacebookTwitter Reader Bernd Felsche has a public service message for those of us living in Germany:
Summer in Germany this year will be on August 2, 2011, from 1:30 pm to 3:30 pm.
A whole 2 hours. Sorry, but that’s all that’s in store for summer in Germany this year.
Germany over the last weeks has been gripped by unusually cool and wet weather, and so many are wondering whatever happened to  summer. The forecast for the days ahead according to the English-language The Local doesn’t look good either as rain and cool with temperatures stuck in the 60s remaining the rule.
Things are not expected to improve until Monday, with summer briefly returning on Tuesday. The Local writes:
Tuesday would be the pick of next week, with highs ranging from 24 to 29 degrees and plenty of sunshine right across the country.
Cold and wet for the next 4 weeks
Are the barbecue summer conditions going to persist? The Local writes:
And the good news? There isn’t any, said DWD [German Weather Service] meteorologist Andreas Friedrich.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




‘A look at the meteorological crystal ball, meaning predictions for the next four weeks, offers nothing good for anyone who still hopes for steady and warm summer weather,’ he said. ‘For the period to the end of August, we can only infer trends but, well, this shows the arrow regarding temperatures pointing further downward’.
Long-term forecasts, Friedrich reminds us, are only 60% accurate, and so there is still some hope that we may end up getting some nice weather to salvage the summer before it ends.
Peddling quackery
Speaking of the DWD (German Weather Service) – which has become one of the key propaganda arms of the German global warming movement – it says here Germans will be able to enjoy nicer summers by the year 2100. The DWD writes in its July 26 press release:
Climate simulations show for Germany further warming of 2 to 4°C by the year 2100. Drier summers and wetter winters and more extreme weather events are anticipated.”
Here they are not talking about plain old longer-term weather forecasts over the next 4 weeks, but of “climate simulations” for the next 90 years. Their accuracy, scientists claim, with the DWD agreeing, are really worth taking to the bank!
Forget it folks. Nobody can make such predictions. That’s pure utter quackery.

Unfortunately, this year’s German summer is turning out to be just the opposite of heat and sunshine (once again!).
Share this...FacebookTwitter "
"The Amur tiger (Panthera tigris ssp. altaica), also known as the Siberian Tiger, is among the world’s rarest and most endangered cat species. The largest and northernmost tiger, it is believed only around 450 of these magnificent, 200kg, three-metre-long cats remain in the wild. However even this precarious situation is better than in the 1930s, when hunting reduced them to the brink of extinction – around only 20 animals. Today the threat they face comes from illegal poaching for body parts taken for Chinese medicine, poaching of their prey such as deer and boar, and steady destruction by logging of their habitat which makes their long term survival precarious.  Such low numbers in the 1930s and 1940s left the species with a “bottleneck” of low genetic diversity, susceptibility to disease, and poor cub survival rates. Recently an infectious disease that causes viral encephalitis, canine distemper, has been identified as a significant threat to the species’ survival. Researchers estimate that this virus has killed at least 1% of Amur tigers since 2009.  But the worldwide network of zoos can provide a bulwark against extinction. The population of Amur tigers in captivity, at around 480 in 185 institutions, is perhaps larger than those in the wild. The World Association of Zoos and Aquariums (WAZA) made the Amur tiger a focus of its global species management plan, bringing together the world’s four key regional zoo associations to maintain sustainable captive populations of Amur tigers as a genetic “lifeboat” for those in the wild. The plan also provides an important political framework through which conservationists and governments can share information and cooperate internationally. Almost all the remaining Amur tigers live in the far east of Russia, mostly in the Sikhote Alin mountain region, and there is little genetic exchange between tigers there and the much smaller sub-population found in southwest Primorye province.  A small population exists in China but it depends on animals moving across the border with Russia. It’s not known whether there are still tigers in North Korea.  The forests of the Russian Far East are declining rapidly due to large-scale illegal logging, mainly to supply Chinese furniture and flooring manufacturers, many of which then export to the US and Europe. This over-harvesting of trees reduces the supply of pine nuts and acorns, which are the main food sources for the tigers’ prey.  Uncontrolled forest fires and agricultural burn-offs are also reducing the tiger’s habitat. Russia became the first country in the world to grant the tiger full protection, and its ban on tiger hunting in 1947 was instrumental in preventing the species from being hunted to extinction. Assisted by organisations such as WWF, Russia now has a national action plan for the Amur tiger, with a major step forward being the establishment of the Sredneussuriisky Wildlife Refuge in 2012. Covering nearly 180,000 acres, this allows Amur tigers to cross the Russia-China border, bringing together otherwise isolated tiger sub-populations.  So while conservation efforts in recent years have seen the IUCN reclassify the species down from critically endangered to endangered, poaching and logging of their habitat is still a problem. Stricter penalties for wildlife crime and banning pine harvesting in tiger forests is crucial.  Sergei Bereznuk is a key player in Russian tiger conservation. The recipient of a 2012 Rolex Award for Enterprise, Bereznuk lobbies against illegal poaching and threats to tiger habitat and heads a small NGO, the Phoenix Fund.  In partnership with the Zoological Society of London and the Wildlife Conservation Society, the Phoenix Fund has improved anti-poaching efforts using better data recording and software tools, and promotes awareness and education for locals living in the area. A key educational event is the annual Tiger Day Festival in Vladivostok, in which over 4,000 schoolchildren and students dressed in tiger costumes paraded through the city in 2013. The Phoenix Fund has also created a network of volunteer fire-fighting teams to tackle forest fires. Russian conservation projects also benefit from the skills of the likes of British veterinary charity Wildlife Vets International, for which John Lewis provides tiger capture and anaesthesia training to local conservation vets, and makes genetic and health assessments of samples from wild Amur tigers. But as well as providing financial support to tiger conservation, consumers in the West can play their part in preventing illegal logging in the tigers’ forests by only purchasing Forest Stewardship Council-certified (FSC) products, which provides assurance that legal, environmental, and social protections are in place and forests are managed responsibly. Perhaps the most difficult challenge is to stem the demand for tiger bone from China. Despite all trade in tiger parts being banned under international law, growing Chinese affluence has seen demand for these products – seen as a symbol of high status and wealth – increase. When one tiger can provide ten years’ income on the black market it seems an irresistible incentive for poachers."
"**There have been more than 1.6 million confirmed cases of coronavirus in the UK and about 60,000 people have died, government figures show.**
However, these figures include only people who have died within 28 days of testing positive for coronavirus and other measures suggest the number of deaths is higher.
**Find out how the pandemic has affected your area and how it compares with the national average:**
If you can't see the look-up click here.
After the first peak in April, cases started rising again in July, with the rate of growth increasing sharply in September and October, before falling again in the past two weeks.
On Wednesday, the government announced a further 16,170 confirmed cases.
It is thought the infection rate was much higher during the first peak in spring, but testing capacity at the time was too limited to detect the true number of daily cases.
The data for cases can also be broken down by region and comparing the change in those figures by week gives a sense of where there has been a recent increase in newly-reported infections.
Coronavirus infections in England have fallen by about a third during lockdown, according to a major study carried out by Imperial College London.
While some of the worst-hit areas saw the biggest improvements, cases have remained relatively high across England, according to the React-1 study, which is based on tests of more than 100,000 people between 13-24 November.
The latest figures from the Office for National Statistics (ONS), suggest about one in 185 (16,400 people) in Wales had the virus in the week ending 21 November.
In Northern Ireland rates are thought to be decreasing at around one in 145 people (about 12,700 people), while in Scotland, the figure was one in 115 (about 45,700 people).
The average number of daily deaths began to rise again in September, following the first peak in April.
On Wednesday, the government announced a further 648 deaths.
Of these, 555 deaths were in England, 51 in Wales, 38 in Scotland, and four in Northern Ireland.
Rules were amended over the summer to include deaths in the coronavirus total only if they occurred within 28 days of a positive test. Previously in England, all deaths after a positive test were included.
England has seen the majority of UK deaths from Covid-19. Using the 28-day cut-off, there have been more than 52,000.
Although hospital admissions for Covid-19 remain below the levels seen in the spring, there are big regional disparities.
The North West, North East and Yorkshire, and Midlands have seen the highest number of admissions but the situation in all three now appears to be improving.
Cases have risen across large parts of England, with other spikes in areas of Scotland, Wales and Northern Ireland.
The red areas on the map below are those currently seeing the highest number of cases per 100,000 people.
Restrictions have been tightened across the UK in an effort to tackle the number of rising cases.
In England, each local authority has been placed in one of the new three tiers following the end of the recent national lockdown.
Wales has announced a range of further measures, including a ban on pubs, restaurants and cafes serving alcohol, from Friday 4 December.
In Northern Ireland, a further two-week ""circuit-break"" lockdown began on 27 November.
Scotland has a five-tier system of alert levels with different measures in place in different parts of the country.
UK leaders have agreed to allow up to three households to meet indoors during a five-day Christmas period of 23-27 December.
When looking at the overall death toll from coronavirus, official figures count deaths in three different ways.
Government figures count people who died within 28 days of testing positive for coronavirus.
But there are two other measures.
The first includes all deaths where coronavirus was mentioned on the death certificate, even if the person had not been tested for the virus. The most recent figures suggest there had been almost 70,000 deaths by 20 November.
The second measure counts all deaths over and above the usual number at the time of year - that figure was more than 75,000 by 20 November.
The most recent figures available from the ONS are for the third week of November, which show there were 14,276 deaths registered in the UK.
Some 3,038 of these deaths involved Covid-19 - 199 more than the week before.
Deaths normally do rise at this time of the year, but the data from the ONS and its counterparts in Scotland and Northern Ireland show the second wave of the virus has pushed the death rate above the average seen over the past five years by about 21%.
Overall, the figures are still well below the peak of 9,495 deaths recorded in a week, reached on 17 April.
The ""R number"" is the average number of people an infected person will pass the disease on to.
If R is below one, then the number of people contracting the disease will fall; if it is above one, the number will grow.
The government's current estimate for the R number across the whole of the UK is 0.9 to 1.
The estimate for England is 0.9 to 1, while for Scotland it is 0.8 to 1. The estimate for Wales is 0.8 to 1 and in Northern Ireland it is 0.9 to 1.1.
The government has said in the past that the R number is one of the most important factors in making policy decisions."
"

Environmental activists usually critical of electrified America must have mixed emotions this time of the year. Though it is a season of good cheer and goodwill toward all, it is also a time of conspicuous energy consumption. To many people, America the Beautiful is at her best in December when so much of the nation is illuminated by billions of tiny stringed light bulbs. Holiday lighting is a great social offering — a positive externality, in the jargon of economics — given by many to all.



While a few energy doomsayers such as Paul Ehrlich rile against “garish commercial Christmas displays,” few of today’s headline grabbers (Arianna Huffington, where are you?) have attempted to stir up debate over the generator‐​hours devoted to making the season glow. Indeed, holiday lighting seems a dazzling exception to the activists’ goal of reducing discretionary energy usage. 



But if holiday energy guzzling can be overlooked, why not excuse outdoor heating and cooling, one‐​switch centralized lighting, and instant‐​on appliances that “leak” electricity, not to mention SUVs? Prancing around to turn on individual lights or waiting for the photocopier to warm up wastes the scarcest and one truly depleting resource: A person’s time. 



Known world oil reserves are more than 20 times greater now than they were when record keeping began in the 1940s; world gas reserves are almost four times greater than they were in the 1960s; world coal reserves have risen fourfold since 1950. Transient developments, often political, can drive supplies down and prices up, but the raw mineral resource base is abundant — and expanding in economic terms thanks to an inexhaustible supply of human ingenuity and exploratory capital.



Record energy consumption has been accompanied by improving air quality. Urban air quality is a third better today than in 1970. The U.S. Environmental Protection Agency reported that air emissions of the criteria pollutants declined by 25 percent, as energy usage increased by 150 percent. Further air emission reductions are expected, but they will not be accomplished by forcing higher prices or inconvenience on consumers. Future reductions will be accomplished with market incentives, technological improvement, and regulation based on sound science, not alarmism.



Should good citizens think twice about holiday lighting, given global warming and other suspected climate changes supposedly caused by increasing atmospheric concentrations of carbon dioxide? Hardly. A moderately warmer, wetter world, whether natural or anthropogenic, such as experienced in the 20th century, is a better world. Carbon dioxide from the combustion of fossil fuels “greens” the biosphere through the well‐​documented carbon fertilization effect. But most importantly, the wealth created from affordable, plentiful energy provides the primary means for societies to improve the environment. In the final analysis, wealth produces environmental health, which explains why increasing energy usage and environmental improvement have gone hand in hand in the Western world.



There is much to be thankful for this holiday season with our energy economy. But thoughts about the less fortunate should be with us, too. The World Energy Council estimates that 1.6 billion people lack electricity for lighting, heating, cooling, or cooking. A Christmas tree for us is likely to be firewood for those living in energy poverty. For fully a fourth of the world’s population, there could be no greater holiday gift than affordable electricity, explaining why the developing world has flatly rejected proposals from environmental elites to forsake future energy usage in the quixotic quest to “stabilize climate.”



Energy consumption is good — for comfort, convenience, and even celebration. May one and all in good conscience enliven this holiday season with lights aplenty. With sources of conventional fuels steadily expanding and energy technologies rapidly advancing, Americans can look forward to even more energetic celebrations and shared goodwill in the holiday seasons ahead. 
"
"
People get busy when questions get raised, and they send me things. I got an email today with a link and quote that read:
The student dissertation the IPCC used in AR4 doesn’t even support their claims.  The student states in his dissertation: “In how far the changes observed  indicate a global change of climate can only be guessed and will show in the  future.”
Huh.
In our last story, referencing the work of the Telegraph, we touched on the what many consider inappropriate citations in the 2007 IPCC AR4 report. See here: IPCC Gate Du Jour: UN climate change panel based claims on student dissertation and magazine article One citation was an article in Climbing Magazine issue 208, while another was a student dissertation. Some said that there’s nothing wrong with citing a student dissertation. Perhaps, but hold that thought until after reading this story from “ClimateQuotes”: (Note – for those who can’t delineate what part is my writing and what part is from ClimateQuotes, that website’s portion is everything after this – you know who you are 🙂 )
The story of the Geography Major’s Dissertation

Dario-Andri Schwörer

A big story in climate science right now is the fact that the IPCC relied on a mountain magazine and a graduate student’s dissertation as their citations for a specific claim in their Fourth Assessment Report. However there are few details, so I decided to do some digging. I found out a bit about the dissertation.
I believe this is the dissertation. It is written by this man, Dario-Andri Schwörer, also here. He was a student at the Geographical Institute of the Universities of Berne and Zurich, which is where he wrote his dissertation in or before 1997. He is now an avid outdoors-men, and a self-described ‘well known expert on the impact of climate change in the Alps’. Right now he is engaged in the TOPtoTOP program to promote climate protection.
The dissertation itself is titled:
An Inquiry into Possible Effects of Climatic Change on the Mountain Guide Trade in the Bernina Region
Subtitled:
Geography Major Dissertation
by
SCHWÖRER DARIO-ANDRI
carried out at the Geographical Institute of the Universities of Berne and Zurich
The dissertation itself is not entirely about climate change. In fact, he mentions the number one reason that mountain guides give for decreased climbs is not climate change, but:
“They attribute this decrease in the first place to the recession and the high exchange rate of the Swiss franc in relation to the German mark. In the second place they mention changes of the natural environment.”
That wasn’t mentioned in the AR4. The ambiguity continues:
Read the rest at ClimateQuotes.
This is the link: The story of the Geography Major’s Dissertation
(for those that have trouble following links to referenced sources, click on the bold portion, you know who you are :-))


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8e6d08ed',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The speed and scale of China’s rapid economic growth has led to widespread degradation of its densely inhabited coastlines, according to an analysis of 60 years of social, economic and environmental data.  This study, published in the Nature group journal Scientific Reports, examined historical trends in 15 different ways humans have affected coastal ecosystems, including fishing, pollution, coastal habitat transformation, and disturbance from shipping. China’s 1978 economic reforms mark a major ramping up of China’s economic development, and since then the country has rapidly shifted from an agrarian society stretched across the country’s vast interior to an industrial economy concentrated in coastal regions.  The analysis found that in all 15 measures human impact had increased over the period, and especially so after 1978. For the ten measures with data that covered the period before and after the reforms, the increase rates of six were significantly higher, four about the same, and none lower following the reforms. Analysis of the environmental data shows that China’s coastal ecosystems have been degraded rapidly since the 1978 economic reforms. For example, across China’s seas, the diversity and body size of marine fish did not change before 1978, but both have steadily shrunk since. Harmful algal blooms have become more frequent, and generally the extent of coral reefs, mangroves and coastal wetlands have been decreasing. The study also examined the relationships between the economic measure of GDP per capita and affects of humans upon China’s coasts. According to the environmental Kuznets curve theory, in the early stages of economic development the impact of humans increases with per capita income, but decreases when per capita income reaches a turning point. We found evidence of these turning points for some of the types of human affects we measured, but not others.  Some appeared to have just been reached, with no meaningful decreases yet in evidence, for example excess fertiliser causing algal blooms. Others are still too distant. For example, if habitat transformation for fish farming is to reach its predicted turning point, this will need considerably more coastline to be made available for fish farming – more coastline in fact than China has. And other turning points show how the impacts of human activity is shifting from richer to poorer regions, meaning the affects on the environment are not decreasing country-wide so much as merely moving around. The study also shows that it is economic growth, rather than population growth, that is the major cause of the damage to China’s coasts. Perhaps because China is the world’s most populous country, it is often assumed – particularly in the west – that population pressure is the major problem. Similarly, previous studies of the impact on coastal and marine ecosystems often address the role of increased population density along coastlines.  In contrast, this study shows that the rate of population increase in China’s coastal provinces remained constant after 1978, and that including population change or population density in the analysis had little effect on the results. The study suggests that strict conservation measures are needed to protect China’s coastal ecosystems and to sustain future growth. China’s GDP per capita remains very low, and without strict conservation efforts, as economic growth pushes per capital income up and spurs further development China’s coastal ecosystems will suffer still further. "
"On Monday, the government did something remarkable. In the windiest country in Europe, it finally ended a five-year block on new onshore wind turbines. It’s a victory for campaigners, and anyone who wants action on the climate crisis and cares about lower energy bills in future.  The government has hopefully ended a strategy – begun by David Cameron in 2015 – of making energy policy in direct contradiction to public opinion. No two issues define this tendency more than the government’s seemingly unshakeable support for fracking – and the insistent de facto ban on new onshore wind turbines. Of course, it didn’t actually ban new onshore wind turbines in 2015. Making them illegal would have been absurd. Instead, post-election, it swiftly attacked from two angles. First, it imposed onerous planning barriers on new wind projects in England, which led planning applications to shrivel by 95% by 2018. Second, it prevented onshore wind projects from bidding for the kind of long-term clean energy contracts that are available to other power sources like nuclear and offshore wind farms that are needed to get them built. Perversely, this was only possible by also blocking solar power from the auctions – meaning the two cheapest sources of clean energy were suddenly out in the cold. In subsequent years, the government’s own in-depth attitude trackers showed public support for fracking plummeting as low as 15% by 2018, yet it backed the industry to the hilt – even proposing at one point to make fracking wells “permitted developments”, effectively reducing them to the planning status of a garden wall. Over the same period, 75% of the public supported onshore wind turbines – by this point cheaper than any source of power from fossil fuels. The government response was to look the other way, despite the UK having a world-class wind resource, and new turbines being the cheapest way to provide power for its citizens. That was until the announcement on Monday that the next round of clean energy auctions, to be held in 2021, would include both onshore wind and solar once more. These auctions award contracts that effectively set a floor price for power sold from new projects, which in effect acts as a subsidy over the lifetime of the project if the floor price is higher than the market price for power. Onshore wind is now so cheap its floor price is expected to be at, or even below, current market prices. To translate, that means new onshore wind projects should be free of government subsidy, another bell tolling on the future of fossil fuels. It shouldn’t have been this hard – and we’ve certainly lost valuable time – but climate campaigners should celebrate forcing the reversal of fortunes between fracking and wind. The former is now fighting for survival, the latter is back in from the cold – just as the public wanted. Add in bringing forward the petrol and diesel car ban to 2035, a pledge to spend £9bn on domestic energy efficiency in the Conservative election manifesto and the decision not to appeal the seismic Heathrow judgment, and you might be forgiven for wondering if we’re seeing something bigger at work within this new government. Of course, it’s too early to tell for sure. The fracking industry was brought to its knees by a combination of brave and indefatigable grassroots campaigning and Tory MPs vexed at what fracking meant for their constituencies. It became clear the government was backing a losing horse. With the economics and social licence of fossil fuels receding weekly, it seems unlikely to remount any time soon. An almost equally long-standing – though less high-profile – public campaign helped ensure the thorn of onshore wind never left the government’s side. But its success is perhaps indicative that from atop his 80-seat majority, Boris Johnson is indeed willing to upset backbenchers to take action on the climate crisis. Make no mistake: there will be parliamentary colleagues of his for whom this change in direction is the stuff of nightmares. A backlash from the usual suspects orbiting 55 Tufton Street – the London home of many groups associated with climate denial – can’t be far off. So, will it stick? This is a question with relevance not just to wind turbines, but to much of what lies ahead if we are to cut emissions rapidly. Whether it’s the intrusive work of replacing domestic gas boilers, the fraught territory of reducing private car use, or delicate nudges to shift diets, around each policy corner lies a potential battle. As ever, this risk is exacerbated when policies are seen to be imposed from above. Yet new wind turbines could be owned and led directly by communities, councils, local businesses or key services such as hospitals. There is no reason wind projects can’t become synonymous with bottom-up climate action that communities are proud of and benefit from. That would put them beyond the reach of reactionary scaremongering. Yet for this to happen, yesterdays’s announcement only takes us so far. In England, while the block on onshore wind competing for clean energy contracts has been lifted, onerous planning blocks on turbines still remain. This rules out a full wind renaissance south of the border. The government must have the courage to remove them, so we can start building our new energy system – this time from the ground up. • Max Wakefield is director of campaigns at the climate action organisation Possible"
"Labor’s federal president Wayne Swan will accuse Scott Morrison of engaging in “predatory centrism” on climate policy by styling himself as the pragmatist between the extremes of climate emergencies and denialism, when the government has no intention of driving meaningful emissions reduction. In a speech to be delivered on Sunday, Swan will argue Labor will only win the decade-long climate wars if it approaches the challenge with “pragmatic policy and ruthless organisation”.  According to speech notes, Swan will say Labor needs to articulate a roadmap for the domestic coal powered industry “which manages its decline”. “There must be a strong dialogue between government, industry, and the unions, and operate on the principle that no one gets left behind. We have to work closely with coal-dependent regions and put in a plan so that everyone gets a good future.” Swan will also warn his former parliamentary colleagues to apply “large doses of healthy scepticism about the hand-on-heart last minute conversion of the Business Council of Australia to the cause”. The BCA criticised Labor policy at the last federal election, but has recently backed a legislated target of net zero emissions by 2050, which aligns with the policy outlined on February 20 by Anthony Albanese. Swan will say Labor is a party that accepts the science of climate change, supports blue-collar jobs, and has sought a way forward to reduce emissions across the economy with “no help from the right or the extreme left”. “Proposals that talk about shutting down the export coal industry instead of focusing on the hard and tough policy which includes reducing emissions across the whole of our economy are entirely counter-productive,” the ALP president will say. “Over the next 20 years there will be a dramatic reduction in coal production delivered by the market, and we will see coal-fired power generation provide less and less of our electricity”. “But the notion put forward by some green groups that we could phase out coal-fired power by 2030 is impossible and a recipe for blackouts and the further erosion of public support for strong economy-wide action on emissions reduction.” Swan will argue Morrison does not have a serious climate policy, and will not develop one, “but as you would expect from a marketing guy, [he has] a clearly articulated PR strategy to use climate as a wedge aimed not just at coal miners but working people more generally and particularly the elderly”. He will say Morrison’s approach will be to argue he is “the reasonable guy in the middle who says the climate is changing” but do little or nothing about it. “We in the Labor party need to expose this marketing strategy for what it is. “And we need to set against it our hard work on what I simply call solving the bloody problem.” The Morrison government has blasted Labor for adopting a 2050 target without a roadmap to get there. On Friday the government signalled plans to shift investment from wind and solar to hydrogen, carbon capture and storage, lithium and advanced livestock feed supplements, as part of a “bottom up” strategy to reduce emissions by 2050. The government will shortly release a technology roadmap outlining its investment strategies, and further policy work is under way. As well as the roadmap, the government is reviewing its much-criticised emissions reduction fund and the operation of the safeguard mechanism, and is working on an electric vehicles strategy, despite blasting Labor during last year’s election, claiming measures to drive the takeup of EVs were a “war on the weekend”. Despite blasting Labor for adopting net zero, the government has not ruled out following suit and adopting a mid-century target. As a signatory to the Paris agreement, the Coalition has implicitly adopted a position of carbon neutrality by mid-century. Albanese spent Saturday in coal country in New South Wales. In a speech to the country Labor conference, Albanese said net zero meant opportunity for regional Australia, not catastrophe."
nan
"A plan to resolve the UK’s housing crisis by adding garden city extensions to 40 towns and cities has won a prestigious economic prize. Urban designer David Rudlin was last night awarded the £250,000 Wolfson Prize for his proposal to build connected satellite settlements around existing large towns. The UK has long had an issue with housing, and the policies that have sought to solve this problem have shaped the urban form of the country. Industrialisation, suburbanisation, inner city estates with tower blocks, and new towns or major extensions have all responded to the need to meet urgent housing demand. Today, a renaissance of the Edwardian garden city idea seeks to challenge the piecemeal in-fill of urban centres and former industrial sites, or the slow creep of suburbia via urban extensions. The original idea was to build new, self-supporting settlements beyond green belts around existing towns and cities. This way people could escape the slums of the cities, or the “slum on wheels” of long-distance rail commuting.  However, while garden cities had a huge influence on large scale suburbanisation in the 20s and 30s the concept was relaunched as a major solution to the post-war housing crisis. It was a programme seen as equivalent in scale to the creation of the NHS. Around 30 new towns in total (from Stevenage, Harlow and Basildon, culminating in Milton Keynes) were built.  What the Wolfson Prize has done is prompt planning and urbanism professionals to look again at the methods and intentions of the garden city idea and apply it to the needs of today’s world to meet today’s housing crisis. As someone who has studied the history of this idea in depth, I believe this is a major moment. David Rudlin’s winning entry is notable in that it describes the principle of new settlements being built as satellites to any existing large town or small city. This revives the original concept of a cluster of garden city settlements linked together by transport. Each can be distinctive, but each can support the other in terms of sharing amenities, from leisure centres to employment hubs. Another advantage is that by providing the model for building from existing towns and cities, local authorities and local communities may be more empowered. Development will therefore take place where it has local support. While Britain’s post-war new towns have much in common with other places subject to major urban redevelopment or expansion in the mid to late 20th century (think Swindon, Basingstoke or Watford), or those re-engineered with ring roads, multi-storeys and shopping malls, the original garden city vision offered a more bucolic vision of genteel Edwardian serenity.  Each was a reflection of its era: the interwar ideal of an Englishman’s suburban castle, versus 1960s technological optimism. Perhaps it is a desire to return to the former that will emerge as such plans come forward, or perhaps it is the latter that will appeal more to the future Generation X and Yers who will become home owners when these places get built. There is an underlying idea that such places should enable greener ways of living (decent sized gardens, cycle paths, public transport) and indeed these same ideas were present in the post-war new towns. However, it is significant that these towns not only sought to build new housing for workers from the bombed out inner cities, but also to provide space for new industries, especially in the white heat sectors of aerospace, electronics, plastics and, to take advantage of the brand new motorway network, logistics.  The fresh start came with a fundamental attempt to redesign how the city, or rather town, would work, and drive an industrial strategy for the country. It was to be clean, healthy, filled with nature, with jobs for residents and innovative, modern facilities.  While the question of where to build sees a distinct logic of linked new settlements, reflected in the Rudlin proposal, the real issue is the underlying economic drivers for a place. While the Rudlin proposal supports localism, economic development and connectivity will still be the main factors behind new projects.  As Rudlin’s report puts it, quality depends on economics as much as design. This was a clear factor behind the relative success of the new towns, with Milton Keynes benefiting from its location midway between London and Birmingham, Warrington at the heart of the motorway network between Manchester and Liverpool, and Crawley next to Gatwick.   If HS2 goes ahead, a new stop en route may justify an urban development. Various key sites like old airforce bases or Didcot power station in Oxfordshire may receive local support under a holistic strategy led by local authorities. Tagging “Garden City” on the name of these areas of employment growth will prompt the view that these places should be built to high quality design standards. The deadline for local authorities to put in bids for support under the Government’s garden city programme has just passed. We will soon discover where such places may take off and make the crucial jump from ideal and vision onto the slow path to delivery.  In the meantime, let’s congratulate all those involved in this work for helping push forward a new era in the country’s urban development. The garden city is a good idea, now receiving cross-party support, which helps bring new impetus to the debate about where we should build the much needed homes for the future."
"
Share this...FacebookTwitterHerman Cain on the Mark Levin Show.
And at the 3:45 mark: “We know that those scientists who tried to concoct the science to say that we had a hockey stick global warning and they were busted because they manipulated the data.”

Share this...FacebookTwitter "
"
Share this...FacebookTwitterNews from Belgium…Yet another professor, Dr. Ir. Henri A. Masson, has resigned from yet another once prestigious organisation, which too has succumbed to the darkness of climate dogmatism and censorship.
In late August the Société Européenne des Ingénieurs et Industriels (European Society of Engineers and Industrialists – abbreviated  SEII) had organised a conference where scientists S. Fred Singer and Prof. Claes Johnson, of the Royal Institute of Technology in Stockholm, had been scheduled to speak on climate change.
I wrote about this here.
This all came to the attention of IPCC Vice Chair Jean-Pascal van Ypersele, who found that skeptic views have no place in the climate religion, and so moved quickly and demanded the SEII disinvite the 2 distinguished speakers. The conference had to be moved.
For SEII event coordinator Dr. Henri A. Masson, this closed-minded attitude by the SEII and van Ypersele became intolerable and so he has submitted his strongly-worded resignation, written in French below, and sent to me by e-mail  (Sorry, no translation in English. But use Google to get the gist of it).
==========================================================
A l’attention de: M. Philippe WAUTERS, Président
Objet: démission de tous mes mandats exercés au sein de la SEII et annulation de ma qualité de membre de la SEII.
Monsieur le Président,
Je viens de prendre connaissance du document officiel établi par le Secrétaire Général de la SEII relatif à l’affaire « Climategate », par lequel il informe les Administrateurs que le Bureau Exécutif, à une très large majorité, vous a réitéré sa confiance, malgré les évidences factuelles, que j’ai fournies antérieurement, qui établissent la réalité des mensonges que vous leur avez faits.
Il appert que, après l’avoir nié par écrit, vous avez bien dû reconnaître que vous avez agi suite à une intervention d’une « tierce personne », comme le qualifie pudiquement le Secrétaire Général dans sa lettre aux Administrateurs, cette intervention d’une tierce personne étant en fait une lettre de protestation du Professeur VAN YPERSELE. Pour moi, en clair, il ne s’agit de rien d’autres que d’avoir participé à un trafic d’influence, basé sur des déclarations diffamantes que vous n’avez même paspris la peine de vérifier, et vous avez bien cédé à des pressions externes à la SEII visant à censurer des intervenants défendant un point de vue opposé à celui de M. VAN YPERSELE et des instances qu’il représente.
Ces faits sont incontestables, quels que soient les arguments casuistiques que vous tentez de développer pour évoquer une faute de procédure grave que j’aurais commise. En l’absence de définition des limites extrêmement floues du mandat qui m’a été confié dans le cadre des activités de formation de la SEII, et plus précisément dans celles visant à animer un « café philosophique pilote consacré à la controverse climatique », je ne vois vraiment pas quelle est la procédure je n’aurais pas suivie, dans la simple exécution d ‘une activité récurrente de ce groupe de travail que j’anime depuis plus d’un an.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Lorsqu’il s’agit d’envoyer un agenda ou un procès-verbal de réunion d’un tel groupe de travail de la SEII, ou encore de tenter d’inviter de nouveaux membres à se joindre à lui, car c’est bien et uniquement de cela qu’il s’agit en l’occurrence, il me semble qu’il est de règle d’employer un papier à en-tête SEII à cet effet, et cela sans avoir à faire intervenir le Bureau à chaque fois; D’ailleurs, sans l’intervention de M. VAN YPERSELE, vous n’auriez plus que vraisemblablement rien trouvé à y redire.
Mais, évidemment, rien n’oblige le Bureau à rester cohérent et objectif dans ses jugements.
Les faits que je vous reproche sont de nature stratégique pour la SEII. Essayer de s’en disculper en utilisant des arguments de procédure spécieux ne vous grandit pas. Il aurait été beaucoup plus judicieux de reconnaître que vous avez été grugé par M. VAN YPERSELE, sur base de la réputation dontil jouit encore en Belgique, malgré ses liens avec la branche la plus radicale de Greenpeace. Preuves à l’appui, Je vous ai fourni l’occasion, pendant une semaine de revoir votre position; vous n’avez pas voulu la saisir.
Je ne peux donc que constater que ni vous, ni le Bureau Exécutif ne partagez un certain nombre de valeurs qui me sont chères et sur lesquelles je n’ai jamais transigé et ne transigerai pas à l’avenir.
En conséquence de quoi, je vous présente ma démission de toutes les fonctions que j’occupais au sein de la SEII. Je souhaite également ne plus figurer sur la liste des membres et ne plus recevoir vos mailings.
Je me réserve, par ailleurs, la liberté de plaider ma bonne foi, preuves à l’appui, dans l’affaire qui nous oppose, auprès des personnes et instances de mon choix.
Je vous prie d’agréer mes sentiments de circonstance.
Prof. Dr. Ir. Henri A. Masson
Cc: Administrateurs de la SEII
===============================================
Expect to see more resignations in the future as once respected societies keep taking up the practices of the Dark Ages.
PS: I don’t know what hurts the warmists the most: their blatant censorship of debate, or them showing up to debate? If it’s truly them showing up to debate, then their science has got to be embarrassingly bad.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterGerman online daily DIE WELT reports here on how Nestlé CEO Peter Brabeck-Letmathe says that the farming of biofuels dramatically exacerbates global hunger. Nestlé is the world’s largest food conglomerate. Hat-tip: oekowatch.org.
Recently a truly lame-brain study (later debunked here) was published claiming that global warming caused war. It would be advisable for the incompetent authors of that study to look at the connection between food prices and war instead.
Die Welt quotes Brabeck-Letmathe:
‘Through biofuels, we have returned hundreds of millions of people back into extreme poverty,’ he said in an interview with the Frankfurter Rundschau with regards to the hunger catastrophe in Somalia and rising food prices.
It is truly amazing that today more than one half of American corn and one fifth of the entire sugar cane harvest gets converted into biofuels while there is not enough food to feed humanity’.”
Brabeck-Letmathe says that the food shortage and spiralling prices are directly caused by the massive biofuel consumption and that the price spiral is leading to civil unrest in poor countries.
Biofuels were once enthusiastically supported by environmental groups, like Greenpeace,and were seen as a way of reducing human CO2 emissions, and thus curbing global warming. Governments worldwide are massively subsidizing the agriculture of biofuels. Farmers now prefer to grow crops for fuels rather than food for feeding the planet because it is simply more profitable.
Now hundreds of millions more people are starving needlessly.
Photo credit: Wikipedia
Share this...FacebookTwitter "
"

 _Every_ time there is some sort of weather disaster somewhere, someone blames it on human-caused global warming. Maybe not directly, but the implication is clear. “While we can’t link individual events to global warming, the increase of this type of event is consistent with our expectations, blah, blah…”   
  
Most recently this came in testimony from White House Science Adviser John Holdren before the Committee on Science, Space, and Technology of the U.S. House of Representatives:   




In general, one cannot say with confidence that an individual extreme weather event (or weather-related event)—for example, a heat wave, drought, flood, powerful storm, or large wildfire—was caused by global climate change. Such events usually result from the convergence of multiple factors, and these kinds of events occurred with some frequency before the onset of the discernible, largely human-caused changes in global climate in the late 20th and early 21st centuries. But there is much evidence demonstrating that extreme weather events of many kinds are beginning to be influenced—in magnitude or frequency—by changes in climate.



Holdren then goes to list a bunch of types of extreme weather whose characteristics have changed (remarkably, all becoming worse), adding that:   




There are good scientific explanations, moreover, supported by measurements, of the mechanisms by which the overall changes in climate resulting from the human-caused build-up of heat-trapping substances are leading to the observed changes in weather-related extremes.



Holdren’s implication is pretty clear—human-caused global warming is leading to changes in extreme weather. And just for good measure, he added this zinger:   




[I]t is reasonable to say that most weather in most places is being influenced in modest to significant ways by the changes in climate that have occurred as a result of human activities.



If this were the case, then there is a lot of good news to be found here, for by and large the weather is pretty good, with rare examples to the contrary.   
  
Take, for instance, what has been all abuzz this week in Washington, D.C.: how great the weather has been. The _Washington Post_ ’s Capital Weather Gang, which keeps close tabs on the pulse of D.C. weather, has commented repeatedly on how remarkable and enjoyable it has been. According to Holdren's logic, we have global warming to thank, and yet I have not seen one news story that links the pleasant weather to human-caused climate change.   
  
Across the country in Tucson, Ariz. (where I reside), the news this week has been dominated by the threat of the passage of the remnants of Hurricane Odile, which were forecast to move into the region from out of the Gulf of California. The predictions were for record-breaking rainfall amounts with the potential for widespread damage from flooding. The outlook stirred up memories of the passage of Tropical Storm Octave in 1983, which resulted in over $500 million (in 1983 dollars) of damage to the region. Thankfully, this did not come to pass. Instead, the heavy rains associated with Odile passed well east of the city, over much more sparsely populated country. Since apparently all weather is influenced by anthropogenic global warming, we have it to thank for averting what could have been a very costly and hugely disruptive situation affecting upwards of a million people.   
  
And speaking of hurricanes, the first major hurricane (category 3 or greater) in almost two years formed in the Atlantic Ocean. But, in encountering conditions arguably consistent with human-caused climate change, Hurricane Edouard quickly weakened and remained far out in the open Atlantic, steering well clear of the U.S. mainland. Major disaster averted. It has now been nearly nine _years_ since the last major hurricane made landfall in the United States, the longest such occurrence going back at least to the year 1900. Thanks, global warming!   
  
I could go on, because there are a lot more cases of non-extreme weather than there are of extreme weather, and as many or more cases to be made for weather catastrophes _averted_ by conditions “consistent with global warming” than caused by it.   
  
So if you want to play the all-weather-is-influenced-by-global-warming game, you are going to lose.   
  
Best bet would be to stick with the science, which for most types of extreme weather events and for most places indicates that a definitive link between event characteristics and human-caused climate change has not been established. Either talk about that situation or leave the attribution issue alone.


"
" Kabay Tamu slows his dusty white ute to walking speed on the dirt road that runs along the south-western shoreline of Warraber, a tiny coral cay in the Torres Strait that is home to about 250 people. “This was the best spot for a day out,” 28-year-old Tamu says, recalling his childhood.   Most of the beach where Tamu used to play is gone, along with several enormous wongai trees that were a barrier of sorts, protecting the dirt road and the nearby dam, which supplies the island’s drinking water, from the sea. Warraber is just 1.4km long, and half as wide, but shrinking fast. Some data suggests that sea levels in the Torres Strait could be rising at twice the global rate. Now, Islanders dump their green waste to hold back the rising sea. The Intergovernmental Panel on Climate Change (IPCC) estimates that by 2100 tides will rise 30–60cm with immediate cuts to carbon emissions, and 61-110cm without.  A 2010 report by researchers from James Cook University found that “continued erosion may necessitate further action in the medium-term” to protect Warraber’s dam. If saltwater breaches it, life would be difficult to maintain. “Being removed, forcibly removed, from here, just because of rising sea levels and the effect of climate change, and becoming climate change refugees, it’s just something that really haunts my mind,” Tamu says. He pulls his ute up on the north-eastern side of the island, where the cemetery and church are protected by a haphazard, failing defence made up of rocks, coral, split sandbags and tyres. Warraber man Danny Billy is worried that the waves will soon inundate his parents’ graves. He says he cannot bear the thought of leaving his ancestors behind. “It’s our right to be here, to have a voice, to be recognised, and to make others realise this is our home … We have the right to live a healthy life, here, on our island.” The Billy family also live on nearby Poruma island, where Danny spent part of his childhood. Just a 15-minute flight away, Poruma is smaller and thinner. On the western shore, a road and buildings are threatened, and 250 coconut trees – a source of food, shelter and leaves used in traditional ceremonies – have already been washed away. Local man Phillemon Mosby feels that loss keenly. The picturesque plantation should be a place to share with children and grandchildren, who would ordinarily take over the nurturing of the site. “That experience was taken away because of climate change, because of the rising sea levels. We’ve seen areas where we used to go fishing that are no longer there. We’ve seen rocks where people used to go diving that are covered.” Community elder Uncle Frank Fauid says being denied the opportunity to walk, garden and fish in peace on homelands would be devastating. “We can’t leave our community and go live down south. The living system … is totally not the way we live up here.” Tamu, Billy and Uncle Frank’s cousin, Nazareth Fauid, are among the eight Torres Strait Islanders who lodged a complaint with the United Nations human rights committee last May against the Australian government, alleging that its failure to reduce emissions or pursue proper adaptation measures across the region impedes their human rights to culture and life. Sophie Marjanac, a lawyer with environmental non-profit ClientEarth, is representing the group, who want the government to meet its targets under the Paris agreement, to reach net zero emissions by 2050 and to phase out thermal coal. In December, the federal government matched an earlier commitment of $20 million from the Queensland governmentto build new seawalls. But there is widespread scepticism among Islanders about when the new walls will be constructed. In early 2018, emergency funding of $650,000 was granted to Poruma to protect its western shore, but the wall was built using geotextile sandbags with a 50-year life expectancy, rather than the rock or brick asked for by the community. More work is required to protect the shoreline. Where the coast remains exposed, coconut trees lie on the beach, their roots slowly ripping away from the island. Other islands including Boigu, Masig and Iama need new seawalls. It is unclear which islands will be prioritised, and if the new funding will cover them all. Tamu is quick to point out that “sea walls are only to buy us time” – the best fix is emissions reduction. “The thing that got me was [the federal government] didn’t announce [the new funding] as seawalls to combat climate change. They said it was ‘an infrastructure development in the community’. They’re still trying to cover up climate change and the rising sea levels here.” Tamu gained international headlines when he asked prime minister Scott Morrison during the UN climate summit in New York last September to visit Warraber. He maintains that the damage visible on Warraber and other islands would shock them into action on climate and coal. “They would actually see for themselves how close the shoreline is to infrastructure and housing up here. And also, so a lot of the leaders can stop denying the connection between climate change and what’s happening to our planet.” The invitation was rejected via email in November, and Tamu says that the government is still “hearing, but not listening” when it comes to nationwide pleas for climate action. “We really want them to lead the way. If we change and go to more renewables and hit all the targets, it won’t change the world, it won’t change everything. But it will [mean] we’re leading the way and showing all the other countries that there’s another way to be.” While the UN complaint won’t be settled until 2021, Danny Billy says Islanders won’t stop making noise until Australia finally offers global leadership on climate change “We won’t stop until justice is served.” Travel and research was supported by funding from the Melbourne Press Club’s Michael Gordon Fellowship program. This story is co-published with The Citizen, a publication of the Centre for Advancing Journalism."
"
Share this...FacebookTwitterMichael Brzoska (Photo credit: NATO)
Folks, here’s a polite something to illustrate what climate science in Germany has decomposed to.
Here’s an interview with a seemingly true believer Michael Brzoska, scientific director of the Institute for Peace Research and Security Policy at the University of Hamburg and a principal investigator at the Integrated Climate System Analysis and Prediction’s (CliSAP) research group “Climate Change and Security”.
The CliSAP is a cluster funded with 32 million euros over five years and was started in October 2007.
As you read the interview you’ll quickly get the impression that Brzoska is convinced that since about 1900 all the world’s conflicts have increasingly been due to man-made climate change and will increasingly be so in the future. The way to stop war is to cut CO2 emissions.
Obviously he lacks historical literacy. History shows that cold periods led to food shortages, and thus uncontrollable social strife. But during warm periods, societies prospered. I’ll submit to Brzoska that war and conflict result much more from political and diplomatic failure by leaders, and much less from imagined man-made weather.
So indeed – isn’t climate change wonderful? Thanks to people like Brzoska, world leaders today have carte blanche to shirk all their responsibilities, completely and without any apprehension, and to blame everything (like war) on man-made climate change, i.e. their own citizens. But hey, maybe these thinkers are accomplishing something valuable with those €32 million they have been generously given.
Q: What do you see as CliSAPs largest achievements so far?
A: I have too little knowledge on most of the research topics and disciplines in CliSAP to answer this question with any confidence. My impression is that CliSAP has advanced quite a bit in its attempt to study climate change issues comprehensively. In social sciences, where I have a better judgment, main gaps remain, but CliSAP has clearly raised the interest of colleagues to get involved.”
Recall that the Cluster was started in October, 2007. So after 4 years he has “too little knowledge to answer this question with any confidence”? Could someone please tell me what we are paying them for? To raise interest of colleagues?
And now here’s Brzoska’s advice for young scientists:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Q: What constitutes “good” science?
A: Max Weber once wrote that it is the purpose of scientists to strive to disprove their own research results. Thus good science is critical science, including of what seems established even by oneself.”
Here I could not decide whether to laugh or to spill my dinner all over my keyboard.
Hint to Brzoska: That advice does not only apply to young scientists, but it also especially applies for the older ones too. Has Brzoska ever considered, for just a fleeting moment, that the true warmist believers could be, ahem, wrong? Seems he hasn’t.
And has he ever considered the horrific human consequences of massively and obstinately planning for the wrong scenario? Bear in mind we are not far from proving willful intent by scientists in producing wrong scenarios. There exists a massive amount of data that indicate things may very well turn out completely different than what they insist. Yet they refuse to acknowledge it.
His advice is of course correct. Science is about being open minded. Unfortunately too many climate scientists have been far too obstinate, elitist, overly pampered, corrupted and cemented deeply in dogmatism, and so the advice falls on many deaf ears. Indeed the greatest risk that global security faces today is not warming, but governments heeding the senseless advice that many scientists demand we accept without question.
Question for Prof Brzoska: 
What should society do with highly influential scientists who absolutely refuse to consider they may be wrong, obstinately insist no matter what that they are right, and actually spend an entire career propping up falsehoods? Should society resist? Well, resisting has led to things like the WBGU advocating a watering down of democracy.
Sorry Mr Brzoska, but your perspective of the world, and on the causes of war, really do frighten me. I expect they frighten many others too. And they ought to be frightened. After all, nothing is more dangerous than a science that becomes decoupled from reality and truth.
Welcome to modern climate science in Germany.
Share this...FacebookTwitter "
"
Share this...FacebookTwitter Some scientists and media have gotten much attention claiming that the world’s coral reefs could disappear in as little as 20 to 30 years – all because of humans consuming fossil fuels and whatever.
Now the Financial Times Germany reports on a study that claims this is all exaggerated.
The world’s largest coral reef off the east coast of Australia is not going to disappear as fast as once previously thought, according to a new study. Warnings that the Great Barrier Reef could die off due to climate change over the next 20 to 30 years are exaggerated says Sean Connolly of the James Cook University.”
This comes to no surprise for skeptics. How many millions of years and through what ranges of  temperature swings have the coral reefs survived so far? Indeed a few tenths of a degree Celsius of change over decades will have no impact on the reefs. And I seriously doubt the reefs are going to do what the models tell them.
The James Cook University Press release here says:
…some current projections of global-scale collapse of reefs within the next few decades probably overestimate the rapidity and uniformity of the decline.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Again, if the relatively sudden transition from ice age to optimum did not kill them, why would a few tenths of a degree over decades or centuries do it?
Wikipedia writes that coral reefs in the Persian Gulf have adapted to temperatures of 13 °C (55 °F) in winter and 38 °C (100 °F) in summer, i.e. 25°C change in 6 months. Like any species on the planet, reefs are always threatened by something. The press release writes:
However reefs are naturally highly diverse and resilient, and are likely to respond to the changed conditions in different ways and at varying rates.”
The James Cook press release, despite its obvious findings, still tries to convey an aura of alarm (for funding) yet admits that climate change is a natural process that has occurred time and again in the past.
Past extinction crises in coral reef ecosystems appear to coincide with episodes of rapid global warming and ocean acidification, they say. This has led some to predict rapid, dramatic, global-scale losses of coral reefs.”
The rapid changes they mention here were measured in degrees per decade and century, and not tenths of a degree as is the case with today’s relatively boring rate of change.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterRecently the German Weather Service (Deutsche Wetterdienst DWD) released its 100-year temperature prognosis chart to the public, claiming that Germans should expect more hot days in the future.
German Weather Service: More hot days in the future
DWD releases highly misleading 100-year outlook
Light blue line (before 2010): temperature record from 1881-2010
Yellow line (before 2010): 5th degree polynomial smoothing (annual mean temp.)
Orange line (after 2010): optimistic prognosis
Red line (after 2010): pessimistic prognosis
Not surprisingly, the chart has the typical catastrophic hockey stick shape. Peter Heller at Science Skeptical here closely examined the chart and found a number of deceptive irregularities that all serve to dramatize the future. As a result he dubbed it: a spot-the-errors diagram. 
Can you spot the errors? Peter Heller has.
========================================
German Weather Service Publishes A “Spot The Errors” Diagram
By Peter Heller
For some people, especially children, a spot-the-errors diagram is a lot of fun. But there is nothing funny about errors and inaccuracies in scientific diagrams. This at times is purposely done in political debates in order to hide just how poorly certain claims truly are. How diagrams can be used to mislead is well known. There are books and plenty of articles about it everywhere. Still, it’s attempted time and again, and often successfully. One particularly perfidious example has been produced by the German Weather Service (Deutsche Wetterdienst – DWD). The following graphic was brought to my attention by the European Institute for Climate and Energy (EIKE) in a recent article.

Figure 1: The DWD “spot the errors” diagram. (Click to enlarge)
Figure 1 shows the diagram with curves and three dots I’ve added to denote 1) the error, 2) the deception done on purpose, and 3) the inconsistency.

Dot no. 1: the error 
The DWD uses a 5th degree polynomial curve (yellow) to smooth Germany’s temperature record (annual mean 1881-2010) in order to clearly illustrate the trend. No question here – that’s perfectly okay to do. Figure 2 is my attempt to reproduce the curves and it shows a somewhat different polynomial trend curve, namely at the end it is flat and in no way increasing upwards as calculated by the DWD. The flattening of the curve is due to the comparatively cool 2010, for which the DWD calculated a mean temperature of 7.8° C. Using the polynomial curve that the DWD uses, I also get a flat end…that is when I use a temperature of 8.6°C for the year 2010. Probably an error.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Figure 2: My reconstruction. (Click to enlarge)
Dot no. 2: the deception
Of course the purpose of the graph is to show the horrific climate change that lies ahead for Germany. This becomes especially pronounced when one draws curves that rise upwards as dramatically as possible after 2010. To amplify the the rate of increase, one simply employs a more effective horizontal scale. Hey, who is going to notice? And so after only 40 years the curves shoot up into an apocalypse. But notice how the horizontal axis shows the year 2100. Maybe DWD just wanted to make some room for the thermometer to make the chart look prettier. In the end, this ends up being an intentional deception in order to make the future look more dramatic.

Figure 3: Graphic using a correct scale for the future. (Click to enlarge)
Dot no. 3: the inconsistency
In Figure 3 I simply superimposed my diagram over the original DWD diagram and gave the chart the correct scale on the right side of 2010. You can already see the difference in both polynomial curves (yellow). The gray line is the linear trend (which the DWD opted not to show) that allows a prognosis. Suddenly much of the drama gets lost. Here one must recall the following: A prognosis is always the extrapolation of a known trend. One can make a prognosis completely independent of having knowledge of the fundamentals behind the trend. And in the following case, making a prognosis would simply mean extending the fluctuation of the annual mean temperature (yellow line) about the gray line. The orange and red lines in Fig. 3, on the other hand, are DWD projections and not prognoses.
To do these projections, models were used that most likely are in accordance to what is propagated by the IPCC, see Figure 4. In any case, there is good agreement with the calculations used in the recently DWD published “Regional Climate Atlas“, which is green in the graphic. The “Klimaatlas Deutschland” has quite identical results and can be seen at the DWD website.

Figure 4: Flattening already in 2100? (Click to enlarge)
 
Interestingly the course of the DWD projection between 2011 and 2100 in Figure 1 is completely different than what is shown in Figure 4. It is concave, turning sharply upwards, and not convex. I would at least call this an inconsistency.
We can summarize as follows: The DWD, as a public institute directly subordinate to the German Federal Government (Ministry of Transportation, Construction and Urban Development), believes it is correct and important to take a clear position in a highly political issue in a press conferencez. And to support this position it resorts to using a diagram that has errors, deceptions and inconsistencies that contradict to its own calculations. All of these are perfidiously placed in the diagram in a way they are not easily detectable. One indeed has to look very closely. All three of the elements happen to dramatize the situation. That’s not a good way to build trust.
Peter Heller
(Translation/editing by P Gosselin)
=================================================
Peter Heller points out that he does not value it as very important, and adds. “Handling of diagrams of all kind in this way happens everywhere and every time – especially in climate related discussions.”

Share this...FacebookTwitter "
"

Weeks after the National Science Foundation released a report about the connection between increases in atmospheric carbon dioxide and the acidity of the oceans, doomsayers continue to prophesy that global warming will kill the coral reefs off our picturesque Florida coast.



The NSF study, released with two other federal research entities and entitled “Impacts of Ocean Acidification on Coral Reefs,” landed with a thud, and it is remarkable how the press has received it. Writers have editorialized about it, literally with one voice, without any critical fact‐​checking. In a July 11 editorial, the editors of the _Cincinnati Post_ wrote, “This report is a fraction of the available evidence indicating anthropogenic climate change.…The evidence is clear and convincing. The global‐​warming critics are neither.” On July 12, the _Albuquerque Tribune_ , in its own in‐​house editorial, printed the same words (without attribution).



It could have done something more original and scrutinized the NSF report. There’s a major problem with it, right at the beginning. Its first paragraph states correctly that, as a result of the burning of fossil fuel and other activities, atmospheric carbon dioxide concentration is rising. From there, however, the report loses its way. “Rates of increase,” it says, “have risen from 0.25% [per year] in the 1960s to 0.75% [per year] in the last five years.”



Really? The standard reference for atmospheric carbon dioxide concentration is that registered at Mauna Loa Observatory, beginning in 1958. The average rate of change in the 1960s was 0.30% per year, and in the last five years, it was 0.55%. This last value is not statistically distinguishable from the average rate for the past 25 years. The real change from the 1960s to the last five years is 0.25% per year, while the NSF‐​sponsored report gives it as twice that.



The precise figure is important, because the rate of increase of atmospheric carbon dioxide is directly related to the amount of warming it creates and to changes in the acidity of the oceans; computer models using a carbon dioxide increase rate twice that which is observed show twice as much warming. And that is precisely what has occurred: there are now four separate, taxpayer‐​supported reports “intercomparing” the dozens of climate models for global warming that have evolved in recent years. Each one uses a carbon dioxide increase of 1% per year, or twice the real rate. Ever wonder why they predict so much warming?



It gets better (worse). The coral report then states that “The current atmospheric CO2 concentration…is expected to continue to rise by about 1% [per year] over the next few decades.”



“Continue”? The average increase for the last decade was 0.49 per year, for the decade before that was 0.42%, and for the decade before that was 0.43%. Again, about half of what the report expects to “continue.”



The current concentration of carbon dioxide in the atmosphere is about 380 parts per million. Before we industrialized — back when life expectancy was in the 40s — the concentration was about 280.



Fewer than 100 million years ago, or 400 million years _after_ corals first arose, the carbon dioxide concentration was a bit less than 3,000 ppm. Around 175 million years ago it was pushing 6,000. If there was that much more carbon dioxide around, the oceans would have been that much more acidic, which would have killed the corals. And yet they lived.



How does the report take this problem into account? It balances the increase in acidification that these concentrations of carbon dioxide would bring about with some countervailing change in its opposite, alkalinity. So the report speculates that “ocean alkalinities _could_ have been higher during periods with high CO2 levels.” (Emphasis added.)



Then there’s the problem of identifying a definite decline in corals. The report says that it is “difficult” to find this effect, and that “on average” it does not exist, because the rates of coral growth are controlled by many other factors that are apparently obscuring their decline.



How on earth did all of this make it through peer review? Or do we no longer care enough to get the facts right before expressing opinions under the mantle of scientific authority?



To many editorialists, when it comes to global warming, facts don’t matter. But here are a few: corals have been around for half a billion years, on a planet that was much, much warmer, had much more carbon dioxide in its atmosphere, got hit by an asteroid or two, experienced ice ages and is now in the midst of a slight warming trend. You can bet that they’ll be around a long time after humans have come to the end of the evolutionary road.
"
"**A nurse who struggled to get face masks to fit her has inspired the design of custom-fitted ones for frontline healthcare workers.**
Gareth Smith set up MyMaskFit after his wife, intensive care nurse Valerie Bednar, struggled to find a filtering face mask (FFP) to fit her.
Based in Swansea, the firm is working with a number of UK universities.
Ms Bednar said the masks are reusable so will reduce stress among staff and be better for the environment.
""I'm one of the people the standard disposable FFP3 masks doesn't fit my face,"" said Ms Bednar, who worked at Morriston Hospital in Swansea at the start of the pandemic but is currently on maternity leave.
""It was just the stress of trying to do what you need to do - the reason we go into nursing is to take care of people, and then the added level of 'am I being safe and do I have the protection that I need?'
""That uncertainty I think was stressful for everyone.""
The company hopes to further develop a prototype designed by researchers at Birmingham University and King's College London.
Swansea University's School of Engineering will help test and manufacture the face mask, which it is hoped will be available to the NHS in Wales in the new year.
MyMaskFit said it is aiming to become the first to make a fully custom-fitted, reusable, filtering face piece masks made to a medical grade standard in the UK.
""We want to make a reusable mask so that staff can feel confident when they come in for their shift it will be there,"" Ms Bednar explained.
""You're involved in cleaning it and owning it - all of that gives people the sense of security and protection.""
To speed up the design process and to achieve a seal which will fit anyone, the company has launched an app which will scan the face and send the data for a mould to be created and 3D printed.
MyMaskFit technology director Paul Perera said current masks vary widely in terms of design.
""There is an inevitable variation in the shape of human faces, and BMA surveys have shown that over 20% of hospital doctors have to try one or more masks to find one that fits,"" he said.
Mr Perera said the firm was also working on a face mask which is made with ""renewable plastics that are transparent"" to aid communication.
He added: ""We're also using a copper, embedded into the plastics, which kills the virus. Therefore the masks can be reusable and therefore more sustainable for the environment.""
The initial manufacturing process and further testing of the prototypes will take place at Swansea University."
"It’s been suggested that a recent fall in recycling rates is due to green fatigue, caused by the confusing number of recycling bins presented to householders for different materials. Recycling rates would rise, according to waste recycling firm Sita UK which carried out the survey, if the process was simplified, for example by collecting all waste for recycling together (known as co-mingling). But the same survey also found that reduced glass and paper consumption had led to a drop in the volume of those materials collected. Perhaps this is the underlying issue and not “green fatigue”. It is well known that fewer people are buying newspapers and that many products previously sold in glass, such as milk, are now sold in plastic. Thus, lower consumption of products in previously widely recycled containers leads to less waste and may be causing lower recycling rates. So is this really a cause for concern, if the drop in recycling rates is down to there being less waste generated in the first place? Well, yes, if this means the 2020 target of 50% household recycling is not met. In that case, the UK government will be fined, and this may affect householders indirectly through higher taxes or lower provision of public services. However, this concern may be misplaced and perhaps is more to do with the way the target is defined, rather than recycling levels.  The relationship between recycling and the waste we generate implies that a drop in the amount of waste produced or an increase in less-recyclable material will lead to a fall in recycling volumes and possibly a drop in the recycling rate. The former should be recognised as a welcome development, whereas the latter reflects changing patterns of consumption. This should prompt new innovation in the waste sector to deal with these types of waste. For example, by developing improved recycling methods or technology to deal with different types or combinations of materials.  So the current preoccupation with the headline recycling rate may be unhelpful. If the concern is the impact that excessive waste is having on the environment, then less recycling due to less waste being generated in the first place should not be a cause for concern. If the drop in recycling reflects changing consumption patterns, any penalties should be aimed at generating incentives to innovate in terms of packaging and new and better ways to deal with different types of waste. We need better analysis to identify the true causes of any change in recycling patterns, and the legislation – designed to drive greater recycling – should recognise why these two situations are different. It may be that green fatigue is one cause among several. To the extent that a lack of information or sheer inconvenience exacerbates the problem, there is evidence to show that improving both these aspects does promote recycling – particularly among those whose attitude is not especially pro-environmental. On the other hand, if the only means to make recycling more convenient is through co-mingling, there is the valid point that the risk of contamination is high and levels of useful recycling may actually fall.  Research indicates that there are several factors that underpin the rate of recycling, from the way the service is provided, to whether recycling is considered a social norm among families, communities or groups banded by age, ethnicity or location. Perhaps the role for government in tackling any dip in the recycling rate is to highlight the prevalence of recycling among certain groups as a way to demonstrate the existence of that recycling norm – and by doing so, encourage it in others."
"**Here are five things you need to know about the coronavirus pandemic this Wednesday morning. We'll have another update for you at 18:00 GMT.**
We now know that three households of any size will be able to get together for five days over the festive period and travel restrictions will be lifted to allow people to move freely across the UK. The leaders of all four nations, though, have issued pleas for caution, especially when it comes to vulnerable relatives. Some scientists have warned the relaxation could spark another wave of infections and further deaths, but ministers hope with compliance up to and after the window the impact can be minimised. Read the rules in detail and our guide to the sort of questions to consider if you're deciding whether to get to together.
The chancellor will explain later how he hopes to protect jobs and help the economy recover from the devastating impact of coronavirus. Rishi Sunak will announce more money for the coming year for the NHS and schools, but less for overseas aid and possibly the majority of public sector workers, in the form of a pay freeze. We'll also get official forecasts about how long the impact of the pandemic will be felt. We'll bring you his speech live at 12:30 GMT and help make sense of it afterwards. In the meantime, read more about why it matters to you, and four things to look out for in particular.
A shortage of personal protective equipment at the start of the pandemic led to the government paying Â£10bn more to secure sufficient supplies than if they'd bought it a year earlier. Spending watchdog the National Audit Office said not enough PPE had been stockpiled and unprecedented demand led to very high prices. The government said the report acknowledged NHS providers had been able to get what they needed - although the NAO heard feedback from staff who believed they were ""not adequately protected during the height of the first wave"".
For many victims of domestic abuse coronavirus has only worsened their suffering - trapped at home with their tormentors, cut off from family, friends and colleagues. About two thirds of women in England living with domestic abuse told Women's Aid their ordeal had got worse during the UK's first lockdown. The UN is calling it the ""shadow pandemic"" and it's a focus of BBC 100 Women this year. Read the story of one woman, Victoria, who managed to escape. See the full list of 100 Women, including those making a difference to those suffering from abuse around the world.
From Christmas bubbles to the Bake Off bubble... the contestants on this year's show stayed together for the duration of filming and provided a much-needed lift to many of our spirits over the last 10 weeks. The final even saw the creation of a ""Bonkers Bake Off bubble cake"". This year's winner has now been crowned, but we won't reveal who it is here, just in case. Click through to find out.
Get a longer news briefing from the BBC in your inbox, each weekday morning, by signing up here.
Find more information, advice and guides on our coronavirus page.
Plus, the BBC News website's health editor Michelle Roberts explains why we should have confidence in the safety of any coronavirus vaccines given the final go-ahead.
**What questions do you have about coronavirus?**
_ **In some cases, your question will be published, displaying your name, age and location as you provide it, unless you state otherwise. Your contact details will never be published. Please ensure you have read our**_terms & conditions _ **and**_privacy policy.
Use this form to ask your question:
If you are reading this page and can't see the form you will need to visit the mobile version of the BBC website to submit your question or send them via email to YourQuestions@bbc.co.uk. Please include your name, age and location with any question you send in."
"
Share this...FacebookTwitterHere’s something you won’t find in the MSM, but thanks to NoTricksZone, a few of us out there will hear about it. (Just imagine if they had been tea-partiers).It’s official: Some of Greenpeace’s members have found guilty. Eleven activists from Britain, Denmark, the Netherlands, Norway, Spain, Sweden, Switzerland and the United States were found guilty today in Denmark of falsifying documents and number plates, this according to Univision.com here. They were sentenced to 14 days suspended prison terms.
On 17 December, 2009, the head of Greenpeace Spain Juan Lopez de Uralde and Norwegian Nora Christiansen crashed the Danish parliament’s security and unfurled a banner with the words: “Politicians Talk, Leaders ACT”.
Yet, the impatient radical activists ought to be happy with the judgement, as they got off easy. Univision.com writes:
In its ruling, the court explained the low sentences by “the character of the circumstances on the one hand, and on the other, the fact that the transgressions were part of a peaceful political happening with the goal of causing debate,” pointing out that “no particular serious rights were infringed upon.”
The goal of causing debate? Right. The last things these kooks want is debate. These are dogmatists who are incapable of listening to other opinions, they deny contradictory data,  and demand that their views of the world be enacted as law immediately. Democracy for them is a just a bothersome obstacle. Luckily democracy is still keeping these loons in check.
Feeling unappreciated and peeved, the planet-saving López de Uralde added:
We were treated almost like terrorists.”
The court also found the Greenpeace Nordic wing guilty of organising the demonstration and fined the group  $15,000.
Of course don’t expect them to be repentant now that they’ve been slapped on the wrist. They believe their actions were justified. Just listen to former Juan López de Uralde, as quoted recently by the leftwing online German Tageszeitung (TAZ) here:
The whole process was completely over the top. and a bit surreal. The worst is: Although the most recent data on climate change are alarming and emissions have increased since the failed summit in Copenhagen, rising 5% just last year, massive action is being taken against activists. There’s a huge and obvious contradiction between the will to take on climate change and the persecution of activists.”
Share this...FacebookTwitter "
"
The Warning in the Stars
By David Archibald
If climate is not a random walk, then we can predict  climate if we understand what drives it.  The energy that stops the  Earth from looking like Pluto comes from the Sun, and the level and type  of that energy does change.  So the Sun is a good place to start if we  want to be able to predict climate.  To put that into context, let’s  look at what the Sun has done recently.  This is a figure from “Century to millenial-scale temperature variations for  the last two thousand years indicated from glacial geologic records of  Southern Alaska” G.C.Wiles, D.J.Barclay, P.E.Calkin and T.V.Lowell 2007:

The red line is the C14 production rate, inverted.  C14 production is  inversely related to solar activity, so we see more C14 production  during solar minima.  The black line is the percentage of ice-rafted  debris in seabed cores of the North Atlantic, also plotted inversely.   The higher the black line, the warmer the North Atlantic was.  The grey  vertical stripes are solar minima.  
As the authors say, “Previous  analyses of the glacial record showed a 200- year rhythm to glacial  activity in Alaska and its possible link to the de Vries 208-year solar  (Wiles et al., 2004). Similarly, high-resolution analyses of lake  sediments in southwestern Alaska suggests that century-scale shifts in  Holocene climate were modulated by solar activity (Hu et al., 2003).  It  seems that the only period in the last two thousand years that missed a  de Vries cycle cooling was the Medieval Warm Period.”
The same periodicity over the last 1,000 years is also evident in this  graphic of the advance/retreat of the Great Aletsch Glacier in  Switzerland:

The solar control over climate is also shown in this graphic of Be10 in  the Dye 3 ice core from central Greenland:

The modern retreat of the world’s glaciers, which started in 1860,  correlates with a decrease in Be10, indicating a more active Sun that is  pushing galactic cosmic rays out from the inner planets of the solar  system.
The above graphs show a correlation between solar activity and climate  in the broad, but we can achieve much finer detail, as shown in this  graphic from a 1996 paper by Butler and Johnson (below enlarged here)::

Butler and Johnson applied Friis-Christensen and Lassen theory to one  temperature record – the three hundred years of data from Armagh in  Northern Ireland.  There isn’t much scatter around their line of best  fit, so it can be used as a fairly accurate predictive tool.  The Solar  Cycle 22/23 transition happened in the year of that paper’s publication,  so I have added the lengths of Solar Cycles 22 and 23 to the figure to  update it.  The result is a prediction that the average annual  temperature at Armagh over Solar Cycle 24 will be 1.4C cooler than over  Solar Cycle 23.  This is twice the assumed temperature rise of the 20th  Century of 0.7 C, but in the opposite direction.
To sum up, let’s paraphrase Dante: The darkest recesses of Hell are  reserved for those who deny the solar control of climate.
This essay is also available in PDF form: TheWarningintheStars


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8dbeb0d9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Billions of small, jellyfish-like creatures known as “by-the-wind sailors” have washed ashore all along the west coast of North America this summer, from southern California to British Columbia. Images of vast swarms of electric-blue sails covering the ocean’s surface and littering the sand are indeed spectacular, but people might well wonder what exactly these strange-looking beings actually are. And this of course leads to the next question – should we be afraid of them? Velella velella (to give them their scientific name) are often assumed to be a type of jellyfish but, while biology does lump them in with jellyfish, sea anenomes, and corals in a group known as Cnidaria, Velellas are not all that closely related to the common or moon jellyfish, Aurelia aurita. Cnidarians have two body forms: the umbrella-shaped, tentacle-trailing “medusa”, your classic jellyfish; and “polyps” such as seas anemonies that typically live attached to the seabed. Velella is a colony of specialised individual polyps, much like their fellow sailors the Portuguese Man o’ War. Instead of living attached to rocks on the seabed, the water surface has become its substrate. The by-the-wind sailor’s body is a flat oval disk 6-7 cm in diameter containing a series of air-filled chambers that provide buoyancy. Below hangs a central mouth surrounded by specialised reproductive bodies that produce tiny medusae, little “jellyfish”, and stinging tentacles – which are harmless to humans.  Projecting vertically up is a stiff translucent triangular vane made of chitin, a substance derived from glucose that is also used in crab and insect skeletons or squid beaks. This vane acts like a small sail. Interestingly, the sail runs diagonally across the top of the float, so that the individual sails at a 45 degree angle to the prevailing wind, just like a sailing boat.  Another striking feature is the bright blue colour, which is thought to serve as camouflage and/or protection from the sun’s rays. Animals that wash up on the beach dry up and become bleached white within a day or two. Velella velella use their stinging tentacles to capture and feed on small fish larvae and zooplankton – microscopic animals that drift in the sea. But this is not their only source of food. If you look closely, you will also see a golden-brown colour inside the tissues which are zooxanthellae – symbiotic photosynthetic microalgae – that provide the host animal an additional source of nutrition. By-the-wind sailor is a very common open ocean organism, living in warm to warm-temperate waters throughout the world’s oceans. It is thought that there is a difference in preferred sailing direction in the northern and southern hemispheres, and on the eastern and western shores of oceans, but this has been hard to prove.  Nevertheless, research suggests that California Velella have a sail which is angled to the right of the main axis. This means that as the wind pushes it along, Velella tacks to the right of the northwesterly prevailing wind and so these animals are usually kept offshore. Occasionally winds come from the southwest so that populations get blown ashore, as in the recent cases in the US.  Similarly, there have been years when large numbers were blown onto the southern coast of the UK, particularly following strong southwesterly winds blowing off the Atlantic. The fact is that every spring and summer, millions of these strange creatures are blown ashore on the west coast of America. But this year, the numbers have been much greater and the strandings even more impressive.  One reason for this is that storms in the eastern Pacific are likely to have blown the Velella on to the beaches. California’s beaches recently saw their largest swells since 1997, as surfers rode monster waves caused by tropical storms hundreds of miles out to sea.  Warmer waters associated with a build up to an El Niño year could have stimulated greater production of new baby Velella out in the mid ocean. Jellyfish and their relatives are all very flexible and are able to rapidly take advantage of favourable conditions. They are relatively short-lived, less than a year, can grow and reproduce very quickly, and produce large numbers of offspring. Therefore when conditions are ripe – waters are warm and food is plentiful – their numbers can suddenly erupt.  The occurrence of “good years” and “bad years” is common across all jelly-like creatures. This year in the UK, for instance, there have been reports of large numbers of Barrel jellyfish sightings along the southwest coast. While this is not unique, certainly these sorts of numbers had not been recorded in that part of the English Channel for a considerable number of years. Like Velella velella, barrel jellyfish mainly live offshore, and it is thought that the very warm spring and early summer coupled with altered water currents enabled large numbers to move in closer to land.  In the Bering and North Seas, where scientists have recorded jellyfish numbers over time, we know that fluctuations have been caused by changing sea temperatures, food availability and long-term climate cycles. At a global scale, analysis carried out by researchers in the Global Jellyfish Group has also revealed large-scale oscillations in the presence of jellyfish and jellyfish-like creatures over the decades. It’s a boom and bust existence. Many are worried that these “jellyfish” blooms are likely to become more common as a result of human-induced climate change, and there may be some truth in this. Huge blooms of giant jellyfish in Japan, or the mauve stinger in the Mediterranean, have indeed become more frequent in recent years, harming tourism, fisheries and aquaculture, and power plants (jellyfish have a habit of clogging up nuclear reactor cooling pipes). But this is not a universal trend; jellyfish aren’t about to take over the world, and neither are their sailor cousins."
"Almost half of the world’s sandy beaches will have retreated significantly by the end of the century as a result of climate-driven coastal flooding and human interference, according to new research. The sand erosion will endanger wildlife and could inflict a heavy toll on coastal settlements that will no longer have buffer zones to protect them from rising sea levels and storm surges. In addition, measures by governments to mitigate against the damage are predicted to become increasingly expensive and in some cases unsustainable. In 30 years, erosion will have destroyed 36,097km (22,430 miles) or 13.6% of sandy coastlines identified from satellite images by scientists for the Joint Research Centre (JRC) of the European commission. They predict the situation will worsen in the second half of the century, washing away a further 95,061km or 25.7% of Earth’s beaches. These estimates are far from the most catastrophic; they rely on an optimistic forecast of international action to fight climate breakdown, a scenario known as RCP4.5. In this scenario of reduced ice-cap melting and lower thermal expansion of water, oceans will only have risen by 50cm by 2100. However, if the world continues to emit carbon at its current rate, sea levels will rise by an estimated 80cm, according to the Intergovernmental Panel on Climate Change. If this happens, a total of 131,745km of beaches, or 13% of the planet’s ice-free coastline, will go under water. Around the globe, the average shoreline retreat will be 86.4 metres in the RCP4.5 scenario or 128.1 metres in the high-carbon scenario, though amounts will vary significantly between locations. Flatter or wilder coastlines will be more affected than those where waterfronts are steeper, or those artificially maintained as part of coastal development. In the best-case scenario, the UK will lose 1,531km or 27.7% of its sandy coast, and 2,415km (43.7%) in the worst case. Australia (14,849km lost) and Canada (14,425km) are predicted to be the worst-affected countries, followed by Chile (6,659km), Mexico (5,488km), China (5,440km) and the US (5,530km). The Gambia and Guinea-Bissau have short coastlines, but both are predicted to lose more than 60% of theirs. The study predicts that the hardest-hit areas in the UK will be west Dorset, north Devon, Great Yarmouth, Barrow-in-Furness and north-east Lincolnshire. In these areas, beach retreat is predicted to be five times the national average. “The length of threatened seashores incorporates locations that will be submerged by more than 100 metres, assuming there are no physical limits to potential retreat,” said Michalis Vousdoukas, an oceanographer at the JRC and lead author of the study, published in the journal Nature Climate Change. “Our 100-metre threshold is conservative since most beaches’ width is below 50 metres, especially near human settlements and in small islands, such as the Caribbean and the Mediterranean.” Large beaches will narrow by 100-200 metres on Atlantic and Pacific coasts and the Australian side of the Indian Ocean, wiping out more than 60% of sand deposits in a number of developing countries that are economically fragile and heavily dependent on coastal tourism. But swift action to limit emissions and fight climate breakdown could help reduce the impact, experts say. “Moderate emissions mitigation could prevent 17% of the shoreline retreat in 2050 and 40% in 2100, thus preserving on average 42 metres of sand between land and sea,” Vousdoukas said. The researchers projected the future anthropogenic and geological changes based on 30 years of observations. Sea-level rise is exacerbating problems caused by construction and barriers on the shoreline such as buildings, roads or dams, which have changed the natural replenishment cycle of sandy beaches. “In the UK, part of manmade erosion results from protecting cliffs whose wearing would normally top up the associated beaches with gravel,” said Robert Nicholls, the director of the Tyndall Centre at the University of East Anglia in Norwich. “This happens, for example, in Bournemouth, to safeguard luxury properties built on top of fancy viewpoints.” In some regions such as the Baltic, marine erosion is compensated by land rise. Sediments may also be brought by rivers, either naturally as in the Amazon, or resulting from artificial activities as in the Chinese deltas that accumulate residues from industrial sites upstream. A third driver of erosion is the intensification of storms, which is associated with climate breakdown. These look on course to further erode the most vulnerable beaches; the study predicts that the British seashores facing most erosion are the east and west coasts, which are more exposed to tidal surges than the south. By the end of the century up to 63% of low-lying coastal regions worldwide will be threatened. In these areas, both population density and development tend to be higher than inland. “Seaward human expansion will continue, mostly in unspoiled coastlines that are particularly extensive in Asia and Africa,” Vousdoukas said. “Adaptive measures are urgently needed.” In many places, the cost of protecting the shoreline often outweighs the benefits. For example, in 2017 a £62m sea wall was built to protect the tourist resort of Blackpool. Besides requiring indefinite spending on maintenance, such concrete defences are seen as more of a problem than a solution as they can disrupt the process by which sand is deposited by ocean currents, exacerbating erosion. In some places the Environmental Agency has chosen to replenish beaches with sand dredged offshore. Not only is this harmful to marine habitats, mining sand from the seabed is expensive. Since 1994 millions of pounds have been spent yearly to replenish the 20km seafront between Skegness and Mablethorpe in Lincolnshire, also helping to preserve 35,000 hectares of farmland. Dr Sally Brown, deputy head of life and environmental sciences at Bournemouth University, said: “Building defences helps maintain coastline position, but defences are known to reduce beach width or depth over multiple decades. Responding to sea-level rise means looking strategically at how and where we defend coasts today, which may mean protecting only limited parts of the coast. “Beach nourishment schemes can help the problem, such as in Bournemouth, but these beaches need a regular top-up. Ultimately, we cannot nourish everywhere for ever, meaning that hard decisions need to be made about how much to spend and how to manage the coast in decades to come. This could affect those living on the coast, and tourists who enjoy the sandy beaches too. Sea-level rise will only make this situation worse.” • An interactive map showing beaches that will lose or gain ground can be found here."
nan
"It is great news that the National Trust has bought Hambledon Hill, a Iron Age hill fort in southwest England, for the nation. Now the expertise of its in-house archaeologists and conservators can be used to preserve this enormously important site.  Why is this hill north-west of Blandford in Dorset so important? It is a site that has been inhabited throughout many periods of early British history, from the early Neolithic (Stone Age), through the Bronze Age, Iron Age, and into the contemporary (by comparison) record of the Romano-British and later Anglo-Saxons. After farming arrived in Dorset and Somerset around 4,200BC, an agricultural society developed in the vicinity of Hambledon Hill over five centuries before the immense undertaking of building the enclosures began that would transform the hill into a fort. The complex was inhabited for 400 years, determined by radiocarbon dating of 160 carefully selected samples. The earliest stage of construction was the main enclosure, dated to around 3,680-3,630BC. A sequence of four periods of building activity followed, with subsequent extensions to the east, south and west continuing through 3,350-3,310BC. The Hambledon Hill fort is a physically impressive, societally significant construction that initially faced east across the River Iwerne onto Cranborne Chase and provided defence from that direction. After a period during which the fort appears to have been attacked on at least two occasions, with young men killed by arrowshot, outworks were built to the west that reversed the role of the site. The hill changed from providing defence to the east, and impressing those living there, to asserting power towards the west and the Vale of Blackmore. This may be the first glimpse of shifting power politics in early Britain, more than 5,000 years ago. At every stage of the detailed investigations of Hambledon Hill, the conclusion has been that while the people had come from farming settlements, Hambledon stood on marginal land, still forested, and certainly not intensively farmed. This was the natural habitat of red and roe deer, marten and badger, whose remains are found on the site. This was border country between populations, relatively remote from settlement. The evidence suggests periodic, probably seasonal, visitation by large groups of people. They brought with them cattle, pigs and sheep for slaughter and feasting, and ready-processed grain and other foods. Judged by the imported goods, these visitors came from the north and west, from the river Severn and as far as Devon if not further. Prestigious objects are largely found in the main ritual enclosure, where pits occur with rich deposits. The ditches of this enclosure and its long barrow were repeatedly recut to deposit food debris and objects. Human skeletons occur frequently, sometimes with traces of exposure and defleshing, and skulls are also frequently found. Recurring patterns are everywhere – an extreme example being the burial of two children, each with a possibly inherited skull deformity, in the same segment of ditch but several generations apart. Once the site was deserted, the Beaker people that came around 1,000 years later recognised and marked the then defunct enclosures. Early Bronze Age fields were laid out, and a Middle Bronze Age settlement was built atop one of the Neolithic enclosures. Later Bronze Age burnt mound activity took place in the Iwerne valley. Eventually, sometime in the middle of the first millennium BC, there was again a need to create an easily defensible fortified habitable centre that could accommodate a large number of people, if only for psychological reasons. Such a fortified centre was built on the northern point of the spur of Hambledon Hill around 600-700BC.  This fortification was apparently succeeded by the main Iron Age phase of the occupation and fortification of Hambledon Hill’s north spur when a defensive ring of multiple ramparts (multivallate) was built on the hill. It remains massively impressive to this day, with highly skilled engineering required to achieve this daunting effect. Some time around 600BC hundreds of relatively tiny round houses were built, seven to nine metres in diameter, originally probably with conical thatched roofs, on carefully dug platforms installed throughout the encloure, until the entire interior was filled with perhaps four or five hundred.  Houses so closely organised and spaced can only have been occupied for a short time. Was this another seasonal meeting place, as it has been suggested the hill was in Neolithic times, 3,000 years earlier? Designed to impress those in the countryside around, it may have heralded the end of a period of relatively untroubled individual farming and the return of more fraught, difficult circumstances.  This fortress-cum-ceremonial centre may have lasted some time before a less remote and more accessible centre was required, probably built on Hod Hill immediately to the south. Later still a Romano-British field system covered the hill, and an Anglo-Saxon cemetery was set by the parish boundary on the spur towards Stepleton Spur. Oliver Cromwell’s roundheads fought a skirmish here in 1645 against local villagers calling themselves the Clubmen who had banded together against the looting by troops of both sides of the Civil War. And a century later, Colonel (later General) Wolfe exercised his troops there before going onto scale the Heights of Abraham in the Battle of Quebec. Hambledon Hill is a beautiful part of the Dorset landscape, rich in plants, flowers and wildlife. But it is also a precious looking glass into 5,000 years of British history – hopefully now preserved in perpetuity for the nation."
"Many species of flower-visiting insect are in trouble in Britain, according to a new report from the Centre for Ecology and Hydrology (CEH) near Oxford, which drew on almost 750,000 observations of insects between 1980 and 2013. The study used population records of 353 wild bee and hoverfly species over large areas of Great Britain to show that one third of these pollinating species declined in range during this time. Most of these losses were in species that were already relatively rare. Some big losers were the red-shanked carder bee, the smooth-gastered furrow bee and the large shaggy bee, all of which had vanished from around half of their previous locations in 1980.  However, the same report also found that other species of bee and hoverfly, about 10% of the total, actually increased. Some of these, like the ashy mining bee and the lobe-spurred furrow bee, are pollinators of field crops like oilseed rape. These two species increased their ranges five-fold during the same period, suggesting that crop-specialist species are thriving at the expense of most others.  The other winners were actually invaders. The ivy bee – most often seen on the plant of the same name – only colonised mainland Britain in 2001 and the range over which it can be found has been expanding by 16% every year since. Despite what may appear to be a mixed bag, the overall diversity of British pollinator species has fallen steadily since 1980. The new study underlines the already alarming downward trend in insect numbers seen in several other studies conducted in the UK, Germany and Central America. In February 2019, a report claimed that current rates of decline might lead to “the extinction of 40% of the world’s insect species over the next few decades”. This almost apocalyptic claim was rapidly taken up by the world’s press and attracted much attention. Even if that story was exaggerated, it is pretty clear that something is wrong in the state of nature. Massive losses of insects are so serious because insects are essential components in almost every ecosystem. It’s their job to eat plants which convert the energy of sunlight into biomass – the foundation of most terrestrial food webs. In turn, these herbivorous insects are eaten by carnivorous insects, which are themselves eventually eaten by larger insect-eating animals. If insects are in trouble, then so is everything else in that ecosystem as serious losses in insect biodiversity threaten all kinds of wildlife. 


      Read more:
      What happens to the natural world if all the insects disappear?


 Wild bee and hoverfly species are globally important in fertilising flowering plants by transferring pollen between them, causing them to set seed. Without them, seed production in many wild flowers is reduced and plant populations decline. With fewer flowers to visit and less nectar and pollen to gather, pollinator numbers decline even further in a vicious cycle. It’s not only wild plants that are affected, but also agricultural crops. Strawberries, apples and oilseed rape are just three of many crops that benefit from pollination by bees and other insects. Production of seeds for planting is also dependent on insects. Without “ecological services” like insect pollination, some of these crops could no longer be grown. The annual value of insect pollinators for the UK alone has been estimated at £603m. Globally, pollination adds US$153 billion to the economy each year. The new report notes that among those pollinating insects whose ranges have expanded, species associated with field crops are well represented. This could be because measures have been taken to encourage them such as planting wildflowers, which provide pollen when crops are not in flower. Alternatively, it may simply be that some species are more tolerant of the progressive intensification of farming practices than others. Superficially, the increases in crop pollinators seem encouraging, but it may not be good news. Loss of pollinator diversity decreases crops yields, and this may be more important than insect numbers. Decreased pollinator diversity may leave insect populations more vulnerable to viral diseases that spread readily among social insects. Such viruses interact with widely-used pesticides and are known to adversely affect both honey bees and bumblebees. Broadly speaking, biodiversity losses in farmland habitats are likely due to increases in the efficiency of farming. Farmers seek to grow the greatest crop yields on the area of land that’s available to them. This ensures that agriculture captures more and more of the sun’s energy, converting it into human food.  As farming efficiency increases, less space and fewer resources are left for anything other than human food crops. The recent study shows that a few crop-specialist pollinators have increased while the majority have not, which shows that fewer plants and animals are thriving in ecosystems which are increasingly dominated by agriculture. There’s a trade-off between wild nature and farming efficiency and it seems that we have to decide how much wild nature we want."
"**Some parents could not wait to get their children back to school after the first coronavirus lockdown.**
But Louise's two children remain at home - initially because she was shielding, but increasingly because ""they are flourishing"".
They are among 806 children removed from school registers by parents in Wales between March and September this year - up almost 50% since last year.
The Welsh Government said it had given councils Â£400,000 for home-schooling.
But some parents teaching their children at home have called for more support.
Figures obtained by BBC Wales Live show 552 children were ""deregistered"" from school between March and September in 2019. In 2018, the figure was 466.
The actual number will be higher as six local authorities have not given their figures.
Local authorities said children were being deregistered for a number of reasons, including anxiety, coronavirus and lifestyle choice.
Louise, 38, from Abergavenny, said her daughter Orlena struggled with learning at home during the first lockdown, ""but when it came to the children going back to school I was afraid and anxious"".
""It was the unknown with Covid - it just filled both me and them with uncertainty.""
Louise suffers with an autoimmune disease which means she would be likely to be ""very sick"" if she caught coronavirus, which made her children ""extremely concerned about bringing it home"".
""So at the end of August we just decided they weren't going to go back for now,"" she explained.
Armed with online resources loosely based around the suggested curriculum for their age, Louise deregistered her children in September.
She has been surprised by how readily Orlena, 12, and nine-year-old Roy have adapted to their new style of education.
""We were just plodding along with school - I was aware Orlena was slow in some things and that Roy was a bit rebellious,"" Louise said.
""Home-educating has helped me see their weaknesses and work on them.
""I ask them how they feel about going back to school and both of them say they don't get as much one-on-one with teachers as they have 30 children to deal with.
""They are flourishing now with someone having the time to sit with them and do things. I do learning through play, and if something doesn't tickle their mind I don't force them to do it.""
She said she now sees home education as a potentially long-term arrangement for the children, with the possibility of them studying for their GCSEs at home.
""If the children want to go back once it is safe for them to do so, they can. But I have decided to leave that up to them,"" Louise added.
""We might look at flexi-schooling, where they go to school for part of the week, but spend a couple of days being home educated too.""
Although she is sometimes unable to teach them because of her illness, Louise said help from friends, her children's desire to learn and online resources makes home education possible.
She said: ""There hasn't been any support, the only support has been through other home-educating parents online.""
Monmouthshire council said it worked with home-schooling parents ""in line with Welsh Government guidance"", and that no parents had been in touch to express disappointment with the level of support.
Freedom of Information requests by Wales Live to local authorities found there were a total of 2,250 children currently being home educated in Wales. In 2019 there were 2,171 while in 2018 that figure was 1,878.
The number may be higher as it is not compulsory for parents to register their children as being home schooled if they have never attended school.
Lockdown has been like an ""extended free trial"" for home schooling and the ""biggest single boost in home education ever"", according to Alastair Lawson, from education resource website Twinkl.
But home schooling has been more challenging for other parents.
Polly, from Ceredigion, started home schooling her 12-year-old daughter Meg in September to shield her husband - who previously had a kidney transplant - from the virus.
""I can't describe how hard I find it having to be teacher, mother, best friend,"" said Polly.
""I've had no official support. Ceredigion council were very supportive when we chose to withdraw her from education - they didn't oppose it and we didn't have any fines and I think we had one letter with a link to online resources.""
Ceredigion council said it was ""not in a position to direct what learning takes place"" at home, but it was ""eager"" to help families find the right learning resources.
She said the experience has made them a lot closer as a mother and daughter, but added: ""Some sort of mentoring for me would really help. Some sort of guidelines.""
Plans to set up a compulsory register of children being home schooled were shelved by the Welsh Government in June due to the pandemic.
As a result, Children's Commissioner Sally Holland has launched a review into regulations surrounding home schooling to see if it should be better regulated.
A Welsh Government spokeswoman said it was updating a leaflet for home-educating parents concerned about coronavirus, and they needed to be aware of the implications of home-schooling.
She said: ""We're aware that some parents may have made the decision to remove their children from school due to Covid-related fears, however, we encourage schools and local authorities to work together with families through a supportive approach to enable a return to school during these challenging times.""
The spokeswoman added the Welsh Government had allocated Â£400,000 to local authorities this year to support home-schooling families, which is ""unique to Wales"".
Watch Wales Live on BBC One Wales at 22.35 GMT on Wednesday, and afterwards on BBC iPlayer."
"
In case you missed it live, Christopher Monckton spent an entire hour on the Glenn Beck program today on the topic of global warming, skepticism, and the Copenhagen Treaty.

The video is now available.
Watch it below.
I think Lord Monckton did a splendid job.
To see the proposed Copenhagen Treaty, see this essay on the subject here.

Parts 1-7 of the hour long video are below. YouTube has time limits on clips, so it is broken up into parts 1-7.









			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e92002122',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"**The hospitality industry in Northern Ireland could reopen before Christmas, the chief medical officer has said.**
However, Dr Michael McBride said it would only be possible with adherence to the current and upcoming restrictions.
Dr McBride called for people to stick to the regulations being introduced for two weeks from Friday 27 November.
He also said with promising vaccine developments, ""we are hopefully at the beginning of the end of this"".
Northern Ireland's top medical official was speaking on BBC News NI's Coronavirus Catch-up.
When asked about hospitality, a sector that has been impacted heavily by restrictions, Dr McBride said it was possible there could be some positive news around reopening.
""If we all work hard and we all make the most of the two weeks of wider restrictions, then I think that's a realistic possibility,"" he said.
""We cannot keep the hospitality sector closed indefinitely.
""It is just not sustainable for us to do so and we know the huge impact that's having, not just in terms of from an economic viewpoint, but actually in stress and anxiety levels of those working in the sector.""
He also said gyms were closing for two weeks because data had shown ""some sizeable outbreaks associated with gyms"" despite protective measures being in place.
However, he also said that there was hope on the horizon with the news of coronavirus vaccine developments.
He revealed extensive planning has already been undertaken for the eventual distribution of a vaccine, if one or more are approved.
Dr McBride confirmed that they would seek support from all sectors, including logistical help from the army, but that that would not entail ""boots on the ground"".
""We have peer vaccinators within our trusts, we have recruited a cadre of volunteers, those who wish to do the online training and become vaccinators,"" he said.
""We have retired doctors, dentists, nurses, a range of other individuals who have volunteered, last we looked we had 600 volunteers in the 72 hours since we out up the online web address for people to volunteer.
""Four hundred or so of those have already been accepted and have gone through the first phase of the training, so we have a very well co-ordinated and thoughtful planned approach to the rollout of this vaccine.""
The chief medical officer expects that some of those vaccine doses, if approved, may arrive and be administered before Christmas.
""We will not however, see the vaccine begin to have a significant impact in terms of reduced need for some of the restrictions until, I would have thought, the springtime,"" he said.
""It's only then we will have sufficient numbers of the vulnerable people isolated that we can begin then to ensure the pressures on our health service are reduced and also the risk to those people who are most at risk is reduced.
""I think we are hopefully at the beginning of the end of this, by the summer we will be in a very, very different place.
""But it is crucially dependent on that vaccine being approved, crucially dependent on the supply of the vaccine but most importantly of all, it's important that we all take up the offer of the vaccine when it's made available to us."""
"

You’ve got to hand it to Tony Blair. When polls showed 80 percent of the British citizenry against America’s military position, Blair stood fast with President Bush. And, as has happened here, his overall popularity (and support for his Iraq policy) has soared since hostilities began and the outrageous nature of Saddam’s regime and tactics became common knowledge.



But selfless political sacrifice is as foreign as chastity in Washington. After the dust settles, Blair wants Bush to drop his steadfast opposition to the Kyoto Protocol on global warming. 



The Kyoto Protocol is wildly popular in Britain largely because the country seems to lack scientists courageous enough to point out that the government’s alarmist view of climate change is without merit. That’s not the case here. And as everyone in the Bush administration knows, warming in the next 100 years, given a very small range of error, is likely to mirror what has happened in the last 40 years. Further, the administration knows that the Kyoto Protocol, while enormously expensive, would stop less than one tenth of a degree (C) of warming in the next half‐​century, an amount too small to be reliably measured.



Soon after Bush took office and National Security Agency head Condolezza Rice said “Kyoto is Dead,” the BBC reported that Blair was under considerable pressure to oppose Bush. In April 2001, Blair’s deputy prime minister, John Prescott, “want[ed] to end cooperation [with the United States] on global trade, national missile defence, and even British support for the U.S. stand against China.” Others in Blair’s cabinet agreed, including International Development Secretary Clare Short and Foreign Secretary Robin Cook. When he came to Washington three months later, Blair made plain his differences with Bush on the Protocol. 



Fast forward to the radically changed world after 9/11. Speaking before the U.N.‘s Earth Summit in Johannesburg in September 2002, the London Guardian reported that “Tony Blair launched into an unexpected broadside against George Bush on climate change,” and added that “what makes it more surprising is that his [Blair’s] aides appeared to be emphasizing the split with Washington.… In what aides said was a direct message to the White House, Mr. Blair said that Kyoto was not enough.” Going even further than the Europe’s radical greens, Blair said, “Kyoto is not radical enough.”



Blair shares more with the discredited Hans Blix than he does with George Bush on global warming. Last month, Blix said, “I’m more worried about global warming than I am of any major military conflict.” On February 25, just three weeks before the start of war, Environmental News Network reported that Blair “said world leaders must not let the crisis in Iraq and the fight against terrorism distract them from long term but equally important environmental problems.”



Blair said, “The only answer is to construct a common agenda that recognizes that both sets of issues have to be confronted for the world’s security and prosperity to be guaranteed.” Further, sounding more radical than Al Gore, he continued: “There will be no genuine security if the planet is ravaged by climate change. We will continue to make the case to the U.S. and to others that climate change is a serious threat that we must address together as an international community.”



It is doubtless that Blair has told Bush the price of military alliance in Iraq: Drop U.S. opposition to Kyoto. 



This won’t happen in a very public fashion. Instead, watch the legislation. The current Senate energy bill contains three provisions that come pretty close to enacting Kyoto. If the administration lets them slide through, the deal has been done. 



One creates a permanent Office of Climate Policy in the White House, which gives radical environmentalists direct access to the president. The legislation also requires a national strategy to cut carbon dioxide emissions, which is a complete surrender by the administration to the nonexistent science propping up a hypothesis of dramatic and disastrous warming. Finally, the bill creates an “early credit” for industries that cut emissions now. These “credits” only have value if some type of legal limit on emissions is imposed, so expect all these creditors to lobby for that limit. That is precisely what Enron pleaded for from the Clinton administration in a well‐​publicized letter from dethroned CEO Ken Lay. 



Bush I and Bush II are men of their word. In the first Gulf War, Bush I promised the Saudis that we would not dethrone Saddam Hussein as the price for usage of their airbases. He kept it, inadvertently creating today’s war. His son’s word is equally his bond, which will become evident if the White House rolls over on Kyoto in the next month. 
"
"
NODC Ocean Heat Content (0-700 Meters) – 2007, 2008 & 2009 Corrections
Guest post by Bob Tisdale
The National Oceanographic Data Center (NODC) recently updated its 4th quarter and annual 2009 Ocean Heat Content (OHC) data. The data that was presented in conjunction with the Levitus et al (2009) Paper now covers the period of 1955 to 2009. There have been changes that some might find significant.
This post presents:
1. A brief look at the revisions (corrections) to the data in 2007 and 2008 OHC data
2. A comparison of the NODC OHC data for the period of 2003 to 2009 versus the GISS projection
REVISIONS (Corrections) TO THE 2007 AND 2008 NODC OHC DATA
Figure 1 is a gif animation of two Ocean Heat Content graphs posted on the NODC GLOBAL OCEAN HEAT CONTENT webpage. It shows the differences between the current (January 2010) version and one that appears to include data through June or September 2009. So this is an “Official” correction (not more incompletely updated data posted on the NODC website discussed in NODC’s CORRECTION TO OHC (0-700m) DATA, which required me to make corrections to a handful of posts). I have found nothing in the NODC OHC web pages that discuss these new corrections. Due to the years involved, is it safe to assume these are more corrections for ARGO biases? As of this writing, I have not gone through the individual ocean basins to determine if the corrections were to one ocean basin, a group of basins, or if they’re global; I’ll put aside the multipart post I’ve been working on for the past few weeks and try to take a look over the next few days.
 http://i48.tinypic.com/14e6wjn.gif
Figure 1
NODC OHC OBSERVATIONS VERSUS GISS PROJECTION (2003-2009)

One of the posts that needed to be corrected back in October was NODC Ocean Heat Content (0-700 Meters) Versus GISS Projections (Corrected). The final graph in that post was a comparison of global ocean heat content observations for the period of 2003 through year-to-date 2009 versus the projection made by James Hansen of GISS of an approximate accumulation of 0.98*10^22 Joules per year. Figure 2 is an updated version of that comparison. Annual Global OHC data was downloaded from the NODC website (not through KNMI). The trend of the current version of the NODC OHC data is approximately 1.5% of the GISS projection. That is, GISS projected a significant rise, while the observations have flattened significantly in recent years. The apparent basis for the divergence between observations and the GISS Projection was discussed in the appropriately titled post Why Are OHC Observations (0-700m) Diverging From GISS Projections?
 http://i47.tinypic.com/20kvhwn.png
Figure 2
Note: The earlier version of that graph (with the NODC’s October 15, 2009 correction)…
http://i37.tinypic.com/i6xtnl.png
…shows a linear trend of ~0.08*10^22 Joules/year. The current linear trend is ~0.015*10^22 Joules/year. Some might consider that decrease to be significant.
NOTE:  I DELETED THE THIRD AND FOURTH PARTS OF THIS POST…
3. GLOBAL, HEMISPHERIC, AND INDIVIDUAL BASIN OHC UPDATE THROUGH DECEMBER 2009, AND
4. TREND COMPARISONS
…UNTIL I TRACK DOWN DISCREPANCIES I CAN’T EXPLAIN. I WILL REPOST THOSE SECTIONS IN A NEW POST. I BELIEVE I UNDERSTAND THE DIFFERENCES, BUT I NEED TO CHECK WITH KNMI.
SOURCES

NODC Annual Global OHC data used in Figure 2 is available here:
ftp://ftp.nodc.noaa.gov/pub/data.nodc/woa/DATA_ANALYSIS/3M_HEAT_CONTENT/DATA/basin/yearly/h22-w0-700m.dat


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8e8071b9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

“It’s fun to be in a place where our crowd is still in office.”  
–Bill Clinton in a speech to the British Labor Party conference 



Socialist Luiz Inacio “Lula” da Silva’s election as Brazil’s next president is another nail in the coffin of the Third Way, the decade‐​long attempt by some leftist politicians to occupy the allegedly middle ground between socialism and capitalism. This political movement recently held sway in the United States and abroad but, as the following survey demonstrates, today many proponents are playing electoral defense. It’s also apparent that those Third Way politicians who are prospering owe their electoral survival more to astute political marketing than to successful policy performance.



During the 1990s, Third Way candidates proved electorally attractive throughout Latin America. After a decade of free market gains, respective national elections saw relatively moderate center‐​left candidates triumph over more conservative opponents. 



But, on Oct. 27, the world’s fourth‐​largest democracy experienced the after‐​effects of a failed Third Way government. Outgoing Brazilian President Fernando Henrique Cardoso was a leading Third Way spokesman. But neither conquering inflation nor privatizing some state‐​owned utilities compensated politically for the anemic economy, rising unemployment, falling real incomes, and high interest rates that stemmed from insufficient deregulation, tax cutting and trade liberalization.



So, Cardoso was unable to provide a boost to his party’s chosen successor, Jose Serra. This compounded the problem of the Third Way’s grudging acceptance of globalization, as reflected in support for greater global economic governance, including capital controls. Such an outlook gave succor to Lula’s more robust economic populism. 



In Europe, British Prime Minister Tony Blair was reelected in July 2001. But this prominent Third Way leader’s electoral success reflected the inadequacies of his conservative opponents rather than a stellar record of policy accomplishment. 



Although Blair has withstood the pressure from his party’s socialist base to raise income taxes, such fiscal parsimony doesn’t tell the whole story. After all, various sources of indirect taxation have increased significantly. Frequent microeconomic meddling, including the introduction of a minimum wage, has balanced Blair’s relative inactivity on the macroeconomic front. Regrettably, Blair brought a somewhat illiberal streak to criminal justice and social regulation issues that emphasizes social conformity over the pursuit of individual preferences.



German Chancellor Gerhard Schroeder, continental Europe’s leading Third Wayer, has had a far rougher political ride. Given the poor economy, symbolized by significant joblessness, Schroeder’s September reelection was surprising. Early on, his government did introduce minor tax and spending reductions and proposed a market‐​oriented reform of the government‐​funded social security retirement program. 



But Schroeder quickly lost his reformist way. He did little to change an overly regulated labor market, the principal source of high unemployment. Fiscal policy was reversed and the government provided taxpayer‐​funded corporate bailouts. Schroeder’s political fortunes were rescued by two fateful events during the campaign: his adroit handling of a flooding crisis (he promised to throw lots of money at the problem) and his vociferous opposition to a U.S.-led invasion of Iraq.



Dutch Prime Minister Wim Kok was also a pillar of the Third Way movement. But in May Dutch voters threw out Kok’s eight‐​year‐​old center‐​left coalition government and replaced it with the right‐​wing opposition. Such was the extent of popular discontent that the parties comprising the center‐​left coalition were relegated to third and fourth place, respectively. 



During the 1990s, Prime Ministers Romano Prodi and Massimo D’Alema expounded the promise of an Italian Third Way. However, the governing center‐​left coalition’s unwillingness to address the structural problems handicapping the Italian economy (e.g., high taxes and government spending combined with heavy state ownership) inadvertently produced a political climate ripe for change. In last year’s election, the incumbent Third Wayers were deposed by Silvio Berlusconi’s right‐​wing Freedom Alliance.



Back here, President Clinton’s own brand of Third Wayism reflected a timid, government‐​knows‐​best inclination. Clinton raised taxes and attempted to micromanage individuals’ financial decision‐​making through tax credits. Some Clinton policies resembled fuzzy versions of social conservatism. His extension of the failed War on Drugs is an obvious example. Revealingly, neither Clinton nor Vice President Al Gore could definitively decide whether they were pragmatic, “New Democrat” centrists or economic populists. As a result, Gore wasn’t elected president in 2000. 



But how will the Third Way fare in the midterm elections? 



It’s helpful, if dispiriting, to look at California. The Third Way’s New Democrat strand has an important adherent in Gov. Gray Davis. But since his 1998 election, Davis has placed particular emphasis upon appeals to labor unions and racial minorities, hence his opposition to school vouchers and support of immoderate affirmative action. The Cato Institute recently awarded Davis an “F” for his fiscal performance. 



Consequently, Davis’s expected reelection reflects two realities: first, the political limitations of his conservative opponent, rather than an endorsement of Davis’s flawed performance; and, second, a political ruthlessness on Davis’s part that enthusiastically embraces cutting‐​edge political marketing techniques. 



Davis may be the poster boy for the successful Third Way politician. Although he’s more rhetorically pleasing than the old‐​school liberal‐​socialist, in practice he’s equally disposed to limiting our freedom.
"
"**Stormont minister Edwin Poots has refused to comment on why he sent an email criticising the Department of Health's response to Covid-19.**
The agriculture minister replied ""no comment"" when asked about it at an event in the Mournes in County Down.
Speaking on Wednesday he also said the email was ""self-explanatory"".
In the email, Mr Poots said the ""failure"" of the health department's response would ""inevitably lead to the failure of the economy"".
The original email was sent by a member of public to MLAs criticising the government for the ""devastating effect"" the tougher restrictions will have.
In a reply to all, Mr Poots agreed, saying the majority of the Executive see things differently.
During the event on Wednesday morning, the DUP minister said the quicker the vaccine and mass testing could be rolled out, the sooner lockdowns could end.
Asked whether he was on board with the latest restrictions, he said he did not have any choice.
He added: ""The Executive has made a decision and we all will abide by that decision.""
He said spread in households contributed significantly to the incidence of the disease and needed to be ""clamped down on"".
When asked about the Christmas bubbling plan he raised concerns.
He said: ""I don't think there's any alternative - people want to have Christmas. They have had an awful year and they need some respite from Covid-19, albeit there will be problems arise from that and there'll be a greater level of spread as a consequence of it.
""I've absolutely no doubt the R-rate will rise in the two weeks before Christmas and over the Christmas period and there'll be a consequence thereafter. That goes without saying.""
Two weeks of Covid-19 lockdown restrictions will take force across NI from Friday.
Mr Poots, who is a former Stormont health minister, was criticised by other parties for the contents of his email.
Ulster Unionist health spokesman Alan Chambers MLA said Mr Poots' comments showed ""how detached from reality he is"".
Mr Poots' party colleague, the South Antrim MLA Pam Cameron, who is DUP deputy chair of the health committee, had previously described Mr Poots' reasoning as ""a little simplistic"".
Stormont sources said the minister, who has previously spoken out against imposing tighter lockdown measures, said it was illogical to close non-essential retail as it could severely damage the high street.
It is believed Mr Poots did not ask for the measures to be put to a vote by the executive and said he would accept whatever measures were agreed by the executive."
"

We’re all expected to love baseball — it’s America’s sport, after all — but I know a few taxpayers in the greater Washington area, maybe even a few thousand, who don’t. You know, people who weren’t — horrors! — glued to their TV sets, rooting for the luckless Red Sox or the jinxed Cubs to finally make it back to the World Series. People who haven’t spent every waking moment since 1971, when the Senators left, plotting to lure a team to town. People who don’t think the city’s image and its future depend on spending millions of taxpayer dollars on a state‐​of‐​the‐​art stadium for a transitory collection of athletes, artificially assembled through league drafts, franchise trades and high salaries. 



Alas, the idea that such taxpayers might exist doesn’t even seem to be on the radar screens of many local officials, who are falling all over themselves trying to scrounge together enough public money to lure a major league baseball team back to the national capital area. District Mayor Anthony Williams is proposing $339 million to build a stadium in town (even though there’s no commitment yet from any team), while baseball boosters in Northern Virginia are pushing millions in state construction bonds to win a franchise. 



These politicians are offering the usual justifications for providing the modern version of bread and circuses to their constituents: municipal prestige, business development, new jobs. But in the end, publicly funded stadiums come down to money — and I don’t mean money for the city that gets the team. I’m talking about money for wealthy sports moguls who have turned extorting taxpayers into an art form. 



Franchise owners typically win taxpayer support only through threats: Pay us off, or we will leave, they say. Give us a new stadium, or we’ll go someplace that will. Take the current competition for the Montreal Expos, who have been up for grabs ever since Major League Baseball assumed ownership of the financially ailing franchise. The only question seems to be, who will offer the biggest inducements to get them? 



Virginia and the District shouldn’t play this game. Stadiums don’t constitute a great unmet social need. Sports should be a private enterprise, privately funded, just as it was during most of the first half of the 20th century. 



Yet the willingness of political elites to sacrifice taxpayers on the omnipresent sports altar spans the country. Oregon faces a serious budget crisis, but that didn’t stop the legislature from recently approving $150 million for a new baseball stadium in a bid to win the Expos for Portland. In Oakland, Al Davis, the irrepressible owner of the Raiders football team, won a $34.2 million verdict against the city stadium authority for failing, he argued, to deliver on its promise of sold‐​out games. In San Diego, meanwhile, negotiations continue between the city and the Chargers, who want a new stadium — eight years after the city renovated the old one. 



Today, government involvement in the sports business seems unexceptional. But as Raymond Keating, chief economist for the Washington‐​based Small Business Survival Committee, observes, “Before the Great Depression, sports subsidies were rare.” Since then, he figures, government has poured well over $20 billion (in current dollars) into sports ventures. Such subsidies cannot be justified in principle. Making some people pay so others, whether franchise owners or restaurateurs or developers, can profit is a misuse of government. The only benefit is private, not public. 



Not only were sports facilities built privately earlier in the last century, they still can be. In 1987 Joe Robbie, owner of the Miami Dolphins, constructed his own stadium. Even the Redskins’ FedEx field was primarily a private venture by owner Jack Kent Cooke when it originally opened in 1997 (as Jack Kent Cooke Stadium), though the state of Maryland chipped in for roads and other public improvements. 



Proponents argue that franchises provide prestige, but the nation’s capital doesn’t have to worry about a lack of prestige. Is Los Angeles impoverished because it can’t keep a football team? Do people move to San Diego to see the Chargers — or for the climate? 



On the flip side, the financial benefits of government support for teams are obvious. Teams avoid having to finance a stadium. They are able to upgrade their facilities at taxpayer expense, even as they cater to a wealthier corporate clientele. 



Most teams are owned by extremely wealthy businessmen, such as the Redskins’ Dan Snyder and the Orioles’ Peter Angelos, (not to mention one former owner by the name of George W. Bush), who are able to resell at a profit. Professional sports investments often are a dilettantish affectation, especially for limited partners, who — besides a quick return on their cash — simply crave proximity to the team. Businessman John Imlay Jr. recently parlayed his $6 million investment in the Atlanta Falcons into $35 million, explaining to The Post’s Thomas Heath: “In ten years, I made five times my money and had a heck of a good time.” 



The taxpayers are not so lucky. Public finance experts Roger Noll of Stanford and Andrew Zimbalist of Smith College found in a recent study that “no recent facility appears to have earned anything approaching a reasonable return on investment and no recent facility has been self‐​financing in terms of its impact on net tax revenues.” Even better stadium projects, such as Baltimore’s Camden Yards, require continuing aid for upkeep. As F.W. Walz, a Cleveland city councilman who opposed the nation’s first subsidized sports facility, a baseball stadium, observed in 1928: “Of course, they say the stadium will pay for itself, but we’ve heard that story before.” 



Stadium proponents argue that owner enrichment is merely incidental to increased regional economic activity and tax collections. And they routinely produce studies claiming significant financial gains. But even if there is an economic benefit, it is small. University of Maryland economists Dennis Coates and Brad Humphreys figure that annual sports‐​oriented tax revenues and personal earnings from sports have been much less than 1 percent of the total earnings and revenues for Baltimore and Maryland. As they explain, “Although the absolute numbers seem large and impressive, they are small compared with the existing tax revenues and local economy, even if one grants that the proponents’ estimates are correct.” 



There’s much to criticize in such estimates, however. For instance, what’s the right “multiplier”? That is, how much is ultimately generated by a dollar spent on sports? Official figures tend to assume, unrealistically, that all of the money, including, for instance, players’ salaries, is spent locally. 



Even more important, though, is that sports spending primarily substitutes for other outlays. Stanford’s Noll figures that the vast majority of those attending games — more than 90 percent — are local residents. They are merely diverting their spending from other leisure activities. Money might shift a bit within a region — from suburbs to city, or from outer to inner suburbs. But, as economists have consistently found, the amount of new economic growth is minimal. Economists Robert Baade of Lake Forest College and Allen Sanderson of the University of Chicago have looked at 10 metropolitan areas that brought in sports teams, and found no net employment increase, as spending was simply realigned. And there was no evident difference in economic performance between cities with or without teams during the 1994 baseball strike, says the University of Akron’s John Zipp. 



So if the goal is trickle‐​down consumer spending and business development, why not build a new automobile factory, retail outlet, grocery store or software facility to attract and maintain companies, jobs and economic growth? Forget a sports team for D.C. Just erect a string of buildings for restaurants. That should draw suburban residents, and their money, here. 



But neither sports boosters nor their political allies are much interested in overall economic impact. Fans want a team, potential franchise owners desire subsidies, and elected officials expect political gain — and the opportunity to snag an invitation to the owner’s box. Government stadiums benefit economic and political elites, not the public. 



Yes, refusing to play the subsidy game might mean losing a franchise. But if the only way to prevent a team from moving or to get one to come to your town is to shovel corporate welfare into a billionaire’s hands, trust the research — it isn’t worth it. 
"
"
From the Times of India – a “put up or shut up” moment – “we’ll go along if you pay us”.

Excerpts below:
BEIJING: In an unprecedented move, India on Saturday joined China and two other developing countries to prepare for a major offensive on rich nations at the Copenhagen conference on climate change next month.
The four countries, which include Brazil and South Africa, agreed to a strategy that involves jointly walking out of the conference if the developed nations try to force their own terms on the developing world, Jairam Ramesh, the Indian minister for environment and forests (independent charge), said.
“We will not exit in isolation. We will co-ordinate our exit if any of our non-negotiable terms is violated. Our entry and exit will be collective,” Ramesh told reporters in Beijing. 
The move comes after reports suggested that rich nations led by Denmark are trying to set the agenda of the conference by presenting a draft containing a set of specific proposals.
…
The four nations issued a joint press release, which made it clear the developed nations should be ready to contribute funds and share green technology if they expected the developing and poor nations to take major actions on environmental protection.
…
The developing nations will also not accept any pressure from developed countries to establish legally binding emission targets at Copenhagen. Developing countries want to be allowed to reduce emissions voluntarily and take what they consider to be “nationally appropriate actions” he said.
Ramesh said India will under no circumstances accept the concept of a peaking year under which each country will have to indicate on what date they will reach the highest level of pollution before beginning to come down.
India will also not accept any unsupported mitigation actions without any effort by developed countries to provide funds and technology support to improve environment in developing nations.
Read the complete article at the Times of India


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e909c85fc',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Drivers hoping their new plug-in hybrid car will help cut down their carbon footprint may have an unlikely enemy: cold weather. Zero-emissions driving can be impossible for some of the UK’s bestselling plug-in hybrid models when the air is chilly or if passengers do as little as switching on the heating – even if the battery is fully charged. Plug-hybrids allow drivers to switch between battery electric power and an internal combustion engine, delivering significant emissions improvements over conventional cars. However, environmental campaigners have long harboured concerns that plug-in hybrids do not offer the environmental benefits suggested by carmakers’ advertising or regulators’ laboratory tests. All of the UK’s top 11 bestselling plug-in hybrids have limitations on pure electric driving ability. Cold temperatures can trigger the internal combustion engine in Volvo’s XC90 SUV, the Mercedes-Benz E Class executive car, and Kia’s Niro crossover. The Mitsubishi Outlander SUV, the UK’s bestselling plug-in last year, has an “EV” button that switches on “EV priority” mode. However, the internal combustion engine will kick in if the driver switches on the adaptive cruise control, to automatically maintain a safe distance from the car in front, or if the battery gets too hot or too cold in more extreme conditions. Jaguar Land Rover’s Range Rover and Range Rover Sport plug-ins will start their internal combustion engines if more power is required than the electric engine can provide alone, as will Porsche’s Cayenne. There are also speed limits on all-electric driving for BMW’s 2, 3 and 5 Series cars as well its Mini Countryman plug-in. The carmakers all carefully avoid making any incorrect claims on their products’ green credentials in their marketing materials. However, campaigners have criticised the emphasis in many brochures and advertisements on zero-emissions driving capabilities, when these may be difficult to achieve in normal use. Greg Archer, UK director at campaign group Transport & Environment, said one leading carmaker “is conning its customers”, after it was approached by an unhappy owner of a plug-in hybrid. The group, which passed the correspondence to the Guardian, has been highly critical of plug-in hybrids, some of which it labels “fake EVs” because of their continued use of internal combustion engines. “A [plug-in hybrid] is not driving with zero emissions if it switches on its engine when the driver de-mists the windscreen,” Archer said. “This is another example of carmakers attempting to mislead their customers about the real emissions from their car.” Separate data from the Miles Consultancy, which tracks fuel use by companies, found that in real life almost all plug-in hybrid cars failed to achieve the mileage found in lab tests, suggesting that many users do not charge them sufficiently. Updated analysis for the Guardian of 1,388 plug-ins used over eight months found they achieved an average of more than 40 miles per gallon when using a mixture of petrol and electric power, only a third of the 127 miles per gallon advertised by their manufacturers. Last month the government said it plans to ban all hybrids from sale in the UK from 2035 or earlier, signalling that promoting battery electric cars with zero exhaust emissions was its priority. Buyers of plug-in hybrids could also risk being caught by tightening emissions limits in British cities. The prospect of a hybrid ban infuriated carmakers, who say the technology is the only way to cut emissions quickly. The carbon dioxide emissions of cars sold in the UK rose for the third consecutive year in 2019. Mike Hawes, chief executive of the Society of Motor Manufacturers and Traders, said: “Plug-in hybrids are an important and attractive stepping stone for people not yet suited or able to invest in a fully electric vehicle, giving the flexibility of zero-emission miles on urban commutes and extended range for longer, out-of-town journeys. “Drivers also have the peace of mind that the engine will kick in to provide the necessary boost if the battery level falls too low to deliver sufficient power for energy-intensive operations such as high-speed overtaking or windscreen de-icing, thus guaranteeing safety and the most efficient use of energy at all times.” Selling thousands of plug-in hybrids is also a key part of carmakers’ plans to meet tightening emissions limits and avoid heavy fines, and industry analysts expect a hybrid “price war” over the coming year as companies try to shift the cars in large volumes. Companies including Toyota, BMW and Daimler have bet heavily on hybrid technologies. “The vast majority of owners we surveyed use their Outlander [plug-in hybrid] as it was engineered and are enjoying a lower carbon footprint and lower running costs as a result,” a spokeswoman for Mitsubishi said."
nan
"
Share this...FacebookTwitterHere is a sampling of the media reaction coming from Germany on CERN’s cosmic ray cloud seeding experiment.
Normally the German mainstream media is quick to report on new scientific developments, especially anything indicating catastrophic global warming. But this time they have been slow and cautious.
FOCUS magazine online starts with:
Climate skeptics doubt that man-made greenhouse gases are to blame for global warming. A new study appears to confirm their claims.”
FOCUS, in its comprehensive 6-part piece, goes on to concede that the questions behind the causes of global warming are far from being answered. FOCUS tries to play down the CERN results, and so resorts to quoting 2 hardline warmists, Jochem Marotzke, Director of the Hamburg Max Planck Institute for Meteorology and Stefan Rahmstorf of the Potsdam Institute for Climate Impact Research (2 institutes that would be pretty much be out of business if the cosmic ray theory proved right).
‘The mechanism is plausible, but it is not quantitatively enough to explain the observed warming’, assessed Jochem Marotzke. Stefan Rahmstorf completely threw out the the idea recently: ‘Cosmic rays have been measured since 1953; they show no increasing or decreasing trend analog to solar brightness. Without such a trend, one can also explain no change in cloud cover,’ he insists.”
Die Welt writes:
And: Which role do clouds play in all this?  ‘Cloud’ has found the first preliminary answers to that. The sun could play a bigger role than first thought. “Could!”, emphasizes Kirkby und Curtius.
Actually Danish scientist Henrik Svensmark found the preliminary answers, and CLOUD simply added a huge dose of confirmation. And funny how Die Welt in the past never emphasized the word “could” in its numerous articles on kook warmist scenarios, but is quick to do so here.
Die Welt also quotes Kirby who compares two charts: one of temperature vs solar activity and Mann’s hockey stick chart. Die Welt writes:
‘Look here’, he said ‘at how striking the correlation between solar activity and global temperature has been over the last 1000 years.’  In comparison another chart stands right next to it, the famous ‘Hockey Stick Chart’, which suggests that there was hardly any climate fluctuation over the last 1000 years and that a sharp rise began only 150 years ago. That would mean: Only man drives the climate, and the sun not. “It turned out to be false”, he [Kirby] said.”
Indeed Mann’s view of the past 1000 years is looking more and more like a fairy tale of epic proportions. Die Welt did find space to mention Svensmark:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Also Danish physicist Henrik Svensmark is working on the interaction between solar activity and climate change. ‘Although our experiments were not very complex, we had similar results three months ago’.”
Yes, Svensmark produced good data – on a shoestring budget. Compare his cost-effectiveness to the countless tens of billions of dollars wasted on the loads of useless junk science produced by warmists. A few million dollars are proving the $100+ billion wrong.
Finally Die Welt writes about the chief of CERN and his controversial request of last July of “not to interpret the results”.
Curtius denies having received such a request. ‘Of course we have to interpret our results’, he said, “otherwise other people will do it’.”
German skeptics slam German stubbornness
Skeptic blogs and sites have been blasting German stubbornness and warming dogmatism, and their refusal to acknowledge CERN’s results. Science journalist Edgar Gärtner at his blog slammed obstinate German science, writing a piece called: Cloud Experiment Exposes Climate Swindle: He writes:
Already 200 years ago it was detected by famous English astronomer William Herschel that the price of bread always increased when the number of sunspots was very low. Svensmark believed he could explain why it was so. But when he published his hypothesis together with his boss Eigil Friis-Christensen, then IPCC Chairman Bert Bolin called them ‘naive and irresponsible’.
Funny how who’s turning out to be “naive and irresponsible”. Gärtner also writes:
One can suspect that the results contain political dynamite, also when taken alone they do not suffice to bring down the greenhouse gas house-of-lies. But in combination with the recently published NASA satellite measurements, which we reported on not long ago, it could very well happen. These measurements have clearly shown that a man-made heat trap in the atmosphere just cannot be. The temperature increase recorded over the earth’s land mass at the end of the last century, which in the meantime has stagnated, has to be attributed to other causes. The successfully completed Cloud experiment on fluctuating solar activity offers a solution. The trillions of euros that the EU wants to have for fighting the supposedly man-made climate change are purely for nothing.”
So could it bring down the “greenhouse gas house-of-lies” as Gärtner suggests? Don’t bet on it.
AGW is now fatally embedded in all German institutions
It’s going to take Germany a long time to wake up from it’s global warming science folly, if at all. The prospects are poor. All of Germany’s major institutions like the public media, government, political parties, science bodies (such as the PIK, German Weather Service, Max Planck Institute), schools, etc., all have negligently and wrecklessly embedded the global warming dogma deep and firmly into their structures and psyche, thus making it the main pillar on which the architecture of Germany’s future society will rest. Now that pillar is cracking and crumbling. Suddenly Germany’s grand plans for a Green Empire are facing the scrap heap.
Will German leaders be able to come to terms with that? They seemingly (and stupidly) have gone beyond a point of no return with their zeal. They’re pushing their heads deeper into the sand. Expect them to get shriller. Germany is stuck in a self-made dilemma. Once a brand of science gets institutionalized nationally, and to the extent that it has in Germany, it is very very difficult – if not impossible – to remove. Germany has tragically been though something eerily similar before. Will a complete demise be the only way out? Germany has a way of hanging in – all the way to the bitter end.
Share this...FacebookTwitter "
"A warm winter means that for the first time in years Germany’s vineyards will produce no ice wine, an expensive golden nectar made from grapes left to freeze on the vine. The German Wine Institute said on Sunday that temperatures had not dropped to the prerequisite low of -7C (19F) in any of the country’s wine regions.  A succession of warm winters have reduced ice wine production in recent years, the wine industry’s marketing arm said. Only seven producers managed to make it in 2017, and only five in 2013. It did not say how far back records went. “If warm winters become more frequent over the coming years, ice wines from Germany’s regions will will soon become an even more expensive rarity than they already are,” said Ernst Büscher, a spokesman for the institute. Freezing the grapes before they are crushed concentrates the sugar and leads to an intensely sweet wine often served with dessert. It has always been a niche product, accounting for about 0.1% of German production, and the low volumes make it expensive. Making ice wine is a tricky business. Workers must race into the vineyards to harvest the grapes with only a few hours notice when the temperature falls, often at night or in the early morning. The grapes have to be pressed while still frozen, so the winemakers work in unheated facilities. Vineyard owners also face the risk that grapes set aside for ice wine will rot on the vine before the temperature drops far enough. Canada’s Niagara peninsula is one of several other places where ice wine is produced, thanks to its cold winters. It is also made in the US in northern Michigan and Ashtabula county, Ohio, near Lake Erie. Major markets for German ice wine include Japan, China, Scandinavia and the US, the institute said.  "
"

Rather than wait on the market to demand more fuel efficient trucks, President Obama, bypassing Congress, has directed the Environmental Protection Agency to draw up a new round of regulations raising the fuel efficiency standards on heavy‐​duty trucks. He promises that this will save billions of dollars in fuel costs, lower prices and reduce greenhouse gas emissions—or, as he describes it, a “win‐​win‐​win” situation.   
  
  
Thank you, Mr. President for taking such good care of us.   
  
  
Apparently, we are too stupid to have realized the manifold benefits of this chain of events ourselves.   
  
  
Or is it that we realize these actions will have no impact of climate change and will probably result in higher prices for new trucks and everything that they transport?   
  
  
You can use our “Handy‐​Dandy Temperature Change Calculator” to see that, using the EPA’s own computer model, if Americans cut _all_ of our carbon dioxide emissions to _zero, today_ , the amount of warming that would be prevented (assuming a warming forecast that is itself probably too high) by 2100 is around two‐​tenths of a degree—an amount that would be virtually impossible to measure against natural climate variability. Increasing the fuel efficiency of heavy trucks would have considerably less of an effect than cutting all carbon dioxide emissions and would simply not be discernible in climate data.   
  
  
And the President’s claim that increasing the fuel efficiency will lower the price of all things neglects the fact that we simply do not know what technology would accomplish this end.   
  
  
Perhaps he should have said—“if you like your truck, you can keep your truck,” that is, until you have to replace it with something that will cost much more than you would have otherwise purchased and not do what it is supposed to do.   
  
  
Again, thank you, Mr. President.
"
"

The media is increasingly embracing the idea that anyone in the scientific community who doesn’t wet their bed over the prospect of future warming is some sort of (a) flat‐​earth know‐​nothing, or or (b) a cynical money grubber who allows oil and coal companies to buy their expertise despite knowing full well that doom is on the horizon.   
  
  
Well, today you can judge for yourself. At a conference co‐​sponsored by the Western Business Roundtable and the Business Industry Political Action Committee (BIPAC), Cato senior fellow Patrick J. Michaels (who, more relevantly, is a professor of environmental science at the University of Virginia and a member of the International Panel on Climate Change) will debate Klaus Lackner, a professor of geophysics at the Earth Sciences center at Columbia University. The debate begins at 1:30 Mountain Standard Time and will be webcast live for all interested. If you count yourself among them, you can go sign up here to listen. 
"
"
Share this...FacebookTwitterCCS process. (Public domain graphic)
The German media today are reporting on the decision by Germany’s upper house of parliament, the Bundesrat, to reject a proposal to capture and sequester carbon dioxide emitted by power plants by pumping it into the ground, read here in English.
This rejection is a major setback for climate hero Angela Merkel and her silly plans to control GHG emissions.
Merkel’s government was hoping to compress, liquefy and store millions of tons of CO2 underground in a bid “to rescue the planet from “dangerous climate change” at a cost of billions to consumers (CCS is estimated to cost about ($30/ton). Indeed many of Germany’s politicians view the dumping of billions of euros into the ground to lower the global temperature by a few ten thousandths of a degree as a wise investment.
The truth is that many are involved in sweetheart deals with special interests and stand to make a killing. Unfortunately the Bundesrat, stirred by activists, thought the scheme was technically “too dangerous”. Just call it one stupidity killing another. But we’ll take a good decision any way we can – even if it is based on the wrong reasons.
Another reason the draft law was slapped down was because of a clause allowing individual German states to prevent CCS facilities from being built, thus enabling them to shirk their responsibilities.
As bad as nuclear energy!
Environmental kook groups like Greenpeace have been leading the protest against the CCS process, claiming dangerous CO2 “poison gas” poses “incalculable risks” and could explode or contaminate groundwater. Germany’s English-language The Local writes there are “fears of possible explosion-like uncontrolled emissions of the gas” and that “pressurised carbon dioxide storage underground was like nuclear power in that it was uncontrollable and not possible to secure.”
So what is left in Germany? Wind parks are facing mounting protests, burning fossil fuels face growing hurdles, nuclear power is being shut down, bio-fuels such as sun-diesel and ethanol are sinking further into controversy. Germany is boxing itself into a darkroom. Thank God the Eastern European countries are not rushing down the same path of folly. Soon Germany is going to need them to supply power.
Share this...FacebookTwitter "
"Have you heard the Morrison government has a plan? The Plan® was very hard to miss, given the prime minister used the word “plan” or “plans” more than 20 times when he answered his first Dorothy Dixer in question time on Thursday. Lest any ambiguity remain, Peter Dutton went on to utter “plan” another 16 times if you happened to be counting, and our indefatigable live blogger, Amy Remeikis, bless her, was counting.  Plans revealed themselves in the preambles to questions, and in the answers to questions; a thicket of plans for potential pandemics, for managing the economy, a plan for beating up Labor over its increasingly uncontroversial commitment to net zero emissions by 2050. Or perhaps that one was a strategy. The urgency and intensity of the messaging was such that I wondered whether we might at any moment see live broadcasts interrupted with a solemn message to the nation at the top of the hour: “Fellow Australians, this is your government speaking. Did we mention we have a plan? Did we mention we have a plan for a plan?” Before I get branded a shocking cynic, let’s be clear. It is obvious the government needs to have a considered and credible plan for managing a mass outbreak of the coronavirus, because the spread of this illness, and the negative economic consequences associated with it, is a deeply serious issue, not some passing bit of political confection or rank opportunism. The world is watching unfolding events in a state of shared nervousness. Leaders are warning an epidemic is on the way and the trajectory of infections prompted a record plunge in the US stock market on Friday as analysts warned the outbreak could wreak economic havoc on a scale not seen since the 2008 financial crisis. In short, this is all profoundly serious, and it is not a drill. So I’m relieved Australia has a plan, because we do need one, and it would great if the government can both develop and execute it competently. Having acknowledged there is a serious challenge to manage and the country absolutely needs a plan to manage it, let’s drill down a bit further. It’s pretty obvious why the government is intent on telling voters there is a plan for managing coronavirus and other potential crises. To put it bluntly, Morrison’s theatre of The Plan® is atonement for the disaster of the summer. If we take a minute or two to recap the sorry story of the bushfires, the only thing the government mastered was looking like a shambles. As the fire emergency spiralled, Morrison presented during a time of terrible crisis as a leader making it up as he went along. In the court of public opinion, the government was constantly running behind events. Haplessness could be measured in increments, from the ill-fated Hawaii holiday through to calling out the defence forces without telling the poor bloke managing the bulk of the blazes. As well as conveying a compelling impression of a government extemporising inelegantly in full public view, Morrison looked resentful when obvious deficiencies and inconsistencies were pointed out to him. Presenting to the public with pursed lips and barely restrained pique, not every time, but often enough to be memorable, I reckon compounded negative voter perceptions and made the inevitable backlash worse. Our Guardian Essential poll in mid-January delivered a stark snapshot of the credibility hit. Morrison’s net approval rating shifted from plus two to negative 12, and the Labor leader Anthony Albanese sprinted ahead of Morrison as preferred prime minister (despite the fact the poll shows consistently voters are still getting a fix on the Labor leader). Because everything is so polarised, because politics has substituted conflict (easy) for reform (hard), because technology is herding people into tribes, and because default mainstream media culture tells people these days it is OK, in fact, desirable, to sit in an enclave that confirms your biases, preferably with noise-cancelling headphones – Morrison didn’t take too much of a hit among rusted-on Coalition voters. They, largely, stuck. But if our poll snapshot is accurate, across Labor and (critically) undecided voters, one in seven people changed their minds about Morrison between December and January. The prime minister also took a significant hit on the attributes questions we ask the Guardian Essential sample regularly about leaders. There was a nine-point drop in the number of voters rating Morrison a capable leader. Worse, there was a 19-point drop in the number of voters saying he was good in a crisis. Now I want to consider these specific attributes a little. Given the Coalition’s defining pitch to Australian voters is always we will keep you safe and secure, because we are conservatives, and that’s what conservatives do (in contrast to wild-eyed progressives, who favour social change over security) – a loss of public confidence in attributes like “capable”, and “good in a crisis” is potentially dangerous for Morrison and the government. The language of safety, security and managerial competence is the most powerful language Australian conservatives have in their toolkit. That language is resonant enough to win elections. Security is such a critical part of the pitch that the disastrous summer Morrison just presided over actually begs an existential question for a Liberal prime minister who likes to style himself as the Generation X John Howard. It’s a simple question. What good is a conservative who can’t keep Australians safe? One more brief observation before we wrap up. Looking ahead, I reckon climate change poses a significant dilemma for the Coalition. Since the Abbott era, the Coalition has been able to weaponise climate change to its political advantage. But weaponising climate change is a whole lot easier when global heating is an abstract risk, or in the minds of some, an entirely hypothetical possibility. I think that becomes harder to navigate when heating is a lived reality – when people are dealing with the practical consequences of natural disasters, like the summer we’ve just experienced. But as they say in the classics, only time will tell. More immediately, Morrison’s challenge is to stabilise and turn public perceptions. This is a critical mission. The prime minister, a former campaign director, respects research and data, so he will know that recent political history suggests voters make up their minds about prime ministers and governments pretty early in a term. Those perceptions, once formed, are hard to budge. I reckon Julia Gillard, Abbott and Malcolm Turnbull could all share a story or two about the increasingly transient nature of political honeymoons in Australia. So, in summary, The Plan® you keep hearing about carries a lot of freight. There are actual, practical plans to manage risks, and there are political projections of risk management. For the government, both will feel important. Implicitly Morrison wants to tell you he is learning on the job. The past week in politics tells us this. The prime minister and the government need a way to apologise for the summer and reboot, without admitting any liability, because introspection and contrition is really not a hallmark of this government, at least not in public. If contrition is impossible, because sorry is such a hard word to say, then perhaps competence, assuming that materialises, can serve as a substitute."
"**A freeze on public sector workers' pay would damage the country's economic recovery, the shadow chancellor will warn in a speech on Monday.**
Chancellor Rishi Sunak needs to find ways to protect public finances after borrowing large amounts to fight Covid.
He declined to say if he would consider freezing public pay but added that it had to be considered in ""the context of the overall economic climate"".
On Wednesday Mr Sunak will set out government spending for the next year.
One think tank, the Centre for Policy Studies, has suggested a three-year pay freeze across the public sector could save up to Â£23bn.
But in a speech on Monday, Ms Dodds will argue that ""freezing the pay of firefighters, hospital porters and teaching assistants will make them worried about making ends meet ahead of Christmas - that means they'll cut back on spending and our economy won't recover as quickly.
""The British people shouldn't have to pay the price for a government that doesn't know the value of public money, splurging it on outsourced contracts to Tory-linked firms that don't deliver.""
Frances O'Grady - head of the Trades Union Congress - also expressed concern, telling Sky News: ""We saw ministers join millions of us clapping firefighters, refuse collectors, social care workers - I don't think this would be the time to reward them with a real pay cut.
""This is not smart politics, it is morally obscene and it is bad economics, too,"" she added.
Also speaking to Sky News' Sophy Ridge On Sunday programme, Mr Sunak said he would not comment on public pay before the spending review but added that it was reasonable to consider the subject in ""the context of the overall economic climate"".
""We need to see what is going on with wages, jobs and hours across the economy,"" he said.
He insisted that the spending review would not signal a return to austerity, arguing that government spending on public services was increasing.
Government borrowing has increased massively as a result of the coronavirus pandemic and a recent estimate by the Office for Budget Responsibility says the government would have to borrow Â£372bn for the current financial year - that compares to the Â£55bn it had expected to borrow pre-pandemic.
Ms Dodds will also use her speech to say that her party would ""make responsible choices"" to protect the economy.
Her plan includes bringing forward Â£30bn in capital spending over the next 18 months to spend on the ""clean industries"", setting up an emergency programme to retrain workers and establishing a National Investment Bank.
She is also expected to blame Mr Sunak for blocking a 'circuit beaker' - a set of time-limited restrictions - which Labour proposed earlier this year. She will argue this led to ""a longer, more painful lockdown"".
""The chancellor's irresponsible choices and unacceptable delays are damaging the economy. That's why we're in the grip of a jobs crisis - and it's got Rishi Sunak's name all over it.""
Speaking to the BBC's Andrew Marr show, Mr Sunak said the economy was ""experiencing significant stress"", adding: ""I think now is the right time to focus on responding to the crisis and that means, yes we will be borrowing quite frankly an enormous sum this year to help us do that.""
Defending the government's use of contracts in the fight against coronavirus, he said the government had been right, at the height of the pandemic, to ""act fast"" rather than relying on 60-day procurement processes.
A Treasury spokesman said: ""This government's actions have protected millions of jobs and businesses across the country - including 9.6 million people on furlough and over a million companies taking government-backed loans.
""Later this week, the chancellor will set out the Spending Review, which will benefit all nations and regions of the UK, as part of our commitment to build back better."""
"
Russ Steele writes: President Obama will soon be on his his way to Copenhagen and his Bagdad Bob moment in Air Force One.  Climategate is sure to create some turbulence.

From  The Chilling Effect
Got any political cartoons on Climategate? Post links to them below.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e90be2e05',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**The Government of Jersey has been given the power by the States Assembly to make wearing masks in shops mandatory.**
Members also approved laws to limit the size of gatherings, as part of updated Covid-19 regulations.
Currently the wearing of masks is not legally enforceable, with the government continuing to promote them in guidance.
The regulations have not created new rules, rather they have set the terms of possible restrictions.
The maximum penalty for individuals breaching any mask or gathering law would be set at Â£1,000.
Children under 12 years old would not have to wear a mask, along with people exempt for health or disability reasons, according to the regulations.
If a law requiring wearing of face coverings is introduced, it would apply to specific workplaces where a member of the public is present as a customer.
The regulations also allow orders to oblige businesses to collect personal data to aid contact tracing and to refuse service to those not wearing masks.
Any restrictions on the size of gatherings will only apply to groups of 10 or more people.
Visiting people's homes in Jersey was banned during the first wave of the pandemic before the ban was lifted in May, although public health guidance has discouraged meeting indoors since."
"**The government is defending a decision not to negotiate with local authorities over which coronavirus tiers they will be in when lockdown ends next week.**
Matt Hancock said the battle in October with Greater Manchester over funding when it was moved it into Tier 3 had been ""bad for public health"".
He said it would not happen again under the new system coming in next month.
The row saw Labour mayor Andy Burnham strongly object to plans to put the region into the strictest restrictions.
Before the second national lockdown in England, ministers undertook negotiations with local authorities in order to settle on a package of measures to control rising rates of coronavirus infection in their areas, along with financial support to help mitigate any impact.
But the Labour mayor accused ministers at the time of treating the region as a ""sacrificial lamb"" by asking it to accept a proposal which the ""government's own advisers say won't work"".
Under new tiers announced by the Prime Minister on Monday, due to come into force once the lockdown ends on 2 December, there will no longer be a set of negotiations with local areas, with ministers instead relying on a formula to decide which areas are placed in what tier.
The allocation of tiers will be dependent on a number of factors, including each area's case numbers, the reproduction rate - or R number - and the current and projected pressure on the NHS locally.
Mr Hancock told MPs: ""The reason we are doing it differently is, whilst in most cases when we negotiated with most areas in the previous tiered arrangement, we had a high quality discussion which led to better outcomes.
""A case in point is Liverpool, where the case rate has fallen by over two thirds in the last three weeks.
""Unfortunately that wasn't the case in all local areas.""
Asked by Labour MP Graham Stringer whether he was referring to Greater Manchester, Mr Hancock said: ""That would be one example but not the only one.
""Sadly, in the case of Greater Manchester, cases carried on going up whilst we were trying to put in place the measures that were necessary.
""So, instead, we've proposed a set of measures within the tiers which are fixed, also financial support which is agreed by formula rather than negotiation.
""We will have engagement but what we won't have is a two-week long negotiation while the cases still go up. That is bad for public health.""
Speaking to Sky News, Mr Burnham accused the government of ""walking away"" from negotiations with him and ten other council leaders.
He further accused ministers of ignoring advice to bring forward a national circuit breaker lockdown in September."
"
This is an interesting survey that cuts across a number of lines and held beliefs. I believe it to be worthwhile to participate in this survey. – Anthony

Guest post by Tom Fuller
If you are tired of having everybody trying to tell you what you think, and especially if what you think isn’t what’s being reported, I heartily encourage you to take this survey. I will be doing the analysis for free and for fun over the next few weeks, and I hope that we will be able to break new ground on the debate over global warming.
Thank you for participating in Examiner.com’s First Annual Survey on Global Warming. The introduction is below. Have fun!
First, let’s start with the ground rules. Your participation is completely anonymous, and no attempt will be made to contact you for any reason as a result of your participation or anything you write in this survey.
Second, this survey is not intended to be used as an opinion poll or a census, and will not be used as such. We are not trying to find out how many people ‘believe’ or ‘disbelieve’ in global warming. Our purpose is to try and find out if there are areas of agreement on possible policy initiatives going forward.
Click here to get started. Examiner.com’s First Annual Survey on Global Warming. 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e91bf420a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Like Others Of Its Ilk, The Minnesota Public Radio Censors Comments On Its Climate Blog
Guest Post by Bob Tisdale
Click to visit website
This morning while checking blogs with the phrase “sea surface temperature” I happened on the Minnesota Public Radio Updraft © climate change blog. Meteorologist Paul Huttner authored a post there titled “Could 2010 be the hottest year ever?” Link:
http://minnesota.publicradio.org/collections/special/columns/updraft/archive/2009/10/could_2010_be_the_hottest_year.shtml
The post begins with, “The numbers are in, and it looks like the “global cooling” theory just melted away.” It has the requisite link to the typical news release (Seth Borenstein’s (AP) article “Statistics experts reject global cooling claims”) and a two-year-old GISS Annual Global Temperature Anomaly Graph, even though a graph of current data would have better helped his cause. But what struck me and caused me to comment there was, first, Huttner’s use of the Climate Change Attribution graph…
 http://i39.tinypic.com/2s0o2uo.jpg
…which he wrongly attributes to Kerry Emanuel, and, second, his projection that 2010 could be the warmest on record while hinting that ENSO would ultimately be responsible for it.
I felt obligated to advise him of his error in attribution of the graph and of the fact that the Climate Change Attribution graph uses outdated TSI data. I also reinforced the ENSO-global climate link over the past decade by quoting from Knight et al (2009), but noting that Knight et al make an error in their assumption that the relationship between ENSO and global temperature is linear. Here’s what I wrote:
############
Paul Huttner: A few things. You attribute the Climate Change Attribution graph to Kerry Emanuel, but it’s actually from Global Warming Art:
http://www.globalwarmingart.com/wiki/Image:Climate_Change_Attribution_png
The graph is obsolete. It relies on an outdated (1993) Hoyt and Schatten TSI reconstruction that was manufactured, in part, to explain the rise in global temperature in the first half of the 20th Century. The current understanding of TSI variability shows little change in solar minimum:
 http://i40.tinypic.com/zjb977.jpg
I discussed this in detail here:
http://bobtisdale.blogspot.com/2009/01/agw-proponents-are-two-faced-when-it.html
As you imply, global temperature variations are dictated by ENSO. This is confirmed by Knight et al (2009) “Do global temperature trends over the last decade falsify climate predictions?”:
http://www.metoffice.gov.uk/corporate/pressoffice/2009/global_temperatures_09.pdf
They write, “El Nino–Southern Oscillation is a strong driver of interannual global mean temperature variations. ENSO and non-ENSO contributions can be separated by the method of Thompson et al. (2008) (Fig. 2.8a). The trend in the ENSO-related component for 1999–2008 is +0.08 +/- 0.07 deg C decade–1, fully accounting for the overall observed trend. The trend after removing ENSO (the “ENSO-adjusted” trend) is 0.00 +/- 0.05 deg C decade–1, implying much greater disagreement with anticipated global temperature rise.”
So there hasn’t been the anticipated rise in global temperature because, after you remove the effects of ENSO, the trend is zero. Therefore, if this year is a record year, it should be attributable to ENSO, not AGW.
Also note that Knight et al (2009) assume the relationship between ENSO and global temperature is linear. It is not.
http://bobtisdale.blogspot.com/2009/09/relationship-between-enso-and-global.html
Have a nice day.
############
And what did Meteorologist Paul Huttner do?
He rejected my comment.
UPDATE from MPR:
The missing comments were indeed being caught up in a spam filter. I’ve released the unpublished comments and they should be visible on the site now.
The comments that didn’t post had a large number of hyperlinks – suspect that’s why the filter didn’t like them.
Paul does not screen comments beforehand.
— Ken Paulman, managing editor for online news, MPR
Posted by Ken Paulman, MPR News | October 30, 2009  5:24 PM
It did take them almost 36 hours to find this and correct it.

 
< Sun wakes up: Strongest sunspot this year? | Main | Fall photos on a sunny day >

Could 2010 be the hottest year on record?
Posted at  8:59 AM on October 27, 2009    by Paul Huttner   (67 Comments)
The numbers are in, and it looks like the “global cooling” theory just melted away.
A new independent statistical analysis of climate records for the past 130 years confirms that the global temperature trend continues upward. The study was performed for The Associated Press by four independent university statistics experts. The four were given blind data sets and asked to analyze the trends, not knowing they were analyzing temperature data to remove any possible bias.

NASA annual surface temperature anomaly relative to 1951-1980 mean, based on surface air measurements at meteorological stations and ship and satellite measurements of sea surface temperature.
Some climate change skeptics have been claiming that the earth has been cooling since 1998, which until that time was the hottest year in the 130 global year surface record. 2005 was slightly hotter according to a NASA analysis. According to NOAA the last 10 years are the hottest decade anywhere in the modern global historical record.
It is remarkable statistically that the 13 warmest years in the modern record have all occurred since 1990. The fact that the 13 warmest years since 1880 could have occured by accident after 1990 corresponds to a likelihood of no more than 1:10 000. That’s the equivalent of flipping a coin and having it come up “heads” 14 times in a row.
Some climate change skeptics point to solar variability as the primary reason for climate changes on earth. The problem is, we’ve just observed two of the least active sunspot years in the last century in 2008 and 2009 during the current solar minimum. You would expect then that those two years would be cooler than average globally if the solar cycle theory is valid.
Instead, 2008 was the 8th warmest year in the global temperature record. And event though parts of the U.S. have been running cool in this year, globally 2009 is on pace to be the 6th warmest year on record. That pretty much shreds the solar variability only theory on global temperatures. Why did we observe two “top 10” warmest years during the lowest period of solar activity in nearly a century? Something else is at play here. Atmospheric changes are likely overcoming any natural solar variability.

Climate forcing graph shows solar variability as a much smaller climate change forcing component than greenhouse gasses.
(Sent to me by Kerry Emanuel MIT, based on Meehl et al. (2004) courtesy globalwarmingart.com)
This brings us to 2010, which is right around the corner. Several key elements appear to be in place that could produce one of the hottest years, if not the hottest year, in the modern global record.
1) The cooling effects of La Nina are gone in the Pacific Ocean. A moderate El Nino is gaining strength as we enter 2010. This may aid a rise in global temperatures in 2010.
2) The deepest solar minimum in nearly a century appears to be over. Sunspot 1029 formed rapidly this week and is the strongest this year. This could indicate the ramping up of solar cycle 24. Most astronomers expect a dramatic increase in solar activity in 2010.
If all these elements fall into place and the trend of recent decades continues, 2010 could be one of the hottest years on record.
Stay tuned.
PH


Comments (67)
Thanks for the nice article. I came across an article at www.icecap.us entitled “Comments on AP story: Statistics experts reject global cooling claims” that suggested the study overlooked something called the upper ocean heat content. What do you think about that?
Thanks.
Posted by Andy  | October 27, 2009  3:38 PM

Hi Andy:
 
Thanks for the comment.
I think the evidence is overwhelming that all measures of the planet are statistically much warmer than any sort of normal level in recent history would suggest.
Let’s see where we are after 2010 and in the next 5 to 10 years.
PH
Posted by Paul Huttner  | October 27, 2009  5:04 PM

Paul Huttner: A few things. You attribute the Climate Change Attribution graph to Kerry Emanuel, but it’s actually from Global Warming Art:
http://www.globalwarmingart.com/wiki/Image:Climate_Change_Attribution_png
 
The graph is obsolete. It relies on an outdated (1993) Hoyt and Schatten TSI reconstruction that was manufactured, in part, to explain part of the the rise in global temperature in the first half of the 20th Century. The current understanding of TSI variability shows little change in solar minimum:
http://i40.tinypic.com/zjb977.jpg
I discussed this in detail here:
http://bobtisdale.blogspot.com/2009/01/agw-proponents-are-two-faced-when-it.html
As you imply, global temperature variations are dictated by ENSO. This is confirmed by Knight et al (2009) “Do global temperature trends over the last decade falsify climate predictions?”:
http://www.metoffice.gov.uk/corporate/pressoffice/2009/global_temperatures_09.pdf
They write, “El Nino–Southern Oscillation is a strong driver of interannual global mean temperature variations. ENSO and non-ENSO contributions can be separated by the method of Thompson et al. (2008) (Fig. 2.8a). The trend in the ENSO-related component for 1999–2008 is +0.08 +/- 0.07 deg C decade–1, fully accounting for the overall observed trend. The trend after removing ENSO (the ‘ENSO-adjusted’ trend) is 0.00 +/- 0.05 deg C decade–1, implying much greater disagreement with anticipated global temperature rise.”
So there hasn’t been the anticipated rise in global temperature because, after you remove the effects of ENSO, the trend is zero. Therefore, if this year is a record year, it should be attributable to ENSO, not AGW.
Also note that Knight et al (2009) assume the relationship between ENSO and global temperature is linear.  It is not.
http://bobtisdale.blogspot.com/2009/09/relationship-between-enso-and-global.html
Have a nice day.
Posted by Bob Tisdale | October 29, 2009  4:32 AM

Here is a history of temperatures in Illinois. 2009 will likely be one of the coldest years on record. We could use a little warming so I hope you’re right. The state crop yield was horrible because of the cold.
 
http://www.isws.illinois.edu/atmos/statecli/Climate_change/iltren-temp.png
Posted by Windy City Kid  | October 29, 2009  8:23 PM

Could you give an example of why a comment would not be accepted on this site? Even from a fellow meteorologist?
 
Posted by Sera  | October 30, 2009  2:12 AM

Let’s parse that AP article:
 
“The statisticians, reviewing two sets of temperature data, found no trend of falling temperatures over time.“
Strawman. 2009 is warmer than 1979 and 1880. But the period between those two start points is not what skeptics have in mind by “over time.” They are referring to the most recent trend.
“And U.S. government figures show that the decade that ends in December will be the warmest in 130 years of record-keeping.”
Another technically correct pseudo-refutation. Since the first half of that period preceded heavy man-made CO2, and therefore warmed from another cause, it indicates there’s a non-anthropogenic component to the long-term warming trend—a component that could still be active. (I.e., the rebound from the LIA.)
“Global warming skeptics are basing their claims on an unusually hot year in 1998.”
Another strawman. Most skeptics (on WUWT, anyway) don’t choose 1998 as their starting point. Instead, they either claim it’s been cooling during the present century, or since 2002, or since 2004.
“They say that since then, temperatures have fallen — thus, a cooling trend. But it’s not that simple.”
A red herring (diversion). It IS that simple, because a short-term flattening and cooling trend falsifies the IPCC’s prediction for this decade, casting doubt on its models’ reliability; because it casts doubt on the implacability (and the urgency of the threat) of CO2’s alleged “forcing”; and because the PDO has flattened and turned negative at about the same time, which suggests that the PDO is the climate “forcer,” not CO2.
If a patient has a fever and the fever “breaks,” that breakage can’t be waved aside with the diversionary argument that the temperature decline hasn’t lasted long enough to be a long-term trend. No one is claiming it is a long-term trend–just that the fever (most likely PDO-driven) has broken.
Posted by Roger Knights  | October 30, 2009  2:14 AM

The Climate Change Attribution chart you show is now known to be incorrect.
1. The sun TSI figures are out of date.
http://www.leif.org/research/TSI-LEIF.pdf
2. Sulphate emissions were once thought to explain the cooling which took place between 1945-1975. It is now known that these emissions were very localised & were almost exclusively in the northern hemisphere and could not have had a global cooling effect.
This is a problem for the GHG hypothesis. How can one then explain the warming in the 1920’s & 1930’s, not dissimilar to the degree & pace of the 1980’s & 1990’s warming? How can we explain the cooling which followed the warming in the earlier period?
 
Posted by Geoff  | October 30, 2009  2:56 AM

I have read Anthony Watts comment which you rejected. It seems perfectly valid and reasonable to me. Perhaps you could explain why you decided to censor his comments.
 
Posted by Rodney Molyneux  | October 30, 2009  3:09 AM

Please explain why Bob Tisdale’s science-based comment was rejected.
 
Posted by Molon Labe  | October 30, 2009  3:23 AM

Paul,
 
I’ve read Bob Tisdale’s comment to your post and don’t understand why you haven’t replied to it. Here is the link to your colleague Anthony Watts’ WUWT blog site where it is posted so it can be compared to your post and evaluated:
http://wattsupwiththat.com/2009/10/29/minnesota-public-radio-cant-handle-comments-on-climate-change/
Your reply that “I think the evidence is overwhelming that all measures of the planet are statistically much warmer than any sort of normal level in recent history would suggest.” may be fine if you exclude the more reliable RSS and UAH satellite temp results showing near anomaly levels. But, why do that in favor of unreliable and openly selective GISS and NOAA results?
Posted by CO2isLIFE  | October 30, 2009  4:19 AM

Regarding the “Global Climate Change” graphic at the top of this page, what is really being measured? Does the Global Historic Climate Network (GHCN) measure the temperature of the earth or the temperature of the network?
 
The two are not the same.
The GHCN and its companion United States Historic Climate Network have a troubling problem with site quality. One that has been well documented by Climatologist Roger Pielke and Meteorologist Anthony Watts.
Many of us Minnesotans have seen the hilarious photo of an air conditioner in Detroit Lakes that exhausts hot air into a weather station sensor. However, few of us are aware of the more subtle problems – like with the station at Zumbrota where an asphalt parking lot has encroached on the USHCN station there.
No wonder NOAA and NASA claim that satellite data is “cooler”.  It lacks their station siting bias.
Posted by GregS  | October 30, 2009  5:24 AM

Paul,
 
RSS and UAH satellite temperature records exclude high latitude arctic regions which show the highest temperature anomalies. This is why they show a slightly lower warming tend the GISS record which includes these regions.
Regards,
Chris
Posted by Chris  | October 30, 2009  6:32 AM

Meteorologists are not having much success predicting annual or even seasonal temperatures of late. The UK Met Office similarly predicted that 2007 would be the hottest year ever (in Jan 2007) only for average temperatures to drop with the result that it was one of the coolest this century. And they have been wrong for about 6 winter / summer seasons in a row predicting mild winters and barbeque summers.
 
Predictions would be somewhat more believable if you could adequately explain why the IPCC2001 predictions have so far completely failed to materialise. Plotting actual temperatures against the predictions (p34 of the Summary report for policy makers) shows them underneath the ENTIRE RANGE. Why is that ?
Rgds
Imran
PS Its not cool to reject comments that are factual and scientific.
Posted by Imran  | October 30, 2009  6:46 AM

Chris,
 
What is the source of your assertion that RSS and UAH “exclude” high latitude arctic regions?
As for GISS including these regions, I would hardly credit a single thermometer in areas larger than Texas, as in inclusion.
On the other hand, to achieve the “hottest years ever” claim, NOAA has excluded the more accurate satellite data set of the oceans, and reverted to reliance on reports from buoys and tramp steamers.
It is all more the stuff of politics than science.
Posted by GregS  | October 30, 2009  6:52 AM

That last graph is awesome! The drop in sulfates and volcanics show an uncanny correlation to the rise in temperatures, far more than the CO2 does.
 
Volcanics have been proven and witnessed to have far more affect on climate than trace gasses.
Posted by Rick  | October 30, 2009  7:04 AM

I find it disturbing that an NPR related site would use outdated, politicized graphs and data, and then reject a comment from a highly qualified responder that merely attempts to update and de-politicize the graphs and data. Censorship of this type seems highly Nixonian, and contrary to the innate mores of NPR. Doesn’t TRUTH matter anymore?
 
Posted by Mike O’Kelly  | October 30, 2009  7:06 AM

I’m a bit curious as to why the second graph seems to end in the early 1990’s? Surely, we have some more current information. I’m also intrigued by the term, “greenhouse gases”. It’s a very prominent line, but quite a broad term, really.
 
Posted by BradH  | October 30, 2009  7:26 AM

Why is a government funded media outlet misrepresenting the facts so blatantly?
Journalism used to be about skeptical, tough looks.
Now, especially in the publicly funded media, the job is to sell the leftist view of any given issue.
That not one global warming prediction has been accurate will not change, no matter how much spin, misleading reporting, or suppression tax payer supported media engages in.
 
Posted by hunter  | October 30, 2009  7:29 AM

One thing in the statistical review article that seems curious to me is that their description of the data is not what anyone would use to describe the CO2 concentration over time (the Keeling curve). Given that CO2 increases are the foundation of the the Anthropogenic Global Warming theory, perhaps something other than CO2 is important.
 
Personally, I like Akasofu’s hypothesis that shows good correlation with a steady recovery from the Little Ice Age plus a 60 year periodic oscillation that fits the PDO.
See the full paper at http://people.iarc.uaf.edu/~sakasofu/pdf/two_natural_components_recent_climate_change.pdf
or comments and discussion at
http://wattsupwiththat.com/2009/03/20/dr-syun-akasofu-on-ipccs-forecast-accuracy/
[Aside – my url is down, apparently due to too many downloads.  It’ll be back Nov 1 or sooner if I throw money at the problem.]
Posted by Ric Werme | October 30, 2009  7:29 AM

I just read that you rejected a comment to this article from Bob Tisdale.   His comment can be found here:
 
http://bobtisdale.blogspot.com/2009/10/like-others-of-its-ilk-minnesota-public.html
I have read it, and it does not appear to contain anything that would cause a blog moderator to reject it.
Please post his comment so the readers here can see some well-documented information which is very pertinent to this article.
Thanks.
Posted by Fred C  | October 30, 2009  7:31 AM

Like Rodney Molyneux and other posters, I’d like to know how you can justify removing posts by those, such as Anthony Watts, who are capable of providing a coherent alternative to your arguments. What would be the public service in denying your readers the opportunity of the realization that you might be wrong? Or do the ends justify the means?
 
Best Regards,
Dr. Stritmatter
Posted by rstritmatter  | October 30, 2009  7:34 AM

Giss must be measuring another Arctic to the Danes.
 
Is there two of them?
http://ocean.dmi.dk/arctic/meant80n.uk.php
Posted by Ripper  | October 30, 2009  7:59 AM

As an Australian I am gobsmacked that censoring an esteemed person such as Bob Tisdale could happen in the so called land of the free.
 
Posted by Ripper  | October 30, 2009  8:01 AM

Silly article, silly “guess” at what will happen in 2010.  Yes, it’s warmer than it was in 1900, or 1901, or 1902 etc.
 
Warming trend since 1998 according to Nasa
http://www.woodfortrees.org/plot/gistemp/from:1998/plot/gistemp/from:1998/trend
Cooling trend since 1998 according to Hadley
http://www.woodfortrees.org/plot/hadcrut3vgl/from:1998/plot/hadcrut3vgl/from:1998/trend
Warming trend since 1999 Nasa
http://www.woodfortrees.org/plot/gistemp/from:1999/plot/gistemp/from:1999/trend
Cooling rend since 2001Nasa
http://www.woodfortrees.org/plot/gistemp/from:2001/plot/gistemp/from:2001/trend
Cooling trend since 2002 Nasa
http://www.woodfortrees.org/plot/gistemp/from:2002/plot/gistemp/from:2002/trend
Do they not teach math or statistics in University level meteorology??
Posted by Dan Robinson, PE  | October 30, 2009  8:42 AM

I warn you skeptics with great warning: Gaia will not be mocked! Cease all this endless caterwauling, or face her wrath!!! There are Three things, Three things you must do for Gaia: You must cease your mockery of the Faith, you must cease your vile consumption of meat, and you must make regular offerings to her prophets through the purchases of “carbon credits.” Oh, and you must dramatically reduce your industry and your emissions! Four, These Four things you Must do for Gaia, or She will smite you with great burning and endless woe and a really nasty heat rash!!!
 
Take heed, oh ye unbelievers!!!
Posted by The Goracle  | October 30, 2009  8:51 AM

Your rejection of the salient and respectful comments of Bob Tisdale concerning your post is very telling. A person of intellectual honesty and integrity would not do such a thing.
That implies you are not such a person.
 
Posted by Preston Calvert  | October 30, 2009  9:34 AM

As a native Minnesotan I am very disappointed in your lack of ethics in censoring a scientific comment. Pointing to the obvious propaganda piece by Seth Borenstein demonstrates a total lack of critical thinking. It is so blatantly cherry picked and unscientific.
 
Climate science is in its infancy. The warming claims are being made based on questionable data and simplistic computer models. Why do you think climate researchers are so much smarter than medical researchers that can’t cure the common cold (or cancers and hundreds of other diseases)? Yet, somehow in just a few years they’ve diagnosed the problem with a much more complex system called Earth and have a cure. More taxes. Why would anyone with an iota of common sense believe claims made about a poorly understood chaotic system when we already know that no scientist in any other field would make such bold claims with such limited knowledge?
Truly mind boggling.
Posted by Richard M  | October 30, 2009 10:00 AM

In the last 2 weeks the Pew Poll and the Harris Poll has indicated that the percentage of Americans who think man-made global warming is real has declined significantly, to about 36%.
 
This issue deserves fair and objective coverage. There is no shortage of knowledgeable people who can speak for the climate sceptics. Why is this side of the debate being ignored, on a public funded station?
Posted by r.wright  | October 30, 2009 10:11 AM

Do you really mean “hottest year ever”, or just the hottest year since the late 1800’s when we have some form of temperature record?
 
If reporters were honest with the facts then maybe the climate change discussions could be more reasonable. The fact that the earth has been much hotter in the past (and survived without any tipping points) is usually not mentioned and in fact hidden by headlines such as yours. Presenting the full facts to people might allow them to make sense of the discussions, rather than sensationalist headlines.
I wish the media would actual perform real journalism on climate change, where’s a good piece showing how the temperature anomaly graphs are created, the fact they use proxies, different number of temperature stations, how sparse the coverage is for a global temperature and how a global temperature is even calculated.
Posted by climatebeagle  | October 30, 2009 10:15 AM

“It’s not hard to hear consensus if you don’t hear any disagreement.”
 
Posted by vanderleun | October 30, 2009 12:15 PM

Where is the report written by these expert statisticians?
 
The only report I am aware of written by expert statisticians on climate controversies was the Wegman report, which confirmed Steve McIntyre’s criticism of the hockey stick picture.
Your headline is up there with those that claimed in 2007 that the arctic would be ice-free in 2008.
The AP article is full of utter nonsense, for example
“Since 1998, temperatures have dipped, soared,…”
There has been no soaring at all, in fact temperatures have levelled out since 1998 – even the head of the IPCC (Pachauri) has acknowledged this.
Posted by PaulM  | October 30, 2009 12:16 PM

On the other hand, although the silence from the author continues apace, it is indeed fortunate Watts noticed this item. Otherwise it would have the blog’s average comment stream: zero to two.
 
Posted by vanderleun | October 30, 2009 12:20 PM

One of the many problems with your report is that the GISS NASA data is dry-labbed. Hansen’s inscrutable algorithms massage (i.e. change) even recorded temperature data from the 1800’s to match his political beliefs. Add the urban heat island effect (which his data-changing algorithms exacerbate instead of mitigate), the unreliability of surface station data (caused by land use changes, for example paving a parking lot right next to the sensors, and moving sensors to be near or on top of buildings so that they can be automated), and the use of small numbers of measurements to cover vast unpopulated areas (thousands of square miles), and you find that GISS is just not trustworthy. It should never be used. Any time I see it used in an article, I disregard all of the author’s conclusions, because a reputable author who has done his homework would know the issues regarding it.
 
Posted by Scott  | October 30, 2009 12:39 PM

Minnesota Public Radio:
 
“Our Mission is to enrich the mind and nourish the spirit, thereby assisting our audiences to enhance their lives, expand perspectives and strengthen their communities.”
How can one enrich the mind and nourish the spirit if one refuses to listen to another point of view? There is more than one perspective to this global warming business and only honest, unbiased reporting by publicly funded media  – can deliver it.
Posted by Richard Just | October 30, 2009  1:05 PM

FYI:
 
I have not rejected Bob Tisdale’s or any other comments on the site. I (and MPR) accept all comments as long as they do not have profanity etc.
If a comment did not appear it was a techincal error. Please re-submit any comments.
You guys must be posting from Australia or something as many of the commetns came in the wee hours of the morning here. Don’t you guys sleep?
I was off duty at an appointment this morning through midday here Minnesota time. I do appreciate the comments and traffic!
More soon…
PH
Posted by Paul Huttner  | October 30, 2009  2:16 PM

//That last graph is awesome! The drop in sulfates and volcanics show an uncanny correlation to the rise in temperatures, far more than the CO2 does.
 
Volcanics have been proven and witnessed to have far more affect on climate than trace gasses.
Posted by Rick | October 30, 2009 7:04 AM //
Yes Rick, large volcanic eruptions have a significant temporary global cooling effect. Tambora and Pinatubo are great examples. They just don’t seem to occur often enough to play a role in long term climate.
PH
Posted by Paul Huttner  | October 30, 2009  2:47 PM

Is there a good reason why the graph shows an increasing solar influence when the current consensus is that over the long term (excluding the pseudo-11 year cycle) total Solar irradiance is near enough constant?
 
Posted by Sean Houlihane  | October 30, 2009  2:51 PM

Minnesota Public Radio:
 
“Our Mission is to enrich the mind and nourish the spirit, thereby assisting our audiences to enhance their lives, expand perspectives and strengthen their communities.”
//How can one enrich the mind and nourish the spirit if one refuses to listen to another point of view? There is more than one perspective to this global warming business and only honest, unbiased reporting by publicly funded media – can deliver it.
Posted by Richard Just | October 30, 2009 1:05 PM //
Richard: I think you can see many perspectives right here in these blog comments. And MPR is roughly 90% funded by our wonderful members’ contributions and underwriting.
People support us precisely because we give a fuller, deeper, more balanced approach to news than any other media outlet. That is why we are the clear number one rated radio station in this market.
PH
——————————————————————————–
Posted by Paul Huttner  | October 30, 2009  2:58 PM

//Do you really mean “hottest year ever”, or just the hottest year since the late 1800’s when we have some form of temperature record?
 
Posted by climatebeagle | October 30, 2009 10:15 AM //
Beagle:
Yes, a more accurate title might have been “Hottest Year on Record?”
I will change it.
Thanks..
PH
Posted by Paul Huttner  | October 30, 2009  3:03 PM

//As a native Minnesotan I am very disappointed in your lack of ethics in censoring a scientific comment. Pointing to the obvious propaganda piece by Seth Borenstein demonstrates a total lack of critical thinking. It is so blatantly cherry picked and unscientific.
 
Posted by Richard M | October 30, 2009 10:00 AM //
As I posted here, no comments have been censored. I have asked Mr. Tisdale to re-post his comment. If it did not make it thought it was purely a technical reason. It would be nice if people would check these things out before they claim “censorship.”
Clearly you can see all of the other posts made it through.
MPR does not censor commentary on blogs.
PH
Posted by Paul Huttner  | October 30, 2009  3:11 PM

I am SHOCKED that MPR would allow the above blog, but not the comments of comments of Bob Tisdale. As pointed out by Anthony Watts, many of what you call facts are outdated or just wrong.
 
For those who aren’t drunk on Al Gore’s cool-aid, and would like to become informed on this subject, Try going to Watts Up Wuth That.
Fewer Americans now believe in the Global Warming hoax than believe in Haunted Houses.  Eventualy the truth will prevail.
2009 Hottest year BALONEY.
Posted by Ronald Hansen  | October 30, 2009  3:15 PM

There are few things more disappointing than an NPR reporter refusing to accept valid criticism and then hiding behind the excuse of “technical error”.
 
The bottom line is Tisdale is correct and you made an error. A grotesque error. Not only in your analysis (which obscures the real science and does it a tremendous disservice), but in the intelligence and knowledge of your audience.
Posted by David Walton  | October 30, 2009  3:17 PM

Bob Tisdale:
 
Neither myself nor anybody at MPR rejected you comment. It must be a technical issue. Please re-submit your comment. As you can see all other comments have posted just fine.
It would be good to check with me personally before you post a claim that MPR “censors” comments. We do not. My contact information is easily available on the MPR site.
It is ironic that all your comments must be approved by the blog author.(you) All comments to Updraft post immediately, without my approval.
From your site: “Comment moderation has been enabled. All comments must be approved by the blog author.”
Amazing.
I attempted to post this on your blog but cannot as I do not have a Google account.
PH
Paul Huttner
Chief Meteoroloigst
MPR
phuttner@mpr.org
Posted by Paul Huttner  | October 30, 2009  3:30 PM

In the web address, it reads ‘publicradio’. Is it? Or does this website, like so many other media outlets that claim to be public, just another arm of ideologists that do not wish to see anything contradicting the AGW agenda.
 
Posted by David Alan  | October 30, 2009  3:52 PM

To All:
 
Thanks so much for the great posts.
With all respect to those who somehow find a way disagree with the fact that our planet is getting warmer, just try this.
Take out a coin and try to get “heads” on a flip 14 times in a row today.
From my Updraft post:
“It is remarkable statistically that the 13 warmest years in the modern record have all occurred since 1990. The fact that the 13 warmest years since 1880 could have occurred by accident after 1990 corresponds to a likelihood of no more than 1:10 000. That’s the equivalent of flipping a coin and having it come up “heads” 14 times in a row.”
This is perhaps the most compelling data that stack the deck in favor of AGW. How can anyone account for the fact that since 1984 we have not observed one year globally cooler than the 1961-1990 average?
http://www.globalwarmingart.com/wiki/File:Instrumental_Temperature_Record_png
You would expect that half the years since 1984 would have been below that average. And yet there is NOT ONE YEAR COOLER THAN AVERAGE since 1984?
Instead, we have seen the 13 warmest years since 1990? That’s a one in 10,000 shot folks.
Global warming “skeptics” are simply on the wrong side of the data. The only way you come to a conclusion that does not recognize global climate change is if you have a preset opinion.
I am always open to credible peer reviewed science that changes scientific theory and thinking. Say what you want, but the overwhelming scientific evidence is on the side of continued planetary warming in the coming decades.
Let’s see where we are after 2010, and beyond.
Enjoy the weekend.
PH
Posted by Paul Huttner  | October 30, 2009  3:54 PM


Richard: I think you can see many perspectives right here in these blog comments. And MPR is roughly 90% funded by our wonderful members’ contributions and underwriting.
 
Paul,
My point is that given the content of the comments it is apparent there is another perspective to the global warming issue. One supported by well-meaning, honest citizens and scientists that does not get published by MPR, NPR or other mainstream media.
MPR receives 64% operating revenue from “public” sources including grants from endowments, foundations, and businesses. About 20% of your budget comes from listener/member contributions (which is admirable.)
One might reasonably speculate that the larger percentage of grants from corporate and foundations may color your selection of climate science news. Which does not necessarily “expand perspectives and strengthen their [audience] communities.”
Posted by Richard Just  | October 30, 2009  3:55 PM

Paul,
 
Re: Instead, 2008 was the 8th warmest year in the global temperature record. And event though parts of the U.S. have been running cool in this year, globally 2009 is on pace to be the 6th warmest year on record. That pretty much shreds the solar variability only theory on global temperatures. Why did we observe two “top 10” warmest years during the lowest period of solar activity in nearly a century? Something else is at play here. Atmospheric changes are likely overcoming any natural solar variability.
As a meteorologist, you no doubt understand seasonal lag. On an oceanic level, there is also a lag to heat or cool it, and then realize the affect on temps, and that lag is much longer. I think it may be a bit premature to declare the solar / temp theory “shredded.” I’m interested to hear your thoughts regarding this.
Posted by Terry  | October 30, 2009  3:58 PM

Paul Huttner: A few things. You attribute the Climate Change Attribution graph to Kerry Emanuel, but it’s actually from Global Warming Art:
http://www.globalwarmingart.com/wiki/Image:Climate_Change_Attribution_png
 
The graph is obsolete. It relies on an outdated (1993) Hoyt and Schatten TSI reconstruction that was manufactured, in part, to explain the rise in global temperature in the first half of the 20th Century. The current understanding of TSI variability shows little change in solar minimum:
http://i40.tinypic.com/zjb977.jpg
I discussed this in detail here:
http://bobtisdale.blogspot.com/2009/01/agw-proponents-are-two-faced-when-it.html
As you imply, global temperature variations are dictated by ENSO. This is confirmed by Knight et al (2009) “Do global temperature trends over the last decade falsify climate predictions?”:
http://www.metoffice.gov.uk/corporate/pressoffice/2009/global_temperatures_09.pdf
They write, “El Nino–Southern Oscillation is a strong driver of interannual global mean temperature variations. ENSO and non-ENSO contributions can be separated by the method of Thompson et al. (2008) (Fig. 2.8a). The trend in the ENSO-related component for 1999–2008 is +0.08 +/- 0.07 deg C decade–1, fully accounting for the overall observed trend. The trend after removing ENSO (the “ENSO-adjusted” trend) is 0.00 +/- 0.05 deg C decade–1, implying much greater disagreement with anticipated global temperature rise.”
So there hasn’t been the anticipated rise in global temperature because, after you remove the effects of ENSO, the trend is zero. Therefore, if this year is a record year, it should be attributable to ENSO, not AGW.
Also note that Knight et al (2009) assume the relationship between ENSO and global temperature is linear. It is not.
http://bobtisdale.blogspot.com/2009/09/relationship-between-enso-and-global.html
Have a nice day.
Posted by Bob Tisdale | October 30, 2009  4:17 PM

You MPR scientific types are getting a reputation. Is it true your easy?
 
Posted by Fred J Harris  | October 30, 2009  4:30 PM

“Take out a coin and try to get “heads” on a flip 14 times in a row today.”
 
Paul, please don’t be so condesending. Many of the skeptics are hard core statisticians. No one is aurguing that the climate is Bernoulli process (like a coin toss process).
The correct question is: how likely is it to see the range of temperatures of the past few years, given the historical temperature range during this interglacial period?
To answer that question, you must know what the temperature variability has been over the past few thousand years.
Just because Google’s stock has traded high for the past month doesn’t mean that its in a run-away race condition and will never cycle back to lower prices.
Posted by mpaul  | October 30, 2009  4:39 PM

@Paul Huttner
 
” All comments to Updraft post immediately, without my approval.”
“Clearly you can see all of the other posts made it through.”
I posted a comment early this morning regarding the fact that GISS not only show more of a warming trend than RSS and UAH; but it also shows more of a warming trend than HadCRUT3. That comment elicited a reply that my comment was being held in moderation pending approval of the blog owner. The comment never made it through.
Posted by Dave Middleton  | October 30, 2009  4:42 PM

Paul Huttner: You wrote, “Neither myself nor anybody at MPR rejected you comment. It must be a technical issue. Please re-submit your comment. As you can see all other comments have posted just fine.”
 
I resubmitted my comment at ~5:15PM today and received the following reply page.
#####
Updraft
Minnesota Public Radio chief meteorologist Paul Huttner blogs about our region’s favorite conversation starter.
Thank you for commenting.
Your comment has been received and held for approval by the blog owner.
Return to the original entry.
####
Check your spam filter, Paul.
Posted by Bob Tisdale | October 30, 2009  4:43 PM


Paul,
 
My point is that given the content of the comments it is apparent there is another perspective to the global warming issue.
Posted by Richard Just | October 30, 2009 3:55 PM
//Hi Richard: “perspective” and “science” are two different animals. This is especially true in climate change discussions.//
PH
One might reasonably speculate that the larger percentage of grants from corporate and foundations may color your selection of climate science news. Which does not necessarily “expand perspectives and strengthen their [audience] communities.”
Posted by Richard Just | October 30, 2009 3:55 PM
Richard: I can assure you that NO ONE at MPR has or ever will tell me what to discuss or publish regarding climate change. If they did, I would walk immediately and make it public.
In fact MPR has the highest journalistic ethics of any news organization I have ever been fortunate enough to work for. And I have been fortunate enough to have worked at some of the best; including WCCO-TV in the Twin Cities and WGN-TV in Chicago.
My analysis and perspective on global climate change are my own, and are not dictated or influenced in any way by MPR or it’s supporters.
PH
Posted by Paul Huttner  | October 30, 2009  4:51 PM

Paul Huttner: You wrote, “Neither myself nor anybody at MPR rejected you comment. It must be a technical issue. Please re-submit your comment. As you can see all other comments have posted just fine.”
 
I resubmitted my comment at ~5:15PM today and received the following reply page.
#####
Updraft
Minnesota Public Radio chief meteorologist Paul Huttner blogs about our region’s favorite conversation starter.
Thank you for commenting.
Your comment has been received and held for approval by the blog owner.
Return to the original entry.
####
Check your spam filter, Paul.
Posted by Bob Tisdale | October 30, 2009 4:43 PM
Hi Bob:
I’m not an IT guy, so I don’t know why you would get that message. I will be happy to forward it to those who would know.
Obviously your comment above comment made it through…along with nearly all the 50+ others. Again, I do not personally approve any comments to Updraft.
I would be happy to respond to any data you can show me. But you are not being censored in any way.
Would you please remove or change your incorrect blog post headline below?
“Like Others Of Its Ilk, The Minnesota Public Radio Censors Comments On Its Climate Blog”
Again, you could have easily contacted me personally before inaccurately claiming to be “censored.”
PH
Posted by Paul Huttner  | October 30, 2009  5:04 PM

“Skeptics are on the wrong side of the data”
 
Simple right? Except exactly what data, what time frame (no cherry picking); do we take into account the recovery from the LIA?
Do we demand that temps continue to rise with increased CO2 or excuse the lack of increase the last decade or so?
Is it necessary for skeptics to prove CO2 does not raise temps much or do AGW fans have to prove it does?
Or is it just get in, shut up and hold on?
Posted by Ed  | October 30, 2009  5:21 PM

The missing comments were indeed being caught up in a spam filter. I’ve released the unpublished comments and they should be visible on the site now.
 
The comments that didn’t post had a large number of hyperlinks – suspect that’s why the filter didn’t like them.
Paul does not screen comments beforehand.
— Ken Paulman, managing editor for online news, MPR
Posted by Ken Paulman, MPR News | October 30, 2009  5:24 PM



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e925df52a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Not since the depths of the financial crisis has panic set in on the scale experienced over the past week. Conversations on the bus, at work and in the pub arefocused on the prospect of a deadly disease spreading uncontrollably. Stock markets are falling as fear about the coronavirus heightens and the potential cost for the world economy becomes increasingly apparent. Wall Street has suffered the fastest reversal since 1933 during the depths of the Great Depression. The Dow lost more than 10% of its value in a week from record-breaking highs to the lowest point since 2016. More than $5tn (£3.9tn) has been wiped off the value of global markets. The FTSE 100 is not immune, plunging the most in a week since the 2008 crash. Markets are expected to fall further this week. In the battle to contain the coronavirus through travel bans, school closures and the cancellation of business conferences, the damage to companies, economic growth and living standards could be unprecedented. According to Capital Economics, world growth could collapse into recession for the first time since 2009. The safeguarding of life takes primary importance as the death toll around the world climbs, but the knock-on economic impacts will also have lasting repercussions.  Given the growing impacton lives and livelihoods, it is clear that another parallel with the 2008 chaos is needed: a coordinated international response to ease fears around the globe. Against a backdrop of nationalism and protectionism in Britain, the US and several other countries, however, such thinking may be wishful. Gordon Brown led the global response to the 2008 crisis by bringing together the world’s richest nations to tackle it together. Labour’s last prime minister played an instrumental role in convening the G20 to organise the fightback by expanding government spending. Until Brown stepped in, handling the fallout had been uncoordinated and shambolic. Even though his task was complicated by a relatively protectionist US president in George W Bush, a quick glance across the Atlantic now seems to render his difficulties benign compared with the current tensions in global relations. After the London meetings of the G20 in early 2009 with the banking system on its knees and riots on the streets, the world’s wealthiest nations agreed an unprecedented and concerted fiscal expansion, promising to create or save millions of jobs which would otherwise have been destroyed. The price tag amounted to $5tn. It is hard to imagine such an approach today. The collaboration to prevent the Great Recession from becoming a rerun of the Great Depression did not, however, last long. SA slide into austerity, populism and protectionism began soon after in many nations, leaving the door open to Brexit, Donald Trump, Alternative für Deutschland (AfD), Viktor Orbán in Hungary and Jair Bolsonaro in Brazil. The world today seems to lack figures who might orchestrate a fightback against the coronavirus. Boris Johnson is missing in action, apparently unable to travel as far as northern England, the West Country or Wales to show leadership following widespread floods. Donald Trump believes that all is fine in his election year, dismissing the biggest single-day crash in US stock market in history and labelling coronavirus as the Democrats’ latest hoax. The story of the outbreak speaks volumes about the constitution of the world economy and political landscape after decades of ever-closer integration. At this time of questioning and challenging the international order, it is the tale of late capitalism writ large. Despite only joining the world trade system two decades ago as a bit-part player, China, which is at the centre of the coronavirus outbreak, accounts for around a fifth of the world economy and is key to global supply chains, a factor that great amplifies the economic fallout. Rising numbers of middle-class citizens, cheap flights and better transport have boosted tourism numbers worldwide from about 500 million in 1995 to more than 1.3 billion in 2017. People are increasingly on the move, and with them, infectious diseases. Health scares are becoming more frequent, with increasingly evident links to global heating and extreme weather events. Rising flood waters, bush fires and hurricanes restrict access to safe water, and food and sanitary living conditions, and put pressure on healthcare systems. After a decade of austerity the fear is that the NHS might not have the capacity to respond, and the eminent academic Sir Michael Marmot confirmed last week that austerity has had a disproportional impact on the life expectancy of the poorest in Britain. Pandemics are more easily spread and have a greater impact as a result of globalisation, complex supply chains, travel and tourism. A world of 24-hour news and social media fuelled by rapid advances in technology – including the spread of fake news – fans the flames of panic. It is a situation where fear can spread faster than the pathogen, according to David Owen, chief European economist at the US Bank Jefferies, who said most economic losses from infectious disease outbreaks result from the actions of unaffected individuals. There are many lessons. Globalisation, technology and climate change make the spread of viral disease easier and incubate many other social and economic ills. Lurching headlong into a protectionist and luddite world will not provide adequate and lasting solutions. The scale of our collective problems demand international coordination."
"**A well-known rest stop on the A9 in the Highlands has closed temporarily after travel restrictions led to a marked fall in visitor numbers.**
The Ralia CafÃ© near Newtonmore is in Highland, a level one area.
But much of its custom comes from people travelling to and from parts of Scotland placed under the tighter rules of levels three and four.
Since April, visitor numbers have fallen by 63% compared with the same time last year.
In a ""normal"" year the cafÃ© owners expect about 150,000 customers, with an additional 100,000 just using its toilets.
The levels system and travel rules have been put in place to supress the spread of Covid-19.
The Ralia CafÃ© is located on the A9 close to Highland Council's boundary with Perth and Kinross - a level three area.
For travellers it is the road's last comfort break going south, after Newtonmore and its facilities, before leaving the Highland Council area, and the first heading north after leaving Perth and Kinross.
The cafÃ© opened in July 2005 on the site of a former tourist information centre and is run by Robin and Sheila Lambie who live in Kingussie.
Mr Lambie said: ""We closed the cafÃ© on Sunday until travel restrictions are lifted, and my team is furloughed yet again.""
The toilets have also been closed.
The cafÃ© could open again before the end of the year when, for a short time over the Christmas break, rules on travel are due to be relaxed.
But Mr Lambie said: ""It is really impossible to tell at this stage.""
He said that despite the ""fantastic location"" for anyone travelling north or south, revenue for 2020 was currently down 63% against the same period last year.
""Going to levels three and four immediately halved our turnover and legally restricting travel halved it again.""
Everyone living in level three or level four local authority areas must, by law, remain within their own council boundaries unless they have a ""reasonable excuse"" for doing so.
People in level four must also keep journeys within their own area to an absolute minimum.
Meanwhile, people in level one or level two areas must avoid any unnecessary travel to areas that are under level three or four restrictions and should minimise unnecessary journeys between areas in different levels."
"The world’s oceans are plagued with the problem of “dead zones”, areas of high nutrients (such as nitrogen and phosphorus) in which plankton blooms cause a major reduction of oxygen levels in the water. Sea creatures need oxygen to breathe just as we do, and if oxygen levels fall low enough marine animals can suffocate. This commonly happens around coastlines where fertilisers are washed from fields into rivers and the sea, but also mid-ocean, where currents trap waters in gyres (large systems of rotating ocean currents). To date most studies have shown that these dead zones have been growing with global warming. But a recent study published in Science by Curtis Deutsch and colleagues suggests that the ocean’s largest anoxic zone – where there has been a total depletion of oxygen – in the eastern tropical North Pacific, may in fact shrink due to weakening trade winds caused by global warming.  The trade winds drive water away from the coast, and the gap is filled by new cold and nutrient-rich waters that come up from the deep. These nutrients trigger algae and plankton blooms upon which larger animals feed, which builds up an accumulation of organic matter. As bacteria decompose this organic matter the oxygen in the water is depleted. This causes low oxygen areas, such as the oxygen minimum zones (OMZs) with very low oxygen content found at intermediate ocean depths. Weaker trade winds would mean less upwelling of these deep nutrient-rich waters, and consequently less plankton and less oxygen depletion. Deutsch and colleagues affirm that although initial oxygen content will be lower due to higher temperatures, oxygen demand will decrease as trade winds do. So, the result would be that low oxygen areas in the tropical north Pacific would shrink. Natural dead zones can be found worldwide, particularly near regions where strong upwelling occurs. These natural dead zones have typically had low oxygen levels over huge lengths of time, due to ocean circulation patterns that prevent mixing. Although these OMZs are natural, they can become larger and more intense due to human activities, such as prolonged and intensive use of fertilisers, changes in land use, deforestation, soil erosion, global warming, and waste waters from cities or industry. All these are well known to cause algal blooms and so drive the expansion of oxygen-depleted areas. In fact, dead zones caused by these human factors have increased over time. Naturally occurring OMZs have also been expanding as temperature rises, so the paper’s prediction that such oxygen minimum zones would shrink flies in the face of previous studies. Animals increase their respiration rates as temperature rises, so they need more oxygen to breathe at higher temperatures. Warmer water also dissolves less oxygen, so as climate change warms the oceans the amount of oxygen decreases, making the effects on marine life even more acute. Warming also encourages water stratification, where the water separates into layers based on temperature or salinity, creating a physical barrier that prevents oxygen reaching deeper waters. Previous studies have predicted a weakening of trade winds in tropical areas, but have also forecasted changes to low-pressure weather fronts over coastlines that would lead to stronger winds, sufficient to replace any upwelling effect lost by weaker trade winds.  It seems likely that, in the same way, greater water stratification will lead to a worsening and expansion of dead zones, counteracting any effect the weakening trade winds might have to halt the process of de-oxygenation, and the paper’s authors acknowledge that this is possible. Taking everything into account, it seems that the process of warming oceans under climate change will inexorably lead to larger areas of oxygen-poor ocean, with all the knock-on effects for marine life that entails."
"There seem to have been a dozen or so explanations for why the Earth’s surface has warmed at a slower rate over the past 15 years compared to earlier decades. This is perhaps not so surprising given the complexity of the climate system – the world’s best detectives will inevitably struggle to disentangle the factors which influence every lump and bump in the surface temperature record.  However, recent research implicates natural changes in the Pacific and Atlantic oceans as the prime culprits.  Just as the apparently random motions in a river’s flow can shift before our eyes from one minute to the next, the gradual sloshing about of our vast ocean waters can influence Earth’s climate from one year to the next and from one decade to the next. It is clear that natural variability has and always will influence the climate. In addition to chaotic ocean fluctuations, changes in the brightness of the sun and variations in the frequency and intensity of volcanic eruptions (which cool the planet temporarily with sunlight-reflecting aerosol particles) influence the surface temperature. The recent Intergovernmental Panel on Climate Change working report found that these natural factors have contributed toward the slowing rate of surface warming since 1998.  However, recent measurements of ocean temperature made by thousands of automated buoys and observations of Earth’s radiative energy budget by satellite instruments indicate that heating has continued at a rate equivalent to every person worldwide using about 20 kettles each to continuously boil the oceans. This is consistent with what is expected from the rising atmospheric concentrations of greenhouse gases due to human activity. If anything, Earth’s heating rate increased between the 1985-1999 and 2000-2012 periods, despite a slowing in the rate of surface warming.  So, how is it possible for increased heating to not directly correspond with surface warming? The Earth’s heating is caused by an imbalance between the amount of absorbed sunlight and the heat emitted back to space. This surplus of heat is primarily absorbed by the oceans since they command the lion’s share of storage capacity compared with other parts of the climate system such as the land, the atmosphere or the cryosphere (ice and snow). This large heat capacity of water is noticable from the amount of time it takes to heat up your pan of vegetables. And there is a lot of water in the oceans; nearly a fifth of a cubic kilometre of water for each person on the planet. Crucially, the temperature at the Earth’s surface depends upon where this heat is deposited in the oceans. If the upper levels warm, so too will the atmosphere above. However, if ocean circulations cause more heat to be drawn down to deeper depths (or less heat to be moved upward toward the sea surface) then surface temperatures will reflect this. Recent research has implicated our largest ocean, the Pacific, as the most likely mechanism for subducting heat to deeper levels. Indeed, atmospheric and ocean conditions in the Pacific have been unusual in the past decade and computer simulations show that decades of slow surface warming despite rising greenhouse gas concentrations are associated with increased heating below 300m depth. The mechanisms for heat absorption are less clear; the simulations show that similar patterns appearing to originate from the Pacific are associated with the draw-down of heat in the North Atlantic and Southern Ocean as well as the Pacific. New research published in Science now shifts the focus towards the Atlantic Ocean. Xianyao Chen and Ka-Kit Tung of the University of Washington show that heating from rising greenhouse gas concentrations has preferentially warmed the ocean’s 300-1500m layer since about 2000, thereby depriving the upper layers of this surplus heat and causing surface warming to slow.  The authors say these changes are part of a natural cycle of knock-on effects, involving ocean circulation responses to changes in how salty (and therefore dense) the upper Atlantic Ocean layers are. This cycle is thought to last around 30 years, contributing a sustained cooling effect then a warming influence on surface temperatures; when combined with steady heating from greenhouse gas increases this leads to a “staircase” effect of stable temperatures followed by rapid warming.  They argue the previous focus on the Pacific was based upon simulations that were unable to fully capture the intricacies of the Atlantic Ocean circulation. An observed decline in the North Atlantic Ocean circulation over recent years has also been identified as part of a longer-term shift based upon evidence from computer simulations. The changes in ocean circulation have also been shown to influence seasonal extremes and, based upon the proposed Atlantic mechanism, may persist for another decade before rapid warming is re-established. However, the nature of internal ocean fluctuations means it is difficult to pin down timings with any confidence.  While it is human nature to seek a single cause for notable events, in reality the complexity of the climate system means that it is unlikely there is one simple reason for any extreme weather event or a decade of unusual climatic conditions. Nevertheless, the recent hiatus in global surface warming has encouraged scientists to further scrutinise and learn in even finer detail than before the workings of our climate system."
"
Share this...FacebookTwitterDer Spiegel here reports on Climategate 2.0, first reminding us that although Climategate 1.0 showed that a group of scientists had indeed stonewalled revealing the data and engaged in gatekeeping, they were cleared by several investigations of misleading and manipulating data.  Der Spiegel also admits Climategate 1.0 scarred the image of the science.
Now we have 2.0.
Der Spiegel writes that the UEA confirms the authenticity of the latest e-mails. Der Spiegel writes:
The chief of the Climatic Research Unit (CRU) of the University of East Anglia had to admit that he had deleted important e-mails on the documentation of his research. He also requested other scientists to destroy data in the same manner.”
Der Spiegel also brings up Ray Bradley’s “pathetic” e-mail, which slammed the 2003 paper by Michael Mann and Phil Jones. But Der Spiegel then quotes climate scientists saying that “such disputes are normal in science”. Maybe so, but the paper is still pathetic and was knowingly used to sway public policy.
Der Spiegel also reports on the “jubilation of the skeptics”, quoting Anthony Watts “What’s Up With That”:
Climate change skeptics cheered the e-mails. ‘They are authentic and spectacular!’ proclaims Watts Up With That?.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Here we see that Der Spiegel refuses to learn that we are not skeptics of climate change, but rather we are skeptics of climate catastrophe fairy tale in the future. It doesn’t matter how many times you tell them. Der Spiegel obviously is unaware that temperatures haven’t risen in almost 15 years, and likely think they are still rising. But we can we do? We (skeptics) are not special education teachers for media who have learning disabilities.
Finally Der Spiegel quotes the author of “pathetic” papers, Michael Mann, saying that Mann told the AP that it’s an organized “campaign by the oil and gas industry and criminal hacking of websites.” and that it “illustrates how desperate the skeptics are”.
At least give Der Spiegel credit for bringing up the story and for linking to the hacked e-mails – though not many Germans are going to read them.
What would it take for the media to take this widespread crime seriously? The lies, cover-ups, and deceit are right there in back and white. What more do you need? Tax authorities, for example, throw people in prison based on a lot less.
Thanks, Der Spiegel.
 
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterNASA photo of Irene
Irene is the first hurricane to hit the US mainland in almost 3 years since Ike hit back in September 2008. So much for global warming causing more hurricanes.
In fact, if no hurricanes had hit the United States this year, it would have been the longest lull between U.S. hurricane landfalls in recorded history, this according to data from the National Oceanic and Atmospheric Administration NOAA.
But that hasn’t stopped the disaster-craved German media from splashing huge block-letter headlines of Irene on the front pages, warning of an “unprecedented event”, a monster scale Armageddon, etc. Call them schadenfreude-junkies. They’ve been waiting 3 years for a fix.
Yesterday German media hopes of a huge hurricane disaster were once again boiling with life. Like the glory days of Katrina. What follows are just a few samples of German media headlines / excerpts (emphasis added).
Der Spiegel: Thousand-Kilometer Storm Threatens America’s Coast
President Obama warns of an ‘extremely dangerous storm’  and has interrupted his vacation: With 175 km/hr wind speeds Irene threatens a number of large cities on the US coast, tens of thousands of people are fleeing. Meteorologists are measuring the hurricane with air crafts. Their data are alarming.
The Big Apple itself is threatened with more than 100 billion dollars in damages; in low-lying areas like Manhattan hospitals and retirement homes have been evacuated.”
and
In the storm’s interior, inferno conditions: The wind near the eye is at 175 km/h; thus the hurricane has reached category 2 or 3 on the 5-category scale.”
Die Welt: In New York It’s Life And Death



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Irene is on course for New York: airports are closing, public transportation is shutting down. The mayor is preparing the the citizens for the worst.”
and
‘If you get an order to evacuate, please heed it’, the President added. The hurricane has to be “‘taken seriously’. All previous information say that it is going to be a “historic hurricane’.”
Bild: Fear of “Irene“! New York Fears Total Chaos
Hurricane Irene has put millions of people on the US coast in a state of fear  – destructive wind gusts and storm surges threaten. A state of emergency has been called in metropolitan New York. The Monster Storm could steer directly for the city and trigger a historic catastrophe!”
and
Hurricane expert Bernard expects that Irene will be scaled up to  category 4 during Friday. If the storm does not weaken again, then it would cause extreme damage to buildings with winds of 210 to 249 km/hr.”
FOCUS: Hurricane Irene Grows To The Size Of Europe 
Millions of Americans are fleeing: hurricane Irene is approaching the east coast. Already the storm has reached about the size of Europe. Due to the immense threat, New York has ordered an unprecedented mass evacuation.
Obama warns of a ‘historic hurricane’.”
Today, all media outlets are backpedalling big time, and look just a bit silly.
Share this...FacebookTwitter "
"**Chancellor Rishi Sunak is unveiling the government's spending plans for the coming year.**
The Spending Review will include details on public sector pay, NHS funding and money for the devolved administrations in Northern Ireland, Scotland and Wales.
Mr Sunak will also set out the extent of the damage done to the UK economy by the coronavirus pandemic.
A No 10 spokesman said the economic forecasts will be ""a sobering read"".
The government's Covid response has led to huge spending and borrowing rises.
The chancellor is expected to begin his statement at around 12:30 GMT following Prime Minister's Questions.
Some Spending Review announcements have already been trailed.
These include an extra Â£3bn for the NHS in England to help tackle the backlog of operations delayed due to Covid, an increase in defence spending and a Â£4.6bn package to help the unemployed back to work.
The government is expected to announce a cut in the UK's overseas aid budget to 0.5% of national income, down from the legally binding target of 0.7%.
There have also been reports that the chancellor is considering a pay freeze for all public sector workers except frontline NHS staff.
Plans to change the way big spending projects are analysed \- which the Treasury says is currently biased in favour of the south east of England - will be published alongside the Spending Review.
The chancellor may also choose to set aside money to tackle climate change and regional inequalities.
Devolved governments will receive money proportionate to any funding England gets in the Spending Review.
This is decided using the Barnett formula - devised by Lord Barnett, a Labour politician, in the 1970s.
Mr Sunak and Treasury Chief Secretary Stephen Barclay updated the Cabinet on Wednesday morning.
A Downing Street spokesman said: ""Cabinet was told the OBR forecasts will show the impact the coronavirus pandemic has had on our economy and they will make for a sobering read, showing the extent to which the economy has contracted and the scale of borrowing and debt levels.
""But - as the IMF (International Monetary Fund), OBR and others have pointed out - the costs would have been much higher had we not acted in the way we have done.""
""It's going to look horrible.""
The simple truth about the Spending Review according to a senior MP.
The chancellor will bang the drum for his plans to keep people in jobs, or help find new ones.
Rishi Sunak will take out the metaphorical megaphone to explain how he'll allocate billions of taxpayers' cash to spend on infrastructure in the coming months.
But the headlines of the Spending Review, when governments put their money where their mouths are, won't be in any rhetorical flourishes at the despatch box, nor likely in any surprise announcements kept back as goodies for the public.
The government had intended to use the Spending Review to set out its plans for the next three years, however this was reduced to just one year due to the economic turmoil caused by Covid.
The difficult financial backdrop will dominate this year's review with the economy projected to be 10% smaller than it was pre-virus.
Tax revenues have fallen as many businesses have been forced to close and government schemes to support furloughed workers have led to soaring levels of spending.
Public borrowing is expected to rise to Â£372bn - compared to the Â£55bn the government had originally expected to borrow.
The Spending Review will be accompanied by economic forecasts from the Office for Budget Responsibility - including predictions on how tax will be raised.
Labour's shadow chancellor Anneliese Dodds said the government's ""irresponsible choices"" during the pandemic had ""led to our country experiencing the worst downturn in the G7, and created a jobs crisis"".
""This prime minister and his government talk a good game but they haven't delivered on their promises - and regional inequality has got worse under their watch,"" she said.
""They clapped for key workers - but now they're freezing their pay, and looking to scrap planned minimum wage increases for the private sector.""
Unions called for Mr Sunak to maintain investment in the public sector, the TUC's deputy general secretary Paul Nowak telling BBC Breakfast ""now is not the time to make cuts to public services"".
And the SNP is calling for a huge stimulus package to support growth and jobs across the whole of the UK.
""The spending has to match the challenges we see in the economy,"" said its economic spokeswoman Alison Thewliss. ""At the moment interest rates are at a record low so the government should be borrowing."""
"
Share this...FacebookTwitterNot that is hasn’t been obvious. The Leibniz Institute for Ocean Sciences of the University of Kiel reports here that ocean chemist Dr. Christa Marandino and lead a group that will use new, innovative measuring techniques to directly measure the exchange of trace gases between oceans and atmosphere. 
Dr. Marandino wants to fill another gap in climate science and the IPCC models. (Photo credit: J. Steffen, IFM-GEOMAR)
Excerpts of the IFM GEOMAR press release are as follows, with my comments:
It sounds so little. Only 0.04% of the earth’s atmosphere consists of CO2. And yet it is this tiny amount of gas that provides for a greenhouse effect on the earth, makes life possible and with a small change can lead to considerable increasing temperatures on the planet.”
Co2 concentrations have gone up about 110 ppm over the last 150 years, yet the temperature is only up 0.8°C. Much of that temperature increase is traced back to solar activity and ocean cycles. So the above statement is certainly a load of BS. Note how they complete ignore acknowledging water vapour, aerosols, the sun and ocean cycles as climate factors.
Other trace gases include methane, dimethyl sulfide or also acetone. And like CO2, these gases are continuously exchanged between the oceans and the atmosphere. To which scale and speed the exchange occurs is an important factor for atmospheric chemistry, and thus for climate change. Unfortunately there are no measurements for many of these material flows at the boundary between water and air. Mathematical formulas that have been used up to now have proven to be inaccurate in getting values.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In other words: The models have failed. This is a clear admission that they don’t understand the climate mechanisms and that the science is filled with gaps.
Dr. Christa Marandino of the Leibniz-Institute for Ocean Sciences (IFM-GEOMAR) wants to close this gap using a new techniques. The method is called eddy correlation technique. ‘Simply said, one just measures the vertical wind speeds, the changes in gas concentrations and connect the two,’ explains the 35-year old scientist. For CO2 this method is already being used by some work groups worldwide. But for the most trace gases with tiny concentrations, the technical difficulties have been too large so far. The measurements have to be extremely fast and highly precise, and that on a bobbing measurement platform like a research ship.”
Don’t worry about it. I’m sure you’ll find the right correction factors and get the results you need. Anything goes in climate science. The money will always be there.
‘That’s the problem’, explains Dr. Maradino. She has already conducted the first promising attempts with newly developed equipment, an atmospheric pressure chemical ionization mass spectrometry“ (APCI-MS), at the University of California in Irvine. Dr. Marandino has been working in Kiel since 2008 and she wants to tweek the technology more here. The Helmholtz-Gemeinschaft Deutscher Forschungszentren (HGF) will support Dr. Marandino beginning January 2012 with the formation of a Helmholtz group. Over the coming 5 years she will receive  €250,000 for her research.”
The press release then ends with the usual “we are very glad to have this young scientist working here” blah blah blah.
I don’t mean to devalue her work here by any means. I’m just surpised to read that even though some insist the science is settled, we hear of yet another admission that it is indeed filled with gaping holes. And that means the IPCC models are discredited.
Share this...FacebookTwitter "
"
Cold event setups in atmospheric circulation patterns are aligning. Two days ago I brought to your attention that there was a strong downspike in the Arctic Oscillation Index and that the North Atlantic Oscillation Index was also negative. See The Arctic Oscillation Index goes strongly negative
Yesterday, Senior AccuWeather meteorologist Joe Bastardi let loose with this stunning prediction on the AccuWeather premium web site via Brett Anderson’s Global warming blog:

What is facing the major population centers of the northern hemisphere is unlike anything that we have seen since the global warming debate got to the absurd level it is now, which essentially has been there is no doubt about all this. For cold of a variety not seen in over 25 years in a large scale is about to engulf the major energy consuming areas of the northern Hemisphere. The first 15 days of the opening of the New Year will be the coldest, population weighted, north of 30 north world wide in over 25 years in my opinion.
The Climate Prediction Center discussion for their forecast also concurs with both of the above:
THE AO INDEX WHICH RECENTLY HAS BEEN VERY STRONGLY NEGATIVE IS FORECAST TO INCREASE SLIGHTLY IN VALUE BUT REMAIN STRONGLY NEGATIVE THROUGH DAY 14. TODAYS BLEND CHART INDICATES BELOW NORMAL HEIGHTS ACROSS ROUGHLY THE SOUTHEASTERN TWO-THIRDS OF THE CONUS, AND ABOVE NORMAL HEIGHTS OVER THE NORTHWESTERN THIRD OF THE CONUS, CONSISTENT WITH A STRONGLY NEGATIVE AO.
Here are two of the CPC forecast maps for the days covered by Bastardi’s forecast. It is fairly typical to see an above average temperature in the west when we get a cold deep jet stream in the east:


I was going to include some Met Office forecasts here but after trying to find something useful at their web site and failing to find anything, I gave up looking.
If you live in these areas: bundle up, stock up. Get ready.

Sponsored IT training links:
Get best quality 98-366  training material to prepare and pass 70-573 and 70-576 exam.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e905adfb0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
The Liberal Party in Australia’s parliament has a new leader.

Herald Sun Blogger and Columnist, Andrew bolt writes to me in an email:
Anthony,
 This may be a first: a major political party has dumped a global warming believer as leader and replaced him with sceptic who last month called AGW “crap”. Tony Abbott has tempered his public pronouncements since, but has today become the new Liberal leader, toppling warmist Malcolm Turnbull, specifically because he was the only one of the three contenders today to promise to delay the Government’s emissions trading scheme.
Bolt adds some background:
Following up with excerpts from new Liberal leader Tony Abbott’s memoir Battlelines, released in July.
On page 171 he quotes, with approval,  Bjorn Lomborg:
“Natural science has undeniably shown us that global warming is man-made and real. But just as undeniable is the economic science, which makes it clear that a narrow focus on reducing carbon emissions could leave future generations lumbered with major costs, without major cuts in temperatures.”
Abbott then adds:
“Without binding universal arrangements, any effort by Australia (on emissions trading) could turn out to be a futile gesture, damaging local industry but making no appreciable dent in global emissions…. Another big problem with any Australian emissions reduction scheme is that it would not make a material difference to atmospheric carbon concentrations unless the big international polluters had similar schemes. Australia accounts for about 1 per cent of global carbon dioxide emissions. At recent rates of growth, China’s increase in emissions in about a year could match Australia’s entire carbon dioxide output. Without binding universal arrangements, any effort by Australia could turn out to be a futile gesture, damaging local industry but making no appreciable dent in global emissions.”
He also questions what climate alarmists truly want:
“It’s hard to take climate alarmists all that seriously, though, when they’re as ferociously against the one proven technology that could reduce electricity emissions to zero, nuclear power, as they are in favour of urgent reduction in emissions. For many, reducing emissions is a means to achieving a political objective they could not otherwise gain.”
======
Lest you think that Climategate had nothing to do with this political shift, please read what Bolt had to say about its impact in my previous post:
The Australian ETS vote: a political litmus test for cap and trade
Several MPs have indeed mentioned the emails in their party room speeches, and your correspondents miss the way MPs actually pick up things.
Andrew Bolt has one of the most read blogs and columns in Australia and is helping to educate both people and politicians alike on the true costs of climatic induced cap and trade, please visit his blog to show some support. – Anthony
http://blogs.news.com.au/heraldsun/andrewbolt/

Following up with  excerpts from new Liberal leader Tony Abbott’s memoir Battlelines, released in  July.
 
On page 171 he quotes, with approval,  Bjorn  Lomborg:“Natural science has undeniably  shown us that global warming is man-made and real. But just as undeniable is the  economic science, which makes it clear that a narrow focus on reducing carbon  emissions could leave future generations lumbered with major costs, without  major cuts in temperatures.”Abbott then adds: 
“Without binding universal arrangements, any effort  by  Australia (on emissions trading) could turn out to be a  futile gesture, damaging local industry but making no appreciable dent in global  emissions.… Another big problem with any  Australian emissions reduction scheme is that it would not make a material  difference to atmospheric carbon concentrations unless the big international  polluters had similar schemes.  Australia  accounts for about 1 per cent of global carbon dioxide emissions. At recent  rates of growth,  China’s increase  in emissions in about a year could match  Australia’s  entire carbon dioxide output. Without binding universal arrangements, any effort  by Australia  could turn out to be a futile gesture, damaging local industry but making no  appreciable dent in global emissions.”
He also questions what climate alarmists truly want:
“It’s hard to take climate  alarmists all that seriously, though, when they’re as ferociously against the  one proven technology that could reduce electricity emissions to zero, nuclear  power, as they are in favour of urgent reduction in emissions. For many,  reducing emissions is a means to achieving a political objective they could not  otherwise gain.”
 




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9089f1f2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

As of this writing, Tuesday, September 11, Hurricane Florence is threatening millions of folks from South Carolina to Delaware. It’s currently forecast to be near the threshold of the dreaded Category 5 by tomorrow afternoon. Current thinking is that its environment will become a bit less conducive as it nears the North Carolina coast on Thursday afternoon, but still hitting as a Major Hurricane (Category 3+). It’s also forecast to slow down or stall shortly thereafter, which means it will dump disastrous amounts of water in southeastern North Carolina. Isolated totals of over two feet may be common.   
  
  
At the same time that it makes landfall, there is going to be the celebrity‐​studded “Global Climate Action Summit” in San Francisco, and no doubt Florence will be the poster girl.   
  
  
There’s likely to be the usual hype about tropical cyclones (the generic term for hurricanes) getting worse because of global warming, even though their integrated energy and frequency, as published by Cato Adjunct Scholar Ryan Maue, show no warming‐​related trend whatsoever.   






_Maue’s Accumulated Cyclone Energy index shows no increase in global power or strength._   
  
  
Here is the prevailing consensus opinion of the National Oceanic and Atmospheric Administration’s Geophysical Fluid Dynamics Laboratory (NOAA GFDL): “In the Atlantic, it is premature to conclude that human activities–and particularly greenhouse gas emissions that cause global warming–have already had a detectable impact on hurricane activity.”   
  
  
We’ll also hear that associated rainfall is increasing along with oceanic heat content. Everything else being equal (dangerous words in science), that’s true. And if Florence does stall out, hey, we’ve got a climate change explanation for that, too! The jet stream is “weirding” because of atmospheric blocking induced by Arctic sea‐​ice depletion. This is a triple bank shot on the climate science billiards table. If that seems a stretch, it is, but climate models can be and are “parameterized” to give what the French Climatologist, Pierre Hourdin, recently called “an anticipated acceptable range” of results.   
  
  
The fact is that hurricanes are temperamental beasts. On September 11, 1984, Hurricane Diana, also a Category 4, took aim at pretty much the same spot that Florence is forecast to landfall—Wilmington, North Carolina. And then—34 years ago—it stalled and turned a tight loop for a day, upwelling the cold water that lies beneath the surface, and it rapidly withered into a Category 1 before finally moving inland. (Some recent model runs for Florence have it looping over the exact same place.) The point is that what is forecast to happen on Thursday night—a major category 3+ landfall—darned near happened over three decades earlier… and exactly 30‐​years before that, in 1954, Hurricane Hazel made a destructive Category 4 landfall just south of the NC/SC border. The shape of the Carolina coastlines and barrier islands make the two states very susceptible to destructive hits. Fortunately, this proclivity toward taking direct hits from hurricanes has also taught the locals to adapt—many homes are on stilts, and there is a resilience built into their infrastructure that is lacking further north.   
  
  
There’s long been a running research thread on how hurricanes may change in a warmer world. One thing that seems plausible is that the maximum potential power may shift a bit further north. What would that look like? Dozens of computers have cranked away thousands years of simulations and we have a mixture of results: but the consensus is that there will be slightly fewer but more intense hurricanes by the end of the 21st Century.   
  
  
We actually have an example of how far north a Category 4 can land, on August 27, 1667 in the tidewater region of southeast Virginia. It prompted the publication of a pamphlet in London called “Strange News from Virginia, being a true relation of the great tempest in Virginia.” The late, great weather historian David Ludlum published an excerpt:   




Having this opportunity, I cannot but acquaint you with the Relation of a very strange Tempest which hath been in these parts (with us called a Hurricane) which began on Aug. 27 and continued with such Violence that it overturned many houses, burying in the Ruines much Goods and many people, beating to the ground such as were in any ways employed in the fields, blowing many Cattle that were near the Sea or Rivers, into them, (!!- _eds_ ), whereby unknown numbers have perished, to the great affliction of all people, few escaped who have not suffered in their persons or estates, much Corn was blown away, and great quantities of Tobacco have been lost, to the great damage of many, and the utter undoing of others. Neither did it end here, but the Trees were torn up by their roots, and in many places the whole Woods blown down, so that they cannot go from plantation to plantation. The Sea (by the violence of the winds) swelled twelve Foot above its usual height, drowning the whole country before it, with many of the inhabitants, their Cattle and Goods, the rest being forced to save themselves in the Mountains nearest adjoining, where they were forced to remain many days in great want.



Ludlum also quotes from a letter from Thomas Ludwell to Virginia Governor Lord Berkeley about the great tempest:   




This poore Country…is now reduced to a very miserable condition by a continual course of misfortune…on the 27th of August followed the most dreadful Harry Cane that ever the colony groaned under. It lasted 24 hours, began at North East and went around to Northerly till it came to South East when it ceased. It was accompanied by a most violent raine, but no thunder. The night of it was the most dismal time I ever knew or heard of, for the wind and rain raised so confused a noise, mixed with the continual cracks of falling houses…the waves were impetuously beaten against the shores and by that violence forced and as it were crowded the creeks, rivers and bays to that prodigious height that it hazarded the drownding of many people who lived not in sight of the rivers, yet were then forced to climb to the top of their houses to keep themselves above water…But then the morning came and the sun risen it would have comforted us after such a night, hat it not lighted to us the ruins of our plantations, of which I think not one escaped. The nearest computation is at least 10,000 house blown down.



It is too bad that there were no anemometers at the time, but the damage and storm surge are certainly consistent with a Category 4 storm. And this was in 1667, at the nadir of the Little Ice Age.
"
nan
"
Share this...FacebookTwitterClearing the way for a mountain top windpark. (Photo credit: Mountain Talk)
Just a few miles away from my hometown, Green Mountain Electric Power Company of Vermont and Quebec’s Gaz Metro are now building a monster mountain-top wind park that will be home (for 25 years) to twenty one 135-meter tall turbines that will go into operation by the end of next year.
To build the windpark they are now busily deforesting acre after acre and literally blasting off the top of the mountain, thus permanently disfiguring in a matter of weeks what took nature and the ice ages (climate change) hundreds of thousands of years to sculpture. Suddenly the environmentalists are fuming mad. Not this way, they insist.
Protesters even launched a website called Mountain Talk and are now making their presence known on the mountain.
Last week two protesters were arrested by the police for allegedly trespassing on the site. The two are reported to be students from nearby Sterling College, which according to its website is a “small, progressive, liberal arts college” that is committed “to grassroots sustainability”.
Were once enthusiastic supporters of renewable energy
What’s strange is that for years Vermont environmentalists railed against carbon based fuels and pressured legislators to produce “clean” energy. Never mind that over 90% of Vermont’s electric power was CO2-free to start with (hydro from Quebec and nuclear from Vermont Yankee). Vermont even elected Obama, giving him a whopping 68% of the vote. Let’s go green was the message. Now windparks are naturally getting installed on Vermont’s beautiful ridgelines. Doing that, though, isn’t easy. It involves massive deforestation to clear land for access roads and the windpark itself. Because Vermont mountains are ruggedly uneven, a good amount of dynamiting is part of it.
Suddenly environmentalists have woken up and are fuming mad about what they had for years enthusiastically endorsed. Many will deny this and claim that they never supported wind turbines. But looking at the website of Sterling College, where the two protesters attend, we see the following under Global Field Studies:


“Research environmental and cultural sustainability by comparing current ecological practices in Denmark, Iceland, Norway, and Sweden. Local scholars and experts serve as guides as we explore alternative energy sites, investigate eco-villages and industries featuring green technology, and study…”
 Either the protesters in Vermont are just plain hypocritical, or terribly confused. Sterling students even went to Japan (by plane) and saw this:




<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“Explore environmentally sustainable practices on the northern island of Hokkaido through visits to managed forests, organic farms, native Ainu communities, and  Zen gardens. Conversations with students and professors at Hokkaido and Obihiro  Universities lead to in-depth investigations into land use practices.”

That’s sure some amazing land-use in the photo above, wouldn’t you all say?
Protesters blame corporations and not the lawmakers for all the mountain mutilation
Okay, the protesters woke up awhile back and now realize that the green dream is in fact a nightmare. You’d think they’d vent their anger at the politicians who cleared the way for this energy transition. You’d be wrong. Throughout the Mountain Talk website, all you hear is a lot of whining and moaning aimed at…the corporations. At this point, I’d say they are confused.
If these protesters ever sat down down with energy managers, the first thing they’d hear is that there are lots of other cheaper, easier, cleaner ways of generating electricity than to blow up mountains and install monster turbines. Windmills and solar are hardly at the top of the power company’s wishlist for ways of generating electricity. They aren’t blowing the tops off mountains because they want to, but because they are being regulated to do so.
It’s too bad the protesters can’t get that through their skulls.
Protesters are turning to the politicians, begging them to stop the evil corporations. Pardon me, but it’s the politicians with the zany green ideas that need to be stopped. Mountain Talk quotes US Senator of Vermont Bernie Sanders here:
We are trying to save the world.”
In trying to “save the world” from a scientifically unsubstantiated doomsday, they are all ruining Vermont.
I truly do hope that the protesters succeed in stopping future mountain top windparks. If they pull it off, it’ll be what I like to call “One stupidity killing another.” Nowadays you have to take the victories anyway you can.
======================================
Hat-tip Benny Peiser: Wind-farms-are-useless-says-Prince-Philip
 

Share this...FacebookTwitter "
"The Obama administration has proposed several ad hoc multi-country economic agreements, and in doing so has abandoned de facto the World Trade Organization (WTO) as insufficiently malleable to its interests.  The two most important of these are the Trans-Pacific Partnership (TPP) and the more recent Transatlantic Trade and Investment Partnership (TTIP). Even as the latter was being negotiated by US and EU officials, the World Meteorological Organization (WMO) reported that the increase in greenhouse gases is more rapid than expected.  The organisation’s secretary-general warned that humankind is “running out of time” to reverse rising levels of carbon dioxide that drive climate change.   These two items, agreements to increase world trade and rapidly rising greenhouse gases, call for a bit of “linked up thinking”. The US trade and investment initiatives have come under considerable attack for handing too much power over public services to private corporations, for reducing employment rights and for harming national sovereignty.   Whatever the validity of these objections, there is a more fundamental problem.  The purpose of the TPP and the TTIP is to increase the volume of trade among countries, and that is inherently bad for humankind because of its environmental effects. I recently attended a meeting in London with environmental activists, including a well-known British climate scientist.  As a result of that meeting I realise that my past critiques of “free trade” have been far too timid and narrow.   The essential problem is not that these treaties foster US and EU corporate interests, though that is undesirable for the rest of us; the problem is international trade itself. The charts below show why.  The two countries with the most exports in 2012 are the US and China, with Germany and Japan considerably further back (both the US and China over US$2 trillion, Germany at just over 1.5). By no accident, China and the United States are at the top of the pollution list, with Japan fifth and Germany sixth. “But wait”, you say, these are also the largest economies in the world, so the issue is their domestic energy use, not whether what they produce is exported. World’s leading exporters 2012 (billions of US$) World’s Leading Polluters 2012 (millions of tons) Well, actually, no.  A moment’s reflection makes the fallacy clear (and I am ashamed to say that I only just realised something so obvious). When something is produced domestically, using it domestically is less polluting than exporting it.  Production represents only the first contribution of a product to the world’s pollution.  While only a fanatic would oppose the transport of bananas from the Caribbean to the UK, restricting the movement of cars between Japan and Europe would seem a no-brainer for the environment. Many studies show that the domestic production component of industrial pollution is least in Germany, followed by the United States, Japan, and China, by far the worst production polluter of five. Producing a product for export requires the company in question to transport it to the importer.  While ocean transport can claim a greener bill-of-health than aviation, that represents a classical example of “damning with faint praise”. Air pollution from ships is increasing, even as land-based emissions are gradually being reduced. As the campaigning group Transport & Environment points out:  If things are left as they are, by 2020 shipping will be the biggest single emitter of air pollution in Europe, even surpassing the emissions from all land-based sources together. When companies transport products over long distances the environment suffers. It really is that simple.   Over the past ten years the number of full sea-borne containers has more than doubled (that is, excluding “turn-around” of empty containers). And air freighting fresh fruit and vegetables puts shipping in the pollution-generating second division, the difference between an environmental misdemeanour and a felony.  One source estimates the 1% of the world’s food that travels to you and me by air contributes 11% of carbon emissions. All these transport-generated emissions occur before the pollution caused by the use and then disposal of the products themselves. Production, consumption and disposal apply to all products, of course, no matter their source and destination.  But exporting and importing make their own – and unnecessary – contribution to the destruction of the environment. The huge and growing amount of pollution due to international trade brings to mind the famous critique of free trade by John Maynard Keynes (infamous and blasphemous for mainstream economists): I was brought up … to respect free trade not only as an economic doctrine which a rational and instructed person could not doubt… As lately as 1923 I was writing that free trade was based on fundamental “truths” which … no one can dispute… [But now] I sympathise…with those who would minimise, rather than with those who would maximise, economic entanglement among nations. Ideas, knowledge, science, hospitality, travel – these are the things which should of their nature be international. But let goods be homespun whenever it is reasonably and conveniently possible, and, above all, let finance be primarily national. Were Keynes with us now (at age 151), he would have cause to omit or at least qualify his endorsement of “travel”.  “Flying Clean”, an organisation that seeks to reduce aviation pollution, reports “long-haul flights produce on average twice as much emissions per mile travelled per passenger than cars, and short-haul flights produce three times as much”.  These estimates let a Humvee pass as green. So let us join Keynes to imagine if we can a world in which goods are “homespun” and finance is “primarily national”.  If we cannot imagine such a world, there is little hope for the planet. The combination of climate deterioration and the ravages of international trade and finance will fulfil the prophecy of Hobbes, rendering life on Earth “poor, nasty, brutish and short”.  Less trade not more, to render life on Earth “prosperous, fraternal, peaceful and long”."
nan
"
Share this...FacebookTwitterThe online German-language Die Presse reports here on new climate research out of Stanford. Did this get picked up in the US press? Too nutty probably.According to the new research, Christopher Columbus was likely the cause of the Little Ice Age.
How, you may ask? Columbus, a really dirty European, and his fellow colonizers sailed the ocean blue in 1492, just before the LIA began in earnest. He brought along with him some nasty diseases which quickly spread across the new world and wiped out the indigneous population. As so after a very short time, there was no one left to burn down trees, and so tree growth exploded, sucked up huge amounts of CO2 from the atmosphere in the process, which in turn lowered global CO2 concentrations, which led to global cooling, and thus the production of the Little Ice Age.
Hey, at least they admit now that the Medieval Warm Period and Little Ice Age weren’t just local North Atlantic phenomena.
Global temperatures began dropping after the end of the Medieval Warm Period, in the 14th century. Richard Nevle, geochemist at Stanford University, claims that the Maunder Minimum, a period of low solar activity from 1653 to 1715, had less to do with causing the Little Ice Age and thinks he has solved the mystery and found the real culprit.
The cooling was already in full swing by the time the Maunder Minimum began and so it had to be something else. Die Presse writes:
It was Columbus and the following colonists. They caused the not so sparsely populated New World to get practically wiped out in a very short time. Back then at the end of the 15th century as Columbus was on the way, between 40 and 80 million people were living mostly in Central and South America, and they cleared forest with fire. But as the Europeans arrived, most of them died of unknown diseases – measles, pox, diphtheria – 90% of them were wiped out, or one fifth of the global population. The forests suddenly stopped being cleared.”
I wonder if this brilliance will be taken up by the IPCC’s next assessment report. What conclusion shall we draw? It confirms how desperate the warmists are in their quest to remove the sun from the climate equation. Pretty damn desperate.
Finally, before you parents think about spending $250,000 to educate your child at Stanford, you may want to think again.
rnevle@stanford.edu
 
Share this...FacebookTwitter "
"A proposal for radioactive waste to appear at a burial site nearby, would be likely to fill the great majority of the UK population with thoughts of danger, cancer – and falling house prices. This illustrates the huge problem of public misperception to overcome when disposing of radioactive waste. Britain’s nuclear reactors have generated low-carbon electricity since 1956, in doing so creating around 260,000m3 (about the size of 700 double-decker buses) of intermediate-level waste and 3,000m3 of highly radioactive high-level waste, as well as spent fuel, plutonium and uranium. The price for decommissioning past and existing nuclear power plant and disposing of that waste is around £70 billion – the single largest item of expenditure for the UK Department of Energy and Climate Change. What to do with radioactive waste is a problem that has so far proved to be intractable to successive generations of civil servants and ministers. In the mid-1970s, it was decided that deep burial would provide the optimum secure solution.  Here, radioactive waste would be packaged and contained for one million years, sealed by multiple chemical and physical barriers within a repository dug out around 500 metres below ground level. A serious attempt was made to investigate a site in West Cumbria close to Sellafield in the 1990’s, but that foundered on the complexity of the geology and flow of deep groundwater, making it difficult to predict how well sealed the waste would be into the far future. In 2003, the government set up an expert committee of social scientists and policy analysts which in 2006 affirmed recommendations to bury waste within a geological disposal facility as the best method for securing radioactive waste. Communities should be asked to volunteer themselves as potential disposal sites. However the only volunteers in 2010 were in West Cumbria – the same sites that had already been rejected in the 1990s. The government carried on, providing several million pounds for community engagement, but in January 2013 the process ground to a halt after local district councils voted in favour, only for the overarching county council to reject the proposition. Several issues of contention emerged. The right for the host community to withdraw was promised by the government, but never transcribed into any contract. A package of benefits to the hosting community was promised, but exactly what and when it would be paid was not stated. The definition of the host community, its boundary, and its relationship with the wider region remained vague. Exactly what waste would be buried was contested. And, as had previously been established, there was no confidence in the site’s geology. The government retired again to lick its wounds. Another review and public consultation was undertaken during 2013-14, from which emerged the White Paper “Implementing Geological Disposal” published in July 2014. The results show the government has done some serious listening, and it provides some distinctly new approaches.   First, a new body Radioactive Waste Management Ltd will be created to pursue a disposal site. The company will be wholly owned by the government and could propose more than one facility for different types of waste. This has been tried in the 1980s and 1990s with UK Nirex – a limited company wholly owned by UK government, which spent £400m investigating just one site. Can you spot any difference? So how this operates will be more important than the definition.   Second, the government states it is keen to “listen and respond to views and concerns”. Yet in the future this search will now become defined as a nationally significant infrastructure project, which means that many powers of local people to decide or influence could be restricted or removed. Specifically, the control and influence of local councils has been removed, combined with a statement that no tier of government will have the right to veto a project. So the responsibility of regional council authorities for managing this waste, and the associated road and rail and excavation infrastructure is also removed.  Third, the search for a site will become national, with a two-year period of geological survey and screening to identify suitable regions (not specific sites). Identifying secure regions may become difficult if the extensive fracking of England goes ahead for shale gas and oil, as, any effect on the underlying geology could affect a site’s long term secure storage potential.   Detailed geophysical surveys and borehole drilling will need planning permission to establish the suitability of a site, before the community opinion is consulted to make a binding test of acceptance, after which there is no withdrawal. For the first time, communities will be able to access independent expert advice and support. We can expect intensive education campaigns. But it is not stated who that “community” includes and how much support of what type is needed. As always, the agreement of the people is potentially the Achilles heel of the entire process.  Fourth – and most contentious – is the proposal that communities who volunteer will receive £1m per year for five years of exploration, followed by £2.5m per year for [up to 15 years](https://www.flickr.com/photos/deccgovuk/14705016331/in/set-72157645405696080/ of further investigations and design, which is likely to include additional surveys or drilling. On top of that potential £40m, there will be “substantial” benefits during construction to an accepting community.  The project will undoubtedly be extremely large.  Analogies with Crossrail are appropriate, which is estimated to cost more than £15 billion at today’s prices. Thousands of jobs will be created over 10-20 years of construction. But the facility will only require a handful of long term employees for day-to-day operation. Finally, as another major shift, the facility will be permanently sealed after 100 years of operation for even greater security. Potentially the most significant statement of all comes from the secretary of state for energy and climate change, Ed Davey, stating that arrangements for waste disposal have to be in place before planning consent will be given for new nuclear power stations. Does this mean that one or more sites need to be specifically identified before construction can start on the new nuclear reactors planned at Hinkley Point and elsewhere?  If formal discussions with new volunteers do not even start until 2016, and could conclude as late as 2030 – by which time Hinkley Point C should be generating power – that seems impossible. Perhaps ministers of the future be satisfied merely to know that the UK “has a plan”? If the past is any guide to the future, relying on such a plan didn’t help to find a nuclear mausoleum in 1978, 1996, or 2012."
"
Share this...FacebookTwitterGerman flagship ARD public television broadcast a half hour show that was actually fully devoted to poking fun at the now religious climate movement in Germany.The show even ended with a warning that the movement may be getting out of hand. Okay, it was aired at 11.15 p.m., long after most Germans had gone to bed. Better than nothing, I guess.
The video at the above link is in German and I can’t post here because of copyright. But if you understand German, it is worth watching.
The show features Henryk Broder and Hamed Abdel-Samad journeying through Germany where they stop at various climate protection events and projects, etc. and at times speak to various climate protection leaders. Although the pair pokes fun at the people they interview, one gets a sense of just how radical and fanatic the movement has become.
In Germany, man-made climate change is long established and institutionalised. Germany is well into its mission to rescue the planet. The media, institutes, information centres, schools, government offices, tax laws, economic central planning programs, subsidies, etc. are all in place and geared at rescuing the climate. Everyone is being advised today to do their part to protect the climate.
At the 17:30 mark Broder and Abdel-Samad stop by and surprise German Green Party Chief Claudia Roth, who clearly doesn’t want to talk to Broder. Eventually she does answer Broder’s question after he presses her to tell us what she thinks sustainability means, about the eco-dictatorship and forcing everybody to live that way. Roth says:
“I don’t want someone going around telling everybody ‘you have to do this, do that, do this. I just want the citizens to have a “human literacy” so that they can decide for themselves. I don’t want to act with bans, but I can see limits are necessary, so if you’ll excuse me I really have to go.”
What she means by “human literacy” is that she wants people to believe the fairy tale and that they behave the way she wants them to. And with “limits” she means that if people go beyond them, then it’ll be very uncomfortable. The green person that Broder is left talking with after Roth is chauffeured)away in a luxury car reveals more about how the green vision looks:
And if we succeed to live well with another, cultural, ecological and technological standard, for example mobility will change, agriculture and nutrition will change, living, that is everything will change …the Great Transformation)’


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Broder replies:
In the end a big beautiful world?”
Broder’s and Abdel-Samad’s journey takes them to the Bavarian Ministry of Environment, where a director tells them about the virtues of separating your garbage, installing solar panels and turning off the lights.
Next stop is a school in Hamburg with a project to climatically brainwash its kids. For example a teacher tells us how children are taught not to open the oven while the pizza is baking because it’s bad for the climate. Throughout the journey, Broder and Abdel-Samad’s often pose the question: “What do we do with those that don’t cooperate?”. The answer is always in a nutshell (paraphrasing):
“We have to convince them to do so…otherwise we’ll make their lives hell.”
Even a young pupil is shown telling us that people who don’t go along have to be made to feel embarrassed and guilty about it. One zealously committed teacher proudly says they are all “acting as role models for the rest of the world”.
At the 25-minute mark, see how it all works in the classroom and how the kids are allled to think it’s all going to lead to utopia. Indeed kids are assigned to run around the school to check the heaters, lights, ventilation, windows, etc.  Eco-absurdity and climate protection madness that has indeed gripped Germany.
Broder appropriately ends with an ominous comment as a warning:
I really believe that in every idea to make a better world, lurks a hidden threat.”
Share this...FacebookTwitter "
"
Share this...FacebookTwitterAwhile back Stefan Rahmstorf took a few readings off the coast of North Carolina and concluded sea levels were rising faster than at any time since Jesus had walked on water.
Well, Der Spiegel noticed it 63 years ago in 1948 and wrote a piece called “America will become less”. Obviously Rahmstorf had only recycled an old story from Der Spiegel when he wrote that paper. Needless to say his remix of that story was promptly debunked (J. R. Houston and R. G. Dean, Journal of Coastal Research 27 (4), 788-790 (2011).
In 1948, just after the 1910-1945 warm period, Mr. H. A. Marmer of the US Coast and Geodetic Survey of the North American Coastal Observation Bureau “reported that the sea level along the east coast of the USA was rising 6 mm/ year” and posed the question:
Is the country sinking or is sea level rising?  It is still not decided. The uniformity of change along the 2000-km coastline allows us to conclude that the sea is partly responsible.”
Der Spiegel then mentioned that sea level rise from sediment build-up from rivers discharging into the ocean adds 3-4 mm of sea level rise per century and writes:
A big part of sea level rise is the melting of ice. During the last ice age there was approximately 40 million cubic kilometers more ice on land than today. The melting of ice made the sea level rise about 100 meters over the the last 10-20 thousand years. And again a strengthened melting of glaciers and polar inland ice like a few decades ago could be responsible for the rising sea level along the American east coast.”
Der Spiegel also mentioned that Scandinavia was rising while the North German coast was sinking at a rate of 20 cm per century. Along North America Der Spiegel wrote that measurements had been taken since 1895, and that sea level in 1948 was accelerating:
…in the time until 1930 the coast sank at a rate that was only one seventh of the current rate. Then it began, from Florida in the south, to Maine in the north, a sinking of 6 mm per year. If that will continue, no one knows.”
As Der Spiegel was writing about melting polar caps and rising sea levels, it was only 26 years later in 1974 that they warned us of a new coming ice age.
Share this...FacebookTwitter "
"
The sun has seen a resurgence of activity in December, with a number of cycle 24 sunspots being seen. The latest is group 1039 seen below:

2009 is ending with a flurry of sunspots. Indeed, if sunspot                      1039 holds together just one more day (prediction: it will),                      the month of December will accumulate a total of 22 spotted                      days and the final tally for the year will look like this:                   From Spaceweather.com

The dark line is a linear least-squares fit to the data.                      If the trend continues exactly as shown (prediction: it won’t),                      sunspots will become a non-stop daily occurance no later than                      February 2011. Blank suns would cease and solar minimum would                      be over.
If the past two years have taught us anything, however, it                      is that the sun can be tricky and unpredictable.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9042f07b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"In an experiment for a forthcoming National Geographic television show Mind Over Masses, a pavement was divided into lanes, separating those that are walking from those walking while talking on mobile phones.  This may be a breezy way of showing the irritation some feel about how others use mobile phones. But the proposed solution is really just another form of control, when what we need is not a test of people’s obedience, but a reassessment of how we interact with each other in public.  Civil society, to remain civil, requires an agreed framework for behaviour in public. You might call it etiquette, although that word has often aroused suspicions, with some seeing it as an infringement of their personal liberties. Justifiably so, if you look at traditional concepts such as deferring to rank, treating women as the “weaker sex”, bowing and scraping. But etiquette based on mutual recognition of fundamental human equality can make civic life more free, and more pleasant, for everyone.  Simple street etiquette could improve safety and comfort for all, and save millions in signage, policing, separation of traffic, and various other complex and expensive means of regulating public space. Imagine uncluttered public landscapes where people are comfortable and can move easily. This is possible only by returning to first principles of civil society, and not resorting to mechanisms of control. The common law, after all, is rooted in ground-up custom, not top-down control. The alternative is a society based on hierarchy, where those accustomed to projecting an air of entitlement claim the largest share of public space. Or one based upon control and obedience, where people are told unequivocally where to be and are punished for stepping out of bounds, or both. The clearest examples are societies run by organised crime such as the mafia. Gangsters opportunistically thrive on mistrust, on inflexible hierarchies, and on a breakdown of civil society.  Democracy is the practice of resisting these forms of hierarchy and of concentration of power, wealth and cronyism – and the violence that often accompanies them. Our own society and institutions are not entirely free of the tendency towards using force and coercion, and it’s important to watch for it and resist it when it arises. But this sort of power play happens on a miniature scale on many streets everywhere in the UK every day. In practice, people walk to the left or right as they will, with many who are able to exercise their dominance preferring the inside of the pavement away from traffic. Some try to keep left as a rule, some keep right as a rule and others duck and dive to find the path of least resistance. It is either the most aggressive pedestrian or the person who can project the greatest air of entitlement that wins.  All of this before even taking into account the oblivious “meanderthal” – of which the finest example is surely the semi-aware, distracted, walking mobile phone user. It is both rude and undemocratic to walk while on the phone; it forces others to take full responsibility for both their comfort and safety and that of others, while the screen-bound meanderthal acts as if they are above such responsibility. Mistrust and micro-aggressions in public space are symptomatic of instability in the larger society. One small and simple measure that could change the pedestrian experience would be if people agreed to keep left on footways as a general rule. This is already loosely called for within the Highway Code, which stipulates that one should “avoid being next to the kerb with your back to traffic”.  This makes good sense, but is impossible for pedestrians to heed when cities are filled with exceptions such as one-way streets, signals, and other contrary signage. A simple, blanket rule could be agreed while we work on re-evaluating the rest. Shared space schemes, proven to make streets safer, seek to remove street clutter and rely on eye contact, interaction, and etiquette.  It’s common in the design of cities to use evidence-based studies and computer models to try to predict behaviour so that it may be accommodated or controlled. This does us a disservice – what and who is our public realm for, who does it serve, if not for our democratic communities? We must first ask how our public landscapes serve our highest common ideals, and then work the rest out from there. "
"The summer just finished was Australia’s second-hottest on record, with the temperature 1.88C above average, the Bureau of Meteorology says. The only hotter summer on record was the previous year, which was 2.14C above average. Temperatures this summer were above average across almost the entire country. Dr Blair Trewin, a senior climatologist with the bureau, said the hot summer, which was marked by the unprecedented bushfire crisis that devastated communities and wildlife in much of the country, was part of a long-term warming trend that had seen the country warm by 1.4C since 1910. Most of that warming has come since 1950. Trewin said: “That tells us the baseline is higher, and with that you have a higher risk of high extremes like we have seen in the past two summers. Our baseline expectation now is for warmer than average summers, and other seasons more generally.” The heat records are relative to the long-term average for the years between 1961 and 1990. The bureau’s summer report came as its officials told a Senate estimates hearing in Canberra that Australia is heating more rapidly than the global average. Karl Braganza, the head of climate monitoring, was asked how much Australia was projected to warm, given a scientific analysis involving the World Meteorological Organisation last year found average global temperatures were expected to rise between 2.9C and 3.4C by 2100 under commitments put forward by national governments as part of the Paris agreement. Braganza said the increase in Australia would be expected to be “closer to 4C” heating than the global average under that scenario, assuming countries did not do more than promised in Paris. Adam Bandt, the Greens leader, said the government was undertaking no planning for the possibility of 4C warming in Australia. “Australia saw this horrific bushfire season with just over one degree of warming. We’re hurtling towards 4C and it’s only going to get worse from here,” he said. This summer saw many more heat records broken than cool records. The bureau data shows 43 sites in NSW broke high temperature records but only five sites saw record lows. In Queensland, there were 10 heat records broken across different monitoring stations, but only one record for cool weather. The 2018-19 summer also saw below average rainfall across the country, with western NSW, south-western Queensland and the Top End particularly dry. Some areas, including Western Australia’s west coast, and parts of the east coast, saw above average rainfall. The second hottest summer comes after the bureau declared 2019  the hottest and driest year on record. Trewin said the summer could be characterised as a season “of two halves”. “We had December and the first few days of January, which was extremely hot almost nationwide, and extremely dry. Those heat extremes in December and early January were quite exceptional,” he said. That exceptional heat included 17 and 18 December, when on two consecutive days Australia recorded its hottest day on record. The national average temperature on 18 December was 41.9C – one whole degree hotter than the previous day. December 2019 also delivered the worst conditions for bushfires on a record going back to 1950. Several records were broken for warmest summer nights in NSW, Victoria and Queensland. On 1 February, nighttime temperatures in the rural area of Condobolin, west of Dubbo, NSW, did not drop below 34.7C. Aside from climate change, there were natural drivers of the heat in the early part of the summer. Trewin said in December and early January, the weather patterns were influenced by a strong Indian Ocean Dipole that had dragged moisture away from the continent and a “strongly negative Southern Annular Mode”. But as both these systems moved into a neutral phase, the rest of January and February was less extreme. Rainfall for January and February was slightly above average, Trewin said, but the extreme dry of December meant that across the three months rainfall was below average. But 10 February still delivered record rainfall for some parts of NSW. At Taralga, 100km west of Wollongong, the town had its wettest summer day on record with 197mm beating the previous record of 130mm, set in 1885. While the bureau’s report does not cover ocean temperatures, the hot summer has also seen the build-up of heat stress on the Great Barrier Reef, with scientists fearing a third mass coral bleaching event in the past five years if temperatures don’t fall in the next few days."
"
Share this...FacebookTwitterSimon Kuper of the Financial Times recently wrote a piece called Climate change: who cares any more?, which clearly reveals the frustration and growing resignation among warmists and how the climate cause is irrevocably ruined.
As the hoax of climate catastrophe becomes increasingly exposed, it is being taken far less seriously today than back at the peak of the scare in 2007. For example Kuper describes how he felt when leaving a climate conference in Britain in 2007:
I left feeling that if you were running a country like Britain in 2007, you probably thought climate change was the single overriding issue. Terrorism, immigration and even the economy were details by comparison.”
My how the mightiest of scares have fallen. If that doesn’t confirm that Gore’s movement is lost, then nothing will. In his opening sentence Kuper himself admits to having given up on protecting the climate, realising it is a senseless endeavour:
When someone offered me a trip to India, I said, “Definitely.” A couple of years ago I’d have fretted about the carbon emissions. But like almost everyone else, I have given up trying to prevent climate change.”
Kuper however blames the failure of the movement on the bad economy and human resignation and apathy, claiming people are more worried about their own prosperity. But the reality is that the science behind climate catastrophe has fallen apart, and this is being made known to the public. The public is realising that all the hype over climate change was mostly a hoax perpetuated by a few select greedy interest groups out to make a ton of money. That’s why the public is turned off over the climate issue. It just isn’t a real threat any more.
Now that the scientific data is getting analysed, people are realising it’s no longer necessary to worry about climate when deciding the direction of energy policy – because coal and other fossil fuels simply don’t have the destructive impact on the climate that was once hypothesized. The data simply doesn’t show it. Indeed while coal consumption globally increased 30% over the last 10 years, global temperatures have actually dropped.


The blue line shows skyrocketing global coal use, yet global temperatures have fallen.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Data source: Review of World Energy
So why would any country do something as stupid as cut back on cheap coal during cooling times? Especially when it would only lead to lots of people freezing to death.
Kuper complains and appears baffled by the fact that the media have dramatically reduced their reporting on climate change. Come on Kuper, admit it: It’s not because people have resigned, it’s because people have woken up, and the phony climate catastrophe has since become a non-issue. Few are interested in it, and
Al Gore’s 24 Hours of Reality clearly demonstrated the folly of crying wolf for the ten thousandth time. Now it’s all falling on deaf ears. Fewer people than ever now believe all the climate catastrophe hogwash. Recall how everyone saw through the whole charade and all the bias behind the reporting of Hurricane Irene, which exposed the desperation of the movement. I ask: just how stupid do you think the public is, Mr. Kuper?
It is over.
A better question is: How stupid can one possibly be not to see the man-made climate change charade, and to continue believing the obvious hoax of global climate catastrophe?
No, it is not resignation by the public. It is an awakening. Rich and poor countries alike have opted to continue to do what they have been doing for thousands of years: ADAPT. As sea levels rise their usual 1 or 2 or 3 mm a year, people will simply take one or two or three steps back each century. That’s what they’ve always done as climate has always changed in the past.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThis morning I found an e-mail from Vermont Senator Patrick Leahy in my mailbox. It was just a mass mailing asking readers to mobilize against the construction of a pipeline and warning of climate change. I’m a native of Vermont.
One last look at Vermont before the turbines come. (Photo credit: Creative Commons Attribution-Share Alike 3.0 Unported, 2.5 Generic, 2.0 Generic and 1.0 Generic license.
I remember visiting Washington back in 1982 and actually dropping by his Washington office out of the blue (back then you could do that) but he was out to lunch, or something.
Anyway I thought I’d send him a reply:
Dear Senator Leahy,
With all due respect, climate always changes. Just 12,000 years ago the Green Mountain State was buried under at least a mile of ice. Thanks to climate change, it isn’t so today.
Concerning today’s climate change, global temperature hasn’t risen in over 10 years, and many scientists say it won’t rise for another 30 or 40 years because of cyclic solar and oceanic activity. You haven’t heard?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




To understand climate, it first helps to know that it is a heck of a lot more complicated than just the straight line equation: CO2 regulates the climate and weather. To tell others that it is that simple is a disservice to the public. Why not talk to scientists from the other side? There are many. You haven’t done so yet? Seems to me it would be the responsible thing to do.
It really is time for you to retire, Mr. Leahy, and to let fresh ideas and open minds back into the Senate. Vermont thanks you for your service, but you’ve been in the Senate far too long.
Sincerely,
Pierre Gosselin
Native of Vermont
PS: I made a trip up to Crystal Lake last summer – nice wind turbines up there on the surrounding mountains. And I hear such a landscape-beautification is in the works for Lowell Mountain too.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterGermany may be finally realising that there are much more pressing problems than the phony global warming scare, like the imminent financial collapse of western governments, to name one.Indeed the Earth has not warmed in 13 years, tropical storm activity is near record lows. Atlantic hurricanes hitting the US coast? Aint seen a real one in 4 years. Global temperature? A whopping twenty hundredths above the (cold period) 1960-1990 normal, and falling.
Germany’s Federal Environment Minister under Chancellor Angela Merkel, green wonder boy Norbert Röttgen (CDU) is also realising the world is not playing along with the Great Climate Hoax, and as a result has dumped cold water on expectations for the UN climate conference in Durban.
The warmist German-language CO2 Handel reports here:
‘Durban will produce less than what’s necessary,’ said Röttgen at a conference of the German Association of Industry (BDI) in Berlin on Tuesday.
Röttgen could hardly conceal his disdain for the Obama Adminstration’s efforts, calling the US “a stonewalling obstructionist”. Röttgen added:
«Us Europeans alone can’t save the world by ourselves, instead we need allies.”
In Germany blaming all the world’s ills on the USA is all that’s left in its political playbook. So all the talk is now about forming a” coalition of the willing” to rescue the climate.
Why do I get the feeling this is less about saving the world, and more about saving a movement and relevance? Let’s call it what it is: the coalition of the duped and desperate. Meanwhile BDI boss Markus Kerber trumpeted on behalf of the hapless Röttgen:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




German industry calls for a legally binding international treaty at the upcoming World Climate Conference in Durban. The debt crisis must not lead to investments in climate protection measures to go without financing”.
Yeah right, just start printing money. Now you know why Europe is in a financial mess. ..with advice like Kerber’s.
The whole must not be allowed to degenerate into a conference circus.”
Too late. This has been a $120 billion circus and it’s been going on for 2o years. The greatest circus in the history of man. Of course Kerber is joyfully singing the climate song because he knows this circus is about to end. You can be pretty sure that German industry bosses will be relieved once Durban is finished.
Saved again from another dumb treaty.
UPDATE – Die Welt here on German industry bosses:
Indeed from the data it is clear: Only 16% of the companies intend to reduce their emissions. And those that do plan to reduce are planning a reduction of a mere 1.3% – not per year, but in total!”
Someone needs to whisper this to Kerber.
 
Share this...FacebookTwitter "
"The Great Barrier Reef is still at risk of a widespread outbreak of coral bleaching despite a cyclone to the far west helping to temporarily cool stressed corals, according to US and Australian science agencies. Clearer skies, weak tides and above-average ocean temperatures are combining to create stressful conditions for corals along much of the world’s largest reef system.  Concerns are rising that southern parts of the reef that escaped major bleaching in 2016 and 2017 may be hit in the coming weeks unless weather conditions change. Australia’s marine science agency said on Thursday it was set to deploy a second underwater glider to monitor water temperatures in the central and southern areas of the reef. Townsville-based Dr William Skirving, of the US government’s National Oceanic and Atmospheric Administration’s (Nooa) Coral Reef Watch, told Guardian Australia: “Everything seems to be lining up. “The clouds are clearing and we can see the heat is still high and we know the tides will provide less and less mixing. Everything is falling in line for a short sharp bleaching event but the severity of it is difficult to predict.” Corals bleach when they sit in abnormally warm water for too long. The algae that live in the corals and provide much of its nutrients and colour leave their host, leaving a visible white skeleton behind. Corals can recover from mild bleaching. Coral reef systems have been long predicted to be susceptible to global warming. The United Nations science panel has said that even at global heating of 1.2C “most available evidence” showed that in the tropics “coral-dominated ecosystems will be non-existent at this temperature or higher”. Dr Katharina Fabricius, a senior principal research scientist at the Australian Institute of Marine Science, said: “The reef is still on a knife edge. The future of the reef now seems to depend on flukes of the weather. That is a big concern. “This is exactly what scientists have been predicting, with an increase in the frequency of bleaching events.” Tropical Cyclone Esther crossed the coast in the Gulf of Carpentaria on Monday morning and while the eye of the cyclone was hundreds of kilometres west, the weather system dragged clouds over the reef, helping to cool temperatures slightly. The Great Barrier Reef Marine Park Authority said late Thursday weather conditions over the next few weeks would “play a key role in determining outcomes for the Reef this summer”. Coral bleaching has been occurring on parts of the reef where thermal stress has accumulated the most over summer. The authority said in-shore reefs in the far north had already experienced “widespread bleaching affecting most coral types” but there had been minimal bleaching on outer reefs. So far, only patchy and isolated bleaching had been reported over large parts of the reef, the authority said. Skirving said: “The clouds helped because they cooled things off, but there’s still a lot of heat in the system. It’s hot enough for the corals to be accumulating stress.” He said the mass bleaching event of 1998 across the reef had taken place over the course of just one week. Weak tides for the next week meant there was less mixing of the warmer surface waters with deeper, cooler water. Skirving said if Cyclone Esther had not formed, “it would be already bleaching up and down the reef”. In updated analysis, compiled on Wednesday, Skirving wrote that Noaa’s tool for predicting heat stress on corals showed there was an 80% chance that most of the reef would experience some bleaching between 1 March and 8 March. For southern parts, the probability was even higher. Bleaching at Magnetic Island, inshore GBR, Feb 26th 2020.About 65% of colonies showing stress, with some quite unusual patterns in the bleaching at present.Next 2 weeks will be critical in determining mortality schedules. #coralbleaching2020 #GreatBarrierReef pic.twitter.com/Tz1vqfI40t Dr Sophie Dove, an associate professor at the University of Queensland’s Coral Reef Ecosystems Lab, said it was important to differentiate mild bleaching from severe events, where corals appeared starkly white. “If that whiteness persists then we are likely to see mortality,” she said, adding it was too early to say if the current conditions would have a serious impact on the reef. The Australian Institute of Marine Science (Aims) has been monitoring satellite data, their own weather stations and a network of more than 170 electronic temperature loggers to check temperatures. Craig Steinberg, an oceanographer at Aims, told Guardian Australia that two autonomous underwater gliders were being used to record water temperatures from the surface to a depth of 200 metres. He said after a brief cooling period, “we are now back into a warming phase” in the southern parts of the reef. Temperatures tend to peak on the reef in mid-March. Prof Terry Hughes, director of the ARC Centre of Excellence for Coral Reef Studies at James Cook University, said southern parts of the reef had a greater number of coral species that were more susceptible to bleaching. He said: “We are at the stage where there’s enough heat to cause widespread, mild bleaching now, and we have two weeks to go.” Fabricius said she had seen a bleaching event unfolding at Magnetic Island, near Townsville, over the past two weeks. At a snorkel trail at Geoffrey Bay, hard coral species were partially bleached, giant clams were turning white and some acropora corals were showing signs of death. “The forecast for Friday is for 35C with clear skies,” she added."
"It sometimes feels as if environmental news is never good news, but that certainly isn’t true when it comes to the ozone layer.  The UN has announced that the ozone layer is showing “signs of recovery”. Evidence has pointed to recovery for some time, but researchers have waited until they were confident that the hole in the ozone layer was beginning to heal. It’s not yet restored to perfect health – that will take a few more decades – but a significant corner has been turned. That good news comes 30 years after governments around the world began to sign up to the Vienna Convention for the Protection of the Ozone Layer. Solving global environmental problems takes time, but the success of the Vienna convention, and the Montreal Protocol that puts the convention in to action, is proof that when the world works together, and keeps working together even when the going gets tough, it can deliver the solutions that we all need. Of course, having written “that we all need” begs an important question. Why does the ozone hole matter to me?  We have all seen those NASA images of the ozone hole over the Antarctic, but that’s a long way from where most of the planet’s population lives. It’s a little like that scene at the end of “Happy Feet” where the politicians challenged to respond to the plight of the penguins ask why they should “worry about a load of flightless birds”. So why should we worry whether or not there is a little more or less ozone, a tiny fraction of the gases in the atmosphere, than there might have been if we hadn’t all changed our fridges and under-arm deodorants? The most obvious answer is that the ozone layer protects us from ultraviolet (UV) light, and that being exposed to too much UV can eventually cause skin cancers.  OK, but just how many skin cancers have been prevented by protecting the ozone layer?   Until recently, it has been hard to answer that with any sort of numbers, but research has begun to model what the world would have been like if we had not protected the Earth’s ozone layer. These “world avoided” models are indicating that without the Montreal Protocol people around the world would already be exposed to increases in UV. Those increases would be enough to be causing skin damage that, over time, would mean more people developing skin cancers.   In fact, the most recent estimate of what would have happened without ozone protection suggests that by 2030 there would have been around 2m more cases of skin cancer a year worldwide.  That can’t be a precise figure, but even if we take as a “ball-park” estimate, that’s 2m people every year being saved from skin cancer because governments acted to protect the ozone layer. Looking over a longer timescale, do the maths. Two million fewer skin cancers a year, year on year on year soon generates some very large numbers. And those figures don’t take in to account the massive ozone depletion that would have occurred worldwide by the middle of this century. That collapse in global ozone is a consistent outcome of “world-avoided” research and would have increased UV levels around the world beyond anything that has ever been experienced since humans evolved.   Maybe we could have coped with that, but it would have been difficult. Yes, we can all reduce our exposure to UV by how we choose to behave, that’s probably the biggest factor affecting our risk of skin cancer in the world we actually live in. But what about in the world avoided? How much sun-cream would you have needed if without protection you would begin to sunburn in just a few minutes? What clothes would you send your children to school in?  Health-warning signs on the beaches? And even if you could cope, what about the damage to crops, to forests and to the oceans that would have resulted from run-away increases in UV, the scale of which we can’t yet really quantify. So yes, the news that the ozone layer is beginning to recover is a good reason to be cheerful. Be cheerful because we have protected the planet. Be cheerful because we have protected human health. Above all, perhaps, be cheerful because the success of the Vienna convention and the Montreal protocol shows that global governments can work together to solve major environmental problems.   When the Vienna convention was signed no one could be really sure exactly how ozone depletion might develop, but governments were brave enough to make tough decisions based on the best estimates of future risks.  30 years later, research allows us to confirm just how right those decisions were. Surely that’s good news not just for ozone, but also as we look ahead to the even tougher challenges of responding to climate change."
"**The SNP's Westminster leader has apologised after being accused of bullying a photographer who he suggested may have broken Covid rules.**
Ian Blackford posted a tweet claiming that Ollie Taylor lives in the south of England, and questioned why he had taken a photograph of the Northern Lights in Caithness.
But Mr Taylor said he moved to Caithness earlier this year.
Mr Blackford has since deleted his tweet and apologised to Mr Taylor.
Speaking to the Press and Journal newspaper, Mr Taylor said he had tweeted a photograph of the Northern Lights that had been taken about five minutes away from his house in the Highlands.
He accused Mr Blackford of ""trying to stir up public hatred"" against him.
And he said the MP could have ""saved himself a bit of embarrassment"" by messaging him privately about his concerns rather than tweeting publicly.
Mr Blackford had tweeted in response to Mr Taylor's photograph: ""As you live in the south of England and travel to Scotland is only for permitted reasons I am sure there will be a valid reason as to why you are posting a photo from the north of Scotland last night?""
After facing a backlash from other Twitter users, some of whom accused him of attempting to orchestrate an online ""pile-on"", Mr Blackford deleted the tweet.
He also posted an apology to Mr Taylor, saying: ""As the local MP for Ross, Skye and Lochaber I know my constituents feel very strongly about the breaking of travel restrictions that we see across the Highlands and islands, which puts people's lives and our public services at risk.
""I will continue to stand up for my constituents who frequently raise these concerns with me but I recognise that it was wrong to query an individual on Twitter and I apologise to @OllieTPhoto for my earlier post, which I have deleted.""
Mr Blackford's constituency does not include Caithness.
Scottish Liberal Democrat leader Willie Rennie said Mr Blackford had ""picked on and bullied"" a private citizen by ""accusing him of breaking the travel restrictions when he had no evidence of him doing so"".
Mr Rennie added: ""Ian Blackford has form for his remarks about people from England who happen to be in Scotland.
""This behaviour could only add to the problems of anti-English sentiment in Scotland.""
Scottish Lib Dem MSP Alex Cole-Hamilton later asked First Minister Nicola Sturgeon whether she backed the ""vigilante action"" taken by Mr Blackford when he accused the photographer of breaking travel rules.
The first minister praised Mr Blackford's ""grace and dignity"" and said he had done the right thing in apologising for ""doing something he recognised he should not have done"".
Scottish Conservative MSP Annie Wells said Ms Sturgeon should have ""called out"" the SNP MP for ""harassing a private citizen"".
She said: ""Ian Blackford should be ashamed of himself. He purposely went after an individual who simply wanted to share a lovely photo on Twitter for people to enjoy.
""I'm afraid it is the same old story with the SNP - stoke up division, and when challenged, simply hold up their hands feigning innocence. It is pathetic."""
"
Guest post from Von Rudolf Kipp
Originally in German here, with some portions translated to English using the Google translator below. 
[update–translation provided by poster EWCZ ~ ctm]
Google translator is largely imperfect, but to read the Google translation in English go here.
If anyone wishes to do a personal translation for the entire article, please leave a note in comments and I will replace it. Of great interest is the global graphic below, which shows that the MWP is a worldwide event, not just limited to portions of the Northern Hemisphere.


 “ “Who controls the past controls the future: who controls the present controls the past.” – George Orwell, 1984
We live in an age of superlatives. When you turn on the TV nowadays, you get offered the choice of best films, the greatest hits or the dumbest opening lines of all time. And even with a detergent it is long ago not sufficient when it  washes whiter than white.  Again, the constant sale appeal to the consumer can be maintained only if the product is billed as “The best thing ever.”
Naturally, also the reporting on climate change must follow this trend. Therefore the upcoming conference in Copenhagen is optionally about  the salvation of mankind, of whole ecosystems, or for those who like it even more bombastic, the salvation of the planet. To achieve this, we continue to learn, enormous changes in our economic and financial system are needed. Production companies and countries should put on bureaucratic manacles to control their CO2 emissions. Best with the help of worldwide dedicated government-like organizations.
What is the purpose of all this? You suspect or know it already. We are experiencing a warming, which has not existed in the history of mankind, or even in the history of the earth. And as a result we will experience the greatest disasters of all time. Honestly!
Click for an interactive graphic that will expand each graph on mouseover
 Medieval Warm Period thesis contradicts the unprecedented warming
However, one must mention that, already the first half of the statement, that about the unprecedented warming, elicits significant question marks in many climate scientists and even at many historians. Wasn’t there something like the medieval warm period? And in the opinion of many scientists, wasn’t it warmer during this period than today?
The idea of a medieval warm period  was formulated for the first time in 1965 by the English climatologist Hubert H. Lamb [1].  Lamb, who founded the UK Climate Research Unit (CRU) in 1971, saw the peak of the warming period from 1000 to 1300, i.e. in the High Middle Ages. He estimated that temperatures then were 1-2 ° C above the normal period of  1931-1960. In the high North, it was even up to 4 degrees warmer. The regular voyages of the Vikings between Iceland and Greenland were rarely hindered by ice, and many burial places of the Vikings in Greenland still lie in the permafrost.
Glaciers were smaller than today
Also the global retreat of glaciers that occurred in the period between about 900 to 1300 [2] speaks for the existence of the Medieval Warm Period. An interesting detail is that many glaciers pulling back since 1850 reveal plant remnants from the Middle Ages, which is a clear proof that the extent of the glaciers at that time was lower than today [3].
Furthermore, historical traditions show evidence of unusual warmth at this time. Years around 1180 brought the warmest winter decade ever known. In January 1186/87, the trees were in bloom near Strasbourg. And even earlier you come across a longer heat phase, roughly between 1021 and 1040. The summer of 1130 was so dry that you could wade through the river Rhine. In 1135, the Danube flow was so low that people could cross it on foot. This fact has been exploited to create foundation stones for the bridge in  Regensburg this year [4].
Clear evidence of the warm phase of the Middle Ages can also be found in the limits of crop cultivation. The treeline in the Alps climbed to 2000 meters, higher than current levels are [5]. Winery was possible in Germany at the Rhine and Mosel up to 200 meters above the present limits, in Pomerania, East Prussia, England and southern Scotland, and in southern Norway, therefore, much farther north than is the case today [6]. On the basis of pollen record there is evidence that during the Middle Ages, right up to Trondheim in Norway, wheat was grown and until nearly the 70th parallel/latitude barley was cultivated[4]. In many parts of the UK arable land reached heights that were never reached again later.
Also in Asia historical sources report that the margin of cultivation of citrus fruits was never as far north as in  the 13th century. Accordingly, it must have been warmer at the time about 1 ° C than today [7].
Archeology and history confirm interglacial
Insects can also be used as historical markers for climate. The cold sensitive beetle Heterogaster urticae was detected during the Roman Optimum and during the Norman High Middle Age in York. Despite the warming of the 20th century, this beetle is found today only in sunny locations in the south of England [8].
During the medieval climate optimum, the population of Europe reached hitherto unknown highs. Many cities were founded at this very time with high-altitude valleys, high pastures and cultivated areas, which were at the beginning of the Little Ice Age again largely abandoned [9].
The Middle Ages was the era of high culture of the Vikings. In this period their expansion occurred into present-day Russia and the settlement of Iceland, Greenland and parts of Canada and Newfoundland. In Greenland even cereals were grown about this time.. With the end of the Medieval Warm Period the heyday of the Vikings ended. The settlements in Greenland had to be abandoned as well as in the home country of Norway, during this time, many northern communities located at higher altitudes [10]. The history of the Vikings also corresponds very well to the temperature reconstructions from Greenland, which were carried out using ice cores. According to the reconstructions, Greenland was  at the time of the Vikings at least one degree warmer than in the modern warming period [11].
Climate scientists want to eliminate contradictions
Until about the mid-90s of last century the Medieval Warm Period was for climate researchers an undisputed fact. Therefore in  the first progress report of the IPCC from 1990 on page 202, there was the graphics 7c [12], in which the Medieval Warm Period was portrayed as clearly warmer than the present. However, the existence of this warm period became quickly a thorn in the side for the scientists responsible. When in 12th century without human influence the climate has been even warmer than at the height of industrialization, why should the current warming have non-natural causes?

Thus, the Medieval Warm Period was soon declared an odious affair. Meanwhile, an e-mail is legendary, which was sent to a U.S. climate researcher David Deming [13] in 1995. This scientist  published an article in the prestigious journal Science in which he had presented research on climate change in North America based on cores [14].
With this publication, he was immediately known among climate researchers, and some of them obviously thought that he was toeing their line [13, 15]:
“With the publication of the article in Science, I gained significant credibility in the community of scientists working on climate change. They thought I would be one of them, someone who would pervert science in the service of social and political causes. So one of them dropped his guard. An important person working in the field of climate change and global warming sent me an astonishing email with the words: ‘We must get rid of the Medieval Warm Period’. ”
Meanwhile, the climate machinery for the eradication of the Medieval Warm Period has already started. In 1995, the English climatologist Keith Briffa published in the journal Nature a study with sensational results. According to his studies of tree rings in the Siberian Polar-Ural, there had never been a Medieval Warm Period and the 20th century, suddenly appeared as the warmest of the last 1000 years [16]. The real breakthrough was the thesis of 20th Century experience as the warmest of the millennium, but not until three years later, and that with the release of Michael Mann’s infamous Hockeystick [17, 18].
Warm period is extinguished
In this diagram that became the icon of human-induced global warming in the 3rd IPCC Assessment Report, the Medieval Warm Period has now been completely eradicated. However, this curve was quickly under attack, mainly because the Canadian mathematician Steven McIntyre had serious doubts about the correctness of the representation and those pursued with the meticulousness of an auditor [19]. McIntyre showed not only that Mann had used an algorithm that resulted in 90 percent of the cases to a hockey stick, but found also serious errors in the selection of the data and the location of places, as well as the use of incorrect data [20].

Of course, the Mann’s gang could not let these allegations unanswered. In response, Realclimate.com was founded, a name intended to suggest the truth, but somehow reminiscent of the Real Ghostbusters, a poorly made copy of the genuine, which in contrast to the original only pretends to be the right thing. This webpage was henceforth used for accusations and slanders against the non-“believers” [21]. It took also increasingly care not to call McIntyre, in the meantime identified as the main enemy, by his name.
Following the publication of Michel Mann’s hockey stick and the criticism, whole series of further studies was published to demonstrate that the results of Mann’s actually represented the real temperatures over the last 1000 years. The highpoint of the debate was the forced disclosure of the raw data from tree ring studies long held under lock and key, which served as one of the principal witnesses for the correctness of the thesis of the unusually warm 20th century. It turned out that clearly the data were selected intently to get the desired result [22].
Conflicting data
Regardless of the debate over the proper or improper use of proxy data like tree rings to determine the temperature history, mainstream climate researchers, however, are still struggling with a whole series of problems. What was with all the archaeological data, the records of weather events in church records and historical facts, which clearly documented that in the Middle Ages, there was an unusually warm period? Quite simply, the attempts to refute these arguments were made based on claims that all these phenomena indeed existed, but only as geographically limited events [23]. If the Middle Ages was warmer somewhere than today, then maybe it was only in England, the Alps, Greenland or North America. Globally, however, as shown in the many hockey stick charts, it has been colder than at the end of the 20th century.
If one, however, provides an overview of the literature on the subject of Medieval Warm Period, which has been published in recent years, there will be a completely different picture. There are now quite a number of studies from around the world, showing all one thing. And indeed, that the High Middle Ages were warmer than today. An excellent overview can be found on the website CO2 Science, which has set up a whole section for studies of this kind [24]. There are now  765 different scientists from 453 research institutes listed that have worked on the medieval warm period. A small portion of these studies is shown in the figure below [Click 25] (by the graph, you get a larger image where you can select individual work).
This survey shows one thing quite clearly. At the time of the Middle Ages, that is, from 1000 to 1300 it was almost everywhere in the world warmer than today. There have been periods of warming, that exceeded 0.6 degree Celsius rise in temperature in the 20th century and totally without the man-made increased emissions of the supposed “climate killer” of CO2. The statements, that there has not been any Medieval Warm Period, or it was merely a localized phenomenon, can safely be regarded as untenable.
It is therefore not surprising that there are influences on the climate, which can by far exceed the CO2 as a driver of climate variability. This hypothesis is massively supported by the observations made during the last 10 years. Finally, we have been experiencing no increase since 2002, the temperatures have dropped slightly [26]. And that even though the emissions of CO2 from fossil fuels in exactly the same period increased to previously unmatched dimensions.
Google translation in English of the full article is here.


Sponsored IT training links:
Best quality 70-293 study pack to help you pass 640-721 exam on first try. Download SK0-003 practice questions to test you knowledge before hand.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e91ae218b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterRahmstorf writes to Overpeck and co-authors:
Dear Peck and IPCC coauthors,
– I know it’s Easter, but I’m having to deal with
Augusto Mangini, a German colleague who has just written an article
calling the IPCC paleo chapter “wrong”, claiming it has been warmer in the
Holocene than now, and stalagmites show much larger temperature
variations than tree rings but IPCC ignores them. What should I answer?
One of my points is that IPCC shows all published large-scale proxy
reconstructions but there simply is none using stalagmites – so please
tell me if this is true?!!
Continue reading…
 
Share this...FacebookTwitter "
"**Yeovil Town FC is considering selling its stadium to help ease the financial worries caused by Covid-19.**
South Somerset District Council is proposing to buy Huish Park then lease it back to the club.
Yeovil Town's chairman says the move is necessary to ""avoid serious financial distress"".
But the move could have ""catastrophic"" consequences according to one long-term fan.
Included in the plan is an option for the club to buy back the land when its finances have improved.
Like many clubs Yeovil, who play in the National League, have suffered due to not having fans in the stadium.
Chairman Scott Priestnall said the current season began after clubs were reassured by the league they would be compensated for lost revenue.
He said that despite that help, which included Â£10m from the National Lottery in October, Yeovil were still facing a financial crisis.
Mr Preistnall added that the club would be in ""deficit"" every months this season and the club ""has to take action"".
The government plans to allow a limited number of supporters back into stadiums, but Mr Priestnall said most income was generated at the start of the season through sponsorship and hospitality so the damage had already been done.
In a statement, South Somerset District Council said it helped many local businesses during the pandemic, giving out millions in grants, and the purchase of Huish Park would be in the same spirit.
The statement added the club ""makes a significant contribution"" to the local economy.
Yeovil fan Louis Purchase said the stadium sale proposal was something that should ""concern every Yeovil Town supporter.""
""If the club sells its main asset, it is unlikely Yeovil Town will ever own their own stadium and land again - and ties the club into paying rent every year.
""For a short-term financial boost, the long-term implications could be catastrophic.""
Huish Park is a registered community asset, which means Yeovil Town Supporters Ltd have to be offered the chance to make their own bid for the site, although the council warned this could delay the process by six months.
More details about the exact finance of the stadium purchase, which will be debated by the council on Thursday, 3 December, are due to be released later this week."
"
Share this...FacebookTwitterThe know-it-all German Greens want to create a new world. But the question is how ugly would their world be? I live in Germany, and I can tell you the windmills everywhere are a total eyesore and are doing nothing to cut back emissions. Here’s a good report on the German movement. H/t: DAmbler.

Green Is Ugly: Style Problems Plague Clean Energy Push
Germany is a world leader when it comes to green energy. But while its windmills and solar panels may be cleaning up the atmosphere, they’re also sullying the landscape.
Recent speeches and interviews by Green Party leaders offer insights into their plans for the future: speed limits, higher taxes and lots of regulations. Fans of fast cars will still be able to enjoy their rides, but not at the expense of the greater good of the people. Introduction of the planned speed limit would kill the one place where Germany is less regulated than the rest of the world, the Autobahn.
The Greens are hoping many people give up their cars altogether. Their re-education efforts – aimed at turning Germans into eternal bike riders – demonize the highway in order to glorify the bike path.
The intellectual cue givers for this planned policy shift leave no doubt about the drastic nature of the change. The government’s environmental advisory council demands nothing less than the reconstruction of civil society.
“The house of mankind is rotten and needs to be repaired urgently,” says climate scientist and advisory council member Hans Joachim Schellnhuber. “We need a sustainability revolution.”  Read more…
Share this...FacebookTwitter "
"

Issue 208, 2002 -click to enlarge
It’s worse than we thought! Now the IPCC has been citing magazine articles, like this one from Climbing Magazine, issue 208, shown at left. We’ve heard the title before, according to their index: “Canaries in a Coal Mine,” – Feature on global loss of glaciers. But wait there’s more! If you think that’s crazy, we also learn that IPCC Chairman Pachauri has penned a “smutty” romance novel! Bizarre, but true.
The Telegraph reports on the magazine issue:
The United Nations’ expert panel on climate change based claims about ice disappearing from the world’s mountain tops on a student’s dissertation and an article in a mountaineering magazine.
The revelation will cause fresh embarrassment for the The Intergovernmental    Panel on Climate Change (IPCC), which had to issue a humiliating apology    earlier this month over inaccurate statements about global warming.
The IPCC’s remit is to provide an authoritative assessment of scientific    evidence on climate change.
In its most recent report, it stated that observed reductions in mountain ice    in the Andes, Alps and Africa was being caused by global warming, citing two    papers as the source of the information.
However, it can be revealed that one of the sources quoted was a feature    article published in a popular magazine for climbers which was based on    anecdotal evidence from mountaineers about the changes they were witnessing    on the mountainsides around them.
The other was a dissertation written by a geography student, studying for the    equivalent of a master’s degree, at the University of Berne in Switzerland    that quoted interviews with mountain guides in the Alps.
The revelations, uncovered by The Sunday Telegraph, have raised fresh    questions about the quality of the information contained in the report,    which was published in 2007.
It comes after officials for the panel were forced earlier this month to    retract inaccurate claims in the IPCC’s report about the melting of    Himalayan glaciers.
Sceptics have seized upon the mistakes to cast doubt over the validity of the    IPCC and have called for the panel to be disbanded.
This week scientists from around the world leapt to the defence of the IPCC,    insisting that despite the errors, which they describe as minor, the    majority of the science presented in the IPCC report is sound and its    conclusions are unaffected.
But some researchers have expressed exasperation at the IPCC’s use of    unsubstantiated claims and sources outside of the scientific literature.
Professor Richard Tol, one of the report’s authors who is based at the    Economic and Social Research Institute in Dublin, Ireland, said: “These    are essentially a collection of anecdotes.
“Why did they do this? It is quite astounding. Although there have    probably been no policy decisions made on the basis of this, it is    illustrative of how sloppy Working Group Two (the panel of experts within    the IPCC responsible for drawing up this section of the report) has been.
“There is no way current climbers and mountain guides can give anecdotal    evidence back to the 1900s, so what they claim is complete nonsense.”
The IPCC report, which is published every six years, is used by government’s    worldwide to inform policy decisions that affect billions of people.
The claims about disappearing mountain ice were contained within a table    entitled “Selected observed effects due to changes in the cryosphere    produced by warming”.
It states that reductions in mountain ice have been observed from the loss of    ice climbs in the Andes, Alps and in Africa between 1900 and 2000.
The report also states that the section is intended to “assess studies    that have been published since the TAR (Third Assessment Report) of observed    changes and their effects”.
But neither the dissertation or the magazine article cited as sources for this    information were ever subject to the rigorous scientific review process that    research published in scientific journals must undergo.
The magazine article, which was written by Mark Bowen, a climber and author of    two books on climate change, appeared in Climbing magazine in 2002. It    quoted anecdotal evidence from climbers of retreating glaciers and the loss    of ice from climbs since the 1970s.
Mr Bowen said: “I am surprised that they have cited an article from a    climbing magazine, but there is no reason why anecdotal evidence from    climbers should be disregarded as they are spending a great deal of time in    places that other people rarely go and so notice the changes.”
The dissertation paper, written by professional mountain guide and climate    change campaigner Dario-Andri Schworer while he was studying for a geography    degree, quotes observations from interviews with around 80 mountain guides    in the Bernina region of the Swiss Alps.
read the complete article at the Telegraph


Sponsored IT training links:
Pass 642-642 exam fast using self paced 640-822 prep tools including 640-863 dumps and other study resources.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8eab22bb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

In the past two decades, New Zealand farmers have moved from an environment in which we were subsidized and the government dictated the type of agricultural goods produced to one in which we farm without subsidies. The clear focus for us now is on the consumer. That is the real reason we are in business: to serve the consumer, not the government.



One advantage for New Zealand is that we are a small country. We can be experimental and help other people understand our progress because of the smaller scale of the New Zealand economy.



 **“Fortress New Zealand”**



Let me take you back in history to the 1950s, when there were worldwide food shortages after World War II. New Zealand was an agricultural nation, feeding the United Kingdom, and had the second‐​highest per‐​capita income in the world. We were incredibly rich. The government was dominated by farmers. Almost all the cabinet ministers were farmers. Farmers had it made.



What did we do? We were our own worst enemy. We went to the government and said, “Prices fluctuate, the climate affects us, we think you should step in and help regulate what we earn, to take away fluctuations in our income.” And the government, being full of farmers, said, “Yep, that’s good. We’ll help you.” So we set up marketing boards and other structures that put constraints on the farming community. They either limited or expanded production, and they controlled the price depending on what the government and often farmer‐​controlled boards thought was best for farming.



Protection and control of the farming sector were bad enough, but New Zealand is also a small country, and the government even then did not believe the future of New Zealand was in agriculture. We needed to start creating industry in New Zealand that would employ people elsewhere. So the government started imposing import tariffs and quotas on nearly everything, including cars and televisions. The aim was to increase the prices of imports and make manufacturing in New Zealand competitive. New Zealand became “Fortress New Zealand.”



We locked out the foreign competition, we “protected” jobs, and we had virtually no unemployment in New Zealand right up to the 1970s. We were rich enough, because of the agricultural income that was coming in, to be able to sustain the jobs. But the trade barriers we imposed on ourselves during that time built in unnecessary costs within the New Zealand economy.



 **Shock, Crisis, and Reform**



Then we experienced two major economic shocks. The first was in 1973 when Britain entered the European Union. Suddenly, the market that used to take much of what we produced was no longer open to so many of our exports. The second was the oil shocks. Basically, we had this lovely little fortress in New Zealand, a top‐​down pricing mentality that ignored market forces, and–what do you know–inflation started to get out of control. But we also had a controlled society, in which interest rates and seemingly everything was controlled in New Zealand.



Finally, after 10 years, New Zealand experienced both fiscal and financial crises in 1984. There was no more money for the government to spend, and the government had run up huge deficits and borrowed a lot of money from overseas to try to keep the wheels of the New Zealand economy turning. Thankfully, the Federated Farmers of New Zealand realized that continually going to the government for more handouts wasn’t working. We were becoming increasingly uncompetitive in the world markets. We needed to change, so we said to the government, “Strip out our subsidies, but we want you to reform the rest of the New Zealand economy as well.”



The government made the reforms, but painfully for farmers it was a left‐​leaning government that made the transition. The government reformed agriculture first because the farmers had traditionally never voted for them anyway. The rest of the economy took six years to be fully liberalized. We now live in one of the most open and unregulated economies in the world.[1] Other than a few tariffs on shoes and some clothing, we are completely open at the border for everything. Before 1984, if you were well‐​off, you could avoid the tariffs. If you wanted to buy a car or a television, you simply took a holiday overseas and brought it back with you to New Zealand. Now you can buy anything anywhere in New Zealand from anywhere in the world. And it’s fantastic.



 **Satisfying the Consumer, Using Land Wisely**



The major development for the New Zealand agricultural sector was that, after enduring the pain of that long transition, responsibility for farming returned to the farmer. So the government was no longer involved in our lives. The key issue that we had to grapple with very quickly was that there was only one component important to farm income, and that was satisfying the consumer. We used to satisfy government officials in what they wanted. We had 70 million sheep and another 50 million lambs, of which the meat industry rendered, in one year, six million lambs into blood and bone (a fertilizer product) because nobody wanted them. In the worst days of the post‐​deregulation period, in a drought in the Canterbury region of New Zealand, farmers were getting only $6 a lamb. Now we actually focus on the consumer. Processors now pass on clear market signals and give farmers higher returns. The farmer now provides exactly what the consumer wants in the most cost‐​effective manner. Now, by meeting market specifications, we get between $60 and $100 a lamb depending on the time of year the lamb is supplied.



The other positive result of the reform process was that farmers began to better fit their agricultural production to the type of land that they farmed. If you don’t have a really good fit with the type of land you are on, your resources are used inefficiently, and then you can’t get your costs low enough to be competitive. New Zealand has reduced the number of sheep from 70 million down to 40 million,[2] but we produce roughly the same amount of sheep meat.[3] We increased our dairy herd from roughly 3 million cows to now more than 5 million in response to the market demand for protein products.[4] We’ve seen a huge diversification of land use in the last 20 years. In the good old days, people just would not have believed what farmers were capable of when everybody just wanted us to produce another lamb.



Farmers have continued to diversify, to change, and to accept that we not only need access to markets and consumers, but we also need an incredibly competitive internal economy. We need to be able to adjust our cost structures quickly to deal with the volatility of climate, product price, and, in a small country like New Zealand, the exchange rate. But we are happy to accept these risks and uncertainty because by facing these risks we make far better decisions. And we’re not blaming anyone else; we only have to rely on ourselves to get on with the job.



 **Rising Farm Productivity and Output**



The New Zealand farm sector before 1984 had a productivity increase of 1 percent a year. Since the reform, it’s been nearly 4 percent per year.[5] We’re performing better than any other part of the New Zealand economy. The agriculture sector in New Zealand has actually grown as a percentage of our GDP, from slightly more than 14 percent of GDP in 1986–87 to 16.6 percent in 1999–2000.[6] This is almost unheard of in any other developed country. So we’re actually playing our part and more for New Zealand.



Agricultural trade is incredibly important for New Zealand because it’s what we do well. We rely upon exports from our biological industries to pay for imported goods from around the world that we have no advantage in producing ourselves.



However, we face lots of criticism. Our competitors criticize the size of our trade in dairy products compared with that of the U.S. or European dairy industries. On the other hand, New Zealand looks at the bigger picture. We look at the total trade between countries, as opposed to sector by sector.



Since the transition period, New Zealand’s move toward a completely open economy has created nearly 450,000 jobs on net.[7] For a small country of 4 million people, that’s a lot of jobs, and we’re still creating more. Many myths abound about trade being bad for jobs, but we’ve found the exact opposite to be true. The reality is that trade is good for jobs.



A liberal trading environment not only encourages investment, but it also enables countries to produce to their comparative advantage and trade to their strengths. This leads to a more efficient use of resources and creates more wealth and jobs.



 **Overcoming the “Fear Factor”**



The New Zealand experience brings two key messages to the world trading environment. The first is that producers must focus on the consumer. The reason we want liberalization in trade is so that we can talk directly to consumers in individual countries. We want to be able to tailor our products specifically to the market in each country. It’s not just about world trade in the abstract; it’s about reaching out to each country’s individual consumers and supplying them with what they want.



I recently attended the World Farmers Congress of the International Federation of Agricultural Producers. A continual theme I heard was “Exports are good, imports are bad,” and that “When you liberalize world trade, it’s a race to the bottom.” Well, in New Zealand we are in a race to the top. But every time we race to the top, a bureaucrat somewhere else in the world tries to squash us. The classic example is Europe, where we exported “spreadable” butter. Because it didn’t meet the specifications of the regulators as butter, they prevented it from entering Europe, even though the demand from the consumer was strong. We were adding value, creating a better product, and meeting what the consumer wanted, but a bureaucrat said, “Ah, but it’s not butter, because it is too soft.” These are the types of issues we have to deal with in global negotiations.



The second message from the New Zealand experience is that we use our resources well. We’ve put our resources where they are most efficiently used. For sustainability, we want the world to use its resources well. Agriculture plays a huge part in that process. It makes no sense for farmers in Europe and the United States to produce sugar at three or four times the world price when you can more efficiently produce it in many tropical countries around the world.[8] The tropical countries can produce sugar at a fraction of the cost and with much more efficient use of world resources. Our vision is for a dual outcome–to liberalize trade and to make better use of the world’s resources as each country produces to its comparative advantage.



In New Zealand, we went through the same fear factors that I hear from other farmers in the developed countries. People said, “We can’t do it. We’re not going to produce any more milk. We’re all going to go broke. The government hates us.” Once you get through all that–and I firmly believe that the developed countries’ farmers can–farmers will start to focus on what they can do. They will focus on what’s good in their region. American and European farmers have a wonderful advantage that we don’t have in New Zealand: their customers are in their backyards. Their customers are not 12,000 miles away from a little country in the middle of the South Pacific. They do not have to transport everything great distances before they reach a consumer.



Producers must move beyond thinking that they can’t produce for the wide range of consumer demands. They must look for the opportunities that exist both in their own backyards as well as beyond their borders. Fear of change often runs rampant, but once you are farming in accord with your comparative advantage, it becomes possible to adapt your production system to suit consumer needs and adjust production costs accordingly.



A good example of this is the New Zealand wine industry. New Zealanders are quite good at producing wine, but we still import more wine than we export. New Zealand consumers like to have choice, and we are willing to give our consumers that choice. We believe it is better for the farming community, better for the New Zealand economy, and ultimately better for the world.



 **Conclusion**



I recognize that our vision is bold, but we’d like the Americans and others to be ambitious in the Doha Development Round of WTO trade negotiations. We’re practical. We accept that there is probably going to be a need for a reasonably long transition period as people move toward an open market, but this is an opportunity that the world really needs to seize. We need to be forward thinking on trade liberalization. Based on the experience of the Uruguay Round and even this round, negotiating these agreements takes a long time. The opportunities don’t come very often. I hope we can turn that opportunity into reality.



[1] New Zealand’s economy now ranks as the third‐​freest in the world, behind only Hong Kong and Singapore, according to James Gwartney and Robert Lawson, _Economic Freedom of the World: 2004 Annual Report_ (Vancouver, B.C.: Fraser Institute, 2004), p. 11.



[2] New Zealand farmers owned 39.6 million sheep as of June 30, 2002, according to the New Zealand Ministry of Agriculture and Forestry, “Agriculture Production Survey,” June 30, 2004,  www​.maf​.govt​.nz/​s​t​a​t​i​s​t​i​c​s​/​p​r​i​m​a​r​y​i​n​d​u​s​t​r​i​e​s​/​l​i​v​e​s​t​o​c​k​/​s​h​e​e​p​/​0​2​-​s​h​e​e​p​-​a​g​e​-​f​s.xls. There were 69.7 million sheep as of June 30, 1984, according to the Policy Information Group, New Zealand’s Ministry of Agriculture and Forestry,  www​.maf​.govt​.nz/​s​t​a​t​i​s​t​i​c​s​/​p​r​i​m​a​r​y​i​n​d​u​s​t​r​i​e​s​/​l​i​v​e​s​t​o​c​k​/​s​h​e​e​p​/​s​h​e​e​p.htm.



[3] New Zealand produced 113,000 tons, bone‐​in (2002–2003), compared to 130,000 tons, bone‐​in (1985–1986), according to the New Zealand Ministry of Agriculture and Forestry, Game Industry Board, The Economic Service, available through Meat and Wool Innovation Ltd., www​.wool​pro​.co​.nz/​e​c​o​n​o​m​i​c​s​e​r​v​i​c​e​/​q​u​i​c​k​s​t​a​t​s​.html.



[4]New Zealand farmers owned 5.1 million dairy cattle as of June 30, 2003, compared to 3.2 million dairy cattle as of June 30, 1984, according to the Policy Information Group, Ministry of Agriculture and Forestry,  www​.maf​.govt​.nz/​s​t​a​t​i​s​t​i​c​s​/​p​r​i​m​a​r​y​i​n​d​u​s​t​r​i​e​s​/​l​i​v​e​s​t​o​c​k​/​d​a​i​r​y​/​d​a​i​r​y.htm.



[5] See Roger Kerr, “A Vision For Agriculture,” Address, New Zealand Business Roundtable, Wairarapa Agricultural Seminar, July 19, 2001; and Federated Farmers of New Zealand, “Life After Subsidies: The New Zealand Farming Experience 15 Years Later,” August 2002, pp. 1–2. Productivity estimates are from Lawrence and Diewert’s Treasury Working Paper 99/5, “Measuring New Zealand’s Productivity,” March 1995, www.treasury.govt.nz/workingpapers/1999/99–5.asp.



[6] This statistic, measuring both agricultural production and downstream processing, is from the Federated Farmers of New Zealand, “Life After Subsidies: The New Zealand Farming Experience 15 Years Later,” August 2002, p. 1, and Rob Davison, “Agricultural Productivity,” _Vetscript_ , May 2003. When downstream activities are not included in a measure of the agriculture sector, agriculture comprised approximately 4 percent of New Zealand GDP in 1985, and just over 7 percent in 2001. In “OECD National Accounts Volume 2,” the OECD calculates New Zealand’s agriculture sector as 6.4% of total GDP in 1986 and 8.3% of total GDP in 2000 based on current prices, and 7.3% of GDP in 1986 compared to 8.3% of GDP in 2000 based on constant prices. See also New Zealand Treasury, “New Zealand Economic & Financial Overview 2002,” and Robert A. Buckle, David Haugh, and Peter Thomson, “Treasury Working Paper 01/33: Calm after the Storm?: Supply‐​side contributions to New Zealand’s GDP volatility decline,” 2001, Figure 5. 



[7] Phil Briggs, “Looking at the numbers: a view of New Zealand’s economic history,” The New Zealand Institute of Economic Research, www​.nzi​er​.org​.nz/​S​I​T​E​_​D​e​f​a​u​l​t​/​S​I​T​E​_​P​u​b​l​i​c​a​t​i​o​n​s​/​r​e​s​e​a​r​c​h​_​m​o​n​o​g​r​a​p​h​s​.​asp#2. And, Statistics New Zealand, “Household Labour Force Survey,” March 2003, June 2003, September 2003, and December 2003.



[8] Oxfam International, “Dumping on the World: How EU Sugar Policies Hurt Poor Countries,” Oxfam Briefing Paper 61, March 2004, p. 7.


"
"
Gore 2.0, now with Pacific Hurricanes, coming to a book store near you. Gore plans to hawk it on David Letterman next Tuesday night. One more reason not to watch Dave anymore. One can always hope though. Maybe he’ll feature Gore as a “stupid human trick”.

Al doesn’t seem to learn when it comes to visuals. Or maybe he just thinks that he’s obligated to put a picture of a hurricane on the front cover to keep the theme of AIT going. Either way. Any imagined link between hurricanes and global warming has evaporated.

My prediction:  sales will be a fraction of AIT, and it probably won’t make the NYT bestsellers list. People are tired of the yap, as indicated by recent polls.
Here’s what he has to say about it on his blog (note: he doesn’t take comments)
From the Press Release:
Today Vice President Gore announced that his next book, Our Choice, will be published by Rodale in the US and by other publishers internationally on November 3, 2009. Picking up where An Inconvenient Truth left off, Our Choice utilizes Mr. Gore’s forty years of experience as a student, policymaker, author, filmmaker, entrepreneur and activist to comprehensively describe the real solutions to global warming. A co-recipient of the Nobel Peace prize in 2007 for his environmental work, Mr. Gore continues to make sense of the pressing issues we face and Our Choice will unquestionably inspire and rally those ready to fight for solutions that were deemed impossible only a short time ago.
Said Vice President Gore, “An Inconvenient Truth reached millions of people with the message that the climate crisis is threatening the future of human civilization and that it must and can be solved. Now that the need for urgent action is even clearer with the alarming new findings of the last three years, it is time for a comprehensive global plan that actually solves the climate crisis. Our Choice will answer that call.”
Since the publication of the New York Times bestseller An Inconvenient Truth and the release of the Academy Award® winning film of the same title, Mr. Gore has led more than thirty “Solutions Summits” with top scientists, engineers and policy experts to examine every solution to the climate crisis in depth and detail. Our Choice draws on conclusions developed through those summits as well as on extensive independent research, describing how the bold choices necessary to save the earth’s climate should also be the foundations of policies worldwide to create new jobs and stimulate sustainable economic progress.
As they did with An Inconvenient Truth, former Vice President Gore and Mrs. Tipper Gore will donate 100% of the proceeds of the book to the Alliance for Climate Protection, a non-profit, non-partisan group dedicated to spreading awareness about the climate crisis and how to solve it.
Our Choice will feature 100% recycled paper, locally produced and sourced editions, low VOC inks, and will be carbon neutral.
“Rodale is honored to continue our relationship with Vice President Al Gore,” said Rodale Inc. President and CEO Steven Pleshette Murphy. “We were proud to publish An Inconvenient Truth and very much look forward to bringing Our Choice to the growing audience of committed citizens who are seeking solutions to the climate change crisis. In the spirit of our longtime mission, we are dedicated to creating the greatest possible platform for Vice President Gore’s work and message.”
Simon & Schuster Audio, who published the Grammy Award winning audio version of Al Gore’s An Inconvenient Truth, will publish the audio edition of the book on CD and digital download simultaneous with the hardcover publication.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e927261e8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**The parents of a 10-year-old boy who died after contracting Covid-19 have spoken of their ""indescribable"" pain.**
Fehzan Jamil, from Bradford, had a number of underlying health issues and is believed to be one of the youngest victims of the pandemic in the UK.
His mother and father, Tayyaba and Mohammed Jamil, said: ""There were four of us, now there are only three. The pain is indescribable.""
Fehzan, who was described as a ""brave fighter"", was laid to rest on Monday.
Speaking to Channel 4 News on Tuesday, his mother said: ""I just can't describe our loss. Everything feels empty now.""
Fehzan had been treated by staff at Bradford Royal Infirmary for several years due to a number of health issues, including epilepsy.
His family had tried to shield him during the pandemic, knowing that his health issues made him vulnerable to Covid-19.
He was kept at home as much as possible, with anybody coming into the house required to wear a mask.
""We tried our best to keep him safe but somehow Covid got to him,"" Mr Jamil said.
Mrs Jamil paid tribute to hospital staff who had treated her son, saying: ""All of the staff were very good to us. They have known Fehzan for many years now and have always looked after him.
""They let us be beside him when he died. It meant a lot.""
On Tuesday, the government recorded another 608 UK deaths within 28 days of a positive Covid test.
This is highest figure since May 12, when 614 deaths were reported, and brings the UK total to 55,838.
In June, a 13-day-old baby, thought to have no underlying health conditions, died with Covid-19.
_Follow BBC Yorkshire on_Facebook _,_Twitter _and_Instagram _. Send your story ideas to_ yorkslincs.news@bbc.co.uk _or_send video here _._"
"
Both WUWT and Climate Audit had posts regarding the ridiculous WaPo story about snowfall being a result of climate change.
This is a follow up to those posts done by guest contributor Steven Goddard.

One of the  NWF claims about global warming is that snow in the Colorado mountains is diminishing and has become very erratic, as seen in the NWF graphic at left.
click for a larger image
In this article I will show that the claim is incorrect – Colorado snowfall has been generally increasing for the last hundred years and that year over year variability has always been extremely high.
Fortunately, there are excellent long term records of snowfall available from  NOAA’s Western Regional Climate Center. I chose the  Crested Butte, Colorado station because it is centrally located in the mountains (so is representative of a wide region) and has the most complete and continuous snow record of every month for the past 100 years.   I have randomly sampled quite a few other stations in Colorado.  None seem to have as a complete a record as Crested Butte, and the pattern described for Crested Butte seems to be fairly consistent in the mountainous regions of the state.
Below are graphs showing  annual and monthly snowfall totals (in inches) for Crested Butte since 1909.  The trend lines were generated using Google Spreadsheet’s linest() function. Note that every month is trending upwards in snowfall and the standard deviation is very high.  Also note that there were several very dry years early in the 20th century with very little snow – and the last few decades have seen more consistent snowfall.  Since 1981, every year has received more than 100 inches of snow.  Prior to 1930, it was not uncommon to have snow years with less than 100 inches of snow.  Prior to 1930, the average annual snowfall was 177 inches.  Since 1930, the average annual snowfall has been 200 inches – a 10% increase.
Note – the raw data is incorrect for 1910, 1919, and 1924 due to a significant number of missing measurements, so I substituted a calculated annual value based on the trend line. This probably overestimates the snowfall for 1919 and 1924, and is thus conservative.
Click images below for full-sized ones.

Standard deviation = 67  Mean = 195  Trend = +7.7 inches per decade

Mean = 23.4  Standard Deviation = 15.1

Standard deviation = 25.9  Mean = 33.5

Standard deviation = 27.9  Mean = 38.4

Standard deviation = 19.3  Mean = 33.5

Standard deviation = 18.2  Mean = 31.0

Standard deviation = 13.1  Mean = 16.9
In summary, snowfall is increasing annually and we see upward trends in the months of “snowfall season” in Colorado.  Year over year variability has always been very high and may actually be lower in recent years. And, the Colorado mountains no longer have extremely low snow years like they did 80 years ago. By the data, it seems the NWF claims are unfounded.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8ec3ab98',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe Swiss online NZZ has a commentary written by Social Sciences Prof. Ulrike Ackermann, Director of the John Stuart Mill Institute for Liberty Research in Heidelberg, read here (in German).It’s no secret that the German Greens have been on the rise and that their movement has spearheaded the drive against the use of fossil fuels and atomic power in Germany and Europe. Ackermann writes: “For the Greens, nature is good, humans are bad” and thus must be always kept under the watchful eye of a powerful, better-knowing state. This means more central (by amateurs) planning, equalization and social uniformity.
It means the destruction of individual responsibility, and thus the individual.
Ackermann reminds us: “History shows us that this is precisely what never has put us on the path to democracy, freedom and prosperity.”
The German Greens advocate massive state intervention and deep interference in our private lives with the aim of squashing individual liberty and independence. The Greens in Berlin, for example, aim to require all do-it-yourself home improvement projects to be subjected to state permitting. This, they say, would ensure cheap and affordable housing. Installing hardwood floors or other similar improvements only serve to make living quarters more luxurious, and thus less affordable to poorer people.
The same massive state intervention is also called for when it comes to transportation. People need to be herded into public transport systems and other forms of “healthy transport modes”…like bicycles, buses, or walking. Airport expansions, on the other hand, are to be stopped at any cost. Ackermann writes:
On the path to this noble target, adult citizens are being treated like children. The program is in ‘easy language’ that is especially designed for people with ‘learning difficulties, reading impairments and speech problems’, and so verily pushes infantilism to a new high.”
To me this indicates that people who do not have the faculties to care for themselves as responsible adults are particularly attracted to Green promises. Green sympathizers probably can be classified in 3 primary categories: 1) people who want to nanny and boss everyone around, 2) people who want to be nannied and bossed around, and 3) the many gullible who actually believe the climate catastrophe scam.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The Green movement of course needs scientific authority to sustain it. This is provided by Hans Schellnhuber’s WBGU, which in English stands for: Scientific Council of the German Government for Global Transformation. Global transformation by Germans?
Here once again we have a few arrogant Germans with an insatiable desire for world domination. One function of the WBGU is, as Ackermann writes:
With a new contract for society, the Council desires to implement the Great Ecological Transformation to a nuclear-free and carbon-free global economy. A powerful, ruling government shall provide for this, and will care for the ‘societal problemtic’ of ‘non-sustainable living styles’. Climate protection will be one of the fundamental targets of the state. Our current parliamentary democratic legal process is to be supplemented by a ‘Future Council’ which will be expressly superordinate over Parliament, party squabbling and conflicting interests, and will be assembled by draw.”
This means a select unelected elitist group of European white guys, would have the final say on global decision making. Clearly the green movement is well on its way to something of quite another color. Ackermann asks:
Haven’t we seen something like this before?”
Ackermann summarizes by writing we do not need a paternalistic, dictatorial state that decides everything for us, thus destroying the individual. Instead, she writes, we need a state that promotes all individuals, their independence, and thus their pride and self responsibility. Only a society that  can produce such individuals can expect to see prosperity, justice and equality. The Green Movement will deliver none of that.
Kudos to the NZZ for publishing Prof. Ackermann’s commentary.
Share this...FacebookTwitter "
"Light is among the fastest growing human-made pollutants of the natural environment. Numbers of outdoor lights are growing rapidly across the world, far outpacing general population growth. We know light pollution costs too much, wastes energy and spoils our view of the night sky.  But these are all human concerns; what does light pollution mean for plants and animals? Even academics ignored what light pollution does to the ecosystem until the turn of the century. A Google Scholar search for scientific articles concerning “light pollution” and “ecology” prior to the year 2000 returns no relevant search results. The study of “Ecological light pollution” didn’t really take off until a 2004 paper of that name by urban wildlife experts Travis Longcore and Catherine Rich. But, ten years on, major gaps still remain in our knowledge about the impact of light on most species. From the limited science we have, it appears that artificial lighting has the potential to disrupt the behaviour and physiology of a huge number of species and even entire ecosystems. For example, by disorienting sea turtle hatchlings, affecting the choice of nest and breeding success of birds, or detritivore activity in freshwater streams. But in truth, we just do not fully know – yet. Our lack of knowledge is confounded by other factors such as the wide variety of different lights, each with their different spectral properties and intensities. Older lights tend to emit a narrow band of orange light whereas the widely hailed “light of the future”, the Light-Emitting Diode (LED), has a broad spectrum of white light. These differences make generalisations between different studies particularly hard.  Similarly, light influences behaviour in lots of different ways. Some animals are most active during daylight, others are nocturnal or crepuscular (active at twilight) – the same light that annoys a diurnal animal trying to get some rest may also ruin the dinner plans of a nocturnal forager. Even age can make a difference, thus the way an organism will respond to light is often difficult to generalise across a single species, let alone a genus or family. To date, 86 studies examine the impact of artificial light on organisms, 70 of which have been published in the past four years. Most focus on the impact of light on different species of plants or animals – birds, invertebrates and mammals (mostly bats) are particularly popular – with the remaining quarter looking at ecosystems and ecological impacts in a more general tone.  My research has examined the impact of artificial light at night on the behaviour of an economically and environmentally valuable freshwater fish, the Atlantic salmon (Salmo salar). For fish, light is thought to alter the natural pattern of light and dark in their ecosystem, affecting their daily behaviour, and that of their prey.  Research such as this is important, as light pollution is exploding and so little is known about what this will mean for ecosystems and organisms. Lighting itself is changing to be more energy and cost efficient, a move that could increase the impact on animals. And then of course there is global climate change, a variable that, to date, has not been accounted for.  Given these changes that are looming on the horizon, it becomes all the more pertinent that we begin to fully engage with the subject of ecological light pollution. The enormous gaps in our knowledge that can only be filled through investment of time and money in researching the impact of lights old and new, on species big and small. It might be annoying for us to see so much energy wasted on illuminating the night sky, ruining our view of the stars. But for some animals, this same pollution could be life-threatening."
"**Some dental practices could close as a result of the pandemic, the British Dental Association (BDA) Scotland and the Royal College of Surgeons of Edinburgh (RCSEd) has warned.**
The number of patients dentists can treat has been cut by rules to stop the virus from spreading.
The RCSEd has warned that for NHS work the government was ""providing very little towards treatment"".
But the Scottish government said the sector has had ""unprecedented support"".
Prof Philip Taylor, of the Faculty of Dental Surgery at RCSEd, said that for NHS work the government was ""providing very little towards treatment"".
Fees are complex and can vary with each patient, but a dentist who fits a new metal crown for a back tooth on the NHS may receive a fee set by government of Â£80 to Â£100 - with the patient paying most of that.
However, if the work was done privately, the RCSEd said the dentist could charge Â£300 to Â£400.
Dentists whose businesses rely on NHS patients have warned that they are struggling to stay afloat with so few patients coming through the door.
Under strict Covid protocols, treatment rooms must be thoroughly cleaned between patients and need to be left empty for a period of time after some procedures.
This has cut the number of patients dentists can see, and those who rely on slim margins from NHS patients have warned that they may close their practices.
Prof Taylor warned that practices closing down would create an access problem for patients.
Many of those that closed were likely to be the ones most dependant on NHS fees - often in poorer parts of big cities - he added.
One dentist told BBC Scotland the number of patients his practice could see in a day had been cut from 100 to 20.
Fewer patients, he said, meant less income.
""I think we are in a dental health crisis,"" said David McColl, of BDA Scotland.
""You might see more practices saying we can't afford to run a service like this with NHS funding and turn private.
""Then you'd have a two-tier system where if you can afford it you can get the treatment, if you can't, you won't.""
He added: ""We want to get back to treating our patients and have a fair system that is not target-driven.
""None of us wants this two-tier system - we need universal access for all"".
But the Scottish government has said it is committed to ensuring all NHS patients who want to access NHS dental services continue to receive treatment.
Scotland's chief dental officer, Tom Ferris, said that from 1 November all NHS treatments had been allowed again.
He said: ""If an NHS patient has been offered or provided with private dental treatment then their dentist must be able to confirm that all available and appropriate NHS alternative treatments have been discussed with the patient and informed consent obtained.""
Mr Ferris added that where a dentist had decided to reduce or altogether withdraw their NHS dental service, then suitable alternative arrangements would be put in place for patients.
On the funding the government had made available to dentists during the pandemic, he said: ""The government is continuing to provide an unprecedented amount of financial support to ensure the continuity of NHS dental services.""
He said payments worth up to Â£12m per month were being made to support the incomes of NHS dental practices.
""As well as deploying the Scottish government budget for NHS dental services, we are investing an additional Â£2.75m per month.""
He added that dentists had been receiving a guaranteed income of 80% of gross fees during the pandemic, which had been increased to 85% from 1 November.
PPE for dentists providing NHS care has also been provided through public money, where they previously provided this themselves.
Mr Ferris added that in December a 2.8% pay rise for dentists would be backdated to April.
''We are working to ensure NHS dental services emerge from this crisis well placed to care for the oral health of the population and will continue to work collaboratively with the dental sector in developing a new model of care,"" he added."
"Milk, a precious resource in many parts of the world, has become a throwaway commodity in wealthy countries. For example, in the UK, an estimated 4.2m tonnes of foodstuffs wasted per year are wasted, of which milk is in the top three. In 2012, the country disposed of 420,000 tonnes of avoidable dairy and egg waste, costing £780m; perhaps no surprise as supermarkets retail milk for as little as 44p per litre. Bottled water can be two to three times the price. Such extreme market forces lead to vanishing profit margins, so the dairy industry has had to become super-efficient: fewer, larger herds typically with several hundred, high-yielding Holstein cows capable of producing 10,000 litres per annual lactation cycle, milked by a single dairyman.  These remarkable cattle are the result of highly selective breeding over many generations using a very small pool of elite bulls capable of producing over a million offspring by artificial insemination. A marvellous exemplar of sustainable intensification and food security though application of modern science and technology … perhaps? From another perspective, the industry has boxed itself into a tight and uncomfortable corner. Modern Holstein dairy cows only last for two to three lactations, rather than the five to eight (or more) of more traditional systems.  These animals carry a heavy burden of nutritional and metabolic diseases and poor fertility, often with adverse consequences for welfare that require routine treatment with antibiotics and hormones – all justifiably of concern to the consumer. An average of 37% of Holstein cattle suffer from painful lameness, significantly more so than other breeds. The Holstein cow is arguably the world’s least fertile farm animal. Around 60% require hormonal treatments for successful pregnancy, an obvious prerequisite to the annual calving and lactation cycle. These treatments may not be harmful to consumers, but routine use of hormones for growth promotion in farm animals was banned in the EU in 1988, and consumers are ill-informed about the risks involved. The prodigious milk yield of Holstein cows involves consumption of energy and protein far beyond the levels available from pasture. They must be fed a grain-rich diet they are ill-equipped to digest, consuming in a single lactation more than their own body weight of cereals.  Feeding cereals to multi-stomached ruminants such as cattle negates much of their evolutionary advantage, namely their ability to digest fibrous plant material such as forage, green waste and by-products that are of low nutritional value to  species such as pigs, poultry or indeed humans. Importantly, cereals are potential human food and are generally produced using polluting artificial fertiliser. In addition, digestive disorders such as displaced abomasum (one of the cow’s four stomachs) were a relative rarity a generation ago but are now commonplace. In the UK, a minority of dairy farmers use alternative breeds, such as the British Friesian, Ayrshire, or the Montbéliarde. They yield up to 8,000 litres per lactation, but these cows are more robust and are fed primarily off grass or preserved forage in winter, with a modest level of concentrate supplements at peak lactation. Lameness, mastitis, metabolic disease and infertility are far less frequent than in intensively managed Holsteins.  Welfare is less of an issue and antibiotics are rarely necessary, if used at all. Many of these breeds are dual purpose, so their male calves are suitable for rearing for beef, unlike Holsteins in which males are generally disposed of at birth. Dairy cows fed in pasture also require less inorganic fertiliser for cereal production, with less associated environmental pollution.  A change to a less intensive dairy production system would be in keeping with a broader vision, laying down a number of the basic principles for sustainable livestock. One of the central tenets is reduction in consumption of livestock products by humans, with consumption focussed on quality rather than quantity. It is worth noting that milk and dairy products from grass-fed cattle are higher in N-3 fatty acids, and conjugated linoleic acids. Finally, much attention has been placed on cattle as a source of methane, accounting for the majority share of the 14.5% of man-made greenhouse gas attributed to livestock. It is difficult to predict the value of managerial change to a less intensive dairy system, but there could be other immediate environmental benefits, such as reduction in artificial fertiliser use. For example, current analysis suggests the overall environmental costs of inorganic nitrogen use in Europe (estimated at €70–€320 billion per year) outweighs its direct economic benefits to agriculture. The Pareto principle (80:20 rule) is arguably at work here: with the Ayrshire and other less extreme dairy breeds you get 80% of the yield for perhaps only 20% of the welfare cost, and maybe just 20% of the environmental costs too. Given that today’s overweight consumers perceive milk as low-value and currently throw much of it away, having only 80% of today’s supply might not be too high a price for a sustainable future with healthier, happier cows."
"

America’s goal of stopping nuclear non‐​proliferation has suffered two serious setbacks in recent years. Both North Korea and Iran appear to be pursuing ambitious nuclear weapons programs. What U.S. officials have not recognized is that such actions are a logical, perhaps even inevitable, response to the foreign policy the United States has pursued since the end of the Cold War. Washington may have tried to shape the international system with the best of motives, believing that taking action against unsavory states would both enhance America’s security and advance the goals of peace and justice in the world. But as generations of realist scholars have shown, other nations may not concede that the motives of an activist power are benign. What might seem to U.S. policymakers justifiable, even noble, behavior may seem threatening to nations that have a less than cordial relationship with the United States. 



Consider the extent of U.S. military action since the opening of the Berlin Wall. The United States has engaged in nine major military operations during that period. Moreover, in his 2002 State of the Union address, President Bush explicitly linked both North Korea and Iran to Iraq (a country with which the United States was clearly headed to war) in an “axis of evil.” In the wake of Bush’s decision to engage in pre‐​emptive regime change in Iraq, it is hardly surprising that Pyongyang and Tehran concluded that they might be next on Washington’s hit list unless they could effectively deter an attack. Yet, neither country could hope to match the conventional military capabilities of a superpower. The most reliable deterrent–maybe the only reliable deterrent–is to have nuclear weapons. In other words, U.S. behavior may have inadvertently created a powerful incentive for the proliferation of nuclear weapons–the last thing Washington wanted.



Of course, the United States must always be prepared to use military force to defend its vital interests. With regard to elective wars, however, U.S. leaders need to become more aware of the range of possible outcomes–including the risk of unintended side effects. In the security environment of the 21st century, Washington should adopt a security strategy that is both more cautious and more flexible.



North Korean and Iranian leaders noticed that the United States treats nations that possess nuclear weapons quite differently than those that do not. That is not a new phenomenon. Just six years after China began to develop nuclear arms, the United States sought to normalize relations–reversing a policy of isolation that had lasted more than two decades. U.S. leaders show a nuclear‐​armed Russia a fair amount of respect, even though that country has become a second‐​rate military power and a third‐​rate economic power. Washington has treated Pakistan and India with far greater respect since those countries barged into the global nuclear‐​weapons club in 1998.



Contrast those actions with Washington’s conduct toward non‐​nuclear powers such as Iraq and Serbia. The lesson that North Korea and Iran learned (and other countries may be learning as well) is that possessing a nuclear arsenal is the way to compel the United States to exhibit caution and respect. This is especially true if the country has an adversarial relationship with the United States. After the Iraq War started, North Korea’s Foreign Ministry declared: “The Iraqi war shows that to allow disarming through inspection does not help avert a war but rather sparks it. [Only] tremendous military deterrent force powerful enough to decisively beat back an attack … can avert a war and protect the security of the country.” Those who cheered U.S. military interventions, conservatives and liberals alike, need to ask themselves whether increasing the incentives for nuclear proliferation was a price worth paying–because greater proliferation is the price we are now paying.



And it is not only “rogue states”, but even long‐​established signatories to the Nuclear Non‐​Proliferation Treaty (NPT) that are considering whether acquiring a nuclear deterrent in the current international climate might be in their interests. That development has provoked a predictable yet unhealthy reaction in the United States. Members of the arms control community have devoted at least as much time and energy to the possibility that stable, democratic, status quo powers such as Germany, Japan, Sweden and South Korea might decide to abandon the NPT and develop nuclear deterrents as they have to the prospect that unstable or aggressive states might do so.



That misguided concern is not confined to the traditional arms control community, often considered to be more liberal. As the North Korean nuclear crisis evolved, some of the most hawkish members of the U.S. foreign policy community became terrified at the prospect that democratic U.S. allies in East Asia might build their own nuclear deterrents to offset Pyongyang. Robert Kagan and William Kristol regarded that prospect with horror: “The possibility that Japan, and perhaps even Taiwan, might respond to North Korea’s actions by producing their own nuclear weapons, thus spurring an East Asian nuclear arms race … should send chills up the spine of any sensible American strategist.”



This attitude misconstrues the problem. A threat to the peace may exist if an aggressive and erratic regime gets nuclear weapons and then is able to intimidate or blackmail its neighbors. But nuclear arsenals in the hands of stable, democratic, status quo powers are not an inherent threat to peace and stability. Kagan and Kristol–and others who share their hostility toward such countries having nuclear weapons–are guilty of embracing a moral equivalence between a potential aggressor and its potential victims.



America’s current one‐​size‐​fits‐​all non‐​proliferation policy is the international equivalent of domestic gun control laws–and exhibits the same faulty logic. Gun control laws have done little to prevent criminal elements from acquiring weapons. Instead, they disarm honest citizens and make them more vulnerable to armed predators. The non‐​proliferation system is having a similar effect. States like Iran and North Korea are well along the path to becoming nuclear powers, while their more peaceful neighbors are hamstrung by the NPT from countering those moves. The United States must therefore extend its nuclear umbrella, placing America at greater risk, to guarantee the security of allies and clients. Washington’s own non‐​proliferation efforts should focus on delaying rogue states in their quest for nuclear weapons, not on causing problems for peaceful states that want to become nuclear powers to deter unfriendly actors in their neighborhoods.



True, such a changed attitude on the part of the United States might well lead to greater proliferation in some regions. That prospect, however, should not be extrapolated to the nightmare vision of a spiraling arms race and endless proliferation. Nations will make their decisions about whether to become nuclear powers based on a host of factors, and it is not a cost‐ or risk‐​free decision–financially, politically or diplomatically.



U.S. policymakers must rid themselves of the notion that all forms of proliferation are equally bad. The United States should concentrate on making it difficult for aggressive or unstable regimes to acquire the technology and fissile material needed to develop nuclear weapons. Even then, American leaders should keep in mind that, at best, U.S. actions will likely only delay, not prevent, such states from joining the nuclear club.



Still, delay can provide important benefits. A delay of only a few years may significantly reduce the likelihood that an aggressive power with a new nuclear weapons capability will have a regional nuclear monopoly and thus the ability to blackmail neighbors. In some cases, the knowledge that achieving a regional nuclear monopoly is impossible may discourage a would‐​be expansionist power from making the effort to begin with. At the very least, it could cause such a power to configure its new arsenal purely for deterrence rather than for aggression. Although one cannot be certain that the nuclear equilibrium that the United States and Soviet Union achieved on a global basis during the Cold War will be replicated on a regional level, the chances of such a stable environment emerging from a balance of power between regional nuclear states are better than they are in a situation where an aggressive, revisionist state enjoys a nuclear monopoly. Indeed, the recent behavior of India and Pakistan provides some cautious evidence that stable regional nuclear balances may be possible.



So while in the general sense it might be true that fewer nuclear weapons in the world (and fewer countries with nuclear weapons) would be a good thing, such logic is not necessarily absolute. Instead of assuming that all proliferation of nuclear weapons is an inherent danger that must be prevented, policymakers should analyze proliferation and assess its consequences on a case‐​by‐​case basis.



There is, of course, one area in which the United States must have a proactive policy: making it clear to new nuclear powers that transferring nuclear technology or weapons to non‐​state actors is utterly unacceptable because such groups are probably not deterrable. And there are three principal states of concern that fall under the heading of potential malignant proliferators: North Korea, Iran and Pakistan.



The ability of the United States to tolerate a nuclear‐​armed North Korea is predicated on North Korea not becoming the global supermarket of nuclear technology. An especially acute danger is that Pyongyang may provide either a nuclear weapon or fissile material to Al‐​Qaeda or other terrorist organizations. North Korea’s record on missile proliferation does not offer much encouragement that it will be restrained when it comes to nuclear materials. Perhaps most troubling of all, Pyongyang has shown a willingness to sell anything that will raise revenue for its financially pressed regime, as evidenced by the recent discovery of its involvement in the heroin trade.



Washington should communicate to North Korea that selling nuclear material–much less an assembled nuclear weapon–to terrorist organizations or hostile governments will be regarded as a threat to America’s vital security interests. Indeed, U.S. leaders should treat such a transaction as the equivalent of a threatened attack on America by North Korea. Such a threat would warrant putting all options on the table, including military action to remove the North Korean regime. It might even include using nuclear weapons in retaliation for any terrorist nuclear attack. Pyongyang must be told in no uncertain terms that trafficking in nuclear materials is a bright red line that it dare not cross.



Iran’s nuclear weapons program is a concern because of that country’s ties to terrorist groups. It is no secret that Iran provides funding, safe haven, training and weapons to anti‐​Israeli groups. But Iran has not supplied terrorist groups with chemical or biological weapons to use against Israel, so it is not clear what incentive Iran would have to give nuclear weapons to terrorists. Indeed, Israel’s nuclear arsenal (believed to consist of up to 200 warheads) serves as a powerful deterrent against Iran taking such action.



Iran’s terrorist ties were also cited by the 9/11 Commission, which implicated Iran in the 1996 Khobar Towers bombing in Saudi Arabia and cited “strong evidence” that Iran facilitated the transit of several Al‐​Qaeda members before 9/11. The potential Iran‐​Al‐​Qaeda connection is a serious one that deserves further investigation. But without clear evidence that the regime in Tehran was involved in 9/11 or is otherwise supporting or harboring Al‐​Qaeda, the United States cannot afford another unnecessary war and military occupation like Iraq. But, as with North Korea, it should be made clear to Tehran that transfer of such weapons, material or technology to terrorist groups will be justification for regime change. This policy must be strictly (and swiftly) enforced, not just with Iran but with any other country that aspires to nuclear status.



There is another point that needs to be raised. It is especially troubling that the United States has no direct ties with Iran or North Korea, the two nations that may be the next members of the nuclear weapons club. That is an unhealthy and dangerous situation. In contrast, throughout the Cold War, when the United States and Soviet Union had thousands of nuclear warheads aimed at each other and a nuclear conflict would have resulted in certain destruction of both societies, the two adversaries did not cut off relations. Despite a hostile relationship, the United States maintained an embassy in Moscow and engaged in normal diplomatic relations without ever conceding the fundamental legitimacy of the Soviet system. Eventually, the two powers developed a crisis hotline and adopted other confidence‐​building measures. By creating normalized relationships with nations with nuclear aspirations, Washington would reduce the danger of miscalculation.



Another benefit of normal diplomatic relations with rogue states is the increased likelihood of better intelligence about a country’s nuclear program. Iraq is an important example. The United States had virtually no intelligence assets on the ground to provide first‐​hand information on the status of Iraq’s nuclear program. Instead, decisions were based on second‐​hand information–primarily from Iraqi exile groups. But the reliability of that information was always uncertain. And–whether for the right reasons or wrong ones–information provided by UN weapons inspectors was not deemed accurate or reliable, and therefore was not trusted.



So, however unsavory it might seem, a more productive approach to U.S. non‐​proliferation efforts would be to develop better and closer relations with the very regimes that are cause for concern–rather than isolating them, as is currently the case. There is, after all, something to be said for the maxim attributed to Sun Tzu: Keep your friends close but your enemies closer.



Pakistan also demonstrates the limitations of current non‐​proliferation thinking. Although the current regime is considered an ally in the War on Terror and has helped capture some important Al‐​Qaeda operatives, the prospect of that country’s nuclear weapons falling into the hands of radical Islamists must be planned for. Pakistan is also a concern because so many nuclear efforts in other countries (such as North Korea, Iran and Libya) were tied to a nuclear bazaar created by Pakistani scientist A. Q. Kahn, who has been hailed as a national hero by Pakistani President Pervez Musharraf. Unfortunately, neither the traditional non‐​proliferation approach nor pre‐​emptive war is a real solution to this problem. Instead, U.S. efforts should focus on creating better security and command and control over Pakistan’s nuclear weapons to prevent them from being used by terrorists. Continuous pressure must also be exerted on Pakistan to make sure that leakage of weapons and materials does not occur. In that vein, there may be lessons to learn from the Nunn‐​Lugar cooperative threat‐​reduction program to safely secure Russian “loose nukes” that could be transferred to the Pakistani nuclear arsenal.



U.S. policymakers must think beyond traditional non‐​proliferation policy. That policy may have served us reasonably well in the past, but a rapidly changing global security environment is rendering it obsolete and potentially counter‐​productive. We can no longer cling to the NPT and all it symbolizes as the answer to all the varied problems of nuclear proliferation. Instead, we need a large policy toolbox with a variety of tools. That involves a tacit admission that the NPT has probably outlived its usefulness–at least in its current form. No policy lasts forever. In its nearly four decades of existence, the NPT has achieved some significant results, most notably in reversing South Africa’s decision to become a nuclear power and delaying proliferation in a number of regions. But the emergence of Israel, India and Pakistan as full‐​blown nuclear powers and of North Korea and Iran as threshold nuclear states shows that the traditional non‐​proliferation regime centered around the NPT is a waning asset.



Some aspects of existing U.S. nuclear policy remain viable. We can continue to rely on the ability of America’s vast nuclear arsenal to deter attacks on the American homeland by other nuclear powers. Nation‐​states have fixed addresses and leaders of those countries understand that an attack on the United States would be met with certain retaliation. America deterred the Soviet Union under Josef Stalin and China under Mao Zedong. We can deter new nuclear adversaries as well.



Although proliferation of nuclear weapons and the need to deter new nuclear powers are not welcome prospects, we must be realistic and recognize the likelihood that the number of nuclear powers in the international system will increase in the coming decades and that many of those new members of the global nuclear club will be unsavory regimes. Washington’s non‐​proliferation efforts should concentrate on delaying rogue states in their quest for nuclear weapons, not on badgering peaceful states that may want to become nuclear powers for legitimate security reasons. The problems confronting a focused non‐​proliferation policy are daunting enough without continuing the vain effort to prevent all forms of proliferation. 
"
"Many parts of the world are likely to experience above-average temperatures over the next few months, even without a natural El Niño effect, according to weather experts. The UN’s World Meteorological Organisation (WMO) said the signal from human-induced climate change was now as powerful as the natural phenomenon, which drives warmer temperatures.  It said there was a 60% chance of a neutral situation without an El Niño or its opposite, La Niña, between March and May. There was a 35% chance of an El Niño developing and 5% for a La Niña. The El Niño southern oscillation (Enso) is a naturally occurring phenomenon in the Pacific with a warming influence on global temperatures. It is also linked to heavy rain, flooding and drought. Despite the expected absence of an El Niño, the WMO forecasts there will be above-average sea surface temperatures in many parts of the world, which will lead to higher than normal land temperatures. Climate change would contribute to these conditions, the WMO said. The WMO’s secretary general, Petteri Taalas, said: “Even Enso-neutral months are warmer than in the past, as air and sea surface temperatures as well as ocean heat have increased due to climate change. “With more than 90% of the energy trapped by greenhouse gases going into the ocean, ocean heat content is at record levels. Thus, 2016 was the warmest year on record as a result of a combination of a strong El Niño and human-induced global warming. 2019 was the second-warmest year on record, even though there was no strong El Niño. “We have just had the warmest January on record. The signal from human-induced climate change is now as powerful as that from a major natural force of nature.”"
"

Recently, a World Wildlife Fund press release was picked up by Reuters. “Himalayan glaciers are among the fastest‐​retreating glaciers globally due to the effects of global warming,” the advocacy group announced.



WWF timed its press release for a two‐​day Energy and Environmental Ministerial conference in London, where the United States was (predictably) criticized because it won’t commit economic suicide by adopting the Kyoto Protocol on global warming.



This is one of those repeating news stories, like “Strife in Haiti” or “Irish unrest.” It goes like this. “The (glaciers, polar bears, butterflies) of (anywhere) are in dramatic decline because of global warming. Unless the (U.S., U.S., U.S.) signs on to the Kyoto Protocol, their continued decline is assured.” 



Here’s another broken record. “It appears that the (U.N., World Wildlife Fund, New York Times) forgot to check the temperature histories where the (ice, polar bears, butterflies) are in decline, and the (U.S., U.S., U.S.) isn’t going along with counterfactual nonsense produced by agenda‐​driven environmentalists.”



We offer this evidence. WWF is especially interested in the Gangotri glacier, in the Indian Himalayas. The glacier is retreating an average 75 feet yearly.



Glaciers are in steady state when the annual snowfall and summer melting rate are roughly in balance. Actually, this is rare. When glaciers melt too much in the summer, they retreat. And if it snows more in the winter than normal, they advance.



The United Nations Intergovernmental Panel on Climate Change (IPCC) publishes historical temperature records around the planet. They are averages for 5 X 5 degree latitude/​longitude rectangles. They used these somewhat large areas so that, in general, many local records are averaged up to form a reliable regional picture. The Gangotri Glacier, which feeds the Ganges River, is in the 30–35N, 75–80E box.



High‐​altitude glaciers melt during the summer. The IPCC has June‐​August temperatures for the Gangotri region back 1875. The net decline in temperature over the last 130 years is striking. In fact, at 1.2 degrees (C), it is one of the largest summer coolings on Earth. That’s right: cooling. In contrast, the temperature for the Northern Hemisphere as a whole increased 0.8 degrees during the same period.



Still, no one doubts the Gangotri glacier is receding. It was expanded far beyond where it is today when the cooling was first noted more than a century ago. Temperatures reached their low in 1990 and have popped up a bit, to the long‐​term average for the last 130 years. Perhaps this has something to do with Gangotri’s recent more rapid retreat.



But that it has been in such a decline as overall century‐​scale temperatures have cooled tells us much about the long‐​term fate of glaciers away from polar regions: They are relics of the Ice Age, destined to melt.



Another place with an ice history that resembles Gangotri is our own Glacier National Park in Montana. There were 147 glaciers in the park 150 years ago, near the start of the Gangotri temperature record. Today there are only 37. What happened to summer temperatures? Unlike Gangotri, they didn’t cool. But temperatures remained fairly constant, with no significant warming since records began in 1895.



Most scientists think the mid‐​19th century marks the end of a multicentury period known as the “Little Ice Age,” though a small but vocal core of skeptics maintain a view known as the “Hockey Stick” history — one in which temperatures do not change for nearly a millennium and then shoot up in the last 100 years, producing a graph that indeed resembles a hockey stick. This view has been pretty much marginalized in a number of papers in scientific literature over the last year.



Indeed, glaciers went into retreat at the end of this cold period. Gangotri is even more tenuous, receding even as local temperatures continued declining.



Incidentally, the Northern Hemisphere’s largest ice mass — the Greenland icecap — is in retreat in the southern part of the island, where temperatures also show a substantial net cooling for the last 75 years.



All this leads to an obvious conclusion. Southern Greenland, Glacier National Park and the Himalayan glaciers are on their way out, with little or no nudging needed from people. They’re relics of the Big Ice Age that ended 11,000 years ago. It’s too bad, though, that in the fight to hype global warming, the truth is also rapidly becoming another relic.
"
"Our mild winter may have helped the UK’s legal problems with nitrogen oxides. In 2016, Department for Transport scientists made a chance finding when testing 38 diesel cars following the Volkswagen scandal. They found that exhaust emissions were far worse when vehicles were tested outside in the cold rather than indoors in the warm, as required in the legal tests.  Drive your diesel car on a cold day and the exhaust clean-up system may be turned down or even off. Manufacturers argued that this protects components from damage, but technical experts disagreed. Last year data was published from snapshot measurements on 200,000 cars as they travelled through light beams placed along UK roads. The average diesel car produced nearly three times more nitrogen oxide pollution at 0C than at 25C and this depended on the clean-up technology used. BMW was the best and General Motors was the worst. Future diesel cars will have to pass on-road rather than just laboratory tests and early evidence shows that new diesels emit less pollution than the old ones that they replace. The loss of public confidence in diesel means that more petrol cars are being sold. These produce little nitrogen oxides whatever the weather. These two factors suggest that nitrogen dioxide pollution should improve."
"One of China’s major genetically modified food projects is now to all intents and purposes dead and buried. The expiry on August 17 of the biosafety certificates issued to strains of GM rice developed in the labs of Huazhong Agricultural University, Wuhan, signals a major blow to the fight to establish GM food in China. The contrast with the rest of the world could hardly be starker. In the UK, for instance, the country’s first genetically modified crops are almost ready for harvesting following a landmark trial. The production of a unique crop of “false flax” camelina – one spliced with genes capable of producing omega-3 fatty acids normally found only in fish – has been hailed as a milestone in the country’s journey towards the creation of GM food.  For China, it didn’t have to be this way. For more than two decades, with government support, Chinese scientists have been frontrunners in researching and developing genetically modified organisms (GMOs). The country has become a global leader in agro-biotechnology, driven by the inescapable need to feed a population of 1.3 billion and by a concomitant determination to increase yields, improve nutrition, ensure food security and tackle the problems caused by pests, diseases and pollution. Now, the head start it enjoyed over many nations has been eroded. Where once there was significant momentum, progress is slowing to an agonised crawl. Scientists who were formerly championed as trailblazers are now habitually condemned as agents of a foreign power. How did this happen? China was actually more cautious than most in embracing genetically modified foods. It issued its first regulations on genetic engineering in 1993, and three years later it followed up with detailed measures on the implementation of biosafety in researching, testing and commercialising agricultural GMOs. In 2001 a high-level State Council regulation established an administrative office for agricultural GMOs, which draws policymakers and experts from across ten government agencies. GM crops have to pass a five-stage process leading to biosafety certification. This requires evaluation in a laboratory setting and during various phases of field testing. Crops have to undergo internationally accepted risk-assessment procedures and are also benchmarked against their non-GM counterparts. It is a commendably thorough protocol – one intended to balance advances in research with broader concerns of both scientists and others. Five years ago, with considerable fanfare, the process culminated in the Chinese government granting biosafety certification to GM rice – at least for research purposes. Unlike the US and Iran, which stopped short of commercialisation, China was widely expected to soon put GM rice on the country’s dining tables. Those very same biosafety certificates have now expired. It will be interesting to see what – if anything – happens now, not least in light of the agriculture ministry’s recent pledge that the illegal growth or sale of any GM grains will be met with “zero tolerance” and “harsh punishments”. In any event, what is becoming ever clearer is that China’s enthusiasm for GM food, in direct contradiction to that of many of its economic rivals, continues to wane. Given their respective trajectories in this regard, it is tempting to ask what a nation like the UK has that China doesn’t. It would be fanciful in the extreme to suggest the regulatory environment is conspicuously slacker; it would also be unrealistic to claim the UK’s needs are somehow more pressing. We would perhaps be better served by reversing the question and asking what China has that the UK doesn’t; and the answer is an anti-GM movement whose power and influence are more than matched by its fervour and sheer, undiluted paranoia. The debate over GM food has been aired around the world. Often it has been one of science versus emotion and even politics. But nowhere have the arguments against been more outrageous – or, almost incredibly, more successful – than in China, where the gainsaying has now even descended into a very public slanging match between the agriculture ministry and the army. In the eyes of its Chinese opponents – whose ranks include Major General Peng Guangqian, who also happens to be deputy secretary general of the National Security Policy Committee – GM food is not merely a cause of cancer and a source of infertility. It is also a grand Western scheme. It is a monumental, supremely devious plot to annihilate the Chinese and other people of colour. It has been created by Monsanto, an American firm, with the backing of the Pentagon and leading private foundations in the US, to control the global food chain. Some critics seek to extend this extraordinary warped logic. They contend that many of the Chinese scientists who have worked on GM rice are American-educated; that they must therefore be in league with the architects of the aforementioned plot; that they must therefore be dedicated to undermining China’s national interests and security; and that they must therefore be – to quote one enraged blogger – “traitors and lackeys”. Thus a recent China Central Television investigation screened late last month saw fit to implicate Professor Zhang Qifa, of Huazhong Agricultural University, in an alleged conspiracy to spread the strains of GM rice developed in his laboratory. It might be ludicrous, but it is effective. Irrational pronouncements have been deemed to carry more weight than empirical evidence. Anti-Western sentiment has been judged more convincing than a raft of studies endorsing the merits of agro-biotechnology. Government support for GM food is dwindling fast, and it seems safe to say that the opportunity to commercialise GM rice – and with it the chance to help address some of China’s most urgent problems – is all but gone. You do not have to be a scientist to see that reason has suffered a crushing defeat. "
"New Zealand prime minister Jacinda Ardern won’t raise climate change when she meets with Scott Morrison in Sydney but will push for a change in Canberra’s position on forced deportations. Ardern was scheduled to hold talks with Morrison in the harbour city on Friday after briefly meeting with New South Wales premier Gladys Berejiklian in the morning. The New Zealand prime minister speaks with Morrison more than any other leader to the point where her officials often joke they don’t need to do any work “because we often just resolve things directly”. But that doesn’t mean there aren’t issues of contention, Ardern acknowledged ahead of Thursday’s flight to Australia from Fiji. New Zealand is held up as a great ally on climate issues in the Pacific while Australia is criticised for promoting fossil fuels. The divide is such that Ardern – who has enshrined into law a pathway to net-zero carbon emissions by 2050 – said she would not bother to raise the topic in Sydney. She pledged to discuss the “corrosive” issue of deportations which has seen hundreds of New Zealanders, some with limited links to their country of birth, deported from Australia after committing serious crimes. One of the most high-profile cases involves AFL star Dustin Martin’s father who was deported to New Zealand after living in Australia for 20 years because of his links to the Rebels bikie club. Canberra has been unrelenting when it comes to the policy and New Zealand is realistic as to whether any concessions may be forthcoming. Overall, however, Ardern knows the special bond between the regional neighbours means “we’re countries that lean on one another in times of need”."
"**London is a city of contrasts containing leafy suburbs, urban high rises and sprawling high-streets. So should the capital be split into more localised regions with different lockdown restrictions?**
Several MPs have called on the government not to consider the capital as one area when it comes to coronavirus.
Instead, it should be split up by inner city and outskirts or borough-by-borough to better reflect local infection rates.
They argue areas with low-infection rates shouldn't be ""punished unnecessarily"" by going into a higher tier designed for suppressing large outbreaks of coronavirus.
Regions will find out which tier they are in on Thursday but the prime minster has argued against splitting up the capital.
As a whole London has an infection rate of 180.5 cases per 100,000, well below average the England national average of 217.6 cases per 100,000.
But between London's boroughs there is huge variation.
Havering, London's worst affected borough, had a weekly case rate of 360 cases per 100,000 up to 20 November.
This is more than three times higher than Camden's 97 cases per 100,000 and Richmond's 108.6.
""London is a wonderful, diverse city - it's not just one unit,"" Conservative MP for Wimbledon, Stephen Hammond said.
""We ought to look at London on a borough-by-borough or sub-regional basis,"" Mr Hammond said.
""I don't see why Londoners should suffer when the rest of the country is being looked at on a more localised basis.""
A borough-by-borough basis is also supported by Bob Blackman, Conservative MP for Harrow East.
""What we need to avoid is a situation where the borough's with the lowest infection rates are placed under the same restrictions as those with the highest infection rates and therefore punished unnecessarily,"" Mr Blackman said.
""In the outskirts of London there are only a few boroughs which have high infection rates.""
But not everyone agrees.
Despite representing one of London's least infected boroughs Sarah Olney doesn't want her Richmond constituency to potentially end up in a lower tier.
The Liberal Democrat MP said: ""I want Richmond to remain as part of an overall London tier, to discourage people from travelling here and increasing the risk of infections spreading.""
The government has not released the exact formula it will use to decide what restrictions will be applied to which region.
In London the rate of infection appears to be slowing but the latest figures show the number of weekly deaths is starting to creep up.
Deaths linked to coronavirus in London have dropped drastically from a peak in early April.
In the latest week with complete data, the week ending 13 November, the number of people who died after contracting coronavirus in the capital jumped by 42% from 87 to 124.
Prime Minister Boris Johnson has said the three-tiered regional measures will return from 2 December, but added each tier would be toughened.
Mr Johnson argued against splitting London into smaller regions with different tiers.
In the House of Commons he said despite London's diversity ""it is held together by a very dense mass transit system"" making it ""difficult to separate one bit of London from another"".
In this he's supported by the Mayor of London, Sadiq Khan, who has said smaller regions will make enforcing the rules harder.
The mayor's office argues that setting different rules for high streets, pubs and restaurants in neighbouring boroughs could push footfall between areas ""with unintended consequences"".
A spokesman for the mayor said the Met Police ""had a centrally coordinated approach"" to enforcing lockdown restrictions.
""Any difference between boroughs would not help clarity of message or rules,"" he said."
"
Share this...FacebookTwitter???
===========================================
UPDATE: Oh wait! Here’s something: http://www.focus.de/wissen/wissenschaft/wissenschafts-dossiers/tid-23443/neue-studie-bestaerkt-klimaskeptiker-die-sonne-unter-verdacht_aid_658937.html
I need to be more patient with Germans. They have a habit of thinking about things from different angles before shooting off. That’s good of course. Will write more about the FOCUS report later today.
Share this...FacebookTwitter "
"
I’ve been very critical of statements made by Dr. Mark Serreze of the National Snow and Ice Data Center. It seems that I’m not the only one critical of his statements to the press. – Anthony
Excerpts from The Times, UK story:
Exaggerated claims undermine drive to cut emissions, scientists warn
Mark Henderson, Science Editor
Images from 2001, top, and 2007 from Philip's Universal Atlas of the World indicated a big decline in Arctic ice, used as proof of climate change
Exaggerated and inaccurate claims about the threat from global warming risk  undermining efforts to cut greenhouse gas emissions and contain climate  change, senior scientists have told The Times.
Environmental lobbyists, politicians, researchers and journalists who distort  climate science to support an agenda erode public understanding and play  into the hands of sceptics, according to experts including a former  government chief scientist.
Excessive statements about the decline of Arctic sea ice, severe weather  events and the probability of extreme warming in the next century detract  from the credibility of robust findings about climate change, they said.
Such claims can easily be rebutted by critics of global warming science to  cast doubt on the whole field. They also confuse the public about what has  been established as fact, and what is conjecture.
The experts all believe that global warming is a real phenomenon with serious  consequences, and that action to curb emissions is urgently needed.
They fear, however, that the contribution of natural climate variations  towards events such as storms, melting ice and heatwaves is too often  overlooked, and that possible scenarios about future warming are  misleadingly presented as fact.
…
“When people overstate happenings that aren’t necessarily climate  change-related, or set up as almost certainties things that are difficult to  establish scientifically, it distracts from the science we do understand.  The danger is they can be accused of scaremongering. Also, we can all become  described as kind of left-wing greens.”
Vicky Pope, head of climate change advice at the Met Office, said: “It isn’t  helpful to anybody to exaggerate the situation. It’s scary enough as it is.”
She was particularly critical of claims made by scientists and environmental  groups two years ago, when observations showed that Arctic sea ice had  declined to the lowest extent on record, 39 per cent below the average  between 1979 and 2001. This led Mark Serreze, of the US National Snow and  Ice Data Centre, to say that Arctic ice was “in a downward spiral and may  have passed the point of no return”.
Dr Pope said that while climate change was a factor, normal variations also  played a part, and it was always likely that ice would recover a little in  subsequent years, as had happened. It was the long-term downward trend that  mattered, rather than the figures for any one year, she added.
“The problem with saying that we’ve reached a tipping point is that when the  extent starts to increase again — as it has — the sceptics will come along  and say, ‘Well, it’s stopped’,” she said. “This is why it’s important we’re  as objective as we can be, and use all the available evidence to make clear  what’s actually happening, because neither of those claims is right.”
…
“In 1998, people thought the world was going to end, temperatures were going  up so much,” Dr Pope said. “People pick up whatever makes their argument,  but this works both ways. It’s the long-term trend that counts, which is  continuing and inexorable.”
Read the entire article here at The Times
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e92194b10',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _Global Science Report_ _is a weekly feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom._   
  
  
From the authors of a new paper just‐​published in the journal _Nature Geoscience_ comes this surprising finding:   




Tropical forests are less likely to lose biomass – plants and plant material – in response to greenhouse gas emissions over the twenty‐​first century than may previously have been thought.



A rare “not as bad as we thought” admission about the impacts of manmade global warming!   
  
  
Not only that, but based on recent findings that the true climate sensitivity is much lower than climate models emulate—findings not incorporated in new study—the results are probably still even more “not as bad as they thought” than they thought!   
  
  
Chris Huntingford from the U.K’s Centre for Ecology & Hydrology and colleagues coupled climate model projections to a land surface/​vegetation model to see how the tropical forests in the Americas, Africa, and Asia respond to changes in atmospheric conditions. Their vegetation model includes interactions between terrestrial plants and influences such as temperature, precipitation, and the carbon dioxide concentration of the atmosphere (a plant fertilizer).   
  
  
Unlike other studies which used a very limited selection of climate models and less sophisticated vegetation models, the Huntingford team found that in virtually all future simulations that the biomass of tropical forests increases over the course of the 21st century. This is a significantly different result than many previous which suggested that anthropogenic climate change would lead to, as Huntingford et al. put it, “catastrophic losses of forest cover and biomass.”   






Perhaps most interestingly, the major driver for the biomass increase is the projected growth in atmospheric carbon dioxide concentration (thanks to our use of fossil fuels). The model projected changes in precipitation had little impact on the biomass predictions and the projected increase in temperature acted to decrease the biomass (although not as much as additional carbon dioxide acted to increase it).   
  
  
Which is why the results probably get even better if there is less warming associated with carbon dioxide emissions than current generation climate models predict (new research suggest that climate models together produce about 50% more warming than they should).   
  
  
The authors are quick to mention that uncertainty abounds, as our level of understanding of forest response to changing environmental conditions is not all that high. But even given these uncertainties, the authors are confident that their results of increasing biomass are robust. Here is how Huntingford described the situation in a press release:   




The big surprise in our analysis is that uncertainties in ecological models of the rainforest are significantly larger than uncertainties from differences in climate projections. Despite this we conclude that based on current knowledge of expected climate change and ecological response, there is evidence of forest resilience for the Americas (Amazonia and Central America), Africa and Asia.



Resilience. A refreshingly honest assessment of an ecosystem response to climate change. And one that is probably a much more apt descriptor of natural systems than “delicate,” “sensitive,” or “fragile.”   
  
  
Now if only the folks in charge of assembling national and international climate impact assessments would realize (or probably more accurately, admit to) this.   
  
  
We are hard at work trying to focus their attention as we are vigorously reviewing the latest draft “National Assessment” of climate change. We will leak out particularly juicy snippets in these pages when the time seems right.   
  
  
**Reference:**   
  
  
Huntingford, C. et al., 2013. Simulated resilience of tropical rainforests to CO2‐​induced climate change, _Nature Geoscience_ , 10.1038/NGEO1741.
"
"
Share this...FacebookTwitterThe European Institute for Climate and Energy (EIKE) has a story on the German Weather Service, and temperature trends for Germany, which are a good indicator for Central Europe.
Cooling in Germany has been accelerating. Source: EIKE
A few days ago the Deutscher Wetterdienst, DWD, (German Weather Service) in a press release warned that Germany’s temperature were likely to rise 2 to 4°C by the year 2100 and that action was necessary. Like the Potsdam Institute for Climate Impact Research (PIK) the DWD too has been transformed into a propaganda mouthpiece for Germany’s powerful government-driven global warming movement.
But there’s a small problem at the DWD. Like the outlier sea level projections made by the PIK, the temperature projections made by the DWD just don’t match observations. For example Germany’s annual temperature over the last 11 years has shown COOLING, and not warming, see chart above produced by EIKE.
This has led scientists at EIKE to comment as follows:
This casts the DWD’s credibility into question.”
Sure many warmists will point to the latest press release issued by the DWD, which claims that the first 6 months of 2011 in Germany have been the second warmest on record.
But if the 2nd half continues on the same path as the July trend, then the DWD, may soon start find itself comparing quarter years, or even months to milk out any warming news from its statistics.
And in the meantime, in the real world, Germans will have to wait until 2050 or even 2100 for any real warming. The DWD should take a close look at what happened to the Met Office in England back when it tried to get into the global warming gig and so began issuing stupid press releases filled with fantasies and not meteorology.
Share this...FacebookTwitter "
"The Sydney Opera House is proof that Danish architects like to do things differently. As do some Danish zoos, who kill giraffes for conservation. And now there is  a Danish proposal to create a zoo there without cages for the animals – a Zootopia where the animals will roam free and humans will observe them from various enclosures.  This in itself is nothing new or radical, but finding the right way to engage zoo visitors is vital for global conservation efforts so it’s worth examining how zoos can manage that. Bars on animal cages are controversial because they remind humans of prisons. This is anthropocentric, however – to monkeys, the bars represent climbing structures. Some barred animal enclosures, such as the gorilla housing at Howletts Zoo, are ugly to the human eye but highly functional for the occupants.  But we can question what message the human visitor takes home. It is interesting to reflect that some zoos referred to their animals as the inmates until the 1960s. Modern zoos justify their existence based on their contributions to animal conservation, research and public education. Arguably the most important role of zoos is in public education – 10% of the world’s population visits a zoo every year and no other type of institution has such potential to globally educate people about the importance of conservation.   Informal zoo education is just as important as the more formal education visitors get from listening to talks and reading signs on their visit. The impressions that a visitor leaves a zoo with are highly important. A significant factor affecting this is enclosure design, this is why good modern zoo designers pay great attention to visitor psychology. The original barless zoo was developed in 1907 by Carl Hagenbeck at  Tierpark in Hamburg. Hagenbeck, an animal collector, loved the natural landscapes where he captured his zoo animals and wished to show the animals to the public in a natural manner. The animals were kept in their enclosures by hidden moats. His zoo was a radical change from small barred enclosures and gave the public an appreciation of the animals in their wild context. It was the birth of Safari Parks in the 1960s which really brought the zoo visitor close to the animals. Here the attraction continues to be the visitors’ closeness to a lion or giraffe. Rather like a rollercoaster, the thrill is a perceived one with your car window acting as an imperceptible barrier. But the problem is in conveying an effective education message when people are driving their own car. Plus, encouraging car use goes against the overall conservation message animal institutions should be telling. Florida is home to Monkey Jungle a zoo where the humans are caged and the animals run free. It makes for an oppressive experience as a human. While it gave me an insight into what it must be like for an animal to live in a small cage, I felt it must be difficult to get over an educational message about conservation in such an environment. The Netherlands have Apenheul Primate Park, a zoo where neither animals nor their human visitors are caged, but instead they share the same woodland space. I remember my visit to this zoo as very enjoyable – it was great fun to see woolly monkeys running across the heads of the visitors and squirrel monkeys playing with children in their pushchairs. But research shows that when humans have direct contact with wild animals they tend to see them as a child and this distracts from the important conservation message the zoo is trying to impart. In the UK there is Trentham Monkey Forest, which of the three I prefer.  Here visitors and monkeys share the same space, but there are human guides to prevent direction human-monkey interactions and to educate the public about the animals. I think this institution has got it right – the visitor has the wow factor of being close to animals in a natural environment and receives good quality information about conservation. All this raises the question, what is novel about the Danish proposal for Zootopia? There are two things. One is the integration of the public’s animal viewing areas into the landscape – to make them invisible to the animals.  The second is a series of futurist mirrored globes, which will allow visitors to float down a river to see the wildlife or be carried overhead on a cable car system. I have some reservations about this concept. The mirrored globes remind me of a cult 1960s TV series that was ironically called The Prisoner. To me this concept sounds like the visitors are voyeurs and does not acknowledge their part in the natural world. Given the looming wildlife extinction crisis, it is more important than ever that zoos get the right message over to their visiting public. This requires balancing how we can learn about animals formally, as well as through the informal messages transferred through the experience of viewing animals. "
"

Al Gore’s defense of global‐​warming hysteria in Sunday’s _New York Times_ has many flaws, but I’ll focus on just one whopper — where the “Inconvenient Truth” man states the opposite of scientific fact.



Gore says, “The heavy snowfalls this month have been used as fodder for ridicule by those who argue that global warming is a myth, yet scientists have long pointed out that warmer global temperatures have been increasing the rate of evaporation from the oceans, putting significantly more moisture into the atmosphere — thus causing heavier downfalls of both rain and snow in particular regions, including the Northeastern United States.”



It’s an interesting theory, but where are the facts?



According to “State of the Climate” from the National Oceanic and Atmospheric Administration, “Global precipitation in 2009 was near the 1961–1990 average.” And there was certainly no pattern of increasing rain and snow on America’s East Coast during the post‐​1976 years, when NOAA says the globe began to heat up.



So what was it, exactly, that Gore’s nameless scientists “have long pointed out”? A 2008 report from the Intergovernmental Panel on Climate Change, “Climate Change and Water,” says climate models “project precipitation increases in high latitudes and part of the tropics.” In other areas, the IPCC reports only “substantial uncertainty in precipitation forecasts.”



In other words, the IPCC said that its models predicted some increases in rain or snow — not observed them. And only in high latitudes or the tropics, which hardly describes New York or Washington, DC.



In fact, recent research actually contra dicts Gore’s claims about “significantly more water moisture in the atmosphere.”



In late January, _Scientific American_ reported: “A mysterious drop in water vapor in the lower stratosphere might be slowing climate change,” and noted that “an apparent increase in water vapor in this region in the 1980s and 1990s exacerbated global warming.”



The new study came from a group of scientists, mainly from the NOAA lab in Boulder. The scientists found: “Stratospheric water‐​vapor concentrations decreased by about 10 percent after the year 2000 … This acted to slow the rate of increase in global surface temperature over 2000 to 2009 by about 25 percent.”



Specifically, the study found that water vapor rising from the tropics has been reduced, because it has gotten cooler there (another inconvenient truth). A _Wall Street Journal_ headline summed it up: “Slowdown in Warming Linked to Water Vapor.”



Moisture in the lower stratosphere (about 8 miles above the earth’s surface) has been going down, not up.



Aside from clouds, water vapor accounts for as much as two‐​thirds of the earth’s greenhouse‐​gas effect. Water vapor traps heat from escaping the atmosphere — but clouds have the opposite effect (called “albedo”) by reflecting the sun’s energy back into space. And snow on the ground from the IPCC’s predicted precipitation in high latitudes would have the same cooling effect as clouds.



What the new research suggests is that changes in water vapor may well trump the effect of carbon dioxide (only a fraction of which is man‐​made) and methane (which has mysteriously slowed since about 1990).



This raises an intriguing question: Since the Environmental Protection Agency declared that it has the authority to regulate carbon emissions because of their presumed effect on the global climate, why hasn’t the EPA also attempted to regulate mist and fog?
"
nan
"
A Myth About The Surface Temperature Record Analyses Perpetuated On Dot Earth By Andy Revkin
By Dr. Roger Pielke Sr.


On the weblog Dot Earth today, there is text from Michael Schlesinger, a climatologist at the University of Illinois, that presents analyses of long term surface  temperature trends from NASA, NCDC and Japan as if these are from independent sets of data from the analysis of CRU.  Andy Revkin is perpetuating this myth in this write-up by not presenting the real fact that these analyses draw from the same  original raw data.  While they may use only a subset of this raw data, the overlap has been estimated as about 90-95%.
The unresolved problems with this surface data (which, of course, applies to all four locations) is reported in the peer reviewed paper
Pielke Sr., R.A., C. Davey, D. Niyogi, S. Fall, J. Steinweg-Woods, K. Hubbard, X. Lin, M. Cai, Y.-K. Lim, H. Li, J. Nielsen-Gammon, K. Gallo, R. Hale, R. Mahmood, S. Foster, R.T. McNider, and P. Blanken, 2007: Unresolved issues with the assessment of multi-decadal global land surface temperature trends. J. Geophys. Res., 112, D24S08, doi:10.1029/2006JD008229.
I discuss this issue in my recent post
Further Comment On The Surface Temperature Data Used In The CRU, GISS And NCDC Analyses
where I document that even the CCSP 1.1. report acknowledged this lack of independence.
Andy Revkin’s post on the surface temperature record data sets is not journalistically accurate.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e90ceedad',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
I don’ t know what sort of world NYT reporters live in, but I am now convinced that some like Paul Krugman have no clue about the real world people live in elsewhere.

‘This Week” with George Stephanopoulos debates ClimateGate – more here
Noel Sheppard over at Newsbusters provides some video and transcript of a debate between Paul Krugman of the NYT and Washington Post columnist  George Will.
KRUGMAN: There is tremendously more money in being a skeptic than there is in being a supporter. ... They get almost equal time in the media.
When I read what Paul Krugman said, I laughed out loud. He’s truly clueless.

Here’s the context:

WILL: Speaking of the marketplace, the biggest industry in the world right now may be fighting climate change. There are billions, trillions of dollars on the table, and when you say, well, they are academics and they are scientists and they talk in funny ways — academics are human beings, and the enormous incentive to get on the bandwagon on global warming, the financial incentive, the market driving this, is huge.
KRUGMAN: There is tremendously more money in being a skeptic than there is in being a supporter.
WILL: Hardly.
KRUGMAN: It’s so much easier, come on. You got the energy industry’s behind it. There are 20 times as many believers as there are skeptics in the scientific community. They get almost equal time in the media.
(CROSSTALK)
WILL: Is there a larger venture capital firm in this country than the Energy Department of this government, which right now is sending out billions and billions of dollars in speculation on green energy?
Noel Sheppard writes:
Skeptics get almost equal time in the media? Yeah, that’s why this appears to be the first time ABC addressed this ClimateGate issue.
As for there being more money in being a skeptic than there is in supporting this myth, the facts say otherwise.
The Science and Public Policy Institute issued a report on the money involved in funding the global warming debate in August concluding, “Over the last two decades, US taxpayers have subsidized the American climate change industry to the tune of $79 billion.”
By contrast, the same study found that the media bogeyman “Exxon Mobil gave a mere $23 million, spread over ten years, to climate sceptics.”
See the video and transcript at Newsbusters
UPDATE: Professor Don Easterbrook left this comment on the ABC news site:
I’ve spent 4 decades studying global climate change and as a scientist I am appalled at Krugman’s cavalier shrugging off the Hadley email scandal as ‘just the way scientists talk among themselves.’ That’s like saying it’s alright for politicians to be corrupt because that’s the way they are. Legitimate scientists do not doctor data, delete data they don’t like, hide data they don’t want seen, hijack the peer review process, personally attack other scientists whose views differ from theirs, send fraudulent data to the IPCC that is used to perpetuate the greatest hoax in the history science, provide false data to further legislation on climate change that will result in huge profits for corrupt lobbyists and politicians, and tell outright lies about scientific data.
Posted by: Don Easterbrook | Nov 29, 2009 1:57:05 PM


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9161a078',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

I’m a news junkie. I’ve bookmarked Drudge, and on Election Day, I kept going to his site to check the results of the morning exit polls that were commissioned by the major news agencies. When the polls showed Kerry way ahead in every battleground except Iowa, like many others, I wondered whether I should even bother to vote. Something similar happened in Florida in 2000, when thousands of Central time zone voters in the Florida Panhandle (largely Republican) walked away from the polls because CNN had already called the state for Gore, based upon (much more Democratic) votes from the closed polls in the Eastern time zone.



My quandary shows how authority and science are used to influence the political process. 



We expect scientists (and pollsters can be considered scientists because they’re applied mathematicians) to be both correct and savvy. So, when I read that the morning sample was based upon a sex ratio of roughly 60:40 (female/​male), I automatically figured that whomever did the polling simply adjusted the results to the expected sex ratio of the electorate, multiplying each male vote by approximately 1.1 and each female one by 0.9 (assuming a 50–50 electorate). In other words, I trusted the “experts” to know what they were doing, and to report it to the public in a clear manner. Most other people do the same thing. For instance, when they hear about some forecast of dire climatic changes made at some prestigious university or federal laboratory, they assume that the people who put the thing together are so smart that they would have compensated for any systematic problems with their methods. 



Here’s an example from global warming: As a matter of convention, most of our computer models for our climatic future assume that carbon dioxide — the main global warming gas — is increasing at a rate of 1 percent per year. The concentration of carbon dioxide in today’s atmosphere is roughly 375 parts per million (ppm). An increase of 1 percent in a computer model for next year’s climate would raise that concentration to 378.8 ppm, and to 382.5 the following year. 



But in reality, that’s not what’s happening. In the last three decades, the percent change per year has averaged 0.39, 0.41, and 0.51 per cent, respectively. What the computer does is more than double the rate of increase that is actually occurring. 



The amount of warming produced by those models is directly proportional to the rate of carbon dioxide increase. In other words, the models are compelled to calculate twice as much carbon dioxide‐​related warming as could possibly occur in coming decades. 



Back during the Clinton presidency, climate scientists produced a so‐​called “national assessment” of the effects of climate warming for the 21st century. They used models that did exactly what I describe above, and the report showed dramatic (and false) results. The Bush administration has since used the false material from the earlier report for its own study of climate change. The Bush document, in turn, served as the basis for climate change legislation by John McCain (R‐​Ariz.) limiting our net emissions of carbon dioxide. This can’t be accomplished without actively discouraging energy consumption, i.e., dramatically raising the price of gas. 



McCain is a consummate political animal, positioning himself for a 2008 presidential run. He authored that legislation for one simple reason: He sees political advantage in claiming to care about global warming. After all, most of his Republican competitors are going to be on the other side, against regulation. 



Politically, it is profoundly easy to demagogue any climate anomaly into global warming. Remember September’s hurricanes? A coalition of scientists — “Scientists and Engineers for Change” — exploited those disasters by plastering central Florida with billboards claiming that re‐​electing President Bush would make hurricanes worse because he’s not doing enough about global warming. Their scientific basis? The same computer models used by McCain, with the wrong increase in carbon dioxide. 



In this case, they failed. They were not able to persuade enough voters to eke Florida into the Kerry column. And, for that matter, neither did the exit polls produce a large enough effect to turn the nation. 



Matt Drudge is a sharp guy. So are the people at Slate Magazine, who kept the erroneous exit poll numbers up all afternoon. (Drudge took them down for a while.) The poll results were leaked with the full knowledge of their political effect — just as scientists know that a computer model that must overestimate global warming will also stir things up. 
"
"The critical links between water, sanitation, and our global consumption of energy – the “energy-water nexus” are more obvious than ever before. But how many of us will take direct action at the most basic level of all? It turns out that the way we use the toilet has a profound impact not only on our water resources, but is implicated in global energy security and perhaps the future of industrial agriculture as we know it. Flushing a standard WC accounts for around 30% of daily domestic water consumption in developed countries. This water must then be decontaminated before it is released back to the environment – an increasingly vital process of recycling as water stress grows globally. Urine is usually nearly sterile and high in nitrogen, phosphorus and potassium – the triad of nutrients known as “NPK” on which intensive farming is based. Faeces are rich in organic matter and carry lots of diseases. Both are recyclable, but the modern practice of mixing them together in the waste stream necessitates expensive, energy-consuming tertiary treatment processes.  One problem is the high nutrient levels from the urine, which can lead to serious ecological impacts on receiving waters if not removed. Although some wastewater treatment plants generate energy from anaerobic digestion of the sewage itself to offset these costs, there is a strong argument for reducing or avoiding the need for the extra energy altogether. So separating the waste at source has substantial merit. The pathogen-free urine can be reused more or less directly as a high-quality, if potentially rather smelly, fertiliser. Without the urine, treatment of the solid waste is easier, less expensive and less energy-hungry and the reduced frequency of flushing lowers stress on sewer infrastructure as well as our demand for water at source. Another key factor is phosphorus. As the “P” in NPK fertiliser, phosphorus supports at least 30-50% of global agricultural output but it is a finite and increasingly vulnerable mineral resource. Once mined and applied to agricultural land, what doesn’t leach into watercourses is harvested and transported around the world to our supermarkets and restaurants. A significant fraction goes to landfill as food waste, while the rest goes, via our digestive tracts, straight to sewage. Recent estimates indicate that primary stocks of P will last at least a century. But the growing demand for food, the vulnerability of mineral commodities to market forces, and the geopolitical implications of reserves concentrated in places such as Morocco and China mean that “closing the loop” on our phosphorus use is critical to sustaining and securing food supplies into the next few decades.  Simplifying wastewater treatment by separating and capturing it before it enters the sewer will play an important role in making this possible. So why aren’t we all doing it already? The devil is in the detail, as studies by major European institutes, and increasingly public responses, have shown. Initial opinion towards “urine-diversion” (UD) toilets – which contain two pans, one for liquids, one for solids – is often strongly positive, but this changes as users have to live with the devices.  In numerous settings where UD toilets have been installed in large-scale developments and public buildings, experiences are negative. Children, used to a single pan, aren’t sure “where to go”. Men in many countries don’t like sitting down to pee, and women can find it difficult to aim.  All of this compromises the entire premise of the device, increases cleaning and maintenance costs, but most importantly makes people less comfortable about going to the loo.  UD toilets are common in small-scale eco-houses, often in combination with no-flush composting systems and complete independence from mains sewerage. But the users in these cases have already modified their lifestyles to accept what many others see as undesirable “hardships”. Recent reports describe an entire eco-development in China retrofitted with standard toilets after large-scale failure of a Swedish-designed UD-composting system. The single-pan, flushing WC ranks as one of the greatest inventions of all time, just above the internal combustion engine according to a 2010 poll. It has turned the most private activity of billions from a noisome necessity to what many regard as a sanitary, peaceful few moments of calm. Like the car, the WC has shaped our modern collective psyche. But the wider negative consequences may prove to be just as profound.  Separating number ones and number twos makes sense in a sustainable vision of the future, but it may need a change of domestic design psychology to take us there."
"

The fat police have tried to frighten us for so long they’ve used up most of their stock of scary images. Yet the media still run with every ‘The Fat End is Nigh’ story, no matter how absurd. 



Exhibit A is today’s World Cancer Research Fund (WCRF) report, _Policy and Action for Cancer Prevention — Food, Nutrition, Physical Activity, and the Prevention of Cancer: A Global Perspective_ , which warns of a global catastrophe from obesity‐​induced cancer. 



As the _Observer_ ’s David Smith breathlessly previewed the WCRF report last weekend, ‘Cancer cases are now rising at such a rate in Britain and the rest of the world that the disease poses a threat to humanity comparable to climate change’. Not to be outdone, the _Mail on Sunday_ ’s David Derbyshire wrote that the ‘obesity epidemic will double the number of cancer deaths within 40 years… At least 13,000 cases of cancer are caused by obesity in Britain each year’. 



The new WCRF report is largely based on a report published two years ago by the same group that claimed that a third of cancers were caused by diet and lack of exercise. That controversial report advised people to be as thin as possible, and to avoid red and preserved meat and alcohol. The problem with this latest effort from the WCRF is that it is as blatantly and foolishly wrong as its 2007 version. This is especially evident in four areas. 



**Are cancer and obesity linked?**



The report’s headline‐​grabbing claim about the link between obesity and cancer is not supported by the majority of the WCRF’s own data from the 2007 study, or by other, more recent British and American studies. Once you dig beyond the bold claim, you find that what is being claimed is that overweight and obesity can increase your risk for six cancers (cancers of the oesophagus, pancreas, colon‐​rectum, breast, endometrium and kidney). But even this more limited claim has little scientific support. 



Take pancreatic cancer, for example. The 2007 report cites 20 case control studies, but only three show a statistically significant association between obesity and pancreatic cancer. Similarly, of 42 cohort studies on colorectal cancer, only 13 show a link with obesity. Again, with breast cancer and obesity, of 16 studies only three are statistically significant, while eight show a decreased risk between breast cancer and obesity. 



Even for oesophageal cancer, the increased risk was generally found in the morbidly obese (ie, those with a body mass index, or BMI, of at least 40 — a very small percentage of the population). And with endometrial and kidney cancers the relative risks were below two. According to the National Cancer Institute, relative risks below two (that is, two times the risk compared to a control group) are so small that they may be due to ‘chance, statistical bias or the effects of confounding factors’. 



These claims about a link between obesity and cancer are further called into question by the UK’s recent Million Women Study, which examined the link between 17 of the most common cancers and BMI. In that study, the incidence of 10 of the cancers does not show a statistically significant association with either higher levels of overweight or obesity. Of the remaining seven cancers, the association between overweight and cancer is non‐​significant in four, and where the results are significant the relative risks (except for endometrial and oesophageal cancer) are never stronger than two, except in the obese. 



The supposed link between cancer and overweight and obesity is again called into question by a study from the US National Cancer Institute and the Centers for Disease Control published in 2007 in the _Journal of the American Medical_ Association. This study found that being overweight was not associated with increase mortality from cancers considered obesity‐​related, and further noted that ‘little or no association with excess all‐​cancer mortality with any of the BMI categories’. In other words, the overall risk of dying from cancer was not related to body weight. 



Indeed, the study suggested that being overweight might be protective against cancer. For example, in individuals aged 25–59, obesity appeared to be protective against death from cancer. Even for those individuals aged 70 and over, BMIs in excess of 35 were not significantly linked with a higher risk of dying from cancer. 



**Will ‘five a day’ keep cancer away?**



The WCRF report claims that there is a link between a certain kind of diet, obesity and cancer, and that fruits and vegetables, which fat people tend to eat less of, can protect against cancer. Once again, the scientific evidence contradicts such claims. 



For example, of the 17 cancers discussed in the report, virtually all have statistically non‐​significant associations with every type of food, which means that they provide no evidence of a link between a particular food and a particular cancer. For example, of the 17 studies cited that looked at a link between colon cancer and processed meat, 13 are not statistically significant. 



Despite the advice to avoid red meat, the report itself concludes that ‘there is limited evidence…suggesting that red meat is a cause of oesophageal cancer’. Or, again, ‘there is limited, inconsistent evidence…that grilled or barbecued animal foods are causes of stomach cancer’. 



If the evidence is so limited and inconsistent, how can the advice to entire populations to reduce red meat consumption or avoid it entirely be so dogmatic? 



What of the extraordinary claim that, since fat people tend to eat less fruit and vegetables, they are more likely to get certain cancers? This claim is contradicted by the largest and most expensive randomised controlled studies of the effect of eating certain foods and weight on the risk of getting breast cancer, colon cancer, heart disease, and stroke, that is, the Women’s Health Initiative Dietary Modification Trial. 



Almost 49,000 American women were followed over an eight‐​year period in terms of eating, weight, and disease. The women in the intervention group ate ‘healthy’ diets that were low fat and high fibre. The results? There were no statistically significant differences between the intervention and the control group in the incidence of breast cancer, colon cancer, strokes, or heart attacks. 



In fact, the women following the healthy diet didn’t even weigh less than they did at the beginning of the study, or less than the group that continued to eat as they always had. So much for the claim that there is a link between eating certain foods and avoiding cancer. 



**Are fat children more likely to get cancer in later life?**



The WCRF’s Martin Wiseman told the _Mail on Sunday_ that ‘the increase in the number of overweight children is deeply troubling because the more overweight a child is, the more likely they are to be overweight as an adult. And the more overweight the population becomes, the more cases of cancer we are storing up for the future.’ 



But if Wiseman had simply looked at the official figures on childhood overweight and obesity in the most recent Health Survey for England, he would have found that since the previous survey in 2006, there was a decrease in obese girls aged 2–15, from 18 per cent to 15 per cent. Among boys aged 2–10, the prevalence of overweight declined from 16 per cent to 12 per cent. Indeed, according to the Health Survey, amongst boys and girls aged between 2 and 15, overweight and obesity has been declining since 2004. 



Nor is Wiseman’s claim about fat kids becoming overweight adults true. Only morbidly obese children — that is, a very small minority of all children — are at risk for adult obesity. As Charlotte Wright in her Thousand Families Study in Newcastle found, there is ‘little tracking from childhood overweight to adulthood obesity’. 



**Does exercise protect against cancer?**



The final problem with the WCRF report is its assumption that there is a scientifically established link between physical activity, obesity, and cancer prevention. To put it charitably, this assumption is open to significant question. 



Most of the summaries of the relevant scientific literature on this supposed connection are either non‐​committal or highly skeptical, despite the common sense claim that exercise is good for one. As one reviewer wrote, ‘It is important to emphasis at the outset that most of what can be written on this topic remains speculative. No study exists which has recorded adequate birth‐​to‐​death information relating physical activity to health’. 



Moreover, the evidence of a specific cancer‐​physical activity link is difficult to establish. Commenting on the supposed association between breast cancer and physical activity, Rissanen and Fogelholm wrote that due to the lack of evidence one could not make a public health recommendation for women to exercise to reduce their risk of breast cancer. 



A recently published meta‐​analysis on 52 studies that looked at the association between colon cancer and physical activity reported that 44 were not statistically significant. So the supposed, obvious link between physical activity and preventing cancer dissolves upon closer examination. 



Once you get beyond the scary headlines, the WCRF’s claims about diet, weight, and cancer turn out to be, at best, dubious and, at worst, simply untrue. 
"
"

On Friday, Aug. 9, the Federal Register posted an announcement calling for public comments on the use of the “social cost of carbon” in DOE rulemaking. The members of the House of Representatives have already presented their opinions on social cost of carbon by passing a bill just prior to recess prohibiting its use by the EPA without consent of Congress. It is unclear whether the Senate will take up the issue, although the prohibition would almost certainly face a presidential veto. But without good cause.



The social cost of carbon is a poor concept from the start. It is an ill‐​conceived, one‐​sided supposed measure of the damages associated with climate change resulting from human emissions of carbon‐​containing greenhouse gases (such as carbon dioxide and methane). Or, rather, it is a measure of the damages predicted to occur by a collection of computer models — computer models which themselves largely fail at capturing the climate evolution during recent decades.



Under normal circumstances, little attention would be paid to the esoteric squabbling of economists arguing about how to place a largely theoretical value on a measure which is imprecise and ever‐​changing by its very nature. However, the social cost of carbon has been elevated to the limelight by the Obama administration which has introduced it into the cost‐​benefit analysis that must be performed for new rules and regulations.





But in its haste to find a way to regulate greenhouse gas emissions, the administration has turned its back on both standing federal guidelines as well as sound science.



The social cost of carbon — or its converse, the alleged benefits conferred by reducing carbon dioxide emissions — has become one the administration’s favorite tools for counteracting the high costs associated with an ever‐​growing string of actual and proposed new rules governing everything from microwave oven efficiency to coal‐​killing power plant emissions standards.



The administration is so empowered by the social cost of carbon, that, realizing still untapped potential, it recently upped its initial estimates of the social cost of carbon by about 50 percent. By assigning a central damage estimate (cost) of $35 for each ton of emitted carbon dioxide rather than $21 per ton, more and costlier regulations can be neutralized by the purported benefits of greenhouse gas reductions.



But in its haste to find a way to regulate greenhouse gas emissions, the administration has turned its back on both standing federal guidelines as well as sound science.



For example, the administration dismisses federal guidelines which require an analysis of the cost of regulations from a domestic perspective. Rather than focusing only on costs expected to occur in the U.S., the administration determines the social cost of carbon from a consideration of perceived global impacts. Since the U.S. is much better positioned to respond to and adapt to climate changes than many other countries, the domestic costs are only a fraction of the total global costs. So what the administration is essentially doing is claiming ill‐​defined foreign benefits to justify the costs of U.S. regulations.



More egregiously, the administration turns its back on science. There is growing realization among climate scientists that the projections of climate change resulting from human greenhouse gas emissions have been overestimated. This realization stems from evidence published in the peer‐​reviewed scientific literature over the course of the past several years suggesting that the warming potential from greenhouse gas emissions is 40 percent lower than that which is currently encapsulated in climate models. Even while admitting that the climate sensitivity to greenhouse gas emissions is a key parameter in its calculations, the administration ignores these new findings and instead increased its estimate of the social cost of carbon in the face of the best science which demands that they should have decreased it.



The social cost of carbon is a concept which is easily gamed to fit the desires of the user — a characteristic emphasized in a recent paper by M.I.T. economist Robert Pindyck where he wrote that the models used to determine the SCC “suggests a level of knowledge and precision that is nonexistent, and allows the modeler to obtain almost any desired result because key inputs can be chosen arbitrarily.”



In this case, the user, the Obama administration, desires to limit greenhouse gas emissions in an attempt to mitigate climate change (an endeavor in which it will ultimately fail as the future course of climate change lies not with the U.S., but with the large, developing nations of the world). Unsurprisingly, the social cost of carbon was determined to be high and has gotten even higher just in time for the new round of regulations and executive actions making up the president’s recently announced Climate Action Plan.



Unbeknownst to most of us, the social cost of carbon is a playing an increasing role in our personal lives as our government uses it to justify making things more expensive — from cars to electricity. To do so, it lays science and best practices by the wayside.
"
nan
nan
"**The first public Covid screening centre at an airport in Scotland has opened at Edinburgh Airport.**
Passengers and staff at the airport, as well as members of the public, will be able to pay for PCR swab tests before receiving their results the next day.
Airport managers said the move could help the aviation industry ""drive Scotland's recovery"".
But they urged people to continue to follow their local coronavirus restrictions.
The tests are being carried out at the ExpressTest site in the FastPark car park area, in front of the terminal.
Airport chief executive Gordon Dewar said: ""Protecting and mitigating risk to public health and providing reassurance and confidence to people who need and want to travel is incredibly important if aviation and all of the industries that rely on it are to recover.
""We have acted with ExpressTest to ensure we are in as strong a position as possible to allow aviation as a facilitator industry to drive Scotland's recovery.
""Until then, people must continue to adhere to local regulations and ensure they understand and follow government guidance to protect themselves and others.""
Test results will typically be emailed or sent by text the next day, with airline passengers advised to schedule a test between two and five days before their departure as a precaution.
Anyone receiving a negative result will be emailed a Fit to Fly certificate authorised by a doctor.
But passengers are advised before booking a test at the airport to check if their travel provider will accept the document.
ExpressTest founder Nick Markham described the development as ""hugely exciting"".
He added: ""The ambition is for us to have 30 locations up and running across the UK in the next few months, and this landmark facility in Edinburgh is just the start.""
Passengers and staff at the airport will be charged a subsidised rate of Â£80 and Â£60 respectively to use the service, which will also be available to the general public for Â£99.
When asked about airport testing during her daily media briefing the first minister said the Scottish government was working with the airports in a bid to get to a position where testing is, if not a complete alternative to quarantine, then a ""partial alternative"".
Nicola Sturgeon said the UK government has already announced its plans to introduce a different system from 15 December but she is continuing to seek clarity on how it would work.
Ms Sturgeon added: ""We have some concerns about the accuracy and the reliability of private testing, that's not to say we rule that out, but we are continuing to take a very careful way through coming to a considered decision here.
""I know how important this is for the airports, but it's also important that we get these decisions right so that we're not increasing the risk that travel is posing to our efforts to control the virus."""
"

Much has been made of a paper published on Jan. 8 in the journal _Nature_ by Chris Thomas and 18 co‐​authors, claiming that global warming will cause a massive extinction of the earth’s biota. Thomas told _The Washington Post_ “we’re talking about 1.25 million species. It’s a massive number.”



It turns out that there are a massive number of glaring problems with their study that clearly eluded the peer review process. This is evinced by the rapid turnaround for the manuscript, with acceptance in final form a mere five weeks after original submission. No one can clear revisions through 19 authors in that time unless there weren’t many revisions suggested, or, if there were, they were ignored by the journal’s editors in a rush to publication. 



In fact, acrimonious debates about what should or should not be published about global warming are the rule rather than the exception, simply because papers are being published — on many sides of the issue — that can be shredded after only a cursory review. Unfortunately, the debate may have started with _Nature_ itself. In 1996, conveniently a day before the U.N. conference that gave birth to the Kyoto Protocol, _Nature_ published a paper purporting to match observed temperature with computer models of disastrous warming. It used weather balloon data from 1963 through 1987. The actual record, however, extended (then) from 1958 through 1995, and, when all the data were used, the troubling numbers disappeared. Since that famous incident, people have been very leery of what major scientific journals publish on global warming. The Thomas extinction paper only throws more fuel on an already roaring inferno.



Thomas et al.‘s work is an interesting exercise in computer modeling that shows again that what comes out of a computer is a product of the assumptions that go in. The scientists examined the distribution of more than 1,000 plants and animal species, calculated their current climatic range, and then used a climate model to determine whether the amount of land the species could occupy in the future would shrink or expand. If there was a likely shrinkage, the researchers expected an increased chance of extinction. 



Fair enough. But this assumes that climate change is the sole driver of changes in biodiversity, which is hardly true. Consider the effects on an ecosystem of the mutation of some previously harmless bacterium, a clearly non‐​climatic cause of extinction. The plethora of factors that influence ecosystems, besides climate, determine the composition of the community. In fact, placing all the onus for extinction on climate also calls the entire dramatic result into question.



Their lowest scenario for warming is bounded at 0.8°C in the next 50 years, and produces an extinction of roughly 20 percent of the sampled species. 



There’s a convenient reality check available. That’s because surface temperatures indeed _have_ risen this amount in the last 100 years. But there is absolutely NO evidence for massive climate‐​related extinctions. (One would think the reviewers at Nature would have picked that up!) 



There are several other major problems: 









Obviously, there is a lot to criticize in this paper. What is surprising is that something with so many inconsistencies and unrealistic assumptions made it unscathed through the review process in such a prestigious journal as _Nature._ The politicization of scientific papers on global warming and the tendency of science journals to rush to judgment have to end. 
"
"Across the open heather moors of upland Britain, last-minute preparations are being put in place for the start of the red grouse shooting season on August 12. On average about 200,000 grouse are shot every year in England and Wales. Yet the management that makes such large numbers of grouse available for the guns in autumn is becoming increasingly contentious. The reason is that there is a growing and convincing body of evidence that suggests that birds of prey, or raptors, are being illegally killed by those who manage grouse stocks. These accusations of illegal killings of birds such as harriers, falcons and eagles has given rise to a clash between those on both sides of the debate. While the main conservation and game shooting organisations claim to be keen to see an end to this illegal activity, they favour different approaches. The conservationists tend to back the strengthening and enforcement of policy. For example, the RSPB demands that grouse shooting be licensed, so that a licence may be revoked should illegal activity be detected. Mark Avery, the former head of conservation science at the RSPB, has gone further and demanded that grouse shooting be banned. There is a day of protest set for August 10, and Marks & Spencer recently found themselves in the protesters’ sights for their plans to sell grouse in their stores. In contrast, supporters of red grouse shooting claim that the land management associated with it has benefited a range of species, as well as bringing income and jobs into remote rural areas. They wish to see some form of legal management of birds of prey. Feelings run high; many question why it seems to be a taboo to discuss the management of raptors and why the desire to increase the number of these birds should come before jobs and communities. The hen harrier finds itself at the epicentre of this conflict. The species has virtually disappeared as a breeding bird on intensively managed grouse moors – killed because of its efficient hunting of grouse. One of the features of the harrier is that it is not particularly territorial, so on some grouse moors it can breed at levels that lead to significant financial losses for those involved. So what is the best way of resolving this problem? A shooting ban would certainly lead to a change in how large swathes of our uplands are managed. However, the costs and benefits of this change for the communities, predators and other species would depend on what it is replaced with. Such an approach would also inevitably anger many of those involved in land management and possibly damage the long-term relationships between hunters and conservation organisations.  Could the management of harriers provide a solution? A forthcoming population modelling study due to be published in the Journal of Applied Ecology explored one approach. The study highlighted the densities at which harriers could co-exist with driven grouse shooting through a quota or brood management scheme.  The idea is simple: once numbers go beyond some agreed level the chicks would be moved into captivity until ready to fly, when they would be released to rejoin the wild population. It may sound an unusual way of managing a wild species, but there is a precedent for it. In continental Europe, harriers breed in agricultural crops and are often killed if they aren’t flying before the combines harvest the crop. Instead, harrier chicks are taken into captivity before harvesting and subsequently released. This approach could allow the co-existence of harriers and grouse without recourse to illegal killing, but it would need to be tested in the field.  The problem is that the conservation movement are nervous because of the precedent this approach sets for active management of birds of prey – even if it ultimately led to more harriers. They argue that any discussion around techniques that would affect birds of prey should only take place once harriers are allowed to breed freely on grouse moors. Unsurprisingly, grouse managers will only trial such a scheme if there is an agreed quota, and an exit strategy that they can work towards.   We are at a crossroads, but it is not yet clear which way we are going to go: enforcement or management? While science has provided evidence, in the end the decision will – as is so often the way – depend as much on political, moral, social and economic arguments as it does on science."
"The government is set to introduce E10 fuel containing 10% ethanol as a new form of “cleaner” petrol aimed at cutting carbon dioxide emissions. Grant Shapps, the transport secretary said the government was consulting on plans to make it the standard grade at British filling stations from 2021.  The new petrol, he said, had the potential to reduce CO2 emissions by about 750,000 tonnes per year or the equivalent of 350,000 fewer cars on the road. Petrol grades in the UK currently contain up to 5% bioethanol, known as E5. The E10 blend is already used in countries including Germany, France, Belgium and Finland. Shapps said the proposed new fuel was a steps towards “a net zero future”. “Before electric cars become the norm we want to take advantage of reduced CO2 emissions today,” Shapps said."
"
Share this...FacebookTwitterWhat follows are just a couple of examples from James A. Marusek’s A Chronological Listing of Early Weather Events (may take a few minutes to load).
This is an oustanding work that the warmists’s will not want you to see. (Hat tip reader Ron de Haan).
585 A.D. “Western Europe was so rainy, that it could be confused with
winter. The bulk of the rains this year caused rivers to overflow their banks and flood the fields and meadows. These floods seriously compromised the crop yields.”
994 A.D. “A destructive storm struck London, England, blowing down fifteen hundred buildings and killing several hundred persons. The summers in the years 994 and 995 in Europe produced very high temperatures and a very persistent heat wave. Historians reported that the drought was so terrible that the fish died in the ponds, the trees caught fire, and the fruit and the flax harvest were destroyed. In 995 the greater part of Europe’s rivers were so shallow that you could wade through them. In 994 in Western Europe, the dearth of rain caused the rivers to dry up. It killed the fish in most lakes. It dried up thousands of trees and burned grassland and crops.”
1186 A.D. “In Germany the winter was warmer than had known for a long time. The vegetation was very advanced. The harvest took place in May and the grape harvest in August. In France, the trees were blooming in the middle of winter.”
Recall how alarmists would love have us believe that before man began emitting CO2 the weather was tame and friendly to all of the earth’s inhabitants, and that terrible storms and extremes began only after man embarked on industrialization and the burning of fossil fuels.
Well breadandbutter.com reminds us that extreme weather extremes occurred just as often in the past. Indeed today’s attempt to have us believe that every occuring weather extreme is a sign of man-made climate change just shows how bankrupt the AGW science has become.   
http://www.breadandbutterscience.com/Weather.pdf.
Share this...FacebookTwitter "
"
  From the BBC: ‘Iceberg the size of Luxembourg threat‘ – click image for video and watch the collision of two giant ice masses. Of course 50 years ago, such things would likely go unnoticed without satellite imagery.

They write:
A vast iceberg that broke off eastern Antarctic earlier  this month could disrupt marine life in the region, scientists have  warned.
They say the iceberg, which is 78km long and up to 39km  wide, could make it harder for the area’s colonies of Emperor Penguins  to find food.
But British and Australian scientists disagree on  whether it could also cause major problems to our own weather patterns.
Well so far, nobody at the BCC is blaming the collision on Global Warming:
BBC tells the truth – shock horror! – iceberg not caused by global  warming
But I don’t think Joe Romm has weighed in on it yet. There’s still time. At least it’s not a bridge in Minnesota.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8dec0e71',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Businesses must improve how they disclose their impact on the environment or risk failing to meet climate targets, the Bank of England governor, Mark Carney, warned the City on Thursday. Without disclosure rules that allow investors to compare how businesses are meeting the climate challenge, the world risks missing targets to be carbon neutral by 2050, Carney said.  Speaking at a climate conference at the Guildhall in London, Carney said he wanted to pull together a set of rules ahead of November’s Cop26 climate summit in Glasgow to spur a switch of investment funds away from polluting industries in time to prevent a global temperature rise of about 1.5C. “Given the scale of the climate challenge and the rising expectations of our citizens, 2020 must be a year of climate action where everybody’s in, and that includes the world’s leading financial centre,” Carney said. “To identify the largest opportunities and to manage the associated risks, disclosures of climate risk must become comprehensive, climate risk management must be transformed, and investing for a net-zero world must go mainstream.” He was joined by Christine Lagarde, the president of the European Central Bank, who said inconsistent reporting by the 26 biggest eurozone banks and insurers meant “we still have some way to go”. Lagarde, who until last year ran the International Monetary Fund, said: “Only five out of the 26 partially disclose the impact of their financial assets, and none of them provide full disclosure.”  Last year, Carney said firms that failed to move towards zero-carbon emissions would be punished by investors and go bankrupt. Carney is due to take over as the UN special envoy on climate finance next month after stepping down as governor. He has also been appointed by Boris Johnson to advise the UK government in the run-up to the Glasgow conference. The G20 group of nations sponsored the development in 2015 of a new rulebook for companies to declare their climate impact. Carney was a sponsor of the project as head of the G20 financial stability board. He said the framework developed by the Task Force on Climate-related Financial Disclosures (TCFD) was supported by more than than 1,000 companies from 54 countries, with a market value of $16.7tn (£12.8tn). “The objective is that every professional financial decision will need to take climate change into account,” he said. The UK government is under pressure to commit to the TCFD rules and force all UK companies to disclose the impact of their activities on the environment. The EU has already said it is prepared to impose rules within the next few years if an agreement at the G20 group of countries, which sponsored the TCFD, cannot be reached. It is likely the EU and UK will need to go it alone following a reluctance of Donald Trump’s US administration to join global efforts to limit global warming. Philipp Hildebrand, the vice chair of BlackRock, told the conference it would help investors play their part in tackling the climate emergency if governments forced companies to comply with newly drafted global disclosure rules. “It would make life easier,” he said responding to concerns that the adoption of rules was being held back by companies and countries reluctant to recognise the climate emergency. Hildebrand, the former head of the Swiss central bank, said: “Let’s face it, finance has had a horrible decade. The way we ignored leverage [ahead of the 2008 financial crisis] is the way we are ignoring climate change.” BlackRock, which manages more than $7tn of private investor funds, has offered its support for the TCFD, but has yet to show how this will affect its relationship with companies that it invests in. David Schwimmer, the head of the London Stock Exchange, said companies needed to conform with the TCFD rules before they were forced to by governments. “We think mandatory disclosure is the direction of travel,” he said. The newly-appointed business secretary, Alok Sharma, whom Johnson named as president of the summit this month after sacking a former energy minister from the role, echoed Carney’s call for the City, London’s financial centre, to embrace disclosure rules. “We are calling on action from everyone – businesses, civil society and each part of the global financial system – to meet the Paris agreement goals,” Sharma said. • Sign up to the daily Business Today email here or follow Guardian Business on Twitter at @BusinessDesk."
"
Share this...FacebookTwitterLook how cozy Revkin is in bed with alarmists like Rahmstorf. And they really detest skeptics, don’t they? Rahmstorf writes to Andy:
Hi Andy,
from over here, it is hard to see this kind of Inhofe speach as anything else than an irrelevant piece of absurd theatre. It doesn’t even bother me any more – he’s simply lost it.
Cheers, Stefan”
And Andy replies to Steffi (emphasis added):
I know. but he still speaks to and for a big chunk of America — people whose understanding of science and engagement with such issues is so slight that they happily sit in pre-conceived positions.”
Read it all here! Looks like I’m going to be up for awhile tonight!
Share this...FacebookTwitter "
"
Share this...FacebookTwitterAnd she used to be a big proponent of the IPCC! My how things are changing.
I’m glad to see that Donna Laframboise’s new book is taking off well. I’ve ordered a copy myself and look forward to reading it. Sounds like it’s a real IPCC slammer. First off, hats off to Donna for taking the time to do all that digging into an organisation which sorely needs transparency.
Judith Curry at Climate Etc here does a good job reviewing the book. She gave it 4.5 stars (professors rarely give out A+s), which means two solid thumbs up. I was particularly amused by her comment:
Does the problems with the IPCC mean that WG1 science is incorrect?  Not necessarily, but I agree that a “new trial” is needed.  WG2 and WG3 reports pretty much belong in the dustbin, as far as I can tell.”
There it is. Now how much money and resources went into producing WG2 and WG3? Paper in the dustbin is what we get for it?
Worse, the IPCC refuses to learn anything from all this. Recently we’ve heard they are setting up shadowy back rooms in order to skirt FOIA. I’m starting to wonder if it would be safe to turn on the light in a room full of these characters.
Donna appears to have set something into motion. The books are well-priced and I suggest readers buy extra copies and give them away as birthday or Christmas presents – or give them to your political representatives. Ask your pol if they can do better with our money.
ORDER INFO HERE: http://nofrakkingconsensus.com/2011/10/13/a-book-is-born/
PS: Donna Laframboise is the midwife of NoTricksZone. I was one of her “citizen auditors” which she mentions in her book and she invited me to WordPress. The rest of the story you know.
Share this...FacebookTwitter "
"Journalist and environmentalist George Monbiot recently wrote a powerful polemic against the concepts of ecosystem services and natural capital, arguing that they were leading us on a neoliberal “road to ruin”. In many cases nature is ignored or trumped by other economic or social priorities, or seen as a barrier to growth to be overcome. Ecosystem services and natural capital help re-frame nature as an asset to society that delivers many benefits. Monbiot’s attack, therefore, is in danger of throwing out this natural asset baby with the “dirty” neoliberal bathwater. The power of ecosystem services and natural capital concepts is that they break down and clarify what nature provides in economic terms. Value is revealed with respect to the benefits it provides to society. So, for example, city parks are more than just attractive green spaces. They improve air quality and help to minimise the heat island effect, they provide a natural health service for people to walk and relax. Their presence actually raises nearby house prices. If they are well designed they can also provide flood protection and improve biodiversity. All these factors contribute to the productive economy and, and so this provides, in theory, an economic incentive to protect them. Now, I am a planning academic who champions the use of ecosystem thinking to help improve the way nature is treated in decision-making. While I share concerns about the descriptive shortfall of sterile economic terminology such as “ecosystem services”, I believe the framework offers useful ways to address its current neglect. The 2011 UK National Ecosystem Assessment (NEA) was the first comprehensive assessment of the state and worth of Britain’s natural environment. It concluded that nature’s services were in significant decline across most habitats – in part because nature is consistently undervalued in decision-making. The NEA  provided a valuable evidence base from which to assess the impact of different future scenarios on key ecosystem services. Most controversially, it calculated the financial worth of the benefits that different habitats provide to society. For example, the  benefits that inland wetlands bring to water quality are worth up to £1.5 billion per year to the UK.  The main problem with economic values is that they fail to account for the intrinsic values of nature. Ecosystem services form only one of the 12 strategic principles of the ecosystem approach. This aims to integrate the management of land, water and living resources with the objectives of biodiversity conservation, sustainable use, and an equitable sharing of the benefits from natural resources. However, most people focus solely on the ecosystem services aspect, cherry picking those services that best fit their agendas, instead of using the 12 principles to inform policy decisions. As one of the principal investigators within the NEA follow-on project between 2012-2014, I sought to address this problem by translating ecosystem science and jargon into practical guidance. This included adapting assessment tools to better take into account the value of nature, directly challenging the view that nature is a barrier. The project highlights the need for decision-makers to fully assess a wider range of policy alternatives from the outset, and to target policies where environmental benefits are maximised.    The key lesson is to work with nature rather than dominate it. If we continue to ignore the value of nature and selectively value only the ecosystem services we want, we risk making poor decisions that will cost UK PLC more in the long term. Plans to engineer multi-million pound flood defence schemes as opposed to implementing much cheaper upland management of our moorlands and better land use management practices is a case in point. Thinking about nature helps us all benefit. There is some evidence of this thinking being incorporated into planning policy. For example the value of ecosystem services has been recognised in planning guidance (under the new National Planning Policy Framework). However, its potential is limited by the alienating language. Hence our work tried to address this by embedding the value of nature within more familiar concepts and tools used in day-to-day work. Planning represents the frontline where decisions are made about building and development that may affect the environment. Yet the level of environmental knowledge in decison-making is often inadequate. We need to find a common language to better communicate the many benefits nature provides us, but we must also avoid simplistic assessments that overlook and poorly account for nature.  The Treasury Green book, for example, uses a method of converting future costs and benefits to a present day equivalent to make them comparable (a discount rate). HM Treasury recommends applying a discount rate of 3.5% for periods of up to 30 years, compared to Germany’s Federal Environment Agency which recommends a discount rate of 1.5% – a significant under-representation of their value. Similarly, biodiversity offsetting schemes have been proposed as a means to account for the loss of ancient woodland, by planting new trees. But this fails to recognise the irreplaceable value of such resources and is little more than an environmental con.  We must no longer see nature as a “bolt-on”, or separate to the economies of cities, countries, and the planet. And we can no longer expect economies to thrive if the health of the environment is compromised. "
"**Up to three households will be able to meet up during a five-day Christmas period of 23 to 27 December, leaders of the four UK nations have agreed.**
People can mix in homes, places of worship and outdoor spaces, and travel restrictions will also be eased.
But a formed ""Christmas bubble"" must be ""exclusive"" and would not be able to visit pubs or restaurants together.
The leaders urged people to ""think carefully about what they do"" to keep the risk of increased transmission low.
They added 2020 ""cannot be a normal Christmas"" but family and friends will be able to see each other in a ""limited and cautious"" way.
However, some scientists have warned that the relaxation of Covid restrictions over the festive period could spark another wave of infections and further deaths.
The measures will see travel restrictions across the four nations, and between tiers and levels, lifted to allow people to visit families in other parts of the UK.
Anyone travelling to or from Northern Ireland may travel on the 22 and 28 December, but otherwise travel to and from bubbles should be done between the 23 and 27.
People will not be able to get together with others from more than two other households, and once a bubble is formed, it must not be changed or be extended further.
The guidance says a bubble of three households would be able to stay overnight at each other's home but would not be able to visit hospitality, theatres or retail settings.
However, existing local restrictions will still be in place mean many pubs and restaurants - such as those in England's tier three or Scotland's level four - will remain closed during the festive period.
The leaders of England, Scotland, Wales and Northern Ireland reached the agreement at a meeting on Tuesday afternoon.
In a joint statement, they said: ""Even where it is within the rules, meeting with friends and family over Christmas will be a personal judgement for individuals to take, mindful of the risks to themselves and others, particularly those who are vulnerable.
""Before deciding to come together over the festive period we urge the consideration of alternative approaches such as the use of technology or meeting outside.""
Published guidance for England gives further details of the rules for 23 to 27 December:
Scientists say a typical Christmas gathering at home is the type of environment where infections can spread.
The guidance also advises people to take precautions when meeting their Christmas bubble such as washing hands frequently and opening windows to clear potential virus particles.
In a video message from Downing Street, the prime minister described the agreement as a ""special, time-limited dispensation"", saying: ""This year means Christmas will be different.""
Boris Johnson said people must make a ""personal judgment"" about the risk of who they form a bubble with or if they visit elderly relatives., adding: ""Many of us are longing to spend time with family and friends... And yet we can't afford to throw caution to the wind.""
Wales' First Minister Mark Drakeford said it was ""not an instruction to travel, it's not an instruction to meet with other people. People should still use a sense of responsibility"".
Scotland's First Minister Nicola Sturgeon added: ""The virus is not going to be taking Christmas off, so although we want to give a little bit of flexibility for Christmas we are still urging people to be very cautious and to use this flexibility responsibly and only if you think it is necessary.""
Northern Ireland's First Minister Arlene Foster said she hoped people would have space to plan, adding: ""We of course recognise how important Christmas time is for so many people.""
Deputy First Minister Michelle O'Neill urged people to ""be responsible"", saying while they wanted to mark Christmas after such a ""desperate"" year the relaxations would increase opportunities for the virus to spread.
She added it was hoped that an alignment with rules in the Irish Republic could be achieved.
What to do about Christmas divides opinion.
Increased mixing indoors will certainly mean there is greater transmission of the virus.
But, as chief medical adviser Prof Chris Whitty said on Monday, there is a balance to be struck between the harm the virus can cause and the societal and economic impacts of trying to control it.
He called for a ""public-spirited approach"".
By that he means adhering to the restrictions in the lead-up to Christmas, being responsible with the opportunity the relaxation gives people, and then immediately switching back to compliance.
If that happens, any impact could be minimised - and, of course, it will be up to individuals to decide just how much they mix within the rules.
These are very fine judgement calls by ministers.
They hope Christmas will provide respite and help steel the public for what is clearly going to be a long, hard winter.
They also feel they have little choice, believing large numbers of people would ignore pleas not to mix - and this way they can provide advice on how to enjoy Christmas as safely as possible.
But there is also the risk by sanctioning it there will be more mixing than there would have otherwise been.
Transport Secretary Grant Shapps earlier said Christmas travellers should plan journeys carefully and prepare for restrictions on passenger numbers to allow for social distancing.
Meanwhile, the government has recorded another 608 UK deaths within 28 days of a positive Covid test. There have also been a further 11,299 cases of people testing positive for coronavirus.
Gyms and non-essential shops in all parts of England will be allowed to reopen from 2 December under a strengthened three-tiered system.
Areas will not find out which tier they are in until Thursday - and the decision will be based on a number of factors including case numbers, the reproduction rate - or R number - and pressure on local NHS services.
Prof Andrew Hayward, director of the UCL Institute of Epidemiology and Health Care, and a member of the government's Sage committee, told BBC Newsnight that allowing families to meet up over Christmas amounted to ""throwing fuel on the Covid fire"".
He said it would ""definitely lead to increase[d] transmission and likely lead to third wave of infections with hospitals being overrun, and more unnecessary deaths.""
Prof Hayward said while you cannot ban Christmas, he called for clearer messaging to families about the ""dangers"" of socialising and inter-generational mixing.
And Paul Hunter, professor of medicine at the University of East Anglia, suggested the relaxation of restrictions at Christmas will ""almost inevitably"" lead to an increase in transmission.
But he said: ""Providing that the new tier system is better managed than in October, any increase in cases could be relatively short-lived.
""After Christmas we will still have to live through a few more months of restrictions at least.""
Jillian Evans, the director of health intelligence at NHS Grampian, said the easing of restrictions over Christmas would cost lives.
""We've got winter weather, we know that people are more susceptible to infection over the colder period, and we've got a festive period where people will be socialising,"" she said.
""Those are facts, and I would rather be honest and tell you that those are the facts, and be truthful about it so people can understand the risks that they're taking.""
Kate Nicholls, chief executive of the UKHospitality lobby group, said there was ""muddled thinking"" over the Christmas rules and they would cause the sector more economic harm.
She said: ""Hospitality venues should be considered part of the solution for providing people a well-deserved safe and enjoyable Christmas, especially given that allowing multiple households to mix in the confines of private homes presents an exponentially greater risk."""
"Our future depends on a transition away from fossil fuels. To map out a path, we need to get to grips with how, and why, the use of coal, gas and oil has risen to unsustainable levels. Most fossil fuels are consumed not by individuals, but by and through large technological systems, such as electricity networks, urban transport systems, built environments, and industrial and agricultural systems. While the media offers plenty of advice on how individuals can cut consumption, how to transform or supersede these technological systems is much less obvious. These unsustainable systems are deeply embedded in day-to-day life. For example, fossil-fuel-driven power stations on average use roughly three units of energy to produce one unit as electricity, while further energy is lost in transmission networks. Steel and cement are also produced in energy-inefficient ways, and used to construct heat-hungry buildings. To engineers, these are all huge opportunities for energy conservation. Car-based urban transport systems could hardly be more fuel-inefficient. That is why Atlanta in the US, a spread-out city dominated by suburban housing and car transport (including many SUVs), has 11 times the greenhouse gas emissions per head of Barcelona, Spain, which has a similar number of people, with similar income levels, but is more compact, with better public transport and a relatively car-free centre. The best way to interpret the growth in fuel consumption is by starting with the evolution of these technological systems, and the way they are embedded in social and economic systems. As I argued in my recent book Burning Up: A Global History of Fossil Fuel Consumption, such an approach can help us through a dizzying array of statistics, which themselves reflect a range of political views of consumption. Here is a guide to the most often-used ones: Consumption-per-head statistics measure a country’s total energy use, and divide it by the number of people in the country. Developing nations use these figures at international climate talks, to underline the historic inequality of consumption. For example, in 2014 the US consumed 31 times more energy products per head than Bangladesh; three decades earlier in 1984, it was 71 times more.  But no US citizen consumes those quantities of energy products directly. His or her share is mostly swallowed by the technological systems. Even those car drivers in Atlanta do not control their own consumption: it’s difficult to live there without a car, except in hardship. These numbers also hide inequality within nations, such as between an extravagant SUV driver and an unemployed cyclist. The prominent economist Thomas Piketty and his colleague Lucas Chancel tried to correct for that anomaly. Using wealth statistics, they estimated individuals’ fuel use, from the super-rich to the poorest, and found even more eye-watering contrasts. But such approaches still do not account for the technological systems that consume most fuels. Consumption-based emissions statistics filter out the effect of one aspect of international inequality. They count greenhouse gases emitted in manufacture according to the countries where products are used, instead of where they are made. So emissions “embedded” in a steel bar, produced in a carbon dioxide-belching coal-fired furnace in China and exported to the US, are counted as American. These numbers underline that, even today, the vast bulk of fossil fuel use is in, or for, the global north. Attributing emissions to fuel-producing companies helps to highlight the role of corporations. The Climate Accountability Institute’s brilliant research shows that nearly two thirds of carbon dioxide emitted since the 1750s can be traced to the outputs of the 90 largest fossil fuel and cement producers. Some headlines proclaim that these corporations are therefore “responsible” for climate change. But that’s only half the story. They produce fuel, others consume it. A list of the companies that do so – electricity producers, metals and engineering consortia, car makers, construction companies, petrochemicals and agriculture giants – would be longer and more complex, because fossil fuel use is so integral to all types of economic activity. So we need sector-by-sector breakdowns, and the International Energy Agency (IEA) publishes those. Flow charts can help visualise things, and materials flow researchers, such as the authors of Sustainable Materials With Both Eyes Open, do those. Then the numbers need interpreting; the bulky Global Energy Assessment had a shot at that. Companies and governments may be hiding things of course. There are tell-tale signs in IEA statistics: more than three times the amount of fuel used for global aviation is used on “other energy industry own use and losses” – that is, fuel the energy companies have either lost, or lost track of. And IEA reports on energy efficiency, which rely on companies to detail improvements, are full of complaints that crucial information is withheld. Military use is largely hidden. The US Department of Defense was in the 2000s the world’s biggest single consumer of commercial energy, wolfing down more than Nigeria. And at least we have that information: many countries simply don’t report military fuel use. Tracing fossil fuel use is not simple. The focus needs to shift from individual consumption to the big technological systems by and through which most fossil fuels are used, and the social and economic factors that make them work the way they do. A harsh light needs to shine on companies that consume the fuels, as well as the producers. Click here to subscribe to our climate action newsletter. Climate change is inevitable. Our response to it isn’t."
"

Former Vice President Al Gore is apparently on the hunt for votes for his prospective presidential campaign. He criticized the Bush administration on just about every ground at a dinner hosted by the Congressional Black Caucus Saturday night. But his greatest moment of unintended hilarity came when he charged that Attorney General John Ashcroft “is not respectful of civil liberties.”



That’s actually true. And Gore was right when he argued: “I believe one of the test of our nation is whether in times of grave challenge, we have the courage to be true to our deepest principles.” 



But while these are legitimate sentiments‐​whether you agree with them or not — it’s a bit curious to see them come from someone in the Clinton‐​Gore administration. After all, William Jefferson & Company perfected the practice of jackboot liberalism. 



Start with Attorney General Janet Reno, apparently defeated in her quest for the Democratic nomination for governor of Florida. On her watch the federal government burned the children in order to save them in its assault on the Branch Davidians in Waco, Texas. 



The same Justice Department supported Draconian restrictions on abortion protesters, including prohibiting the display of any “images” that were “observable” from abortion clinics. In the same vein, the Defense Department attempted to gag military chaplains, preventing them from discussing the Catholic Church’s Life Postcard Campaign regarding the President’s veto of legislation banning partial‐​birth abortion. Clinton, Gore & friends politicized the FBI, using it to justify the White House Travel Office purge. Presidential aides snooped through FBI files on potential administration opponents. 



The IRS audited a suspiciously large number of conservative foundations and groups. Proof that this reflected a conscious campaign was scarce, but no liberal groups reported undergoing similar reviews. The White House pressured the Treasury Department over the latter’s probe of Madison Guaranty, which financed the Clintons’ dubious Whitewater investment. 



The Department of Housing and Urban Development used intimidated opponents of federally subsidized housing projects. HUD launched dozens of investigations against local activists and groups; subpoenaed copies of organization membership lists and financial information, people’s diaries, and other records; demanded cessation of public criticism; and threatened protestors with prosecution for speaking out.



Similarly, in 1995 the U.S. Commission on Civil Rights issued subpoenas to two leaders of anti‐​immigration groups. The commission, whose chairman and staff director were appointed by the Clinton‐​Gore administration, wanted computer printouts, internal documents, reports, and other information from the organizations which were, of course, engaged in First Amendment political activities. The commission retreated, but only under congressional pressure.



Intimidation has been an administration hallmark. In 1994 President Clinton expressed outrage that radio talk‐​show host Rush Limbaugh could get on the air and “have three hours to say whatever he wants. And I won’t have an opportunity to respond.” White House Communications Director Mark Gearan called for radio talk shows to put on opposition — meaning administration — guests. Senior adviser George Stephanopoulos suggested resurrecting the misnamed “Fairness Doctrine,” to be enforced by Clinton‐​Gore appointees on the Federal Communications Commission, to regulate political broadcasts. 



Then there was the Department of Energy’s press‐​rating system. Reporters were judged on their coverage; sources were rank‐​ordered based on their opinion of the department. Department press secretary Barbara Semedo explained that a low rating “meant we weren’t getting our message across, that we needed to work on this person a little.” But, of course, getting the message meant spouting the department’s line.



The Food and Drug Administration’s grab for control over the tobacco industry was amazing: The FDA sought to prohibit even the use of brand names on non‐​tobacco products (such as lighters and t‐​shirts) and the use of non‐​tobacco brand names on tobacco products. (Alas, state attorneys general and the trial bar later achieved the same end through extortionate litigation.) The administration supported labeling restrictions on the alcohol industry, unsuccessfully urging the Supreme Court to void the firms’ First Amendment rights. The Clinton‐​Gore administration also backed FCC Chairman Reed Hundt’s abortive campaign to bar the advertising of distilled spirits on television. The same administration supported the Communications Decency Act, which would have attempted to ban the transmission of “indecent” materials over the Internet. Though well intentioned, the law, voided by the Supreme Court, inevitably meant heavy‐​handed federal censorship of the most free communication medium today. 



Although President Clinton spoke of reforming affirmative action, his administration promoted it instead. Perhaps the ugliest episode was his Justice Department’s support for the Piscataway, New Jersey school district that fired a teacher because she was white. Justice eventually flip‐ flopped in the case, but left its support for the government’s vast system of racial spoils otherwise undisturbed. The Education Department responded to California’s passage of Proposition 209 by threatening to prosecute the university system. 



Within the Clinton‐​Gore administration “diversity” became a code word for preferential treatment of politically advantaged groups. HUD required that employees not only implement federal diversity policy, but demonstrate “interest” and “personal commitment” to diversity, be active in “minority, feminist or other cultural organizations,” and participate in “cultural diversity activities outside of HUD.” The Department of Agriculture reassigned an employee for criticizing, on his own time, the department’s policy of offering spousal benefits to same‐​sex partners. 



There were also haphazard bureaucratic witch‐​hunts. The State Department fired Timothy Hunter, a retired Army counterintelligence officer who served in a number of government agencies before joining the State Department in 1990, for raising questions about agency administrative practices, discriminatory hiring and firing policies, make‐​work foreign‐​service jobs, and the State Department’s failure to defend the religious freedom of Americans working in Saudi Arabia.



But the harshest examples of jackboot liberalism have come from the Justice Department and federal law enforcement agencies. The Branch Davidian case continues to stand as an example of government run amok, persecuting people who wanted little more than to be left alone. Yet the Clinton‐​Gore administration steadfastly resisted attempts to hold anyone accountable in either Waco or Ruby Ridge, Idaho, where federal agents earlier killed the wife, son, and dog of loner Randy Weaver in order to arrest him in a case verging on entrapment.



The administration did, however, use the Oklahoma City bombing as an excuse to propose sweeping new federal powers — such as restricting the right of habeas corpus and expanding use of wiretaps — even though proponents were unable to point to a single example where civil‐​liberties protections prevented the police from deterring terrorism. Several of its proposals were turned into law. 



Clinton, Gore & Company, who constituted the most wiretap‐​friendly administration in U.S. history, essentially sought to eliminate the requirement of a warrant for searches from the Fourth Amendment. The president claimed to possess “inherent authority to conduct warrant‐​less searches for foreign intelligence purposes.” The administration required public‐​housing residents to sign away their constitutional right that authorities procure a warrant to search their dwellings and personal property. The Justice Department backed warrant‐​less (indeed, suspicion‐​less) drug tests for high‐​school athletes. The administration requested greater FBI authority to conduct “roving wiretaps,” without a court order. In the same way, Clinton‐​Gore officials pushed the Communications Assistance Act, which required telephone companies to retrofit their systems to ease police surveillance, supported restrictions on the sale of Internet encryption technology, and requested legislation forcing firms to give the government the “keys” to such technology. 



The administration was tougher than its predecessor on drugs. Marijuana arrests were up 50 percent over Bush‐​41 years and the Clinton‐​Gore administration consistently sought to frustrate state voters who approved measures to allow the desperately ill — victims of AIDS and cancer, in particular — from using marijuana to ease their nausea and pain. Administration appointees even threatened to prosecute any physician who provided a prescription for medical use of marijuana as allowed by state law. When asked about the criticism that sellers of crack were being punished far more severely than those who peddled cocaine, the president responded that penalties for the latter — which already ensured that minor drug dealers spend more time in jail than do many armed robbers, rapists, and murderers — should be raised. (As was his wont for shifting with the political winds, he later proposed moving modestly in the other direction, cutting the disparity from 100 to ten‐​to‐​one.) No more squishy, compassionate liberalism. The Clinton‐​Gore administration was enthusiastic about throwing people in prison. 



The administration also jailed people for resisting federal designation of their (very dry) property as “wetlands,” and committing other environmental offenses. In 1994 the Justice Department relaxed its control of environmental prosecutions in order to allow individual U.S. attorneys great latitude in prosecuting business. But the Justice Department retained the right to proceed if a local U.S. attorney refused to bring charges. 



Interior Secretary Bruce Babbitt attacked energy companies for criticizing administration scare mongering about global warming. He charged the firms with attempting “to distort the facts and to mislead,” adding: “I think that the energy companies need to be called to account, because what they are doing is un‐​American in the most basic sense.” He left unsaid how he would call “un‐​American” businesses “to account,” but climate scientists have long reported that the administration uses its control of research funding to reward researchers who tow the party line and punish those who express skepticism of climatic Chicken Littles. 



And the Clinton‐​Gore administration advanced additional thuggish policies and proposals — curfews for kids, random drug tests for welfare recipients and kids seeking drivers licenses, attacks on the requirement of a jury trial, ex post facto tax hikes, attempts to gain court sanction for uncompensated property takings, prosecutions implicating the double‐​jeopardy clause, pretentious claims of federal criminal jurisdiction, infringements of the Second Amendment right to possess a firearm, et al. 



Tim Lynch, assistant director of the Cato Institute’s Center for Constitutional Studies, covered these and more in his devastating study, Dereliction of Duty: The Constitutional Record of President Clinton. He observed that “Although President Clinton has expressed support for an ‘expansive’ view of the Constitution and the Bill of Rights, he has actually weakened a number of fundamental guarantees.” 



Perhaps any particular decision could be defended on one ground or another, but Wired magazine’s John Heilemann accurately called the Clinton‐​Gore civil‐​liberties record “breathtaking in both the breadth and the depth of its awfulness.” 



Former Vice President Al Gore says he is worried about our civil liberties. How quaint. Too bad he didn’t evidence a similar concern when the administration of which he was a key member was routinely putting power before liberty. There’s no reason to believe that a Gore administration would be any different than a Clinton‐​Gore administration: We all would almost certainly be paying for more jackboot liberalism with our freedoms.
"
"Transportation continues to generate a large proportion of emissions worldwide, even as emissions from other areas of the economy fall. In the EU, transport accounts for around 30% of CO2 emissions, and is rising. It’s the transport sector that is set to derail the EU’s overall emission reduction objectives.  Globally, the number of cars is expected to double by 2035, and the air travel industry is expecting its passenger volumes to triple by 2050, yet there has been little political acknowledgement of this issue. In the meantime, the airline and automobile industries go to great lengths to convince politicians and the public that technology alone can solve this problem, while the weight of scientific evidence suggests technology cannot rein in transport emissions sufficiently. There’s growing evidence to suggest we need tougher regulation on planes and cars, but there’s no political willingness to introduce restrictive policies. Our research suggests policies that would support sustainable transport have been largely ignored by European policymakers because of a number of “transport taboos”. These are issues that constitute a fundamental barrier to implementing any significant transport-related climate policy, ignored because of their political risk. If politicians violate a norm by grappling with one of these hot potatoes – even if the science clearly supports it – they can be punished by powerful lobby groups, by peers, or at the ballot box. In our paper, published in the Journal of Transport Geography, we identify a series of transport taboos. Aircraft and cars are the most important from an emissions perspective. One example is from Germany: even though opinion polls are in favour of a speed limit on the autobahn, and the importance of speed limits for reducing carbon emissions is well documented, no party is willing to touch the issue because of the outrage that would ensue from car associations, manufacturers and some drivers. Another taboo is the matter of who contributes to the volume of transport on our roads and in our skies. This is skewed heavily towards a small number of people, mostly from higher income classes, who are responsible for a large share of the overall distances travelled. This is particularly evident in the context of air travel. The travel patterns of the highly mobile need addressing, yet those from the political classes in power tend themselves to be included in this hypermobile group. Paradoxically the most environmentally aware are also among the most mobile, yet there is a distinct unwillingness among this section of society to fly less.  A further taboo is that most measures to reduce transport emissions in the EU are market-based, and so will disproportionally affect the less wealthy. For instance, car taxes are based on the CO2 performance of individual models, but this does not take account of income inequalities. A SUV might use twice the amount of fuel as a small car and be taxed twice as much, but its driver is likely to earn several times the average income. Lower income groups will shoulder a heavier relative burden. Tackling this taboo carries the same kind of political risk as increasing income tax rates in the higher tax bands.  Similar issues apply in the context of flying, where taxes disproportionally affect lower income groups, yet are not high enough to seriously impede the mobility patterns of frequent-flying elite. These continue to enjoy the effects of market distortions, where their flights are subsidised through the exemption of international air travel from VAT. And so the costs of flying, one the most environmentally harmful modes of transport, remain largely externalised. The airline industry and its lobbyists work hard to instil the idea that “mobility is freedom”, and that to restrict such mobility through regulation is nothing short of an infringement of that liberty; another taboo. If we are to have any chance of slowing the rise of transport emissions in the EU and worldwide, these and many more transport taboos need to be confronted and overcome. We need more research on these taboos and how they operate, so that strong supporting evidence can be put before political leaders. Even then, any change will need to be publicly palatable, and building that support will be hard. After all, for a great number of people this will still be an inconvenient truth."
"
A Guest Post by Basil Copeland



Like many of Anthony’s readers here on WUWT, I’ve been riveted by all the revelations and ongoing discussion and analysis of the CRUtape Letters™ (with appropriate props to WUWT’s “ctm”). It might be hard to imagine that anyone could add to what has already been said, but I am going to try. It might also come as a surprise, to those who reckon me for a skeptic, that I do not think that anything was revealed that suggests that the global temperature data set maintained by CRU was irreparably damaged by these revelations. We’ve known all along that the data may be biased by poor siting issues, handling of station dropout, or inadequate treatment of UHI effects. But nothing was revealed that suggests that the global temperature data sets are completely bogus, or unreliable.
I will return to the figure at the top of this post below, but I want to introduce another figure to illustrate the previous assertion:

This figure plots smoothed seasonal differences (year to year differences in monthly anomalies) for the four major global temperature data sets: HadCRUT, GISS, UAH and RSS. With the exception of the starting months of the satellite era (UAH and RSS), and to a lesser degree the starting months of GISS, there is remarkable agreement between the four data sets – where they overlap – especially with respect to the cyclical pattern of natural climate variation. This coherence gives me confidence that while there may be problems with the land-sea data sets, they accurately reflect the general course of natural climate variation over the period for which we have instrumental data. While we need to continue to insist upon open access to the data and methods used to chronicle global and regional climate variation, and refine the process to remove the biases which may be present from trying to make the data fit the narrative of CO2 induced global warming, it would be wrong to conclude that the “CRUtape Letters” prove that global warming does not exist. That has never really been the issue. The issue has been the extent of warming (have the data been distorted in a way that would overstate the degree of warming?), the extent to which it is the result of natural climate variation (as opposed to human influences), and the extent to which it owes to human influences other than the burning of fossil fuels (such as land use/land cover changes, urban heat islands, etc.). And flowing from this, the issue has been whether we really know enough to justify the kind of massive government programs said to be necessary to forestall climate catastrophe.
Figure 2 plots the composite smooth against the backdrop of the monthly seasonal differences of the four global temperature data sets:

Many readers may recognize the familiar episodes of warming and cooling associated with ENSO and volcanic activity in the preceding figure. With a little more smoothing, we get a pattern like that depicted in Figure 3, which other readers may notice looks a lot like the cycles that Anthony and I have attributed to lunar and solar influences (they are the same):

In either case, the thing to note is that over time climate goes through repetitive episodes of warming and cooling. You have to look closely on Figures 2 and 3 – it is much clearer in Figure 1 – but episodes of warming exist when the smooth is above zero, and cooling episodes exist when the smooth is below zero. Remember, by design, the smooth is not a plot of the temperature itself, but of the trend in the temperature, i.e. the year to year change in monthly temperatures. The intent is to demonstrate and delineate the range of natural climate variation in global temperatures. It shows, in effect, the trend in the trend – up and down over time, with natural regularity, while perhaps also trending generally upward over time.
Which brings us to Figure 1. Here we are focusing in on the last 30 years, and a forecast to 2050 derived by a simple linear regression through the (composite) smooth of Figure 3. (Standard errors have been adjusted for serial correlation.) There has been an upward trend in the global temperature trend, and when this is projected out to 2050, the average is 0.114°C per decade ± 0.440°C per decade. Yes, you read that right: ± 0.440°C per decade. Broad enough to include both the worst imaginations of the IPCC and the CRU crowd, as well as negative growth rates, i.e. global cooling. Because if the truth be told, natural climate variation is so – well, variable – that no one can say with any kind of certainty what the future holds with respect to climate change. Be skeptical of any statistical claims to the contrary.
I think we can say, however, with reasonable certainty, that earth’s climate will remain variable, and that this will frustrate the effort to blame climate change on CO2 induced AGW. Noted on the image at the top of this post is a quote from Kevin Trenberth from the CRUtape Letters™: “The fact is that we cannot account for the lack of warmth at the moment, and it is a travesty that we can’t.” Trenberth betrays a subtle bias here – he cannot acknowledge the recent period of global cooling. It is, rather, “a lack of warmth.” But he is right that it is a “travesty” that we cannot fully account for the ebb and flow of earth’s energy balance, and ultimately, climate change. I think Trenberth just sees it as a lack of monitoring methods or devices. But I think there still remains a considerable lack of knowledge, or understanding, about the mechanics of natural climate variation. If you look carefully at Figure 1, you will notice that there seem to be upper and lower limits to the range of natural climate variability. On the scale depicted in Figure 1 (the scale is different with other degrees of smoothing), when warming reaches a limit of approximately 0.08-0.10°C per year, the warming slows down, and eventually a period of cooling takes place, always with the space of just a few years. Homeostasis, anyone? While phenomenon like ENSO are the effect of this regularity in natural climate variation, they are not the cause of it.
In my opinion, what is the real travesty of the global warming ideology is the hijacking of climate science in the service of a research agenda that has prevented science from investigating the full range of natural climate variation, because that would be an inconvenient truth. We see this, quite clearly, in the CRUtape Letters™ where the Medieval Warm Period is just “putative,” and a rather inconvenient truth that needs to be suppressed. Or the “1940’s blip” that implies that global temperatures increased just as rapidly in the early part of the 20th Century, as they did at the end of the 20th Century, an inconvenient truth at odds with the narrative preferred by the IPCC. 
It is a truism that “climate varies on all time scales.” With respect to the variability demonstrated here, I’m convinced that someday it will be acknowledged that variability on this scale is dominated by lunar and solar influences. On longer scales, such as the ebb and flow from the Medieval Warm Period, through the Little Ice Age, and now into the “Modern Warm Period,” I do not think climate science yet has any real understanding of the underlying causes of such climate change. If we are, as seems possible, on the verge of a Dalton or Maunder type minimum in solar activity, we may eventually have an answer to whether solar activity can account for centennial scale changes in earth’s climate. And I do think it is reasonable to conclude, at the margin, that human activity has had some influence. It is hard to imagine population growing from one to six billion over the past one and a half centuries without some effect. Most likely, the effect is on local and regional scales, but this might add up to a discernible impact on global temperature. But until all of the forces that determine the full range of natural climate variability are understood better than they are now, there is no scientific justification for the massive overhaul of economic and government structures being promoted under the guise of climate change, or global warming.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e90e487d3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The _Washington Times_ ’ columnist Don Lambro recently wrote that some supply‐​siders (not me) were wary of Arnold Schwarzenegger because adviser Warren Buffett said it makes no sense to pay more property taxes on his $500,000 house in Omaha than on his $4 million house in Laguna Beach. 



The Buffett remark bothered many but just struck me as naive. Having lived in California for 27 years, I assumed a half‐​million bucks in Omaha probably buys a much nicer place than $4 million in a California beach town. Mr. Buffett has owned that California house for decades, so he didn’t pay much for it. 



Ever since the 1978 Proposition 13, California’s constitution requires that property taxes be initially based on what you paid for a house (not on what you might sell it for) and annual increases are limited to 2 percent. Since all houses eventually are sold, this works out surprisingly well. The assessed value of California property rose 4.8 percent a year from 1990 to 2002, virtually identical to the 4.9 percent annual growth of personal income. Mr. Schwarzenegger obviously understands this, though Mr. Buffett did not. 



What made no sense was for Mr. Buffett to even hint about a policy change that would require an unlikely change in the state constitution. There are much easier and quicker chores to attend to. Mr. Buffett’s blunder demonstrated it made no sense for Mr. Schwarzanegger to go all the way to Nebraska in search of economic advice from a noneconomist. The tens of thousands who flee California taxes each week are not heading for Nebraska, which the Tax Foundation rates as having the 45th worst tax climate in the nation (California is 49th). 



If Mr. Schwarzenegger were looking for economic advice from a Democrat, he could have called Robert Hall at Stanford University, co‐​architect of the Hall‐​Rabushka flat tax. If he would settle for a Republican, he could have asked Mike Boskin, David Henderson, Art Laffer or Ben Zycher. There’s plenty of local talent, and not just among such older giants as George Shultz. Indeed, Mr. Schwarzenegger himself seems fundamentally sound on the critical tax and spending issues, and so do rivals Bill Simon and Tom McClintock. 



The trouble with California taxes is not that property taxes are too low, but that income and sales taxes are way too high. The personal income tax rates are high enough to provoke an exodus of skilled people — a “brain drain.” Corporate tax rates are high enough to provoke an exodus of business capital — “capital flight.” Sales tax rates are high enough to provoke wholesale tax avoidance. 



California’s tax revenue per capita ranks as “only” the sixth‐​highest in the nation, but comparing tax revenues understates the damage. The state’s uncompetitive tax rates ultimately yield more economic distress and less revenue because they erode the tax base. Profitable companies leave the state, while companies with chronic losses stay. People in top tax brackets leave the state, while nontaxpayers who expect to gather tax‐​financed benefits arrive in droves. 



In 1991, as State Sen. Tom McClintock recently recalled, “An 18 percent increase in the sales tax and a 15 percent increase in upper brackets of the income tax were supposed to produce a net of $7 billion of new revenues. But they didn’t. … We didn’t take in $7 billion more — we took in $1 billion less. We lost another $1 billion the next year.” 



Indeed, real per capita personal income fell 5.6 percent during the three years following the 1991 tax increase, even though the national economy was recovering. Californians now arguing for another tax increase — notably Lt. Gov. Cruz Bustamante — are conveniently forgetting what happened after that was tried in 1991. 



There is obviously plenty of room for spending cuts, since California’s budget grew by 44 percent in the past four years. The appearance of tiny spending cuts in the new July 29 budget was a fraud. The state’s legislative analyst’s office reports, “Most of this increase can be explained by four factors: the [vehicle licensing fee] rate increase … new federal funds, borrowing to cover the state’s 2003–2004 pension obligations and the MedicCal accounting shift from an accrual to cash basis.” 



There was predictable whining after fees at the California State University and University of California were finally raised a bit after remaining unchanged for eight years. California has long been overtaxing some residents so people like me could get a valuable college education almost for free. Once that education begins to raise our future income, however, many of us leave the state to avoid steep taxes. 



The 9.3 percent income tax also applies to capital gains (though Bill Simon wants to cut it to 5 percent), which is one reason aging investors like me could never be enticed to bring our capital back to California. Sell stock, and the state will hit you with a big tax — don’t sell, and they’ll get you with an estate tax. But that only works if intended victims don’t know how to vote with their feet. 



California’s personal income tax is 9.3 percent on income above $38,291 — a definition of “rich” that could only make sense to Hollywood liberals. That brutal tax rate makes it difficult for companies to recruit and retain skilled people without paying higher salaries. That, in turn, raises the cost of doing business for skill‐​intensive industries and makes them uncompetitive until they move jobs out of California. 



California’s corporate tax is above 8.8 percent — much higher than any nearby state. The corporate tax in Washington and Nevada is zero. Companies with chronic losses don’t mind staying in California, but many profitable firms become much more profitable (after taxes) by relocating. 



The sales tax varies locally from 7.25 to 8.5 percent, which is high enough to foster significant tax avoidance (e.g., shopping in Oregon, which has no sales tax, or ordering from another state). States with lower sales tax rates than California collect more money from this tax. They also collect more income taxes because a lower sales tax helps in the building of profitable retail businesses, who hire more people. 



California never will be able to compete well for new businesses and the skilled people those firms require until its tax system becomes reasonably competitive. At a minimum, that means sales and income tax rates no higher than 6 percent. 



A state with no sales tax like Oregon can get away with a higher income tax. A state with no income tax like Washington can get away with a higher sales tax. But Sacramento’s politicians have been foolishly rapacious with both income and sales taxes. That is killing the state economy, and it has to stop. 



There are two or three gubernatorial candidates who seem to get it. I suspect or at least hope Arnold Schwarzenegger may be one of them. 
"
"
From the “weather is not climate department” another report of ice further south than has been recently experienced. Here’s a picture from this China Daily news story:

From Maritime Global.net
CHINA PORTS FREAK WEATHER ALERT
By David Hughes
Published: Tue, 26 January 2010
Freak weather conditions and/or abnormal weather patterns have been  reported in several parts of the world during recent months warns the  American P&I Club. One of the latest examples is a significant  build-up of sea ice in some major northern Chinese ports, the volume  exceeding, it says, anything experienced in more than 30 years.
In an alert to its members, the club says the problem is centred around  Bohai on the northern Yellow Sea coast, affecting ports such as Bayuquan  and Dalian. At Bayuquan, patches of ice 500-600mm thick have formed in  some places, while lesser patches have been seen in the immediate  vicinity of the port.
Three icebreakers are working to avoid delays to ships, while the local  Maritime Safety Authority is strictly supervising inbound and outbound  vessel traffic.
Other northern ports – such as Jingtang, Caofeidian and Xingang – are  said to be not so seriously affected. On January 17, the Chinese  National Sea Weather Forecast Station reported that floating ice around  Liaodong Gulf extended as far as some 60 nautical miles from shore, at  Bohai Gulf around 22 miles, Northern Yellow Sea around 14 miles, and  Laizhou Gulf around 33 miles.
However, with more cold weather fronts expected later, ice coverage  around the Bohai coast could expand, according to the club’s  correspondents in China, Huatai Agency & Consultant Services Ltd.
The club advises that vessels scheduled to call at northern ports,  especially Bayuquan, should be ready for extreme temperatures and ensure  Port State Control requirements are strictly followed to avoid  unnecessary delay.
===============
Here’s another news story from AsiaOne News:
Bohai bay turns into block of ice 
h/t to Ron De Haan (Note: please fix your email!!)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8ef70049',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
And yet…it was claimed to be a “success”. Note to organizers: the media is laughing at you.
Snowstorm squelches climate change protest
By Judy Fahys, The Salt Lake Tribune Updated: 12/30/2009 06:36:58 PM MST
From KSL-TV - click for video and full report
A downtown [Salt Lake City] protest of the climate change talks in Copenhagen became a victim of Wednesday’s snowstorm.
“Not many people showed up because of the blizzard conditions,” said organizer Clea Major, an international studies student at the University of Utah.
It didn’t take long for the six friends to pack up a bullhorn and posters they’d planned to use for their “scream-in,” an outlet for their frustration about the failure of the Copenhagen climate talks earlier this month to curb the pollution blamed for climate change.
Still, they chatted with a few passers-by during the commuter-hour protest near the Gateway, and explained that, blizzard aside, climate change is expected to bring chaos to the global climate, said Major.
She called Wednesday evening’s effort a success and possibly the first in a series. As for the snow, it’s not entirely new; a protest she attended last year in Washington, D.C., suffered a similar fate.
“There is always the irony element,” Major said.
###
============================================
Here’s the original announcement from KCPW in Salt Lake:
Protesters Scream for Climate Change
12.30.2009 by Elizabeth Ziegler
(KCPW News) Climate change activists will stage a “scream-in” today at the Gateway Mall in downtown Salt Lake City to vent their frustrations about the Copenhagen Accord adopted by global leaders two weeks ago. University of Utah student Cléa Major says the demonstration is intended to call attention to the fact that the accord doesn’t require countries to reduce greenhouse gas emissions.
“Basically we want your average shopper to go home tonight after work and say, ‘Man, you know, I was returning my Christmas gift and there were these people screaming on the sidewalk, you know, what’s that all about?’” Major says. “We wanted to make an impression on people and we wanted to maybe put it in a location where it wasn’t necessarily expected to get people out of the post-Christmas haze to just sort of listen.”
Major sees the Copenhagen Accord as a failure, akin to the cap and trade legislation, The American Clean Energy and Security Act, that activists had hoped would set the tone in Copenhagen. The bill has stalled in the Senate after garnering a narrow margin of support in the House of Representatives this summer. All three of Utah’s Congressmen voted against it.
Major says the scream-in will be cathartic for those participating, who see the Copenhagen Accord as a missed opportunity to reverse climate change.
“We just all felt so helpless, we felt betrayed by this,” Major says. “We felt helpless and we felt furious because it was like we had just been looking to this to be the big thing that could turn it around or at least be a major jumping off point. And it just kind of stalled and failed and now there’s kind of this feeling of we don’t really know where to go next.”
The scream-in takes place at 5:30 p.m. on the northwest corner of 400 West and 200 South at the Gateway Mall.
﻿


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8f576028',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterProf. Claes Johnson’s blogsite here reports on how an IPCC Vice Chair recently used his UN position to suppress scientific dissent and discourse. Hat-tip: Hans Labohm.Johnson, professor of applied mathematics at the Royal Institute of Technology in Stockholm, had been asked by the Société Européenne des Ingénieurs et Industriels (European Society of Engineers and Industrialists) to participate (along with Fred Singer) in discussions on climate science in Brussels Sept 1-2.
Spooked, an IPCC higher-up intervened and cancelled the discussion – obviously too much scientific dissent over a science that can no longer take it.
Johnson got the following letter from the SEII dated August 20 signaling a coming invitation:
SEII (Société Européenne des Ingénieurs et Industriels, Prof Henri Masson) organizes a conference for Fred Singer and Claes Johnson at the Fondation Universitaire in Brussels on September 1, at 18.00 h. Official invitation from SEII follows by E-mail.
The next day 2 September there will be a workshop with some of our Think Tank. Our preliminary programme looks as follows:
– 18.15 S. Fred Singer: What is new in climate change?
– 19.00 Claes Johnson: Blackbody radiation and Climate Thermodynamics
– 19.45 to 20.30: Questions and Answers”
But for Johnson, the invitation never came. Instead a letter (written in French) was sent August 22 by IPCC Vice Chair Jean-Pascal van Ypersele, who is also a member of the Belgium Royal Academy, was sent to the Fondation Universitaire.
The effect of the letter: The SEII/Fondation Universitaire seminar was cancelled.
Yes, that cancellation of open scientific debate was brought about by a “forceful intervention” by IPCC vice president Jean-Pascal van Ypersele, who wrote in his letter to the Fondation Universitaire:
[…] You should know that Fred Singer is a person who leaves very little to be desired when it comes to scientific honesty. His activities of disinformation are financed by the fossil fuel lobbies (see XXXXXXXXXXXXXX), and it is scandalous that such a person could be remotely or closely associated with the SEII and to the Fondation Universitaire.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Some eiminent colleagues have also written me that M. Johnson is no better. One of his recent textbooks, where he spoke up against climate change, published by the Royal Institute of Technology (KTH, Sweden), should have been retracted because it contained errors.
I thank you in advance if you’ll rapidly inform me of the measures the SEII intends to take in order to distance itself from this ‘event’?  […]
Cordially, Prof. Jean-Pascal van Ypersele
There you have it – a Vice Chair of the IPCC reacting to scientific dissent like Superman reacts to kryptonite. They haven’t learned that science is all about taking a hypothesis and putting it on the test-stand of rigorous scrutiny. Not that this has never been done in climate science – indeed it has been done many times. Therein lies the problem! The AGW hypothesis holds up about as well has a sugar cube left out in a hurricane. The only dissent that the IPCC accepts is dissent that agrees with their science.
“…these people will go to any length to suppress scientific dissent”
Here’s S. Fred Singer’s reaction, who got news of the suppression soon after:
– Why am I not surprised by this disreputable action of this IPCC officer? After all, we know from Climategate emails that these people will go to any length to suppress scientific dissent. Even to libel and to use bald-faced lies.
– Of course, I am not supported by fossil-fuel industry. That is complete nonsense and invention.
– My Europe visit is paid by the Ettore Majorana Foundation — to give an invited talk at a climate conference in Erice. I am using the occasion to accept additional invitations to speak (without lecture fees) at the Univ of Hamburg, Imperial College, Univ of Paris – Jussieu, and of course at the KNMI in De Bilt. By happenstance I was also invited to address 100+ engineers in Zurich.
– Our IPCC colleague van Yp also questions my honesty. Well now —  the IPCC has been using me as a scientific reviewer, I publish regularly in peer-reviewed journals and am an elected Fellow of several scientific societies. So there must be some who disagree with van Yp.”
The more the IPCC suppresses and tries to shut down dissent and debate, the more suspicious other scientists will get. Hardly a good way to build trust and respect.
UPDATE 1: Also read Russell Cook’s piece here.
=====================================
PS: You can contact the SEII and asked them why they refuse to have professors Singer and Johnson.
Tel.: Fondation Universitaire: 32 (0)2 545 04 00 (ask for SEII)
Fax: 32 (0)2 502 98 31
Email: info@seii.org or seii@tiscali.be
Share this...FacebookTwitter "
"

Chairman McCrery, Ranking Member Levin and members of the subcommittee, I thank you for giving me the opportunity to express my views on the reform of our Social Security system. Eleven years ago, on October 4, 1994, I had the opportunity to speak before this same Committee and in my written testimony I offered:



As both a son and a father, I am interested that the elderly are well cared for, and that the young have the opportunity to build sufficient assets so that they, too, can retire in dignity. Social Security, as presently structured, ultimately will achieve neither objective. Although compassionate in its original intent, it is flawed in design.



The system’s financial structure is fundamentally unsound. Legislation of 1977 and 1983 attempted to address this by raising taxes and cutting benefits; Social Security was to be on sound financial footing well in to the 21st century. And now, just a few years later, The 1994 Board of Trustees’ report suggests that the system will run out of money seven years earlier than it projected just one year ago. Legislative initiatives to reduce benefits further and raise taxes are again on the drawing board. This did not work in 1977 or 1983; it will not work now. Social Security’s financial integrity requires an entirely different approach. I offer this testimony in the spirit of the starting point for an alternative: a concept of privatization wherein Americans benefit from the engine of a free economy and free choice. With privatization properly structured, today’s elderly will be protected, the young will retire with higher incomes, and our political leaders will have offered, once and for all, a lasting solution for which all voters will be thankful.



Since that testimony our nation has had a vigorous and open discussion. Many new ideas have been offered, ideas not developed prior to 1994. The climate of opinion has changed; more Americans are now aware of the issue, more Americans want the option to save and invest for their own future. We are getting closer to the point where the “rubber meets the road,” when you, as Members of Congress, will have to vote. Your decision is more important than perhaps you know. It has been eleven years since my first testimony on this issue and in many ways, but certainly not all, little has changed politically; we’re still talking about raising taxes and reducing benefits. We have wasted precious time.



 **A Collision Course**



Like other nations we face an unprecedented challenge of how to deal with a reality that mankind has never confronted before and one that most people are unaware of. How we and other governments respond will affect each American citizen, our families, businesses across the land, indeed our very way of life. The reality is not only unprecedented, it is unyielding.



Dr. Karl Otto Pohl, former president of the German central bank, the Bundesbank, stated it this way: “In a relatively short period, we must adapt our domestic institutions, international relationships, and even our individual life plans to a new, and powerful reality.”



What he was speaking of, and what confronts each of us here, is the fact that there are two powerful forces on a collision course. The first is the aging of society, the reality that the elderly population is increasing more rapidly then the population as a whole. In America, but even more so in other countries, the elderly rely on Social Security to survive financially. Should Social Security falter, many elderly will be destitute.



The second force is that most Social Security systems, including ours, are, in fact, failing. They are financially unstable, and not sustainable as they are presently structured.



The challenge is to avoid the collision of these two forces. In my view, the risks are high that we will not. But should we prevail by structuring a lasting solution, the rewards will be as unprecedented as the challenge itself.



 **The Early Years: Social Security’s Roots**



Social Security was enacted in 1935 during the Great Depression. During the first half of the 1930s real GDP fell by about 25 percent, unemployment jumped to 22 percent and the stock market virtually imploded and fell about 70 percent. Our nation was on her economic knees. President Roosevelt had to do something, something big, but large government programs were anathema to the frontier spirit of our young nation. In order to achieve his goals he needed unprecedented authority. To grasp that authority he went before the nation on March 4, 1933 in his first Inaugural Address and asked for authority “… as great as the power that would be given me if we were in fact invaded by a foreign foe.” He achieved his goal and ushered in Social Security, the flagship program of the New Deal.



Much like other Social Security programs that preceded ours, the first being Germany’s in 1889, benefits paid to the elderly were financed by payroll taxes. In our case, during the Great Depression, people who had jobs were considered the wealthy. It wasn’t like today wherein Americans have portfolios of stock and bonds, real estate, defined benefit and contribution plans and the like; you were considered wealthy if you had a job. And needs were so urgent that the “payroll wealth” was taxed. A saving and investment structure would not have worked at that time because it takes time to compound investment returns to accumulate wealth, and time was short.



 **Today: A Fundamentally Flawed Program**



Over the decades, however, this sort of urgent safety net has turned into the rough equivalent of a defined benefit plan. Yet its financial structure has not advanced. The Old‐​Age and Survivors Insurance part of Social Security, as its finances are presently structured, is inefficient, financially unsound and fundamentally flawed.



Because benefits are paid by taxing payroll, benefits can increase by no more than payroll increases, assuming that the tax rate on payroll is held constant. Over the last four decades or so, payroll has increased by about 1.5 percent per annum in real terms. That is roughly equivalent to saving and investing and receiving a rate of return of 1.5 percent. To put this into perspective, if one were to save $1,000 each year for a 45‐​year working career and earn 1.5 percent, the saving would accumulate to about $64,000. During the last 79 years a mixed portfolio of 90/10 percent large/​small company stocks earned an inflation‐​adjusted average annual return of 9.7 percent. One thousand dollars invested annually for 45 years earning that return would accumulate to about $650,000. And a conservative portfolio of 60/40 percent stocks and bonds, respectively, would accumulate to about $288,000. These different values give a glimpse of the lost opportunity that our citizens incur by being required to finance their retirement through payroll taxes.



But it is worse. For any particular age group it matters how many workers pay taxes relative to the number of retirees who receive benefits. The change in this ratio is largely determined by the change in national wealth, or GDP per capita. As national wealth rises, life spans also rise. We observe this not only here but across all parts of the globe. When Social Security was enacted life expectancy at birth was 61 years of age; it is now about 78. In the post‐​war period global life expectancy has increased from 45 to 65 years of age, a greater increase in the last 50 years or so than in the previous 5,000 years. This is all new. We didn’t expect it. But now we think it will continue.



Also, as nations become more wealthy birth rates fall. In many countries they have fallen below the population replacement rate of 2.1. The combination of rising life expectancy and falling birth rates causes havoc with pay‐​as‐​you‐​go financed Social Security systems. In the United States there were 16 workers per beneficiary in 1950; today there are about 3.3. It is expected that there will be only two in just 35 years. Therein lies an interesting paradox: as countries become more wealthy their Social Security systems become more poor. The oddity is driven by the causal relationship between increasing wealth‐​and increasing life expectancy as well as decreasing birth rates‐​all wrapped around pay‐​asyou‐ go financing.



Birth rates have fallen to such low levels in Europe-France-1.9, Germany-1.4, Italy-1.3, Spain-1.3-that “there is now no longer a single country in Europe where people are having enough children to replace themselves when they die.”



 **The Global Political Response: Raise Taxes**



The political responses to the changing demographics that squeeze Social Security’s finances are frequently the same across the world. Governments and politicians tend to see the problem in the narrowest of lights: merely a solvency issue‐​too many benefits paid, too few taxes received. This near‐​sighted analysis is further compounded by the focus on just today’s solvency and not tomorrow’s.



But from this myopic perspective the options are clear; raise taxes, cut benefits. Of the two, governments tend toward raising taxes first. This makes sense for at least a couple of reasons. There are more workers to tax than there are retirees from whom to cut benefits. Therefore, if the choice were only one or the other, raising taxes inflicts a lesser per capita burden. The second reason is that workers are younger than retirees, therefore, they have more time to adjust to a tax increase than retirees have to adjust to a benefit cut.



The short‐​sighted strategy of raising taxes has been employed world‐​wide. In the United States, for example, in 1950 when there were 16 workers per beneficiary, the maximum Social Security tax any American worker paid was $90 a year. At that time the tax rate was 3 percent on only $3,000 of wage income. As the glacial force of demographics slowly and unrelentingly squeezed the system, the $90 tax rose and squeezed the worker. Now, the tax, just for the retirement portion of Social Security, is 10.6 percent of $90,000, or $9,540. After adjusting for inflation over the last 55 years, that tax has increased almost 2,000 percent. In all likelihood, the reason that we stood for this is that the tax rose slowly; the increase was never really noticeable in any one year, but over time it has become more of a burden than the income tax for about three quarters of American workers.



Our friends in Europe, however, would consider us lucky. The payroll tax in France is about 50 percent and in Germany, Italy and Spain it ranges roughly between 38 and 42 percent. It is true that these countries’ systems provide more services than ours, but this is not a plus. Europeans are dependent on more of their needs from government programs that are not sustainable.



As many European nations have raised their payroll taxes to prohibitive levels they have choked individual economic freedom and incentive. Economic growth is stagnant, and unemployment rates hover around 10 percent, even 12 percent in Germany. Civil unrest is now more common in Germany and France as governments tell their people that benefits are no longer affordable and will have to be cut, while at the same time they extol the virtues of the welfare state. We are on the same path, but for the moment we trail far behind.



 **Then, Cut Benefits**



At some point, the strategy of raising taxes approaches a political wall. People sense that maybe, just maybe, they could achieve more with their hard‐​earned wages than they get from Social Security. Politicians sense this and move to the lesser desirable option of cutting benefits. Such blunt language, however, is not commonly uttered. Code is employed: progressive price indexing, longevity indexing, adding a third bend point, reducing bend point factors, increasing the NRA, decreasing the PIA, and it goes on and on. It’s all code for cutting benefits.



 **Fundamental Reform: Retarded by the Claim of Insurance**



Eventually, after cutting benefits hits its political wall, the thinking shifts to fundamental reform, saving and investing in wealth‐​producing assets for all workers. This idea of market‐​based financing for retirement income is not new, in fact it is old and well established in the private sector, but it is viewed with some disdain from advocates of the status quo. They object to the notion that Social Security should be an investment structure and defend their objection by claiming that it is insurance. This claim had some merit decades ago. Not now. In fact, Social Security’s finances are in trouble largely because they are inappropriately based on the insurance model.



Insurance works well when many people are subject to an event that has little chance of happening to any single individual. A good example is homeowners’ fire insurance. Many people buy fire insurance to protect their homes and yet few homes burn. Because the number of homes insured is many times the number of homes that burn, the annual insurance premium is very low relative to the replacement cost of one’s house. Insurance companies are simply the medium through which individual uncertainty of loss is transferred to, and financed by, the group.



The insurance model does not work well when the group is subject to an event that the entire group experiences. For example, if it were certain that everybody’s house would burn down, say, when the owner reached age 65, then insurance companies would have to charge annual premiums the future value of which would be the cost of rebuilding all the houses. This premium would be a large multiple of the premium charged for the uncertain case. Central to the insurance model is that the ratio of the annual premium to the dollar value of what it protects is negatively correlated to the uncertainty of individual loss.



Social Security is frequently heralded as insurance, more precisely social insurance. The ‘social’ part of the term merely means that the government plays the role of the insurance company. Other than that, it remains the insurance model. When Social Security was enacted in 1935, life expectancy was 61 but benefits weren’t payable until age 65. Now benefits are payable at age 62 and life expectancy is 78. The element of uncertainty has kind of flipped upside down. Because of this, the retirement component of Social Security isn’t insurance; once born, reaching age 62 and needing retirement income is almost certain. As a result, there is very little risk, or uncertainty, to transfer to the group,resulting in the fact that annual premiums must be enough to accumulate to a sum, including interest, that will finance retirement income.



Under these conditions, social insurance cannot provide such income at a lower cost than saving and investing for retirement. Unfortunately, however, it can and does provide it at a higher cost because it is financed through the payroll tax and is subject to unyielding demographic forces. In a perverse way Social Security’s finances and its adherence to the insurance model are caught in a kind of time warp; in the age of the iPod Social Security is a 78 RPM, wind‐​up phonograph. Unless protected by the power of the state, it can neither compete nor survive.



 **The State Monopoly Faces Competition**



Being protected by the power of the state really means that for 10.6 percent of their wage income American workers are not free to choose among alternatives for their retirement. Bad as that is, the 10.6 percent doesn’t buy much relative to reasonable and available alternatives. This is why Social Security is mandatory; few would participate if they had the freedom not to. Competition, as always, is a threat to the status quo. For workers, however, competition is their hope.



Competition would allow all workers to invest part of their payroll tax in capital markets around the world, in professionally managed portfolios that are highly diversified across asset classes, national borders and time. Such an opportunity would allow one to accumulate enough wealth to replace the pay‐​as‐​you‐​go benefit entirely.



For an average wage worker retiring this year at age 65, Social Security’s scheduled benefits are projected to replace about 42 percent of his last year’s wage. But for workers retiring in the future full benefits won’t be paid until age 67. For those future retirees, should they choose to retire at age 65, benefits will replace only 36 percent of their last wage. The worker’s cost for these scheduled benefits, which are in excess of what is affordable based on present law, is the 10.6 percent payroll tax.



 **The Market‐​Based Alternative**



The market‐​based alternative is significantly more attractive. Over the last 79 years a conservative portfolio of 60/40 percent stocks and bonds, respectively, earned a real return of just a little over 7 percent. Investing just half of the retirement payroll tax, 5.3 percent, each year for 45 years would provide a retirement benefit equal to 97 percent of one’s last year’s wage. This assumes that there is no interruption in saving each year, that the market return is as stated and falls by 2 percent during the distribution phase, and that life expectancy upon retirement is 20 years. Each of these assumptions can be changed. Work may be interrupted. Markets may do worse or better. Life expectancy may be more or less than 20 years once retired.



To take a conservative path, if the market return were only 5.5 percent and if life expectancy were 30 years at the onset of retirement‐​about 10 years more than assumed by Social Security‐​then under these conditions the replacement rate would 39 percent, higher than Social Security’s scheduled benefits at age 65 and significantly higher than payable benefits.



 **Americans Understand the Tradeoffs**



Our citizens sense these tradeoffs, risks, uncertainties, and the fundamental differences in providing retirement income from a tax system versus a saving and investment system. This is why, but only part of why, they want the option, the freedom to choose.



If they could acquire this freedom they also would have personal property rights over their accumulated wealth. They have no such rights to Social Security benefits. They also could bequeath some or all of their retirement assets. They cannot under Social Security. They would benefit from the direct relationship between effort, their saving, and reward, their accumulating wealth. They would have the dignity that comes with being personally responsible for their future. They would no longer be tethered to the government. They would no longer be subject to politicians’ preferences over when they can retire, how much they can get, how their spouses are treated, how much they’re going to pay, and all of the rules and regulations that have evolved to the point of being incomprehensible. They would be free.



It’s been eleven years since I had the opportunity to speak before this Committee. Although much of what I am saying today is what I said then, I hope that we are closer to fundamental reform. If we are not, and the two powerful forces that I mentioned above in fact collide, we will edge closer to the wrenching difficulties that Europe is now facing.



 **You, Congress, are the Hope**



But should we grasp the extraordinary opportunity that this challenge offers, we will forever strengthen our nation, our economy, our freedoms, and our ability to finance the many needs that the future will undoubtedly require. It is a matter of will and political leadership in seeing the benefits of greater personal freedom and acting to ensure them. You, as Members of Congress, have the unique opportunity to offer, once and for all, a lasting solution for which all Americans will be forever thankful.



Thank you,



William G. Shipman
"
"
Share this...FacebookTwitterUPDATE 1: Roger Pielke Jr:
Here is another good example why I have come to view parts of the climate
science research enterprise with a considerable degree of distrust.”
===============================================
With the IPCC 5th report due to come out in the near future, the order books at hockey stick factories are full.
The Potsdam Institute for Climate Impact Research (PIK) offers a complete line…whether you’re looking for sea-ice sticks, temperature sticks, storm frequency sticks, tornado sticks, drought sticks, flood sticks, glacier sticks….you name it, they can craft it.
And all now come with even sharper than ever blade angles. Here’s the latest junk science from the PNAS:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Increase of extreme events in a warming world
Stefan Rahmstorf and Dim Coumou, Potsdam Institute for Climate Impact Research, PO Box 601203, 14412 Potsdam, Germany
Edited by William C. Clark, Harvard University, Cambridge, MA, and approved September 27, 2011 (received for review February 2, 2011)
Abstract
We develop a theoretical approach to quantify the effect of long-term trends on the expected number of extremes in generic time series, using analytical solutions and Monte Carlo simulations. We apply our method to study the effect of warming trends on heat records. We find that the number of record-breaking events increases approximately in proportion to the ratio of warming trend to short-term standard deviation. Short-term variability thus decreases the number of heat extremes, whereas a climatic warming increases it. For extremes exceeding a predefined threshold, the dependence on the warming trend is highly nonlinear. We further find that the sum of warm plus cold extremes increases with any climate change, whether warming or cooling…continue abstract here.
By the way, the authors conclude that man is responsible for the heat (and not nature or the sun).
Share this...FacebookTwitter "
"
From the University of California, San Diego Press Release
The previously unknown eruption in 1809 was larger than the Mt. Pinatubo eruption. Credit: USGS
A team of chemists from the U.S. and France has found compelling evidence of a previously undocumented large volcanic eruption that occurred exactly 200 years ago, in 1809.
The discovery,  published online this week in the scientific journal Geophysical Research Letters, offers an explanation as to why the decade from 1810 to 1819 is regarded by scientists as the coldest on record for the past 500 years.
“We’ve never seen any evidence of this eruption in Greenland that corresponds to a simultaneous explosion recorded in Antarctica before in the glacial record,” said Mark Thiemens, Dean of the Division of Physical Sciences at UC San Diego and one of the co-authors of the study. “But if you look at the size of the signal we found in the ice cores, it had to be huge. It was bigger than the 1991 eruption of Mount Pinatubo in the Philippines, which killed hundreds of people and affected climate around the world.”






Jihong Cole-Dai of South Dakota State U. headed the research team. Credit: South Dakota State U.



Led by a chemist from South Dakota State University, the team of scientists made its discovery after analyzing chemicals in ice samples from Antarctica and Greenland in the Arctic, where the scientists visited and drilled ice cores three years ago. The year-by-year accumulation of snow in the polar ice sheets records what is going on in the atmosphere.
“We found large amount of volcanic sulfuric acid in the snow layers of 1809 and 1810 in both Greenland and Antarctica,” said professor Jihong Cole-Dai of SDSU’s Department of Chemistry and Biochemistry, who was the lead author of the paper.
Joël Savarino of the Laboratoire de Glaciologie et Géophysique de l’Environment in Grenoble, France, and a former postdoctoral fellow at UC San Diego, was also part of the team.
Cole-Dai said climate records show that not only were 1816 — the so-called “year without a summer”— and the following years very cold, the entire decade from 1810 to 1819 was probably the coldest for at least the past 500 years.






The team drilled ice cores in Greenland’s ice sheet. Credit: Mark Thiemens, UCSD



Scientists have long been aware that the massive and violent eruption in 1815 of an Indonesian volcano called Tambora, which killed more than 88,000 people in Indonesia, had caused the worldwide cold weather in 1816 and after. Volcanic eruptions have a cooling effect on the planet because they release sulfur gases into the atmosphere that form sulfuric acid aerosols that block sunlight. But the cold temperatures in the early part of the decade, before that eruption, suggest Tambora alone could not have caused the climatic changes of the decade.
“Our new evidence is that the volcanic sulfuric acid came down at the opposite poles at precisely the same time, and this means that the sulfate is from a single  large eruption of a volcano in 1809,” Cole-Dai said. “The Tambora eruption and the undocumented 1809 eruption are together responsible for the unusually cold decade.”
Cole-Dai said the Tambora eruption was immense, sending about 100 million tons of sulfur gas into the atmosphere, but the ice core samples suggests the 1809 eruption was also very large — perhaps half the size of Tambora — and would also have cooled the earth for a few years. The researchers reason that, because the sulfuric acid is found in the ice from both polar regions, the eruption probably occurred in the tropics, as Tambora did, where wind patterns could carry volcanic material to the entire world, including both poles.






UCSD’s Mark Thiemens (upper left) pulls a cylinder in Greenland containing an ice core. Credit: UCSD



Cole-Dai said the research specifically looked for and found a special indicator of sulfuric acid produced from the volcanic sulfur gas in the stratosphere.
The special indicator is an unusual make-up of sulfur isotopes in the volcanic sulfuric acid. Isotopes are different types of atoms of the same chemical element, each having a different number of neutrons (but the same number of protons). The unique sulfur isotope composition is like a fingerprint of volcanic material that has reached the stratosphere, said Cole-Dai.
The stratosphere is the second major layer of the Earth’s atmosphere, reaching from about six miles to about 30 miles above the Earth’s surface at moderate latitudes. To impact global climate, rather than local weather, the sulfur gas of a volcanic eruption has to reach up into the stratosphere and once there, be spread around the globe.
“The way in which that these volcanoes affected the average temperatures of our planet gives us a better idea of how particulates in the atmosphere can affect our climate,” said Thiemens. “People talk about the possibility of geo-engineering our climate, but the question is how? In this case, nature has done an experiment for us.”
Other members of the research team were South Dakota State post-doctoral researcher David Ferris and graduate student Alyson Lanciki; and Mélanie Baroni of CEREGE (Le Centre Européen de Recherche et d’Enseignement des Géosciences de l’Environnement) at L’Université Paul Cézanne in Aix-en-Provence, France.
The researchers were funded by the National Science Foundation, French Polar Institute (IPEV) and the Institut National des Sciences del’Univers (INSU).


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e91f10ef8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Picture the scene: a dinner for MEPs organised by leading fossil fuel firms to explain the lengths to which their industries have gone to combat climate emergency. On the guest list, the environment minister of Croatia, current holders of the EU’s rotating presidency, and Guido Bortoni, an adviser in the European commission’s energy directorate. Nobody at all from civil society or the NGO sector. In other words a perfect Brussels lobbying event. Billed as “Oil and Gas and the Green Deal” this dinner took place on 17 February, just a fortnight before the unveiling of the EU’s first ever climate law. The meal was sponsored by the International Association of Oil and Gas Producers (IOGP), which represents 29 of Europe’s main fossil fuel operators, including Total, Shell, BP and ExxonMobil. According to the EU transparency register the IOGP spent €350,160 in 2018 lobbying in Brussels, so it can afford to take a few MEPs out to dinner. The European Energy Forum (EEF) is the perfect organiser: it is headed by Jerzy Buzek, an MEP for the European People’s party (EPP), a former prime minister of Poland, a former president of the European parliament and currently chair of its industry research and energy committee. The forum boasts 82 associate members, firms that pay “at least” €7,000 a year in membership fees. Predictably they are all in the oil and gas sector. The EEF politely turned down a media request to attend an earlier gathering on 26 November entitled “Gas: Driving the Energy Transition”. “We do not accept journalists at our events,” was the response. The event was sponsored by Eurogas, the gas industry’s main lobbying organisation, and two German firms, Uniper and Wintershall. An official from the European commissions DG Energy also attended. The European Green Deal, the commission’s €1tn plan to prepare the European economy for confronting the climate crisis, has prompted a round of intensive lobbying activity. “Since 2010, the five main oil and gas corporations and their fossil fuel lobby groups have spent at least a quarter of a billion euros buying influence at the heart of European decision-making,” claims a report produced by a coalition of climate NGOs (Corporate Europe Observatory, Food & Water Europe, Friends of the Earth Europe and Greenpeace). Such firms, their business founded on extracting fossil fuels, have an interest in delaying or undermining policies designed to mitigate the effects of the climate crisis. They make much use of Brussels-based thinktanks such as the Centre for European Policy Studies (CEPS). A week before the newly sworn-in commission presented its climate policies, CEPS hosted on 4 December, a “by invitation-only” event on the Green Deal funded by Equinor and Fortum, the energy market leaders in Norway and Finland, respectively.  Firms are welcome to join CEPS as corporate members. ExxonMobil, for example, pays €15,000 a year for this access. The thinktank has organised members-only breakfast meetings with guests such as Frans Timmermans, now EU commissioner in charge of the Green Deal.  Members of commission president Ursula von der Leyen’s team have been the focus of relentless attention from energy industry and particularly gas lobbyists for months. In the last 10 days of January alone, Eurogas representatives met six commissioners and officials from departments for the environment, trade, budget, agriculture and economic affairs. Each time the declared business of the meeting was the Green Deal. The energy commissioner, Kadri Simson, will be the keynote speaker at this year’s Eurogas conference on 19 March in Brussels. Six members of the association’s staff are accredited for access to EU parliament buildings. Eurogas spent €800,000 on lobbying in 2018 alone. “DG Energy is very permeable to the gas lobby,” says Pascal Canfin, a Renew Europe MEP and chair of the parliament’s environment committee. The IOGP has been busy too. On 17 December it met Ditte Juul Jorgensen, the head of DG Energy, then in January two other European commission members, in different directorates. The topic was of course the Green Deal. Similarly employers’ group Business Europe has shown great interest in the commission’s new plans for the environment. Speakers at the federation’s annual gathering on 4 March – the day the EU’s new climate law is being unveiled – include Von der Leyen in person, two of her vice-presidents and three commissioners. Another means of influencing European policymakers is to use lobbying firms. FTI Consulting is one of the most effective. Transparency International ranked it second for setting up meetings with the EU executive in 2018. With 38 staff members accredited for access to the European parliament FTI billed Eurogas, then a new customer, between €25,000 and €50,000 in 2018. Earnings from ExxonMobil, its second largest account totalled €600,000 to €700,000. The same year it carried out confidential research for Gas Infrastructure Europe. Measures for a Sustainable Gas Storage Market, the resulting study, was subsequently posted and is still on the European commission’s website. The firm has made good use of the many revolving doors available in Brussels. In 2016-18, for instance, Philip Lowe, after heading DG Energy (2010-14), was a senior adviser to FTI on public affairs. Jean-Arnold Vinois is another example of the revolving doors in operation. He is an energy policy adviser at the Jacques Delors Institute, a well known thinktank, but also an honorary director for energy at the commission and a consultant at FleishmanHillard, another big lobbying outfit in Brussels. This firm counts the European Chemical Industry Council (Cefic) – earning it almost €1m in 2018 – the gas lobby Gas Naturally (€25,000 to €50,000) and Fuels Europe, another oil and gas federation, among its customers. Carbon capture and storage (CCS) is the gas industry’s latest hobby-horse, seen as a means of justifying its fossil fuel activities. The snag is that CCS technology is far from being ready for widespread use and even if research succeeds in perfecting the techniques, a drastic reduction in emissions will still be needed over the next 20 years. The gas industry nevertheless persists in promoting it as a solution. Eurogas is working hand-in-hand with the Global CCS Institute, the key lobby in this field. On 18 February, in partnership with the United Kingdom Mission to the EU, it held a conference on the topic in Brussels. It is particularly difficult to pin down the role of EU governments in advancing the interests of their firms. There is no equivalent to the EU transparency register to keep track of the national governments or their missions. Industry is by no means alone in attempting to influence EU lawmakers on climate issues. Powerful environmental thinktanks and NGOs – such as the World Wildlife Fund (WWF), Transport & Environment, the Climate Action Network or the European Climate Foundation (ECF) – are hard at work too. Their financial means are substantial but less than their opponents. In 2018 the ECF reported expenditure of between €500,000 and €600,000, while WWF spent €2.2m to €2.4m over the same period. Aude Massiot writes for Libération on environment issues. This article is part of a This is Europe collaboration with Libération and Tageszeitung."
nan
"Plans for a third runway at Heathrow airport have been ruled illegal by the court of appeal because ministers did not adequately take into account the government’s commitments to tackle the climate crisis. The ruling is a major blow to the project at a time when public concern about the climate emergency is rising fast and the government has set a target in law of net zero emissions by 2050. The prime minister, Boris Johnson, could use the ruling to abandon the project, or the government could draw up a new policy document to approve the runway. The government is considering its next steps but will not appeal against the verdict. The transport secretary, Grant Shapps, said: “Our manifesto makes clear any Heathrow expansion will be industry-led. Airport expansion is core to boosting global connectivity and levelling up across the UK. We also take seriously our commitment to the environment.” Johnson has opposed the runway, saying in 2015 that he would “lie down in front of those bulldozers and stop the construction”. Heathrow is already one the busiest airports in the world, with 80 million passengers a year. The £14bn third runway could be built by 2028 and would bring 700 more planes per day and a big rise in carbon emissions. Johnson is thought to have been looking for a pretext to withdraw support for the extra runway and could make the argument for Birmingham to provide increased airport capacity for London given that train journey times will be reduced by HS2. The court’s ruling is the first major ruling in the world to be based on the Paris climate agreement and may have an impact both in the UK and around the globe by inspiring challenges against other high-carbon projects. Lord Justice Lindblom said: “The Paris agreement ought to have been taken into account by the secretary of state. The national planning statement was not produced as the law requires.”  What just happened? For the first time judges have said that plans for a major infrastructure project are illegal because they breach the UK's commitments to reduce greenhouse gas emissions to tackle the climate crisis. This is a groundbreaking legal decision that could effect future infrastructure developments and puts the UK’s commitment to cut emission to net zero by 2050 at the forefront of future policymaking. What will happen next? The government has been told by the court of appeal to declare its decision to allow Heathrow airport expansion - contained in its airline national policy statement - illegal. Ministers have two choices now. They can withdraw the whole policy statement or try to amend it to make it compatible with the UK’s commitments to reduce greenhouse gas emissions to net zero by 2050.  Will the runway be built? If the government can prove that expanding Heathrow is compatible with its commitments under the Paris agreement to very radically reduce greenhouse gas emissions, the runway may go ahead. But the prime minister has always been against the third runway, and the government has told the court it will not be appealing against its decision on Thursday.  There now hangs a very big question mark over whether the bulldozers will ever start work on the runway. “It’s now clear that our governments can’t keep claiming commitment to the Paris agreement, while simultaneously taking actions that blatantly contradict it” said Tim Crosland, at legal charity Plan B, which brought the challenge. “The bell is tolling on the carbon economy loud and clear.” Plan B’s intervention was one of a number of legal challenges against the government’s national policy statement, which gave the go-ahead for the new runway in 2018 after MPs backed it by a large majority. Others were brought by local residents, councils, the mayor of London, and environmental groups including Friends of the Earth and Greenpeace. The challenges were dismissed in the high court in May 2019 but the complainants took their cases to the court of appeal, which delivered its verdicts on Thursday. Plan B argued that the Paris agreement target, which the government had ratified, was an essential part of government climate policy and that ministers had failed to assess how a third runway could be consistent with the Paris target of keeping global temperature rise as close to 1.5C as possible. “This is an opportunity for Boris Johnson to put Heathrow expansion to bed and focus on the most important diplomatic event of his premiership, the UN climate summit in Glasgow in November,” said Lord Randall, a former Conservative MP and climate adviser to the former prime minister Theresa May. “It’s his chance to shine on the world stage.”  The court of appeal did not overturn the high court’s dismissal of the other challenges, which related to air and noise pollution, traffic, and the multibillion pound cost of the runway. But the Paris agreement ruling is far-reaching, according to Margaretha Wewerinke-Singh, an international public law expert at Leiden University, in the Netherlands. “Its implications are global,” she said. “For the first time, a court has confirmed that the Paris agreement temperature goal has binding effect. This goal was based on overwhelming evidence about the catastrophic risk of exceeding 1.5C of warming. Yet some have argued that the goal is aspirational only, leaving governments free to ignore it in practice.” Prof Corinne Le Quéré, at the University of East Anglia, said: “Government needs to put climate targets at the heart all big decisions, or risk missing their own net zero objectives with devastating consequences for climate and stability. I am relieved this is finally recognised in law.” Climate campaigner Greta Thunberg said: “Imagine when we all start taking the Paris agreement into account.” Heathrow and proponents of the third runway say it would provide an economic boost and is important for international business, particularly after Brexit. “The court of appeal dismissed all appeals against the government – including on ‘noise’ and ‘air quality’ – apart from one, [i.e. climate change] which is eminently fixable,” said a spokeswoman for Heathrow. “We will appeal [as an interested party] to the supreme court on this one issue and are confident that we will be successful. Expanding Heathrow, Britain’s biggest port and only hub, is essential to achieving the prime minister’s vision of global Britain. We will get it done the right way.” Mike Cherry, at the Federation of Small Businesses, said: “The verdict is a blow to small firms who need greater regional and global connectivity, as well as more opportunities to export.” However, most flights are taken for pleasure and just 20% of the UK population take more than two-thirds of international flights. Critics say the economic benefits are illusory given, for example, the estimated £10bn of taxpayers’ money needed to alter road and rail links to the airport, and would draw investment towards the south-east. “No amount of spin from Heathrow’s PR machine can obscure the carbon logic of a new runway,” said John Sauven, at Greenpeace UK. “Their plans would pollute as much as a small country.” Geraldine Nicholson, from local campaign group Stop Heathrow Expansion, said: “This is the final nail in the coffin for Heathrow expansion. We now need to make sure the threat of a third runway does not come back.” At a separate event on Thursday, Alok Sharma, the business secretary and president of November’s UN COP26 climate summit, said: “The only economy which can avoid the worst effects of climate change, and thus continue to deliver growth, is a decarbonised economy. Our choices will make or break the zero-carbon economy.” • This article was amended on February 28 2020. An earlier version had mistakenly called the business secretary Ashok Sharma, rather than Alok Sharma. This has been corrected. "
"

California’s newly released regulatory initiative to reduce greenhouse gas emissions from new cars sold in that state represents the triumph of symbolism over substance. It’s an ill‐​considered gesture that ought to annoy partisans on both sides of the global warming fence.



How much will these new emission rules help in the fight against global warming? “Not much” would be a charitable answer.



Back‐​of‐​the envelope calculations derived from computer simulations performed by climatologist Tom Wigley (who, by the way, supports aggressive action to address the threat of global warming) suggest that even if every state in the union adopted California’s new program, global temperatures would drop by something less (actually, probably far less) than one‐​tenth of 1 degree Fahrenheit by 2050. What everyone in the scientific community understands but few want to discuss publicly is that stopping global warming — or even slowing it down appreciably — requires the near total abandonment of fossil fuels.



Suggesting otherwise only encourages the public to believe that they can meaningfully address climate change without sacrifice.



Because I’m against economic sacrifice, that’s fine with me. While the planet is indeed warming — probably due in no small part to industrial greenhouse gas emissions — the warming has been modest, benign, and largely confined to northern latitudes during winter nights. There are good reasons to expect that warming pattern to continue. And that warming pattern does not threaten to usher in the convulsive climatic events we are warned about in the press or in the movie theaters. In fact, some scientists and economists can make a pretty good case that global warming will prove a net plus to both the economy and the global environment.



So perhaps I should applaud California’s regulations. Unfortunately, this empty gesture isn’t completely cost‐​free. 



The California Air Resources Board estimates that their plan will add about $1,000 to the cost of a new car by 2015. Question: How many people would be willing to pay a $1,000 tax each time they buy a new car to reduce global temperatures 46 years from now by a number too small to measure? How many economists could you find who’d accept that such a tax passes any sane cost‐​benefit test? Not many, I’ll bet.



When pressed on this, environmentalists counter that anything that begins the long regulatory journey they envision before us is valuable in and of itself. But if the first baby steps necessary to initiate this journey are so expensive and so obviously incapable of passing any reasonable cost‐​benefit test, how expensive do you think future steps might be that environmentalists dare not forward at the moment?



Then there are the hidden costs. Because there are no technologies available to remove greenhouse gas emissions from automobile tailpipes, the only way to comply with the California mandate is to improve auto fuel efficiency. Yet improving fuel efficiency reduces the marginal cost of driving one’s car, and economists have demonstrated that people respond to those lower driving costs by actually driving more. 



What does more driving mean? More pollution, that’s what. Economist Andrew Kleit of Pennsylvania State University calculates that a 50 percent increase in the fuel efficiency of the automobile fleet — essentially what California is requiring through these regulations — will increase net automobile emissions of volatile organic compounds by 1.9 percent, nitrogen oxides by 3.4 percent, and carbon monoxide by 4.6 percent. In other words, environmentalists are asking us to trade off an infinitesimal reduction in global temperature for more smog than we might experience otherwise.



This wouldn’t be the case if environmentalists were more honest with the public about the costs associated with their policy prescriptions. The fact is that the only way to reduce greenhouse gas emissions is to increase the cost of fossil fuels via an implicit or explicit tax. But imposing a fuel tax makes the cost of these policies transparent and, environmentalists fear, difficult to defend politically.



They’re right.
"
"

Many long‐​held dreams were finally realized on Saturday. The enlargement of the European Union saw eight countries from Central and Eastern Europe, along with Malta and Cyprus, return home after a long and eventful journey, reuniting the continent after 50 years of division. The enlargement has been hailed by some as the “end of history.” But while this is a historic occasion that offers a good opportunity to remember heroes of the struggle for freedom on the both sides of the Iron Curtain, the history of Europe is certainly not ending. In fact it is just beginning.



The effects of EU enlargement will go deeper than most people are predicting, changing Europe beyond recognition. From the core bureaucratic structure of the union itself, to fiscal policy and ideological identity, it will not be possible for the union to continue as before. The current structure will be unmanageable with 25 member states and will have to undergo a radical overhaul. The Common Agricultural Policy will have to be reformed, as there simply isn’t enough money to extend it to new member states in its present form.



Meanwhile, the enlarged EU will have to reconcile the low‐​tax, low‐​regulation economies of the incoming members with the high‐​tax, high‐​regulation policies in much of the existing union. This will be the great tension within the union over the next decade.



Nearly all the new member states — even those more inclined to the left — have introduced far‐​reaching and necessary economic reforms. They have undergone the most radical fiscal reforms and abolished most of their subsidies. They have privatized their economies and opened them to foreign investment. Social‐​security networks and pension systems have been overhauled. The role of agriculture in GDP has been dramatically decreased. The business climate in new member states is open, the labor market is not overregulated, the tax burden is low.



But if “old Europe” is to compete effectively with “new Europe,” it will have to lower taxes and rethink the social‐​welfare systems that high taxation supports. Ten years ago Estonia became the first country in Europe to introduce flat rate proportional personal income tax, a policy designed to energize our people and stimulate growth. It was a huge success. Latvia and Lithuania followed, then Russia, Ukraine and now Slovakia. We can only sit back and wait for the next dominoes to fall. It looks quite possible that within five years the whole of Central and Eastern Europe will move to flat‐​rate income taxes.



Such developments put pressure on old Europe, pushing down taxes in countries neighboring new member states, and so creating more room for investment and development. But this has also made old Europe nervous. Enthusiasts of social welfare see enlargement as a serious threat to European civilization, akin to the barbarian invasions at the end of the Roman Empire.



The welfare state is considered a core part of European identity, despite its negative impact on European competitiveness, and its long‐​term unsustainability. Swedish Prime Minister Goran Persson and German Chancellor Gerhard Schroeder have complained that the rich are not taxed heavily enough in new member states and are seeking to extend the EU’s power into areas of taxation and take away the national veto in a number of areas of regulation. Both of these would have a negative impact on the open economies of new member states.



The danger is that such moves will crystallize the problems inherent in the economies of current members and export them to EU level, interfering negatively with those economies that have increased competitiveness through radical reforms — reforms that have been suggested by every EU panel and expert group over the past decade, but which have been consistently rejected by European leaders for domestic political reasons.



Enlargement should be the catalyst that at last forces their hand. Europe’s economic malaise must be confronted if it is to compete with its global rivals. The Continent needs a clear vision and a new agenda for the 21st century. The enlargement should provide the impetus to work out this agenda and regain the momentum for reform. New member states are poor today and still bear the burden of their communist heritage, but if they stiffen their resolve and maintain their liberal approach and open economies they may succeed not only in improving their own countries and economies, but also in injecting all Europe with a new dynamism and momentum for reform.



It is essential that new members, along with Britain, resist attempts to introduce tax harmonization and increase the burden of regulation in the enlarged EU, sucking vibrant new members into the old stagnant Europe — a Europe that is slowly but steadily losing its importance in the world, and consistently lagging behind its global competitors economically. This is the moment when Europe can move in a new direction. If everybody dreaming of a new Europe works together, then they can do miracles. They can make all Europe new again.
"
"
Since we’ve been talking about snow quite a bit recently, this seems fitting. WUWT reader Tom in Texas tips us to this image:

A composite of archival Hubble data taken with the Wide Field Planetary Camera    2 and the Advanced Camera for Surveys. Like a whirl of shiny flakes    sparkling in a snow globe, Hubble caught this glimpse of stars in  the    globular cluster M13. The cluster is home to over 100,000 stars, packed    closely together in a ball approximately 150 light-years across, and is    located at a distance of 25,000 light-years. Picture: AFP / NASA / ESA 
Click    here to see a high-res version of the Hubble snow globe I wonder what the sky would look like from a world in the center of that cluster? Would some of the stars look like bright marbles in the sky?
If you really want to see some interesting things from the HST, have a look at this gallery:
Hubble Space Telescope Advent Calendar 2009
Like the photo above, it gives some perspective about our place and scale in the universe. 





			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8fb5ccdc',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Exactly a year ago, the world was wrestling with the possibility of another US-led military assault on an Arab state, following the horrific gas attacks in Damascus, Syria.  When US military action did come in early August this year, it was in northern Iraq against the Islamic State (IS) which evolved out of the Syrian civil war. In the context of the spiralling humanitarian crisis, swift and co-ordinated IS advances, and single acts of astonishing barbarity, ongoing US attacks have become focused on control of a dam. It is the latest and most visible chapter in the world’s growing water crisis and confirmation of water’s central role in conflicts. The Mosul Dam blocks the Tigris River south of the Turkish border, forming a reservoir 11 billion cubic metres in volume – the fourth largest in the Middle East. Much of the military rhetoric has focused on the potential for deliberate destruction of the structure, releasing catastrophic flood waves reaching 4.6m high as far downstream as Baghdad, 350km away. But politically and economically it is the control of the dam’s hydroelectricity which gives it priority. Engineers, meanwhile, noting the reservoir’s unorthodox setting (on water-soluble karstic geology ) fear an accidental breach of the dam if vital geotechnical work, including continuous injection of impermeable grout, is not properly maintained. Strategically, the use of the dam to determine water levels and supplies to large parts of the country makes it the largest prize in what security analysts describe as a “battle for control of water” which many observers see as defining IS’s aims in Iraq.  This plan was evident as early as June this year, following extensive flooding caused by the deliberate closure of the captured Nuaimiyah Dam west of Baghdad.  But this is not the first time water has been used as a weapon in the “Fertile Crescent” at the confluence of the Tigris and Euphrates rivers. Saddam Hussein targeted water resources during the Iran-Iraq War and his oppression of the Marsh Arabs in southern Iraq during the 1990s centred on the drainage of 6,000 km2 of wetlands, destroying a subsistence economy perhaps 10,000 years old. This was a “war by other means”, according to engineer Azzam Alwash, who won the 2013 Goldman Environmental prize for his post-2003 work to re-establish the marshlands. The tactical use of water supplies in war dates back almost as far as civilisation itself. Limiting and depleting water supplies has been used as a siege weapon throughout history. The “Dambusters” are even part of the UK’s popular cultural memory of World War Two. But is the current zeitgeist – that this century will be marked by wars dominated by water – representative of a real or imagined threat? The UN was widely seen to endorsed this thesis in its 2009 World Water Development Report. Shortly after, an opinion article in the journal Nature roundly rejected it, claiming instead that “inequitable access to water resources is a result of…broader conflict and power dynamics: it does not itself cause war” and concluding that wars over water are a myth which distract from a globally progressive approach to co-operation in water management. So which position is correct? Mark Zeitoun, an expert on Middle East water politics, has developed a theory of “hydro-hegemony” in which control over water supplies is an intrinsic component of unequal power relationships. This is perhaps nowhere better illustrated than in relations between Israel and its neighbours which shift constantly and all-too-visibly from armed to unarmed conflicts, encompassing unilateral annexation of both land and water resources as well as uneasy bilateral agreements. In this view, water is an integral component of all kinds of conflict, from cultural antagonism to military aggression. It follows that as global demand for water grows and areas already experiencing water stress suffer further under predicted climate change, then the importance of water in tensions at all scales will grow proportionally.  Water is at the heart of many conflicts worldwide, whether between nations such as Egypt and Ethiopia, where diplomatic tensions are high regarding the construction of the massive Grand Renaissance Dam on the Nile; between developing world communities and multinational corporations, for example Coca-cola in India; or between regions within countries, such as in the western US where various states are in legal battles over the Rio Grande. We should remain confident that the strong frameworks of national and international law will continue to confine many of these conflicts to council chambers and diplomatic conferences. However, where these mechanisms break down then a shift on the spectrum of conflict towards violent confrontations, shaped by our fundamental human need for water, does seem possible if not inevitable. In the past months in northern Iraq, from an escalating Syrian crisis in which water stress likely played a destabilising part, we may have seen the first shots fired."
"

Following the dubious example set recently by U.S. legislators, French politicians have informally proposed slapping punitive tariffs on goods from countries who refuse to curb greenhouse gas emissions. The German State Secretary for the Environment has, quite rightly, called foul: 



There are two problems — the WTO (World Trade Organization), and the signal would be that this is a new form of eco‐​imperialism,” Machnig said.   
  
  
“We are closing our markets for their products, and I don’t think this is a very helpful signal for the international negotiations.”



I have a paper forthcoming on the carbon tariff issue, but in the meantime here’s a recent op‐​ed (written jointly with Pat Michaels) on climate change policy mis‐​steps.
"
"
Share this...FacebookTwitterLooks like Thanksgiving is early this year. Here’s a feast for kings. 🙂
http://foia2011.org/index.php?id=2
Update: The real doozies are marked in bold print. having gone through them, I have to say sorry, but these scientists are nothing but liars. If the governments don’t act now on this, then we are truly living in anarchy. It will mean that all has turned into a free for all.
/// The IPCC Process ///
Thorne/MetO:
Observations do not show rising temperatures throughout the tropical
 troposphere unless you accept one single study and approach and discount a
wealth of others. This is just downright dangerous. We need to communicate the
uncertainty and be honest. Phil, hopefully we can find time to discuss these
further if necessary […]
Thorne:
I also think the science is being manipulated to put a political spin on it
which for all our sakes might not be too clever in the long run.
Carter:
It seems that a few people have a very strong say, and no matter how much
talking goes on beforehand, the big decisions are made at the eleventh hour by
a select core group.
Wigley:
Mike, The Figure you sent is very deceptive […] there have been a number of
 dishonest presentations of model results by individual authors and by IPCC […]
Overpeck:
The trick may be to decide on the main message and use that to guid[e] what’s
included and what is left out.
Overpeck:
I agree w/ Susan [Solomon] that we should try to put more in the bullet about
“Subsequent evidence” […] Need to convince readers that there really has been
an increase in knowledge – more evidence. What is it?
Wanner/NCCR:
In my [IPCC-TAR] review […] I crit[i]cized […] the Mann hockey[s]tick […]
My review was classified “unsignificant” even I inquired several times. Now the
internationally well known newspaper SPIEGEL got the information about these
early statements because I expressed my opinion in several talks, mainly in
Germany, in 2002 and 2003. I just refused to give an exclusive interview to
SPIEGEL because I will not cause damage for climate science.
Coe:
Hence the AR4 Section 2.7.1.1.2 dismissal of the ACRIM composite to be
instrumental rather than solar in origin is a bit controversial. Similarly IPCC
in their discussion on solar RF since the Maunder Minimum are very dependent on
the paper by Wang et al (which I have been unable to access) in the decision to
reduce the solar RF significantly despite the many papers to the contrary in
the ISSI workshop. All this leaves the IPCC almost entirely dependent on CO2
for the explanation of current global temperatures as in Fig 2.23. since
methane CFCs and aerosols are not increasing.
Briffa:
I find myself in the strange position of being very skeptical of the quality of
all present reconstructions, yet sounding like a pro greenhouse zealot here!
Jones:
I too don’t see why the schemes should be symmetrical. The temperature ones
certainly will not as we’re choosing the periods to show warming.

Trenberth:
[…] opposing some things said by people like Chris Landsea who has said all the
stuff going on is natural variability. In addition to the 4 hurricanes hitting
Florida, there has been a record number hit Japan 10?? and I saw a report
saying Japanese scientists had linked this to global warming. […] I am leaning
toward the idea of getting a box on changes in hurricanes, perhaps written by a
Japanese.
Jones:
We can put a note in that something will be there in the next draft, or Kevin
or I will write something – it depends on whether and what we get from Japan.
Jones:
Kevin, Seems that this potential Nature paper may be worth citing, if it does
say that GW is having an effect on TC activity.
Jones:
Getting people we know and trust [into IPCC] is vital – hence my comment about
the tornadoes group.
Jones:
Useful ones [for IPCC] might be Baldwin, Benestad (written on the solar/cloud
issue – on the right side, i.e anti-Svensmark), Bohm, Brown, Christy (will be
have to involve him ?)
Stott/MetO:
My most immediate concern is to whether to leave this statement [“probably the
warmest of the last millennium”] in or whether I should remove it in the
anticipation that by the time of the 4th Assessment Report we’ll have withdrawn
this statement – Chris Folland at least seems to think this is possible.
/// Communicating Climate Change ///
Humphrey/DEFRA:
I can’t overstate the HUGE amount of political interest in the project as a
message that the Government can give on climate change to help them tell their
story. They want the story to be a very strong one and don’t want to be made
to look foolish.
Fox/Environment Agency:
if we loose the chance to make climate change a reality to people in the
regions we will have missed a major trick in REGIS.
Adams:
Somehow we have to leave the[m] thinking OK, climate change is extremely
complicated, BUT I accept the dominant view that people are affecting it, and
that impacts produces risk that needs careful and urgent attention.
Lorenzoni:
I agree with the importance of extreme events as foci for public and
governmental opinion […] ‘climate change’ needs to be present in people’s
daily lives. They should be reminded that it is a continuously occurring and
evolving phenomenon
Jones:
We don’t really want the bullshit and optimistic stuff that Michael has written
[…] We’ll have to cut out some of his stuff.
Mann:
the important thing is to make sure they’re loosing the PR battle. That’s what
 the site [Real Climate] is about.
Ashton/co2.org:
Having established scale and urgency, the political challenge is then to turn
 this from an argument about the cost of cutting emissions – bad politics – to
 one about the value of a stable climate – much better politics. […] the most
 valuable thing to do is to tell the story about abrupt change as vividly as
 possible
Kelly:
the current commitments, even with some strengthening, are little different
from what would have happened without a climate treaty.
[…] the way to pitch the analysis is to argue that precautionary action must be
taken now to protect reserves etc against the inevitable
Singer/WWF:
we as an NGO working on climate policy need such a document pretty soon for the
public and for informed decision makers in order to get a) a debate started and
b) in order to get into the media the context between climate
 extremes/desasters/costs and finally the link between weather extremes and
 energy
Torok/CSIRO:
[…] idea of looking at the implications of climate change for what he termed
“global icons” […] One of these suggested icons was the Great Barrier Reef […]
It also became apparent that there was always a local “reason” for the
destruction – cyclones, starfish, fertilizers […] A perception of an
“unchanging” environment leads people to generate local explanations for coral
loss based on transient phenomena, while not acknowledging the possibility of
systematic damage from long-term climatic/environmental change […] Such a
project could do a lot to raise awareness of threats to the reef from climate
change
Minns/Tyndall Centre:
In my experience, global warming freezing is already a bit of a public
 relations problem with the media
Kjellen:
I agree with Nick that climate change might be a better labelling than global
 warming
Pierrehumbert:
What kind of circulation change could lock Europe into deadly summer heat waves
like that of last summer? That’s the sort of thing we need to think about.
/// The Medieval Warm Period ///
Pollack:
But it will be very difficult to make the MWP go away in Greenland.
Rahmstorf:
You chose to depict the one based on C14 solar data, which kind of stands out
in Medieval times. It would be much nicer to show the version driven by Be10
solar forcing
Cook:
A growing body of evidence clearly shows [2008] that hydroclimatic variability
during the putative MWP (more appropriately and inclusively called the
“Medieval Climate Anomaly” or MCA period) was more regionally extreme (mainly
in terms of the frequency and duration of megadroughts) than anything we have
seen in the 20th century, except perhaps for the Sahel. So in certain ways the
 MCA period may have been more climatically extreme than in modern times.
/// The Settled Science ///
Warren:
The results for 400 ppm stabilization look odd in many cases […] As it stands
we’ll have to delete the results from the paper if it is to be published.
Wils:
[2007] What if climate change appears to be just mainly a multidecadal natural
 fluctuation? They’ll kill us probably […]
Wilson:
Although I agree that GHGs are important in the 19th/20th century (especially
 since the 1970s), if the weighting of solar forcing was stronger in the models,
 surely this would diminish the significance of GHGs.
[…] it seems to me that by weighting the solar irradiance more strongly in the
 models, then much of the 19th to mid 20th century warming can be explained from
 the sun alone.
Hoskins:
If the tropical near surface specific humidity over tropical land has not gone
up (Fig 5) presumably that could explain why the expected amplification of the
warming in the tropics with height has not really been detected.
Jenkins/MetO:
would you agree that there is no convincing evidence for kilimanjaro glacier
melt being due to recent warming (let alone man-made warming)?
Jones:
[tropical glaciers] There is a small problem though with their retreat. They
have retreated a lot in the last 20 years yet the MSU2LT data would suggest
that temperatures haven’t increased at these levels.
Jones:
There shouldn’t be someone else at UEA with different views [from “recent
 extreme weather is due to global warming”] – at least not a climatologist.
Crowley:
I am not convinced that the “truth” is always worth reaching if it is at the
 cost of damaged personal relationships
Briffa:
Also there is much published evidence for Europe (and France in particular) of
increasing net primary productivity in natural and managed woodlands that may
be associated either with nitrogen or increasing CO2 or both. Contrast this
with the still controversial question of large-scale acid-rain-related forest
decline? To what extent is this issue now generally considered urgent, or even
real?
Crowley:
Phil, thanks for your thoughts – guarantee there will be no dirty laundry in
 the open.
Steig:
He’s skeptical that the warming is as great as we show in East Antarctica — he
 thinks the “right” answer is more like our detrended results in the
 supplementary text. I cannot argue he is wrong.
Jones:
This will reduce the 1940-1970 cooling in NH temps. Explaining the cooling with
 sulphates won’t be quite as necessary.
Haimberger:
It is interesting to see the lower tropospheric warming minimum in the tropics
 in all three plots, which I cannot explain. I believe it is spurious but it is
 remarkably robust against my adjustment efforts.
Klein/LLNL:
Does anybody have an explanation why there is a relative minimum (and some
 negative trends) between 500 and 700 hPa? No models with significant surface
 warming do this
Osborn:
This is an excellent idea, Mike, IN PRINCIPLE at least. In practise, however,
it raises some interesting results […] the analysis will not likely lie near to
the middle of the cloud of published series and explaining the reasons behind
this etc. will obscure the message of a short EOS piece.
Norwegian Meteorological Institute:
In Norway and Spitsbergen, it is possible to explain most of the warming after
 the 1960s by changes in the atmospheric circulation. The warming prior to 1940
cannot be explained in this way.
/// The Urban Heat Effect ///
Jenkins/MetO:
By coincidence I also got recently a paper from Rob which says “London’s UHI
has indeed become more intense since the 1960s esp during spring and summer”.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Jones:
I think the urban-related warming should be smaller than this, but I can’t
think of a good way to argue this. I am hopeful of finding something in the
data that makes by their Figure 3.
Rean:
[…] we found the [urban warming] effect is pretty big in the areas we analyzed.
This is a little different from the result you obtained in 1990.
[…] We have published a few of papers on this topic in Chinese. Unfortunately,
when we sent our comments to the IPCC AR4, they were mostly rejected.
Wigley:
there are some nitpicky jerks who have criticized the Jones et al. data sets —
we don’t want one of those [EPRI/California Energy Commission meeting].
Jones:
The jerk you mention was called Good(e)rich who found urban warming at
all Californian sites.
Jones:
I think China is one of the few places that are affected [urban heat]. The
paper shows that London and Vienna (and also New York) are not affected in the
20th century.
Jones:
[…] every effort has been made to use data that are either rural and/or where
the urbanization effect has been removed as well as possible by statistical
means. There are 3 groups that have done this independently (CRU, NOAA and
GISS), and they end up with essentially the same results.
[…] Furthermore, the oceans have warmed at a rate consistent with the land.
There is no urban effect there.
/// Temperature Reconstructions ///
Wilson:
any method that incorporates all forms of uncertainty and error will
undoubtedly result in reconstructions with wider error bars than we currently
have. These many be more honest, but may not be too helpful for model
comparison attribution studies. We need to be careful with the wording I think.
Jones:
what he [Zwiers] has done comes to a different conclusion than Caspar and Gene!
I reckon this can be saved by careful wording.
Mitchell/MetO
Is the PCA approach robust? Are the results statistically significant? It seems
 to me that in the case of MBH the answer in each is no
Wilson:
I thought I’d play around with some randomly generated time-series and see if I
could ‘reconstruct’ northern hemisphere temperatures.
[…] The reconstructions clearly show a ‘hockey-stick’ trend. I guess this is
precisely the phenomenon that Macintyre has been going on about.
Bradley:
I’m sure you agree–the Mann/Jones GRL paper was truly pathetic and should
never have been published. I don’t want to be associated with that 2000 year
“reconstruction”.
Osborn:
Because how can we be critical of Crowley for throwing out 40-years in the
middle of his calibration, when we’re throwing out all post-1960 data ‘cos the
MXD has a non-temperature signal in it, and also all pre-1881 or pre-1871 data
‘cos the temperature data may have a non-temperature signal in it!
Esper:
Now, you Keith complain about the way we introduced our result, while saying it
is an important one. […] the IPCC curve needs to be improved according to
missing long-term declining trends/signals, which were removed (by
dendrochronologists!) before Mann merged the local records together. So, why
 don’t you want to let the result into science?
Cook:
I am afraid that Mike is defending something that increasingly can not be
 defended. He is investing too much personal stuff in this and not letting the
 science move ahead.
Cook:
One problem is that he [Mann] will be using the RegEM method, which provides no
better diagnostics (e.g. betas) than his original method. So we will still not
know where his estimates are coming from.
/// Science and Religion ///
Wigley:
I heard that Zichichi has links with the Vatican. A number of other greenhouse
 skeptics have extreme religious views.
Houghton [MetO, IPCC co-chair]
[…] we dont take seriously enough our God-given responsibility to care for the
 Earth […] 500 million people are expected to watch The Day After Tomorrow. We
must pray that they pick up that message.
Hulme:
My work is as Director of the national centre for climate change research, a
job which requires me to translate my Christian belief about stewardship of
 God’s planet into research and action.
Hulme:
He [another Met scientist] is a Christian and would talk authoritatively about
the state of climate science from the sort of standpoint you are wanting.
/// Climate Models ///
Watson/UEA:
I’d agree probably 10 years away to go from weather forecasting to ~ annual
scale. But the “big climate picture” includes ocean feedbacks on all time
 scales, carbon and other elemental cycles, etc. and it has to be several
decades before that is sorted out I would think. So I would guess that it will
not be models or theory, but observation that will provide the answer to the
question of how the climate will change in many decades time.
Shukla/IGES:
[“Future of the IPCC”, 2008] It is inconceivable that policymakers will be
willing to make billion-and trillion-dollar decisions for adaptation to the
projected regional climate change based on models that do not even describe and
simulate the processes that are the building blocks of climate variability.
Lanzante/NOAA:
While perhaps one could designate some subset of models as being poorer in a
lot of areas, there probably never will be a single universally superior model
or set of models. We should keep in mind that the climate system is complex, so
that it is difficult, if not impossible to define a metric that captures the
breath of physical processes relevant to even a narrow area of focus.
Santer:
there is no individual model that does well in all of the SST and water vapor
 tests we’ve applied.
Barnett:
[IPCC AR5 models] clearly, some tuning or very good luck involved. I doubt the
 modeling world will be able to get away with this much longer
Hegerl:
[IPCC AR5 models]
So using the 20th c for tuning is just doing what some people have long
 suspected us of doing […] and what the nonpublished diagram from NCAR showing
correlation between aerosol forcing and sensitivity also suggested.
Jones:
Basic problem is that all models are wrong – not got enough middle and low
 level clouds.
Jones:
GKSS is just one model and it is a model, so there is no need for it to be
correct.
/// The Cause ///
Mann:
By the way, when is Tom C going to formally publish his roughly 1500 year
reconstruction??? It would help the cause to be able to refer to that
reconstruction as confirming Mann and Jones, etc.
Mann:
They will (see below) allow us to provide some discussion of the synthetic
example, referring to the J. Cimate paper (which should be finally accepted
upon submission of the revised final draft), so that should help the cause a
 bit.
Mann:
I gave up on Judith Curry a while ago. I don’t know what she think’s she’s
 doing, but its not helping the cause
Berger:
Phil,
Many thanks for your paper and congratulations for reviving the global warming.
Jones:
[on temperature data adjustments] Upshot is that their trend will increase
Jones:
[to Hansen] Keep up the good work! […] Even though it’s been a mild winter in
the UK, much of the rest of the world seems coolish – expected though given the
La Nina. Roll on the next El Nino!
Schneider:
Even though I am virtually certain we shall lose on McCain-Lieberman, they are
forcing Senators to go on record for for against sensible climate policy
/// Freedom of Information ///
Jones:
I’ve been told that IPCC is above national FOI Acts. One way to cover yourself
 and all those working in AR5 would be to delete all emails at the end of the
 process
Briffa:
UEA does not hold the very vast majority of mine [potentially FOIable emails]
anyway which I copied onto private storage after the completion of the IPCC
task.
Osborn:
Keith and I have just searched through our emails for anything containing
“David Holland”. Everything we found was cc’d to you and/or Dave Palmer, which
you’ll already have.
McGarvie/UEA Director of Faculty Administration:
As we are testing EIR with the other climate audit org request relating to
communications with other academic colleagues, I think that we would weaken
that case if we supplied the information in this case. So I would suggest that
we decline this one (at the very end of the time period)
Jones:
[FOI, temperature data]
Any work we have done in the past is done on the back of the research grants we
 get – and has to be well hidden. I’ve discussed this with the main funder (US
 Dept of Energy) in the past and they are happy about not releasing the original
 station data.
/// FOIA 2011 — Background and Context ///
“Over 2.5 billion people live on less than $2 a day.””Every day nearly 16.000 children die from hunger and related causes.”
“One dollar can save a life” — the opposite must also be true.
“Poverty is a death sentence.”
“Nations must invest $37 trillion in energy technologies by 2030 to stabilize
greenhouse gas emissions at sustainable levels.”
Today’s decisions should be based on all the information we can get, not on
hiding the decline.
This archive contains some 5.000 emails picked from keyword searches. A few
remarks and redactions are marked with triple brackets.
The rest, some 220.000, are encrypted for various reasons. We are not planning
to publicly release the passphrase. We could not read every one, but tried to cover the most relevant topics such
as…
 
 
Share this...FacebookTwitter "
"New cars sold in the UK produce more carbon dioxide than older models, according to new research that suggests the industry is going backwards in tackling the climate crisis. Cars that reach the latest standards of emissions use cleaner internal combustion engine technology to combat air pollution, but the relentless rise in demand for bigger, heavier models meant that average emissions of the greenhouse gas rose, according to the consumer group Which? The latest generation of cars produced 7% more emissions than those manufactured to earlier standards, testing of 292 models released in the UK since 2017 found. Cars account for just over 18% of UK emissions, according to government figures, and reining back pollution from the sector is seen as crucial to efforts to cut the country’s carbon emissions to net zero by 2050. Lisa Barber, editor of Which? magazine, said: “It is shocking to see our tests uncover increasing levels of carbon dioxide emissions for the latest cars that are being built and sold to UK consumers. “Manufacturers must ensure that they are doing everything in their power to create cleaner vehicles that are fitter for our planet and its future.” Overall, cars that met the latest emissions regulations (standards known as Euro 6d and Euro 6d-temp) produced 162.1g of CO2 per kilometre, 10.5g more than those in the previous generation (Euro 6b and Euro 6c). That was far above the 95g target carmakers must meet across all EU sales in order to avoid steep fines. Manufacturers across Europe are racing to make and sell new electric models in order to meet the rules, although many are relying on hybrid models that combine internal combustion with battery power. Mike Hawes, chief executive of the Society of Motor Manufacturers and Traders (SMMT), the industry lobby group, said: “We can’t comment on the results of non-official tests by commercial organisations where the methodology is unclear. “Only the official, Europe-wide WLTP [Worldwide Harmonised Light Vehicle Test Procedure] test – the toughest and most comprehensive in the world – can be relied upon by consumers to accurately compare vehicles on a like-for-like and repeatable basis. This shows that new cars emit, on average, some 29.3% less CO2 than models produced in 2000, the effect of which drivers can see at the pump.” However, the new findings tally with SMMT data which found that the average CO2 output of cars sold in the UK has risen for the past three years. Cars sold in the UK in 2019 produced average emissions of 127.9g of CO2 per kilometre. The Which? analysis found that carbon emissions were rising across every segment of the car market, from smaller city cars through to SUVs, as manufacturers packed more technology into their cars. Emissions rose fastest in the hybrid segment, up by 31% between generations, in part because of the weight of two different power sources. Newer cars performed significantly better on air quality issues, with the latest models slashing emissions of carbon monoxide and nitrogen oxides, both of which directly harm human health. The tests also found that carbon emissions were higher than official readings carried out by EU regulators, which do not measure extended use at motorway speeds or take into account a car full of people using the air conditioning and the radio. Doug Parr, chief scientist at Greenpeace, said the figures showed that the government should ban the sale of new petrol and diesel cars from 2030, earlier than current plans to ban internal combustion engines from 2035. “It’s clear that we can’t simply rely on the car industry’s good will to make progress,” he said. “We need decisive intervention from government, starting with enforcing existing rules on car emissions and bringing in a firm ban on sales of new diesel and petrol vehicles by 2030.” • This article was amended on 28 February 2020 because an earlier version misnamed the Society of Motor Manufacturers and Traders, as the Society for Motor Manufacturers and Traders. This has been corrected."
"Octopuses grow quickly, have lots of tasty flesh and are found all over the world. As the world’s supply of fish diminishes while the number of humans keeps increasing, it seems these creatures would make an ideal mass-produced food for our hungry mouths. So where are all the octopus farms? The main thing that prevents octopus farming at large scale is that the common octopus – Octopus vulgaris – is tough to feed in captivity, especially when first born. After hatching, octopuses first exist as tiny organisms known as paralarvae, drifting around the upper ocean in among clouds of plankton which they feed on. It is this stage – before they become fully fledged young adults and descend further into the sea – which is hardest to replicate in aquaculture.  Feeding octopuses adequately during their first two months of life is a challenge. In this period, octopuses have highly selective feeding habits, and acceptable survival rates are hard to achieve. At an industrial scale, the only possible solution is to take wild juveniles caught at sea and grow them in floating sea cages. Fishermen start with individuals of about 800 grams and grow them until they are more than 2-3kg, supplying them with crustaceans and low value fish over a period of three or four months.  Fishermen cooperatives in north-western Spain grow octopus in sea-cages. They sell them at high season – Christmas and summer – where large creatures can reach €10-12 per kilo, double the usual price. So far, research has allowed small-scale production by artisanal producers in Vigo, Galicia, reaching a production of just ten metric tons per year. But this system is highly dependent on the success of the initial catches; without a good crop of smaller octopuses to grow in the cages, end results will always be limited. This is why farmed-raised octopus hasn’t had commercial success yet. Over the past 15 years the Spanish Institute of Oceanography (IEO) in Vigo has carried out important and successful research to overcome the problems with octopus cultivation, and the institute is now focused on rearing octopus across a full life cycle – from hatch to catch. In fact they managed to complete full cultivation across the life cycle of several octopuses for the first time ever in 2001. This experiment was achieved after using live crustacean larvae known as zoeae as prey along with the commonly used artemia, a brine shrimp. However, it is very difficult to obtain these zoeae in large quantities, making large-scale production prohibitively expensive. So researchers are now focusing on analysing the biochemical composition of the larvae to find out what makes them tick – and what makes octopuses find them so tasty (or at least edible). Once identified, the idea is to ensure the cultured enriched artemia has the same features. But once the octopus is up to a certain size, there is another step to be solved: the transition between paralarvae and juvenile. This stage is another mortality peak in octopus farms.  Working with other species that don’t have a paralarvae phase would help, such as the Mexican Four-Eyed Octopus – known scientifically as Octopus maya. Like cuttlefish, these octopuses hatch ready for the deep seas, with all the same features as their adult selves. There is still a transition phase of sorts though, when they still need commercial feed pellets to grow adequately. Octopus maya cultivation represents the most advanced attempts at commercially sound cephalopod aquaculture. But even with this species, it has been necessary to rely on targeting a specialised gourmet-level market. In conclusion, the best octopus farms cannot yet compete directly with the common wild-captured product.  Developments in feeding products and techniques over the next few years will be key. Once small octopuses can be fed in large numbers, the development of a full undersea farming industry will be much easier."
"The Morrison government will on Friday signal plans to shift investment from wind and solar to hydrogen, carbon capture and storage, lithium and advanced livestock feed supplements, as part of a “bottom up” strategy to reduce emissions by 2050. Angus Taylor will use a speech to an economic thinktank to put some flesh on the bones of the Coalition’s much-vaunted technology roadmap. The emissions reduction minister will also declare Australia will take a technology-based long-term emissions reduction “strategy” to the United Nations-led climate talks in Glasgow at the end of this year. While not ruling out adopting a specific emissions reduction target, Taylor will contend the “top down” approach of countries proposing emissions reduction targets in the global climate framework has “failed” because countries are not delivering on their commitments. According to a speech extract circulated in advance, Taylor will say the government intends to roll out “a series of detailed pieces of work” between now and the United Nations climate change conference in Glasgow, known at COP26, in November. Taylor will say Australia wants “to lead the world” on a new approach to laying out domestic abatement plans. As well as the roadmap, the government is reviewing its much-criticised emissions reduction fund and the operation of the safeguard mechanism, and is working on an electric vehicles strategy, despite blasting Labor during last year’s election, claiming measures to drive the takeup of EVs were a “war on the weekend”. Taylor will say the looming technology investment roadmap will form the cornerstone of the government’s 2050 emissions reduction strategy, providing guidance to the public and the private sector on “what future energy and emissions-reduction technologies the government will prioritise”. He will say the government has already invested $10.4bn into clean technology projects with a value of $35bn, but declare “we are coming to an end of the value of these investments”. “Wind and solar are economic as a source of pure energy at least, and the government should not crowd out private sector investment,” the minister will say. “We must move our investments to the next challenges – hydrogen, carbon capture and storage, lithium and advanced livestock feed supplements to name a few”. Taylor will say the roadmap will include “measurable economic goals for technology that will allow us to assess the cost curve for technologies and to give a clear signal for when a technology is commercial”. The government will be a player in investing in emerging technology “as both a market signal and leader” but the objective will be to generate significant private sector investment. “To be successful from both a portfolio and from a technology perspective we must track how much private sector and other investment in R and D and early deployment follows our own investment,” Taylor will say. “To measure the success of the overall portfolio I think we should be aiming for a four or five time multiplier. That is for every dollar invested I want to see four or five dollars from the private sector following over the course of our investments.” It is unclear from the speech extract what policy mechanism will drive the new investments and who will administer the research and development, and it does not address why the government is looking at technology such as carbon capture and storage, or CCS. Most low emissions scenarios for the future consider CCS a potentially important technology if the world is to keep global heating to 1.5C about pre-industrial levels, but only when used with biomass, not fossil fuels. The technology has not proved commercially viable with fossil fuel power generation despite being promised billions in taxpayer support through initiatives such as the CCS flagships program. There are fewer than 20 CCS projects across the globe in the industrial sector. Chevron last year announced a $2.5bn CCS project at one of the country’s largest liquefied natural gas developments, in the Pilbara, had finally begun injecting carbon dioxide underground after three years of delays. Taylor’s speech flags a consultation paper to inform the design of the roadmap. The tone of the speech extract suggests the government is reluctant to adopt a target, having blasted Labor since the opposition adopted a net zero target by 2050 in its first major climate policy announcement since losing the 2019 election. But it is not definitive. On Thursday, the Liberal MP Trent Zimmerman continued his call for the leadership to embrace net zero. “I think it is a target we need to look at,” the member for North Sydney told the ABC. “We have a good target for 2030 but we need to look beyond that as we head into Glasgow.” Taylor will say on Friday: “If I could stand up today – announce a target and see the CO2 reduce – then I would. “If setting a target today would lower emissions – then today would be a short speech. I wouldn’t have to outline our plan and I wouldn’t have to outline all the work that has led to today.” Guardian Australia revealed on Thursday new analysis by ClimateWorks Australia suggesting Australia can achieve a transition to net zero emissions by 2050 with known technologies, but the deployment of low emissions options will need to be accelerated significantly."
"

Suppose you think the federal government is a bloated monstrosity in need of a stomach‐​stapling, extreme makeover. What should be done? Small‐​government types — free‐​market conservatives and libertarians — are increasingly at odds on this question.



Tax hawks like Grover Norquist, of Americans for Tax Reform, maintain that we should “starve the beast”: create pressure on Congress to reduce spending by cutting the government’s intake of taxes and running up deficits. This is the approach prescribed last year by Milton Friedman and Gary Becker, both Nobel Prize‐​winning free‐​market economists, in separate Wall Street Journal op‐​eds. Friedman predicts that “deficits will be an effective… restraint on the spending propensities of the executive branch and the legislature. The public reaction will make that restraint effective.”



However, economist William Niskanen, chairman of the Cato Institute (also my employer), has presented econometric evidence that federal spending tends to increase when tax revenues decline, flatly contradicting the starve‐​the‐​beast theory. Furthermore, according to William Gale and Brennan Kelly of the Brookings Institution, members of Congress who signed the President’s “No New Taxes” pledge were more, not less, likely to vote for spending increases, which is hard to square with the starve‐​the‐​beast theory.



“Starve the beast” is really a conjecture about the psychology of voters and legislators. The idea embodied in Friedman’s statement is that mounting deficits will spur voters to choose representatives who will impose fiscal discipline. But why would voters react that way? Will they be worried about deficits causing rising interest rates, or about the prospect that their children will be stuck with a huge bill?



It seems just as likely that current voters would prefer to have their kids and grandkids foot the bill. In the long run, we’re all dead, and the dead don’t pay taxes. If the doctor gives you a month to live, why not run up the Visa?



Niskanen’s analysis suggests that when current spending is financed by current taxes, voters see it as their money being spent, and so are more motivated to be frugal. But when current spending is financed by debt, voters see it as future voters’ money being spent. If voters prefer to benefit now and have some one else pay later, there is no good reason to think legislators will see deficits as a reason to restrain themselves.



Starve‐​the‐​beast advocates might retort that the theory has yet to be tried. Sure, we’re running record deficits. Sure, we’ve had tax cuts. Sure, most Republicans in Congress nevertheless voted for plush increases in education, defense, Medicare and more. And sure, President Bush has never seen a spending bill he wouldn’t sign. The reason “starve the beast” has yet to kick in, they say, is that things aren’t bad enough yet.



But if the deficit reaches crisis proportions — and it will, quickly, if it continues to grow at the current rate — we should not imagine that the government will rush to contain the crisis by rapidly cutting the fat from government. As George Mason University economist Alex Tabarrok recently argued, “The combination of changing demographics and current tax cuts is seeding our economy for a fiscal ‘perfect storm.’ When the storm hits, there will be a crisis, and… small government rarely does well in a crisis.”



So, given our monumentally huge deficits, and the unsustainability of current policy, should small‐​government folks give up on further tax cuts, at least for now? That’s a harder question than you might think.



For many libertarians and conservatives, cutting taxes is about more than efficiency; it’s about morality. We have a moral claim to the fruits of our labor. Every cent the government takes from us beyond what is strictly necessary to secure our basic rights is a token of injustice. Cutting excess taxes is rectification, a way of making abused taxpayers whole. Therefore, for many proponents of smaller government, passing up a chance at a tax cut, or, worse, defending a tax increase, is a willing perpetuation of injustice.



However, if further tax cuts would _accelerate_ deficit spending, justice would be threatened. Under present conditions, further tax cuts would largely be tax shifts, moving the burden of government spending to future generations. And there is nothing notably moral about raising taxes on the future to subsidize the present.



By promulgating the idea that given a tax cut, spending will take care of itself, advocates of the “starve the beast” theory have helped produce a political climate in which principled vigilance about spending seems unnecessary.



But we need principled political discipline now more than ever. It is not enough to cut out the pork. According to economists Jagadeesh Gokhale of the Cato Institute and Kent Smetters, cutting the entire discretionary budget forever would still not be enough. The real fiscal beasts are Social Security and Medicare. Unless they are tamed by serious reform, they will grow out of control and devour almost all future federal revenues.



A sustainable and just America requires the principled will to eliminate the unconstitutional, the inessential and the ineffective, and the courage to reform Social Security and Medicare today so that future generations will inherit a world at least as well‐​off as our own. 
"
"
UPDATE: Some readers took exception to my title, and I can see why now. I regret my choice of wording for the title. “Regulate its escape into the atmosphere” is where I was going. “Regulate” from my perspective in engineering things and making things work is different than what others might think. I wasn’t implying legislation. Recycling and recovery systems is what was in my mind.  Gas regulator valves and all that. This passage from the story below was my focus: “Since we already know how to capture methane from animals, landfills, and sewage treatment plants at fairly low cost, targeting methane makes sense,”.
I’ve amended the title [in brackets] -Anthony
…
According to the 2007 IPCC AR4  Methane has a “global warming potential” of 25 times that of CO2 over 100 years. Here’s a CH4 budget pie chart. Note that there are several sources where we can manage methane without affecting energy creation. Starting on Methane, rather than CO2, is an idea that I could get behind because it can be recycled and used for many things.



A new paper from Drew Shindell from NASA JPL prompted Roger Pielke Jr. to write:


For years my father has been arguing that:
. . . attempts to “control” the climate system, and to prevent a “dangerous intervention” into the climate system by humans that focuses just on CO2 and a few other greenhouse gases will necessarily be significantly incomplete, unless all of the other first order climate forcings are considered.
His views are now being robustly vindicated as a quiet revolution is occurring in climate science.  Here is how PhysOrg reports on a study out today in Science by NASA’s Drew Shindell and others:
According to Shindell, the new findings underscore the importance of devising multi-pronged strategies to address climate change rather than focusing exclusively on carbon dioxide. “Our calculations suggest that all the non-carbon dioxide greenhouse gases together have a net impact that rivals the warming caused by carbon dioxide.”
In particular, the study reinforces the idea that proposals to reduce methane may be an easier place for policy makers to start climate change agreements. “Since we already know how to capture methane from animals, landfills, and sewage treatment plants at fairly low cost, targeting methane makes sense,” said Michael MacCracken, chief scientist for the Climate Institute in Washington, D.C.
This research also provides regulators insight into how certain pollution mitigation strategies might simultaneously affect climate and air quality. Reductions of carbon monoxide, for example, would have positive effects for both climate and the public’s health, while reducing nitrogen oxide could have a positive impact on health but a negative impact on the climate.
“The bottom line is that the chemistry of the atmosphere can get hideously complicated,” said Schmidt. “Sorting out what affects climate and what affects air quality isn’t simple, but we’re making progress.”
Of note, Shindell et al. cautiously suggest that the entire framework of international climate policy may be based on an overly-simplistic view of the human effect on climate, by focusing on carbon dioxide equivalencies in radiative forcing (i.e.,g “global warming potential” or GWP), from their Science paper out today (emphasis added):
There are many limitations to the GWP concept (25). It includes only physical properties, and its definition is equivalent to an unrealistic economic scenario of no discounting through the selected time horizon followed by discounting to zero value thereafter. The 100-year time horizon conventionally chosen strongly reduces the influence of species that are short-lived relative to CO2. Additionally, GWPs assume that integrated global mean RF is a useful indicator of climate change. Although this is generally reasonable at the global scale, GWP does not take into account the rate of change, and it neglects that the surface temperature response to regionally distributed forcings depends on the location of the RF (26) and that precipitation and circulation responses may be even more sensitive to RF location (27). Along with their dependence on emission timing and location, this makes GWPs particularly ill-suited to very short-lived species such as NOx, SO2, or ammonia, although they are more reasonable for longer-lived CO. Inclusion of short-lived species in agreements alongside long-lived greenhouse gases is thus problematic (28, 29).


Read his complete commentary here


Here’s the press release from NASA/JPL with comments from Drew Shindel also.


Interactions with Aerosols Boost Warming Potential of Some Gases



Surface Methane - Credit NASA Goddard
This map shows the distribution of methane at the surface. New research shows that methane has an elevated warming effect due to its interactions with other substances in the atmosphere.   For decades, climate scientists have worked to identify and measure key substances — notably greenhouse gases and aerosol particles — that affect Earth’s climate. And they’ve been aided by ever more sophisticated computer models that make estimating the relative impact of each type of pollutant more reliable.
Yet the complexity of nature — and the models used to quantify it — continues to serve up surprises. The most recent? Certain gases that cause warming are so closely linked with the production of aerosols that the emissions of one type of pollutant can indirectly affect the quantity of the other. And for two key gases that cause warming, these so-called “gas-aerosol interactions” can amplify their impact.
“We’ve known for years that methane and carbon monoxide have a warming effect,” said Drew Shindell, a climate scientist at the NASA Goddard Institute for Space Studies (GISS) in New York and lead author of a study published this week in Science. “But our new findings suggest these gases have a significantly more powerful warming impact than previously thought.”
Mixing a Chemical Soup
When vehicles, factories, landfills, and livestock emit methane and carbon monoxide into the atmosphere, they are doing more than just increasing their atmospheric concentrations. The release of these gases also have indirect effects on a variety of other atmospheric constituents, including reducing the production of particles called aerosols that can influence both the climate and the  air quality. These two gases, as well as others, are part of a complicated cascade of chemical reactions that features competition with aerosols for highly reactive molecules that cleanse the air of pollutants.

“Emissions-based” estimates highlight the indirect effects that emissions of certain gases can have on the climate via aerosols, methane, ozone, and other substances in the atmosphere. Credit: NASA/GISS › Larger image
Aerosols can have either a warming or cooling effect, depending on their composition, but the two aerosol types that Shindell modeled — sulfates and nitrates — scatter incoming light and affect clouds in ways that cool Earth. They are also related to the formation of acid rain and can cause respiratory distress and other health problems for those who breathe them.
Human activity is a major source of sulfate aerosols, but smokestacks don’t emit sulfate particles directly. Rather, coal power production and other industrial processes release sulfur dioxide — the same gas that billows from volcanoes — that later reacts with atmospheric molecules called hydroxyl radicals to produce sulfates as a byproduct. Hydroxyl is so reactive scientists consider it an atmospheric “detergent” or “scrubber” because it cleanses the atmosphere of many types of pollution.
In the chemical soup of the lower atmosphere, however, sulfur dioxide isn’t the only substance interacting with hydroxyl. Similar reactions influence the creation of nitrate aerosols. And hydroxyls drive long chains of reactions involving other common gases, including ozone.
Methane and carbon monoxide use up hydroxyl that would otherwise produce sulfate, thereby reducing the concentration of sulfate aerosols. It’s a seemingly minor change, but it makes a difference to the climate. “More methane means less hydroxyl, less sulfate, and more warming,” Shindell explained.
 Many atmospheric pollutants compete for access to hydroxyl radicals (OH), highly reactive molecules that “scrub” the atmosphere of pollutants. This diagram illustrates hydroxyl converting methane (CH4) into carbon dioxide (CO2) and sulfur dioxide (SO2) into sulfate aerosols. Credit: NASA/GISS › Larger image
His team’s modeling experiment, one of the first to rigorously quantify the impact of gas-aerosol interactions on both climate and air quality, showed that increases in global methane emissions have caused a 26 percent decrease in hydroxyl and an 11 percent decrease in the number concentration of sulfate particles. Reducing sulfate unmasks methane’s warming by 20 to 40 percent over current estimates, but also helps reduce negative health effects from sulfate aerosols.
In comparison, the model calculated that global carbon monoxide emissions have caused a 13 percent reduction in hydroxyl and 9 percent reduction in sulfate aerosols.
Nitrogen oxides — pollutants produced largely by power plants, trucks, and cars — led to overall cooling when their effects on aerosol particles are included, said Nadine Unger, another coauthor on the paper and a climate scientist at GISS. That’s noteworthy because nitrogen oxides have primarily been associated with ozone formation and warming in the past.
A New Approach
To determine the climate impact of particular greenhouse gases, scientists have traditionally relied on surface stations and satellites to measure the concentration of each gas in the air. Then, they have extrapolated such measurements to arrive at a global estimate.
The drawback to that “abundance-based approach,” explained Gavin Schmidt, another GISS climate scientist and coauthor of the study, is that it doesn’t account for the constant interactions that occur between various atmospheric constituents. Nor is it easy to parse out whether pollutants have human or natural origins.
Natural sources of methane include wetlands, termites, decomposing organic materials in ocean and fresh water, and a type of ice called methane hydrate. Man-made methane sources include livestock, rice paddies, biomass burning, landfills, coal mining, and gas production. Credit: U.S Dept. of Energy Technology Laboratory
› Larger image “You get a much more accurate picture of how human emissions are impacting the climate — and how policy makers might effectively counteract climate change — if you look at what’s emitted at the surface rather than what ends up in the atmosphere,” said Shindell, who used this “emissions-based” approach as the groundwork for this modeling project.
However, the abundance-based approach serves as the foundation of key international climate treaties, such as the Kyoto Protocol or the carbon dioxide cap-and-trade plans being discussed among policymakers. Such treaties underestimate the contributions of methane and carbon monoxide to global warming, Shindell said.
Unpacking the Implications
According to Shindell, the new findings underscore the importance of devising multi-pronged strategies to address climate change rather than focusing exclusively on carbon dioxide. “Our calculations suggest that all the non-carbon dioxide greenhouse gases together have a net impact that rivals the warming caused by carbon dioxide.”
In particular, the study reinforces the idea that proposals to reduce methane may be an easier place for policy makers to start climate change agreements. “Since we already know how to capture methane from animals, landfills, and sewage treatment plants at fairly low cost, targeting methane makes sense,” said Michael MacCracken, chief scientist for the Climate Institute in Washington, D.C.
This research also provides regulators insight into how certain pollution mitigation strategies might simultaneously affect climate and air quality. Reductions of carbon monoxide, for example, would have positive effects for both climate and the public’s health, while reducing nitrogen oxide could have a positive impact on health but a negative impact on the climate.
“The bottom line is that the chemistry of the atmosphere can get hideously complicated,” said Schmidt. “Sorting out what affects climate and what affects air quality isn’t simple, but we’re making progress.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e91dd7f8d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Thank you very much for this opportunity to testify on mandatory retirement age regulations in the United States. I am very honored by it.1



Ideally, the decision about when to retire should be made voluntarily by workers in response to labor market conditions. Mandatory retirement age rules have been eliminated in most private sector jobs as a result of anti‐​age‐​discrimination laws that were introduced beginning in the 1960s. Were they allowed, however, private sector employers would likely incorporate them in employment contracts designed for ensuring and improving worker‐​efficiency. Instead, private firms structure long‐​term incentive contracts including features of defined benefit pension plans, other non‐​wage benefits, and severance packages to induce early job terminations. Retirement incentives incorporated in such long‐​term incentive contracts appear to have spurred the trend toward earlier retirement in the United States.



Mandatory retirement age rules still prevail in some private and public‐​sector occupations: State and local police (55–60) and firefighters (55–60); federal firefighters (57); federal law enforcement and corrections officers (57); and air traffic controllers (56, if hired after 1972); and commercial airline pilots (60). These are “earlier‐​than‐​normal” retirement ages compared to the vast majority of other occupations.



Mandatory retirement age restrictions were introduced in these occupations several decades ago, primarily for ensuring their safe and effective conduct. Under today’s conditions, however, these retirement age rules appear to be outdated. The need to revise these rules appears urgent due to impending worker shortages. It appears desirable to introduce long‐​term incentive compensation structures in these jobs similar to those in the private sector. However, the manner in which they should be introduced and whether they would deliver retirement choices to workers while simultaneously ensuring safe and effective job performance requires further examination.



My testimony is comprised of several parts. First I report findings on private sector compensation structures that are designed to elicit worker efficiency and loyalty, and yet induce timely retirements. The findings suggest that these contracts involve divorcing current productivity from current compensation–by postponing compensation from workers’ early‐ to late‐​career stages. Such contracts would not be profitable if workers stayed on the job to collect wages in excess of their productivity for too long. Thus, it appears that private firms could put mandatory retirement age rules to good use.



However, these “incentive” contracts would become infeasible if employers terminated workers too early to avoid paying them seniority rents. This makes the case for anti‐​age discrimination rules. As explained below, it turns out that prohibitions against age discrimination are more useful than mandatory retirement age rules in making such contracts feasible. Evidence on U.S. firms suggests the prevalence of long‐​term incentive contracts. Thus, in the United States, firms induce workers to retire early by appropriately structuring non‐​wage elements of compensation, while anti‐​age discrimination laws provide an external commitment mechanism against premature discharges–thereby inducing workers to accept long‐​term incentive contracts.



Employment contracts are structured differently in the academic sphere. The recent removal (in 1994) of the age‐​70 mandatory retirement rule has resulted in many older and highly compensated faculty members remaining on university payrolls. Universities have not since been successful in inducing earlier retirements via pension plan and other incentives because these are costly to offer to faculty who are already close to retirement. However, universities may over time adopt long‐​term incentive contracts for younger employees similar to those prevalent in large private companies.



Second, health and longevity of the U.S population generally has been improving. This may mean that jobs that could not be conducted effectively by workers after their late fifties, now can be. A historical comparison of mortality rates suggests that those aged in their early sixties today are as healthy as were those in their mid fifties a few decades ago–when the mandatory retirement age rules were first imposed in the occupations under consideration.



Existing mandatory retirement age rules appear unfair for some categories of workers–such as pilots and air‐​traffic controllers that are subject to such rules. Because they spend their careers in jobs requiring specific, non‐​transferable skills, early job separation now results in longer spells of unemployment or forced retirement despite possessing the ability to conduct their jobs competently. Evidence suggests that independently of their tenure in earlier jobs older workers have greater difficulty in finding jobs in the private sector.



Third, improvements in technology imply that, other things equal, federal police, firefighting, and air‐​traffic‐​control jobs may have become physically easier to conduct. Evidence also suggests that health is now much less important as a consideration for the decision to retire than was the case several decades ago because of both, lower physical job demands and improvements in the treatment of chronic conditions.



Fourth, better technology and equipment increase the need for trained and experienced personnel to operate and coordinate activities. Hence, although better technology makes the jobs easier to perform, it could require more rather than fewer skilled workers.



Finally, as the baby boomers exit the workforce, the burden on working generations to support a larger and longer lived retiree population will increase. Baby boomer retirements will likely create skilled‐ and experienced‐​worker shortages in many occupations. The shortages are likely to become more acute in occupations that impose relatively early mandatory retirement age restrictions.



Overall, impending worker shortages in occupations with mandatory retirement age restrictions motivate the revision of these rules. However, as experience in the U.S. academic sphere indicates, addressing the shortages by immediately removing these restrictions may lead to other problems–such as a disproportionately older workforce. Introducing long‐​term incentive contracts to improve worker efficiency and yet provide flexibility in retirement decisions appears to be desirable. However, such employment contracts “pay off” only in the long‐​term and should be restricted to younger workers and new hires. Worker shortages in the near term could be addressed by revising mandatory retirement age rules upward. The decision to elimate these rules may never become necessary if younger worker and new hires are offered long‐​term incentive contracts. As these contracts become more widespread they may improve worker efficiency and induce timely retirements. Mandatory retirement age rules will automatically fall by the wayside as workers who remain subject to them leave the workforce.



 **Section 1: Mandatory Retirement Age Rules vs. Anti‐​Age‐​Discrimination Laws in the Private Sector**



Anti age discrimination laws and mandatory age retirement rules are polar opposites. Would private employers enforce mandatory retirement age rules in the absence of anti‐​age discrimination rules? The answer to this question _appears_ to be in the affirmative for those jobs and occupations requiring firm‐​specific skills–that is, full knowledge of company policies, operating rules, personnel, technology, on‐​going innovations and specific features of the work environment. These job requirements arise in managerial positions where staff must learn the nature of the business over several years. These requirements also apply in occupations requiring special on‐​the‐​job training–coordinating activities on a construction site, scheduling to run a factory work‐​shops etc. On‐​the‐​job acquisition of specific skills is also needed in varying degrees from a safety and job‐​effectiveness perspective, as is the case with air‐​traffic‐​controllers, pilots, law enforcement officers etc.



Whenever workers are required to possess “specific human capital,” it is in the employers’ interest to ensure that workers don’t quit immediately after acquiring those skills. However, because slavery is illegal, firms must induce workers to stay on the job by incorporating appropriate incentives in employment contracts, which may involve _implicit_ agreements on some elements. For example, workers may be paid less than their productivity during the early part of their tenure in exchange for (the implicit promise of) being paid more than their productivity during the later periods of their tenure with the firm. This implies the creation of “seniority rents.”



Several studies have found evidence consistent with the existence of long‐​term incentive contracting in the private sector. Overall, the evidence generally points against the “spot” market explanation of how compensation generally varys with age. In particular, the evidence suggests that compensation exceeds productivity at older ages providing a basis for employers to treat older workers differently.



Thus, in occupations requiring firm‐​specific skills, workers’ compensations may rarely, if ever, match their current productivity. Instead, the employers seek to match prospective productivity with prospective compensation over the workers entire tenure with the firm.



Apart from wages and salaries, compensation includes pensions, health insurance and other benefits. Employers generally use non‐​wage elements of compensation to design work and retirement incentives. Such incentives address multiple firm objectives: Ensuring worker bonding with the firm over long periods; ensuring that workers do not shirk on the job; inducing older workers to leave the firm at the “right” time, and so on.



Pension vesting and benefit accrual patterns of defined benefit pension plans can be designed to achieve all of these objectives. Vesting rules in DB pension plans generally require workers to be with the firm for 5 or 10 years before pension benefits begin to accrue. Pension accruals–the annual additions to the present value of pension benefits from additional years of work–are also designed to provide early retirement incentives.



Pension accrual patterns produce age‐​profiles of compensation that are initially steeper than workers’ age‐​productivity profiles. Pension accrual begins upon vesting and increases sharply at the early retirement age–usually age 55. The steep increase in pension accrual at age 55 arises because the eligibility to retire early and collect benefits immediately is associated with a smaller than fair reduction in benefits–compared to retiring at age 65 with full benefits. Moreover, delaying retirement beyond age 55 reduces pension accruals sharply‐​possibly making accruals negative.



These features of pension accurals create incentives for workers to retire early‐​keeping them from collecting seniority rents by staying on the job for too long. Firms can fine tune their compensation structures with other non‐​wage compensation elements, including severance packages. Thus, jobs involving the acquisition of significant firm‐​specific skills may exhibit productivity and compensation patterns such as those shown in Figure 1.





The productivity and compensation profiles of Figure 1 are estimated for male managers based on employment and compensation data from a Fortune 500 firm with over 300,000 employees. It shows several features:



Not all workers may be compensated under long‐​term incentive contracts. Routine office workers, support staff, sales agents, and so on appear to be compensated on a “spot” basis rather than under long‐​term incentive contracts. For example, we estimated annual productivity and compensation to be aligned more closely for salesmen in the above firm–as shown in Figure 2.





It should be noted that the adoption of long‐​term incentive contracts does not obviate the need to monitor worker performance. Indeed, the threat of being caught shirking and discharged is an integral part of the incentive structure.



Workers may also prefer contracts with rising compensation by age–if they can obtain them. One possible reason is their inability or lack of discipline to save — as is suggested by some psychological studies of saving behavior. They may prefer to receive lower compensation in the near term and higher compensation in the future as a forced saving mechanism.2



For long‐​term incentive contracts to be feasible, however, workers must also be convinced that employers will not arbitrarily discharge them as soon as they begin accruing seniority rents.



One conjecture is that firms’ incentives to “cheat” in this manner are reduced by the need to maintain their reputations–in order to continue hiring workers. However, purely reputational effects need to operate extremely strongly to effectively police against mid‐​career job terminations by employers. In addition, evidence suggests that early worker terminations can occur through other mechanisms — for example, after hostile takeovers of corporations. In one of my studies I find that post‐​hostile takeover managements discharge older workers, implying little or no negative reputational consequences.



The weakness of reputational effects in preventing premature worker terminations provides a possible rationale for anti age discrimination laws. A law prohibiting terminations purely on the basis of age can serve as external, and therefore more credible, commitment mechanism‐​an external check against the temptation to discharge workers prematurely–and makes long‐​term incentive contracts easier to implement.



How effective are long‐​term incentive contracts in inducing early retirements? Table 1 contains an answer based on data from the firm mentioned earlier. It shows retirement “hazard” rates–that is, the fraction of those employed at the beginning of the year that leave the firm within the year. The rates are shown by age and tenure with the firm.





The table suggests that because of the inducement to retire early (at age 55) provided through the pension accrual pattern, retirement rates step up to the 10–12 percent range between ages 55 and 59. Without the retirement incentives, they would remain at about the 3 percent level that prevails prior to age 55. Note that those aged 55–61 who are not yet fully vested in the pension plan (that is, those with less than 10 years of service) exhibit separation rates around 3 percent annually. Job separation rates at 10–12 percent per year rather than 3 percent per year can have substantial cumulative effects on overall labor force participation between the ages of 55–61. It is noteworthy that job separation rates increase even more dramatically at age of 62 and 65. These increases in retirement hazards probably occur as workers not subject to long‐​term incentive contracts respond to the retirement incentives provided by the Social Security program at these ages.



Prior to the 1980s, defined benefit (DB) plans covered two‐​thirds of workers and defined contribution (DC) plans covered about one‐​third. As is well known, defined benefit pension plan coverage has been declining and defined contribution plan coverage has been growing during the last two decades. Evidence shows that DB plans’ usefulness has declined for both employers and employees in an environment of rapid technological and structural changes in the economy. Under such conditions, the desirability of long‐​duration employer‐​employee matches has declined. Employers attempting to adapt to new technologies may require greater flexibility in workforce composition. Employees may prefer greater portability of pension assets if expected job‐​durations are shorter.3



However, the surge in DC pension plans since the early 1980s has not extinguished the use of DB plans: About one‐​third of the workforce continues to be covered under DB plans. Moreover, because early retirement incentives are not incorporated into DC plans, retirement rates among those in their early sixties have declined–a reversal in the trend established over several decades.



In the current context, offerring efficiency enhancing compensation structures to federal and state and local workers similar to those adopted in the private sector appears to be desirable–to the extent such incentive contracts are not offerred today. This recommendation is motivated by the need to elicit worker efficiency, and is independent of the fact that public operations are not driven by a profit maximizing motive. Moreover, such incentive compensation structures would provide greater retirement flexibility and help achieve employers’ objectives of safety and effectiveness in job performance.



However, any revision of public sector employment contracts along these lines would require a careful examination of whether such compensation structures are feasible, the manner in which they should be introduced, and how effective they could be as substitutes for the mandatory retirement age rules currently in force.



 **Section 2: Mandatory Retirement Ages, Occupational Development, and Personnel Abilities**



For the occupations under consideration, adopting a single mandatory retirement age is not necessarily better or cheaper than adopting flexible compensation‐​based incentives to retire.



A fixed retirement age potentially introduces two types of errors from the perspective of retaining qualified workers. First, those who are less qualified than others would remain on the work force because they are younger than the mandatory retirement age. Second, those who are better qualified than others are forced to retire because they are older. The mandatory retirement age could be set to minimize the sum of both types of errors. For example, if the mandatory retirement age were set at 40, we would force many qualified workers to retire prematurely. Similarly, if the retirement age were set at 80 many workers who are no longer competent would be retained. These errors would be minimized by setting the mandatory retirement age between these extremes.



The mandatory retirement age rules in the occupations under consideration were set several decades ago. Assuming that they were initially set optimally to minimize the sum of the two types of errors‐​they are probably obsolete today for a number of reasons.



 _Improving Health and Fitness_



The significant progress achieved in medical innovation and health care have increased the longevity of the U.S. population in general.4 People in their early sixties today enjoy similar health and lifestyles today with greater frequency as did those in their mid fifties several decades ago. One indication of the better health of today’s workers is the downward trend in mortality rates.



For example, mortality information from the Social Security Adminstration suggests that 55‐​year‐​old men in 1960 faced the same likelihood of dying within the year as do 62 year‐​old men today. And today’s 66 year old men have the same chance of dying as did 60 year old men in 1960. For women, mortality improvements are somewhat smaller. Women aged 55 in 1960 experienced the same average mortality as do 60 year old women today. And the same fraction of 60 years old women died in 1960 as do 64 year old women today.



Evidence that health among 50–60 year olds is improving can also be gleaned from surveys of self‐​reported health. Figures 3 and 4 show calculations based on the Panel Survey of Income Dynamics (PSID).5 The calculations reported below are based on weighting each household to convert the survey’s sample into a representative U.S. household population.



Figure 3 shows that by 1997, a sizable majority of men and women were in good or better health. Over a 14 year period between 1985 and 1997, the fraction of men aged 56 through 65 who reported being in good or better health increased by almost 5 percentage points to 71 percent. Figure 4 shows that for women, the share of those in good or better health increased by about 7 percentage points to 67.4 percent.







Although these data are based on self‐​reported health by survey respondents and spouses, a study (based on a different survery) shows that such responses are representative of the type of information used in professional evaluations of health and disability status.6



These data go back only through 1985. Projecting them further back in time would presumably reveal even more substantial gains in the health and fitness of those in their mid‐​fifties and early sixties. Indeed, other studies have indicated that health is now much less important as a consideration for the decision to retire than was the case several decades ago.7 This is because of both, lower physical job demands and improvments in the treatment of chronic conditions.



Finally, the data indicate sizable gains in longevity and health for the _general_ U.S. population. I do not have direct evidence of similar health gains for the subset of the population that forms the base for recruitment into the occupations under consideration. To the extent that such gains have occurred, revising mandatory retirement ages upward by a few years may be feasible.



 _Technology Induced Demand and Projected Worker Shortages_



The technology used in executing jobs in many of the occupations under consideration is much better today compared to 3 or 4 decades ago. The largest improvement has occurred in communications and information technology, and in all occupations; firemen have better heat resistant and fire‐​retardant materials; pilots have planes that are easier to fly an land; police officers have better investigative, forensic, and interdiction techiques, better body armour, DNA analysis, computerized laboratories etc. Not only does newer technology allow jobs to be executed faster and more efficiently, they can be executed with lesser exertion of effort.



They also call for a workforce with a wider range of skills–which implies that the availability of new technology is not necessarily labor‐​saving overall. It requires more training to operate and maintain newer equipment and requires more experienced personnel to coordinate job activities. On the other hand, as the baby‐​boomer generation retires, many jobs requiring trained and experienced workers will begin to experience shortages. Those jobs where retirements are mandatory at younger ages will experience acute shortages earlier.



Personnel shortages in key jobs that must be executed on time provoke the imposition of mandatory overtime. However, forced overtime over long periods imposes additional burdens and is likely to lower worker morale. A high‐​stress atmosphere is likely to induce additional accelerated retirements and make worker shortages even more acute. Hence, worker shortages in crucial occupations can be self‐​reinforcing unless dealt with in a timely manner. The prospect of increased shortages because of the impending surge in retirements along with an increasing demand for security and aviation efficieny in a new post‐​9/​11 world makes it necessary to revisit mandatory retirement age rules in the occupations that currently enforce them.



 **Conclusion**



Private sector enterprises get by without the imposition of mandatory retirement age rules: They successfully hire high skilled workers under long‐​term incentive contracts. Indeed, anti age‐​discrimination laws–the polar opposite of mandatory retirement age rules — appears more important for making such contracts feasible. An important element of private long‐​term incentive contracts are defined benefit pension plans and other non‐​wage compensation to achieve firms’ objectives of eliciting worker efficiency and ensuring timely retirements. Similar compensation arrangements could be usefully considered in the public sector as well, despite the lack of a profit‐​maximizing objective.



Mandatory retirement age rules in certain private, federal, and state and local occupations have been in place for several decades. Their revision appears worthy of consideration for several reasons.



The improvement in general health and abilities of those aged between 55 and 65 in general may imply that mandatory retirement at these ages is unfair for a growing number of workers who retain the ability to execute their jobs competently but cannot transfer their skills to other occupations–such as pilots and air‐​traffic controllers. Evidence suggests that older displaced workers find it much harder to find jobs compared to younger workers and suffer larger wage declines upon re‐​employment. 8



Better technology makes the conduct of these jobs physically less taxing. Moreover, newer technologies are likely to require a larger workforce with a broader set of skills to fill these positions. And, the onset of baby‐​boomer retirements is likely to create acute shortages of experienced personnel, especially in occupations with mandatory retirement set at younger ages.



Summarily eliminating mandatory retirement age rules to prepare for upcoming worker shortages may not, however, be the correct policy response. Doing so may create other problems as in the U.S. academic institutions where retirement rates have plummetted after mandatory retirement at age 70 was abrogated. That has led to slower turnover of teaching staff and aging faculties. Because long‐​term incentive contracts pay off only when introduced at the beginning of worker careers, implementing such contracts for older workers becomes expensive.



Hence, existing mandatory retirement age rules should be revised in two steps. To deal with impending shortages, existing mandatory retirement ages could be advanced by a few years. Long‐​term incentive contracting should be introduced for younger workers and new hires. The workforce subject to long‐​term contracting should be designed to both provide retirement choices to workers and satisfy employers’ objectives of work‐​safety and efficiency. The revised mandatory retirement age rules will be automatically phased out over time as the workers to which they apply retire over time.



 **Notes**



1. I am Jagadeesh Gokhale, Senior Fellow at the Cato Institute in Washington D.C. I have conducted studies on labor market contracting in the private sector and the effects of long‐​term employment contracts and worker tenure on the market for corporate control. I have also written on demographic and retirement issues relating to the sustainability of the federal budget.



2. R. H. Frank and R. M. Hutchens, 1993, _Economic Journal,_ Vol. 21.



3. See Friedberg and Owyang, National Bureau of Economic Research, Working Paper No. 10714.



4. Frank Lichtenberg “Sources of U.S. Longevity Increase: 1960–1997,” National Bureau of Economic Research, Working Paper No. 8755, February, 2002.



5. The PSID is conducted by theUniversity of Michigan’s Survey Research Center. This survey’s sample contained just over 10 thousand U.S. households in 1985 and it attrited to about 6,700 households by 1997.



6. Hugo Benitez‐​Silva, Moshe Buchinsky, Hiu Man Chan, Sofia Cheidvasser, and John Rust, National Bureau Economic Research Working Paper No. 7526.



7. Costa (1994), NBER Working Paper No. 4929.



8. See David Shapiro and Steven L. Sandell, 1985, _Southern Economic Journal,_ Vol. 52.
"
"**With much of the UK under lockdown, many shops, pubs and restaurants are shut, and large parts of the economy are effectively closed.**
The government has spent hundreds of billions on measures to support businesses and jobs, and fight the pandemic. But how will it pay for these?
We won't know how big the final bill will be until after the crisis is over. But the government will certainly have to borrow enormous amounts of money because it is spending more than it is taking in from tax.
On 25 November, the Office for Budget Responsibility (OBR), which keeps tabs on government spending, estimated that borrowing would be Â£394bn for the current financial year (April 2020 to April 2021).
That's the highest figure ever seen outside wartime.
To put that into context: before the crisis, the government was expecting to borrow about Â£55bn for the whole financial year.
This year the government is spending a staggering Â£280bn on measures to fight Covid-19 and its impact on the economy.
That includes Â£73bn for measures to support jobs such as the furlough scheme, where the government pays most of the wages of employees who cannot work.
The NHS and other public services have been given Â£127bn extra to combat the pandemic, and Â£66bn will be spent on grants and loans to support businesses.
The government will also raise Â£100bn less tax than it hoped because of the crisis. Unemployed or furloughed workers pay less income tax, businesses pay less tax if their profits are lower, and shoppers pay less VAT if they buy fewer things.
With more money going out and less coming in the government has only one option - to borrow.
Even if the pandemic ends quickly, there will still be higher costs and lower tax receipts in future years too, all of which means more borrowing.
**Will I have to pay more tax?**
Some economists argue that all the costs of the crisis could be covered by borrowing alone, but many disagree.
If the government wants to get borrowing down, it will have to cut spending or raise taxes, or most likely, both.
Raising taxes would be politically awkward, because the Conservative 2019 manifesto promised not to raise the three biggest taxes. These are income tax, national insurance and VAT - and together they bring in more than half of government revenue.
Increasing taxes means people have less money to spend, which could slow the economy down further. However, the respected Institute for Fiscal Studies think-tank has warned that tax rises of more than Â£40bn a year are ""all but inevitable"".
Cutting spending will also be difficult. There have been big cuts over the past decade, and many of the easy savings have already been made.
Some areas have long been protected, such as the NHS - and it would be difficult to reduce health spending after a big pandemic.
State pensions, another big spending item, are protected by a so-called ""triple lock"", which guarantees they rise with wages, prices, or 2.5% every year, whichever is highest. The manifesto promised to keep this, too.
In Wednesday's Spending Review the chancellor, Rishi Sunak, did make some cuts such as freezing pay for many public sector workers.
He also cut the amount the UK will spend on overseas aid, despite a manifesto pledge to keep it at 0.7% of national income.
The chancellor could say the pandemic also makes other manifesto promises impossible to keep. But difficult choices will have to be made.
If taxes go up, people will soon realise they have less money to spend.
Likewise, people will notice if lower public spending results in worse public services, such as longer waiting times in hospitals or fewer police on the streets.
Public sector workers whose wages are frozen will feel that impact very keenly.
And if pensions or benefits are cut, or even just increased less rapidly, that will be felt directly by anyone who depends on them.
At first the government will raise money by borrowing from investors.
They could be individuals, companies, pension funds, or foreign governments who lend money to the UK government by buying bonds.
A bond is a promise to pay the money back in the future, and pay interest on the loan in the meantime.
The Bank of England is buying huge amounts of bonds, to support the economy by encouraging more spending and investment, in a process called ""quantitative easing"".
This year the Bank is buying Â£450bn worth of bonds, which makes it much easier for the government to borrow money.
In recent years, the government has been able to borrow easily at very low interest rates, which makes its debt more affordable.
At the moment it pays just 0.32% interest to borrow for ten years.
There is a limit to how much the government can borrow, before interest payments become so great it can't afford them. No-one knows quite where that limit is.
But those interest payments will still weigh on future generations until the debt is paid off, and will mean there is less money available to spend on public services, or tax cuts."
nan
"
Share this...FacebookTwitterHistoric October Snowstorm Still Crushing New England
A historic October snowstorm is still crushing New England with heavy snow and howling winds before cruising away into Atlantic Canada.
Snow amounts have already topped two feet across portions of New England, while record-shattering snow hammered the major Northeast cities from Washington, D.C., and Philadelphia to New York City and Hartford, Ct…Keep reading…
Share this...FacebookTwitter "
"**There have been more than 1.6 million confirmed cases of coronavirus in the UK and about 60,000 people have died, government figures show.**
However, these figures include only people who have died within 28 days of testing positive for coronavirus and other measures suggest the number of deaths is higher.
**Find out how the pandemic has affected your area and how it compares with the national average:**
If you can't see the look-up click here.
After the first peak in April, cases started rising again in July, with the rate of growth increasing sharply in September and October, before falling again in the past two weeks.
On Wednesday, the government announced a further 16,170 confirmed cases.
It is thought the infection rate was much higher during the first peak in spring, but testing capacity at the time was too limited to detect the true number of daily cases.
The data for cases can also be broken down by region and comparing the change in those figures by week gives a sense of where there has been a recent increase in newly-reported infections.
Coronavirus infections in England have fallen by about a third during lockdown, according to a major study carried out by Imperial College London.
While some of the worst-hit areas saw the biggest improvements, cases have remained relatively high across England, according to the React-1 study, which is based on tests of more than 100,000 people between 13-24 November.
The latest figures from the Office for National Statistics (ONS), suggest about one in 185 (16,400 people) in Wales had the virus in the week ending 21 November.
In Northern Ireland rates are thought to be decreasing at around one in 145 people (about 12,700 people), while in Scotland, the figure was one in 115 (about 45,700 people).
The average number of daily deaths began to rise again in September, following the first peak in April.
On Wednesday, the government announced a further 648 deaths.
Of these, 555 deaths were in England, 51 in Wales, 38 in Scotland, and four in Northern Ireland.
Rules were amended over the summer to include deaths in the coronavirus total only if they occurred within 28 days of a positive test. Previously in England, all deaths after a positive test were included.
England has seen the majority of UK deaths from Covid-19. Using the 28-day cut-off, there have been more than 52,000.
Although hospital admissions for Covid-19 remain below the levels seen in the spring, there are big regional disparities.
The North West, North East and Yorkshire, and Midlands have seen the highest number of admissions but the situation in all three now appears to be improving.
Cases have risen across large parts of England, with other spikes in areas of Scotland, Wales and Northern Ireland.
The red areas on the map below are those currently seeing the highest number of cases per 100,000 people.
Restrictions have been tightened across the UK in an effort to tackle the number of rising cases.
In England, each local authority has been placed in one of the new three tiers following the end of the recent national lockdown.
Wales has announced a range of further measures, including a ban on pubs, restaurants and cafes serving alcohol, from Friday 4 December.
In Northern Ireland, a further two-week ""circuit-break"" lockdown began on 27 November.
Scotland has a five-tier system of alert levels with different measures in place in different parts of the country.
UK leaders have agreed to allow up to three households to meet indoors during a five-day Christmas period of 23-27 December.
When looking at the overall death toll from coronavirus, official figures count deaths in three different ways.
Government figures count people who died within 28 days of testing positive for coronavirus.
But there are two other measures.
The first includes all deaths where coronavirus was mentioned on the death certificate, even if the person had not been tested for the virus. The most recent figures suggest there had been almost 70,000 deaths by 20 November.
The second measure counts all deaths over and above the usual number at the time of year - that figure was more than 75,000 by 20 November.
The most recent figures available from the ONS are for the third week of November, which show there were 14,276 deaths registered in the UK.
Some 3,038 of these deaths involved Covid-19 - 199 more than the week before.
Deaths normally do rise at this time of the year, but the data from the ONS and its counterparts in Scotland and Northern Ireland show the second wave of the virus has pushed the death rate above the average seen over the past five years by about 21%.
Overall, the figures are still well below the peak of 9,495 deaths recorded in a week, reached on 17 April.
The ""R number"" is the average number of people an infected person will pass the disease on to.
If R is below one, then the number of people contracting the disease will fall; if it is above one, the number will grow.
The government's current estimate for the R number across the whole of the UK is 0.9 to 1.
The estimate for England is 0.9 to 1, while for Scotland it is 0.8 to 1. The estimate for Wales is 0.8 to 1 and in Northern Ireland it is 0.9 to 1.1.
The government has said in the past that the R number is one of the most important factors in making policy decisions."
"
Earlier I wrote about the Arctic Oscillation Index going strongly negative in December and what new cold to expect in January. From NASA’s Earth observatory, we have a high resolution temperature anomaly map that provides visualization of the effects. This image was taken while the Copenhagen Climate Conference was in progress.


Deadly Cold Across Europe and Russia



Click image above to enlarge or download large image (3 MB, JPEG) 		acquired December 11 –  18, 2009



It will be interesting to see how the NASA imagery compares with the anomaly maps of GISS and HadCRUT for December when they are made available. More images are available at links below.
A wave of frigid air spilled down over Europe and Russia from the Arctic in mid-December, creating a deadly cold snap. According to BBC.com, at least 90 people had died in Europe, including 79 people, mostly homeless, in Poland. In places, the bitter cold was accompanied by heavy snow, which halted rail and air traffic for several days during the week of Christmas.
This image shows the impact of the cold snap on land surface temperatures across the region from December 11–18, 2009, compared to the 2000–2008 average. The measurements were made by the Moderate Resolution Imaging Spectroradiometer (MODIS) on NASA’s Terra satellite. Places where temperatures were up to 20 degrees Celsius below average are blue, locations where temperatures were average are cream-colored, and places where temperatures were above average are red. Light gray patches show where clouds were so persistent during the week that MODIS could not make measurements of the land surface temperature. The biggest anomalies were in northern Russia, but a swath of below-average temperatures stretched across the countries around the Baltic Sea as well.
See also:

Daily, 8-day, and monthly land surface temperature anomaly maps
Animation of monthly global land surface temperature anomalies

h/t to WUWT reader “JT”



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e900dc066',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
\---   
  
Poof, it was gone.   
  
Just like that, the human fingerprints on a century-long warming trend in Northwestern United States were erased and replaced instead by the telltale signs of natural variability.   
  
That is the conclusion of new research published last week by a pair of scientists from the University of Washington. James Johnstone and Nathan Mantua published their paper titled “Atmospheric controls on northeast Pacific temperature variability and change 1900-2012” in the _Proceeding of the National Academy of Sciences_ (PNAS).   
  
So as not to be accused of putting words in their mouth, here, in full, are the contents of a box labeled “Significance” from their paper:   




Northeast Pacific coastal warming since 1900 is often ascribed to anthropogenic greenhouse forcing, whereas multidecadal temperature changes are widely interpreted in the framework of the Pacific Decadal Oscillation (PDO), which responds to regional atmospheric dynamics. This study uses several independent data sources to demonstrate that century-long warming around the northeast Pacific margins, like multidecadal variability, can be primarily attributed to changes in atmospheric circulation. It presents a significant reinterpretation of the region’s recent climate change origins, showing that atmospheric conditions have changed substantially over the last century, that these changes are not likely related to historical anthropogenic and natural radiative forcing, and that dynamical mechanisms of interannual and multidecadal temperature variability can also apply to observed century-long trends.



  
  
Translation: Natural variability in the atmosphere/ocean dynamics of the northern Pacific Ocean rather than human-caused global warming can largely explain the century-long rise in temperature in the Pacific Northwest.   
  
And the authors have the figures to prove it.



The left-hand panel of Figure 1 (below) illustrates the observed trends over the period 1900 _–_ 2012 for weather observing stations in the contiguous United States located west of longitude 116°W—this includes all of Washington and Oregon, most of California, and parts of Idaho and Nevada. Notice all the red, upward-pointing arrows indicating an overall temperature rise since 1900 across the entire region. The right-hand panel shows the trend in the same stations once the natural variability identified by Johnstone and Mantua has been removed. If global warming were having an influence, it would be evident in both panels—more strongly so in the right-hand one. Instead, in that panel, we have a mixed bag of up and down trends of very low magnitude—no strong signal of any kind, no sign of global warming.   






  
  
_Figure 1. (left)_ _Northeast Pacific annual temperature trends, 1901_ _–_ _2012\. Map of trends (°C/century) of annual (July_ _–_ _June) mean surface air temperature for observing stations west of 116°W. Triangles mark statistically significant (_ _p_ _< __0.05) trends (red, upward pointing: positive; blue, downward pointing: negative). Gray dots mark locations of insignificant trends. (Right) Map of residual surface air temperature trends after removal of natural variability temperature signals (adapted from Johnstone and Mantua, 2014)._   
  
Now, we’ll be among the first to admit that _PNAS_ does not have the reputation of publishing the most robust of studies, but, still, this result is intriguing. We hope that it will be verified by publication in a more rigorous peer-reviewed journal in the near future.   
  
In the meantime, we can only wonder what the authors of the recent U.S. National Climate Assessment (NCA) are thinking. After all, they have an entire chapter of their new report (a report basically crafted to support President Obama’s Climate Action Plan) dedicated to present and future climate change in the Northwest. While the NCA authors coyly admit that the region’s “climate trends include contributions from both human influences (chiefly heat-trapping gas emissions) and natural climate variability” they are quick to add “[t]hey are also consistent with expected changes due to human activities.”   
  
Hmm. While the observed trends may be “consistent with” the NCA authors’ “expected changes due to human activities,” in actuality they are, in fact, _not_ caused by human activities.   
  
This is a good lesson that “consistent with” does not equate with “a result of.” Which means that all the other regional changes in the Northwest noted in the NCA—wildfires, insect outbreaks, changes in the timing of stream flow, etc.—are also largely a result of influences other than human-caused climate change. And, it means that all the projections of future climate changes forwarded by the NCA are also nonrobust.   
  
Somehow, we doubt a correction will be forthcoming.   
  
After all, the NCA and the president are not particularly interested in what the science actually says about climate change and its causes and effects, but rather how it can be molded to support the climate change activism that will shape the legacy of the current administration. Inconvenient truths are too readily brushed aside.   
  
Reference:   
  
Johnstone, J. A., and N. J. Mantua, 2014. Atmospheric controls on northeast Pacific temperature variability and change, 1900-2012. _Proceedings of the National Academy of Sciences_ , doi:10.1073/pnas.1318371111.


"
"
Readers may recall yesterday where I posted this stunning image of cold for Europe and Russia for mid December 2009 from the NASA NEO MODIS satellite imager.


Deadly Cold Across Europe and Russia


Click image above to enlarge or download large image (3 MB, JPEG) 		acquired December 11 –  18, 2009
In that story were links to additional images, and I’d planned to return to them for a comparison. Inspired by my posting, METSUL’s Alexandre Aguiar saved me the trouble. There’s an interesting comparison here between the surface anomaly done by weather stations (NASA GISS) and that of satellite measurement (NASA NEO MODIS) – Anthony

Guest post by Alexandre Aguiar, METSUL, Brazil
COMPARE THE TWO MAPS

NASA GISS on the left, NASA MODIS on the right
Here’s the same images but larger – click either image for full size:



South America: The vast majority of the continent is near average or below average in the NEO map, but according to GISS only the southern tip of the region is colder. The most striking difference is Northeast Brazil: colder in the NEO map and warmer at the GISS.
Africa: Most of the continent is colder than average in the NEO map, but in the GISS most of Africa is warmer than average.
Australia: The Western part of the country is colder than average in the NEO map, but the entire country is warmer in the GISS map.
Russia: Most of the country is colder than average in the NEO map, a much larger area of colder anomalies that presented in the GISS map.
India: Colder than average at NASA’s NEO website and warmer at NASA’s GISS map.
Middle East: Huge areas of the region (Israel, Jordan, Turkey, Iraq, Syria) are colder than average in the NEO map and average/warmer in the GISS map.
Europe: Near average or slightly above average in the NEO map and much above average in the GISS map.
Greenland: Entire region colder than average at NEO and much of the area warmer at GISS.
Same source (NASA), but very different maps !!!
Why:
At NEO, land surface maps show where Earth’s surface was warmer or cooler in the daytime than the average temperatures for the same week or month from 2000-2008. So, a land surface temperature anomaly map for November 2009 shows how that month’s average temperature was different from the average temperature for all Novembers between 2000 and 2008.
Conclusion
Despite being very warm compared to the long term averages (GISS, UAH, etc), November 2009 was colder in large areas of the planet if compared to this decade average.
See PDF here. December should be very interesting in the northern hemisphere.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8f434e61',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**A care home put residents at ""serious risk"" by allowing a staff member who tested positive for Covid-19 to work, inspectors have found.**
Preston's Aadamson House Care Home also let a second person with symptoms work a shift, a Care Quality Commission (CQC) inspection in October found.
The CQC said it had ""consistently"" failed to protect people from ""catching and spreading infections"".
The care home said it had ""taken steps to rectify the concerns highlighted"".
Rating the care inadequate and placing the home, which opened in July 2019, in special measures, the CQC said the inspection had followed concerns raised about the management of coronavirus and staffing levels.
Its report said people at the premises on Peel Hall Street were ""exposed to the risk of harm"" because the provider ""failed to ensure a staff member who tested positive for Covid-19 isolated"" and ""another member of staff worked one shift with symptoms"".
Inspectors said it ""failed to consistently protect people... from catching and spreading infections"" and had not enforced social distancing in communal areas.
The CQC also found staff did not consistently wear personal protective equipment and staff had not received training in how to deal with Covid-19.
Breaches were identified by inspectors in a number of other areas and in one case, the CQC found a person who was hurt in a fall was not been adequately assessed and the risk of further falls was not mitigated.
The report also said there were not enough staff trained in basic life support, moving and handling, falls awareness and night-time emergency evacuation procedures and residents had not been protected from the risk of fire, with inspectors finding one fire escape blocked.
Aadamson House Care Home, which was caring for 13 people at the time of the inspection, said it had appointed a new manager and compliance consultancy.
It said it ""speedily put in place new systems and proactive steps to prevent any further failings and to rectify those already highlighted"", adding: ""We will work tirelessly... to ensure the residents' safety remains at the centre of the service.""
_Why not follow BBC North West on_Facebook _,_Twitter _and_Instagram _? You can also send story ideas to_northwest.newsonline@bbc.co.uk"
"Boris Johnson should publicly declare climate deniers as wrong in order to secure the UK’s standing in vital UN climate talks this year, campaigners have urged, as climate deniers with links to the Tory party prepare for a new battle. As the UK leaves the EU, and its emphatic environmental commitments including the European green deal, those who want to see less action on the climate crisis are hoping Johnson’s government will be more amenable to delaying and watering down green measures. The EU on Wednesday sets out its first ever climate law, the long-awaited centrepiece of the European green deal, which is designed to prepare the EU economy for dealing with the climate emergency. The new law would enshrine a 2050 net zero emissions target in legislation and empower Brussels to take governments to court if they fail to comply. But the UK prime minister has yet to articulate a strategy for meeting Britain’s target of net zero by 2050, giving the advocates of delay and distraction everything to play for. Climate denial is taking new forms, some experts say, moving from an outright rejection of science to covert attacks on green policies and spending on efforts to cut carbon. The EU’s green deal has prompted frantic lobbying in Brussels by powerful fossil fuel interests, as a Guardian collaboration with other European media organisations this week reveals. But the Tory party retains links with prominent individuals who are climate deniers, or who are connected to organisations that either deny the climate crisis or actively work to undermine climate policies, such as the Global Warming Policy Foundation, the thinktank set up by the former Conservative chancellor Lord Lawson. “The GWPF have shown themselves to be tremendous opportunists,” said Bob Ward, policy director at the Grantham Research Institute on climate change at the London School of Economics. Ward claims the thinktank has been fundraising heavily in the US – as well as spreading its views to ministers and across Whitehall – to raise money for a major campaign to influence the Johnson government. “I expect that as the economy continues to take a hit from Brexit that the GWPF will attempt to mislead policymakers into believing the cause is climate policy.” Sir Michael Hintze, a hedge fund billionaire who is a long-time major donor to the Tory party, is understood to be a funder of the GWPF, which is not required to disclose its funding, and is also understood to have provided support to Priti Patel, Dominic Raab and Andrea Leadsom, among other Tory MPs. Matt Hancock has accepted donations from Ian Taylor, the chairman of oil trader Vitol, while Michael Gove and Liz Truss have been linked to the the American Enterprise Institute, which lobbies political, business and public opinion against action on the climate crisis. There is no suggestion of wrongdoing on the part of the Tory MPs. However, in the current climate crisis, experts have raised questions about the Tories accepting funds from individuals with fossil fuel interests or linked to climate denial organisations. “Well-connected climate sceptics must be called out by ministers, from the prime minister down,” said Shaun Spiers, the executive director of the Green Alliance thinktank. “We have seen the damage that well-funded rightwing campaigns against climate action have done in the US, Australia and Brazil. Conservative environmentalism is stronger than ever, with the party officially recognising the seriousness of the climate emergency. It should disassociate itself from the deniers.” This is Europe is a new stream of Guardian journalism that investigates the big challenges that transcend national boundaries, and seeks out the solutions that could benefit us all. These are testing times, and crises are not limited by national borders. But then neither are we.  This year’s UN talks on the climate are the most important since the Paris agreement in 2015, as the world is now far adrift of the Paris goals. The Cop26 summit set for Glasgow in November is seen as a last chance to get back on track to avoid climate breakdown, but the UK will face an uphill struggle to bring other countries on board with strong new carbon-cutting commitments. That will not be helped by any perception that the Tory party is sympathetic to, or funded by, climate sceptics and deniers, and fossil fuel interests, experts told the Guardian. Johnson should claim the mantle of Margaret Thatcher, advised Mohamed Adow, the director of developing country thinktank Power Shift Africa, and a longtime observer of the UN climate talks. “[Cop26] is an opportunity for Johnson to show that he’s broken with those dinosaurs of the past, and is instead showing that tackling climate change transcends tribal politics. His predecessor Margaret Thatcher was the first western leader to give a speech on climate change at the UN, so he won’t be the first Tory to show leadership on this issue,” he said. Other countries would be watching closely, warned Adow: “It is a problem that the Conservatives are so closely linked to discredited organisations like the GWPF. If Britain is going to be taken seriously by the rest of the world, [the Conservative party] should distance itself from these shady groups who try to undermine efforts to address the climate crisis.” As well as disavowal of denialist arguments, Johnson must set out clear measures to fulfil the UK’s own climate goals, added Alasdair Cameron of Friends of the Earth. “The voices of denial or delay are obviously scrabbling to stay relevant, but in truth are fading fast,” he said. “However, it will not be enough for government to simply accept that there is a problem. The UK has the opportunity to be a leader in building a positive zero-carbon future, but it will require urgent and concerted action at home and abroad, and right across government.” Doug Parr, the chief scientist at Greenpeace, said that although the government was espousing green policies, it was inconsistent. “Inside the Boris Johnson government, climate denial seems to have morphed into something else – a form of cognitive dissonance. It’s the contrast between acknowledging that there’s a climate emergency and keeping expanding fossil fuel exploration, funding oil and gas projects overseas, and spending billions on new roads. This way of thinking is ultimately on a collision course with reality.” Johnson has made only one public appearance on Cop26 so far this year, attending the launch in front of a group of schoolchildren and more than 100 dignitaries. But the event was overshadowed by criticism of Johnson by Claire O’Neill, the former energy minister who was to be Cop26 president but was abruptly sacked a few days before. She accused Johnson of a lack of knowledge and commitment on the climate crisis. She was replaced last month by Alok Sharma, the new business secretary. Some green campaigners are concerned that his dual role will create a conflict of interest and prevent him from standing up to vested interests reluctant to move urgently to a low-carbon economy. Hintze said in a statement from his spokesman: “I believe that the increase in concentration of carbon dioxide is in part due to human activity over the past century and that it has been a cause of global warming. But the sole focus on CO2 is too narrow and there are many contributors that need to be considered. All sides must be heard to reach the right conclusion for society as a whole.” Benny Peiser, the director of the GWPF, said the organisation was always fundraising, but did not disclose its donors. He said Cop26 was unlikely to make much progress: “The international community is deeply split on this and I do not expect anything more than a continuation of the deadlock.” A Conservative party spokesperson said: “Tackling climate change is one of the Conservative party’s top priorities, and this government is working tirelessly day in, day out to achieve our ambitious targets.”"
"

In a world where many adults receive their science education from newspapers and television, a great deal of misinformation about global warming exists. The media is quite skilled at making highly untenable predictions of greenhouse doom and gloom appear credible‐​foretelling drastically rising sea levels, the increased fury of hurricanes, and even plagues of locusts. The inaccuracies about how humans inadvertently warm earth’s tenuous atmosphere so pervade popular culture that the actual science behind this notion is hardly given a second thought. 



So how do we separate the global warming wheat from the greenhouse chaff? As a meteorologist practicing my trade for nearly 20 years, I recommend a read of the latest work by Patrick Michaels, a professor at the University of Virginia and senior fellow in environmental studies at the Cato Institute. The book, _Meltdown_ , is the latest in Michaels’ sequence of books on the topic of global warming. 



_Meltdown_ presents the flip side of what most people have heard about global warming, a cogent counterpoint to the view that the introduction of anthropogenic carbon dioxide is pushing the fluid systems of this planet into hyper‐​overdrive. The book presents a vast body of highly credible and growing knowledge that has been largely ignored. It includes scientific information that does not get reported in the papers or in government reports, because this information threatens to undermine the great doom and gloom establishment. The basic thesis of _Meltdown_ is that, yes, there has been a recent upward trend in the temperature of the atmosphere. But the increase is small and unlikely to mushroom into something truly catastrophic; the public, policy, and scientific distortions that have emerged are way off the mark. The book is steeped in scientific fact, with no fewer than 100 references to journal literature, but Michaels distills, synthesizes, and cuts through the morass like a beacon. His coverage is broad, and the distortions he uncovers are organized into topics dealing with ecosystems, drought and flood, severe storms, diseases, and the cryosphere.



The book is also rewarding because Michaels’ writing is a pleasure to read. His style is conversational, nontechnical, and often quite humorous. In fact, we get the sense that Michaels enjoys agitating the beehive, poking and jabbing at the current paradigm of greenhouse disaster. His tone conveys an often blunt, common sense way of thinking about the problem. If meteorology had its own weekly news show, a _60 Minutes_ on climate change, Michaels would be the perfect Andy Rooney.



 _Meltdown_ provides a global warming education that is unlikely to be gleaned from textbooks or a formal introductory college course on meteorology. It also may clear up common misconceptions picked up over the years. Right from the start Michaels’ explains that anthropogenic global warming and the greenhouse effect are not one and the same process. In fact, the greenhouse effect has been operating steadily for millennia, actually making the earth habitable. Global warming has many causes, and the statistics can be misleading. For instance, an anomalously warm year can be cloaked in the form of a strong El Niño; a virulent tornado outbreak in the Midwest is not a sure signal of humaninduced greenhouse enhancement.



Michaels operates like a pathologist, sifting through the minutiae of datasets, time series, and statistics to construct a convincing argument against the imminent melting of glaciers and hurricanes spinning rapidly out of control. Michaels applies rigorous tests to the data, pointing out where the datasets frequently break down or are stretched beyond the point of credibility. _Meltdown_ provides a badly needed balance, throwing some cold water on those waiting for the planet’s thermometer to boil over.



Those interested in climate science owe it to themselves to complete their education by reading _Meltdown_ and Michaels’ earlier works. In a room that has grown quite stuffy during the past few years, this book throws open the windows and lets in some fresh air.
"
"
Share this...FacebookTwitterWhat happens when you take the work of energy production and management away from the experts and power companies, and transfer it to government bureaucrats and environmental ideologues who surround themselves with clueless media cheerleaders?
Guenter Keil tells us here at Science Skeptical.
In a nutshell, it’s like taking a country’s central bank and putting it into the hands of communists – i.e. it gets run into the ground in short order.
Late last October in the German state of Brandenburg (East Germany), German television and print media were all present and enthusiastically cheered as State Minister Matthias Platzeck put “the world’s first” wind-driven hydrogen hybrid power plant officially into operation. Hooray, everyone cheered and patted each other on the back – we’re rescuing the planet!
“…the entire big problem of renewable energy sources is solved,” proclaimed German daily Süddeutsche Zeitung!
Werner Diwald Chairman of operating company Enertrag AG, which no doubt is getting subsidized up the wazoo for this, proclaimed:
It’s more than a project; it’s the cornerstone of the energy transformation.”
Brandenburg’s Minister Matthias Platzeck (Socialist Party) added that environmentally the plant is “a huge step forward”.
So just what is this marvel of German green engineering: the hydrogen-hybrid plant? How does it work?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Science Skeptical tells us. Electricity produced by wind parks is used to produce hydrogen gas whenever the wind power is not needed. This hydrogen is then stored and used later when needed to drive a gas turbine, which powers an electric generator, which in turn produces electricity. What a clever and elegant solution! At least that’s what the media, politicians and some environmentalists think.
19% efficiency
Now let’s return to reality. It only takes a freshman engineering student to explain the utter folly of this system. Breaking it down in steps we have: wind > wind turbine rotor > wind generator > AC electricity > DC electricity > electrolysis > hydrogen > pump/storage vessel > gas turbine > generator  > AC electricity > transmission lines > consumer. That’s a long chain of energy transformation steps. Unfortunately though, each step involves LOSSES.
Of course, none of the media, politcians or greens even bothered to calculate the efficiency and cost of this system. But thankfully Keil has, and reached a grand efficiency of 19%, meaning 81% of the energy is lost as heat into the atmosphere. According to Keil with such a system electricity will end up costing the consumer over $1.00 per kwh. And if the hydrid system uses solar panels to provide the primary energy, then the cost rises to over $1.20/kwh.
Conclusion: when bureaucrats and other energy engineering morons take over the management of energy, you get a system that is hardly more efficient than the Greek government.
Finally the Süddeutsche Zeitung quotes GDR physicist and now current Chancellor of Germany Angela Merkel, who led the project’s groundbreaking ceremony in 2009:
We are going to learn something from this power plant.”
Indeed we will.
====================================
For those of you who may have forgotten, here’s yet another government-run energy engineering marvel (again from East Germany): click here!
 
Share this...FacebookTwitter "
"Whenever I tell people I work with solar cells I am asked the same two questions: are they ever going to be really cheap? And can you get me some? While the answer to the second question is no, the answer to the first is a lot more positive. Year on year, solar panels have been plunging in price and improving in the efficiency with which they can convert light into energy.  At the same time fossil fuel costs continue to rise, and in the next few years we will reach the point where the costs overlap – some figures suggest this may have already happened. The question is not whether solar energy can supplant fossil fuels as the cheapest means to produce energy, but rather when.  While this has provided an enormous boost to the solar industry, the main excitement in the solar sector today is due to a new type of material called perovskite. Combining some of the best qualities of more mainstream materials, it has proved incredibly flexible – to the point that University of Sheffield researchers have manufactured perovskite solar cells as a spray-on liquid. So what is perovskite, and what’s the buzz around it? Solar cells, the component of solar panels that reacts to light, are built from what are known as photovoltaic materials. When light hits these materials electrons are freed to move through the material. With careful design of the structure of these solar cells, these electrons can be collected into a flow of electrical current. This is the process that provides the somewhat magical property of solar panels – sunlight in and electricity out. Broadly speaking, solar cells can be divided into two distinct groups – those based on inorganic photovoltaic materials, such as silicon or cadmium telluride, and those based on specific organic compounds, such as PCDTBT. Both have their own respective advantages and disadvantages.  The inorganic materials are already industrially well-established, capable of converting light to electricity at greater than 20% efficiency and create solar panels with more than 25-year lifespans. The downside is that the raw materials required, particularly with silicon, can be expensive. Organic solar cells are based on potentially low-cost materials and can even be manufactured from a liquid solution, which makes them very fast and cheap to produce. However, even on a laboratory-scale, organic solar cells struggle to achieve efficiencies of more than 10%. Even more crucially, the organic compounds gradually decompose under light, often reducing panel lifetime to the order of months or weeks rather than years. Consequently these organic materials have rarely been used to produce solar panels, as no one likes the idea of having to climb on their roof to replace them every six months. Ideally we want a solar cell with the performance and long term stability of inorganic materials with the ultra-low cost of organic materials. In the past few years solar energy research has witnessed the emergence of a remarkable new class of materials known as perovskites. This is a hybrid organic-inorganic material, essentially an organic compound with an inorganic element attached. Perovskite refers to the specific type of crystal structure, which occurs naturally in certain minerals. These hybrid compounds have this crystal structure but are also a complex combination of organic ammonia and methyl groups with inorganic lead iodide or lead chloride molecules attached.  The reason for the excitement surrounding these materials is the frankly staggering rate at which they have developed. Previously whenever a new material was discovered it had taken some 10-20 years of research to reach an efficiency rate of even 10%. Perovskite solar cells only emerged in 2012, but have already clocked up conversions of more than 19% efficiency. This blistering rate of development is unprecedented in solar research. As a hybrid material, as well as boasting good efficiencies as with inorganic materials, perovskites can also take advantage of organic solar materials’ capacity to be applied as a liquid solution. This is what Professor David Lidzey’s group at the University of Sheffield has taken advantage of, spraying the perovskite as a liquid coating onto a substrate material. This allows solar cells to be manufactured at high volumes and low cost. Does this mean that all future solar cells will be based on perovskites? It’s far too early to say. Although they have many benefits there are still a number of key challenges to be overcome.  There are some questions regarding the potential environmental impact of the lead content of the material (although work is ongoing to remove the requirement for lead) and how easily production can be up-scaled to a useful commercial size. As with organic solar cells, their long term stability is also highly questionable and they are particularly sensitive to moisture – a few drops of water can completely destroy the material. So building a perovskite solar panel module capable of surviving for decades outdoors is most likely still some way off – in fact there’s no guarantee it’s even possible. But what is for certain is that the potential of perovskite solar cells is staggering, and if the material’s promise can be realised it could completely revolutionise the capabilities of solar energy. "
"Public bikesharing schemes are sprouting up in towns and cities worldwide. The bikes are generally provided without helmets, and this has led to concerns regarding the risk of serious head injuries.  It has been shown that the users of these bike hire schemes are less likely to wear helmets, high-visibility clothing or specialist cycling Lycra than people riding their own bikes. We’ve argued this is a good thing, as it helps normalise the image of cycling away from a specialist past-time, reducing the perception that riding a bicycle is a risky activity or only for super-sporty people.  But a recent study by Janessa Graves and colleagues published in the American Journal of Public Health concluded that there was a link between the introduction of bikesharing schemes in North American cities and the risk of bicycle-related head injuries.  So it was argued that helmets should be incorporated into the schemes as standard from the outset. We suggest that these concerns are misplaced, and agree with the many other commentators who have argued that the study’s data don’t justify the authors’ conclusions. In fact, the paper’s data could be reasonably interpreted to argue the opposite – that the take-up of bikesharing schemes leads to lower risk of injury.  The authors compared the number of injuries (head and non-head) in five cities with bikesharing schemes, and five without, over a 12 month period. Their result is the percentage of cyclist injuries that are head injuries, and this does significantly increase in cities with bikesharing schemes, from about 42% pre-bikeshare scheme to 50% afterwards. In the control cities, the figure dipped from 38% to 36%. But the result does not represent the risk of head injuries; that is, they did not divide the number of head injuries recorded by a measure of the total amount of cycling being done in the city, to get the “risk per trip” or “risk per kilometre cycled”.  As the study’s data don’t include the amount of cycling trips in each city, we’re also unable to calculate the risk per kilometre cycled. But we can examine what happens to the absolute numbers: the number of head injuries falls in absolute terms in cities with bikesharing schemes relative to the baseline. Crucially, it also falls relative to the control cities. The only reason the percentage of head injuries rises is that the number of non-head injuries fall even faster, leaving head injuries as a greater proportion – but not greater in absolute numbers.  This can be represented as the change over time in the likelihood of injury in cities with a bikesharing scheme versus those without. On this measure of relative change (technically, an odds ratio) all values are less than 1, which indicates that things have got relatively safer in the bikesharing cities.  So a reasonable interpretation would be that cities with bikesharing schemes experienced decreases in all types of injury, including a trend towards a decrease in head injuries, relative to those cities without. In other words, the data suggest injury rates seem to have gone down in bikesharing cities. It’s quite plausible that this actually underestimates the true decrease in risk, since introducing bikesharing schemes is likely to have led to an increase in total cycling. There are other reasons to be sceptical of the study’s conclusions.  One of the surprising findings in the study is that the percentage of head injuries rises fastest among children. This is precisely the opposite of what one would expect if bikesharing schemes were to blame, since children are less likely to use bikesharing schemes – the bikes are adult-sized, and riders typically need a credit card. So blaming a rise in head injuries among children on a lack of helmets among bike hire users doesn’t seem consistent, as researcher Kay Teschke has pointed out. The study’s conclusions are also not supported by other evidence pointing to lower injury rates on bikeshare bikes than on personal bikes. The Graves study doesn’t record who is being injured – that is, whether or not they are helmet wearers and whether or not they are bikeshare users. Our recent modelling study of the London bikesharing scheme did have these details, and found that injury risks were lower among bikeshare users compared with the average for cyclists in the same area. This was despite the fact that those hiring bikes were much less likely to wear helmets or hi-viz than other cyclists. Initial reports from New York also suggest that risk of injury among bikesharing users is lower than expected. It’s important to be cautious before attributing either more or fewer injuries to bikesharing schemes. For a scheme to be responsible for the 41% reduction of non-head injuries reported by the Graves paper, the scheme would have needed to have replaced a substantial proportion of existing cycling trips and carry a very low risk of non-head injuries among users (for example, equivalent to a 41% replacement of existing cycling combined with a zero risk of non-head injuries among users).  Such replacement of existing cycling levels are just implausible, because hire bikes are usually only available in a relatively small, downtown part of a city, and represent only a small proportion of cycle trips even in those areas. Our study in London also suggested the majority of trips are new cycling trips. As an example, bike hire trips in London make up only around 4% of all cycle trips. The study’s authors do not estimate the proportion of bikesharing trips in their cities, but do show that the average number of shared bicycles is three per 1000 population, which is only around 2.5 times higher than in London. So a plausible estimate of the amount of cycling due to bikesharing (2.5 x 4%) is around 10% of cycling journeys.  In other words, far smaller than the 41% replacement of existing cycling needed  to be responsible for the change in injuries observed. It’s more likely that the introduction of a bikeshare scheme was one of a number of policies and changes that, taken together, led to safer cycling in these cities. So a broad ecological study such as the paper from Graves and her co-authors tells us little about the effect bikesharing schemes have on injury rates amongst users. To truly understand this we need direct evidence and individual-level statistics about users and non-users and injury rates. To understand how cycle hire may affect risks for all cyclists we are going to need evidence from different sources, such as qualitative data, policy analysis, and systems models.  What this study does tell us fits with the possibility that injury risks may actually be lower when using hire bikes, and that the introduction of these schemes may go hand-in-hand with a general lowering of risk. So for now, calls for bikesharing schemes (or all cyclists) to require helmets are not supported by the evidence available. Hard Evidence is a series of articles in which academics use research evidence to tackle the trickiest public policy questions."
"**France will begin to ease its strict coronavirus restrictions this weekend, allowing non-essential shops to reopen, President Emmanuel Macron has said.**
People will also be able to share ""moments with the family"" over the Christmas period, Mr Macron announced.
But he said bars and restaurants would have to remain closed until 20 January.
France has reported more than 2.2 million cases and more than 50,000 confirmed coronavirus-related deaths since the start of the pandemic.
In a televised address on Tuesday evening, Mr Macron said the country had passed the peak of the second wave of virus infections.
He said that the bulk of lockdown restrictions would be eased from 15 December for the festive period, with cinemas reopening and general travel restrictions lifted, as long as new infections were at 5,000 a day or less.
On Monday, France reported 4,452 daily Covid-19 infections - its lowest tally since 28 September.
The latest seven-day rolling average for new infections in France is reported to be 21,918. That figure peaked at 54,440 on 7 November.
Mr Macron said the recent news of successful vaccine trials offered ""a glimmer of hope"" and that France would aim to begin vaccinations against Covid-19 ""at the end of December or at the beginning of January"", starting with the elderly and most vulnerable.
The French president said the situation would be reviewed on 20 January, and if infections remained low, bars and restaurants would then be permitted to reopen. Universities would also be able to accept students again.
However, if the situation had worsened, he said he would look at options to avoid triggering a third wave.
""We must do everything to avoid a third wave, do everything to avoid a third lockdown,"" Mr Macron said.
He later tweeted to say that all businesses forced to remain closed during the restrictions, such as restaurants, bars and sports halls, would have the choice of receiving up to â¬10,000 (Â£8,900) from a ""solidarity fund"" or the payment of 20% of their turnover.
He said that France's ski resorts may have to remain closed until next year because the current risks associated with the virus made it difficult for such sports to resume.
However, he said he would discuss the issue with other European leaders and provide an update in the coming days.
Ski resorts were responsible for numerous outbreaks of Covid-19 cases across Europe in the early days of the pandemic.
Mr Macron said the lockdown would be replaced by a nationwide curfew between 21:00 and 07:00, except on Christmas Eve and New Year's Eve.
France has been under a second national lockdown since the beginning of November. People have only been permitted to leave home to go to work, buy essential goods, seek medical help or exercise for one hour a day. Anyone going outside must carry a written statement justifying their journey.
While all non-essential shops, restaurants and bars have been shut, schools and crÃ¨ches have remained open. Social gatherings have been banned.
Measures to deal with coronavirus outbreaks remain in place across Europe, but a reduction in daily reported cases in some areas - coupled with the reported success of a number of vaccines - has led countries to revisit their restrictions. Some of the latest developments include:"
"**Gyms and non-essential shops in all parts of England will be allowed to reopen when lockdown ends next month, the prime minister has announced.**
Boris Johnson told the Commons that the three-tiered regional measures will return from 2 December, but he added that each tier will be toughened.
Spectators will be allowed to return to some sporting events, and weddings and collective worship will resume.
Regions will not find out which tier they are in until Thursday.
The allocation of tiers will be dependent on a number of factors, including each area's case numbers, the reproduction rate - or R number - and the current and projected pressure on the NHS locally.
Tier allocations will be reviewed every 14 days, and the regional approach will last until March.
The PM, who is self-isolating after meeting an MP who later tested positive for coronavirus, told MPs via video link he expected ""more regions will fall - at least temporarily - into higher levels than before"".
He said he was ""very sorry"" for the ""hardship"" that such restrictions would cause business owners.
Speaking later at a Downing Street briefing, Mr Johnson added that ""things will look and feel very different"" after Easter, with a vaccine and mass testing.
He warned the months ahead ""will be hard, they will be cold"" - but added that with a ""favourable wind"" the majority of people most in need of a vaccination might be able to get one by Easter.
Until then, the PM said, there would be a three-pronged approach of ""tough tiering, mass community testing, and [the] roll-out of vaccines"".
Describing how the tiers had become tougher, the PM said:
Where pubs and restaurants are allowed to open, last orders will now be at 10pm, with drinkers allowed a further hour to finish their drinks.
Indoor performances - such as those at the theatre - will also return in the lower two tiers, although with reduced capacity.
In terms of households mixing, in tier one a maximum of six people can meet indoors or outdoors; in tier two, there is no mixing of households indoors, and a maximum of six people can meet outdoors; and in tier three - the toughest tier - household mixing is not allowed indoors, or in most outdoor places.
In all tiers, exceptions apply for support bubbles. From 2 December, parents with babies under the age of one can form a support bubble with another household.
Mr Johnson said the tiers would now be a uniform set of rules, with no negotiations on additional measures for any particular region.
Measures in Scotland, Wales and Northern Ireland continue to be decided by the devolved administrations, but a joint approach to Christmas, involving all four nations, will be set out later in the week.
The prime minister said: ""I can't say that Christmas will be normal this year, but in a period of adversity time spent with loved ones is even more precious for people of all faiths and none.
""We all want some kind of Christmas; we need it; we certainly feel we deserve it.
""But this virus obviously is not going to grant a Christmas truceâ¦ and families will need to make a careful judgement about the risks of visiting elderly relatives.""
For the third week running we have had some positive vaccine news, but the announcement about the toughened tiers is a reminder, if we needed any, that the next few months will be tough.
Ministers and advisers have been hinting for the past week that the tiers will be toughened - and that is exactly what has happened.
Attention will now naturally turn to which areas will be in which tiers.
Deciding that is a complex equation that will take into account whether the cases are going up or down, the percentage of tests that are positive, hospital pressures and infection rates among older age groups.
To give a flavour of how complex this is places in the North West and Yorkshire have some of the highest rates but they are falling the fastest.
London and the South East have lower rates and more hospital capacity but cases are going up.
Fine judgements will have to be made. We will find out on Thursday.
Mr Johnson also announced changes to sport for both spectators and participants.
While elite sport has continued behind closed doors during the lockdown, grassroots and amateur sport has been halted since 5 November.
From 2 December, outdoor sports can resume, while spectators will be allowed to return in limited numbers. Some organised indoor sports can also resume.
In the lowest risk areas, a maximum of 50% occupancy of a stadium, or 4,000 fans - whichever is smaller - will be allowed to return. In tier two, that drops to 2,000 fans or 50% capacity, whichever is smaller.
In tier three, fans will continue to be barred from grounds.
In tiers one and two, business events can also resume inside and outside with tight capacity limits and social distancing, as can indoor performances in theatres and concert halls, the government's plan says.
Labour leader Sir Keir Starmer described the government's return to the regional system as ""risky... because the previous three-tier system didn't work"".
He added that decisions on which areas will belong to each tier must be taken without delay - ""I just can't emphasise how important it is that these decisions are taken very quickly and very clearly so everybody can plan.
""That is obviously particularly important for the millions who were in restrictions before the national lockdown, because the message to them today seems to be 'you will almost certainly be back where you were before the national lockdown - probably in even stricter restrictions'.""
Helen Dickinson, of the British Retail Consortium, said shops would be ""relieved"" at the decision to allow them to reopen.
""Sage data has always highlighted that retail is a safe environment, and firms have spent hundreds of millions on safety measures including Perspex screens, additional cleaning, and social distancing and will continue to follow all safety guidance,"" she said.
But the UK hospitality industry warned the new rules ""are killing Christmas and beyond"" and said pubs, restaurants and hotels faced going bust.
Meanwhile, a further 15,450 positive coronavirus cases were recorded across the UK on Monday. There have also been a further 206 deaths within 28 days of a positive test. Figures can be lower on a Monday, due to a lag in reporting.
Earlier, it was announced that daily coronavirus tests will be offered to close contacts of people who have tested positive in England, as a way to reduce the current 14-day quarantine period.
Mr Johnson said people will be offered tests every day for a week - and they will not need to isolate unless they test positive.
He also said rapid tests will allow every care home resident to have up to two visitors tested twice a week."
nan
"There has been a huge increase in attention recently to the problem of wildlife poaching, mostly from the stream of grisly stories from Africa about rhino and elephants illegally killed for their horn and ivory.  At the same time there has been a growing awareness of trophy hunting, with pictures of hunters, sometimes minor celebrities, posing next to their kill prompting furious outcry. But these two forms of wildlife are being wrongly lumped together. Despite both resulting in an animal’s death, they are entirely different. Poaching is the illegal killing of wildlife, undertaken for reasons that may include revenge, meat for food or sale, tradition or money.  Poachers might be poor locals from the area to foreigners capitalising on the lucrative illegal wildlife trade. By contrast, trophy hunting is the entirely legal killing of wildlife, often carried out by rich foreigners for sport and enjoyment. Both result in one animal fewer in the wild, but the similarity ends there. Poaching is uncontrolled and unmonitored. With wildlife seen as a common pool resource by poachers, animals suffer from the tragedy of the (illegal) commons where poachers may feel that if they do not kill (and benefit from) that elephant, someone else will. The result is massive over-exploitation. The basic economic fact of rising prices due to high demand and low supply means that the rarer an animal gets, the higher the price. The incentive drives poachers to kill as many animals as possible. Trophy hunting on the other hand is typically a strictly controlled, monitored and regulated business where safari game parks must apply for government-issued permits for the number of animals they wish to hunt.  If animals are to be taken out of the country, further permits are needed for their transport. So we have a good idea of the number of animals trophy hunting kills worldwide, and because no business wants their income to stop, game parks tend to look after their wildlife very well, which sees population numbers increase over the years. For comparison, take a typical livestock farmer: he wouldn’t take all his cattle to market at once as there would be none left to sell the following trip. It makes sense to only sell a few at a time, and in the meantime grow his herd in order to have more animals to take on the next trip and so more to sell. Safari game parks see their wildlife in a similar fashion and prefer to increase their game animals.  In fact, time and again it has been shown that trophy hunting operations can expand wildlife populations rather than reduce them. As they are able to legally reap the economic rewards from the wildlife they look after, the game parks have an incentive to conserve these species. And some benefits trickle down to the local community – for example creating more jobs – which means locals have an incentive not to poach wildlife illegally because now they can benefit from its value through the legal economy.  This approach has worked wonders in Namibia through the conservancy model, in which local communities living around the wildlife to be protected are put in charge of their wildlife. They are able to offer trophy hunts to tourists, and so reap the benefits.  This has dramatically reduced poaching, and some trophy species populations are booming as a result. So the call to ban trophy hunting or prevent hunters from bringing home their trophies is worrying. For example, the recent ban of ivory imports from Tanzania and Zimbabwe to the US may have a dramatically negative effect on those countries elephant populations. By taking away the opportunity for local communities to benefit from these animals the land is likely to be turned over to agriculture.  In a previous instance, Kenya stopped trophy hunting in the 1970s, and land that had previously been set aside for wildlife was converted to livestock and crop farming. The result was widespread population declines of all game species, as crops and cattle now held more value. Whether this is the result in Botswana and Zambia, where trophy hunting was recently banned, remains to be seen. Not all forms of wildlife killing are the same, and we must be careful to ensure the differences are understood. It’s easy for Westerners to jump to conclusions about wildlife management, to see a picture of a dead lion and suppose any form of killing to be wrong. But if we take a step back and see the big picture about how these communities – isolated, poor – have to live with dangerous animals such as elephants and lions, we must understand that the picture is far more complex.  Indeed, as the Namibian example has shown, carefully managed trophy hunting may be one way to reduce poaching of wildlife."
"Landslides don’t attract the same media attention as more familiar geological hazards such as earthquakes and volcanoes. And yet they can be just as disastrous and, in fact, 2014 has been a particularly bad year.   In Hiroshima, Japan, a series of landslides has left 39 people confirmed dead and a further 52 missing. In March a hillside collapsed in Washington state, US, leaving 43 dead, and in May massive mudslides in Afghanistan caused several thousand deaths. In early August, landslides in Nepal left almost 200 dead or missing. Landslides can vary greatly in speed, water content and size. Different landslides can look very different and can vary greatly in their destructive power – size is not everything. Nevertheless, they pose a significant hazard to human life, buildings and transport routes.  Statistics about deaths from landslide-generated disasters can be a little difficult to come by, since some agencies group “wet” landslides with floods and damage caused by landslides generated during earthquakes are often classed as earthquake damage.  The recent Japanese landslides, for example, are mud flows or debris flows (a watery mix of rock and soil) generated from a slope collapsing further up river. Heavy rainfall had caused large volumes of slope collapse material to be incorporated into the river waters, giving the mud flows and debris flows, which have overwhelmed villages within the mountain valleys. Villages built a little further up from the floor of the valleys will be much less prone to the destructive debris flows, which occur in the base of the mountain valleys. Landslides tend to be most frequent and destructive in steep mountainous areas, as they are an expression of a natural process which reduces steep slopes to less steep slopes. Data collected over many years suggests that landslides are ranked 7th in the natural disaster table well after the major historical killers of droughts, floods and storms, but close on the heels of earthquakes and volcanic eruptions.  Landslides generally require a trigger, most commonly extreme rainfall or large earthquakes. Earthquakes initiate landslides by locally – and very briefly – changing the gravity experienced by a slope, which tips it beyond its stability point. Extreme rainfall temporarily drives the water pressure within a slope to a critical level; the stresses within the slope then exceed their stability point and the land begins to slip down the slope. Landslides are then driven simply by gravity, often assisted by a loss of strength in the region at the base of the slide.  So parts of the world with steep slopes, intense rainfall and large earthquakes tend to be most prone to this kind of geological disaster. But they aren’t always massive, deadly disasters. Smaller, often more benign landslides can be initiated by roadway construction, building works, river or wave/tidal undercutting of slopes. If engineers or builders mess up and make a slope too steep, it may no longer be held up by the intrinsic strength of the rock. In the same way, apparently simple things like leaky water pipes or inadequate drainage on man-made slopes can also start landslides. This is particularly so water is either retained in, or drains particularly slowly from, the rock or soil of the slope. Certain kinds of clays are particularly notable for these features – and this seems to have exacerbated the landslides at Hiroshima.  Such conditions are generally well understood by geo-technical engineers, so can normally be predicted in artificial slopes. Next time you are on a train or car look out for adjacent slopes which have lines of gravel or rock-fill running down them to drain the water away. These have been assessed by engineers as potential unstable slopes, should the area be exposed to heavy rainfall over long periods of time. A lack of trees can also make steep slopes more prone to landslide, since trees naturally intercept and slow heavy rainfall and their roots help bind the soil together. This is another example of natural and man-man factors overlapping – environmentalists blamed deforestation for a recent landslide in India which killed 30 people. These facts gives clues as to how to limit landslides (triggered by heavy rainfall) on natural slopes, by managing water flow across slopes, limiting water ingress into slopes and by planting tree cover to slow water delivery into water courses and so provide natural slope binding. The most destructive landslides of all are those that end up in water (or occur on underwater slopes) and generate tsunamis. For example, this kind of destruction can happen if a volcano flank collapses into water while erupting. Indeed, the largest tsunami ever recorded was set off by a landslide in Lituya Bay, Alaska, in 1958. Waves reached a height of more than 500m, far taller than any skyscraper built at the time, but killed just five people in the sparsely populated fjord. Fortunately such disasters have been very rare in human history – the 1792 Unzen eruption, earthquake, landslide and tsunami, which killed 15,000 people in Japan is a notable instance. However we know from geological evidence on the sea floor, for example around the Canary Islands, that mega-tsunamis must have been generated by similar slides – landslides that would make 2014’s crop seem tiny by comparison."
"
Of course the big news today is the 8.8 earthquake in Chile and the Tsunami warning stemming from it. There’s not much I could add that’s not already being covered, but I thought this image from the American West  Coast and Alaska Tsunami Warning Center was interesting. They posted this map with  estimated arrival times of tsunami waves generated by the 8.8 earthquake  earthquake off the coast of Chile:

Even more interesting is the map they published of the path of energy distribution in the waves. It looks like Hawaii will dodge the worst of it:

The image above depicts wave height in centimeters.
I’m not posting direct links to these images at the center since I don’t want their server to be overwhelmed, so I’ve stored them locally.
It looks like the Aleutian islands may get some significant portion of this as will New Zealand.
The Tsunami Warning Center has a very detailed list of estimated  arrival times for waves generated by Saturday’s 8.8 magnitude  earthquake at many locations along the west coast of the United States. On the US West coast, the first waves to arrive will be in San Diego just after noon PST.
BONUS:
Quite possibly the stupidest science headline ever,  from MSNBC and LiveScience:
Big quake question: Is nature out of control?
and
Chile Earthquake: Is Mother Nature Out of Control?
Newsflash: Nature has never been within our control.
This article at Livescience which MSNBC picked up was written by Jeanna  Bryner, 
who has also written articles on “The Perils of Text Messaging While Walking” and “Wanted:  The Equation of Love”
Her apparent justification for the current headline:
“One scientist, however, says that relative to a time period in the past,  the Earth has been more active over the past 15 years or so.”
Since the introduction of the Internet and proliferation of live global satellite news coverage, also in the past 15-20 years, we certainly do hear more about what goes on around the planet, often within minutes of occurrence. Does that mean the planet is getting more active? Not neccessarily, but you can draw the conclusion that are reporting system has improved dramatically during that period.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8dd9fedf',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Confident predictions of catastrophe are unwarranted.

A commentary by Richard S. Lindzen in the WSJ
Is there a reason to be alarmed by the prospect of global warming? Consider that the measurement used, the globally averaged temperature anomaly (GATA), is always changing. Sometimes it goes up, sometimes down, and occasionally—such as for the last dozen years or so—it does little that can be discerned.
Claims that climate change is accelerating are bizarre. There is general support for the assertion that GATA has increased about 1.5 degrees Fahrenheit since the middle of the 19th century. The quality of the data is poor, though, and because the changes are small, it is easy to nudge such data a few tenths of a degree in any direction. Several of the emails from the University of East Anglia’s Climate Research Unit (CRU) that have caused such a public ruckus dealt with how to do this so as to maximize apparent changes.
The general support for warming is based not so much on the quality of the data, but rather on the fact that there was a little ice age from about the 15th to the 19th century. Thus it is not surprising that temperatures should increase as we emerged from this episode. At the same time that we were emerging from the little ice age, the industrial era began, and this was accompanied by increasing emissions of greenhouse gases such as CO2, methane and nitrous oxide. CO2 is the most prominent of these, and it is again generally accepted that it has increased by about 30%.
The defining characteristic of a greenhouse gas is that it is relatively transparent to visible light from the sun but can absorb portions of thermal radiation. In general, the earth balances the incoming solar radiation by emitting thermal radiation, and the presence of greenhouse substances inhibits cooling by thermal radiation and leads to some warming.
That said, the main greenhouse substances in the earth’s atmosphere are water vapor and high clouds. Let’s refer to these as major greenhouse substances to distinguish them from the anthropogenic minor substances. Even a doubling of CO2 would only upset the original balance between incoming and outgoing radiation by about 2%. This is essentially what is called “climate forcing.”
The full article may be found here.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e90aa8677',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Britain, like many countries, has already witnessed the establishment of many non-native species of plants and animals, and about 15% have become problematic and so termed “invasive”. Some were deliberately introduced, for example plants brought for their attractive flowers such as giant hogweed (Heracleum mantegazzianum), while others have stowed away on ships or escaped from captivity.  These new arrivals can be considered one of the major threats to native biodiversity. Some prey directly on native species, compete for the same food or habitat, or bring with them diseases to which native species have no defence. Some generate such profound effects on their new environment they are called “ecosystem engineers”. This is a threat that has to be taken seriously and managed.  We brought together a group of experts to examine the changing landscape and predict what species may pose problems in the next 10 years in order to be better prepared.  While our international group of ecologists covered potential threats from land, sea, and freshwater habitats, we looked only at the impact they might have on biodiversity. More often than not however, it’s likely that new invasive alien species threats would have an impact on the economy too, and could have also an impact on human health. Our experts drew up a list of species with the potential to arrive in Great Britain and become problematic within the next ten years. A total of 591 non-native species were considered, of which 93 were considered to constitute at least a medium risk to native biodiversity, and a final ranked list of 30 species was compiled. Seen as the most serious threat, at the top of that list is the quagga mussel (Dreissena rostriformis bugensis). It was unanimously awarded the highest score for the threat it poses based on its arrival, its ability to establish populations, and the impact it has on its new environment. This ecosystem engineer is capable of dramatically altering the aquatic environments in which it thrives.  Quagga mussels are extremely efficient filter-feeders, changing the chemical nature of the water, resulting in water becoming clearer. A seemingly simple change, this can have serious cascading effects that affect the vital phytoplankton and zooplankton upon which the food webs are based. This in turn could stimulate the competitive release of cyanobacteria which would lead to an increase in frequency of toxic algal blooms. Argentine ants (Linepithema humile) are another example of an ecosystem engineer highlighted as an impending threat. They form enormous super-colonies, with one such super-colony recorded as covering 6000km from Spain to Italy.  The Asian shore crab (Hemigrapsus sanguineus) was a species ranked within the top ten, and has subsequently been confirmed as a new arrival to Britain, found on beaches in Wales and Kent.   Other species within the top ten include the raccoon (Procyon lotor), the African scared ibis (Threskiornis aethiopicus) and the Asian hornet (Vespa Vespa velutina).   Racoons are considered a threat due to their highly adaptable and intelligent nature, and the possibility of establishment in Britain. This could come from escape from zoos or private collections.  The African sacred ibis is a wading bird capable of heavily preying on birds, fish, amphibians and invertebrates, and a breeding population is already established in France.  The Asian hornet is also now well-established on the continent and is a voracious predator of pollinating insects, including honeybees and is anticipated to arrive in Great Britain from the continent. The GB Non-native Species Secretariat has been forward-thinking in developing an alert system for the arrival of the Asian hornet. It’s hoped this system will, in time, provide a means to watch for many potential new arrivals to Great Britain.  It’s essential that we are prepared not only to manage problematic invasive species effectively but also to work to prevent new arrivals establishing populations that could cause threats in the first place.  Increasing our understanding of invasion biology is a necessary building block that will underpin action we take in the future. A draft EU regulation on invasive species has been adopted and will influence national policy across Europe in coming years, and a Europe-wide index of invasive species has been compiled.  It’s important to raise awareness among the public of what aliens to expect and where they might see them, as surveillance will be critical. An alert system from the Biological Record Centre designed to help tracking the appearance of invasive alien species provides an excellent starting point for people to record what they’ve seen."
"On Thursday, when climate activists spilled on to the road, singing “We’ll take climate change seriously” to the tune of My Bonnie Lies Over the Ocean, they felt confident of a big victory. Heathrow’s expansion had been declared illegal because it is incompatible with legislation committing the government to zero carbon emissions by 2050. The proposal, which would have brought 260,000 new flights a year to the airport, was dead and campaigners celebrated because similar schemes were now doomed.  But are they? It’s tempting to forecast the future using simple cause and effect, assuming that the court of appeal ruling must now cause Heathrow and similar climate-busting projects to stop, thus accelerating Britain’s transition to a carbon-free economy. But sadly, that’s just one possible scenario. The effect of the ruling could be to expand smaller, regional airports. Most Heathrow flights are for pleasure; determined travellers could pile into Luton, Stansted, Birmingham. More might decide to drive. Even though the government says it won’t appeal, the transport minister, Grant Shapps, has also said, in principle, it does “want to see airport expansion”. Heathrow will appeal against the decision. It could win. Nature could still lose. The future is uncertain because cause and effect are rarely simple. A single cause can have any number of effects, some predictable but many unforeseen. Something can be generally certain – such as the law courts having to take the government’s commitment to the Paris accord seriously – but specifically ambiguous: because no two projects are identical, all future judgments may not favour the environment. Minor differences in projects may result in widely varying outcomes. You can see the same difficulty with epidemics. That they will keep happening is generally certain, but it is risky to extrapolate from one to the other because there is no profile of an epidemic. We don’t know where or when the next epidemics will break out, or what diseases might cause them. They don’t repeat themselves. Expecting to see the future through a simple unfolding of cause and effect is easy but risky. But simple cause-and-effect thinking has plagued forecasting since telling the future became big business at the end of the 19th century. One of the founding fathers of economic forecasting, Roger Babson, was adamant in his Newtonian view of the world. For him, every action had to be associated with an equal and opposite reaction. He not only assembled the largest collection of Newtoniana in North America, but based his entire economic theory on this premise, one of the first (but not the last) to believe he could develop laws of economics that had the same absolute certainty as the laws of physics. His view of cause and effect had a boost in 1929 when, as he had predicted, Wall Street crashed. This, Babson argued, was because periods of wasteful exuberance always produce the need for sensible self-discipline. But he had been forecasting a correction for the previous three years. And when, in May 1931, he announced the market had bottomed out and it was time to get back in, he was wrong: the US economy didn’t recover for a decade. Like most pundits, Babson was poor at reviewing his own forecasts; he continued to believe everything in life could be reduced to simple laws. As a young man, he was told he had contracted TB – at the time, the single greatest cause of death in the western world. However shattering, the diagnosis was nonetheless ambiguous: it could mean a quick death or that the disease would lie latent for decades. Not a man to stand by and wait, Babson devised a health regimen that he was convinced would keep catastrophe at bay. He chose an office surrounded by windows, all of which he kept wide open, even throughout the brutal Massachusetts winters. He and his secretary wore mittens and hooded, floor-length wool cloaks as they worked, the secretary typing with wooden mallets and Babson wearing an electrically heated pad in his back. Was it these eccentricities that allowed Babson to live to 91? It’s impossible to know. He might not even have had TB, since no accurate test was devised until years later. The same problem bedevils studies of leadership. It is fashionable to follow high-profile business leaders and draw from them lessons of success. Steve Jobs and Bill Gates created huge, valuable companies – but how far was their success attributable to them as individuals, and how far the result of other factors: the thousands of people who worked for them, the short-sightedness of their competitors, the wealthy ecosystem they inherited, and a growing educated populace? It’s impossible to do the experiment, Apple without Jobs, or Microsoft without Gates. Seeing individuals as single causes of complex success is an attribution error routinely made when analysing politicians too: did Churchill really win the Second World War, or were Hitler’s unforced errors and the brute force of the Soviet Union more important? Simple causes make memorable stories, but that doesn’t mean they’re true. The sheer complexity of life militates against straightforward narratives and it can feel as though the sides have come off the billiard table, balls dropping off or arriving from anywhere. You don’t know how many variables there are, where they may come from or which, if any, will matter. That’s why, increasingly, efficiency gives way to robustness. When the stakes are high (as in epidemics) a bold response is safer than complacency. Which is why the campaigners celebrating last week’s appeal court judgment will have to return to their fight if they want to increase their chances of environmental success. • Margaret Heffernan is the author of Uncharted: How To Map the Future Together"
"**At Christmas, coronavirus restrictions will be eased to allow people to mix with a slightly wider circle of family and friends.**
Across the UK, people will be able to form ""bubbles"" of three households over a five-day period. Who can be in your bubble will depend on where you are.
Between 23 and 27 December, the three households in a ""Christmas bubble"" can mix indoors and stay overnight.
Northern Ireland has a window of 22 to 28 December, to allow time to travel between the nations.
Bubbles will be allowed to meet each other:
The bubbles will be fixed, so you will not be able to mix with two households on Christmas Day and two different ones on Boxing Day. Households in your Christmas bubble can't bubble with anyone else.
There will be no limit to the number of people who can join a bubble in England, Wales and Northern Ireland, although the English guidance says it should be ""as small as possible"".
But the Scottish government has said Christmas bubbles should contain no more than eight people. Children under 12 will not count in the total.
The rules about what counts as a household also depend on where you are.
In England if you have formed a support bubble with another household, that counts as one household, so you can join with two other households in a Christmas bubble.
The Scottish government guidance says any Christmas bubble should contain no more than one ""extended household"".
People who are self-isolating should not join a Christmas bubble. If someone tests positive, or develops coronavirus symptoms up to 48 hours after the Christmas bubble last met, everyone will have to self-isolate.
You will still be able to see other people who are not in your bubble outside your home, subject to the rules of the tier where you are staying.
Travel restrictions will be lifted to allow people to visit their families anywhere in the UK.
But the government has warned that there will not be extra public transport laid on.
It has urged people to plan their travel in advance.
You will not be allowed to go with your Christmas bubble to hospitality settings, such as pubs and restaurants, or to entertainment venues.
You can meet people outside your Christmas bubble, but only outside the home and in line with the tier rules of the area in which you are staying.
Places you can meet include parks, beaches, open countryside, public gardens, allotments and playgrounds.
Some traditional Christmas activities will also be allowed in England:
Scottish First Minister Nicola Sturgeon said the relaxation of restrictions will not be extended to cover New Year's Eve.
She said: ""I know New Year is special for people, perhaps slightly more so in Scotland than in other parts of the UK, but the virus is still there.""
The planned firework displays in Edinburgh and London have been cancelled.
The virus will not call a truce because it is Christmas, and will be as contagious as at any other time, UK Prime Minister Boris Johnson has said.
Even if it is within the rules, meeting friends and family over Christmas will be a ""personal judgement"", the government says.
People should consider the risks to themselves and others, particularly those who are vulnerable.
Scotland's First Minister Nicola Sturgeon urged people to use the Christmas rules responsibly and ""only if you think it is necessary"".
Children aged under 18 whose parents live apart are allowed to be part of two separate Christmas bubbles. This means they can see both parents without being counted as part of another household.
University students may return to their parents in the early December ""travel window"" and be counted as part of their household straight away.
But if parents have three or more grown-up children who are not at university, then they cannot all form a Christmas bubble with their parents.
Individual households can split for Christmas. So, if three people are sharing a home, they can all go and form separate Christmas bubbles with their families and come back to form a single household again afterwards.
In England, care home residents have been told not to take part in Christmas bubbles, while visits out of the home should only be considered by residents who are of ""working age"". This is because of the increased risk of the resident catching coronavirus, and spreading it to other vulnerable people when they return, the government says.
People are asked to consider whether visiting at the care home would ""provide meaningful contact in a safer way"", while residents who do leave will be tested and asked to isolate when they return."
"
The  GISS Temperature Record Divergence Problem
Guest post by Tilo Reber
In connection with James Hansen’s  explanation of why his GISS temperature record diverges from that of  HadCRUT, I decided to check on the legitimacy of what GISS was doing.  Dr. Hansen’s article is here at RealClimate. This is the specific chart of interest:

(click  on chart to expand)
In this chart, Dr. Hansen explains how the GISS  record is different from HadCRUT.   The essential difference is that  there are areas at the poles where GISS has filled in the values by  extrapolating from the nearest land based records.   Dr. Hansen created a  mask of all the areas that HadCRUT does not cover.  He then applies  this mask to the GISS record and deletes the readings for the areas that  are masked off.  The resulting chart is marked GISS/HadCRUT mask  above.   Dr. Hansen then goes on to provide a graph which shows that the  GISS divergence with HadCRUT no longer exists after the missing HadCRUT  boxes have been removed from the GISS record.
So far so good.  I think that it is safe to say  that the divergence of the GISS record is due to the interpolation and  extrapolations at the poles.  Now we need to ask the next question –  are the GISS interpolations/extrapolations legitimate.  GISS has 2005 as  the hottest year for his surface temperature record.  HadCRUT and the  satellite records (UAH, RSS) have 1998 as the hottest year.  So Dr.  Hansen compares 1998 to 2005 in his chart, allowing the reader to see  why the difference exists.  Of course Dr. Hansen considers that his  method produces the more accurate result.
One of the first things that pops out about the  charts is how different the top row of polar cells is between the two  records.  For example, if one looks at the top row of the HadCRUT 2005  chart, one sees a group of cells directly above Svalbard that are shown  as having a cool anomaly in that record.  Then, when one looks up at the  same cells in the GISS record, one sees that GISS has the same cells  colored to the maximum hot anomaly.  In fact, the cells that HadCRUT has  in the top row (polar area) of that record look very different from the  top row of the GISS record.  The fact that these cells are so different  and that they are accounted for in both records makes me wonder what is  going on.  Looking at the GISS site, they say  this.
“Areas covered occasionally by sea ice are masked  using a time-independent mask.”
So if there is sea ice coverage for any part of the  year, GISS will not use SST values to cover those cells for the  entire year.  Those cells must be covered by extrapolations from land  for that year.  This means that when the area is cover with ice or with  water or with part ice part water, it will have it’s anomaly  extrapolated from land, regardless.  HadCRUT, on the other hand, does  not extrapolate their coverage.  But they will use SST values for a cell  when SST values are available for part of the year.  If the area is  covered with ice for the entire year, HadCRUT will not assign it a  value.  Therefore we get polar areas that are covered by extrapolation  by GISS and not covered at all by HadCRUT.
When we look at the HadSST2  record, we see that the cool cells that show up above Svalbard in  2005 are consistent with the numbers in that record.   And these then go   into creating the sea surface portion of the HadCRUT3 temperature  record.  So, obviously, how cells are filled with data can have a  profound effect on the anomaly value that those cells have.  This leads  one to wonder if extrapolations at the pole are legitimate.  I decided  to look at some of the northern Russian stations, at the GISS site, that  show up as being so hot in the 2005 version of the GISS chart when  compared with the 1998 version of the chart.  I found that those big  changes are in fact represented in the individual records – especially  for the coastal stations.  Here are three of them.
Kanin  Nos:   68.7  N, 43.3 E.
1998 Annual Mean –   -3.39
2005 Annual Mean –   0.60             1998 – 2005 delta    3.99 C
Ostrov Vize:   79.5  N,  77.0 E.
1998 Annual Mean – -14.99
2005 Annual Mean – -10.79            1998 – 2005 delta   4.2 C
Gmo  Im.E.K F:  77.7  N,  104.3 E.
1998 Annual Mean –  -15.96
2005 Annual Mean –  -12.67           1998 – 2005 delta   3.29 C
For comparison, let’s look across the Arctic ocean and see what was  happening in Canada and Alaska at the same time.
Eureka, N.W.T.:   80.0  N, 85.9 W.
1998 Annual Mean –  -17.38
2005 Annual Mean –  -17.34           1998 – 2005 delta   0.04 C
Barrow,  Ak.:     71.3  N, 156.8 W.
1998 Annual Mean –  -8.80
2005 Annual Mean –  -10.44           1998 – 2005 delta   -1.64 C
So it seems that the North American side of the Arctic changed  little, or even got cooler between 98 and 05, the Russian side warmed  considerably.  Why is that?  I think that this ice  cover map gives us the answer.  As is immediately apparent, the  coastal ice cleared out far earlier in 2005 in northern Russia than it  did in 1998.  This is even though the rest of the globe was slightly  warmer in 1998 than in 2005.  When dealing with coastal stations,  removing the ice and exposing the water is like taking the hatch off a  heating source for the coastal thermometers.  For stations that are in  areas where the temperature is well below zero, exposing the immediate  area of that thermometer to a surface that is above zero, changes  everything.  Looking at Ostrov Vize, we see that it is a small island,  and therefore even more subject to changes in coastal sea ice.  And when  we compare 1998 months on this island with 2005 months we can see that  there are differences in some of the monthly means that are larger than  10C.  Even a partial ice cover as opposed to a complete ice cover will  supply the stations with more heat.
So I think that we can safely say that the huge change in  the anomalies of Russian coastal stations is mostly due to coastal sea  ice changes.  In fact, if we look at stations further inland in Russia,  the coastal effect begins to decline.  With this in mind, we need to ask  if the GISS extrapolations of land based stations, particularly coastal  stations,  to the poles is appropriate.
The answer would seem to be that it is not, and the Svalbard case  makes this perfectly clear.  There we had a case where the SST anomaly  was actually cool, and yet the land based extrapolation actually turned  those sea based cells more than 3C hotter.  Reaching across the Arctic  Ocean with temperatures that are the result of a coastal sea ice effect  cannot give valid answers for what the temperature anomalies away from  those coastal stations should be.  In fact, taking the variation that is  represented by those coastal stations and extrapolating into the  interior of Russia is also not appropriate, because the interior areas  did not undergo the magnitude of temperature change of the coastal  stations.
Looking at the SST temperature  anomalies that NOAA uses for 1998 and  2005 it again looks like nothing  exceptional was happening in the Arctic (Note, the chart will not retain the months that I selected;  so use your own sample months and they will plot).  It seems, from  this analysis, that GISS polar extrapolations and interpolations are  likely to simulate large variations away from the Arctic coasts that are  really only present as changes at the Arctic coasts.  And the GISS  divergence from HadCRUT, as well as from UAH and RSS are likely to be  errors instead of enhancements.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8f0a7d59',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Out of the smoke of November 2002, one issue‐​conspicuous by its absence in the recent campaign‐​is about to emerge. That issue is the global environment. And it may make Al Gore, who’s scheduled to announce his presidential plans soon, the surprise rising star for 2004. For what it’s worth, he’s already been booked for Barbara Walters, Hardball, and Saturday Night Live in the last few weeks.



Sure, other issues are more compelling: Social Security, Iraq, and taxes, for example, but anyone that tried to beat the administration with those issues wound up beating themselves. It’s hard to get passionate about committing political suicide. In every race where Social Security was an issue, candidates who favored private accounts won. Almost everyone‐​win or lose‐​was with the president (at least publicly) on Iraq, and raising taxes just isn’t vote‐​productive. 



What’s left? The Washington political process, in its irrepressible attempt to produce productive controversy (i.e., votes), is behaving like Tom Wolfe’s doomed test pilots in The Right Stuff. “I’ve tried A. I’ve tried B,” and the darned thing won’t stop rolling.



Well, try “E.” About the only fights that may produce political gain are going to be over energy and environment, which means fights over drilling in the Arctic National Wildlife Refuge, fat subsidies tilting at windmills and solar energy, and “directions” to industry to produce tons of power from the same subsidized sources. If you’re wondering who pays for this, look in the mirror.



All of the above are in the current energy bill, a bipartisan compromise between the House and the then‐​Democratic Senate. Along with most of the other unfinished business from the last Congress, there’s no way it’s going to remain intact. How much it will be changed is a matter of conjecture. But one provision, ultimately requiring just about every business in the country to report its annual emissions of carbon dioxide (read: fuel consumption), is surely outta’ there. Everyone, on both sides of the aisle, knows this is the first step towards a cap on energy use, which must be defined before it can be cut any specific amount. 



Carbon dioxide, of course, is the principal cause of dreaded global warming, so any attempt to dilute current legislation will raise the bloody green shirt. And where global warming goes, Gore is soon to follow. 



For nearly a year, Dick Morris has been pleading with the Democrats to pick up this issue. In fact, in his recent book, “Power Plays,” he argues that Gore would have won in 2000 if he had followed his own instincts on fighting climate change, which is the real reason why he wanted to be president. Democrats see this coming, too. Last month, the Democratic Leadership Council issued a broadside entitled “Turn Up the Heat on Climate Change,” spoiling for a fight on global warming.



Don’t expect a clear, reflexive Republican voting pattern. The DLC is touting draft legislation written in part by Lincoln Chaffee (R-RI) who is at odds with the White House on global warming. Expect him to forge an alliance with other down‐​east Republicans, like Maine GOP‐​ers Susan Collins and Olympia Snowe, and John McCain (R‐​Ariz.), another global warming hawk, who hasn’t yet tired in his campaign for more, more, and more face‐​time against the President.



It’s not that Gore is a proven winner. But on the environment he is the proven champion. And if, maybe, he gains traction by railing against the administration along with McCain, et. al., he becomes a serious contender for the nomination. 



On the other hand, Gore may simply be too radioactive, still burning from the loss in 2000 and having actively campaigned in many of the debacles of 2002. In that case, John Kerry (D‐​Mass.) is sure to emerge as the global warming maven. He’s actually more radical (if less versed) on the issue than Gore, and there’s a school of thought in Democratic circles these days that says they lost the Senate because they were too much like Republicans. Kerry is no Republican.



All of that makes climate change look like a big issue in the next election cycle, with the flashpoint being current energy legislation. Will it be big enough to determine who runs against George Bush, and will that person stand much of a chance of success? Right now it doesn’t look good, but “A” and “B” have been tried, and maybe it’s time for “E.”
"
"During Chinese premier Li Keqiang’s last visit to the UK, China signed a series of deals on energy and low carbon technology, and a declaration of cooperation on climate change. A few weeks later, similar deals were signed with the US. The question is whether this demonstrates a genuine commitment from China to environmental protection, or whether it’s just rhetoric in the run up to important climate change conferences in New York and Paris. There is no doubt that the Chinese leadership has instigated a number of measures to address the problems of environmental pollution at home. The world’s largest polluter has done much more than any other government in the developing world. In recent years China has standardised environmental laws and regulations, encouraged local governments to take on responsibility for environmental protection, and introduced the environmental information disclosure, urging enterprises to meet certain requirements.  Environmental NGOs, the mass media and online communication are also playing an increasing role in China’s environmental policy-making. The country is now the largest investor in renewable technology and ambitious targets have already been set to increase the portion of renewable energy generated in China and reduce its dependence on polluting coal. China sees international cooperation on climate change as an opportunity. Its leadership wishes to be regarded as responsible global stakeholders by collaborating with major powers on big problems, without touching on sensitive issues concerning human rights, cyber-espionage or tensions in the East and South China Seas.  There is also an opportunity for China to open up foreign markets for its low-cost renewable energy products such as solar panels, ensuring itself a slice of the fast-growing renewable energy industry market. Nowhere is this most apparent than in those countries where Chinese state-owned companies and banks already provide infrastructure, investment and finance, in Africa and Latin America. As their economies grow these countries will face similar environmental problems, and so they watch China’s transformation carefully. Not only to see how a leader of the developing world tackles climate change and environmental issues, but also as a potential future supplier of environmentally-friendly technology.  Despite China’s aspirations and the positive ring to the speeches and deals signed with western nations, the key fact remains that China is not willing to sign up to any major international commitments on climate change. This has been demonstrated before: just before the Copenhagan COP15  in 2009, China announced its intention to reduce carbon dioxide emissions per unit of GDP by 40-45% (based on 2005 levels) by 2020. But during the conference it refused to agree to any specific targets.  China not only advocated on behalf of developing countries but also accused major powers of trying to divide the developing world. The Copenhagen summit took place just a few weeks after President Obama’s first visit to China, when the two countries agreed on joint cooperation on climate change and to establish the Sino-American Clean Energy Research Centre. How much China commits itself to cutting emissions will depend on how far  western powers will go in their own national climate policies. With the current divisions among EU member states on directions for energy strategy and a declining priority on the issue following the recession, the Chinese leadership can feel at ease. Subsequent UN climate talks in Bonn in June have also yielded only disappointing results. Other nations’ lack of commitment to emissions reduction gives China more room to manoeuvre. Next year’s COP21 summit in Paris to replace the Kyoto Protocol might be another occasion for Chinese leaders to cut western counterparts down to size.  China’s leaders’ rhetoric is clear: the developed world should take the lead in addressing climate change, while China – seen when it needs to be as a developing country despite its place as the world’s largest economy and second largest polluter – prioritises national economic development.  Seen in this light, cooperation is best understood if placed in the context of China’s interests. Bilateral agreements under the umbrella of renewable energy can help China advance technologically, provide opportunities for state-owned enterprises to access markets abroad, and enhance China’s energy security. These are likely to succeed; any collaboration beyond this scope – agreements for the good of the international community and the world at the expense of Chinese interests – is doomed to failure. "
"The rise of hydraulic fracturing, or fracking, has ushered in an era of intense drilling that has been called the great shale gas rush. Fracking allows oil and natural gas to be extracted from horizontal wells, thousands of metres below the Earth’s surface. We tried to piece together the environmental impact of the great shale gas rush, and quickly discovered how little is actually known about the effects this booming industry is having on plants and wildlife. To help shed light on this area where there is little research, we convened a team of eight scientists with diverse expertise in plants, birds, amphibians, mammals, wildlife disease, hydrology, and public policy. Our study, published in the journal Frontiers in Ecology and the Environment, examined 12 ways in which shale development can harm ecosystems. Using an objective ranking system, we identified the highest-priority threats for future research.  Much of the public debate around shale development has focused on the technique of fracking itself. The above-ground footprint of each well is relatively small at between 1.5-3 hectares of land cleared per well, but many are drilled in close proximity and each is connected by a labyrinthine network of roads and pipelines which must be built and in place before the fracking can start. This can add up to many thousands of hectares of disturbed land, contributing to habitat loss and fragmentation, generating light and noise pollution, and affecting air quality. The huge amounts of water required for fracking, on average 20m litres over the lifespan of a single well, can take a heavy toll on groundwater levels and water flow in streams, adding to siltation and affecting wildlife dependent on those habitats. The whole process of drilling for oil and gas also provides opportunities for unpredictable and potentially disastrous events – chemical spills or pipeline ruptures – which pose high risks to surrounding ecosystems. Understanding these risks and effects is essential because many shale basins occur in regions of exceptional biological diversity. For example, the Devonian and Marcellus Shale in the Appalachian Basin overlap with the highest areas of amphibian and freshwater fish diversity in the United States. Each of the threats we identified were assessed on their extent, duration, the difficulty of mitigating their effects, and the amount of relevant scientific information available. With limited resources, our top research priorities should be threats that are under-studied, widespread and whose effects take considerable time or effort to reverse. We found the highest risk and therefore highest research priorities were related to chemical contamination from spills, equipment failure, the movement of fluid underground, or storage leaks. Although our focus was on plants and wildlife, these problems could also affect human health. More surprising was the lack of accessible and reliable information on the chemicals used in fracking (as part of the fracking fluid blasted underground to fracture the rock), spills, and the disposal of this fluid as wastewater. We reviewed the chemical disclosure statements provided by fracking companies for 150 wells in three of the top gas-producing states and found that, on average, two out of three wells were fracked with at least one undisclosed chemical, while some wells were fracked with 20 or more. It’s impossible to accurately predict the effects of environmental pollution if we do not know the identity and concentration of chemicals being released into soils and waterways. Government and industry have made small steps towards openness, for example by creating the FracFocus registry and by maintaining regional databases. (for example the Pennsylvania Department of Environmental Protection). But these steps are not enough. Only five of the 24 US states involved in fracking – Pennsylvania, Colorado, New Mexico, Wyoming and Texas – maintain any public records of spills, and a lack of complete and consistent reporting is a major obstacle to understanding the environmental impact of shale development. For instance, less than half the reports in the Pennsylvania database included information on where spills occurred and fewer than one in 15 reports noted the date and time when the spill occurred. In fact, in Pennsylvania alone (an area about half the size of the UK), the Department of Environmental Protection cited companies 22 times in 2013 for discharging industrial waste into waterways.  The collective impact of shale development as illustrated above is not easily seen from the ground. Yet each modest-sized drilling operation contributes to at least 12 distinct environmental threats. The combined impact of these threats is itself a major research priority, but is exceptionally difficult to isolate and study.  With shale production projected to increase exponentially during the next 30 years, it’s vital that scientists, the industry and government regulators co-operate to minimise damage to the natural world. Scientists need access to reliable information about spills, violations, fluid disposal and chemicals used in fracking. This information must be seen as essential to prevent the shale industry from posing unacceptable risks to the living world, our families and communities. "
"

The wrangling over the corporate tax bill in Congress continues, but prospects for passage before the election are becoming increasingly dim.



That may not be a bad thing because the bill is the worst show of special interest tax lobbying in years. Hundreds of narrow provisions litter the bill, illustrating congressional sausage‐​making at its most complex. The list of provisions in the Senate version of the bill is 7 times longer that the list of provisions in last year’s income tax cut bill. 



This sausage started out as a lean reform bill, which responded to a World Trade Organization ruling a special tax break for U.S. exporters was illegal. The House and Senate both agreed the break should be repealed to comply with WTO rules. Then each chamber added some meaty reforms to simplify the tax code and help U.S. corporations compete abroad. 



Currently, the United States applies uniquely complex rules to the foreign activities of its corporations. These rules are so complex that, for example, four‐​fifths of Dow Chemical’s 7,800-page federal tax return relate to its foreign investments. 



Proposed reforms to such items as the “interest expense allocation rules” and “foreign tax credit baskets” would help U.S. firms compete with firms based in countries with less burdensome systems. 



But that’s where the lean ends and the pork begins. The House bill includes tax breaks for alcohol fuels, electric vehicles and $10 billion for tobacco farmers. The Senate bill includes dozens of tax incentives for coal, oil and gas, fuel cells, biodiesel and other energy activities. 



While most such special interest provisions distort the economy, some may be justified if they make the tax code simpler and more neutral. For example, the House bill repeals the 10 percent excise tax on fishing tackle boxes. Tackle box manufacturers apparently suffer because fishermen buy similar utility and sewing boxes that don’t face the tax. In this case, repeal makes sense to neutralize an existing distortion. 



However, the biggest item in the House and Senate bills is a tax cut for manufacturers, which would add a large distortion to the tax code. The House bill would reduce the tax rate for manufacturers from 35 percent to 32 percent while the Senate bill would create a special deduction. That would increase tax code complexity and put manufacturers into a separate lobbying camp less interested in overall tax reforms. 



Many other industries add great value to the U.S. economy such as financial services. Shouldn’t tax policy encourage growth in those industries as well? 



There is a better way. If the House and Senate can’t reconcile their different bills, Congress should start fresh with a simple across‐​the‐​board corporate tax rate cut. That would provide a direct competitive response to the recent decline in tax rates around the world. 



The average corporate tax rate for the 30 major industrial countries has fallen from 38 percent in 1996 to just 30 percent today. By contrast, U.S. corporations face a rate of about 40 percent, including the 35 percent federal rate plus an average state rate of 5 percent. 



The downward trend in global corporate tax rates is expected to continue, further increasing competitive pressures on U.S. companies. This will cause investment to gravitate toward countries with more attractive tax climates, reducing U.S. productivity and wages. Another problem with a high tax rate is it creates a big incentive for firms to adopt Enron‐​style tax shelters to move paper profits abroad. 



Most analysts would agree a corporate rate cut makes economic sense, but point to the huge budget deficit as a barrier to reform. The solution is to combine a corporate tax cut with an equal cut of federal spending on corporate subsidies, which total about $90 billion yearly. Such a reform package would reduce special handouts that distort the economy while spurring growth across all industries. 



Because such a clean reform package would have to overcome special interest lobbying, presidential leadership would be needed. The current corporate tax bill is a mess partly because the Bush White House has hinted from the sidelines it would sign any pork barrel bill that passes Congress. White House invisibility on the issue has been irresponsible, but perhaps understandable, given election year politics. 



The good news is Democratic presidential hopeful Sen. John Kerry has introduced his own corporate tax plan, which gives the White House cover to begin engaging the issue. Mr. Kerry’s plan includes wrongheaded rule changes for foreign investment, but he does propose a small corporate tax rate cut. Mr. Kerry also says he favors cutting corporate subsidies. 



President Bush should one‐​up Mr. Kerry and draft a reform bill that cuts the tax rate enough to put U.S. companies on an equal footing with foreign competitors. 



While the public may think both Republicans and Democrats shower corporations with big tax breaks, the truth is corporations have not received a substantial tax cut in decades. The landmark 1986 tax act cut the corporate tax rate, but raised corporate taxes overall. The depreciation tax cut enacted in 2002 expires at the end of this year. Thus there is a pent‐​up demand for corporate tax reforms, especially since other countries have cut tax rates so much. 



That pent‐​up demand is why the corporate tax bill has generated such a lobbying frenzy this year. The Bush administration can play a constructive role by channeling that frenzy into tax changes that are good for the whole economy, not just businesses on the inside track in the halls of Congress.
"
"

 ** _Congress should_**



• recognize that the U.S. economy has experienced a downward shift in the long‐​term rate of growth, with the pace of growth one‐​half to two‐​thirds that of the 20th century;  
  
• understand the huge impact of the growth slowdown: if current growth rates persist, a person born in 2000 will spend her retirement years in an economy half as big as it would have been if 20th‐​century growth rates had been maintained;  
  
• recognize that reversing the growth slowdown should be a top priority across the political spectrum, as disagreements about how to divide the economic pie are subordinate to the common interest in a larger pie;  
  
• understand that policy changes can have a significant positive effect on the long‐​term rate of growth; and  
  
• seek out pro‐​growth reforms that attract support across the political spectrum, so that the common interest in higher growth is not a casualty of partisan polarization.



The 21st century has witnessed a major downward shift in the trajectory of U.S. economic growth. From 1900 to 2000, real (i.e., inflation adjusted) gross domestic product (GDP) per capita rose at an average annual rate of 2.1 percent. The long-term growth path remained remarkably steady even in the face of massive macroeconomic fluctuations. For example, over the 20 years from 1929 to 1949 — a period that encompassed the twin convulsions of the Great Depression and World War II — the average growth rate clocked in at 2 percent per year, right on trend.



In contrast, from 2000 to 2015, annual growth in real GDP has averaged only 1 percent — half the rate of 20th-century growth. At the center of this story is the Great Recession of 2007–2009 and its aftermath. Between the fourth quarter of 2007 and the second quarter of 2009, U.S. GDP shrank by 4.2 percent — the sharpest decline since the Great Depression. Normally in U.S. economic history, severe recessions are followed by vigorous recoveries, but not this time. Instead, the economy experienced its weakest expansion since World War II. As of October 2013, 70 months after the recession began, real GDP had grown only 5.3 percent. In contrast, after the prior six recessions, real GDP growth averaged a robust 20 percent over the same period of time.



For most Americans, the economy has been performing even worse than these aggregate numbers suggest. With the rise in income inequality, the benefits of growth in terms of rising incomes are now skewed toward the upper reaches of the socioeconomic scale. As a result, most Americans have been experiencing not just a slowing rate of improvement, but stagnation and even decline. Real median household income (family income for Americans in the exact center of the income distribution) was 7 percent lower in 2014 than it was in 2000. Using different adjustments for inflation, it is possible to massage those figures to make them look slightly less bleak. It is impossible to massage them into looking good.



Unfortunately, the economy's sluggishness is likely to persist. At a Cato Institute conference in December 2014, two of the nation's leading experts on productivity growth presented long-term growth projections for the U.S. economy. Dale Jorgenson of Harvard University projected annual growth in aggregate real GDP of 1.75 percent, while John Fernald of the Federal Reserve Bank of San Francisco projected a slightly faster growth rate of 2.1 percent. After taking account of likely population growth, Jorgenson's projection puts annual growth in real GDP per capita at below 1 percent, while Fernald's comes in under 1.4 percent. In other words, in Fernald's more optimistic scenario, growth in the years to come will be more than a third off its 20th-century pace, while in Jorgenson's scenario, the long-term growth rate has been cut in half.



The reasons for Jorgenson and Fernald's pessimism have nothing to do with any recent events — neither lingering effects from the severe recession nor problems with policies enacted in its wake. Rather, both recognize the impact of deep-seated factors that once propelled growth but that now have shifted in an unfavorable direction.



To understand what's going on, let's break down measured economic growth into the constituent elements tracked by conventional growth accounting: (1) growth in labor participation, or annual hours worked per capita; (2) growth in labor quality, or the skill level of the workforce; (3) growth in capital deepening, or the amount of physical capital invested per worker; and (4) growth in so-called total factor productivity, or output per unit of quality-adjusted labor and capital.



Over the course of the 20th century, these various components fluctuated in their contributions to overall growth. The fluctuations, however, tended to offset each other, so that the long-term trend line of growth overall remained stable. In the 21st century, however, this pattern of offsetting fluctuations has come to a halt as all growth components have fallen off simultaneously.



One way to get faster growth is for more people to work or for people to work longer hours. Between the mid-1960s and 2000, average annual hours worked per capita surged from under 800 to above 950, powered by rising labor force participation among women and the influx of baby boomers into the work force. Since 2000, however, the labor force participation rate for both men and women has been in steep decline: from its overall peak of 67.3 percent in early 2000, it has dropped all the way to 62.7 percent as of June 2016, the lowest rate since 1978 (Figure 6.1). With labor hours shrinking, output per worker hour (otherwise known as labor productivity) has to rise just to keep the economy from shrinking. Accordingly, hours worked per capita has gone from providing a strong tailwind for growth to now resisting growth with a stiff headwind.





SOURCE: Data from 1900–2005 come from Valerie Ramey, http://www.econ.ucsd.edu/~vramey/research.html. Data from 2006–2010 are a recreation of Ramey's methods using Current Population Survey civilian hours plus Ramey's military figures divided by the U.S. Census's annual population figures.



While the quantity of labor supplied is an important factor in determining output, so is the quality of labor. Since workers with higher skills and more experience produce more output in a given hour than do their less skilled, more junior colleagues, boosting the skill level of the workforce is an important way to create economic growth. Over the course of the 20th century, huge investments in mass schooling — first at the secondary level, then at the postsecondary level — led to a much more highly skilled workforce. Harvard economists Claudia Goldin and Lawrence Katz estimate that rising educational attainment accounted for about 15 percent of total growth over the period 1915–2005. But the rate of increase in years of schooling per worker has slowed dramatically in recent decades: the pace of improvement between 1980 and 2005 was less than half that during the period between 1960 and 1980 (Figure 6.2). And looking ahead, the educational level of the workforce is expected to plateau. Another important source of growth has therefore petered out.





SOURCE: 1900–1939: ""120 Years of American Education: A Statistical Portrait"" found at http://0-nces.ed.gov.opac.acc.msmc.edu/pubs93/93442.pdf. 1940–1990: Current Population Survey of the BLS Table A-1. 2000–2010: UNDP, http://hdrstats.undp.org/en/indicators/103006.html.



An additional source of growth is investment: workers with more and better tools are able to produce more. Unfortunately, net national investment (investment net of depreciation charges) as a percentage of net national product has been falling for decades, dragged down by the more widely reported drop in the national savings rate (Figure 6.3). There is therefore no current basis for expecting a surge in investment to counteract the unfavorable trends regarding hours worked and educational attainment.





SOURCE: Jagadeesh Gokhale, Cato Institute.



In the case of labor hours, worker skills, and investment, growth is created by adding more inputs. If you increase inputs with more hours worked, more training, and more equipment, then you will produce more output. The final source of growth, innovation, involves figuring out how to get more output from a given set of inputs — either through inventing new products or by developing more efficient production processes. Economists' best measure of innovation is known as total factor productivity (TFP) growth: the increase in output from a given unit of labor and capital. From 1996 to 2004, TFP growth surged after a long slump that began in the 1970s; since 2004, however, TFP growth has returned to the low rates of decades past (Figure 6.4). Admittedly, shifts in the rate of TFP growth are unpredictable, so it is possible that another round of rapid growth is just around the corner. But at present, no signs of such a turnaround are visible.





SOURCE: 1871–1950: Robert Gordon's 1999 ""U. S. Economic Growth Since 1870: One Big Wave."" 1950–2011: The Bureau of Labor Statistics, ""MFP Tables: Historical multifactor productivity measures (SIC 1948–87 linked to NAICS 1987–2011),"" Nonfarm Tables, June 2013, http://www.bls.gov/mfp/.



In the 21st century, the U.S. economy has thus experienced simultaneous weakening in all four components of economic growth. This does not mean that slow growth is inevitable from here on out: the current trends are not set in stone. Nevertheless, it is difficult to avoid the conclusion that the conditions for growth are less favorable than they used to be.



The long-term implications of a 50 percent drop in the growth rate are huge. With real GDP per capita rising 2 percent a year, output per capita doubles in around 35 years. With the growth rate cut to 1 percent, doubling takes 70 years. Accordingly, if 21st-century growth rates persist, a person born in 2000 will spend her old age in an economy only half as rich as it would have been if 20th-century growth rates could have been extended.



Conservatives and libertarians should require little convincing that more economic output is generally something to be desired, and that therefore the prospect of a prolonged growth slump is a matter of serious concern. Progressives may be more skeptical. In particular, they might object that, because of income inequality, all the extra output created by higher growth wouldn't translate into commensurate income gains for most Americans.



That's an understandable concern, but disappointment with the pace of median income growth will hardly be assuaged by a slowdown in the economy's overall expansion. In the era of rising inequality and falling median wage growth that began in the 1970s, there has been only one period of strong growth in real earnings: the late 1990s, when GDP and productivity growth were surging as well. Thus, even in an age of income inequality, faster growth redounds to the benefit of average workers. Indeed, in the current circumstances, it appears that only strong growth can stave off stagnation or disappointingly sluggish growth in workers' pay.



Many progressives might argue that, even with faster growth, the gains from economic growth are no longer shared widely enough. More redistribution, they contend, is needed to correct the imbalance. Libertarians and conservatives can be counted on to dispute the point, but that is an argument for another day. For present purposes, it suffices to point out that slow growth will make it all but impossible to fund any more generous provision for the less well-off.



Because of the aging U.S. population and rising health care spending, entitlement spending on the elderly figures to put the squeeze on everything else the federal government does. According to the Congressional Budget Office, spending on Social Security and the major federal health care programs is projected to balloon to 14 percent of GDP by 2039, double the 7 percent average over the past 40 years. Meanwhile, spending on everything besides interest payments would fall to 7 percent of GDP, well below the 11 percent 40-year average — indeed, a smaller share of GDP than at any time since the late 1930s. And even with this hit to the relative size of nonentitlement spending, by 2039, federal debt as a share of GDP is projected to hit the all-time historical peak of 106 percent (set back in 1946 at the end of World War II). It will continue upward from there. To put it mildly, this is not a fiscal environment that augurs well for big new federal social programs.



Alas, we cannot simply grow our way out of this predicament. Yes, higher growth directly inflates the denominator of the debt-to-GDP ratio; but it also leads to increased spending under entitlement programs and thus works indirectly to boost the numerator as well. Nevertheless, faster growth does mean a considerably larger economy over the longer term and, consequently, more resources available for funding income transfers than would otherwise be the case.



Consequently, progressives and libertarians should be united on the desirability of higher growth. They may have different ideas about what to do with the extra money, but both sides have a stake in getting the chance to fight it out.



In addition to economic reasons for favoring higher growth regardless of ideology, there are also important political reasons. Indeed, in our present situation, the political stakes have assumed an urgency that swamps any calculations of mere dollars and cents. The health of American democracy and the basic character of American society are now on the line. As Harvard economist Benjamin Friedman documented in _The Moral Consequences of Economic Growth_ , a prosperous, growing economy promotes the democratic virtues of tolerance and openness. When incomes and living standards are rising generally, the welfare of other groups is less likely to be perceived as a threat to one's own. But when the economy stagnates, gains for some necessarily mean losses for others. In this zero-sum environment, the ugly, defensive reactions of bigotry, xenophobia, and belligerent nationalism gain traction.



With the recent rise of authoritarian populism here in the United States, as well as in countries across Europe, the broader, political implications of the growth rate are no longer a matter of merely theoretical concern. Fundamental American ideals are under active assault, and all who still hold to those ideals, regardless of their position on the political spectrum, need to recognize the role that the growth slowdown is playing in strengthening the other side. Reviving economic growth is perhaps the most potent means at our disposal to counter and defeat this illiberal challenge to the country's founding principles.



Can anything be done to stir the economy out of its current doldrums? The heartening answer is yes, absolutely. Current trends in labor participation, labor quality, investment, and innovation point to a permanent reduction in the U.S. economy's long-term growth path, but those trends do not exist in a vacuum. They are situated in the larger context of the nation's laws and economic policies, which combine to shape the incentives of individuals and firms along countless different margins. If you change those laws and policies, then you can change those incentives; change those incentives, and you can change the economic trends.



The fact is — and it's hard to imagine who would disagree — that American public policy is far from optimal when it comes to facilitating economic growth. Look at the factors that shape each component of growth and you will find laws and policies that push in the wrong direction: laws and policies that discourage participation in the labor force, frustrate accumulation of human capital, deter productive investment, and inhibit innovation or block its diffusion throughout the economy. In the circumstances, this is good news: it means that there is wide room for improving public policy, and consequently wide room for improving economic performance.



To explore the wide variety of possible pro-growth reforms, the Cato Institute hosted a special online forum during late 2014, in which 51 of the nation's top economists and policy experts were asked to identify one or two policy changes that could trigger faster economic growth, whether temporarily through a one-time change in the level of output or indefinitely through accelerating the growth rate. The proposed reforms covered a long list of policy domains: tax policy; budget policy; education and training policy; health care financing policy; financial regulation; monetary policy; health, safety, and environmental regulation; regulations on starting a business; trade policy; immigration policy; intellectual property law; land use regulation; and even foreign policy. In addition, some of the contributors have advocated what might be called ""meta policy"" changes — that is, reforms to the policymaking process rather than specific substantive changes to rules or programs.



While everyone might agree on the need to change public policy, finding agreement on what particular changes to make is considerably trickier. After all, American politics today is characterized by deep ideological divisions and intense partisan polarization over government's proper role in the economy. There are many fronts in the political conflicts of recent years: the trajectory and composition of federal spending, the level and structure of taxation, health care policy, regulation of the financial sector, immigration, climate change, and environmental regulation more generally. All involve policy issues with important implications for the level of output or the permanent rate of growth. On these questions, and many others besides, the respective sides are miles apart when it comes to the proper direction of policy change. Yes, there is a shared interest in continued healthy economic growth that transcends the left-right divide. But agreement on ends need not translate into agreement on means, and in the present case, there is disagreement aplenty.



Under current political conditions, the most promising path forward is to identify policy ideas that are not already the subject of high-profile, politically polarized debate. America's growth slowdown is a new problem, and policy responses that address that problem are more likely to gain traction if they are not recycled ideas originally put forward to address other problems. And if a policy idea is already clearly associated with either the left or the right, in today's highly contentious environment, it is all but guaranteed that the other side will fight tooth and nail against it. That makes progress of any kind difficult in the absence of large congressional majorities and unified partisan control of the White House and Congress.



The good news is that, notwithstanding the extent of polarization, it is still possible to construct an ambitious and highly promising agenda of pro-growth reforms that steers largely clear of the red-versus-blue divide. Chapter 7 explains how.



Friedman, Benjamin. _The Moral Consequences of Economic Growth_. New York: Vintage Books, 2006.



Lindsey, Brink. ""Why Growth Is Getting Harder."" Cato Institute Policy Analysis no. 737, October 18, 2013.



\---. ""Low-Hanging Fruit Guarded by Dragons: Reforming Regressive Regulation to Boost U.S. Economic Growth."" Cato Institute White Paper, June 22, 2015.



Lindsey, Brink, ed. _Reviving Economic Growth: Policy Proposals from 51 Leading Experts_. Washington: Cato Institute, 2015.



\---. _Understanding the Growth Slowdown_. Washington: Cato Institute, 2015.
"
"
Mayon – Shades of Pinatubo
2001 Image from NASA via the Space Shuttle: click for very hi res image
Here’s a recent AP report and bulletin from local authorities. Meanwhile, fools rush in as 2400 tourists a day flock to the area.

From the Philippine institute of Volcanologyand Seismology
30 December 2009 7:00 AM
For the past 24 hours, one ash explosion occurred at Mayon Volcano (13.2576 N, 123.6856 E). The explosion produced a dirty white ash column that rose to about 100 meters above the summit and drifted to the northwest. Lava continued to flow down along the Bonga-Buyuan, Miisi and Lidong gullies. The lava front has now reached about 5.9 kilometers from the summit along the Bonga-Buyuan gully.
Mayon Volcano’s seismic network recorded 16 volcanic earthquakes. A total of 150 rock fall events related to the detachment of lava fragments at the volcano’s upper slopes was also detected by the seismic network. Yesterday’s measurement of Sulfur Dioxide (SO2) emission rate yielded an average value of 4,397 tonnes per day (t/d). The volcano edifice remains inflated as indicated by the electronic tilt meter installed at the northeast sector of the volcano.
The status of Mayon Volcano is maintained at Alert Level 4. PHIVOLCS-DOST reiterates that the Extended Danger Zone (EDZ) from the summit of 8-km on the southern sector of the volcano and 7-km on the northern sector should be free from human activity.  Areas just outside of this EDZ should prepare for evacuation in the event hazardous eruptions intensify.  Active river channels and those perennially identified as lahar prone in the southern sector should also be avoided especially during bad weather conditions or when there is heavy and prolonged rainfall. In addition, Civil Aviation Authorities must advise pilots to avoid flying close to the volcano’s summit as ejected ash and volcanic fragments from sudden explosions may pose hazards to aircrafts. PHIVOLCS–DOST is closely monitoring Mayon Volcano’s activity and any new significant development will be immediately posted to all concerned.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e902da7c3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
I was working on another project related to the CRU emails and came across this email from Dr.Phil Jones. I was stunned, not only because he was dissing another dataset, but mostly because that dissing hit many of the points about problems with the NASA GISS products we’ve covered here on WUWT and at Climate Audit.
Here’s the email with my highlights added. Email addresses have been partially redacted.
click for larger image
The original email can be seen at this link:
http://www.eastangliaemails.com/emails.php?eid=1042&filename=1254850534.txt
Here’s the thing, we’ve seen the problems with CRU’s temperature series in the code already. If Dr. Jones is aware of those problems, and he thinks GISS is inferior, well then, wow, just how bad is GISS?
I thought this statement was quite telling:
Their non-use of a base period (GISS using something very odd and NCDC first differences) means they can use
very short series that we can’t (as they don’t have base periods) but with short series it is impossible to assess for homogeneity.
One thing about GISS that has bothered a lot of people – the base period they use for calculating temperature anomaly is for 1951-1980. See it listed here on the GISTEMP page. No other data sets use that period. Critics (including myself) have said that by using that period, it makes this graph’s trend look steeper than it would if the current 30 year period was used.
click for larger image
In the past couple of years we’ve seen two significant errors with NASA GISS that had to be corrected after they were discovered through the work done here at at WUWT and Climate audit. Public errors have not been found in CRU products during that time, because the data an code have been withheld.
To the credit of NASA GISS, they have been more transparent than CRU on data, stations used, and code.
Here are some of the relevant posts on WUWT where we address issues found with the NASA GISS temperature products:
How bad is the global temperature data?
And now, the most influential station in the GISS record is …
GISS for June – way out there
NASA GISS: adjustments galore, rewriting U.S. climate history
Absence makes the chart grow fonder
A comphrehensive comparison of GISS and UAH global Temperature data
Getting crabby – another missing NASA GISS station found, thanks to a TV show
More on NOAA’s FUBAR Honolulu “record highs” ASOS debacle, PLUS finding a long lost GISS station
Revisiting Detroit Lakes
Weather Station Data: raw or adjusted?
GISS Divergence with satellite temperatures since the start of 2003
Divergence Between GISS and UAH since 1980
GISS’s Gavin Schmidt credits WUWT community with spotting the error
GISS, NOAA, GHCN and the odd Russian temperature anomaly – “It’s all pipes!”
Corrected NASA GISTEMP data has been posted
Adjusting Pristine Data
A new view on GISS data, per Lucia
The Accidental Tourist (aka The GISS World Tour)
Rewriting History, Time and Time Again
Why Does NASA GISS Oppose Satellites?
Cedarville Sausage
How not to measure temperature, part 52: Another UFA sighted in Arizona
How not to measure temperature, part 51.
NASA’s Hansen Frees the Code !
Does Hansen’s Error “Matter”? – guest post by Steve McIntyre
1998 no longer the hottest year on record in USA


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e91068242',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Last November, my book on the distortion of the science of global warming hit the streets. If I had not finished it by then, I would have devoted a chapter to CNN’s handling of the subject in its global warming documentary that aired on March 27 and is set to re‐​air three times on April 2.



The thesis of my book is simple: All scientific issues, including global warming, compete with each other for a finite amount of taxpayer largesse. So, logically, in order to gain advantage in that competition, scientists tend to pitch dire and drastic scenarios whenever they can.



The projections of gloom and doom by eminent scholars merit news coverage. The politicians respond to the incessant drumbeat by holding hearings and writing legislation for funding or regulation. What sane scientist would testify that global warming may be no big deal? After all, it’s currently so big a deal that the new budget proposes spending $4 billion researching it.



In its documentary, CNN had an opportunity to present a dramatically different view of global warming. They asked me to talk about global warming science. Here is what they were told.



To study climate change, we really have only two tools at our disposal: the historical record and computer models for the future. Neither is very satisfying when applied singly. But together they provide a very clear picture.



Climate models are simply strings of computer code that attempt to simulate the earth’s varied weather patterns and then estimate how they change if we slightly alter the planet’s natural greenhouse effect. That’s an atmospheric property, contributed largely by water vapor and secondarily by carbon dioxide, that makes the surface and the lower atmosphere warmer than it would be in their absence. Add more of either and the temperature should rise a bit.



Although there are dozens of these models, a comparison of them yields this average behavior: Once warming is established from human addition of carbon dioxide to the atmosphere, it tends to place at a constant rate. The different models largely just produce different rates.



So, to determine what the future holds, all we have to establish is 1) that warming in recent decades is largely from human addition of carbon dioxide, and 2) that it is truly a constant rate. If those two are satisfied, then we know the rate of future warming.



It’s easy to establish that the warming that began around 1970 is indeed largely from human influence. Greenhouse‐​effect science predicts that cold, dry air should warm preferentially to warm, moist air. This has been the case, with the greatest nonpolar warming observed over cold, dry Siberia in the winter.



All that’s left then is to demonstrate that the global temperature change is indeed constant. In fact, the warming trend has been so steady that there has been virtually no departure from a straight‐​line trend.



So nature has now discriminated between all those models, and the warming trend works out to a mere 1.2°F for the next half‐​century. This is right at the low end of the range of projections made by the United Nations in 2001. But, unless all the money we threw into climate models has been wasted to the point that we can’t even tell whether it will be a constant or an increasing rate, we now know the answer to a very small range of error.



How small is that range? The variation in the constant warming trend has been so tiny over the last 35 years that one can say with confidence that the range should be between 1.0 and 1.4°F from 2001 to 2050. 



CNN had the chance to prove my book wrong and to show something new and different at the same time. But the cable network failed to do so. The program transcript is 6,497 words long. Two scientists, myself and MIT’s Richard Lindzen, different than the 19‐​odd other people promoting gloom and doom, were awarded 98 and 68 words, respectively. My segment was followed by the host, Miles O’Brien, saying, “Michaels’ position is in the minority.”



Hardly. It is the consensus of dozens of climate models, produced by an army of researchers, simply adjusted to the very constant rate of warming that has been observed.



Incidentally, the complete title of the book is _Meltdown: The Predictable Distortion of Global Warming by Scientists, Politicians, and the Media_.
"
"

he emergence of John Kerry as frontrunner for the Democratic nomination suggests that free trade might be off the table in 2004, at least as a national issue. It’s certain to come up, however, in a number of congressional, senatorial, and gubernatorial campaigns. And, of course, as long as Lou Dobbs is still kicking at CNN, we’ll continue to hear nightly nativist tirades against the loss of manufacturing jobs, the off‐​shoring of tech jobs, immigration, and general alarmism about the “outsourcing of America.”



The truth, of course, is a bit more complicated than the simplistic picture painted by protectionists. The United States is still far and away the world’s leading exporter of services. Direct corporate investment in India — generally the target of protectionist rants on tech jobs — actually declined from 2001 to 2003. As for manufacturing jobs, sure, it’s likely that free trade agreements played a part in the loss of jobs in the last five years, but so too did a host of other factors, including exchange rates, changing consumer preferences, upgrades in technology and equipment, the recession, and new federal regulations. Michigan’s Mackinac Center for Public Policy, to cite just one example, estimated in 2002 that a federal appeals court ruling favoring procedural matters over hard science in federal environmental regulatory policy could cost the state as much as $2.6 billion, or about 10,000 jobs.



Which brings us to state policy. Time and again, when we look at the states attracting and retaining jobs, and we compare them to the states losing jobs, we find that the states doing well are those with tax and regulatory schemes most friendly to business. It’s only when the cost of staying local becomes too burdensome that companies pick up and relocate elsewhere. Perhaps that’s not surprising. But just how strongly the data shakes out might be.



For example, according to the Economic Policy Institute, the five states losing the most jobs between 1993 and 2000 were, in order, California, New York, Michigan, Texas and Ohio. According to figures from the Bureau of Labor Statistics, New Jersey, Pennsylvania, Illinois and Massachusetts also rank near the bottom, particularly when you take jobs as a percentage of population. The left‐​leaning EPI blames these losses chiefly on NAFTA, and perhaps that’s partially the case. But aggressive tax and regulatory climates play a pretty big role, too.



Each year, _CFO_ magazine asks financial executives to assess the business‐​friendliness of tax policy in their respective states, which the magazine then compiles and ranks. Ranking in the bottom 10? California, New York, Michigan, Texas, Ohio, New Jersey, Pennsylvania, Illinois and Massachusetts — the very states that seem to be bleeding jobs. The most recent unemployment figures from the Labor Department put California, Texas, Ohio, Illinois, and Michigan all in the bottom 10 there, too, all with unemployment rates at 7.0 percent or higher.



The Small Business Survival Committee also puts out a report ranking the states on business‐​friendly public policy. In the SBSC report, Ohio ranks 39th, New York 45th and California 46th. Oregon, also with one of the country’s highest unemployment rates, ranks 41st.



A 2003 ranking by the Tax Foundation focusing mainly on tax policy and business tells the same story. It puts California 49th, Ohio 47th, and New York 44th.



Only Texas and Michigan score relatively well on the Tax Foundation and SBSC reports, suggesting that at least in these two states, free trade may have played a more significant role in job loss than poor public policy (and when you think about what Michigan manufactures, and where Texas is located, that makes some sense).



The Cato Institute’s Alan Reynolds wrote recently about San Jose, California, a city that lost about 120,000 jobs over two years. Reynolds points out that despite the debacle in San Jose, the communities of San Diego, Riverside, and Orange County actually added almost as many jobs over the same span of time.



San Jose was one of the first jurisdictions in the area to implement a so‐​called “living wage” ordinance, mandating that businesses contracting with the city pay their lowest‐​paid workers around $11 per hour, more than double the federal minimum wage. Of course, a living wage law in and of itself won’t wipe out 120,000 tech jobs, but it’s certainly indicative of the sort of “progressive” anti‐​corporate sentiment that might cause local businesses to pick up and spill out into friendlier communities.



Protectionists often bring up Ohio as the prototype of a hard‐​working, breadbasket state whose manufacturing sector has fallen victim to free trade. But Ohio is also a case study in how a state government hostile to business pushes jobs to more hospitable locales. You’ve read the numbers above. But additionally, in the last few years, Ohio legislators have begun to feel the hangover caused by big spending habits fomented back in the freewheeling 1990s. As of 2003, the state faced a $720 million deficit. Ohio governor Bob Taft has promised to shrink the deficit not with cuts in state spending, but with new taxes, tax hikes, and new fees, as well as rollbacks of promised tax breaks. Taft’s tax‐​happy policy earned the Republican condemnation from the Club for Growth’s Steve Moore, who called Taft one of the “worst governors in America.”



The Buckeye Institute, an Ohio free market think tank, reports that Ohio’s aggressive pro‐​labor policies cost the state jobs even during the relatively strong economic period of 1982–1998. Zeroing in on the effect of mandatory union memberships on state economies, the Institute emphasizes that during that 16‐​year period, states that mandated union membership in the manufacturing sector lost a net 996,000 jobs, while “right to work states” gained 493,000.



Let’s look at the flip side. How well are states with business‐​friendly public policy doing at attracting and retaining jobs? The anecdotal evidence suggests they’re doing pretty well.



According to the Bureau of Labor statistics, the only state that actually gained net manufacturing jobs from 2000 to 2003 was Nevada. It ranks 2nd on the SBSC’s business‐​friendly list. It ranks 3rd on the Tax Foundation list. It ranks in the top four of _CFO’_ s list. Alaska lost only 900 manufacturing jobs over those same four years, which is likely due to its population. Still, Alaska too ranked in the top four on the _CFO_ list. Virginia made a big push in the late 1990s to attract tech firms to its D.C. suburbs and the Dulles corridor. Despite the tech bust, Virginia still has one of the lowest state unemployment rates in the country and, perhaps not coincidentally, ranks 14th on the SBSC list (and would likely rank higher were it not for Gov. Mark Warner’s recent promise to raise taxes). South Dakota, which ranks number one on the SBSC list, also has one of the four lowest unemployment rates in the country (as of December 2003).



On its face, this cursory look at the data makes a lot of sense. For all the talk of off‐​shoring, the cost of packing up a domestic plant and moving it overseas is pretty significant. Even outsourcing tech support and programming doesn’t always make economic sense. American workers are still far more productive than, for example, Indian workers, even when you factor in the lower wages. It’s only when the onus of complying with federal, state, and local tax laws and regulations becomes overly burdensome that it makes economic sense for a corporation to shop jurisdictions for a better deal.



So the next time a local politician (or news anchor) blasts NAFTA or greedy corporatism for the loss of local jobs, it might not hurt to take a look at just how friendly that politician’s state or city taxes, regulatory and labor policies are toward business. Check where his state ranks on the Tax Foundation, SBSC or _CFO_ lists. If he’s a governor, see how he did on the Cato Institute’s Governor’s Report Card. If relocation really is the cause of the job hemorrhage he’s complaining about (and often it isn’t), it’s likely that same politician’s policies are a big reason those jobs left. 
"
"The Gobi Desert in East Asia conjures images of a remote landscape, with nomads riding across the steppe. In fact, today it is home to herders and farmers, the world’s fastest-growing economy, vast copper and gold mines and is China’s main domestic energy source. The imagined expanses and agro-pastoral livelihoods exist alongside mountains of coal, modern cities, desert agriculture and environmental challenges to its viability and future well-being. As Chinese president Xi Jingping arrives in Mongolia to discuss a series of trade and energy deals that would give Mongolia better access to global markets, it is worth looking at the shared desert that lies between Beijing and Ulaan Baatar. As the two nations work together, reconciling differences in the Gobi will be a major challenge. At 2.3m km2 the Gobi is the world’s third largest desert, covering most of Mongolia and much of northern China. Yet 25m people live in an area that stretches from the edge of Beijing to the Kazakh and Russian borders in the west and north. Across the Gobi, conservation reflects a shifting balance between human development and natural fragility. Home to the world’s highest sand dunes (more than 300 metres), most of the Gobi is a dry gravel plain and sparse rangeland. The landscape challenges residents with extreme cold (to -40C), hot summers (to 40C), periodic droughts and minimal surface water. Humans drive the need for extensive groundwater use, particularly in China, where the government encourages farming even though annual precipitation is often less than 200mm. This leads to competition for limited environmental resources between agriculture, cities such as Hohhot, Baotou and Urumqi (each more than 2m people), mining and traditional herding (Mongolia) and settled livestock-raising (China). Today the vast majority of water in Inner Mongolia – the autonomous Chinese province that borders Mongolia itself – goes for coal extraction and processing to meet China’s energy demands. Conservation of the tenuous desert environment and rural livelihoods encounter several socio-economic forces and physical challenges. Though a shared landscape, the issues differ greatly between China and Mongolia as policy, culture, history and democracy/autocracy separate the neighbours. In Mongolia water is essential for animals and household needs yet supply is obscured in shallow and deep aquifers that are difficult for locals to tap. Groundwater, essential for the desert’s new copper, gold and coal mining, requires money and technology to exploit and thus is pursued by regional and international mining companies. This results in conflict between local residents and businesses for limited water and raises issues of land use and livelihood viability among mobile herders, still the dominant lifestyle in rural Mongolia. In China strong state control and intervention has resulted in a manipulated water system where farmers need swipe-cards to get allocated water, use of natural pastures for animals is restricted and ecological resettlement sees once-mobile herders settled in villages by government decree. Removal of livestock opens land for farming and most importantly, for profitable mining that often is owned, or directly benefits, local governments. Mining in the region has led to economic growth, jobs, pollution, land degradation, dust generation and settlements that lack basic infrastructure. The notion of conservation and the role of nature in everyday life is integral to the Mongolian conception of the world whereas the Chinese model is focused on economic and infrastructure development irrespective of environmental impact. This splits the Gobi at the border; on one side roads, fencing, settlement, degradation and policy has ended free movement in China and sees the environment as something to be managed and exploited to ultimately benefit the several layers of government.  This leaves one to ask “what conservation” as water, land and vegetation are used for financial benefit, not as an inherent social good to protect. In Mongolia national parks comprise 13% of the country and species such as the Gobi bear, gazelle, marmot and Saker Falcon benefit from social conceptions of nature’s importance and varying degrees of protection. Though a vast area, the Gobi’s harsh environment and intricate ecosystem make wide swathes of open land and limited human use of nature key to conserving flora and fauna. This means creating non-financial value for wild steppe and desert regions. Without care the environment can become less productive and potentially experience desertification. Preserving nature takes insightful policy, sustainable land use, recognition of environmental benefits and the support of rural and mining communities.  In the Gobi this takes place against Mongolia’s weak institutional framework and China’s all-powerful bureaucracy. While the Communist Party remains in power conservation will be sacrificed for perceptions of growth and social stability. The picture in Mongolia is more optimistic as history and cultural preferences favour a strong role for nature in Mongolia’s conception of the world."
"**YouTube has suspended the One America News Network (OANN) for sharing misinformation about a Covid-19 ""cure"".**
The channel, which is a favourite of President Trump, is also suspended from making any money on YouTube.
The suspension will last a week, during which time no new videos can be put up. To make money in future, the channel must rectify the issues.
YouTube is attempting to clean up its platform and has also removed QAnon and pizzagate-affiliated accounts.
Pizzagate is a conspiracy theory about a paedophilia ring involving members of the US Democratic Party operating out of a Washington pizza restaurant, while QAnon believers think President Donald Trump is waging a secret war against elite Satan-worshipping paedophiles in government, business and the media.
In a statement about OANN, YouTube said. ""Since early in this pandemic, we've worked to prevent the spread of harmful misinformation associated with Covid-19.
""After careful review, we removed a video from OANN and issued a strike on the channel for violating our Covid-19 misinformation policy, which prohibits content that claims there's a guaranteed cure.""
YouTube operates a three-strikes policy - any second violation in a 90-day period would result in a two-week suspension. A third strike would lead to a permanent ban.
OANN must also reapply to the YouTube Partner Program if it wants to make money from its videos.
YouTube, along with rival social media firms Facebook and Twitter, is under intense scrutiny for the amount of misinformation and hate speech that finds its way onto its platform.
It said that it had removed 200,000 videos related to dangerous or misleading Covid-19 information since February.
This week, a group of Democratic senators wrote to YouTube chief executive Susan Wojcicki, urging it to remove all election outcome misinformation.
YouTube has removed adverts and added a warning label to a video falsely declaring that President Trump had won the election.
President Trump has tweeted that OANN and other news sources which question Joe Biden's election victory are a ""great alternative"" to Fox News."
"

Reposted from Climate Audit:
A savage article in the Times today by Ben Webster about the UEA submission to the  UK Parliamentary Inquiry – the letter in which they tried to “trick” the  Committee about the contents of the letter from the Information  Commissioner. (A “trick”, according to Gavin Schmidt and the Penn State  Inquiry, is a “good way” to solve a problem.)
The article – worth reading in full – re-caps correspondence  discussed in yesterday’s  post on the topic.
The UEA has now posted  up all its correspondence.
Webster provides an interesting new statement from Dr Evan Harris,  Liberal Democrat member of the Science and Technology Committee:
“It seems unwise, at best, for the University of East  Anglia to attempt to portray a letter from the Information  Commissioner’s Office in a good light, in evidence to the select  committee, because it is inevitable that the Committee will find that  letter, and notice any discrepancy.
“It would be a wiser course for the university not to provide any  suspicion that they might be seeking to enable the wrong impression to  be gained.”
Yup.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8e000ae2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"**The government has said there were no delays in delivering rapid coronavirus tests kits to Hull.**
Earlier, the prime minister pledged to ""immediately"" investigate why the kits had not yet arrived.
Hull was due to receive 10,000 ""lateral flow"" kits as part of a national rollout announced on 10 November.
The Department of Health and Social Care (DHSC) said it would send the tests ""once delivery information"" was received from the local authority.
Hull City Council has been approached for comment.
The city continues to have the worst infection rates in England, with 568.6 cases per 100,000 people in the week to 19 November.
Hull North Labour MP Dame Diana Johnson raised the issue on Monday in the House of Commons.
She said: ""Two weeks ago Hull was promised 10,000 lateral flow tests, but they still have not arrived.
""Shouldn't the prime minister focus on delivering on the ground what has already been announced, rather than grand new promises?""
Mr Johnson replied: ""I will take up immediately the point she makes about Hull and try to understand why they haven't got the lateral flow tests that she rightly wants to see.""
However, the Department of Health and Social Care later said there had been ""no delays in delivering tests to Hull"".
""We have requested the delivery information from the Local Authority and once this is received the tests will be sent,"" a spokesman added.
Last week, Hull City Council leader Stephen Brady wrote a letter to residents urging them to strictly follow lockdown measures imposed by the government on 31 October.
In the letter, he said: ""This really is our final chance to stop the spread by staying apart and thinking of others.""
He warned restrictions for the city were ""expected to remain severe"" after the current national lockdown ends on 2 December.
According to figures from Public Health England, Hull's infection rate has been slowing down but is still the highest in the country.
The city's rate has fallen from 779.9 per 100,000 people in the week to 12 November.
_Follow BBC East Yorkshire and Lincolnshire on_Facebook _,_Twitter _, and_Instagram _. Send your story ideas to_yorkslincs.news@bbc.co.uk _._"
"**After being labelled the ""patient zero"" of an outbreak of Covid, a Congolese-Canadian physician says he became a target for racist threats, a pariah in his community, and a ""scapegoat"" for local officials.**
When Dr Jean-Robert Ngola heard that he had to pick up his daughter last May, he quickly did the maths.
His daughter lived in Montreal with her mother, about seven hours away from his home in Campbellton, New Brunswick.
As a family physician, he knew that he needed to be as safe as possible to limit the spread of Covid to others. But as a parent, he had to come get his child so that her mother could attend a family funeral in Africa.
In order to get her and have contact with as few people as possible, he hopped in his car and drove all day, spending the night at his brother's before driving her back.
Before leaving, Dr Ngola called the local police, asking for clarity around the laws about self-isolation.
New Brunswick has one of the strictest quarantine policies in Canada. Along with several other eastern provinces, it has formed an ""Atlantic bubble"" - in the early months of the pandemic, most forms of travel into the bubble were restricted, and anyone entering had to quarantine for 14 days.
But as a frontline worker, he says police told him he was exempt.
Not wanting to leave his patients without a doctor, he decided to go back to work.
On 25 May, Dr Ngola heard that one of his patients had been diagnosed with the virus. He got tested, and began to self-isolate with his four-year-old daughter.
At 11am on 27 May, he learned he, too, had the virus, although he had no symptoms.
Then, his life began to fall apart. Within the hour, his identity began to spread online. Later in the afternoon, Premier Blaine Higgs, who leads the provincial government, was chastising him on live television.
At least two other people had contracted coronavirus ""due to the actions of one irresponsible individual,"" Mr Higgs said, after nearly two weeks without a single case.
Although the premier did not name Dr Ngola directly, by that time, people had connected the dots, and photographs of his office were circulating online.
Provincial health officials told the media Dr Ngola had contracted the disease in the neighbouring province of Quebec, and spread it to others because he did not follow the 14-day mandatory quarantine for people who had been out of New Brunswick.
But Dr Ngola, and his lawyer Joel Etienne, say the rules were not clear, and Dr Ngola was following the same practices as people around him.
They also dispute the province's claim that he was ""patient zero"".
Although no criminal charges were filed, Dr Ngola faces a civil charge for violating the Emergency Measures Act and could face a fine up to C$10,200 ($7,600, Â£6,000). The case is currently making its way before the courts.
His employer, VitalitÃ© Health Network, immediately suspended him without pay for breaking protocols.
""I was the scapegoat. As soon as my diagnosis is madeâ¦ one hour later, my life changed,"" he said.
In a statement to the BBC, a spokesperson for Vitalite confirmed that Dr Ngola's suspension continues, and declined to comment further.
The premier's office did not respond to the BBC's request for comment.
The reaction from the community was swift and brutal. Dr Ngola, who is originally from the Democratic Republic of Congo, said people were telling him to ""go back to Africa"" and other forms of racist abuse.
Quarantining in his home with his small daughter, he feared for his safety when his address was leaked online and had to go under police protection. But he was also constantly under police scrutiny, he says, because people kept phoning in with bogus ""tips"" claiming to have seen him break quarantine.
Dr Ngola says the harassment was so bad, he had to leave the province. He was offered a job in a small community in Quebec, where he has been living for the past several months. He says he feels welcome there, but his experience in Campbellton has left psychological scars.
""I cannot have the same life because now I'm public,"" he says.
This is not the first time he's had to start over. Born in the Congo, Dr Ngola had wanted to be a doctor since he was a small child, after coming down with polio and being unable to get the appropriate medical treatment because it was too costly.
""My childhood ambition was to become a doctor in order that children like myself would be spared,"" he says.
He paid his way through school by tutoring other students, and practised medicine during his country's brutal civil war.
In 2000, he immigrated to Belgium, where he had to retrain in order to continue to be a doctor. Then five years later, he relocated to Canada, because he felt he would face less prejudice as an African immigrant. Once again, he went back to school in order to practise medicine.
Now, he feels disillusioned. Dr Ngola says he believes his race and immigration status played a factor in how he was treated not only by the public at large, but by the province's top officials.
""What is the difference with me? The difference is I'm black and I'm a foreigner,"" Dr Ngola said.
Although he felt like a pariah at home, around the country his fellow physicians were rallying to his defence. In September, 1,500 physicians signed a letter condemning his treatment, and demanding an investigation into how his name was leaked to the press.
""All of us signed below have felt tremendous anger, discomfort and frustration with the backlash that followed once you were publicly identified. What unravelled thereafter was unjust, unkind and dehumanizing,"" the letter, which was spearheaded by Dr Danusha Foster, read.
""We strongly believe that systemic racism coupled with the stigma surrounding individuals infected with the Covid-19 virus have significantly contributed to the crucifixion of your character within the public eye.""
Dr Foster, a family physician who lives in Ontario, says that when she first heard about Dr Ngola, she, like many others, judged him.
""We're in a deadly pandemic, and health professionals should be held to a higher standard at this time, because we're supposed to be modelling for the general population what we should be doing,"" she told the BBC.
But after hearing about the abuse he was suffering, and reading media articles critical of the province's investigation, she began to feel sympathy.
""He was being judged in the public eye before the facts were known,"" she says.
She says patient confidentiality is the ""core"" of the doctor-patient relationship, and whoever leaked his name should be held to account.
After talking about his case on online physicians groups, she decided to organise the letter of support, to show Dr Ngola that he wasn't alone.
""I hadn't even written my letter, and I already had 800 names that wanted to sign,"" she says. ""We realised as we watched this case that what happened to Dr Ngola, could have happened to anyone of us... if we made one little mistake.""
Over 40 cases and two deaths have been connected to the outbreak in the Campbellton region since 27 May. But it remains unclear if Mr Ngola was the source.
Anyone entering New Brunswick from another province outside the Atlantic Bubble is supposed to quarantine for 14 days.
But residents of Campbellton, which is on the border of Quebec, were allowed to cross the border without self-isolating for certain reasons, such as if they worked in Quebec, had to attend a medical appointment in Quebec or if they shared custody of a child in Quebec.
According to the provincial guidelines, ""all such workers and individuals who are exempt from self-isolation must travel directly to and from work and/or their accommodations, self-monitor and avoid contact with vulnerable individuals""
Dr Ngola spent about 30 hours in Quebec, including an overnight stay with his brother to rest up after the seven-hour drive. He also saw two colleagues in Trois-Rivieres, although they were masked and socially distanced. He also had contact with a petrol-station employee.
Dr Ngola said many of his co-workers went back and forth to Quebec and did not fully self-isolate, so he did not think he was breaking the rules.
His employer, Vitalite Health, told the CBC's Fifth Estate that all workers were ordered to self-isolate after returning to the province unless they lived in Quebec.
Mr Etienne says regardless of whether his client broke the rules, the province failed to do its due diligence before blaming him for the outbreak.
He says they had not finished contact-tracing the four individuals in Quebec whom Dr Ngola had contact with, before claiming the doctor was the source of the Covid cluster in New Brunswick. He says their own private investigator found that neither his brother, the two colleagues, nor the gas station employee tested positive for Covid.
His daughter, however, did. Both she and her father have made a full recovery.
There had been at least one confirmed case of Covid in Campbellton in the days prior to Dr Ngola's diagnosis, and his lawyer says he could have got it from community spread."
"
Share this...FacebookTwitterThe German online CO2 Handel here writes that things are looking bleaker than ever for a Kyoto successor climate treaty.
Once again German Environment Minister Norbert Röttgen has dumped more cold water on expectations for a climate treaty in Durban later this year. CO2 Handel writes:
‘A breakthrough for a global climate protection treaty as a successor to the Kyoto Protocol which expires in 2012 is unrealistic,’ said Röttgen on Friday in Capetown.”
Even worse for the junk-science-based climate rescue movement is that their numbers are shrinking rapidly and approaching fringe status. As China and USA refuse to sign a treaty, CO2 Handel writes:
After the announced departures of Japan and Canada from the Kyoto process, only the EU as well as Norway and a few other countries see themselves contractually obligated to systematically reduce CO2 emissions. Rising CO2 emissions of countries, among them the USA and China, as well as the shrinking number of countries in the Kyoto process could result in Kyoto countries being responsible for only 16% of the global CO2 emissions.”
Share this...FacebookTwitter "
"

One reaction to President Bush’s plan for a permanent moon base and a trip to Mars is, “Great! It’s about time NASA stopped going around in circles in low Earth orbit and returns to real science and exploration.” Unfortunately, there’s not a snowball’s chance in the sun that the same agency that currently is constructing a downsized version of its originally planned space station, decades behind schedule, at 10 times its original budget, a few hundred miles up in orbit, will be able to build a station several hundred thousand miles away on the moon.



If Americans are again to walk on the moon and make their way to Mars, NASA will actually need to be downsized and the private sector allowed to lead the way to the next frontier.



The lunar landings of over three decades ago were among the greatest human achievements. Ayn Rand wrote that Apollo 11 “was like a dramatist’s emphasis on the dimension of reason’s power.” We were inspired at the sight of humans at our best, traveling to another world. In announcing NASA’s new mission, President Bush echoed such sentiments, speaking of the American values of “daring, discipline, ingenuity,” and “the spirit of discovery.”



But after the triumphs of Apollo, NASA failed to make space more accessible to mankind. There were supposed to be shuttle flights every week; instead, there have been about four per year. The space station was projected to cost $8 billion, house a crew of 12 and be in orbit by the mid‐​1990s. Instead, its price tag will be $100 billion and it will have only a crew of three. Worse, neither the station nor the shuttle does much important science.



Governments simply cannot provide commercial goods and services. Only private entrepreneurs can improve quality, bring down the prices, and make accessible to all individuals cars, airline trips, computers, the Internet, you name it. Thus, to avoid the errors of the shuttle and space station, NASA’s mission must be very narrowly focused on exploring the moon and planets, and perhaps conducting some basic research, which also might serve a defense function. This will mean leaving low Earth orbit to the private sector.



Thus, the shuttle should be given away to private owners. The United Space Alliance, the joint venture between Boeing and Lockheed‐​Martin that refurbishes the shuttle between flights, would be an obvious candidate. Let a private owner fly it for paying customers–including NASA, if necessary — if it is still worth flying.



NASA also should give up the money‐​draining space station, and sooner rather than later. The station might be turned over to international partners or, better still, to the mostly private Russian rocket company, Energia — and the Western investors who were in the process of commercializing and privatizing the Mir space station before the Russian government brought it down for political reasons. If need be, NASA can be a rent‐​paying station tenant.



NASA centers that drive up its overall budget but do not directly contribute to its mission should be shut down. If the government wants to continue satellite studies of the climate and resources or other such functions, they could be turned over to other agencies, such as EPA and Interior Department. 



NASA and the rest of the government should contract for launch services with private companies, which would handle transportation to and from low Earth orbit. Contracting with private pilots with private planes is what the Post Office did in the 1920s and 1930s, which helped the emerging civil aviation sector. Further, to facilitate a strong private space sector, the government needs to further deregulate launches, export licensing and remove other barriers to entrepreneurs.



Creating enterprise zones in orbit would help make up for government errors of the past. Rep. Dana Rohrabacher proposes a “Zero Gravity, Zero Tax” plan that would remove an unnecessary burden from “out‐​of‐​this‐​world risk‐​takers.”



NASA will also need to do business in new, innovative ways. For example, if a certain technology is needed for a moon mission, NASA could offer a cash prize for any party that can deliver it. The federal government used such an approach for aircraft before World War II, modeled after private prizes that helped promote civil aviation.



Even if the federal government foots the bill for a moon base, it should not own it. Rather, NASA should partner with consortia of universities, private foundations and even businesses that are interested in advancing human knowledge and commercial activities. NASA could simply be a tenant on the base.



Or consider a radical approach proposed by former Rep. Bob Walker. The federal government wouldn’t need to spend any taxpayer dollars if it gave the first business to construct a permanent lunar base with its own money a 25‐​year exemption from all federal taxes on all of its operations, not just those on the Moon. Think of all the economic activity that would be generated if a Microsoft or General Electric decided to build a base! And the tax revenue from that activity probably would offset the government’s revenue losses from such an exemption.



If we’re true to our nature, we will explore and settle planets. But only individuals with vision, acting in a free market, will make us a truly space‐​faring civilization. 
"
"Conservationists have recently become very excited about financial incentives. The idea is to pay people to do things that will help biodiversity, for example, where farmers are paid not to till crops that reduce soil erosion or where landowners are given money to plant trees to capture carbon in the atmosphere.  This technique, considered by some as bribery to do what you want, has actually changed environmental behaviour for the better in some significant instances. “Great”, I hear conservationists say, “let’s pay everyone to do exactly what we want!”  The possibilities are endless: we could give farmers money to set aside land for nature, we could make fishing more sustainable, or we could reward people to not kill threatened predators.  There are, however, problems with the system behind paying people to do things that they wouldn’t normally do, not least because sometimes money cannot solve everything. Take carnivores, for instance: menacing, bloodthirsty killers of infamous myth and legend that have plagued farmers’ nightmares for millennia.  These species provoke such raw emotions in some people that intolerance has become a cultural identity, learned through generations of ingrained hatred.   So when conservationists came up with the idea to pay reindeer herders to tolerate wolves in Sweden, it is no surprise that the herders declined the payment in favour of managing wolves themselves (that is: killing them). Such a long-standing conflict between livestock farmers and carnivores is too embedded in the psyche to overcome purely by throwing money at people.  If anything, it could be seen as an insult to their integrity, just as financial incentives paid to families of organ donors have been criticised for turning human life into a commodity.  To many farmers, a cow is worth more than its market value, just like their farm is worth more to them predator-free (at the expense of compensation) than with predators (and a financial reward).  This may be one of the reasons why schemes have often failed to increase tolerance even when reimbursements have been paid for livestock killed by predators. To pay a farmer to accept a predator onto a farm is almost as absurd as paying Israelis to tolerate Palestinians: both are long-standing, deep-rooted conflicts that cannot be bought out unless underlying issues are resolved.  Both situations can therefore be regarded as “wicked problems” with no clear way.  As such, is it even useful to use financial incentives for carnivore conservation? As with other challenges in wildlife management, the answer is not clear cut.  Yes, sometimes it does appear to work with species that may not have caused much controversy in the past, such as the aforementioned Swedish scheme which increased tolerance towards wolverine and lynx.  But for highly emotive species such as wolves or lions, bribery appears impotent.  Consequently, we must try other tactics to improve opinions towards the predator species that produce these immensely charged responses.   Educating children on how important predators are to the ecosystem may improve attitudes before they have been shaped by their parents’ perceptions, but this tactic takes time and effort (and may tread on dodgy ethical ground if it is seen to be brainwashing children ). Or it may be that farmers find their own way to benefit from carnivore presence on their land, such as by offering tourists or trophy hunters the chance to “shoot” wildlife (with a camera or a gun).   But this will not sway the extremists who cannot fathom accepting such a beast onto their land.  For these old-school farmers, the only way they can envisage a predator is hanging as a skin on their wall or in the security of a fenced protected area far from their land."
"When the cross-party Treasury committee is officially appointed on Monday, one of its first tasks will be to question Andrew Bailey over his role as the next governor of the Bank of England, which he is due to start on 16 March. This is standard procedure for the influential group of MPs, but comes at a time of great challenge for an incoming governor, not least amid the unfolding economic collapse caused by the coronavirus outbreak. Bailey, who replaces Mark Carney and was chosen as his successor in December by the former chancellor Sajid Javid, will face several urgent tasks in his opening weeks and months as governor to address – as well as some potentially awkward questions. Here are some of the key subjects the committee is likely to quiz him on. His recordWhen Bailey was appointed he was widely hailed as a “safe pair of hands”. However, critics of his time at the Financial Conduct Authority beg to differ. Gina Miller, the businesswoman best known for her legal challenge to the government’s attempt to push through Brexit without parliamentary approval, is leading a group calling for a review of Bailey’s appointment and has welcomed the Treasury committee’s opportunity to examine his record. Miller has highlighted his reaction to the collapse of investments run by London Capital & Finance, and the liquidation of Neil Woodford’s once-popular flagship investment fund, among other things. “The hundreds of thousands of Britain’s depositors, savers, investors and pensioners whose lives have been ruined and savings squandered, on Mr Bailey’s watch as CEO of the Financial Conduct Authority, deserve no less,” Miller said. Bailey has argued on many of the issues that the FCA’s statutory powers were limited. However, the scandals have overshadowed his time at the regulator. CoronavirusThere are questions about how much the Bank can do to support the economy given the global nature of the coronavirus outbreak and the limited firepower central banks have at their disposal to combat an economic downturn. Bailey arrives with interest rates still close to the lowest levels in the Bank’s 325-year history, at 0.75%, only 0.25 percentage points higher than when Carney started at the Bank almost seven years ago and limiting the scope for further cuts if deemed necessary. Whether Bailey will be able to make much of an inroad into returning rates to more normal levels will be closely watched. BrexitThe British economy could be put under renewed strain later this year as the Brexit negotiations gather pace ahead of the deadline for the UK’s exit from the transition period at the end of December, which Boris Johnson has warned he will not extend. Once the dust from the coronavirus outbreak has settled, the path for interest rates and the continued stability of the financial system will partly depend on the nation’s new trading relationship with the EU, and the extent to which there are tougher barriers, friction at Britain’s borders and whether there is a smooth or abrupt transition. Carney had been accused of bringing the central bank’s independence into question, given his repeated warnings about the threat posed by Brexit to the UK economy and banking system. Serving under the government of Boris Johnson, the new governor could face more questions about Britain’s future relationship with the EU. ClimateAs governor, Mark Carney brought to the top of the agenda the importance of global heating as a problem for central bankers to address, warning about the existential crisis facing the world financial system. Threadneedle Street is preparing to run a “catastrophic” climate-related stress test next year, a landmark moment. Bailey will need to show early on that he will take the issue as seriously as his predecessor. Hedge fund scandalBailey is likely to face questions about the embarrassing revelation  about a backup audio feed that without the Bank’s knowledge potentially allowed traders early access to an audio feed of market-moving Bank press conferences. The answers to those questions could have broader implications for how the Bank protects key national infrastructure, amid criticisms of where accountability lies for failures at the institution. DiversityBailey is set to become the 121st governor – and the 121st man to take the role. That imbalance is mirrored throughout the top levels of the Bank, with a lack of gender or ethnic diversity in top jobs. There is only one woman, Silvana Tenreyro, on the Bank’s nine-member rate-setting monetary policy committee. Across the Bank Bailey will be pressured to close ethnicity and gender pay gaps, as well as increase representation, which sometimes went backwards during Carney’s tenure. IndependenceThe Bank of England is run at arm’s length from the government to preserve its independence at setting interest rates and producing economic forecasts. As the government expands its spending – boosting the economy – the Bank of England may feel the need to raise interest rates to stop inflationary pressures building. That could bring the Bank into conflict with the government and Johnson’s choice as chancellor, Rishi Sunak. How Threadneedle Street and Downing Street work together will be a key question of Bailey’s regime – and potentially his legacy as governor."
"

It has been four years since President Bush declared the Kyoto Protocol a dead letter, but the campaign to impose industrial greenhouse‐​gas emission controls on the American economy shows no signs of letting up. Although Sen. John McCain’s bid last month to include concrete emission controls in the pending energy bill attracted only 38 votes, the Senate subsequently passed a resolution calling on Congress to “enact a comprehensive and effective national program of mandatory, market‐​based limits on emissions of greenhouse gases that slow, stop and reverse the growth of such emissions.” An attempt to table the resolution was opposed by Republican senators Lamar Alexander, Lincoln Chafee, Susan Collins, Mike DeWine, Pete Domenici, Lindsey Graham, Judd Gregg, Richard Lugar, John McCain, Olympia Snowe, Arlen Specter, and John Warner. Apparently, even red‐​state Republicans are having doubts about Bush’s position on climate change.



One might think that the increased political buzz around global warming is driven by science. One would be wrong. The scientific case for alarm is no more compelling today than it was yesterday. 



The first (and sometimes last) stop in the global‐​warming debate is the question, Is it real? The answer seems to be yes. Ground‐​based and oceanic temperature records show warming of about three‐​quarters of a degree Celsius in the last century. About half of that warming, however, occurred before World War II and is widely thought to be related to solar activity. Satellite and weather‐​balloon records, which do not go back as far, show less warming in the late 20th century than the land‐​based stations.



What’s causing this warming? We don’t know. As the vice president of the U.N.‘s Intergovernmental Panel on Climate Change (IPCC), Yury Izrael, wrote bluntly last month, “There is no proven link between human activity and global warming.” Given the extreme variability of global temperature, warming might simply be statistical noise. It might result from solar and/​or volcanic activity. It might be caused by industrial emissions. And it might come from some combination of the three.



What do most scientists suspect is going on? The best way to ascertain the “scientific consensus” is to look at the latest report of the IPCC (released in 2001), which purports to summarize the state of scientific knowledge on global warming. Here’s what it says: “Most of the observed warming over the last 50 years is likely to have been due to the increase in greenhouse gas concentrations.” The report finds that it is “unlikely (bordering on very unlikely) to be entirely the result of internal variability,” and that “natural forcing alone [i.e., solar and/​or volcanic activity] is unlikely to explain the increased rate of global warming since the middle of the 20th century.”



The promiscuous use of such vague terms as “likely” and “unlikely” by scientists who are trained in precision speaks volumes about how much is unknown. At the very least, such language makes it impossible to accept the Greens’ claim that “the debate is over,” particularly given all the uncertainty — fully discussed in the IPCC report — regarding long‐​term climate records and important data on atmospheric feedbacks. In fact, uncertainty about future climate conditions is greater in the 2001 IPCC report than it was in the 1995 IPCC report.



Do other reviews of the scientific literature tell a different story? It depends on whom you ask. An article by Naomi Oreskes in Science last December examined 1,000 scientific papers published since the early 1990s. Oreskes concluded that 75 percent of those papers either directly or implicitly supported the argument that industrial emissions are driving global warming, and none directly argued to the contrary. A subsequent review of the same articles by Benny Peiser, a senior lecturer on the science faculty at Liverpool John Moores University, found nothing of the kind. Peiser concluded that only one‐​third of the papers reviewed by Oreskes actually supported the “consensus view,” and only 1 percent did so explicitly.



In any case, debating what constitutes the mainstream thinking on climate change is not particularly enlightening. Regardless of how one defines “the consensus,” scientific truth is not revealed by a show of hands. As Thomas Kuhn demonstrated in The Structure of Scientific Revolutions, the history of scientific progress is a history of once‐​solid consensuses being overthrown by minority skeptics. In short, today’s consensus proves nothing.



Even more heated than the debate about the cause of climate change is the debate about its likely effects. In fact, most of the so‐​called skeptics who publish in peer‐​reviewed literature accept the contention that mankind is probably responsible for most present‐​day warming. They argue, however, that the warming has been and will continue to be quite modest, and that the pattern of warming we’re seeing does not suggest that a parade of horribles awaits us.



The skeptics are on solid ground here because the atmosphere simply has not proven to be as sensitive to industrial greenhouse‐​gas emissions as some theorists have feared. Unless some temporary phenomenon is masking the effect of such emissions, atmospheric physics suggests that warming will occur at a linear rate — a conclusion affirmed by almost all the computer climate models in existence. This insight suggests a simple exercise: Plot temperature data over the last 50 years and draw a trend line to see what the future has in store. Doing so suggests that warming will likely be at the low end of the IPCC’s projections — about 1.5 degrees Celsius by the year 2100.



Should we worry about such modest warming? From an ecological perspective, probably not. Because water vapor is responsible for 94 percent of the natural greenhouse effect, industrial greenhouse gases have a greater impact in dry air masses than in wet ones. Fully 78 percent of the warming has been concentrated in the driest air masses, which are primarily found during the winter (when 69 percent of the warming has occurred), at night, and in the northern latitudes.



The fact that winter nighttime lows in the Northern Hemisphere aren’t quite as cold as they used to be need not cause anyone to panic — and there seems not to be an increased incidence of the destructive weather events that would follow from warming in wet air masses. According to the IPCC, “[T]here is little sign of long‐​term changes in tropical storm intensity and frequency,” and “no compelling evidence” that local severe‐​weather events are on the rise. Most important, “no significant acceleration in the rate of sea level rise during the 20th century has been detected.” Precipitation in the northern hemisphere has likely increased by a meager 0.5–1 percent a decade, but “no comparable systematic changes in precipitation have been detected in broad latitudinal averages over the Southern Hemisphere.”



There are good reasons to think that a warmer world might be a better world. Agronomists, for instance, are fairly convinced that heavier concentrations of carbon dioxide in the atmosphere, as well as the longer and somewhat wetter growing seasons that follow from the greenhouse warming pattern, have already increased crop yields and will continue to do so. Warmer weather also leads to declines in energy use, and probably fewer weather‐​related deaths.



Not surprisingly, economists who have examined the implications of the warming projections offered by the IPCC have had a hard time proving the existence of net negative effects. In fact, Yale forestry economist Robert Mendelsohn has demonstrated that nations north of the equator will probably benefit from global warming, and that warming will likely prove an economic wash for the world as a whole.



Both sides in the global‐​warming debate contend that “sound science” should dictate public policy. For the foreseeable future, though, it’s unlikely that scientists will be able absolutely to prove or disprove the proposition that industrial greenhouse‐​gas emissions are ushering in a dangerous warming trend. Even if scientists could prove this, they have no particular expertise at choosing among competing policy responses. Nor are scientists’ levels of risk aversion, or their choices about how to hedge against risk, necessarily superior to those of anyone else.



Scientists cannot tell us how best to handle the threat posed by global warming, no matter how much we, or they, wish otherwise. They can help inform the debate — but they have less to contribute than most people think.
"
"
Share this...FacebookTwitterSkepticism of junk climate science has taken root even in Germany. Already the 4th International International Climate And Energy Conference is taking place in Munich on November 25-26, 2011.
Leading scientists, experts and critics of the AGW science and man-made climate change are gathering to present the newest results. Mark it down on your calendar!
New speakers this year will include:
– Andrew Montford
– Werner Kirstein
– Henrik Svensmark
– Chris Horner
– Piers Corbyn
They’ll be joining an impressive line-up of speakers like Nir Shaviv and Jan Veizer. Last year I attended the conference in Berlin and I hope to attend this year too.
This year’s line up of speakers looks even better, and the program (still preliminary and likely to change some) has been expanded to 2 full days. Read more at EIKE. Here’s how the program looks right now:
Friday – November 25
8:00 a.m.
Registration
9:00 a.m.
* Welcome – Why do we still deal with climate change?
Dr. Holger Thuss, European Institute for Climate and Energy (EIKE)
Wolfgang Müller, Berlin Manhattan Institute (BMI), European Institute for Climate and Energy (EIKE)
9:30 a.m.- 12:30 p.m.
Panel: Measuring vs. modelling
* Real temperature measurements vs. climate alarmism
Prof. em. Dr. Horst-Joachim Lüdecke Hochschule für Technik und Wirtschaft des Saarlandes
* Glaciers as climate witnesses
Prof. em. Dr. Gernot Patzelt, University of Insbruck
* Anthropogenic sea level rise: from scenario to panic
Dipl. Meteorologe Klaus-Eckart Puls, Press spokesperson, EIKE
11:30 a.m. – 12.00 a.m. 
Break
– Mission impossible – geological facts of carbon capture and storage in Germany
Prof. em. Dr. Friedrich-Karl Ewert, University of Paderborn
12:30 p.m. – 2:00 p.m. 
Lunch – conference venue
2:00 p.m. – 4:00 p.m.
Panel: Cosmic rays, CO2 and climate
* Climate, water, CO2 and the sun
Prof. Dr. Jan Veizer, Department of Earth Sciences, University of Ottawa
* The cosmic ray climate link – evidence and implications to the understanding of
climate change
Prof. Dr. Nir Shaviv, Racah Institute of Physics – The Hebrew University of Jerusalem
* The impact of solar activities and cosmic rays on the world climate
Prof. Dr. Henrik Svensmark, Centre for Sun-Climate Research of the Danish National Space Centre
4:00 p.m. – 4:30 p.m.
Break
4:30 p.m. – 5:30 p.m.
Panel: Update on the CERN study cosmic rays and climate change
Prof. Dr. Jan Veizer, Prof. Dr. Nir Shaviv, Prof. Dr. Henrik Svensmark
5:30 p.m. – 6:30 p.m.
Climategate – The story of a cover-up
Andrew Montford, Bishop Hill Blog
Followed by dinner – buffet


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Saturday – November 26
9:00 a.m. – 11:00 a.m.
Panel: Forecasts vs. scenarios
* Climate change between statistics, models and substitute religion
Prof. Dr. Werner Kirstein, Institute for Geography, University of Leipzig
* Accurate long term weather forecasts are possible
Piers Corbyn, Weather Action, London
11:00 a.m. – 11:30 a.m.
Break
11:30 a.m. – 12:15 p.m.
* The urban legend of the Hockey Stick
Andrew Montford, Bishop Hill Blog
12:15 p.m. – 13:00 p.m.
* Investing wisely– opportunities and dangers in alternative energy
(tbc)
13:00 p.m. – 2:30 p.m.
Lunch – at conference venue
2:30 p.m. – 4:00 p.m.
Panel: Climate- and Energy Policy – Wish and Reality
* The green economy: Crony capitalism’s newest big idea
Dr. Christopher C. Horner
Center for Energy and Environment – Competitive Enterprise Institute, Washington, DC
* The costs of Germany’s green energy agenda – plan vs. reality
Prof. Dr. Gerd Ganteför, University of Konstanz
4:00 p.m. – 4:30 p.m.
Break
4:30 p.m. – 5:30 p.m.
* Covering their tracks: the IPCC and transparency
Dr. Christopher C. Horner
* Center for Energy and Environment – Competitive Enterprise Institute, Washington, DC
5:30 p.m. – 6:30 p.m.
* Climate policies – a threat to liberty
Prof. Dr. Gerd Habermann; University of Potsdam, Hayek Society
6:30 p.m. – 7:00 p.m.
* Closing remarks
Dr. Holger Thuss, President Of The European Institute for Climate and Energy (EIKE)
7:00 p.m.
Reception / end of conference
Prices for admission
€80.00 for 1 day, private individuals
€140.00 private individuals for 2 days
€290.00 person for companies
Prices already include VAT.
Also included are the documentation, 3 meals and 4 coffee breaks/day. Note this is only the preliminary plan and is subject to change!
Registration form will be made available soon.
More info here at EIKE.
Share this...FacebookTwitter "
"
By Bill Steigerwald

“Abraham, Martin & Grandpa”
WASHINGTON, D.C.
Grandpa was afraid their special glasses weren’t working properly and the police officers could tell they were polar bears, so he yelled from the sidewalk.
“Hello, officer. We’re visiting from out of town. Do you know if there’s a Wal-Mart nearby that’s open all night?’
“What do you want with a Wal-Mart?” the suspicious policeman responded, moving his hand to the handle of his service pistol. “They’re illegal in this city.”
“It’s his favorite store,” Junior piped up. “He doesn’t even care if the toys are made in China.”
“We’re just looking to ‘save money and live better,’” Mother said, smiling as innocently as she could. Her big white teeth and eyeglasses reflected the shine of the policeman’s light.
The policeman stared at the bears for what seemed like forever. “Move along,” he finally said as he switched off the spotlight and slowly drove away.
“Man,” the policeman said to his sleepy partner in the front seat, “Didn’t she remind you of that Sarah Palin woman?”
“Nah,” his partner said. “Too big and hairy. They must be from Russia or something.”
Free from the watchful eyes of the DC police, the bears continued to their next stop, the venerable Lincoln Memorial. Through the locked entrance doors the bears could see the statue of Abraham Lincoln sitting in his gigantic stone chair.
Standing on the memorial’s marble steps in the dawn’s early light, the three bears admired the Washington Monument and its reflecting pool.
“These steps are where Martin Luther King gave his ‘I Have a Dream’ speech in 1963,” Grandpa said to Junior. “See that tall white dome with the point on it?” he said, pointing to a building behind the Washington Monument.
“That’s the U.S. Capitol Building. It’s where the country’s laws are made, for better or for worse. I’ll be there tomorrow fighting for the freedom of all the world’s polar bears.”
“Aren’t you scared, Grandpa?” Junior asked.
“Not any more,” Grandpa said. “I know I’m fighting for what’s right. Plus, I have Mr. Jefferson, Mr. Lincoln and the Rev. King on my side.”
When the three bears arrived at the U.S. Capitol Building at 9 a.m., a wild scene awaited them on the West Lawn.
Thousands of people had come to demonstrate their love for polar bears and their support for a new law to place them on the Endangered Species list. Everywhere the three real bears looked they saw cute and cuddly make-believe polar bears.
High above them were two polar bear hot-air balloons and blowing along the snowy ground were a dozen inflatable bears eight feet tall and topped with Santa Claus hats. Scores of humans danced in cheap polar bear costumes and wore “Stop Global Warming” T-shirts or waved hand-made signs that read “Polar Bear SOS!” or “Save Me.”
Hundreds of public school children brought in by bus were chanting “Save our polar bears” and selling plastic polar bear figurines to raise money to fight against drilling for oil in the Arctic.
In a row of carnival booths surrounding the Capitol Building Christmas tree, a lot of money was being made by the many environmentalist groups. The Natural Resource Defense Council was seeking donations for its “Polar Bear S.O.S.” campaign. The World Wildlife Foundation offered symbolic polar bear adoption kits for $250.
Grandpa, Mother and Junior made their way through the crowd in their human costumes. Thanks to their special eyeglasses, no one realized they were polar bears. When they reached the bottom of the Capitol Building’s steps, Grandpa turned and surveyed the crazy scene.
“Saving polar bears is big business. Too bad we don’t get any royalties,” he said with a laugh. Then Grandpa pushed his eyeglasses to the back of his nose, unbuttoned his suit coat and started up the steep steps of the Capitol Building. “Come on, kids. Let’s roll.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8f3269cb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**The number of daily deaths from Covid-19 in Scotland's second wave has begun to go down. Hospital admissions are also declining, as are infection rates in the worst affected areas. Has Scotland's second wave peaked, or is there still danger ahead? Here are five numbers to watch.**
Across the whole of Scotland, the average number of new cases every day is no longer rising.
The figure appears to have peaked towards the end of October and has mainly been going down since then.
However, there are still significant areas of Scotland where cases are on the rise and this remains a concern for the Scottish government as it attempts to slow the spread of the virus.
On 20 November, the 11 local authorities with the highest infection rates were moved to level four, which has the toughest set of restrictions under Scotland's five-tier system.
The three worst-affected local authorities in level four at the end of last week were Glasgow City, North Lanarkshire, and Renfrewshire.
The infection rate in North Lanarkshire has mainly been declining since mid-October. The rate also now appears to be going down in Glasgow after several weeks at ""stubbornly high"" levels - as ministers have repeatedly noted.
However, there's no such decline evident in Renfrewshire, which has seen fluctuating rates between 200 and 300 cases per 100,000 since early October.
It's worth nothing that South Lanarkshire, which reached almost 400 cases per 100,000 people in October, has shown a sustained decline in infections since then.
But it's not just the local authorities with the highest rates that cause concern.
Anywhere which shows a big increase in cases over a short period will catch the attention of public health officials, who want to stop the virus becoming seated in a new community.
Two weeks ago the focus was on Angus, Fife and Perth and Kinross, which all moved from level two to level three restrictions after a sharp rise in cases.
Despite a dip in the number of cases in Fife, it's too soon to tell if the rate will show a sustained decline here - and in Perth and Kinross the rate is still very much on the rise.
First Minister Nicola Sturgeon also highlighted significant rises in Stirling and Inverclyde.
The rate in Inverclyde is now declining, but it's still going up in Stirling - as it is in neighbouring Clackmannanshire, which up until October had seen some of the lowest infection rates across the central belt.
Infection rates are not the only indicator the Scottish government uses to review levels, but they are a key measure to watch.
The daily number of new confirmed cases of Covid-19 is an important statistic - but it is difficult to make a comparison between now and March using this figure as the number of people being tested has risen so significantly.
One number that is arguably a better gauge of where Scotland is in its second wave is hospital admissions.
The daily admissions for Covid-19 in Scotland hit a low in mid-July.
They then started a slow rise which gathered pace into the autumn, but for the last two weeks the average number of admissions has mainly been going down.
If this decline is sustained, the peak will have been significantly lower than in the spring.
The sickest Covid-19 patients are likely to end up in intensive care, where the mortality rate is high.
A report by Public Health Scotland published in July found that almost 40% of coronavirus patients died within 30 days of being admitted to intensive care.
The number of daily ICU admissions began to rise at the end of the summer - there were six Covid-19 patients in intensive care on 11 September and there were 111 on 8 November.
Numbers have been fluctuating in the last few weeks, but there doesn't yet appear to be the same decline in ICU admissions seen in hospital admissions.
The Scottish government announces daily figures of deaths within 28 days of a positive test for Covid-19.
Scotland went 40 days over the summer with no deaths recorded at all using this measure.
The average number of new Covid-19 deaths being registered each day rose steadily from mid-September, but has now begun to decline.
The National Records of Scotland counts all death certificates that mention Covid-19, even if the person has not been tested for the virus.
A similar pattern is evident with this measure, with the first decline in weekly deaths since the beginning of October recorded last week.
It appears that daily hospital admissions have peaked at a much lower rate than they did in the spring, so why is that when the number of new cases has undoubtedly surged in the past two months?
One reason could be that fewer over-65s are being infected now than they were in the early stages of Scotland's pandemic in the spring.
Younger age groups are much less likely to end up in hospital, or die, from Covid-19.
This next chart shows that there were more infections among younger age groups at the start of Scotland's second wave, with that steep rise in mid-September driven by outbreaks in student accommodation.
Infection rates among older age groups began to rise in September and October, but now appear to have stabilised.
So has Scotland's second wave peaked?
The decline in both death rates and hospital admissions is encouraging, but there are significant risks ahead.
Local outbreaks are still occurring and health officials will be very aware that the relaxation of restrictions over Christmas will likely cause an increase in infections.
The Scottish government and NHS Scotland will be working to drive cases as low as possible before Christmas to avoid that increase turning into a new surge."
"No longer should our survival be an afterthought. If we are to withstand the climate crisis, every decision should begin with the question of what the planet can endure. This means that any discussion about new infrastructure should begin with ecological constraints. The figures are stark. A paper published in Nature last year showed that existing energy infrastructure, if it is allowed to run to the end of its natural life, will produce around 660 gigatonnes of CO2. Yet, to stand a reasonable chance of preventing more than 1.5°C of global heating, we can afford to release, in total, no more than 580 gigatonnes. In other words, far from building new fossil power plants, the survival of a habitable planet means retiring the damaging projects that have already been built. Electricity plants burning coal and gas and oil will not secure our prosperity. They will destroy it. But everywhere special interests dominate. Construction projects are driven, above all, by the lobbying of the construction industry, consultancies and financiers. Gigantic and destructive schemes, such as the Oxford-Cambridge Expressway, are invented by lobbyists for the purpose of generating contracts. Political support is drummed up, and the project achieves its own momentum; then, belatedly, a feeble attempt is made to demonstrate that it can somehow become compatible with environmental promises. This is what destroys civilisations: a mismatch between the greed of economic elites and the needs of society. But last week something momentous happened. The decision to build a scheme with vast financial backing and terrible environmental impacts was struck down by the court of appeal. The judges decided that government policy, on which planning permission for a third runway at Heathrow was based, had failed to take account of the UK’s climate commitments, and was therefore unlawful. This is – or should be – the end of business as usual. The Heathrow decision stands as a massive and crucial precedent. Now we must use it to insist that governments everywhere put our survival first, and the demands of corporate lobbyists last. To this end, with the Good Law Project and Dale Vince, the founder of Ecotricity, I’m pursuing a similar claim. In this case, we are challenging the UK government’s policy for approving new energy projects. On Tuesday, we delivered a “letter before action” to the Treasury solicitor. We’ve given the government 21 days to accept our case and change its policy to reflect the climate commitments agreed by parliament. If it fails to do so, we shall issue proceedings in the high court to have the policy declared unlawful. We’ll need money, so we’ve launched a crowdfunding appeal to finance the action. It’s hard to see how the government could resist our case. The Heathrow judgment hung on the government’s national policy statement on airports. This, the judges found, had not been updated to take account of the Paris climate agreement. New fossil fuel plants, such as the gas burners at Drax in Yorkshire the government approved last October, are enabled by something very similar: the national policy statements on energy infrastructure. These have not been updated since they were published in 2011. As a result, they take no account of the Paris agreement, of the government’s new climate target (net zero by 2050, as opposed to an 80% cut) or of parliament’s declaration of a climate emergency. The main policy statement says that the EU Emissions Trading System “forms the cornerstone of UK action to reduce greenhouse gas emissions from the power sector”. As we have left the EU, this obviously no longer holds. The planning act obliges the government to review its national policy statements when circumstances change. It has failed to do so. It is disregarding its own laws. Once a national policy statement has been published, there is little objectors can do to prevent damaging projects from going ahead, as the statements create a presumption in favour of new fossil fuel plants. In approving the Drax plant, Andrea Leadsom, the secretary of state for business and energy at the time, insisted that the policy statement came first, regardless of the climate impacts. Catastrophic decisions like this will continue to be made until the statements change. They are incompatible with either the government’s new climate commitments or a habitable planet. While we are challenging the government’s energy policies, another group – the Transport Action Network – is about to challenge its road-building schemes on the same basis. It points out that the national policy statement on road networks is also outdated and incompatible with the UK’s climate commitments. The policy statement, astonishingly, insists that “any increase in carbon emissions is not a reason to refuse development consent”, unless the increase is so great that the road would prevent the government from meeting its national targets. No single road project can be disqualified on these grounds. But the cumulative effect of new road-building ensures that the UK will inevitably bust its carbon targets. While carbon emissions are officially disregarded, minuscule time savings on travel are used to justify massive and damaging projects. Transport emissions have been rising for the past five years, partly because of road-building. The government tries to justify its schemes by claiming that cars will use less fossil fuel. But because they are becoming bigger and heavier, new cars sold in the UK now produce more carbon dioxide per kilometre than older models. The perverse and outdated national policy statement locks into place such damaging projects as the A303 works around Stonehenge, the A27 Arundel scheme, the Lower Thames crossing, the Port of Liverpool access road, the Silvertown tunnel in London and the Wensum link road in Norfolk. A government seeking to protect the lives of current and future generations would immediately strike down the policy that supports these projects, and replace it with one that emphasised walking, cycling and public transport. A third action has been launched by Chris Packham and the law firm Leigh Day, challenging HS2 on similar grounds. Its carbon emissions were not properly taken into account, and its environmental impacts were assessed before the government signed the Paris agreement. Already, the Heathrow decision is resonating around the world. Now we need to drive its implications home, by suing for survival. If we can oblige governments to resist the demands of corporate lobbyists and put life before profit, humanity might just stand a chance. • George Monbiot is a Guardian columnist"
"Ever had one too many at the zoo and thrown your beer at a tiger, or stripped off and attempted to jump into the penguin pool? I’d hope not, but these are just two examples of inappropriate behaviour by visitors at London Zoo’s controversial party nights.  We all know how alcohol causes people do silly things and temporarily lose control. In a zoo we are there to observe, learn and enjoy – not to interact with the animals and certainly not to bother them. At their best, zoos are a wonderful form of theatre; at their worst, a grotesque pantomime, featuring unwilling animal actors. Theatres and zoos survive in the days of cinema and wildlife documentaries because they provide a more personal experience.  In a world where people share their life through social media a visit to the zoo provides them with something that their friends can experience, without being the cloned experience of watching a wildlife documentary. Unfortunately, we have all heard a cell phone ringing in a theatre, thereby breaking the suspension of reality.  But should zoo visitors be passive observers?  If you follow the school of immersive zoo design, where the visitor is transported through enclosure design to the tropical rainforest of Africa, then a cell phone going off is going to ruin that experience. In my experience there is never a signal in such remote places. Talking during a performance has always been a big no-no in theatres. But in zoos, talking at normal volume is not a problem; however, I personally find it extremely uncomfortable when people start to shout at zoo enclosures.  And the animals, how do they feel?  Before answering this question we should look at why people shout at animals in zoos.   Many zoo animals are nocturnal and so we are visiting them at their least active time of day. And a number of popular species such as lions are naturally very inactive. For every gazelle hunt worthy of a BBC Nature special there are many, many hours of lazing around. Of course this isn’t what zoo visitors have paid their money for – they want to see active animals. In some of my recent research we showed that a number of zoo species become more active when the public is noisy. Chimpanzees start to roam around their enclosure, for example. And there may be positive feedback at work here; that is, loud visitors provoke higher levels of animal activity, which in turn causes the humans to shout or flash their cameras, which disturbs the animals further, and so on. Undoubtedly zoo visitors affect animal behaviour and well-being.  Studies show, in general, that non-agitated and quiet groups of visitors do not appear to stress animals, whereas large, agitated crowds cause all kinds of unwanted changes. Animals can become more aggressive, less sociable or more vigilant (an indication they feel less secure). Our research has shown that zoo visitors increase noise levels on average by more than 10 decibels; that is, they double sound noise levels (decibels are measured on a logarithmic scale).  If you have even been in a noisy bar you know one consequence of people talking loudly is you need to talk louder yourself to be heard (what is known as the Lombard Effect).   Many species naturally depend on their hearing to know when to feed their young or when a predator is nearby. Thus, being deprived of this sense is stressful. So what can be done about loud zoo visitors? Simply asking people with signs to be quiet is not usually effective. When I worked at Edinburgh Zoo in the 1990s, we deliberately made the visitor areas dark.  People in spaces with reduced lighting generally talk more quietly and are less agitated – think of libraries.  But zoos should not be libraries – but they should be places of learning.  I think a good zoo should encourage people to discuss what they are seeing. Some zoos have human attendants at the enclosures to control public behaviour but this seems rather draconian.  There must be better solution and of course environmental education can help to a certain extent. But the education message can go too far: many zoo visitors now feel that must say they visit a zoo for conservation purposes. Ironically, the elephant in the room at the zoo is to say you are there for a good day out.  Despite the fact that people enjoying themselves will learn more about conservation. We can of course provide zoo animals with enclosures that are acoustically isolated from the public.  But I feel this would deprive the public of the great sensory experiences of zoos: their sounds and smells.  The deafening duets of gibbons and the foul smell of maned wolf faeces have stayed in my memory from my first zoo visits more than 40 years ago. Good zoos take the visitor experience very seriously and the welfare of their animals even more so. The challenge is to make zoos a pleasant experience for both animals and people – a happy medium that lets us enjoy and appreciate a zoo’s inhabitants, without causing adverse stress levels."
"Republican lawmakers under pressure to address the climate crisis are trying to move beyond denying the problem and start proposing solutions. But they still refuse to commit to what scientists say is necessary if the US is to rapidly cut back on burning fossil fuels.  A recent package of legislation proposed by House Republican leader Kevin McCarthy would encourage capturing climate pollutants from power plants but use them to drill for oil. It would also lead to the planting of a trillion trees to absorb carbon emissions but also ramp up logging, an idea Donald Trump has endorsed. “It’s greenwashing,” said Randi Spivak, public lands director at the Center for Biological Diversity, which organized a letter of opposition on the tree bill from dozens of environment groups to the House natural resources committee. “The science is very clear,” Spivak said. “We need to slash our carbon pollution by 50% over the next 10 years if we want to avert the worst impacts of global warming and keep global warming to a 1.5C increase.” The Republican party has seen a pendulum swing. In 2008, its election platform emphasized the importance of cutting emissions. By 2016, Donald Trump ran on a campaign of climate denial. Now at least some prominent Republicans are breaking from the president and veering away from questioning the science and toward efforts that do not directly attack the coal, oil and gas industries. Facing pushback from the far right, on the other side of the aisle they are criticized by some Democrats who believe they are not proposing legislation in good faith. Alex Flint, executive director of the Alliance for Market Solutions – a right-of-center organization that advocates reducing carbon pollution while growing the US economy – said the House bills were “directionally correct” but “need to grow to address the scale of our climate problem”. Flint’s group backs revenue-neutral carbon tax and deregulation. “I give [House Republicans] a great deal of credit for acknowledging the problem and stepping forward with proposals and recognizing that the politics of this has changed,” he said. “But also acknowledging that they are in the early stages of really substantive climate proposals.” Bruce Westerman, an Arkansas Republican who introduced the trees bill, insisted his proposal would curb emissions even as it promotes logging. When trees are cut down, they stop absorbing carbon. Westerman, a forester who worked for an engineering consulting firm in the timber, pulp and paper business, said forest managers would just plant them again. Products made from the trees would pay for continual planting, he said, and the wood would be used for sustainable buildings with lower emissions footprints. A scientist called to testify by Democrats, Yale ecology professor Carla Staver, strongly disagreed with Westerman’s proposals. Forest management is an important way to fight climate change but it is not enough and it must be done properly, experts agree. A bill from Democrats would aim to conserve forests without increasing logging and by banning oil and gas drilling on public lands if climate emissions exceed targets. Westerman told the Guardian Democrats should be working with Republicans who want to address climate change. “Even someone who’s not a forester should be able to recognize an olive branch when they see one,” he said. Even as Westerman defended his bill, the top Republican on his committee seemed to dismiss the overall effort. At the beginning of the meeting, to which he was late, Rob Bishop of Utah suggested the elevators in House office buildings would be more timely under the rule of Benito Mussolini than they are with Democrats in charge. Democratic climate proposals, he said, offered not a “silver bullet” to fix the problem but “another bullet that is going to be used to shoot ourselves in the foot”. He then showed a graph demonstrating how US heat-trapping emissions have declined over time. While the Republican bills are far from aggressive, the right of the party is pushing back. The conservative Club for Growth Pac painted the package as “stifling liberal environmental taxes, regulations, and subsidies” and vowed to withhold support from any backers. At the annual CPAC gathering near Washington, the climate change denial group the Heartland Institute presented a German teen activist who calls herself a “climate sceptic” as a foil to Greta Thunberg. In Oregon, Republican state senators fled the state capitol in order to derail a climate change bill. Despite that backdrop, the top Democrat on the House natural resources committee welcomed Westerman’s proposal. Raúl Grijalva said he hoped for a “new chapter”, focusing on solutions not denial. He said: “For too long my friends on the other side of aisle denied that this was even a real issue. They would reject or even mock the overwhelming scientific consensus that climate is warming humans are responsible and urgent action needs to be taken.”"
"

Africa defies conventional logic: grinding poverty amidst immense mineral riches. Africa’s economic growth of 5 percent in 2004, though more respectable than in previous years, was less than the 7 percent needed to achieve the United Nations Millennium Development Goals of reducing poverty and child mortality and improving education. At that rate, the United Nations Development Program has warned that the achievement of the millennium development goals may take 150 years.



The Commission on Africa, which was established by British prime minister Tony Blair, seeks to raise $50 billion a year on the international capital markets and use it to reverse Africa’s economic atrophy. Blair made aid to Africa the centerpiece of the British presidency of the G-8 meeting in Gleneagles, Scotland, in July 2005. President George Bush has tripled aid to Africa to $4.3 billion since he took office in 2001. In addition, the Bush administration’s Millennium Challenge Account (MCA) seeks to boost grants to poor African countries. France proposes an international tax on financial transactions or items such as plane tickets. Japan favors a $200 million fund to nurture private‐​sector companies in Africa to improve the continent’s investment climate and credit rating. The UN is calling on rich countries to increase their foreign aid to 0.7 percent of GDP by 2015. The UN argues that lack of resources is a major impediment to economic growth and that additional funds will be well spent. But will any of those plans help Africa?



Most Africans are skeptical. They have heard those righteous calls before. Every decade or so, a throng of Western donors, African governments, and international organizations gathers to announce grand initiatives to pull the world’s poorest continent out of its economic miasma. Congratulatory pats on the back are exchanged. Delegates return home and then nothing much is heard after that. Back in 1985, the United Nations held a Special Session on Africa to boost aid to Africa. In March 1996, the United Nations launched a $25 billion Special Initiative for Africa. They all fizzled. Why should Africans place any faith in the current initiatives to reverse Africa’s economic atrophy? 



**The Failure of Aid**



Helping Africa is a noble cause, but the campaign has become a theater of the absurd – the blind leading the clueless. The record of Western aid to Africa is one of abysmal failure. More than $500 billion in foreign aid – the equivalent of four Marshall Aid Plans – was pumped into Africa between 1960 and 1997. Instead of increasing development, aid has created dependence. The budgets of Ghana and Uganda, for example, are more than 50 percent aid dependent. Said President Aboulaye Wade of Senegal: “I’ve never seen a country develop itself through aid or credit. Countries that have developed — in Europe, America, Japan, Asian countries like Taiwan, Korea and Singapore — have all believed in free markets. There is no mystery there. Africa took the wrong road after independence. 1



The more aid poured into Africa, the lower its standard of living. Per capita GDP of Africans living south of the Sahara declined at an average annual rate of 0.59 percent between 1975 and 2000. Over that period, per capita GDP adjusted for purchasing power parity declined from $1,770 in constant 1995 international dollars to $1,479. The evidence that foreign aid underwrites misguided policies and feeds corrupt and bloated state bureaucracies is overwhelming. 



Tanzania’s ill‐​conceived socialist experiment, Ujaama, for example, received much Western support. Western aid donors, particularly in Scandinavia, gave their enthusiastic backing to Ujaama, pouring an estimated $10 billion into Tanzania over a period of 20 years. Yet, between 1973 and 1988, Tanzania’s economy contracted at an average rate of 0.5 percent a year, and average personal consumption declined by 43 percent. Today, Tanzania’s largely agricultural economy remains devastated. Some 36 million Tanzanians are attempting to live on an average annual per capita income of $290—among the lowest in the world. Other African countries that received much aid between 1960 and 1995 – Somalia, Liberia, and Zaire – slid into virtual anarchy. 



Much of the aid received was simply looted. Speaking at the New Partnership for African Development (NEPAD) meeting in Abuja, Nigeria, in December 2003, the former British secretary of state for international development, Lynda Chalker, noted that 40 percent of the wealth created in Africa is invested outside the continent. “If you can get your kith and kin to bring the funds back and have it invested in infrastructure, the economies of African countries would be much better than what they are today,” she said. 2 The chairman of the session and president of the African Business Round Table, Alhaji Bamanga Tukur, agreed: “It is really difficult to ask foreign investors to come and invest on our continent when our own people are not investing here. There is no better factor to convince foreign investors than for them to see that our own people, both those based at home and those in the Diaspora, invest in Africa. 3



Indeed, the amount of capital leaving Africa is staggering. Percy Mistry of the Oxford International Group pointed out that the external stock of capital held by Africans overseas could be as much as $700 billion to $800 billion. 4 The World Bank estimated that “nearly 40 percent of Africa’s aggregate wealth has fled to foreign bank accounts. 5 Considering the missing billions in export earnings from oil, gas, diamonds, and other minerals that are not openly accounted for, it is dubious that Africa suffers from a poverty trap, as Jeffrey Sachs of Columbia University argues. 



Africa’s case for more aid and debt relief has not been helped by President Olusegun Obasanjo of Nigeria, which is arguably the most mismanaged economy in Africa. As he was pleading for more aid at the World Economic Forum in Davos, Switzerland, in February 2005, four of Obasanjo’s state governors were being probed by London police for money laundering. The most galling was the case of Plateau State Governor, Chief Joshua Dariye, who was accused of diverting some $90 million into his private bank accounts. Dariye was dragged before the Federal High Court in Kaduna by Nigeria’s Economic and Financial Crimes Commission. Incredibly, Justice Abdullahi Liman ruled that although Dariye was a principal suspect in the case, Section 308 of the Nigerian Constitution protected sitting governors from criminal prosecution.



In February 2005, Nigeria’s police chief, Inspector General Tafa Balogun, was forced into early retirement after investigators probing money‐​laundering allegations found $52 million hidden in Balogun’s network of 15 bank accounts. At the time of his early retirement, Balogun had been on the job for only two years. Even Nigeria’s senate is riddled with scams and inflated contracts, with proceeds pocketed by sitting senators. According to the president of the Institute of Chartered Accountants of Nigeria, Chief Jaiye K. Randle, individual Nigerians are currently lodging $170 billion in foreign banks – far more than Nigeria’s foreign debt of $35 billion.



In July 2005, Nigeria’s Economic and Financial Crimes Commission revealed that a succession of military dictators stole or squandered $500 billion – equivalent to all Western aid to Africa over the past four decades. 6 Even when the loot is recovered, it is quickly re‐​looted. The Nigerian state has recovered $983 million of the loot of the former president, General Sani Abacha, and his henchmen. But the Senate Public Accounts Committee found only $12 million of the recovered loot in the Central Bank of Nigeria. 7



 **Aid for Reform**



Foreign aid given to support reform in Africa has not been successful either. According to the United Nations Conference on Trade and Development: “Despite many years of policy reform, barely any country in the region has successfully completed its adjustment program with a return to sustained growth. Indeed, the path from adjustment to improved performance is, at best, a rough one and, at worst, disappointing dead‐​end. Of the 15 countries identified as ‘core adjusters’ by the World Bank in 1993, only three ( Lesotho, Nigeria and Uganda) are now classified by the IMF as ‘strong performers.’ 8



The World Bank evaluated the performance of 29 African countries to which it had provided more than $20 billion in “structural adjustment” loans between 1981 and 1991. The bank’s report, _Adjustment Lending in Africa_ , concluded that only six African countries had performed well: The Gambia, Burkina Faso, Ghana, Nigeria, Tanzania, and Zimbabwe. That gives a failure rate in excess of 80 percent. More distressing, the World Bank concluded that “no African country has achieved a sound macro‐​economic policy stance.” Since then, the World Bank’s list of “success stories” has shrunk. The Gambia, Nigeria, and Zimbabwe are no longer on that list.



In 1998, four new “success stories” were added ( Guinea, Lesotho, Eritrea, and Uganda). However, the senseless Ethiopian‐​Eritrean war and the eruption of civil wars in western and northern Uganda have knocked those two countries off the new “success stories” list. Uganda depends on foreign aid for 58 percent of its budget. There are growing concerns about its democracy, defense spending, and rampant corruption. Yet, in December 1999, Uganda’s aid donors announced the country’s biggest‐​ever loan of $2.2 billion – with no visible strings attached. As _The Economist_ pointed out, “Cynics might say that Uganda can hold the world to ransom because the World Bank, the IMF and the other foreign donors cannot afford to let their star pupil go under. 9



 **The Western Aid Lobby Is Partly to Blame**



Africans themselves have realized that Western aid has not been effective. David Karanja, a former Kenyan member of parliament, for example, said: “Foreign aid has done more harm to Africa than we care to admit. It has led to a situation where Africa has failed to set its own pace and direction of development free of external interference. Today, Africa’s development plans are drawn thousands of miles away in the corridors of the IMF and World Bank. What is sad is that the IMF and World Bank experts who draw these development plans are people completely out of touch with the local African reality. 10



The donors themselves contributed much to the failure of Western aid to Africa. Foreign loans and aid programs in Africa were badly monitored and often stolen by corrupt bureaucrats. “We failed to keep a real hands‐​on posture with aid,” said Edward P. Brynn, former U.S. ambassador to Ghana. “We allowed a small, clever class that inherited power from the colonial masters to take us to the cleaners. It will take a whole lot of time and money to turn Africa around. 11



More maddening, the donor agencies knew or should have known all along about the motivations and activities of corrupt African leaders. They knew or should have known that billions of aid dollars were being spirited into Swiss banks by greedy African kleptocrats. “Every franc we give impoverished Africa comes back to France or is smuggled into Switzerland and even Japan” wrote the Paris daily, _Le Monde_ in March 1990. 12 Even famine relief assistance to Africa was not spared. Dr. Rony Brauman, head of Médecins sans Frontières, lamented in the 1980s: “We have been duped.… Western governments and humanitarian groups unwittingly fuelled – and are continuing to fuel – an operation that will be described in hindsight in a few years’ time as one of the greatest slaughters of our time. 13



Patricia Adams of Probe International, a Toronto‐​based environmental group, charged that, “in most cases, Western governments knew that substantial portions of their loans – up to 30 percent, says the World Bank – went directly into the pockets of corrupt officials, for their personal use. 14



Yet, the World Bank considered those same African governments “partners in development.” When the United Nations launched a $25 billion Special Initiative for Africa in March 1996, the bank’s president James Wolfensohn said that he was “pleased that the Special Initiative is designed to be supportive of and a ‘true partnership’ with African leadership.’ 15 In fact, World Bank loans have often bailed out tyrannical regimes in the past. After shattering Ghana’s economy, the Marxist government of Jerry Rawlings found that the Soviet and Cuban governments could no longer provide it with assistance. Rawlings made overtures to the West, which responded with alacrity, eager to win one more “convert.” The regime signed a “structural adjustment” agreement with the World Bank in 1983. Slight improvements in the economy were hailed, and Ghana was declared a “success story” and a “role model for Africa.” Twelve years later and after the infusion of more than $4 billion in loans, the World Bank admitted that declaring Ghana a “success story” was a mistake and not in the country’s own best interest. 16



In recent years, loans provided by the World Bank for various poverty‐​reduction programs in Ghana have continued to be embezzled by the political elites. According to Goosie Tanoh, leader of the newly formed National Reform Party, “It is an open secret that so many grants from Japan, Canada, USA and Britain had been given to party functionaries who have misapplied it. 17



In Kenya, Nairobi’s deputy mayor, Abdi Ogle, demanded the resignation of the World Bank’s country director for Kenya, Harold Wackman, a Canadian, accusing him of turning a blind eye to embezzlement of an emergency loan of $77.5 million in July 1998 to repair infrastructure damaged by heavy rains. “Not a cent of this money has come to the City Council because it has disappeared into private pockets within the Ministry of Local Government,” fumed Ogle. 18



Kenyan constitutional reform, which was supposed to have addressed the problem of pervasive corruption in that country, has stalled under the watchful eyes of the ruling elite. Widespread government corruption has caused international donors to withhold money allocated to fight AIDS. The disease has killed about 1.5 million in Kenya since 1984. The government estimates that about 1.4 million Kenyans are infected with HIV/AIDS. Yet Kenya’s health ministry is riddled with graft. A recent audit revealed the existence of “ghost workers” whose salaries worth $6.5 million per year are collected by living workers. In June 2004, the same health ministry paid $1.8 million for a radiography machine for the Kenyatta National Hospital that was never delivered. Over the course of the past 12 months, Kenya has been rocked by corruption scandals in various ministries, but little action has been taken. The ministers who were involved were sacked, but not prosecuted to recover the loot.



During his 24 years in power, Daniel Arap Moi’s government embezzled and stole an estimated $3 billion to $4 billion. The country’s central bank was looted. The money was stolen by making fictitious payments on foreign debt; kickbacks were collected on all public contracts, and when that didn’t supply enough cash, politicians awarded themselves fake contracts. A report by Kenya’s recently created Anti‐​Corruption Commission estimates that up to $3 billion of the missing money is still stashed overseas. After he left office, Moi and his family were among the wealthiest people in Kenya, with seven big homes and connections to at least 30 major business firms. But he also left behind an economy crippled with foreign debt, collapsed infrastructure, unemployment hovering at 70 percent, and nearly two‐​thirds of the population living under the poverty line. 



**The Need for Domestic Reform**



Foreign funds can help only those African countries that undertake political, economic, and institutional reform, but the commitment to reform has been woefully lacking. The democratization process in Africa has stalled through political chicanery and strong‐​arm tactics. Only 16 of the 54 African countries are democratic, and political tyranny remains the order of the day. Often, those countries that are democratic remain deeply corrupt. Intellectual freedom is stuck in the Stalinist era: only eight African countries have free and independent media. The record on economic reform is abysmal. Only Botswana, Mauritius, Namibia, and South Africa can be described as “success stories.” 



At the July 2004 African Union Summit in Abuja, Nigeria, frustrated UN Secretary‐​General Kofi Annan told African leaders of their lack of progress on meeting the UN’s Millennium Development Goals that they agreed to in 2000. Four years earlier, he was less restrained, lashing out at African leaders and blaming them for most of the continent’s problems. 19



African children echo the same sentiments. At the United Nations Children’s Summit held in May 2002 in New York, youngsters from Africa ripped into their leaders for failing to improve their education and health. “You get loans that will be paid in 20 to 30 years and we have nothing to pay them with, because when you get the money, you embezzle it, you eat it,” said 12‐​year‐​old Joseph Tamale from Uganda. 20



Tony Blair and Jeffrey Sachs should listen to the voices of average Africans, who have not benefited from aid in the past and are unlikely to benefit in the future. Without domestic reforms, African politicians will line their pockets, but Africa will remain desperately poor.



 **Notes:**



1 Norimitsu Onishi, “Senegalese Loner Works to Build Africa, His Way,” _New York Times_ , April 10, 2002, p. A3.



2 Kunle Aderinokun, “Africa at Large: 40% of Continent’s Wealth Invested Outside,” _This Day_ , Nigeria, December 4, 2003, cited in George Ayittey, _Africa Unchained_ ( New York: Palgrave Macmillan, 2005), p. 324. We are indebted to George Ayittey, from whose research and writing we have borrowed in preparing parts of this text.



3 Aderinokun.



4 Percy Mistry, “Aiding Africa,” letter to _The Economist_ , July 14, 2005.



5 Karen DeYoung, “Giving Less: The Decline in Foreign Aid,” _Washington Post_ , November 25, 1999, p. A1.



6 Peter Goodspeed, “Corruption’s Take: $148B,” _National Post_ ( Canada), July 4, 2005, p. A1.



7 Ayittey, _Africa Unchained_ , p. 439.



8 George Ayittey, “Corruption, the African Development Bank and Africa’s Development,” Testimony before the Senate Foreign Relations Committee, September 28, 2004, p. 7.



9 Cited in Ayittey, _Africa Unchained_ , p. 161.



10 Cited in George Ayittey, _Africa in Chaos_ (New York: St. Martin’s, 1998), p. 275.



11 Blaine Harden, “The US Keeps Looking for a Few Good Men in Africa,” _New York Times_ , August 27, 2000, p. 1.



12 Cited in Jonathan C. Randal, “French‐​Speaking Africa Hit by Popular Discontent,” _Washington Post_ , March 26, 1990, p. A17.



13 Rony Brauman, “Famine Aid: Were We Duped?” _Reader’s Digest,_ October 1986. 



14 Patricia Adams, “The Debts of Corruption,” _Financial Post_ ( Canada) May 10, 1999.



15 _African Recovery_ , May 1996, p. 13.



16 World Bank, _Ghana Country Assistance Review: A Study in Development Effectiveness_ (Washington: World Bank, January 1996). 



17 _Ghanaian Chronicle_ , August 14, 2000.



18 _Daily Graphic_ , January 9, 1999, p. 5.



19 _Daily Graphic_ , July 12, 2000; p. 1.



20 “African Children Accuse Leaders,” _BBC News_ , May 10, 2002. 
"
"**Counter-terrorism police say Covid-19 could be behind a fall in referrals from people worried about friends or family members becoming radicalised.**
They are concerned that people vulnerable to radicalisation are spending more time online at home.
But due to people not mixing with friends, at school or in workplaces as regularly, reports are not being made.
Det Ch Insp Alistair Stenner said the virus was behind a 64% drop in referrals in the South West of England.
""We're really worried about the impact of Covid and young and vulnerable people spending more time online,"" said Det Ch Insp Stenner, of Counter Terrorism Policing South West.
""It's difficult to know who they are talking to, what they are looking at and what impact this is having on them and their outlook on the world.
""From 1 January to the middle of November there was a 64% reduction in referrals in comparison to the same period last year and whilst there may be a number of reasons for that, I actually believe the main one is Covid,"" he said.
He said that social distancing measures meant people were not mixing in society the way they would before the pandemic, making changes in behaviour or extremist views harder to spot.
""People aren't picking up the phone and reporting things to us. We think that's because there is less mixing between people, individuals are spending less time with friends, at school and in the workplace, so they become more insular.""
He cited the case of Bristol man Andrew Ibrahim, who was jailed for plotting to blow up a city shopping centre, as an example of how important it was to report concerns early.
Brad Evans, professor of political violence and aesthetics at the University of Bath, said radicalisation was a ""very complex"" problem but poverty and education played large roles.
""We need to ask why these children with anger inside them feel they an only direct that anger through violence. There are also wider issues, particularly the use of technology in society and how they engage with technology in a world that looks increasingly divisive.
""We could have a generation of young children growing up who are being radicalised and believe that violence is the way to solve problems and that's deeply troubling.""
A new website and advice line, ACT Early has been launched to encourage people to report concerns."
"**The UK oil and gas sector is in ""economic turmoil"" amid the coronavirus pandemic with about a fifth of firms expecting more redundancies in 2021, according to a new report.**
Aberdeen and Grampian Chamber of Commerce (AGCC) said reduced activity levels and project cancellations had seen business optimism ""slashed"".
Confidence is now said to be as low as during the industry downturn in 2015.
The findings came in the 32nd AGCC Oil and Gas Survey.
It covers the six months to October.
The survey was carried out in partnership with the Fraser of Allander Institute and KPMG UK.
It asked firms about the initial Covid impact, how they expected activity to recover, and further issues such as energy transition and Brexit.
The survey found that only 13% of contractors were working at, or above, optimum levels in the UK Continental Shelf (UKCS) compared with 47% a year ago - with 82% predicting a decrease in revenue in 2020.
A total of 23% of contractors reported cancelling projects as a result of the coronavirus outbreak, with a further 34% putting activities on hold.
More than three quarters of businesses - 78% - were less confident about activities going forward, while only 1% were more confident.
AGCC said that while businesses typically reported higher levels of optimism about their international activities, the latest results marked the lowest recorded levels of confidence in global markets in the history of the survey.
About half of contractors surveyed reported a decline in their workforce - 22% of which said reductions equated to more than 10% of their workforce - and about a fifth of surveyed firms said they expected to make further reductions in 2021.
A total of 83% of contractors furloughed employees.
AGCC research and policy manager Shane Taylor said: ""Over the course of this year we have seen drastic and unpredictable disruption to business globally due to Covid-19, combined with the collapse in oil and gas prices.
""Although government support has had clear value in supporting firms and jobs through this challenging period of suppressed demand, the only sustainable way to give businesses and workers clarity is a clear route to heightened levels of activity in the future.""
Martin Findlay, senior partner at KPMG in Aberdeen, added: ""From the significant oil price decline, which started earlier in the year, to a global pandemic, and localised lockdown in Aberdeen, the oil and gas industry has, once again, endured profound challenge and uncertainty.
""However, there is room for some optimism. The industry, unlike so many others, is incredibly resilient and frequently deals with instability and challenge.""
Industry body Oil and Gas UK (OGUK) said the finding were further confirmation of the ""stark conditions"" faced bythe sector.
Chief executive Deirdre Michie said: ""We remain particularly concerned about the health of our world-class supply chain.
""OGUK continues to work with industry to see what we can to together to safely increase activity and protect jobs.""
The survey involved 100 firms employing more than 22,000 people across the UK and 400,000 globally."
"**Personal protective equipment (PPE) stockpiles in England were inadequate for the Covid pandemic and price rises earlier this year cost taxpayers about Â£10bn, the spending watchdog has said.**
The National Audit Office said there had been a particular shortage of gloves and aprons.
The government said the NAO's report recognised that NHS providers had been able to get what they needed in time.
Almost Â£12.5bn was spent on 32bn items of PPE between February and July 2020.
During the same period in 2019, 1.3bn items were bought at a cost of Â£28.9m.
Each item had been ""substantially"" more expensive in 2020, because of very high global demand, the NAO said, from almost triple the cost for respirator masks to more than 14 times as much for body bags.
Had the government been able to pay 2019 prices, it would have spent Â£2.5bn on PPE in 2020.
In reality, it had spent Â£12.5bn, including hundreds of millions on ""unsuitable"" items that could not be used.
Some had ""passed its expiry date or did not meet current safety standards"", the watchdog said, with ""insufficient checks"" meaning Public Health England had had to recall eye protectors that did not meet standards.
In Parliament, on Wednesday, Labour leader Sir Keir Starmer accused Prime Minister Boris Johnson of ""wasting"" taxpayer's money on equipment that ""can't be used"".
But Mr Johnson replied ""99.5%"" of the 32 billion items of PPE bought between February and July 2020 ""conform entirely to our clinical needs"".
Earlier, the Department of Health and Social Care (DHSC) said ""only 0.49% of all the purchased PPE tested to date"" had not been fit for purpose.
NAO head Gareth Davies said: ""As PPE stockpiles were inadequate for the pandemic, government needed to take urgent action to boost supplies.
""Once it recognised the gravity of the situation... the price of PPE increased dramatically, and that alone has cost the taxpayer around Â£10bn.""
Before the Covid-19 pandemic, there were two emergency stockpiles of PPE:
But the NAO said: ""The EU exit stockpile held few items of PPE other than a large number of gloves.""
Meanwhile, the flu stockpile, as well as having shortages of some key items, did not include any gowns or visors despite the fact they had been ""recommended for inclusion in June 2019 by the New and Emerging Respiratory Virus Threats Advisory Group (Nervtag)"".
Public Health England told the NAO it had been analysing the market to work out which gowns to buy, when the pandemic had begun, which it said was the ""normal approach"" to find a lower price.
In mid-March, the government had still believed its two stockpiles would provide ""most of the PPE needed to manage a Covid-19 pandemic"" and so focused on distributing this PPE rather than buying more, the NAO reported.
The situation had become ""precarious"" in April and May, with stocks threatening to run out.
At one point, only 3% of the required number of gowns had been available.
But the nation did not at any point run out of any type of PPE.
The scramble for PPE in the early stages of the pandemic was not confined to the UK. Every healthcare system was desperate to secure protective equipment and prices soared. But the National Audit Office lays bare how the UK was at the back of the queue, having failed to spot the warning signs and how woefully inadequate the stockpiles were.
A failure to anticipate what might be needed for anything other than a flu pandemic in essence cost the taxpayer Â£10bn - the extra money needed to secure supplies such as gowns and visors during the Covid crisis.
The report highlights poor distribution of PPE with many staff saying they did not have the right equipment. The NAO notes starkly that health and care employers have reported more than 100 deaths among staff because of exposure to coronavirus.
An official inquiry, when it happens, will look hard at many aspects of the UK's preparedness and handling of the crisis and the PPE issue will be central. With a series of reports, the NAO has now done important groundwork but there is much still to find out.
A DHSC official said: ""As the NAO report recognises, during this unprecedented pandemic all the NHS providers audited 'were always able to get what they needed in time' thanks to the Herculean effort of government, NHS, armed forces, civil servants and industry"".
But the NAO heard feedback from care workers, doctors and nurses that showed ""significant numbers of them considered that they were not adequately protected during the height of the first wave of the pandemic"".
Employers have reported 126 deaths among health and care workers linked to exposure at work.
And there were concerns about training and whether the equipment was appropriately fitted, particularly from women and people belonging to ethnic minorities.
In a Royal College of Nursing survey of 5,000 NHS staff, 49% of respondents belonging to ethnic minorities said they had been adequately ""fit tested"" for a respirator, compared with 74% of white nurses.
The DHSC said it was ""listening to the reported practical difficulties with the use of some PPE experienced by women and black, Asian and minority ethnic (BAME) individuals, among others, and... taking action to make sure user needs are adequately addressed in future provisions"".
In a separate report, the Public Accounts Committee, a parliamentary body which works closely with the NAO, said it was ""concerned that the department had no plan before the pandemic for how it might increase critical care equipment in the event of an emergency"".
""This lack of preparedness was exacerbated by the fact that it did not know how many ventilators were available to the NHS to begin with,"" the PAC said.
But it added the government had managed to buy an additional 26,000 ventilators for use in the NHS, a ""significant achievement""."
"**A main road through Cardiff city centre which has been used as a dining area since July is to reopen on Sunday.**
Castle Street will reopen to buses, taxis and emergency vehicles, although a pop-up cycleway which has been in place during the pandemic will remain.
The pavement opposite the castle will be widened to allow extra outdoor seating for cafes and more room for pedestrians to socially distance.
The council said there had been strong arguments for and against the closure.
Private cars will still have to use alternative routes.
A public consultation is being held on the street's future status.
The council said the temporary partial reopening would help buses and taxis cross from east to west during ongoing road works in the city centre.
Under the new arrangement, Castle Street will have two lanes for traffic, one in each direction, and the cycle lane next to the castle.
The pavement on the adjoining Westgate Street has also been widened to give extra outdoor space for bars and restaurants.
Caro Wild, cabinet member for strategic planning and transport, said: ""The council recognises that the closure of Castle Street has divided opinions, with strong arguments being made in favour and against the changes that have been implemented in recent months.
""Alongside a detailed modelling exercise on future traffic flows, we will undertake a comprehensive consultation exercise, involving city centre businesses, local residents, and citizens across Cardiff, to help determine the final plan for the street.""
The council is also monitoring congestion and air quality across the city centre and will undertake detailed modelling on long-term plans to help decide if extra mitigation measures are needed in neighbouring areas of the city."
"The former weather presenter Francis Wilson has said it is now more important than ever for TV forecasters to be serious experts “in a time of floods and fires” caused by global heating. Wilson, who presented forecasts on BBC Breakfast between 1981 until 1992 and at Sky News from 1993 until 2010, stressed the need to keep audiences engaged and to report accurately as extreme weather becomes more common. “A report has to be engaging,” he told Radio Times. “A stuffy, pompous forecast, laden with jargon, is a switch-off and that’s self defeating. “We want people to be engaged with the weather … every weather presenter has an even greater obligation to get the tone right.” He warned that because of the climate crisis, the UK would now experience more severe weather events such as the floods that have caused catastrophic disruption in the north of England and Wales. In recent weeks, the UK has been hit by Storms Ciara, Dennis and Jorge, which have inflicted destruction on homes, businesses and roads. Councils are facing large bills to repair the damage and on Monday it was reported that the UK had experienced its wettest February on record. Wilson referred to the infamous BBC lunchtime report on 15 October 1987 where weatherman Michael Fish “cheerfully dismissed” a viewer’s concerns that “a hurricane was on the way”. Later that evening, what is now known as the Great Storm of 1987 happened. Eighteen people were killed and around 15m trees were blown over. “That [Great Storm of 1987] was a once-in-200-years storm but now, thanks to global warming, we have a more energetic atmosphere,” Wilson said. “We are already seeing this new reality, here in the UK. It’s common sense that lightly dismissing a viewer’s concerns is wildly inappropriate when large areas of Yorkshire, the Midlands and Wales are under water, but we also need to be confident that the presenter knows what they are talking about. It’s a tricky balance.” Wilson also said there was a “moral obligation” on TV forecasters to tell viewers that the more extreme climate events they are reporting on are caused by global heating. “We need to tell people to stop warming the atmosphere, to stop adding carbon dioxide to the atmosphere,” he said. “That way, viewers won’t lose sight of the fact that they can actually do something about it. The BBC used the Met Office as its main source for forecasts for 94 years until 2017 when it switched to MeteoGroup. Marco Petagna, a senior operational meteorologist at the Met Office, said it continued to provide “high resolution data and weather warnings” for the BBC. Petagna added that his organisation currently provided data and presenter briefings for Sky and ITV. The weather bulletins for Channel 5 are presented by Met Office presenters during the week and they provide voiceover scripts and graphics at the weekend. MeteoGroup also provides weather data for Channel 4. Wilson also referenced the changing intensity of weather events elsewhere around the world. “Across the world, storms will be fiercer, floods will be deeper, droughts will be longer, deserts will be drier and wild fires will be wilder.”"
"Just over ten years ago the Nobel Prize-winning atmospheric scientist Paul Crutzen coined the term “Anthropocene” for a globe totally transformed and dominated by humans, a state he suggested we were in now. Although this idea was not totally new, the term Anthropocene caught on and is now regularly used outside the environmental and earth sciences.  Perhaps the appeal of the term is partly because it adheres to the traditional suffix of “-cene” which in geology indicates a geological epoch, such as the two most recent epochs, the Pleistocene (from 2.5m years ago) followed by the Holocene, technically the current geological period from the beginnings of agriculture around 12,000 years ago to the present day.  In 2009 it was suggested to the world’s authoritative geological bodies that the term should be formalised – something much easier said than done. Defining geological boundaries has never been easy and several important boundaries have only been formally defined in the past 20 years. Geological boundaries are, to a large extent, arbitrary – even political – but they are essential. As the bedrock of geology they are the only way of correlating rock sequences and usefully discussing Earth’s 4.5 billion year history. Several geological committees in the UK and other countries are looking at whether the Anthropocene should be defined as a new geological period and, if so, what type it should be and when it should start. The idea is that the Anthropocene period relates to, and takes its name from the time in which humans have come to completely dominate the planet. When that period began is moot, with some believing it must be a gradual process, affecting some parts more than others over many millennia.  Others favour the idea of using the spike in radionuclides associated with the nuclear age as a marker, while others still believe such dominance is yet to happen. Myself, and others, would argue that it could be dated back to the adoption of agriculture, over the past 6,000 years. Why is this of importance to anyone, even other academics, outside the slightly dusty realm of the earth sciences? There are several reasons, some pragmatic, others more theoretical. Geological classification is used by other disciplines, most notably archaeology and engineering.  A formalised Anthropocene would have little effect on engineering, but it would put archaeology - the study of human activity in the past - in a curious position, namely that of having the vast majority of its subject matter (with a few exceptions such as Cold War archaeology) placed into a past, and by implication pre-human geological period, the Holocene. The disparity would only enlarge the gulf between geological and archaeological ways of looking at the past. It is also difficult to see how this would work in relation to geological maps, as the period assigned to a rock deposit is based on its known age. Even sediments of human origin might or might not be of Anthropocene age, depending upon where the boundary was set.  And what of the existing Holocene epoch, which would end at the beginning of the Anthropocene? It could be unfeasibly short, just a few thousand years, raising the question of whether it was sensible to have carved the Holocene out of geological time in the first place. The principal reason it was separated from the Pleistocene, which itself contains more than 50 similar globally warm periods between ice ages, was that it was the geological period during which human civilisation had evolved.  More theoretically but equally important is that the rules of geological taxonomy were never designed to identify a boundary falling within recorded history and which is not based on natural geological changes. The whole question is reminiscent of the famous statement by the historian Francis Fukuyama who in response to the collapse of socialism in 1989 declared that history was dead. It clearly wasn’t but the world had changed fundamentally, and likewise the Anthropocene would be a geological period like no other. Given these difficulties one is forced to wonder why bother? Its advocates argue that recognising the Anthropocene might move society towards a more sustainable path. This is not the purpose of the geological column, and raises the concern that this is about funding priorities: in a world where research funding is increasingly driven by political agendas and relevance, it’s clear that the Anthropocene could quite wrongly be seen as somehow more relevant and worthy of funds than other areas. So does this academic debate serve any positive purpose? Yes, in that it has forced earth scientists to look deeper at how we recognise, define and demarcate the human component in all parts of the earth system and not just climate. Humans clearly do not act in isolation, and complex fluctuations exist in the atmosphere and oceans that are driven by factors outside of human control. Whether or not the Anthropocene is formalised there is little doubt that we are living at a critical period in Earth’s history, as the world changes around us."
nan
"
By Bill Steigerwald

“Grandpa does Washington”
JEFFERSON MEMORIAL, WASHINGTON, D.C.
It would have been an odd sight if any humans had been around to see three polar bears walking across the Tidal Basin and climbing the steps of the Jefferson Memorial. Open to the cold air and blowing snow, its cavernous icy marble interior was empty except for a 19-foot bronze statue of Thomas Jefferson.
“He’s as big as you are,” Junior said to Grandpa.
“He’s much bigger than I am,” Grandpa whispered as if he were in a church. “See those words engraved there on the wall. They’re from the Declaration of Independence. Thomas Jefferson is the human who wrote them.”
“What do they say?” Junior asked.
Grandpa smiled and winked at Mother. “They say, ‘We hold these truths to be self-evident, that all polar bears are created equal, that they are endowed by their Creator with certain inalienable rights, among these are life, liberty, and the pursuit of happiness.’”
“Those words are some of the greatest ever written about freedom,” Mother said. “Too bad so many humans no longer believe in them,” she added, opening her brown suitcase and taking out a neatly folded stack of human clothes.
“These should fit,” Mother said, handing Grandpa a dark herringbone three-button wool suit, matching vest and wide-striped tie like the one she had seen Jimmy Stewart wearing in the movie “Mr. Smith Goes to Washington.” “Your eyeglasses are in the breast pocket.”
“And here’s your costume, Junior,” Mother said, giving him a pair of home-made blue jeans and a Chicago Cubs T-shirt to go with his backpack and Cubs baseball cap. “And your glasses. Don’t ever take them off when we’re in the presence of humans.”
After Mother put on her black skirt, blouse and seashell pink blazer, she pulled out her pair of gray Kawasaki 704 eyeglasses and put them on. Except for her black nose, she looked eerily like Sarah Palin.
“What do you think, Dad?” Mother asked Grandpa. “They were a little pricy, even on the Internet. But I think they work.”
The three bears looked at each other’s outfits admiringly. They weren’t the latest fashions, but as far as any humans who looked at them could tell the trio looked like an ordinary – if large – family of humans who’d come to Washington to see the sights.
For several hours the three bears explored the snowy, deserted streets of downtown Washington. Grandpa had a long list of places he had always dreamed of visiting and they were all carefully plotted on the old map he carried.
They walked across the frozen Tidal Basin to the Washington Monument, where Grandpa hoped to take an elevator ride to the top. But it was closed because of the horrible weather, so instead they visited the National World War II Memorial and the Vietnam Veterans Memorial.
As they strolled past the brightly lit White House, two wary policemen in a patrol car slowed down to look them over.
“Wave, Mother,” Grandpa said under his breath as the policeman driving the car shined a spotlight on them. “Wave, Junior.”
The policeman hesitated. He squinted his eyes. Something seemed very, very fishy. He unlocked the shotgun attached to the dashboard of his patrol car.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e902222c0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Just when you think things can’t get any more bizarre with the IPCC, having just learned that the IPPC 2007 report used magazine articles for references, head of the IPCC, Dr. Rajenda Pachauri, provides comedy gold. According to the UK Telegraph, he’s just released what they describe as a “smutty” romance novel, Return to Almora laced with steamy sex, lots of sex. Oh, and Shirley MacLaine.
Here’s the good doctor, grinning like a Cheshire cat at his book launch in India on January 10th.
Click for more photos from his book release
The Telegraph’s Robert Mendick and Amrit Dhillon in Delhi write:
As the UN’s climate change chief, Dr Rajendra Pachauri has spent his career    writing only the driest of academic articles. But the latest offering from    the chairman of the UN’s climate change panel is an altogether racier tome.
Some might even suggest Dr Pachauri’s first novel is frankly smutty.
WARNING ADULT CONTENT FOLLOWS:
(First time I’ve had to do that on WUWT)

Return to Almora, published in Dr Pachauri’s native India earlier this    month, tells the story of Sanjay Nath, an academic in his 60s reminiscing on    his “spiritual journey” through India, Peru and the US.
click for bookseller
On the way he encounters, among others, Shirley MacLaine, the actress, who    appears as a character in the book. While relations between Sanjay and    MacLaine remain platonic, he enjoys sex – a lot of sex – with a lot of    women.
In breathless prose that risks making Dr Pachauri, who will be 70 this year, a    laughing stock among the serious, high-minded scientists and world leaders    with whom he mixes, he details sexual encounter after sexual encounter.
The book, which makes reference to the Kama Sutra, starts promisingly enough    as it tells the story of a climate expert with a lament for the denuded    mountain slopes of Nainital, in northern India, where deforestation by the    timber mafia and politicians has “endangered the fragile ecosystem”.
But talk of “denuding” is a clue of what is to come.
By page 16, Sanjay is ready for his first liaison with May in a hotel room in    Nainital. “She then led him into the bedroom,” writes Dr Pachauri.
“She removed her gown, slipped off her nightie and slid under the quilt    on his bed… Sanjay put his arms around her and kissed her, first with    quick caresses and then the kisses becoming longer and more passionate.
“May slipped his clothes off one by one, removing her lips from his for    no more than a second or two.
“Afterwards she held him close. ‘Sandy, I’ve learned something for the    first time today. You are absolutely superb after meditation. Why don’t we    make love every time immediately after you have meditated?’.”
More follows, including Sanjay and friends queuing to have sexual encounters    with Sajni, an impoverished but willing local: “Sanjay saw a shapely    dark-skinned girl lying on Vinay’s bed. He was overcome by a lust that he    had never known before … He removed his clothes and began to feel Sajni’s    body, caressing her voluptuous breasts.”
Take a cold shower, and read the rest of the steamy  (possibly a water vapor feedback loop) novel at the Telegraph here
Note to the U.N. – Time to kick Pachy to the curb, he’s not just toast now, he’s carbonized.
In other news, The Love Guru has this relevant quote from a hockey team member: “there’s no connection between hockey and my love life”

UPDATE: Steve McIntyre quips:
In breaking news, Vivid Entertainment has bought the film rights to the IPCC Fourth Assessment Report. They plan to give new meaning to the terms Working Group 1, Working Group 2 and Working Group 3. They promise to give “peer review” an entirely new interpretation.

Sponsored IT training links:
The credible HP0-S27 training really helps you pass CISM certification. Get the 642-982 latest dumps to fasten your success in first try.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8e93200b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterSolar panels, electric cars, windmills, biofuels – It’s all been ballyhooed as the next socio-technological revolution. One that would transform our energy supply and ensure “sustainability”, thus saving the earth from climate doom. “Go green” was the motto.All that was needed was a little help from the state. Now it looks as if even a Soviet style intervention is not going to save the green movement. Rather, it looks as if state intervention has doomed it. Everywhere the green economy is in tatters.
It seems everything that the government touches nowadays ends up turning into a folly. We are seeing it with solar energy, see here, here, here and here. The same goes with wind, biofuels and deforestation, and even the toxic mercury-laden light bulbs that are poisoning the land. How much longer before it all goes bankrupt?
Just recently Marc Morano unplugged the Chevy Volt, revealing the folly behind government supported electric cars, see here. No one wants them – even with the massive subsidies. And not even in Green Germany.
Germans opting more for gasoline and diesel engine cars 
Here a recent study released by oil company Aral shows that Germans are once again interested in acquiring a new car, read here in German. That’s good news for the economy. The problem is that fewer people are expressing interest in buying electric cars and hybrids. The Aral press release writes (emphasis added):
The preferred drive system remains the Otto engine by a clear margin. After 2009 when only 51% were interested in buying this conventional type of drive system, the number has since risen 10 percent to 61%. Also diesel engines can be happy with a share of 28% (+2%). A significant decrease was posted by cars driven by natural gas. Here the number of potential buyers has dropped by 50% over the last 2 years, going from 10% to 5%. Also electric cars have suffered a setback: Only 28% of those surveyed said they could imagine buying an electric car. Two years ago the figure was 36%.
Share this...FacebookTwitter "
"**A man who killed and dismembered a retiree in a bid to steal his money dumped his victim's remains in a badger sett, a court has heard.**
Daniel Walsh, 30, is accused of murdering Graham Snell, 71, whose body parts were found in various locations around Chesterfield.
On the last day he was seen alive, Derby Crown Court heard, Mr Snell told police the defendant had been stealing from his bank account.
Mr Walsh denies murder.
Prosecutor Peter Joyce QC told jurors Mr Snell went to a police station on 19 June last year and told officers he had ""a problem with a man who comes and stays at my house without being invited"".
By the time officers went to his home in Marsden Street the next morning, Mr Joyce said, ""Graham Snell was lying dead in his house"".
""Also in his house but not answering that door was Daniel Walsh,"" he said.
The next day, the court heard, the defendant purchased 10 rubble sacks and two large saws ""to cut through the bones of Graham Snell"".
On 24 June, jurors were told, Mr Walsh loaded two or three large black bags containing ""many parts of Mr Snell's body"" into a taxi.
He then travelled to Barbon Close where he ""buried or pushed"" them into various parts of a badger sett, Mr Joyce said.
Three days later, he once again travelled by taxi to dump parts of the victim's torso in communal bins at a block of flats.
They were discovered on 2 July, the court heard.
Mr Snell's head and arms were eventually found in a wood ""a little way away"" in February this year, the prosecutor said.
""What he did was awful and what he did was murder,"" Mr Joyce said.
""He killed him, he chopped him up, he fed him to the badgers, he put... his torso in a communal bin.
""It was murder to get his hands on this old, retired man's money and just dispose of him as a piece of rubbish.""
The jury was also told Mr Walsh had previous convictions for stealing Â£5,000 from Mr Snell in 2009 and assaulting him in 2014.
Jurors were told the trial had originally begun in March but was stopped due to Covid-19.
_Follow BBC East Midlands on_Facebook _, on_Twitter _, or on_Instagram _. Send your story ideas to_eastmidsnews@bbc.co.uk _._"
"

Ultra‐​orthodox Jews in heavy beards and heavier black coats pray for hours each day at Jerusalem’s Western Wall, even under a sweltering summer sun. Each year, Shiite Muslims whip their backs bloody with chains during the religious holiday of Ashura. Religious vegetarians in Phuket, Thailand, similarly drive knives and skewers through their cheeks.



From an outsider’s perspective, religious displays of self‐​inflicted pain can seem pointlessly barbaric. But many anthropologists and evolutionary psychologists believe they have an important function: to facilitate collective action by requiring members to send a costly, hard‐​to‐​fake signal of commitment to the group’s common creed.



The same impulse, in a rather less impassioned form, seems to animate the Democrats’ climate change bill. Coordinating international political action to achieve significant reductions in carbon emissions is a collective action problem of grand, global scale. One way to achieve and maintain such coordinated effort is to detect and punish shirkers. (Governments keep money rolling into their treasuries by threatening tax dodgers with jail.) However, there is no world government with the power to bring wayward nations into line, no world‐​ranging whip to keep countries pulling in time.





The Democrats’ cap‐​and‐​trade bill is stalled in legislative limbo because Americans are far from united about its merits.



This is the glaring flaw in plans for carbon taxes and cap‐​and‐​trade regimes: The world’s wealthy nations may now be willing to paddle their boat upstream, but if the developing world won’t row along with them, if they insist on a free ride, the boat is going nowhere.



Yet there are other tricks for encouraging cooperation and weeding out “free‐​riders.” Consider the self‐​flagellating Shiites and face‐​piercing Thai vegans. These are extreme examples of a cooperation‐​enabling strategy that game theorists call “costly signaling.” Those who display an unflinching devotion to even the most burdensome rules of common life are more likely to pull their weight, to uphold their end of a deal. Talk is cheap, but the willingness to pay a price signals to others the commitment of a real team player.



President Obama would like to walk into the climate‐​change talks in Copenhagen this December flashing a clear signal that America is willing to pay a price in the fight against carbon and its depredations. Indeed, the best one can hope from the climate legislation languishing in Congress is that, if passed, it will put the world on notice that the United States, the Earth’s greatest per‐​capita carbon font, can be trusted to pull its weight in a global climate deal.



The signal would certainly be costly. According to the Congressional Budget Office, the Waxman‐​Markey cap‐​and‐​trade scheme passed by the House would reduce GDP growth between .03 percent and .09 percent per year for the next 40 years. That may not sound like much, but annual growth rates, like annual interest rates, are compounding, which means that the cost grows considerably over time. At the conservative .03 percent annual penalty, the CBO estimates the U.S. economy in midcentury will be short more than $300 billion a year compared with a future without Waxman‐​Markey.



What would Americans get in return? Nothing, nada, zip, zilch—unless most of the world plays along. As the CBO put it: “As long as a significant fraction of the world did not adopt similar policies, some of the reductions in the United States would probably be offset by increases in emissions elsewhere.” That is to say, if countries like India and China won’t agree to (and, more important, stick with) painful cuts that will slow their steady rise from poverty, American sacrifice will do next to nothing to combat the threat of melting ice caps and a more livable Canada.



Costly signals can make sense if they deliver the benefits of cooperation. Won’t proof of our faith help skeptical governments in the developing world see that international cooperation is possible after all? It’s unlikely.



The Democrats’ cap‐​and‐​trade bill is stalled in legislative limbo because Americans are far from united about its merits. It would be reasonable for international players to suspect that an American electorate unhappy with the costs of a future carbon cap might have a change of heart. And then there’s the bill itself: a patchwork of exemptions, subsidies, and special favors. If political horse‐​trading produced something so convoluted from the start, it is fair to assume that it will become even more compromised as time goes on, leaving the U.S. unable to actually meet the legislation’s aims. Most important, a costly signal clinches trust only among those on the same wavelength. Overheated ultra‐​orthodox Jews and lacerated Shiite Muslims probably don’t much impress each other. Likewise, the signal broadcast by the willingness of wealthy nations to cut their carbon emissions may fail to impress poorer counties with fundamentally different priorities. They are not free‐​riding if they never asked to be in the boat.



It is hard to see the point of legislation that promises certain costs and improbable benefits. Still, there could be a point. Many Americans would find profound meaning in passing legislation like Waxman‐​Markey and gladly bear its costs—even if it does little to secure international cooperation, and even if it does nothing to slow global warming. The law would nevertheless speak to what Americans value, what we aspire to, who we are, what we’re about. It would say that we’re not so bad, that we repent our industrial sins, that over here we know full well that green is the new black.



Alas, this is not a statement of faith most Americans are prepared to make, or a cost they are prepared to pay. They should not be asked to don a green hair shirt just to show the world that some of us care.
"
"**An agreement to relax Covid rules over Christmas is not ""an instruction to meet with other people"", Wales' First Minister Mark Drakeford has said.**
Three households from around the UK will be able to meet from 23 December until at least 27 December.
It follows an agreement between the UK government and ministers in Wales, Scotland and Northern Ireland.
Mr Drakeford said he believed people would be unwilling to stick to ""strict rules"" over the Christmas period.
It comes as Welsh ministers consider whether more restrictions will be needed in the run up to Christmas, as cases rise among the under-25s.
The first minister called for a ""common approach"" to dealing with the aftermath of Christmas - earlier he warned that relaxing rules would lead to an ""inevitable"" rise in coronavirus.
Welsh Conservatives welcomed the agreement, but Plaid Cymru warned that ""hard-gotten gains"" must not be lost ""for the sake of a few days"".
Mr Drakeford told BBC Wales: ""If we ask people just to stick to the strict rules we have now I'm afraid lots of people will not be prepared to do that.
""So it's not a choice between relaxation or no relaxation.
""It's having a form of relaxation where there are rules that people will recognise that will allow people to enjoy Christmas, but we'll do it in a controlled way.""
The Welsh Labour leader added: ""People will be allowed to do what the law will allow them to do, but this is not an instruction to travel, it's not an instruction to meet with other people.
""People should still use a sense of responsibility, should still ask themselves whether what they are doing is keeping themselves and other people safe.""
Under the agreement, made at a meeting of Cobra on Tuesday afternoon:
The agreement said that ""existing, more restrictive"" rules on pubs and restaurants, and meeting in other venues will be maintained.
Ministers have been considering tighter restrictions in Wales in the run up to the festive period.
""The cabinet will meet before the end of this week again,"" the first minister said. ""If we're in a position to make an announcement this week, then that's what we will do.""
He added: ""Where does coronavirus spread? It's spread in people's homes, it spreads in hospitals and it spreads in hospitality.
""We have to think about all three of those settings and do our very best to bear down on the virus, which spreads so fast if it's given an opportunity.""
Vaccines and mass testing are ""not going to come to our rescue in January"", the first minister added.
""There is still a pull through to the spring before we will see the real benefit of those things, and we are going to have to ask people to go on living with the virus, living with it sensibly, living with it in a way that limits the damage while we are bringing those new possibilities, fully on stream.""
Plaid Cymru leader Adam Price said a ""compassionate but responsible approach"" was ""sensible"", but said the Welsh Government ""has a responsibility to ensure clear communication over the festive period, encouraging people to follow the guidelines"".
Welsh Conservative Senedd leader Paul Davies welcomed the decision and said it showed ""what can be achieved when governments work together"".
Simon Hart, the UK government's Welsh Secretary, said he was delighted with the agreement, but urged people ""to continue to be careful and responsible over the Christmas period to keep themselves and their families safe"".
The broad questions about Christmas have been answered, but plenty of questions remain about what happens before and after.
Mark Drakeford has said repeatedly that the festive relaxation will lead to more Covid cases and ""payback"" will be needed.
Does that mean a tightening of the rules in the run up to Christmas? Probably.
Could they be introduced to coincide with the new regime starting in England on 2 December? That would continue the theme of a communal UK approach.
And while talk of vaccines and mass testing have given us hope for a better 2021, how quickly can they be rolled out to the general population?
Whatever the calendar says, January could still feel very 2020."
"
Share this...FacebookTwitterPortuguese skeptic site Ecotretas reports on the crashing carbon market:
It’s nothing new. It started over a year ago, when US
carbon trading crashed. Two weeks later, it
closed. Now, it’s our turn in Europe. It had already started
earlier this Summer. But now, as can be seen in the left graphic, obtained from
Bloomberg, carbon prices are diving even more! And this is yesterday’s
graph, as today, as I write, it is diving another -10.773% to € 7.040.
Charts and more here…
Share this...FacebookTwitter "
"Two companies in Japan recently announced they are to begin building two huge solar power islands that will float on reservoirs. This follows Kagoshima solar power plant, the country’s largest, which opened in late 2013 and is found floating in the sea just off the coast of southern Japan. These moves comes as Japan looks to move on from the Fukushima disaster of 2011 and meet the energy needs of its 127m people without relying on nuclear power.  Before the incident around 30% of the country’s power was generated from nuclear, with plans to push this to 40%. But Fukushima destroyed public confidence in nuclear power, and with earthquakes in regions containing reactors highly likely, Japan is now looking for alternatives. Solar power is an obvious solution for relatively resource-poor nations. It is clean, cost-competitive, has no restrictions on where it can be used and has the capability to make up for the energy shortfall. A small fact that solar researchers love to trot out is that enough sunlight falls on the earth’s landmass around every 40 minutes to power the planet for a year. To put this another way, if we covered a fraction of the Sahara desert in solar panels we could power the world  many times over.  The technology already exists, so producing enough solar power comes primarily down to one thing: space. For countries such as the USA with lots of sparsely populated land this is not an issue, and there have already been a large number of “solar farms” installed around the country.  In places like Japan where space is limited, more inventive solutions are required. This is the principle reason behind the decision to move their solar power generation offshore. While the land is highly congested, and therefore expensive, the sea is largely unused. It therefore makes a good degree of sense to use this space for floating power plants. Somehow though the concept of floating solar plants initially does seem rather jarring. Water plus electricity? We’ve all been raised by popular culture and public safety films to know that those two really don’t mix.  But this isn’t some sort of vast technological challenge that mankind will struggle to overcome. The panels are designed to be waterproof and a number of these types of plants have been built in Japan already, including the large installation in Kagoshima.  Part of the beauty of solar power is how simple it is to use. At a basic level, once you buy the off-the-shelf photovoltaic module, it’s simply a case of plugging it in. The principle engineering challenge of offshore solar farming consists of little more than building a pier and covering it in solar panels.  This may be a slightly glib oversimplification, but consider the relative difficulties in comparison to the construction of an offshore oil drilling platform. These represent a true engineering challenge and a true risk when that challenge is failed, as we saw all too clearly with the Deepwater Horizon spill in the Gulf of Mexico in 2010. The risks and difficulty associated with off-shore solar are vanishingly small by comparison.  Floating solar also has some interesting fringe benefits. Solar modules function much better when cooler, so situating them near water actually helps performance. In India they have also been used as an interesting dual purpose solution. In the state of Gujarat, solar panels were installed atop the Narmada canal, serving to both generate power and prevent water evaporating from beneath.  There is also no reason why the design needs to be so functional. By far the most unique application is the concept of “energy ducks”, giant floating solar panel-coated water fowl which have been proposed to sit in Copenhagen harbour acting as both a tourist attraction and carbon-neutral power source. This may never happen unfortunately, but it is a rather wonderful demonstration of how solar power can be applied in many different ways.  Solar islands could certainly be a solution for other countries where space is an issue – it’s possible that one day a significant portion of Europe’s power could be generated by giant solar pontoons in the ocean. The technology exists and the engineering challenges are nothing that can’t be overcome. The only questions now are whether the will is there to push solar islands as a solution and more importantly do we make them duck shaped?"
"Unfurling across paintings, poems and carvings, Cymbidium orchids are more than just wild plants in China. They are inextricably linked with the country’s culture. But this rich blooming of human response to orchids that has endured for millennia is fragile, and as Cymbidium orchids increasingly vanish from the wild so too do the words and knowledge that humans have about them. Every year the Royal Botanic Gardens, Kew, and the New York Botanical Garden open their doors to thousands of visitors who flock to their orchid shows. Easily grown and long-lasting orchids such as generic Phalaenopsis form the bulk of these temporary displays. But behind the scenes these institutions engage in longer-term work to conserve not just living plants but also records of the culture attached to them.  Kew’s Spirit Collection contains ghostly flowers of Cymbidium kanran; their colour washed out but their three-dimensional shape preserved by immersion in a mixture of alcohol, glycerol, and water. In collaboration with the Institute of Medicinal Plant Development, Beijing, Kew has also developed a collection of plants – including orchids – in the forms they are used in traditional Chinese medicine – chopped, dried, fried and so on. Indexed with scientific botanical names, this collection is a repository of knowledge and a reference tool for authenticating botanical ingredients. Dr Barnabas Seyler, assistant researcher in the department of environment at Sichuan University, looked at the relationship between biodiversity and cultural diversity by examining changes in knowledge of Cymbidium orchids in Liangshan Yi autonomous prefecture, Sichuan. “As an ethnobotanist, I find all facets of biocultural diversity to be fascinating,” says Seyler.  “Many people, particularly in the west, but also in rapidly changing, urbanising, and modernising China do not fully appreciate the magnitude of symbolism and pride that Cymbidium have held throughout history in traditional Han Chinese culture,” he says. “Symbolism, metaphors, and poetry associated with Cymbidium are credited to have begun with Confucius’s own sayings and infuse traditional Confucian thought today. If you walk into any Chinese restaurant around the world, or into any tea parlour or salon in China, you will likely find paintings, furniture, place settings, or other material culture items depicting Cymbidium orchids.” Orchids, like other wild species, are vulnerable to the impact of climate change and habitat loss. But Cymbidium species native to Sichuan studied by Seyler have an additional vulnerability – their beauty. Between 2005–2008, when Cymbidium prices were at their peak, wild-collected rare forms could be sold for six-figure sums. In 2006 one was bought for 4.6m Chinese yuan (£511,000). Interviewing individuals in predominantly Yi and Han communities, Seyler and his colleagues assessed whether people could identify different Cymbidium species. Additionally, they asked about local ecological knowledge: how to find, harvest and grow orchids, as well as orchid business knowledge, and awareness of orchids in arts, academia and idiomatic expressions. In all categories of knowledge, they found that when species were locally extinct, knowledge about them had declined, except among individuals involved in their trade.  Seyler believes that botanical gardens contribute much towards conserving biocultural diversity, “through ex-situ collections, for example collecting and showcasing plants like Cymbidium, advocating for their conservation, and educating the general public, and documenting traditional knowledge, stories, and cultural traditions associated with these plants”. In the US, New York Botanical Garden is a designated plant rescue centre. When the US Fish and Wildlife Service finds shipments of orchids that lack the paperwork to prove they were either cultivated or sustainably collected from the wild, they are sent to a plant rescue centre. If the country of export does not request their return within 30 days, they are incorporated into the institution’s collection, but remain US government property. “Botanical gardens, properly functioning, serve as local drivers of economic development, education capacity building, and as mechanisms for community cohesion, showcasing the unique beauty and value of a region’s biocultural diversity to visitors,” says Seyler. Han Chinese culture won’t collapse if Cymbidium orchids become extinct in the wild in China. Culture doesn’t entirely disappear because of the loss of one plant or group of plants. But what happens if species loss continues? Seyler believes botanical gardens are playing a vital role in keeping the culture alive. “They can help to address some of the educational, social, and economic challenges that contribute to the unsustainable overharvest of biodiversity,” he says. • The orchid festival at Royal Botanic Gardens, Kew, runs until 15 March. The orchid show at the New York Botanical Gardens is open until 19 April. This article was edited on 6 March 2020 to reflect the announcement of extended opening dates for Kew’s orchid festival"
"**Chancellor Rishi Sunak is unveiling the government's spending plans for the coming year.**
The Spending Review will include details on public sector pay, NHS funding and money for the devolved administrations in Northern Ireland, Scotland and Wales.
Mr Sunak will also set out the extent of the damage done to the UK economy by the coronavirus pandemic.
A No 10 spokesman said the economic forecasts will be ""a sobering read"".
The government's Covid response has led to huge spending and borrowing rises.
The chancellor is expected to begin his statement at around 12:30 GMT following Prime Minister's Questions.
Some Spending Review announcements have already been trailed.
These include an extra Â£3bn for the NHS in England to help tackle the backlog of operations delayed due to Covid, an increase in defence spending and a Â£4.6bn package to help the unemployed back to work.
The government is expected to announce a cut in the UK's overseas aid budget to 0.5% of national income, down from the legally binding target of 0.7%.
There have also been reports that the chancellor is considering a pay freeze for all public sector workers except frontline NHS staff.
Plans to change the way big spending projects are analysed \- which the Treasury says is currently biased in favour of the south east of England - will be published alongside the Spending Review.
The chancellor may also choose to set aside money to tackle climate change and regional inequalities.
Devolved governments will receive money proportionate to any funding England gets in the Spending Review.
This is decided using the Barnett formula - devised by Lord Barnett, a Labour politician, in the 1970s.
Mr Sunak and Treasury Chief Secretary Stephen Barclay updated the Cabinet on Wednesday morning.
A Downing Street spokesman said: ""Cabinet was told the OBR forecasts will show the impact the coronavirus pandemic has had on our economy and they will make for a sobering read, showing the extent to which the economy has contracted and the scale of borrowing and debt levels.
""But - as the IMF (International Monetary Fund), OBR and others have pointed out - the costs would have been much higher had we not acted in the way we have done.""
""It's going to look horrible.""
The simple truth about the Spending Review according to a senior MP.
The chancellor will bang the drum for his plans to keep people in jobs, or help find new ones.
Rishi Sunak will take out the metaphorical megaphone to explain how he'll allocate billions of taxpayers' cash to spend on infrastructure in the coming months.
But the headlines of the Spending Review, when governments put their money where their mouths are, won't be in any rhetorical flourishes at the despatch box, nor likely in any surprise announcements kept back as goodies for the public.
The government had intended to use the Spending Review to set out its plans for the next three years, however this was reduced to just one year due to the economic turmoil caused by Covid.
The difficult financial backdrop will dominate this year's review with the economy projected to be 10% smaller than it was pre-virus.
Tax revenues have fallen as many businesses have been forced to close and government schemes to support furloughed workers have led to soaring levels of spending.
Public borrowing is expected to rise to Â£372bn - compared to the Â£55bn the government had originally expected to borrow.
The Spending Review will be accompanied by economic forecasts from the Office for Budget Responsibility - including predictions on how tax will be raised.
Labour's shadow chancellor Anneliese Dodds said the government's ""irresponsible choices"" during the pandemic had ""led to our country experiencing the worst downturn in the G7, and created a jobs crisis"".
""This prime minister and his government talk a good game but they haven't delivered on their promises - and regional inequality has got worse under their watch,"" she said.
""They clapped for key workers - but now they're freezing their pay, and looking to scrap planned minimum wage increases for the private sector.""
Unions called for Mr Sunak to maintain investment in the public sector, the TUC's deputy general secretary Paul Nowak telling BBC Breakfast ""now is not the time to make cuts to public services"".
And the SNP is calling for a huge stimulus package to support growth and jobs across the whole of the UK.
""The spending has to match the challenges we see in the economy,"" said its economic spokeswoman Alison Thewliss. ""At the moment interest rates are at a record low so the government should be borrowing."""
"
Share this...FacebookTwitterHalloween is already in full swing at the Potsdam Institute For Climate Impact Reseaarch in Germany. Witches, goblins, flesh-eating zombies and scary monsters are stirring about and spreading disturbing horror visions. Right now Stefan Rahmstorf is frozen by such a vision like a deer in the headlights.
North Pole stop in the 1950s. (Photo US Navy)
Rahmstorf, who has the unscientific habit of taking cyclic graphs and drawing straight trend lines along their upward or downward flanks and extending them all the way out to doomsday, presents the latest horror vision that has been haunting his mind lately. Indeed the visions seem to be getting more intense and vivid. In a piece in the Gulf Times he warns of “grave warning signs”.
Obviously he appears unaware of the fact that the Arctic all but melted away in the 1950s (see photo above) before rebounding to the record high levels of the 70s. You can’t start at the top of a cycle a draw a straight line along its downward flank all the way to yonder (hat-tip DAmbler):
A grave warning sign in the Arctic
By Stefan Rahmstorf /Berlin
Largely unnoticed, a silent drama has been unfolding over the past weeks in the Arctic. The long-term consequences will far outstrip those of the international debt crisis or the demise of the Libyan dictatorship, the news stories now commanding media attention. The drama – more accurately, a tragedy – playing out in the North is…continue reading
Oh no! Here comes doomsday!!!

Share this...FacebookTwitter "
"
Share this...FacebookTwitterA recently released BP report here shows that global coal consumption has risen over the last 10 years by almost 50%. So wouldn’t you think that all those millions of tons of emitted CO2 (food for plants) as a result would drive the global temperatures up? Have temperatures risen along with all that extra coal burning?
No they haven’t. In fact they’ve dropped slightly over the same period. So go figure!

The blue line shows skyrocketing global coal use, yet global temperatures have fallen. 
In the above chart the blue line shows global coal consumption, data taken here, Review of World Energy. According to the report, India and China alone are responsible for 90% of the world’s coal consumption increase, while renewable energy in the 2 countries plays nary a role. According to BP figures, global CO2 emissions rose 5.8% in the year 2010.
The International Energy Agency (IEA) says that China will add a whopping 600 gigawatts of coal power plant capacity by the year 2035, equivalent to the current capacity of the USA, EU and Japan – combined! So as China adds one coal power plant each week, Europe and the USA are lucky to get a single one approved during an entire year.
Demand for coal is not about to change directions any time soon. The IEA estimates that the global population will climb to 8.5 billion people by the year 2035. That means a huge growth in demand for power. Already today the sad truth is that 20% of the global population still has no access to electricity. Forcing the prices up with CO2 emission trading schemes and carbon taxes will only make the situation worse for the very poor.
But now that we know burning coal has hardly a noticeable impact on temperature and climate (zero-correlation), it’s high time to double our efforts in producing more coal so that the world’s demand can be satisfied so that bitter poverty may be alleviated once and for all.
Share this...FacebookTwitter "
"**Home Secretary Priti Patel was warned on several occasions to treat staff with respect, a former top official who resigned over the row has told the BBC.**
An inquiry found Ms Patel's behaviour broke the rules - although she was ""unaware"" of her conduct.
But ex-official Sir Philip Rutnam has contradicted this, saying Ms Patel was told not to shout and swear at staff a month after becoming home secretary.
The home secretary has apologised for any ""upset"" she caused people.
But she insisted the inquiry's findings had made clear that ""issues were not pointed out"" to her in the course of the department's ""deeply challenging"" work.
The PM has said he does not believe Ms Patel is a bully and regards the matter as closed despite the resignation of his adviser on ministerial standards, Sir Alex Allan, whose report found Ms Patel's conduct fell below the standards expected of government ministers.
Sir Philip Rutnam quit as permanent secretary at the Home Office in February, after complaining about Ms Patel's conduct, and is currently suing the government for unfair dismissal.
He said he was never asked to contribute to the bullying inquiry despite resigning over the matter.
Furthermore, he maintains that Ms Patel was ""advised"" on a number of occasions about the need to treat staff with respect, despite the report saying she had been ""unaware"" of her conduct.
""As early as August 2019, the month after her appointment, she was advised that she must not shout and swear at staff.
""I advised her on a number of further occasions between September 2019 and February 2020 about the need to treat staff with respect.""
Sir Philip said he respected Sir Alex and regretted his resignation but took issue with the ex-adviser's suggestions that the Home Office did not provide sufficient support to Ms Patel when she took on the job.
""Enormous efforts were made from top to bottom in the Home Office to support the new home secretary and respond to her direction and significant achievements have resulted. The advice does not fairly reflect this.""
In his report, Sir Alex said there was ""no evidence"" that Ms Patel was aware of the impact of her behaviour, and no feedback was given to her at the time"".
The BBC's political editor Laura Kuenssberg said Sir Philip's comments were important because they challenged this verdict.
Ms Patel has given an ""unreserved apology"" for any upset caused, which she said ""was completely unintentional"", but argued she was not ""supported"" by her department at the time when claims were made.
Mr Johnson has said he retains ""full confidence"" in Ms Patel while Tory MPs have rallied behind her.
Former minister Simon Clarke told the BBC she had been working ""under the most challenging circumstances to bring about major change in a department that needs it"".
But the head of the Committee on Standards in Public Life, Lord Evans, said there were ""serious questions"" about the process for investigating breaches of the ministerial code which must be ""urgently"" looked into."
"Jonathan Brearley might be the most important British climate pioneer you’ve never heard of. In the past 10 years, the former civil servant has quietly steered the UK to its first climate legislation, and then to the policy framework responsible for creating Britain’s cleanest electricity system since the 1880s. As the new chief executive of Ofgem, Brearley plans to inject climate action into the core of the UK’s energy regulation, too. This may prove his toughest challenge yet. Brearley’s first task as Ofgem’s new boss was to answer critics who claimed the watchdog was out of touch with the UK’s “net zero” agenda because its rigorous checks and balances were founded upon the duty to protect customers, not the environment. In the balancing act between ambitious green spending and home energy bills, many companies feared that Ofgem would err on the side of short-term savings. Brearley’s nine-point plan to prioritise climate action seeks to prove the doubters wrong. “We take net zero very seriously, and as seriously as our other objectives. What you will see is us making sure that this is factored into all our decisions in quite a fundamental way.” Brearley joined the energy regulator in April 2018 to lead Ofgem’s energy networks team after stints within government building the foundations of Britain’s clean energy system. First, as the lead director behind the UK’s first climate legislation, the 2008 Climate Change Act, then as the architect of the government’s clean energy reform, which ignited a swing from fossil fuels to green electricity. His wide-ranging green strategy aims to help get 10m electric vehicles on our roads by 2030 and support a fourfold increase in offshore wind generation and a shift towards low-carbon heating. Ofgem is still committed to keeping a lid on energy bills, but will also lead a crackdown on “greenwash” energy deals and push for new tariffs that encourage homes to help balance the energy system. Brearley plans a rewiring of the energy regulator itself, too. He says Ofgem’s 1,000-strong workforce plans to “get out of the building” and beyond the routine box-ticking required of every industry regulator. “We know we need to be faster moving, more engaged with the world that we’re working in, and these things will be part of what we’ll do as we change over the next few years.” He admits there are “absolutely” tensions between plans for a bold, green future and the need to safeguard vulnerable homes, which risk being left behind. This challenge may prove greater for Brearley than his predecessors. The pace of change in the energy market will mean more investment in a low-carbon energy system, paid for through energy bills, as well as a rush of disruptive new technologies and companies into the market. Brearley keeps one particular customer in mind. She is of retirement age, living in social housing, and was being overcharged hundreds of pounds by her existing energy supplier. It took almost six months to fix the problem with the help of an energy adviser at a charity. “Two things struck me when I met her,” says Brearley. “The first is how much of a struggle it was for her to fix something which represented an important amount of money in her life. But the second was that when I asked her if she planned to switch away from her supplier she said she wouldn’t – because she’s just not comfortable choosing a new supplier.” “There are people who are poised to benefit from changes in the energy system, but those who will find it difficult. That means making decisions which are based on the investment that we need to get to net zero, but also how we can share those costs out in a way that’s fair,” Brearley says. In the past, Ofgem has been criticised for overseeing “rip-off” energy deals and poor customer service from suppliers, while network companies profiteered at the expense of rising bills. Brearley says Ofgem “will be less afraid of stepping in where we feel that customers are being harmed” and will “be driving a very hard bargain” with energy network owners. “I’m sure at some point these companies will claim that they can’t make investments without higher rates of return. Our job is to stick to the evidence, to stand firm, and get what we think is a fair deal for customers,” Brearley says. “I am very clear that we won’t make this transition to net zero if customers feel ripped-off.”"
"

Circling Yamal 3 – facing the thermometers
Guest post by Lucy Skywalker

Let’s look closely and compare local thermometer records (GISS) with the Twelve Trees, upon whose treerings depend all the IPCC claims of “unprecedented recent temperature rise”.
For my earlier Yamal work, see here and here. For the                        original Hockey Stick story, see here and here.
Half the Hockey Stick graphs depend on bristlecone                        pine temperature proxies, whose worthlessness has already                        been exposed. They were kept because the other HS graphs, which depend on Briffa’s Yamal larch treering series, could not be disproved. We now find that Briffa calibrated centuries of temperature records on the strength of 12 trees and one rogue outlier in particular. Such a small sample is scandalous; the non-release of this information for 9 years is scandalous; the use of this undisclosed data as crucial evidence for several more official HS graphs is scandalous. And not properly comparing treering evidence with local thermometers is the mother of all scandals.

I checked out the NASA                        GISS page for all thermometer records in the vicinity of Yamal and the Polar Urals, in “raw”, “combined”, and “homogenized” varieties. Here are their locations (white). The Siberian larch treering samples in question come from Yamal and Taimyr. Salehard and Dudinka have populations of around 20,000; Pecora around 50,000; Surgut around 100,000; all the rest are officially “rural” sites. Some are long records, some are short.

Russia has two problems. First, many records stopped or became interrupted around 1990 after the ending of Soviet Russia; worst affected are the very telling Arctic Ocean records. Second, during Soviet Russia (and possibly now for all I know), winter urban records were “adjusted” downwards so that the towns could claim more heating allowances. Nevertheless, it will become clear that these issues in no way impede the evidence regarding treerings.
 
Click to enlarge these graphs. The first shows the 20 GISS stations closest to Yamal and the Polar Urals. The second shows treering width changes over time (only 10 of the 12 trees here). This was supposedly compared with local thermometer records, and used to calibrate earlier treering widths as temperature measurements to create a 1000-year temperature record. It was a pig to turn these graphs into a stack of transparent lines at the same scale as the GISS records for comparison, but finally, interesting material started to emerge.

I scaled all the GISS thermometer records to the same temperature scale, and ran them all from 1880 to 2020 at the same time scale (GISS graphs do not do this). I overlaid them as transparent lines along their approximate mean temperatures for comparison. Mean temperatures (visually judged) vary from around -2ºC (Pecora) to -13ºC (Selagoncy, Olenek, Hatanga, and Ostrov Uedine) and even -15ºC (“Gmo Im E.K. F”). The calibrations are degrees Centigrade anomaly, and decades.

Ha! Straightway we see clear patterns emerging.                      Let’s agree them:
Thermometer records: (1) time-wise, thermometers show temperatures rising from 1880 to 1940 or so; (2) temperatures fall a little from 1940 to 1970; (3) temperatures then rise a little but do not quite regain the heights of the 1940’s; (4) despite mean temperatures ranging from -2ºC to -15ºC (total means range 13ºC), and a range of temperature                      anomalies from each mean of only 9ºC from warmest year to coldest year, when mean temperatures are aligned, clear correlations emerge; (5) there are high variations between adjacent years. We shall investigate all this more closely in a minute.
Treering records: I’ve shown here the full records given for the 10 trees that runs from 1800 to 2000; but below, I use the same timescale as the thermometer records (1880-2020) for comparison. It is useful to see a few things here already: (6) treering sizes are increasing from 1830; (7) before that they show a decrease; (8) they do show correlation from 1880 on (this is NOT proof that the correlation is due to temperature).

Yamal area: (9) The 7 stations around Salehard seem to go in lock step with each other pretty well. (10) The five Yamal treering records (YAD) also correlate with each other, showing spikes around 1910, 1925, 1940, 1955, 1965, and 1980-1990. (11) But the treerings fall out with each other 1990-2000; and (12) these treering spikes do NOT correspond to the thermometer temperature spikes; but (13) there is a slight correlation with the longterm temperature; however, (14) crucially, there is no hockeystick blade in the thermometer record (15) nor is there one in the treering record if we remove the red YAD061 which is clearly an outlier – only a plateau’d elevation of the peaks throughout the 20th century starting before the real CO2/temp rise (and this is actually matched by pre-1800 values at times).
Excuse me for wondering if treerings beat to a different drum than temperature – it is certainly curious that there appears to be something causing correlations in the treerings. Wind? Sunspots? The moon? But let’s check by zooming in a little closer…

Salehard close-up: (16) all the nearby                      thermometer records mirror Salehard closely, although stations are up to 500 miles apart, the range of mean temperatures is -2ºC to -9ºC, and the range of annual temperatures at each station is up to about 9ºC – altogether a remarkable consistency. Click to see animated version of these records. (17) The close                      fit of Mys Kamennij (pale sea-blue) is particularly significant, since it is maritime and rural, and the same distance as Salehard from the treering site (some 120 miles), but in the opposite direction; (18) Ostrov Waigatz (Vaigach Island) shows the same pattern but with greater extremes; (19) in comparison with all this, the treering records show virtually no correlation at all – yet since treering differences between summer and winter exist at all, one would expect to see some correlation with warmer and colder years. (20) Perhaps if a far larger sample were used, a correlation might be detected, but clearly (21) we have trees here that are far too individual – especially YAD061.

Polar Urals: Here are seven station records around the Polar Urals site, compared with the five Taimyr (POR) treering records. (22) Mean temperatures are lower here – further North but also more continental, so perhaps the summers are as warm as Yamal, with similar near-treeline environment. (23) more noise in the temperature record, but the overall pattern is still the same; (24) 1943, 1967, 1983 are warm in common with the Salehard records, and 1940 is cold; other years are harder to compare. (25) The early fragmentary records for Dudinka and Turuhansk still fit together and overlay the Salehard records well, showing clear temperature rise between 1880 and 1940. (26) The treering records are fairly coherent, more so than the Yamal ones, and (27) they fit the Yamal records’ spikes in 1910, 1925, 1940, 1955, 1965, and 1980 on, but (28) again, this does not fit the temperature record.

The best of both record series: Really rural thermometer records from the maritime Arctic: (29) show the strongest pattern yet which (30) fits the other two sets of thermometer records but (31) does not fit the treering records even though (32) the treerings show coherent patterns within themselves, despite the two sites being some 800 miles apart.

Briffa’s full chronology: The Yamal chronology Briffa used (black) is compared with Polar Urals (green) and shows recent temperatures exceeding the Medieval Warm Period but (33) this is highly questionable, as is the recent final uptick. No MWP supports the alarmist “Unprecedented!” yet Polar Urals generally have been shown to fit local thermometer records better than Yamal for the period of overlap.

More GISS Arctic graphs: There are many serious problems with GISS but we can only take the evidence here. (34) GISS 64ºN+ shows a misleading trend line – temperatures rise to 1940, fall to 1970, rise to 2000 but not higher than 1940, then level off after 2000; (35) I don’t know what stations went into this composite – the final uptick alerts my suspicions to some UHI or other station problems; (36) Tamino takes the biscuit for cherrypicked trends in the GISS 80ºN+ North Polar winter record (sea green) – it clearly opposes the general worldwide fall in temperatures 1940-1970. However, it’s interesting to see such extremes.

GISS’ homogeneity adjustments: Thankfully, only a few of these Russian records are “adjusted”. But the alterations are telling. Surgut spikes upwards over Salehard from about 1960 on – but (36) the adjustment (probably UHI) is perversely done by truncating and moving earlier records upwards, instead of adjusting later records downwards. And (37) why were Salehard’s and Ostrov Uedine’s earlier “raw” records omitted in the adjusted records? I think every correction here will tend to amplify global warming trends.

GISS world temperatures, 2008: This map                      was shown in Tingley & Huybers’ latest Hockey Stick presentation                      at  PAGES                      conference. GISS’ own station records around Yamal and Polar Urals appear to show (38) this map is misleading, since according to GISS’ own records, above, averages local to Yamal / Polar Urals after 2000 are at the most 1.5ºC anomaly (above local mean).

CRU Arctic temperatures, seasonal anomalies:                      (graph by romanm) Since this is from uncheckable individual station records, (39) the figures could be contaminated by various “correction” factors, (41) UHI is especially likely in the winter. But note that (42) the difference in character between months, and between summers and winters, is striking – summers have hardly changed – and (43) still no definitive Hockey Stick as per illustrations and per Briffa’s Yamal treering record, nothing beyond the range of natural patterns clearly evidenced here. Even the known slight overall increase during the twentieth century takes place mainly earlier in the century.
Conclusions: There is no sign whasoever of a Hockey Stick shape with serious uptick in the twentieth century, in the thermometer records. Yet these records are clearly very consistent with each other, no matter how long the record or how cold, high, or maritime the locality, with a distance span of over a thousand miles. Neither does the Hockey Stick consistently show in the treerings except in the case of a single tree. Even with thermometer records that are incomplete and suffering other problems, the “robust” conclusion is –
“Warmist” treering proxy temperature evidence is                      falsified directly by local thermometer records.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e92451de4',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterIt wasn’t long ago a PNAS study led by Stefan Rahmstorf had come out claiming sea level rise is “accelerating”. This of course was followed by the mainstream media jumping on the global warming bandwagon and trumpeting doom and gloom would strike sooner than we ever thought, maybe even before we die.
Unfortunately, the acceleration has been in the opposite direction, thus making the authors of the PNAS study look just a bit foolish.
The latest NASA satellite data show that sea levels have dropped 6 mm over the last year – the biggest drop ever recorded since satellite data has been taken. This is hardly the kind of acceleration Rahmstorf had in mind. You’d think the media would be falling all over themselves to report this good news. They have not. Only a tiny few German media outlets have reported the plummeting sea level news.
It’s due to a “weather shift”!
Der Spiegel rolled out a report called: Weather Shift Drops Global Sea Level, authored by Axel Bojanowski, hat-tip Dirk Maxeiner here. Caution: don’t be fooled into thinking Der Spiegel writers have become sceptical. To the contrary, they are cleverly, indirectly, blaming global warming for the “peculiar” sea level drop.
Global warming, you see, leads to weather shifts, which then leads to sea level drop. Hence global warming leads to sea level drop. Of course Der Spiegel will never admit this is what they are claiming, but they do indeed want you to believe it’s all because of “unusual freak weather” (which started when humans started driving SUVs).
The eastern Pacific heated by up to 10°C, huge quantities of water evaporated – and then later the mass of water fell to the ground via numerous storms over South America and later over Australia during the La Niña period.”
As is often claimed with temperature, sea level drop is now weather and sea level rise is climate. To Der Spiegel’s credit, Bojanowski at least admits that sea level rise has slowed down (emphasis added):



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




However since 1993, the oceans have been measured by satellites. They have detected a rise of 3 mm per year. During the last eight years, the rate of increase has slowed down.”
Leading German tabloid Bild here also expressed shock that sea levels have dropped by more than half a centimetre over the last year. Here, Bild blames the ENSO (er, weather) for the sea level drop.
Over the last 12 months, more precipitation than usual poured down over the continents, for example the destructive flood in Australia. The blame for this: the especially pronounced weather phenomena El Niño and La Niña.”
German sceptics mock bogus “accelerating” sea level claim
Germany’s online auto-reporter.net expresses doubts about the coming climate catastrophe, citing that back in the 1980s Germans were projecting the end of the forests due to acid rain. 25 years later the forests are as healthy as they have ever been.  Auto.reporter.net questions the supposed sea level rise:
It is supposed to be rising rapidly and submerging many countries. Now scientists have determined that sea level is sinking. […] The causes have yet to be determined. Scientists had expected a continuous increase.
What can we learn from that? That scientists can never exactly know what is happening. And this is the case concerning alleged man-made climate change. It is foolhardy when people think they can impact the climate over 100 years. The political target of limiting the temperature increase to 2°C  is haphazardly selected. […]  We’ll probably laugh about the climate change discussion in 20 or 30 years just as we laugh today over forest die-off, which in reality never came to pass.”
Finally German science publicist Dirk Maxeiner here simply could not contain his urge to mock the alarmists:
Global sea level has dropped by more than half a centimetre over the last 12 months. That equals 5 metres of sea level drop over the next 1000 years – at least that’s what my computer simulation shows. Now how on earth are the island states supposed to cope with all this expanding land? What a catastrophe! We have to immediately form a special commission charged with the task of managing the great transformation of these regions and setting down ecological guidelines. Professor Schellnhuber – it’s up to you!”
Share this...FacebookTwitter "
"Lizards from the deserts of Australia to the tops of mountains in Costa Rica have given us insights into how animals take advantage of their environment to be less cold-blooded.  Lizards seek out sunny patches or the warm underside of rocks where they can soak up the heat to enhance digestion or run faster.  When it gets too hot, they can escape the heat by finding shade or retreating to burrows underground.  In particular, tropical species, including lizards, are thought to be especially vulnerable to climate warming because they  already live at temperatures that can be dangerous. Without the sweat glands or metabolic control that mammals take for granted, lizards can heat up very quickly if they find themselves caught out in the sun for too long.   Species living in the tropics are also thought to have behavioural adaptations that are finely tuned to stable and predictable weather regimes, such as daily activity rhythms. Such behaviours that may be ill-suited to the increasing variability that is predicted with climate change where flexibility may be an asset.   In a world of greater climatic extremes, lizards may over-expose themselves to dangerous temperatures, or may find themselves with only a few opportunities to feed or find mates if their activity patterns are constrained to a particular window in temperatures. This question of how animals might respond to a warmer and more variable world is the focus of a new study aimed at understanding how evolution might come into play as our climate changes. Are species’ current tolerances and behaviours fixed – or can we expect scope for rapid evolutionary change through adaptation? Will some animals be saved by evolution? Michael Logan from Dartmouth College in the US conducted a clever experiment with his colleagues to test whether tropical lizards have the potential to change their physiology over generations to better adapt to a warmer environment, but also one that is less predictable.   The authors moved a population of brown anole lizards (Anolis sagrei) from a forested site in the Bahamas to a nearby open peninsula where daily temperatures on the ground were more than 2°C higher than the lizards were used to. They found that in a warmer and more variable climate, those lizards that survived functioned better in the heat, were fast and were also active over a broad range of temperatures.  The authors conclude that a new environment rapidly selected the lizards that were best suited to survival. They expect subsequent generations of peninsular lizards will continue with hard-wired evolutionary changes in their physiology and behaviour – eventually a new form of the species may emerge, tailored to towards the hotter peninsula.   Even so, the authors do not distinguish whether the shifts in the characteristics of the peninsular lizards are genetically based, which would be a prerequisite for evolutionary change – the traits need to be heritable and passed on to subsequent generations. Indeed, there isn’t much evidence for genetic change in response to climate change. Yet animals as diverse as pink salmon and soil mites have shown rapid evolutionary changes can occur in just a few generations, as opposed to the typical view that evolution takes hundreds of years to manifest itself. Yet even though evolution may rescue some species as the world warms, we don’t factor it in to our predictions of which plants and animals will be the most vulnerable to climate change. There are some obvious reasons for this.  While “rapid” evolution is possible, it still takes a while to unfold – longer than the duration of typical research grants and PhD programmes. For some slow-reproducing species, such as those from cold polar environments or large mammals, it may take decades or more to observe. Only the most patient evolutionary biologists would devote their life to investigating how each generation of elephants has further adapted to climate change. The capacity for evolutionary change is also tricky to predict as so much depends on context. Some populations of a particular species will contain individuals with certain characteristics – the capacity to tolerate extreme heat, say – that will allow those individuals best suited to a new environment to survive and the population to carry on, as seems to be the case in the brown anole lizards experiment. Other populations, without these adaptable individuals, will simply die out. We presently face the most exceptional extinction rates of modern times.  Rapid environmental change is already outpacing the capacity for many species to adapt and survive, but certainly some will beat climate change. One of the big questions this research poses relates to conservation: which species can we best assist through establishing new populations and supplementing declining populations with measures such as artificial breeding programmes? But this may be too narrow a focus. These fast-adapting lizards show that evolutionary change itself could yet be put to good use in conservation. While efforts have focused on saving threatened creatures by moving populations to safer places or trying to preserve their habitats, this study shows that moving them to more extreme environments can pre-adapt populations to a warmer world. Crucially, it suggests another tool to help identify those species that will have a chance under climate change and opens the possibility that we could give some species an adaptation head start."
"
Today while looking for something else I came across an interesting web page on the National Climatic Data Center Server that showed a study from 2002
A continuous multimillennial ring-width chronology in Yamal, northwestern Siberia (PDF) by Rashit M. Hantemirov and Stepan G. Shiyatov
That study was tremendously well done, with over 2000 cores, seemed pretty germane to the issues of paleodendroclimatology we’ve been discussing as of late. Jeff Id touched on it breifly at the Air Vent in Circling Yamal – delinquent treering records?
A WUWT readers know, the Briffa tree ring data that purports to show a “hockey stick” of warming in the late 20th century has now become highly suspect, and appears to have been the result of hand selected trees as opposed to using the larger data set available for the region.
OK,  first the obligatory Briffa (Hadley Climate Research Unit) tree ring data versus Steve McIntyre’s plot of the recently available Schweingruber data from the same region.
Red = Briffa's 12 hand picked trees   Black = the other dataset NOT used
The Hantemirov- Shiyatov (HS) tree ring data that I downloaded from the NCDC is available from their FTP server here. I simply downloaded it and plotted it from the present back to the year 0AD (even though it extends much further back to the year 2067 BC) so that it would have a similar x scale to the Briffa data plot above for easy comparison. I also plotted a polynomial curve fit to the data to illustrate trend slope, plus a 30 year running average since 30 years is our currently accepted period for climate analysis.
Compare it to the Briffa (CRU) data above. 
Click for larger image
When I first saw this plot, I thought I had done something wrong. It was, well, just too flat. But I double checked my data import, the plot, the tools used to plot, and the output by running it 2 more times from scratch. Then I had Jeff Id over at the air vent take a look at it. He concurs that I’ve plotted the data correctly.
The trend is flat as road kill for the past 2000 years, though it does show an ever so slight cooling.
So the next task was to look at more recent times. Here’s the last 200 years of the data:
Zoomed to last 200 years - click for larger image
Still flat as road kill.
Finally, since Tom P made a big deal out of the late 20th century with his analysis where he made the mistake of combining two data sets that had different end points, I thought I’d show the late 20th century also:
Zoomed to last 50 years - click for larger image
Still flat.
Note that in the graph done by Steve McIntyre showing both Briffa and Schweingruber data, both of those data sets are also quite flat until we get into the late 20th century. So out of the 3 data sets we’ve looked at, the Briffa data, the data kept hidden for almost 10 years,  is the only one that shows any propensity for sudden 20th century warming.
But don’t take my word for it that this record is so flat. Look at the authors results. Their results seem identical to what I’ve plotted. Here is the last 2000 years of data charted taken from their paper:

Figure 8 Reconstructed southern Yamal mean June–July temperature anomalies relative to mean of the full reconstructed series.
But for those that want more close up views, I’ve done some additional graphs. Since the authors used a 50 year window in one of their graphs I did the same. I also changed the Y scale to show a zoomed in +/- 0.3°C as the range rather than the +/- 4.0°C the authors used in the plot above. Some details begin to emerge, but once again the trend is essentially flat, and slightly negative.
Click for a larger image
And here are the last 200 years zoomed
Click for a larger image
The period around 1800 was warmer than the late 20th century according to the data viewed this way, but we can see that slight rise in temperature for the 20th century. However compared to the rest of the Yamal HS data record it appears insignificant.
The authors insist that this wood contains a valid climatological record.
Holocene deposits in the southern Yamal Peninsula contain a large amount of subfossil tree remains: tree trunks, roots and branches. This is the result of intensive accumulation and the good preservation of buried wood in the permafrost. The occurrence of this material in the present-day tundra zone of the Yamal Peninsula was described for the first time by Zhitkov (1913). Later, Tikhomirov (1941) showed that, on the evidence of remains of trees preserved in peat, during the warmest period of the Holocene, the northern tree-line reached the central region of the Yamal Peninsula (up to 70°N), whereas today the polar timberline passes through the southernmost part of the peninsula at a latitude of 67°309 N.

By 1964, attention had been drawn to the potential significance of Yamal subfossil wood for reconstructing climatic and other natural processes over many thousand years, as a result of  fieldwork carried out within the valley of the Khadytayakha River in the southern part of the Yamal Peninsula (Shiyatov and Surkov, 1990).
I was impressed with the amount of field work that went into this paper. The authors write:
We travelled by helicopter to the upper reaches of the river to be sampled. Small boats were then used for locating and collecting cross-sections from wood exposed along the riverbanks. It was also possible, when going with the stream, to explore the nearest lakes. 
The best-preserved material from an individual tree is usually found at the base of the trunk, near to the roots. However, many of these remains are radially cracked and it is necessary to tie cross-sections, cut from these trunks or roots, using aluminum wire before sawing. This wire is left in place afterwards as the sections are air-dried.
Here’s how they got many of the tree samples using a rubber boat:

And here is how they sum up the last 2000 years from a tree line analysis they did:
From the beginning of the first century bc to about the start of the sixth century ad, generally warm conditions prevailed. Then began a quasi 400-year oscillation of temperature, cooling occurring in about 550–700, 950–1100, 1350–1500 and 1700–1900. Warming occurred in the intermediate periods and during the twentieth century. The more northerly tree-line suggests that the most favourable conditions during the last two millennia apparently occurred at around ad 500 and during the period 1200–1300. It is interesting to note that the current position of the tree-line in Yamal is south of the position it has attained during most of the last three and a half millennia, and it may well be that it has not yet shifted fully in response to the warming of the last century.
Interestingly while the authors note some warming in the last century, they don’t draw a lot of attention to it, or refer to it as being “unprecedented” in any way. There’s no graphs of nor mention of “hockey stocks” either.
Here’s the link to the source data:
ftp://ftp.ncdc.noaa.gov/pub/data/paleo/treering/reconstructions/asia/russia/yamal_2002.txt
Feel free to make some plots of your own.
===
UPDATE: While I had originally surmised this data supported Steve McIntyre’s recent findings with respect to Briffa, Steve notes in comments that the methodology is different between the two data sets:
Steve McIntyre: I’ve made MANY references to Hantemirov and Shiyatov 2002 in my posts on Yamal. In my first post on Yamal after getting access to the data, I discussed the Hantemirov and Shiyatov 2002 reconstruction as archived at NCDC see http://www.climateaudit.org/?p=7142
In that post, I observed that the standardization method used in H and S 2002 was different than Briffa 2000, that the H and S method would be unable to recover centennial scale variability and that it was not relevant to the issues at hand.
The H and S reconstruction does not “support” my point in respect to Yamal. It’s irrelevant to it.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9285497b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Bamboo, a common grass which can be harder to pull apart than steel, has the potential to revolutionise building construction throughout the world. But that’s not all. As a raw material found predominantly in the developing world, without a pre-existing industrial infrastructure built to skew things towards the rich world, bamboo has the potential to completely shift international economic relations.  The past century has seen an unprecedented transfer of products and predefined solutions – instead of capacity-building programs – from the rich countries to poor, under the rubric of “development aid”. The economic incentives for the former are obvious: when developed nations introduce, for example, their reinforced concrete technology to developing nations, those countries must also acquire the proper machinery, the technical expertise to maintain them, and the building materials suitable for those machines, and they must buy all of those things from the developed countries.  This divides our planet between those who produce goods and services, and those who are meant just to consume. Unless new materials, developed from the resources available in developing territories, enter the market, the system will remain the same. Bamboo could be the material which turns this relationship on its head.  For an example of the exploitative trade system currently in place, look no further than steel. Steel-reinforced concrete is the most common building material in the world, and developing countries use close to 90% of the world’s cement and 80% of its steel. However, very few of these nations have the ability or resources to produce their own steel or cement, forcing them into an exploitative import relationship with the developed world. Out of 54 African nations, for instance, only two are serious steel producers. The other 52 countries all compete in the global marketplace for this ever-more-expensive, seemingly irreplaceable material. But steel is not irreplaceable. Bamboo provides a material alternative, and it grows in the tropics, an area that coincides closely with the developing world. One of nature’s most versatile products, bamboo belongs to the botanical family of grasses and is extremely hard to tear apart.  Its strength comes from the way the grass evolved, adapting to natural forces. In contrast to wood, the bamboo culm or haulm – botanical terms for the stem of a grass – is thin and hollow. This allows it to move with the wind, unlike a tree, which tries to simply withstand any natural forces it is exposed to. This adaptation for flexible movement required nature to come up with a very light but tension-resistant fibre in the bamboo culm which is able to bend in extreme ways without breaking. Bamboo is harder to pull apart than timber or even reinforced steel. Bamboo is also a highly renewable and eco-friendly material. It grows much faster than wood, and is easy to obtain in great quantity. It is also known for its unrivalled capacity to capture carbon and could therefore play an important role in reducing carbon emissions worldwide – another advantage for developing nations in light of the trade in carbon emission certificates. Simply from an economic perspective, most developing nations should be interested in the material. It could strengthen local economies and lower dependency on international markets. The great social, economic, and material benefits of bamboo and its widespread availability are not reflected in the demand for the material, however. Despite its strengths, bamboo has a number of weaknesses as a construction material. Water absorption, swelling and shrinking behaviour, limited durability, and vulnerability to fungal attacks have limited most applications of bamboo so far.  Today, bamboo usage is generally limited to being a structural component in regions where it reflects local architectural traditions; early attempts to use it as an untreated, non-composite reinforcement material in concrete were not successful.  But bamboo fibre could be extracted and combined with other materials to create a composite, harnessing its natural strengths as part of a viable building material, an alternative to steel and timber. Indeed, this is exactly what researchers at ETH Singapore’s Future Cities Laboratory are working on.  There are approximately 1,400 known species of bamboo, which comes in all sorts of shapes, sizes and strengths. Using new technologies, we’re studying which bamboo species are most suited for usage in construction and how we can overcome some of its limitations by combining bamboo with adhesive matter.  Bamboo composite material can be produced in any of the familiar shapes and forms in which steel and timber are produced. Like them, the material can be used to build wall structures for houses or any other buildings. More interestingly, it can be used for specific applications that best take advantage of the material’s tensile strength, such as reinforcement systems in concrete or beams for ceilings and roof structures. Today, bamboo costs less than a quarter as much, by weight, as steel reinforcement. And because steel is 15 times denser than natural bamboo, the figures by volume are even more extreme. In South-East Asia alone, there is enough bamboo already in cultivation to meet equivalent demand for construction steel 25 times over. Bamboo largely grows in developing countries which, with this new technology, could potentially develop substantial value chains. Farmers, collection centres, distributors and, finally, production facilities could form a strong economic power – so long as the bamboo is not simply exported as a raw material.  Developing countries must develop and sustain knowledge and industrial know-how in order to strengthen their economic capacities. The production of a high-strength building material could establish strong new rural-urban linkages and create an alternative source of revenue for farmers. Expanding cultivation would help farmers in other ways, too; due to its fast growth, bamboo can secure open soil and protect it against erosion. Being a grass, bamboo also keeps the water table high and therefore improves the productivity of adjacent fields planted with food crops.  Bamboo could play an important role not only as a traditional construction resource but also as the major component of an industrialised product, enabling the creation of a “smoke-free” industry in developing nations."
"A dozen countries have called for an EU climate target for 2030 to be drawn up “as soon as possible”, if the bloc is to galvanise the rest of the world before vital UN talks in Glasgow later this year. In a letter to the EU’s top official on climate action, Frans Timmermans, the dozen EU member states say “the EU can lead by example and contribute to creating the international momentum needed for all parties to scale up their ambition” by adopting a 2030 EU greenhouse gas emissions reduction target “as soon as possible and by June 2020 at the latest”.  This year’s UN talks in Glasgow are crucial, as the world is far adrift of goals set at the landmark 2015 Paris conference, including the aspiration to limit global heating to 1.5C above pre-industrial levels. Even half a degree higher will significantly increase the risks of drought, floods, extreme heat and poverty for millions of people. The letter piles pressure on Timmermans, who is due to unveil the EU’s long-awaited climate law on Wednesday. A leaked draft of the law shows Timmermans’ plans to propose an EU-wide 2030 target by September. The target would probably be an emissions reduction of 50-55% compared with 1990 levels, which green activists say is not enough to guarantee meeting the EU’s goal of net-zero emissions by 2050. EU officials think a couple of months’ difference in proposing the target makes little difference, but would allow them to bring onboard more reluctant countries, including Poland, which has not yet signed up to the EU-wide goal of net zero emissions by mid-century. The climate and environment ministers argue timing is crucial, as they want the EU to have a 2030 target, before an EU-China summit in September and well ahead of Glasgow climate talks in November. “No other major economy is prepared to take the lead to ensure an ambitious implementation of the Paris agreement,” they write. Wendel Trio, the director of the Climate Action Network Europe, said: “By proposing a 2030 target increase only in September, the commission will give member states no time to reach an agreement by Cop26 in November, the international deadline by which all countries must commit to new, ambitious climate pledges for 2030. The EU needs to have its own house in order, and quickly to push other countries to make substantial contributions well before the deadline.” The letter, organised by Denmark, was also signed by France, Italy, Spain and the Netherlands. Only two signatories – Slovenia and Latvia – are central and eastern countries that joined the EU after 2004. Germany is conspicuous by its absence. The EU climate law is the centrepiece of the European Green Deal, which aims to transform Europe’s economy to confront the climate emergency. The law could set Brussels on a collision course with populist governments in Poland, Hungary and the Czech Republic, which have been the slowest moving on the climate emergency. Putting the 2050 net zero target into law means Poland could be outvoted if it continued to refuse to sign up. The European Green Deal is the EU’s answer to what the European commission’s new president, Ursula von der Leyen, called the “existential issue” of the climate emergency. Most EU countries have signed up to goal of a climate neutral EU by 2050, a goal demanding dramatic change in energy use, farming, housing, transport, trade and diplomacy. The European Green Deal is only the start of the journey: laws will have to be drafted and agreed by EU ministers and MEPs; money will have to be raised; plans will have to be implemented. It is a route map, rather than a destination, and time is running out. Under the law, Brussels would also be able to take a government lagging behind on its climate target to the European court of justice, which can issue daily fines for failure to uphold EU law. Poland, which generates 80% of its electricity from coal, is seeking EU funds to help wean its economy from fossil fuels. The European commission has proposed a €100m (£87m) “just transition” fund to help countries with coal mining jobs adjust to a green economy, but Warsaw has yet to come onboard. Climate activists have accused the commission of lacking ambition, as the climate law gives scant detail on how the EU will meet the 2050 net zero target, either by phasing out fossil fuel subsidies, reforming the EU’s common agricultural policy or regulating industry. “With no 2030 climate target and no measures to end subsidies for fossil fuels, industrial farming and other destructive industries, the commission has left a big hole in what’s meant to be the flagship of the European Green Deal,” said Sebastian Mang, a climate and energy policy adviser at Greenpeace. The Swedish climate activist Greta Thunberg is likely to give her verdict on the EU climate plans when she meets MEPs on Wednesday. She will also lead a “European strike” in Brussels on Friday with Belgian campaigners."
"**Testing has uncovered 106 cases of Covid-19 at a Cornwall meat processing factory.**
Cornwall Council said the employees of Kepak in Bodmin - of which 80-85% were asymptomatic - were now self-isolating at home.
In total, 271 staff members were tested over the past week after a small number initially reported positive tests.
Rachel Wigglesworth, the council's director of public health, said the situation would be monitored closely.
She said: ""Outbreaks such as this are always tough for everyone involved, but we have worked with Kepak and our health service partners in taking quick and robust action to help stop the virus spreading.""
The council said testing was continuing at the site and anyone identified as a close contact linked to the positive cases would be contacted by NHS Test and Trace.
A spokesperson for The Kepak Group said it was providing support and advice for staff who needed to self-isolate and was working closely with public health teams.
They said : ""The Group is working tirelessly to protect its staff as well as ensuring the continuity of secure food supply during this pandemic."""
"**Officials at the Department for the Economy believed wind turbines ""should be eligible"" for coronavirus payments.**
Fifty-two wind turbine owners in Northern Ireland received emergency coronavirus funding from Stormont.
The sector was subsequently ruled ineligible, after more than Â£500,000 was paid.
The Department of Economy said once new information came to light, ""eligibility was reconsidered"" to protect the public purse.
New papers obtained by the BBC Nolan Show reveal that officials ""could not decide"" on a way to withhold payments from owners already receiving the lucrative green energy subsidy.
The documents suggest that weeks after the initial 52 wind turbine payments were made in April, a further 328 payments were still on the table.
They suggest that the cost of the payments would have been over 10 times greater if officials from Land and Property Services (LPS) had not intervened.
On 5 May, the Department for the Economy told Land and Property Services that wind turbines should be eligible for the Â£10,000 Covid grants.
Keith Forster, director of the department's strategic policy division, said: ""We discussed this issue at length, and we could not decide on an approach that would enable us to meaningfully withhold payment to any business that meets the criteria of the scheme.
""On this basis, wind turbines registered as receiving small business rates relief should be eligible for the scheme.""
Ian Snowden, LPS chief executive, later warned the department that LPS had found ""another 300 previously unrated properties, all of them wind turbines.""
He was concerned about a potential payment of Â£6.28 million for wind turbine owners. At this stage, LPS was already withholding 328 turbine payments.
In a meeting with Department for the Economy officials on 19 May, LPS raised further concerns.
Minutes of the meeting state that ""many wind turbines are registered as individual limited companies, however deeper analysis of the initial assessment estimated that the maximum payments that could potentially be received by a director of multiple wind turbine companies would be between Â£40k- Â£80k"".
On 12 June, the Department for the Economy instructed LPS not to issue the payments.
Finance committee member Jim Wells MLA said the Department for the Economy initially made the wrong call on this issue.
""The Audit Office report indicated that due to the very high levels of subsidy, the owners of wind turbines did not need or deserve any form of financial aid.
""There should have been no question that they warranted Small Business Rates Relief or the coronavirus grants"".
He said payments should stop immediately, creating a ""windfall"" of extra money that ""could be put to good use elsewhere'"".
The Department of Economy said the initial decision on wind turbines ""was considered on the baseline eligibility criteria"".
A spokesperson for the department said: ""Decisions were made in the context of available information and when new information came to light, decisions were made on that basis.
""Once full details came to light, in regards wind turbines, the policy position regarding eligibility was reconsidered in order to protect the public purse.""
BBC's Nolan show also revealed that almost 2% of businesses in receipt of the small business rates relief scheme are wind turbine owners.
This cost the taxpayer Â£310,000 in 2020/21. Wind turbines have been receiving the rates discount since 2010, with the annual cost rising from Â£178,035 in 2015.
Many wind turbines in NI are already receiving ""excessive' subsidies"", according to the Audit Office.
The Department of Finance say the small business rate relief scheme for 2020/21 was approved without change by the Executive and Finance Committee."
"**There have been more than 1.6 million confirmed cases of coronavirus in the UK and about 60,000 people have died, government figures show.**
However, these figures include only people who have died within 28 days of testing positive for coronavirus and other measures suggest the number of deaths is higher.
**Find out how the pandemic has affected your area and how it compares with the national average:**
If you can't see the look-up click here.
After the first peak in April, cases started rising again in July, with the rate of growth increasing sharply in September and October, before falling again in the past two weeks.
On Wednesday, the government announced a further 16,170 confirmed cases.
It is thought the infection rate was much higher during the first peak in spring, but testing capacity at the time was too limited to detect the true number of daily cases.
The data for cases can also be broken down by region and comparing the change in those figures by week gives a sense of where there has been a recent increase in newly-reported infections.
Coronavirus infections in England have fallen by about a third during lockdown, according to a major study carried out by Imperial College London.
While some of the worst-hit areas saw the biggest improvements, cases have remained relatively high across England, according to the React-1 study, which is based on tests of more than 100,000 people between 13-24 November.
The latest figures from the Office for National Statistics (ONS), suggest about one in 185 (16,400 people) in Wales had the virus in the week ending 21 November.
In Northern Ireland rates are thought to be decreasing at around one in 145 people (about 12,700 people), while in Scotland, the figure was one in 115 (about 45,700 people).
The average number of daily deaths began to rise again in September, following the first peak in April.
On Wednesday, the government announced a further 648 deaths.
Of these, 555 deaths were in England, 51 in Wales, 38 in Scotland, and four in Northern Ireland.
Rules were amended over the summer to include deaths in the coronavirus total only if they occurred within 28 days of a positive test. Previously in England, all deaths after a positive test were included.
England has seen the majority of UK deaths from Covid-19. Using the 28-day cut-off, there have been more than 52,000.
Although hospital admissions for Covid-19 remain below the levels seen in the spring, there are big regional disparities.
The North West, North East and Yorkshire, and Midlands have seen the highest number of admissions but the situation in all three now appears to be improving.
Cases have risen across large parts of England, with other spikes in areas of Scotland, Wales and Northern Ireland.
The red areas on the map below are those currently seeing the highest number of cases per 100,000 people.
Restrictions have been tightened across the UK in an effort to tackle the number of rising cases.
In England, each local authority has been placed in one of the new three tiers following the end of the recent national lockdown.
Wales has announced a range of further measures, including a ban on pubs, restaurants and cafes serving alcohol, from Friday 4 December.
In Northern Ireland, a further two-week ""circuit-break"" lockdown began on 27 November.
Scotland has a five-tier system of alert levels with different measures in place in different parts of the country.
UK leaders have agreed to allow up to three households to meet indoors during a five-day Christmas period of 23-27 December.
When looking at the overall death toll from coronavirus, official figures count deaths in three different ways.
Government figures count people who died within 28 days of testing positive for coronavirus.
But there are two other measures.
The first includes all deaths where coronavirus was mentioned on the death certificate, even if the person had not been tested for the virus. The most recent figures suggest there had been almost 70,000 deaths by 20 November.
The second measure counts all deaths over and above the usual number at the time of year - that figure was more than 75,000 by 20 November.
The most recent figures available from the ONS are for the third week of November, which show there were 14,276 deaths registered in the UK.
Some 3,038 of these deaths involved Covid-19 - 199 more than the week before.
Deaths normally do rise at this time of the year, but the data from the ONS and its counterparts in Scotland and Northern Ireland show the second wave of the virus has pushed the death rate above the average seen over the past five years by about 21%.
Overall, the figures are still well below the peak of 9,495 deaths recorded in a week, reached on 17 April.
The ""R number"" is the average number of people an infected person will pass the disease on to.
If R is below one, then the number of people contracting the disease will fall; if it is above one, the number will grow.
The government's current estimate for the R number across the whole of the UK is 0.9 to 1.
The estimate for England is 0.9 to 1, while for Scotland it is 0.8 to 1. The estimate for Wales is 0.8 to 1 and in Northern Ireland it is 0.9 to 1.1.
The government has said in the past that the R number is one of the most important factors in making policy decisions."
"

It’s a fact that TV meteorologists tend to be a bit more skeptical about the upcoming end of the world than some federal climatologists. One needs only to look at the bare ground in our Nation’s Capital to understand why.



Washington just witnessed the biggest busting of a major snow forecast in a long, long time; in fact, back to November 11, 1987, when double digits fell out of a sky that was supposed to drop maybe an inch. In the recent case, what would have been a record‐​or near‐​record snow was forecast by virtually everyone and for very good reasons.



It’s important to define a busted forecast. That’s not one where the forecaster changes his or her mind based upon new information prior to an event. To qualify as a disastrous forecast, a major event has to be predicted and held on to even as it fails to materialize.



Big busts are rare because our computer models are so darned good, lead by the flashy Euro, run twice a day out of the United Kingdom, which has developed a reputation for nailing disruptive Atlantic Coast cyclones days ahead of time. One of those was named Sandy, as it transitioned from a large but not particularly intense hurricane into a large and particularly vicious coastal storm. The U.S. models, especially the newer SREF (Short Range Ensemble Forecast) aren’t too shabby either, especially compared to what was online during the 1987 bust.





Our “best science” can be wrong.



The recent disaster also occurred on the watch of a collection of some of the world’s best forecasters, who, for a variety of reasons, are clustered in Washington.



Led by our modern computer output, all of their forecasts failed. The list includes Bob Ryan, a past‐​president of the 14,000-strong American Meteorological Society, who works for the local ABC outlet. Also included would be Jason Samenow who runs the very popular www​.cap​i​tal​weath​er​.com at the _Washington Post_ , and enjoys cult status in DC. When he walked into my classroom at UVA, I felt like Roy Hobbs’ (“The Natural”) high school coach must have.



The new head of the National Weather Service is the legendary Louis Uccellini. If young Samenow is The Natural, Louis is the Godfather, and his specialty is—wait for it—the Mid‐​Atlantic snowstorm. In fact, he and Paul Kocin literally wrote the book on them.



I’m pretty sure if Uccellini thought his troops were making a mistake on what was going to be a very public storm, he’d probably call someone and have a chat.



They had every reason to hold onto the snow forecast. Precipitation was moving in from the Southwest. As forecast, what began as rain in central Virginia transitioned over to snow in a couple of hours; even a bit earlier than it was supposed to. Charlottesville, about 80 miles southwest of DC, eventually racked up over 14 inches. The central Shenandoah Valley, thirty miles west of there, hit 20.



At 11:30pm on March 5, it began to rain in Washington, D.C. itself. Within a mere 20 minutes, it switched over to snow as the temperature began to drop through the upper 30s. The ground began to whiten. As the precipitation intensified and the low pressure system became more intense, basic physics said more cooling was on the way. Let the Big One begin!



Congratulatory emails went flying, and beer cans popped forth. Most forecasters went to bed expecting to wake to a winter wonderland.



My thermometer showed 35° and dropping. I set it to alarm at 37°, on the crazy, impossible, off‐​chance that something would go terribly awry. It went off two hours later, signaling that the bust had begun.



What happened is hardly an indictment, but rather a statement of the human condition. Our “best science” can be wrong.



So, in summary, not only were the best models and the best forecasters largely in agreement (there was one exception: the sophisticated Euro was somehow missing the heaps of snow that were already piling up down in Virginia), a _ll_ evidence indicated that the snowstorm was unfolding as planned. See any parallels with global warming? Sophisticated climate models and highly trained scientists, and a smart warming that began around 1975.



Oh, the lack of any warming for seventeen years now? My climate alarm just went off.
"
"Bovine tuberculosis (TB) is expected to cost British taxpayers nearly £100m in 2014. Scientific evidence is a vital weapon in the fight to protect cattle from TB. Why, then, has the government just fought and won a legal battle to avoid consulting independent scientists on its most high-profile TB control effort? Wild badgers play a role in transmitting TB to cattle, and culling badgers seems an obvious solution. A new round of badger culls is about to start, but it is risky . A complex interaction between badger behaviour and TB transmission means that the results of culling could, depending on various factors, increase TB levels, instead of reducing them. To add to that, badger culling is expensive.  This is why, in 2013, the government started a pilot that it hoped would be give them a cheap and effective way to control cattle TB. Farmers, rather than government, would pay for the culling. And, rather than being cage-trapped, badgers would be shot in the wild.  This pilot was started in just two areas – and for good reason: the whole approach was untested, and the stakes were high. Marksmen shooting at night might endanger public safety. Shooting free-ranging badgers might cause suffering. And, worst of all for the aims of the approach, failing to kill enough badgers, fast enough, would worsen the cattle TB situation that the culls were intended to control. In the face of such uncertainty, the government adopted a commonly used approach. It appointed an Independent Expert Panel to assess the safety, humaneness and effectiveness of the pilot project. The expectation was that this panel’s conclusions would reflect scientific evidence, whether or not they supported government policy. The Independent Expert Panel found that farmer-led culling was far from effective. Tasked with killing at least 70% of the local badgers within a six-week period, cull teams only managed to kill between 28% and 48%. Culling periods were extended, but still the total kill rose to only something between 31% and 56%, according to government figures. Unless more badgers could be killed, and faster, farmer-led culling risked worsening the problem it was intended to solve. The 2013 culls also failed to meet their targets for animal welfare. Between 7.4% and 22.8% of badgers were still alive five minutes after being shot and were assumed to have experienced “marked pain”. Despite facing these failures, the government decided to repeat culls in the same areas in 2014. If effectiveness and humaneness could be improved sufficiently, culling might be extended to more areas in 2015. If not, the government might need to reconsider their policy. One would think, then, that measuring effectiveness and humaneness would be a central goal of 2014’s culls. The Independent Expert Panel, together with government scientists, selected the most accurate and precise ways to estimate the effectiveness and humaneness of the 2013 culls. Measuring effectiveness is challenging because – being nocturnal and shy – badgers are hard to count. The panel overcame this problem by using genetic “fingerprints” to identify badgers from hair snagged on barbed wire. They measured humaneness primarily through independent observers recording the time that shot badgers took to die. The panel recommended that the same approaches be used for subsequent culls. But the government rejected this recommendation. This year there will be no attempt to count badgers in the cull areas, either before or after the culls. The time badgers take to die will not be recorded. There will be no oversight by independent scientists. Instead, the effectiveness of the culls which start tonight will be judged using a method so utterly inadequate it was barely considered in 2013. Key data will be collected by marksmen themselves: people with a vested interest in the cull being designated “effective” and “humane”, who in 2013 collected data so unreliable it was considered unusable by the panel. Available information suggests that any future claim that the 2014 culls have reduced badger numbers sufficiently to control TB will be completely baseless. Why the change in approach? Government cites cost, and hired some expensive lawyers to defend its position when the Badger Trust sought, and eventually lost, a judicial review of the decision to scrap independent scientific oversight of this year’s culls. Yet the cost of pushing forward with an ineffective culling policy would far outweigh the cost of properly assessing effectiveness and humaneness. Government has repeatedly referred to its programme of badger culling as “science-led”. One would expect a science-led policy to entail gathering reliable information on management outcomes, and using this and other evidence to inform future decisions. Choosing – against formal expert advice – to collect inconsistent, inadequate and potentially biased data is an insult to evidence-based policymaking. When ineffective culling can make a bad situation worse, failing to collect the evidence needed to evaluate future policy fails farmers, taxpayers and wildlife. Next, read this: Cattle herd model reveals best ways to halt spread of TB – and a badger cull isn’t one of them"
"By some strange quirk of fate, it is exactly 12 years to the day since I, alongside fellow climate activists, climbed on to the roof of the House of Commons to protest against plans for a third runway at Heathrow. Today’s high court judgment is a vindication of everything climate activists have been saying for more than a decade: Britain cannot honour its national commitment to tackle climate change at the same time as building a new runway at one of the busiest airports in the world. To be precise, the court did not quite say this. It ruled that ministers’ failure to take the UK’s climate change commitments into account rendered the Airports National Policy Statement (ANPS) – which effectively gave the green light to a third runway – unlawful. In order to be lawful, the ANPS would have to be rewritten to include a credible plan for squaring expansion with our commitment under the Paris Agreement to seek to limit global temperature rise to no more than 1.5C. The court was careful to clarify that it has no opinion on whether or not this is possible.  As someone who has been fighting these plans for 15 years I can confirm that it isn’t. If the third runway is built, there are only three possible outcomes: either we will fail on climate change; or we will have to constrain capacity elsewhere, roughly equivalent to closing Manchester airport; or we simply won’t be able to use the new runway at Heathrow, making it one of the most expensive white elephants in history. It is difficult to overstate the significance of this decision. Heathrow airport is a bastion of the global fossil fuel economy, so the symbolism alone of this defeat will resonate loudly around the world, giving courage to the movement fighting for a livable future – while striking fear in the hearts of the corporate fossil interests still determined to profit while the planet burns. It also sends a clear and timely message in advance of the UK hosting the most important UN climate summit since Paris, Cop26: Britain is prepared to lead the world in tackling the climate crisis. Scrapping Heathrow expansion is a surprise gift to climate diplomacy. But the mechanics of this decision could be even more important for the climate struggle. A British court has ruled, quite sensibly, that domestic policy decisions must be assessed against their impact on the UK’s ability to fulfil commitments under the Paris Agreement. The British judicial system remains incredibly influential globally, with courts around the world modelled on our own, so this means any high-carbon infrastructure project – from motorways to fracking wells to coal-fired power plants – could potentially now be blocked as unlawful in any of the 195 countries that are signatories to the Paris Agreement. So what happens next? Well, the airport will appeal, but it will lose – because the argument is unwinnable. Expanding Heathrow will have no positive impact on the UK’s economy. Pressure to expand Heathrow has nothing to do with increasing the number of international business flights, which are in sustained decline across all of London’s airports. In reality, the industry’s push for expansion is overwhelmingly about handling ever more international transfer passengers, alongside more and more outbound leisure flights by wealthy frequent flyers from London and the south-east. These are all journeys that cost the UK money rather than bringing it in. My own suspicion is that it may not even get as far as an appeal, as Heathrow’s investors will now get cold feet and find excuses to withdraw from the project. The third runway is dead, and cannot be resuscitated. More widely, today’s judgment marks a turning point in the climate struggle. It looks like the beginning of the British state taking the implications of the climate emergency as seriously as its citizens have now begun to. For UK air travel, this means there is turbulence ahead. We can no longer muddle on with the pretence that ever-increasing demand for flights can be met while also reducing emissions down to zero. The uniquely generous tax breaks that have kept air travel artificially cheap must end, but if the climate movement wants to maintain the support of the wider public, we must do this in the fairest way possible. That means bringing in a frequent flyer levy, which would protect access to some air travel for all, regardless of income, while still keeping aviation emissions within safe limits for the climate. Whether Boris Johnson’s government has the stomach for this kind of medicine remains to be seen. • Leo Murray is a co-founder and director at climate charity Possible"
"
Share this...FacebookTwitterAnd so how many more studies do we need to tell us the obvious?  There are so many studies out there that conclude renewable energy subsidies are a failure, yet you can be sure they will all be ignored by the next IPCC report, which instead will focus on some oddball quack paper by Ottmar Edenhofer.The University of  Witten/Herdecke has put out a press release here. Hat tip: oekowatch.de.
According to the press release, Prof. Dr. André Schmidt has drawn a harsh conclusion on the German EEG feed-in laws for renewable energies.
In his study of the economic and ecological impacts of the EEG Feed-in Act for favouring renewable energies for the Federal Office of Research, Prof. Dr. André Schmidt, economist at the University of Witten/Herdecke, has reached a devastating conclusion: they are counter-productive! “In Europe the feed-in act does not save a single microgram of CO2, subsidises carbon power plants in foreign countries, solar module  manufacturers in China, and so the German solar industry as a result gains no benefits on the market.“
Harsh words, and he has arguments behind them: “Through the EEG Act, power from solar cells has a price that is eight times higher (€ 0.34 /kwh) than conventionally produced power,“ he calculated. And he asked what do we get in return?
Carbon dioxide: When climate gases decrease because of the EEG, then the supply of of salable emission rights also goes down  (if  a functioning trading system indeed exists). ” The biggest polluters at home and abroad can cheaply purchase a free pass instead of thinking about filters.“
Employment: For the 48,000 German jobs (Source: Federal Association of Solar Economics for 2009) subsidies to the tune of €8.4 billion were forked out in 2008. “That comes out to €175,000 per job! When one compares this to coal mining subsidies, which are a relatively modest €75,000 per job, coal looks really good!“, Schmidt grumbles.
Competitiveness: 48% of all solar systems installed in Germany originate from China because German capacity simply cannot meet the demand. The global market share of German companies is at about 15%, and trending down: “When India and Thailand come onto the market soon, we’ll be at 8-10%. Here in Germany companies are investing too little in R&D, productivity advancement is sub-par, sales have stagnated. In 2010 there was a €4.3 billion trade deficit in solar modules.“. In Schmidt’s view, the inflated and guaranteed feed-in rates have paralyzed innovation in this industrial sector.“
In summary, the German EEG feed-in act is a flop.
 
 
Share this...FacebookTwitter "
"From unprecedented bushfires in forests that used to be too wet to burn to warming seas that have killed giant underwater forests, Australia is experiencing the effects of the global climate crisis more rapidly than much of the world. Over the past three weeks, Guardian Australia has told these stories in a major six-part series that was paid for by readers.  The Frontline: inside Australia’s climate emergency has also looked at what happens when towns run out of water, at the health effects of cities and towns being engulfed in smoke for weeks on end, and at extreme heatwaves that are killing people prematurely. On Monday, we publish the final episode in the series, The lost harvest. On Tuesday 3 March, readers will have the chance to ask experts in these fields questions about the series, what the science tells us and the impacts already being felt, in a Frontline live blog running from 10am-3pm. The scientists and professionals taking part are (all times AEDT): 10am-11am: Prof Lesley Hughes, ecologist, distinguished professor of biology and pro-vice-chancellor (research) at Macquarie University, and Climate Councillor. Hughes has expertise on the impact of climate change on species and ecosystems. 11am-12pm: Greg Mullins, former commissioner of Fire and Rescue NSW, volunteer firefighter and Climate Councillor. 12pm-1.30pm: Prof Michael Mann, climatologist, geophysicist and director of the Earth System Science Center at Pennsylvania State University. Mann is currently based in Sydney. 2pm-3pm: Assoc Prof Donna Green, from the University of New South Wales’ climate change research centre. Green has expertise in the health effects of climate change and air pollution. We will be taking questions in advance and during the blog. Please send them to frontline.live@theguardian.com or leave a comment here. The Twitter hashtag for questions and the discussion on Tuesday will be #frontlinelive See you then."
"

I’ve been warning for years of the dangers of the federal Racketeering Influenced and Corrupt Organizations law and how it gives prosecutors and enterprising private lawyers leverage to target above‐​board businesses in search of punishment or profit. Since the law’s passage in 1970, RICO has seldom been used against violent organized crime. Instead, it has been aimed at a wide array of white‐​collar defendants, as William Anderson noted in _Regulation_ six years ago, and especially at unpopular industries like gun and cigarette makers, as Cato’s Bob Levy noted in 2000. The latest fillip is Sen. Sheldon Whitehouse’s proposal to aim racketeering charges against groups that promote wrongful thinking on climate change. The civil side of the statute (“civil RICO”), which can be used in private litigation, is especially susceptible to tactical use by private lawyers who know that the vagueness of the law, the high cost of response, the triple‐​damages provisions, and the racketeering stigma especially are useful in forcing adversaries to the bargaining table. The more those adversaries value respectability, the more powerful the leverage.   
  
  
Now comes word that a Washington, D.C.-based tough‐​on‐​crime group calling itself the Safe Streets Alliance has filed suit seeking, in its words, “to hold those involved with Colorado’s recreational marijuana industry liable under federal racketeering statute and to have Colorado marijuana business licenses held invalid.” Its press release is at least honest enough to acknowledge that the targets include “the citizens of Colorado” for what it believes was their faulty decision to enact Amendment 64 in 2012. In one case SSA, representing a local Holiday Inn franchisee that didn’t care to have a medical marijuana shop near its business, succeeded in forcing owner Jerry Olson (no relation) out of business. A key tactic in the suit — one quite familiar to those of us who follow hardball civil litigation in general — was to name as racketeering co‐​defendants a variety of risk‐​averse, often respectable businesses that had in some way done business with the main target. Thus AP reports:   




…just last week, a bonding company in Des Moines, Iowa, paid $50,000 to get out of the lawsuit.   
  
  
“We are out of the business of bonding marijuana businesses in Colorado and elsewhere until this is settled politically,” said Therese Wielage, spokeswoman for Merchants Bonding Company Mutual.



Thus does the litigation accomplish its goal whether or not it ultimately prevails before a judge:   




“This lawsuit is meant more to have a chilling effect on others than it is to benefit the plaintiffs,” said Adam Wolf, Olson’s lawyer.



SSA lawyer Brian Barnes of Cooper & Kirk doesn’t seem to contradict that:    




“We’re putting a bounty on the heads of anyone doing business with the marijuana industry,” Barnes said.



I’m occasionally asked why I bother to worry about the legal woes of unpopular industries whose goods I don’t even care to consume. A different way to look at the question is that almost anyone’s line of business — whether it be soft drinks or accounting or putting up visitors in one’s home or charitable non‐​profit work or electioneering or employing entry‐​level workers at minimum wage — is one public‐​vilification campaign, or one round of lawsuits, away from becoming an unpopular industry. 
"
"Energy prices are rising, and it hasn’t gone unnoticed that the profits of the handful of large energy supply companies are rising too. While it can be argued that there is no direct causal relationship between the two, there is clearly a case for consumers to have access to better tools that help them access the best tariffs and lower their bills.  In particular, those consumers with energy consumption patterns that are more predictable or desirable for the companies that supply them should be able to demand better prices than others without. The problem is that a single individual customer doesn’t have much in the way of negotiation power in the market. One possible answer is for them to band together into groups, and engage in collective bargaining to get a better deal, a concept called collective energy purchasing. The trend of joining forces to negotiate better prices for their electricity started in continental Europe, particularly in Belgium and the Netherlands, but has gained considerable traction in the UK. The Department for Energy and Climate Change ran a £5m fund in 2012 to provide support to organise such group bargaining.  Schemes such as the Big Switch and the Big Deal provided thousands of consumers with a way to switch, saving up to 25-30% on their annual electricity bill. More recently, several start-ups firms and local initiatives have started offering similar schemes. The process of joining together is typically mediated by a third party, and a large part of it can be automated. For instance, given the adoption of smart meters, it is not hard to imagine a near future in which consumers can simply upload (or provide access to) information about their usage, and a web service can work out which is the optimal tariff and carry out the collective purchasing and switch on their behalf.  It’s a compelling idea, but there’s no guarantee an automatically selected tariff will always be the best choice for all the group’s customers. Buying as a group may provide an optimal result for the group, but this is an averaged result rather than one that applies with respect to each individual customer. Individual customers (or subgroups of customers) may be better off switching individually, or forming their own subgroup around another tariff. This type of phenomena is called coalitional stability, and has been long studied in coalitional game theory, and more recently, in distributed artificial intelligence. Recent research has started using AI techniques to address the challenge of designing more efficient group-buying aggregators. One central issue is modelling how predictable each customer is. Distribution companies must estimate how much electricity their consumers will use and buy long-term, forward contracts. Any shortfall of electricity has to be bought on the spot market or during balancing, typically at a higher price. The converse also holds, in that any electricity bought in excess has to be sold during balancing, usually at a loss. So having accurate data about consumption is important. A “prediction of use” tariff, which asks customers for a prediction of their electricity use (or estimates this from their past consumption records) and charges them accordingly would better match their cost to the supplier based on how predictable their energy use is. Crucially, while each consumer may be unpredictable, grouping them together in a collective reduces their aggregate uncertainty, making their consumption more predictable. In fact, a market could comprise a whole range of these prediction of use tariffs. Some of could be flat, like existing tariffs, where the utility company would carry the risk but charge higher prices. Others would encourage greater predictability from customers, lowering the risk to the supplier who in turn provides a much better price for electricity consumption within the predicted limits (and extra charges for use beyond the predicted amount). Different consumers with different requirements could be dynamically clustered, depending on how well they predict their consumption, with buying groups formed around particular tariffs. The sort of coalitional game theory that can help design software and tools to provide the best tariffs can also divide the bill in the most fair way. On way is the concept of marginal payment, where a customer pays the difference between what the group pays including him or her, and what the group would pay if he or she were not a member. It’s conceivable that better artificial intelligence techniques can help us provide incentives for people to form groups. A well known problem in electricity group buying is that people are reluctant to commit until a critical mass is reached. So marginal payments could depend not only on ease of predicting consumption but also on how early a member joined their collective purchasing group. Used wisely, such collective schemes can raise consumers’ awareness of their energy usage, lowering overall energy consumption, leading to less carbon emissions, lower costs for supplier and consumer, and less wastage. "
"
Share this...FacebookTwitterI think I’ve found the root of Joe Romm’s problem. He needs to go back to school and learn more maths and natural sciences! At least that’s what a recent Yale University study shows.Somehow this paper got by me. Maybe this is old news, and so forgive me if this is already known. It’s nothing you’d hear about from the “enlightened” media, after all.
Recall how climate alarmists always try to portray skeptics as ignorant, close-minded flat-earthers who lack sufficient education to understand even the basics of the science, and if it wasn’t for them, the world could start taking the necessary steps to rescue itself.
Unfortunately for the warmists, the opposite is true. The warmists are the ones who are less educated scientifically. This is what a recent Yale University study shows. Hat tip: www.politik.ch.
Professor Dan M. Kahan and his team surveyed 1540 US adults and determined that people with more education in natural sciences and mathematics tend to be more skeptical of AGW climate science. Of course this means that people will less education are more apt to be duped by it.
Surprised? Here’s an excerpt of the study’s abstract (emphasis added):
The conventional explanation for controversy over climate change emphasizes impediments to public understanding: Limited popular knowledge of science, the inability of ordinary citizens to assess technical information, and the resulting widespread use of unreliable cognitive heuristics to assess risk. A large survey of U.S. adults (N = 1540) found little support for this account. On the whole, the most scientifically literate and numerate subjects were slightly less likely, not more, to see climate change as a serious threat than the least scientifically literate and numerate ones.
Time for you warmists to go back to school (though I seriously doubt many of you are capable of learning much of anything, on account of extreme cultural cognition disability).
To learn more, here’s a video on Cultural Cognition and the Challenge of Science Communication which looks at risk perception w.r.t. the issues of climate change, and here’s a video on Cultural Cognition Hypothesis.
Share this...FacebookTwitter "
"For hundreds of thousands of years humans lived in hunter-gatherer societies, eating wild plants and animals. Inequality in these groups is thought to have been very low, with evidence suggesting food and other resources were shared equally between all individuals. In fact, in the hunter-gatherer societies that still exist today we see that all individuals have a say in group decision making. Although some individuals may act as leaders in the sense of guiding discussions, they cannot force others to follow them. But it seems that with the beginning of agriculture around 10,000 years ago, this changed. An elite class began to monopolise resources and were able to command the labour of others to do things, such as build monuments in their honour. So how was it that egalitarian societies, where all men were equal, transitioned into hierarchical societies where despots reigned? In recent years archaeologists have tended to focus on the means by which would-be leaders could coerce other individuals into following them (so-called theories of agency). But while leaders probably did coerce their followers once they were in power, it is difficult to see how they could do so at the outset. After all, if all individuals started out with equal resources and equal status, how could one individual force 30 others to do their bidding? This problem forces us to examine the benefits that would-be leaders could provide to their followers – and this is where agriculture comes in. While hunting wild game did not involve much co-ordination beyond placing traps and positioning hunters, agriculture presented an opportunity to massively increase the amount of food that could be produced. A classic example is the development of irrigation systems, which allowed crops to be grown further away from rivers and water sources. Although the role of irrigation systems in creating despotic states has been overstated in the past, they certainly would have created an opportunity for would-be leaders to behave entrepreneurially by managing their construction. Those that chose to follow their agricultural-technologist leader would then benefit from access to irrigation. This would provide the benefit of increased food production, enhancing both their quality of life and the number of surviving offspring they could produce. In this way, social hierarchy could initially arise voluntarily – because individuals that chose to follow the leader were materially better off than those that did not. But under what conditions does this voluntary leadership, where everybody benefits, turn into despotism? I tried to answer this question with a new computational model, which has highlighted two key linked factors. The first is population growth. When populations are small it’s relatively easy for individuals to go back to a leaderless way of life, for example by moving to a new patch of land. This seems to happen in modern hunter-gatherer groups, where people may simply walk away from a bullying leader in the middle of the night. But as population density increases, it becomes harder and harder to find free land to move to that is not controlled by the leader and their followers. Model simulations demonstrate that positive feedback between leaders increasing resource production and population growth can create an obligatory hierarchy, destroying the viability of leaderless life in the area. And empirically, hierarchy formation most often co-occurs with an increase in food production that drives population growth.  The second factor is the cost of changing the leader. Even if individuals are locked into a hierarchy, despotism is not inevitable if individuals can readily choose to follow a different leader. For example, by moving to a different group with a different leader. Group membership in hunter-gatherer societies is quite fluid, so this is relatively easy. But with agriculture, individuals would have become tied to a plot of land in which they had invested, making leaving the group very costly. This would become even more extreme with irrigation farming, where farmers would be tied to the system. Indeed, the most despotic early states arose in locations such as Egypt, where agriculture had to happen in a narrow valley along the Nile, making dispersal very difficult. So the use of agriculture established human societies and provided for them in some ways that improved over hunter-gathering. But it shattered the social norm and facilitated the rise of despotism by attracting followers to entrepreneurial leaders that could provide them with benefits, by increasing population density which reduced the ability for others to survive outside the hierarchical group and by making it so costly to leave the group that to do so was unattractive even when faced with despotic leaders. Even in ancient times at the dawn of agriculture there was, it seems, no such thing as a free lunch."
"
You won’t want to miss Lord Christopher Monckton (Former advisor to UK Prime Minister Thatcher) on Glenn Beck  – Today Friday, October 30th!
Monckton as many WUWT readers know, is a prominent skeptic and has been making presentations around the USA at college campuses, similar to what Al Gore does. Monckton recently criticized the Copenhagen Treaty and the potential for President Obama to sign it as possibly ceding US sovereignty to the UN on the issue.
Times below:

Monckton will be on Fox News Glenn Beck Show, with former UN Ambassador John Bolton, for the full hour. The topic will be all aspects of the Global Warming Scare and the push for a “new world order” to “deal” with it.
Expect fireworks!
FOX cable news Glenn Beck Show 
Time:  5:00pm Eastern time zone
For viewers that don’t have Fox News, check this page afterwards and we’ll put up links to the recoreded video when it is available.
For now, this video of his recent presentation can be seen here

UPDATE:
Video of the interview is now available  here

 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e922c930d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
In Debunking National Wildlife Federation Claims – Part 2 some commenters claimed that the snow data cited from WRI “was not good enough”. OK then, on to a bigger catchment. Steve Goddard replies in this brief essay.
From Wikimedia: Lake Powell from above Wahweap Marina. July 2004, by Dave Jenkins
Lake Powell (Arizona and Utah) provides a good proxy for western slope snowfall, because much of the snow in Wyoming, Colorado, Utah and Northwestern New Mexico drains into the lake via the Colorado, Green and San Juan Rivers.  The lake currently contains more than 4.5 trillion gallons of water and is 490 feet deep at the dam.

http://en.wikipedia.org/wiki/File:Greenutrivermap.png
Between 2000 and 2005, drought conditions (combined with greatly increased water usage in Arizona, California, Nevada and Colorado) caused Lake Powell levels to drop nearly 120 feet. This prompted a considerable consensus of global warming hysteria.
Every scientific study confirms that global warming will cause the amount of water in the Colorado River to decline
 http://www.ucpress.edu/blog/?p=151 
But a strange thing happened in 2006 – the lake level stopped declining and instead started increasing rapidly.  As you can see in the table below from lakepowell.water-data.com, since 2005 the lake elevation has increased by more than 60 feet above the 2005 low of 3562 ft.  As of January 29, at 3622 ft. the lake is within three feet of the January 29 average of 3625 feet elevation.  The volume of water in the lake has increased by 65% in the last five years to 4.5 trillion gallons. (At movie theater prices for bottled water, that could almost erase the US National Debt.) 














DATE MEASURED


ELEVATION


CONTENT


INFLOW


OUTFLOW


HIGH TEMP


LOW TEMP


WATER TEMP




Wed       Jan 29, 1964


3414.40


1108997


4979.00


1140.00


51.0


24.0


--




Fri       Jan 29, 1965


3491.52


4198819


10445.00


9730.00


54.0


32.0


--




Sat       Jan 29, 1966


3535.41


6797291


8500.00


11800.00


46.0


22.0


--




Sun       Jan 29, 1967


3517.50


5641536


7591.00


6570.00


53.0


29.0


--




Mon       Jan 29, 1968


3525.19


6108008


6872.00


8700.00


47.0


31.0


--




Wed       Jan 29, 1969


3540.30


7097730


15583.00


13700.00


49.0


32.0


--




Thu       Jan 29, 1970


3569.60


9303379


8521.00


10500.00


43.0


23.0


--




Fri       Jan 29, 1971


3600.37


12052938


12221.00


3920.00


58.0


29.0


--




Sat       Jan 29, 1972


3608.01


12794734


12475.00


11800.00


51.0


34.0


--




Mon       Jan 29, 1973


3601.37


12112452


10572.00


20900.00


37.0


23.0


--




Tue       Jan 29, 1974


3647.08


17150100


9599.00


8940.00


50.0


25.0


--




Wed       Jan 29, 1975


3645.75


16958398


9145.00


8620.00


41.0


24.0


--




Thu       Jan 29, 1976


3666.82


19671442


7622.00


9700.00


53.0


28.0


--












All Lake Powell water data for January 29th




Sat Jan   29, 1977


3651.91


17675554


9198.00


19000.00


51.0


27.0


--




Sun Jan 29,   1978


3625.36


14502612


5437.00


6660.00


48.0


32.0


--




Mon Jan   29, 1979


3629.33


14921634


8785.00


20200.00


28.0


14.0


--




Tue Jan   29, 1980


3673.18


20435414


10583.00


17100.00


52.0


25.0


--




Thu Jan   29, 1981


3679.78


21351354


7612.00


12000.00


52.0


30.0


--




Fri Jan   29, 1982


3663.93


19101078


7252.00


16900.00


52.0


32.0


--




Sat Jan   29, 1983


3683.31


21807370


13109.00


10600.00


51.0


39.0


--




Sun Jan   29, 1984


3680.79


21404754


13849.00


24300.00


51.0


26.0


--




Tue Jan   29, 1985


3680.74


21366038


16915.00


25400.00


44.0


31.0


--




Wed Jan   29, 1986


3685.73


22105498


13913.00


20100.00


55.0


30.0


--




Thu Jan   29, 1987


3679.39


21169534


15213.00


26200.00


52.0


39.0


--




Fri Jan   29, 1988


3683.04


21703966


9039.00


11700.00


62.0


31.0


--




Sun Jan   29, 1989


3676.87


20805426


5984.00


13600.00


50.0


8.0


--




Mon Jan   29, 1990


3655.74


17940848


5025.00


9510.00


n/a


22.0


--




Tue Jan   29, 1991


3630.86


14949762


5525.00


10200.00


46.0


26.0


--




Wed Jan   29, 1992


3621.42


13914222


6840.00


12700.00


48.0


22.0


--




Fri Jan   29, 1993


3613.74


13110446


6142.00


13400.00


52.0


28.0


--




Sat Jan   29, 1994


3657.30


18141516


7817.00


11300.00


46.0


28.0


--




Sun Jan   29, 1995


3647.12


16859904


6100.00


9580.00


51.0


30.0


--




Mon Jan   29, 1996


3678.07


20978340


8740.00


15600.00


57.0


28.0


--




Wed Jan   29, 1997


3671.32


20021628


11037.00


18400.00


49.0


33.0


--




Thu Jan   29, 1998


3679.18


21138864


10921.00


19900.00


56.0


26.0


--




Fri Jan   29, 1999


3680.71


21361650


7397.00


14395.00


49.0


n/a


--




Sat Jan   29, 2000


3679.33


21160650


8314.00


13156.00


55.0


24.0


--




Mon Jan   29, 2001


3666.53


19363092


7456.00


14876.00


44.1


28.0


--




Tue Jan   29, 2002


3652.58


17539014


3868.00


13424.00


39.9


28.0


47.0




Wed Jan   29, 2003


3615.58


13300186


4328.00


12787.00


55.0


37.0


50.0




Thu Jan   29, 2004


3592.09


11010776


4858.00


12815.00


50.0


28.0


49.0




Sat Jan   29, 2005


3562.14


8486755


8181.00


14100.00


50.0


37.9


49.0




Sun Jan   29, 2006


3594.59


11241152


6438.00


11534.00


53.1


30.9


47.0




Mon Jan   29, 2007


3599.72


11723383


8253.00


13519.00


48.9


30.9


47.0




Tue Jan   29, 2008


3590.80


10893087


9906.00


12773.00


35.1


19.9


46.0




Thu Jan   29, 2009


3614.36


13174179


7893.00


13360.00


48.9


28.9


48.0




Fri Jan   29, 2010


3622.33


14011695


6974.00


14437.00


39.9


32.0


--




Source:
http://lakepowell.water-data.com/
The yearly change in volume is determined by the formula :
delta H = inflow – outflow – evaporation – seepage
Evaporation is relatively constant from year to year as is seepage, so the formula can be written as:
delta H = inflow – outflow – K
Outflow (water usage) has greatly increased over the last few decades due to massive population increases in Phoenix, Las Vegas and Southern California – not to mention the large and ever increasing amount of water being used by the biofuels industry.  (It has been estimated by the  University of Twente in The Netherlands that the manufacture of one liter of biodiesel requires 14,000 liters of  water).
The point being that despite large  increases in outflow, the lake level has been rapidly recovering. This could be  due to only one explanation – lots and lots of snow in the Rocky Mountains  during the last five years.
And an  extra bonus from the “weather is not climate” department – January 29, 2010 at  39.9 degrees was ten degrees below normal and the second coldest on  record.
  

Lake Powell (Arizona and Utah) provides a good proxy for western slope snowfall, because much of the snow in Wyoming, Colorado, Utah and Northwestern New Mexico drains into the lake via the Colorado, Green and San Juan Rivers.  The lake currently contains more than 4.5 trillion gallons of water and is 490 feet deep at the dam.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8e5a1597',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"Anthony Albanese will travel to New South Wales coal country over the weekend in an effort to persuade regional Australians that net zero by 2050 means opportunity for blue-collar workers and for farmers. In an address to the country Labor conference in Singleton on Saturday, the Labor leader will blast the Nationals for engaging in “lazy cynicism” and for selling out regional communities by opposing action to reduce greenhouse gas emissions.  Federal Nationals have been out since Labor announced it would adopt the mid-century net zero target, claiming that shift would spell the death of agriculture. But a former deputy leader of the NSW Nationals, Niall Blair, said this week the target Labor adopted would provide a great opportunity for the agricultural sector in Australia to diversify and thrive. Albanese will tell Saturday’s conference the global community wants Australia to reduce emissions, but it is not demanding Australia stop exporting coal. “In fact, the global steel and aluminium industries – all needed to build solar panels and wind towers – will continue to demand high-quality Australian coal for decades to come”. Labor suffered negative swings in the Hunter Valley region during last year’s election because of a backlash in coal communities to the party’s climate policies. Albanese will say coal will remain part of such regional economies, but also point out regional economies are increasingly diversified. “The Hunter doesn’t have all its eggs in the coal basket,” Albanese will say. “Contrary to Nationals’ rhetoric, regional Australia is more than resources alone. Look around you. The mighty Hunter is Australia’s largest regional economy, with an output of more than $47bn each year”. “Could you imagine Australia without Hunter wine? Could you imagine horse-racing without Hunter thoroughbreds? Down the road we have the University of Newcastle. Up the road, the University of New England. Two of the jewels of our tertiary sector. “To ignore the diversity of regional Australia makes no sense.” Albanese will also argue new technologies will create new jobs. “With the development of an Australian hydrogen industry, regional Australia would be a natural home for expanded industries in aluminium, steel, silicon and ammonia.” He will say there are huge opportunities in regional Australia to contribute to the abatement task and gain new incomes streams through carbon farming. “Australia has the potential to capture 1bn tonnes of carbon dioxide a year, generating a new source of income for our farmers in the process.” Albanese will also nominate forestry and rare earths as blue-collar regional industries that will continue through the transition to decarbonisation. “Just as coal and iron ore fuelled the industrial economies of the 20th century, [rare earth and forestry] will fuel the clean energy economies of the 21st,” the Labor leader will say. “If we leave it to the Nationals, we will drift back towards the 19th century. They would rather cling to yesterday and run scare campaigns than embrace the opportunities of tomorrow. “This lazy cynicism is shameful. They sell out their own communities and our full potential as a nation. To deny energy alternatives as the Nationals do is to rob regional communities of their future”."
"**The government is planning to pass new laws to cut Britain's overseas aid budget, the BBC has learned.**
It has raised fears among MPs that the reduction could be permanent.
There had been speculation the chancellor was proposing a temporary, one-off cut to help pay down the government's record deficit.
Foreign Secretary Dominic Raab said the UK is ""a leading, if not one of the leading, countries on aid"" and ""that will continue"".
The idea behind a temporary cut was to reduce aid spending next year to just 0.5% of national income, down from the legally binding target of 0.7%.
But the BBC understands that Rishi Sunak's reforms will require new legislation to be passed by Parliament, which MPs believe implies a permanent cut to the aid target or even its abolition.
The issue is that the 0.7% baseline for Britain's aid budget is enshrined in law by the International Development (Official Development Assistance Target) Act.
This does allow the government to miss the target in certain circumstances, such as if there is a substantial change in the country's national income.
Foreign and Development Secretary Dominic Raab is simply required to lay a report before Parliament explaining why he has missed the target.
But there is a growing belief at Westminster that this exemption can apply only retrospectively.
The act places a duty on Mr Raab to ensure the 0.7% target is met. If he misses it, the act requires him to describe what steps he has taken to ensure the target is met the following year.
Some MPs and charities believe these two provisions mean the government cannot declare in advance its intention to miss the target without breaking the law.
To cut the aid budget without fresh legislation might lay the decision open to judicial review.
MPs also believe that a one-off cut to the aid target - a saving of about Â£4bn - would hardly touch the sides of the Â£350bn deficit projected for this year. They say it only makes sense for the Treasury if the cut is permanent.
They also believe that if the government is going to reduce aid spending and face significant political and international criticism, ministers will be tempted to go the whole hog and scrap the target entirely.
Almost 200 charities, two former prime ministers, opposition parties, church leaders, ex-heads of the armed forces, global philanthropists have all come out against the cuts.
The Archbishop of Canterbury Justin Welby has urged Prime Minister Boris Johnson not to go ahead with cuts, saying that ""helping the world's poorest is one of the great moral and ethical achievements of our country"".
The risk for the government is that passing new legislation would give critics of the aid cut the chance to oppose and potentially block the changes in Parliament.
Although the government has a working majority of more than 80, it has seen a number of rebellions of late. One senior Tory MP said defeating the government on this ""would be entirely doable"".
Such is the scale of the reforms to the aid target that Mr Raab is expected to make a statement to MPs about it on Thursday."
"A development of 750 new homes in the small town of Ilfracombe on England’s north Devon coast has been approved by the local council. The news would be unremarkable if it weren’t for the identity of the project’s backer – the artist Damien Hirst. One wonders if the “artist’s impressions” of the prosaically titled Southern Extension are by the man himself. I suspect not. The new-town’s name is matched by a series of rather sparse, equally prosaic sketch-renders of predictable developer mass housing. The only hint we get at an artist’s vision behind Southern Extension is a statement from the architect: Hirst, we are told, has a “horror of ‘anonymous, lifeless buildings’ and wants ‘the kind of homes he would want to live in’”, and is intent on developing a thoroughly sustainable, wind and solar powered-eco town. New towns are nothing new. And neither are housing developments led by social and cultural visionaries, and industrialists. From the model villages of mustard-magnates in the early 19th century (Trowse, in Norfolk, was expanded by the Coleman family in 1805); Titus Salt’s Saltaire in Yorkshire (built by the industrialist to replace slum conditions for workers at his mills in 1851); to Tomáš Baťa’s shoe company town in Essex (effectively a communist enclave in south-eastern England, built in 1933), and Prince Charles’ Poundbury (built from 1993 onwards, on the Prince’s Duchy of Cornwall land in Dorset), industrial (and agricultural) endeavour, social ambitions and architectural desires have been the drivers behind the building of villages and towns with utopian aspirations. What is new, however, is the involvement of artists as the force behind these future imaginings. Southern Extension is to be partially built on land owned by Hirst, adjacent to one of his residencies in the north Devon town, with the development backed by the artist. In the model villages of old, wealthy, socially minded industrialists wished to provide accommodation for their workers that bettered the social ills that they saw. They established communities around the mills, factories and fields that needed supplies of well-rested, well-fed and reasonably happy workers to generate their wealth. But while they were indeed socially minded, they were very much industrial capitalists at heart. These villages and towns were industrial utopias – work and production were at the core of their ambitions, as well as social well-being. In 21st century Britain, we no longer work in the traditional industries. Model towns and villages are not built by the Salts and Baťas of our day. But what we most certainly do have is a culture industry. And any artist worth their salt (and certainly those like Hirst) are adept business people. They are entrepreneurs, and within the culture industry, they can be seen as the industrialists of our time.  But today’s culture industry is markedly different from the industries of the past: the primary commodity is cultural capital, and the most important producers in the chain are not the artists, or even the artist’s assistant’s who produce the works, but the consumers who give the art its value in the first place. The consumer, by endowing the objects of art (and their prints, and coffee-mugs, and dot-painted bins that are associated with Hirst’s art) with cultural capital, produces the value of the art in itself. Hirst already has a strong, and controversial, presence in Ilfracombe. He has established a gallery, a restaurant, a café and plans for further businesses in the area. Verity, a 20m tall statue created by Hirst of a naked pregnant woman, looks out over the town’s harbour. So why would an artist build a new town next to his adopted home-town, and use his own capital to drive the development? We can look to the prosaic, un-visionary “artist’s impressions” of Hirstville for our answer. These drawings belie the bottom-line economics at the heart of Hirst’s drive to develop this corner of England. For an artist who operates on shock value, there is little shocking (expect perhaps for the lack of shock) in this vision of a future English town. The imaginings and associated descriptions (“sustainable”, “eco” and the like) and the sparseness of the drawings allow the local council to pass the planning application without undue concern: this town is much like that of any new development – unthreatening, free of any substantial critical vision, and capable of being delivered at high margins, turning a tidy profit for the artist-cum-developer.  Where Hirst’s first stroke of genius comes into play, however, is simply through his own involvement. The Hirst brand will convert this town into more than housing: it will attain cultural capital though association and consumption (and not, judging by the “artist’s impressions”, through any form of design), and therefore command higher prices for the real estate. Economically, the development almost can’t fail to be a success for Hirst – for who wouldn’t want to live in a house that shared the magic of the man who put a shark in a tank of formaldehyde? But perhaps we can also see faint echoes of the model-village aspirations of yore? For the important point to notice here is the association between the Hirst-ville development and Ilfracombe – or Hirst-on-Sea as it’s become known by some. Like the industrialists of the 19th and 20th centuries, the model village of Southern Extension will provide a steady stream of productive employees in the culture industry, ready to work hard by eating, drinking and buying in the cafes, restaurants, and art galleries of Hirst-on-Sea: the hard graft of productive consumption.  And what better way to secure both your future workforce and consumer base, than by creating the brand convergence between the home (the Hirst-house) and the place of work and play (the Hirst-café/restaurant/gallery)?  With several thousand new residents housed in the latest eco-homes, and imbued with the cultural capital of The World’s Most Famous Living Artist, the business interests (the culture factories) of Hirst’s Ilfracombe are almost guaranteed to remain productive for many years to come, through the dedicated service of those “creatives” to whom the development will no doubt be marketed."
"**Personal protective equipment (PPE) stockpiles in England were inadequate for the Covid pandemic and price rises earlier this year cost taxpayers about Â£10bn, the spending watchdog has said.**
The National Audit Office said there had been a particular shortage of gloves and aprons.
The government said the NAO's report recognised that NHS providers had been able to get what they needed in time.
Almost Â£12.5bn was spent on 32bn items of PPE between February and July 2020.
During the same period in 2019, 1.3bn items were bought at a cost of Â£28.9m.
Each item had been ""substantially"" more expensive in 2020, because of very high global demand, the NAO said, from almost triple the cost for respirator masks to more than 14 times as much for body bags.
Had the government been able to pay 2019 prices, it would have spent Â£2.5bn on PPE in 2020.
In reality, it had spent Â£12.5bn, including hundreds of millions on ""unsuitable"" items that could not be used.
Some had ""passed its expiry date or did not meet current safety standards"", the watchdog said, with ""insufficient checks"" meaning Public Health England had had to recall eye protectors that did not meet standards.
In Parliament, on Wednesday, Labour leader Sir Keir Starmer accused Prime Minister Boris Johnson of ""wasting"" taxpayer's money on equipment that ""can't be used"".
But Mr Johnson replied ""99.5%"" of the 32 billion items of PPE bought between February and July 2020 ""conform entirely to our clinical needs"".
Earlier, the Department of Health and Social Care (DHSC) said ""only 0.49% of all the purchased PPE tested to date"" had not been fit for purpose.
NAO head Gareth Davies said: ""As PPE stockpiles were inadequate for the pandemic, government needed to take urgent action to boost supplies.
""Once it recognised the gravity of the situation... the price of PPE increased dramatically, and that alone has cost the taxpayer around Â£10bn.""
Before the Covid-19 pandemic, there were two emergency stockpiles of PPE:
But the NAO said: ""The EU exit stockpile held few items of PPE other than a large number of gloves.""
Meanwhile, the flu stockpile, as well as having shortages of some key items, did not include any gowns or visors despite the fact they had been ""recommended for inclusion in June 2019 by the New and Emerging Respiratory Virus Threats Advisory Group (Nervtag)"".
Public Health England told the NAO it had been analysing the market to work out which gowns to buy, when the pandemic had begun, which it said was the ""normal approach"" to find a lower price.
In mid-March, the government had still believed its two stockpiles would provide ""most of the PPE needed to manage a Covid-19 pandemic"" and so focused on distributing this PPE rather than buying more, the NAO reported.
The situation had become ""precarious"" in April and May, with stocks threatening to run out.
At one point, only 3% of the required number of gowns had been available.
But the nation did not at any point run out of any type of PPE.
The scramble for PPE in the early stages of the pandemic was not confined to the UK. Every healthcare system was desperate to secure protective equipment and prices soared. But the National Audit Office lays bare how the UK was at the back of the queue, having failed to spot the warning signs and how woefully inadequate the stockpiles were.
A failure to anticipate what might be needed for anything other than a flu pandemic in essence cost the taxpayer Â£10bn - the extra money needed to secure supplies such as gowns and visors during the Covid crisis.
The report highlights poor distribution of PPE with many staff saying they did not have the right equipment. The NAO notes starkly that health and care employers have reported more than 100 deaths among staff because of exposure to coronavirus.
An official inquiry, when it happens, will look hard at many aspects of the UK's preparedness and handling of the crisis and the PPE issue will be central. With a series of reports, the NAO has now done important groundwork but there is much still to find out.
A DHSC official said: ""As the NAO report recognises, during this unprecedented pandemic all the NHS providers audited 'were always able to get what they needed in time' thanks to the Herculean effort of government, NHS, armed forces, civil servants and industry"".
But the NAO heard feedback from care workers, doctors and nurses that showed ""significant numbers of them considered that they were not adequately protected during the height of the first wave of the pandemic"".
Employers have reported 126 deaths among health and care workers linked to exposure at work.
And there were concerns about training and whether the equipment was appropriately fitted, particularly from women and people belonging to ethnic minorities.
In a Royal College of Nursing survey of 5,000 NHS staff, 49% of respondents belonging to ethnic minorities said they had been adequately ""fit tested"" for a respirator, compared with 74% of white nurses.
The DHSC said it was ""listening to the reported practical difficulties with the use of some PPE experienced by women and black, Asian and minority ethnic (BAME) individuals, among others, and... taking action to make sure user needs are adequately addressed in future provisions"".
In a separate report, the Public Accounts Committee, a parliamentary body which works closely with the NAO, said it was ""concerned that the department had no plan before the pandemic for how it might increase critical care equipment in the event of an emergency"".
""This lack of preparedness was exacerbated by the fact that it did not know how many ventilators were available to the NHS to begin with,"" the PAC said.
But it added the government had managed to buy an additional 26,000 ventilators for use in the NHS, a ""significant achievement""."
"**A former Manx commissioner has died after testing positive for Covid-19 while receiving treatment in England.**
David McWilliams, who was in his mid 70s, had been flown to north-west England for spinal surgery.
His grandson Marcus Taylor paid tribute on Facebook to his ""hero"", adding that the Ballasalla resident had been ""my role model, my guide, my best friend"".
Health Minister David Ashford said the former Malew commissioner's death was ""tragic news"".
He said it was still safe to travel to the UK for treatment and people should not be ""reconsidering their medical treatment"".
Mr McWilliams, who was registered blind, served with Malew Commissioners between 2001 and 2004 and also campaigned for better provisions and benefits for those with disabilities.
He was also a former chairman of Sailing for the Disabled on the island.
A Malew Commissioners spokesman said Mr McWilliams would be ""greatly missed"".
""He was well known and respected around Ballasalla Village and the wider community,"" he said.
""David took a great interest in local politics and cared much for the people of the island.
""He always wanted to ensure that people were treated fairly and with respect.""
_Why not follow BBC Isle of Man on_Facebook _and_Twitter _? You can also send story ideas to_northwest.newsonline@bbc.co.uk"
"
I’ve mentioned problems with airports as climate stations in the past, mostly that they are pockets of UHI that have grown with the 20th century aviation boom. A good example is Chicago O’Hare airport. I’ll bet that many of you don’t know that the ICAO ID for O’Hare, is KORD, and FAA uses ORD which is what you see on airline luggage destination tags. “ORD” has nothing to do with the name O’Hare, which came after the airport was established. It has everything to do with the name “Orchard Field” which is what the airport started out as, which at the time was far more rural than it was now. You can read about its early history here.


Here is what it looked like in the 1940’s:
Looking down runway 22 at Orchard Field - photo circa 1943 - Image courtesy of the Bensenville Community Public Library O'Hare collection
Here’s that same view today from Google Earth:

Looking down runway 22 today - click for larger image
Look at O’Hare today, a sprawling megaplex of concrete and terminals surrounded by urbanization:
Click for interactive view
The weather station location above is designated by the orange pushpin. Here’s a closeup view:
Click for larger image
Note that there’s two electronics equipment buildings nearby with industrial sized a/c exhaust vents. While not USHCN, NCDC metadata lists O’Hare as a Class “A” station, which means it does in fact record climate. Data from O’Hare can be used to adjust other stations with missing nearby data.
The point I’m making with all the photos is that airports are far from static, especially since airline deregulation in the 1980’s. The are just as dynamic as the cities they serve. We measure climate at a great many airports worldwide. E.M. Smith reports that the majority of the GHCN record is from airports.
Even NOAA meteorologists admit that airports aren’t necessarily the best place to measure climate. In a series of stories I did…
How not to measure temperature, part  88 – Honolulu’s Official Temperature ±2
..about the failure of the aviation weather station at Honolulu causing unparalleled record highs, the NOAA Meteorologist there had this to say:
“ASOS…placed for aviation purposes…not necessarily for   climate purposes.” 

The key issue here is “aviation purpose, not climate purposes”. The primary mission is to serve the airport. Climate is a secondary or even tertiary consideration. And that’s exactly what happened in the story from the Baltimore Sun below. The observer used FAA guidelines rather than NOAA guidelines to measure snow for the climate record. NOAA doesn’t like the record because he didn’t follow their procedures, so they toss it out.
However, when a new high temp record is set in Honolulu due to faulty equipment, NOAA thinks THAT’s alright to keep in the records:
NOAA: FUBAR high  temp/climate records from faulty sensor to remain in place at Honolulu
A nearby station shows the error:
This is your Honolulu  Temperature. This is your Honolulu Temperature on ASOS. Any questions?
So it is with some disgust that I provide an excerpt of this article on NOAA rejecting a record snowfall at the BWI airport, where they set up a snow measuring board, but didn’t follow through on procedure. Again, the airport was doing measurements to serve the airport interests, not NOAA.
=====================================

Sat 20 Feb 2010
By Frank D. Roylance
Shawn Durkin, weather station manager who has worked for Pacific Weather Inc. for 16 years, stands on the rooftop location at BWI where Pacific Weather takes its snow measurements, using a snow board, mounted on the bench to his left, and an 8-inch rain gauge, at right. Baltimore Sun photo by Amy Davis / February 18, 2010



…
A contractor working for the  Federal Aviation Administration at BWI Thurgood Marshall Airport, paid  to measure the snow for the aviation industry’s needs, did not follow a  separate protocol required by the National Weather Service and the  National Climatic Data Center for valid climate data.
So while  the contractor measured 28.8 inches of snow during that storm, the  National Weather Service has thrown out the reading. Instead,  climatologists will rank the storm as “only” 24.8 inches – a number that  almost surely understates the “true” total.
Worse, for  climatologists, it now appears the weather service’s rules for snow data  had been ignored for years at BWI, throwing a cloud over the validity  of snow totals as far back as 1998, when the FAA took the job over from  the weather service.
Only BWI’s data are known to be affected,  but the problem could be more widespread. That possibility has caught  the attention of top officials at the FAA.
“We plan to meet with  the National Weather Service next week to begin a discussion on making  sure that we’re all on the same page in terms of measuring snow  accumulations at our airports,” FAA spokesman Jim Peters said. “There  will be a national discussion.”
In the meantime, the weather  service’s Baltimore- Washington Forecast Office in Sterling, Va., is  preparing to convene a committee of climatologists and other experts to  review Baltimore’s snowfall records from the 2010 and 2003 storms, and  perhaps back to 1998.
“I feel very strongly about historical  records and getting the climate data correct,” said James E. Lee, the  meteorologist-in-charge at Sterling. “Obviously, with the increased  media attention and political attention to climate, it is really up to  NOAA [the National Oceanic and Atmospheric Administration, of which the  National Weather Service is an agency] to make sure … the climate  record is a genuine one, and consistent to the best of our ability.”
The  problem at BWI came to light Feb. 6, as snow accumulations reported at  the airport passed 26 inches. They seemed poised to break the record set  in February 2003 – the storm listed on Sterling’s Web site as  Baltimore’s biggest.
But when reporters called asking about a new  record, Lee said that because of measurement errors by an FAA  contractor at BWI, the two-day storm total would be pegged at “only”  24.8 inches. He had discarded a 28.8-inch measurement from BWI because  it was the sum of hourly measurements throughout the storm – a method  invalid for climatological data.
Even at 24.8 inches, Lee said,  the storm total beat the previous two-day record of 24.4 inches, set at  BWI during two days of the four-day 2003 event. “I’m convinced that was  the most amount of snow Baltimore has seen [from a two-day storm] in  recorded history.”
But Lee had to use the most conservative  reading from the airport – a “snow depth” measurement of the total on  the ground when the storm ended, after hours of compaction.
The  FAA requires its observers to take hourly snow measurements and wipe the  boards clean after each hour, adding the totals as they go. That  provides pilots with better real-time information about changing  conditions. But it virtually eliminates compaction and so inflates  accumulation. Climatologists require measurements every six hours,  striking a balance between the hourly and snow depth readings. Some  airports maintain separate snow boards for the different protocols. But  not BWI.
Richard Carlson, vice president of Pacific Weather Inc.,  said his company has experienced weather observers at 20 U.S. airports,  including eight at BWI. Pacific has held the contract there since 2008.
“We  follow the FAA manual … and that is the guide book on how these  meteorological observations are to be taken,” Carlson said. “We had  heard about the six-hour measuring thing, but … if you have high winds  at all, this really is not going to work.”
…
Read the full article at the Baltimore Sun

Read Frank Roylance’s blog  on MarylandWeather.com



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8d774233',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Still Hiding the Decline
by Steve McIntyre
Even in their Nov 24, 2009 statement, the University of East Anglia failed to come clean about the amount of decline that was hidden. The graphic in their statement continued to “hide the decline” in the Briffa reconstruction by deleting adverse results in the last part of the 20th century. This is what Gavin Schmidt characterizes as a “good thing to do”.
First here is the Nov 2009 diagram offered up by UEA:

Figure 1. Resized UEA version of Nov 2009, supposedly “showing the decline”. Original here ,
Here’s what UEA appears to have done in the above diagram. 
While they’ve used the actual Briffa reconstruction after 1960 in making their smooth, even now, they deleted values after 1960 so that the full measure of the decline of the Briffa reconstruction is hidden. Deleted values are shown in magenta. Source code is below.

Figure 2. Emulation of UEA Nov 2009, using all the Briffa reconstruction.
 
R SOURCE CODE:
##COMPARE ARCHIVED BRIFFA VERSION TO CLIMATEGATE VERSION
#1. LOAD BRIFFA (CLIMATEGATE VERSION)

 # archive is truncated in 1960: ftp://ftp.ncdc.noaa.gov/pub/data/paleo/treering/reconstructions/n_hem_temp/briffa2001jgr3.txt”
loc=”http://www.eastangliaemails.com/emails.php?eid=146&filename=939154709.txt”

 working=readLines(loc,n=1994-1401+104)

 working=working[105:length(working)]

 x=substr(working,1,14)

 writeLines(x,”temp.dat”)

 gate=read.table(“temp.dat”)

 gate=ts(gate[,2],start=gate[1,1])
#2. J98 has reference 1961-1990

 #note that there is another version at  ftp://ftp.ncdc.noaa.gov/pub/data/paleo/contributions_by_author/jones1998/jonesdata.txt”
loc=”ftp://ftp.ncdc.noaa.gov/pub/data/paleo/contributions_by_author/jones2001/jones2001_fig2.txt”

 test=read.table(loc,skip=17,header=TRUE,fill=TRUE,colClasses=”numeric”,nrow=1001)

 test[test== -9.999]=NA

 count= apply(!is.na(test),1,sum)

 test=ts(test,start=1000,end=2000)

 J2001=test[,""Jones""]
#3. MBH :  reference 1902-1980

 url<-""ftp://ftp.ncdc.noaa.gov/pub/data/paleo/contributions_by_author/mann1999/recons/nhem-recon.dat""

 MBH99<-read.table(url) ;#this goes to 1980

 MBH99<-ts(MBH99[,2],start=MBH99[1,1])
#4. CRU instrumental: 1961-1990 reference

 # use old version to 1997 in Briffa archive extended

 url<-""ftp://ftp.ncdc.noaa.gov/pub/data/paleo/treering/reconstructions/n_hem_temp/briffa2001jgr3.txt""

 #readLines(url)[1:50]

 Briffa<-read.table(url,skip=24,fill=TRUE)

 Briffa[Briffa< -900]=NA

 dimnames(Briffa)[[2]]<-c(""year"",""Jones98"",""MBH99"",""Briffa01"",""Briffa00"",""Overpeck97"",""Crowley00"",""CRU99"")

 Briffa= ts(Briffa,start=1000)

 CRU=window(Briffa[,""CRU""],start=1850)

 tsp(CRU) #  1850 1999  #but starts 1871 and ends 1997

 delta<-mean(CRU[(1902:1980)-1850])-mean(CRU[(1960:1990)-1850]);

 delta  #   -0.118922

 #used to get MBH values with 1961-1990 reference: compare to 0.12 mentioned in Climategate letters
#get updated version of CRU to update 1998 and 1999 values

 loc=""http://hadobs.metoffice.com/crutem3/diagnostics/hemispheric/northern/annual""

 D=read.table(loc) #dim(D) #158 12 #start 1850

 names(D)=c(""year"",""anom"",""u_sample"",""l_sample"",""u_coverage"",""l_coverage"",""u_bias"",""l_bias"",""u_sample_cover"",""l_sample_cover"",

 ""u_total"",""l_total"")

 cru=ts(D[,2],start=1850)

 tsp(cru) #  1850 2009
#  update 1998-1999 values with 1998 values

 CRU[(1998:1999)-1849]= rep(cru[(1998)-1849],2)
#Fig 2.21 Caption

 #The horizontal zero line denotes the 1961 to 1990 reference

 #period mean temperature. All series were smoothed with a 40-year Hamming-weights lowpass filter, with boundary constraints

 # imposed by padding the series with its mean values during the first and last 25 years.

 #this is a low-pass filter

 source(""http://www.climateaudit.org/scripts/utilities.txt"") #get filter.combine.pad function

 hamming.filter<-function(N) {

 i<-0:(N-1)

 w<-cos(2*pi*i/(N-1))

 hamming.filter<-0.54 – 0.46 *w

 hamming.filter<-hamming.filter/sum(hamming.filter)

 hamming.filter

 }

 f=function(x) filter.combine.pad(x,a=hamming.filter(40),M=25)[,2]
## WMO Figure at CRU

 #http://www.uea.ac.uk/mac/comm/media/press/2009/nov/homepagenews/CRUupdate

 #WMO: http://www.uea.ac.uk/polopoly_fs/1.138392!imageManager/1009061939.jpg

 #2009: http://www.uea.ac.uk/polopoly_fs/1.138393!imageManager/4052145227.jpg
X=ts.union(MBH=MBH99+delta,J2001,briffa=briffa[,""gate""],CRU=cru )  #collate

 Y=data.frame(X); year=c(time(X))

 sapply(Y, function(x) range(year [!is.na(x)]) )

 #      MBH J2001 briffa  CRU

 # [1,] 1000  1000   1402 1850

 # [2,] 1980  1991   1994 2009
smoothb= ts(apply(Y,2,f),start=1000)
xlim0=c(1000,2000) #xlim0=c(1900,2000)

 ylim0=c(-.6,.35)

 par(mar=c(2.5,4,2,1))

 col.ipcc=c(""blue"",""red"",""green4"",""black"")
par(bg=""beige"")

 plot( c(time(smoothb)),smoothb[,1],col=col.ipcc,lwd=2,bg=""beige"",xlim=xlim0,xaxs=""i"",ylim=ylim0,yaxs=""i"",type=""n"",axes=FALSE,xlab="""",ylab=""deg C (1961-1990)"")

 usr 1960

 points( c(time(smoothb))[temp],smoothb[temp,""briffa""],pch=19,cex=.7,col=”magenta”)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9075ad31',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
The London Times is reporting:
“The chairman of the leading climate change watchdog was informed that claims  about melting Himalayan glaciers were false before the Copenhagen summit, The  Times has learnt.
Rajendra Pachauri was told that the Intergovernmental Panel on Climate Change  assessment that the glaciers would disappear by 2035 was wrong, but he  waited two months to correct it. He failed to act despite learning that the  claim had been refuted by several leading glaciologists.”
See the Times article here
And from Richard North at The EU Referendum, this video news report link and his commentary:

Less than a week after he claimed the IPCC’s credibility had increased as a result of its handling of the “Glaciergate” scandal, Pachauri’s own personal credibility lies in tatters as The Times accuses him of a direct lie.
This is about when he first became aware of the false claim over the melting glaciers, Pachauri’s version on 22 January being that he had only known about it “for a few days” – i.e., after it had appeared in The Sunday Times.  
However, Ben Webster writes that a prominent science journalist, Pallava Bagla – who works for the Science journal (and NDTV as its science correspondent) – claims that last November he had informed Pachauri that Graham Cogley, a professor at Ontario Trent University and a leading glaciologist, had dismissed the 2035 date as being wrong by at least 300 years. Pachauri had replied: “I don’t have anything to add on glaciers.”
Bagla interviewed Dr Pachauri again this week and asked him why he had decided to overlook the error before the Copenhagen summit. In the taped interview, he asked: “I pointed it out [the error] to you in several e-mails, several discussions, yet you decided to overlook it. Was that so that you did not want to destabilise what was happening in Copenhagen?”
Dr Pachauri replied: “Not at all, not at all. As it happens, we were all terribly preoccupied with a lot of events. We were working round the clock with several things that had to be done in Copenhagen. It was only when the story broke, I think in December, we decided to, well, early this month — as a matter of fact, I can give you the exact dates — early in January that we decided to go into it and we moved very fast.”
According to Pachauri, “… within three or four days, we were able to come up with a clear and a very honest and objective assessment of what had happened. So I think this presumption on your part or on the part of any others is totally wrong. We are certainly never — and I can say this categorically — ever going to do anything other than what is truthful and what upholds the veracity of science.”
Without even Bagla’s input, we know this to be lies.  Apart from anything else, there was the crisis meeting under the aegis of UNEP – which we reported on Thursday – which concluded that the 2035 claim “does not appear to be based upon any scientific studies and therefore has no foundation”.
Read his complete essay here


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8edca52b',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The discovery of a 4,000-year-old wine cellar in Israel has provided the best direct evidence yet of the raucous, boozy celebrations that were a key part of the region’s culture at the time.  The cellar was found during a recent excavation at Tel Kabri, Israel, described in a new paper in the journal PLOS ONE. In the remains of a palatial storage complex, archaeologists uncovered ceramic jars and fragments of other vessels dating from the Middle Bronze Age, a period which ran from roughly 1900 BC to 1600 BC. Analysis of ceramic samples showed the presence of tartaric and syringic acids, which are standard wine identifiers in archaeological contexts. Tests also revealed various combinations of herbal additives such as honey, juniper, mint, myrtle, and cinnamon. Wine was an essential part of Bronze Age life in Canaan (as well as the larger Levantine area) – the region roughly corresponding to modern day Israel-Palestine. This is partly due to the fertile soil and warm climate, which made the area ideal for vines to thrive. The abundance of grapes and therefore wine explains its significance and presence in a variety of events. The test results match with what we know from historical sources such as a list of wine types and additives from Mari, Northern Mesopotamia, in 18th century BC, or the Ebers Papyrus from Egpyt. Dating from 1550 BC, the Ebers Papyrus was a predominantly medical text, and included such prescriptions as a remedy involving powdered dung mixed with wine and administered orally. The vessels were found in a storeroom off the main hall at Tel Kabri, and could have held 2,000 litres of wine. Though this seems like a sizeable stockpile, it could have been consumed fairly quickly and must have been replenished every season. Wine is known to have a longer shelf life than other alcoholic beverages (such as beer) in the Bronze Age, but even a large cellar such as this wouldn’t have lasted forever. So how could one court get through around 3,000 modern day wine bottles in one year? Large feasts involving the ritual consumption of alcohol are one reason. In the Levant, such celebrations were known as marzeah, involving an elite cadre of well-heeled people, generally men, who threw parties characterised by heavy wine drinking. Marzeahs could be held to venerate a god (though they were not directly religious), remember a deceased ancestor, to celebrate hunting, warfare, or a certain rite of passage.  The alcohol consumption was key, especially wine. The reasons for such excess are unclear: perhaps to commune with the gods, to demonstrate their own form of divine qualities, or simply social drunkenness among men in a scene reminiscent of an ancient fraternity party. The overall concept of the marzeah was likely one that affirmed social status and hierarchy. The celebrations included young, nubile girls; at one point even two unmarried daughters of the god El attended a marzeah (they were apparently drunk and badly behaved, but the surviving texts make no mention of sexual activity). Overall the evidence on women’s role is mixed.  Excluded wives had their own marzeahs, or similar events, but some sources note that men and women did drink together, both under watchful eyes and at weddings. Wine was also used in religious contexts, where it was often used as an offering to the gods, or in wakes where mourners played music and drank wine. In festivals, it used in sacrificial rituals or mythological battle re-enactments, where wine often took the place of blood.  Wine served as thanksgiving and a symbol of the payment system, which was essential in the economic system.  It was so commonplace in the Levant that it was included in lodging rations, and possibly used to pay tithes, as one would with grain. Even the army were sometimes paid in wine. But it is the big parties that are most relevant to this newly discovered cellar. Ultimately what was going on here was conspicuous over-consumption of an essentially unproductive/luxury commodity – wine. Thorstein Veblen, an early analyst of such indulgence among the upper classes, once described this as lending a “means of reputability to the gentleman of leisure”. Veblen was writing in 1899 about capitalist society, but his point was such consumption at the top was a hangover from feudalism and before.  The finding at Tel Kabri is a case in point – who needs 3,000 litres of wine? By using wine in this way, the elite of the Bronze Age were signifying and justifying their power."
"**People from three households in Northern Ireland will be allowed to meet indoors for five days over the Christmas period, the first and deputy first ministers have said.**
The decision will apply to all four devolved nations.
Three households will be allowed to bubble from 23 to 27 December.
NI is due to begin a two-week lockdown from this Friday until 11 December, in a bid to curb the spread of the virus before Christmas.
The UK government has said anyone travelling to or from Northern Ireland can travel on 22 and 28 December, but ""only meet with their Christmas bubble"" between 23 and 27 December.
The Stormont Executive will meet on Thursday to consider the arrangements for Christmas in more detail.
First Minister Arlene Foster said she hoped the announcement would give people space to plan over the holiday period.
She added that it was difficult to ""balance"" Christmas festivities with managing the spread of the virus.
Deputy First Minister Michelle O'Neill described Tuesday's announcement as a ""message of hope"", but urged people to be responsible, safe and mindful of healthcare workers.
""There is a risk associated with allowing people to come together,"" she said.
Northern Ireland's Chief Medical Officer Dr Michael McBride said he had ""no doubt"" that Northern Ireland would see increased cases after the Christmas period.
However, he said the authorities needed to ""balance"" that risk with recognising that Christmas is ""a very important time of year"" for families who have had a difficult time in 2020.
He added they recognised ""that people will come together irrespective of the advice that we give and it's important therefore that we give advice to the population to ensure that they can come together in as safe a way as possible"".
Dr McBride urged the public to follow the rules, saying that ""what none of us wants, is to be in the very difficult situation - which we can't rule out - of advising the executive that a further period of restrictions is required in the new year.""
Politicians like announcing good news, which might explain why we got our first joint press appearance of Arlene Foster and Michelle O'Neill since tensions between their two parties over the Covid-19 lockdown erupted several weeks ago.
The announcement will come as a relief to many, but the relaxation is not designed to be a blank cheque.
Ministers are relying on public buy-in and compliance with these festive rules.
There are still plenty of questions about how this will work - particularly with an influx of students expected home in the coming weeks.
Some people will likely already have booked their flight or boat to Northern Ireland for Christmas, probably not within the tight 22-28 December travel window being mooted.
Then there's the added complexity of choosing who makes a bubble and the worry of having to cut out some family or friends, at a time which is traditionally all about coming together.
Each Christmas bubble will be allowed to meet at home, at a place of worship or in an outdoor public space.
It will also mean families can travel from one part of the UK to another without any sanctions.
But the four governments have said existing, more restrictive rules on hospitality and meeting in other venues will be maintained over the Christmas season.
The first minister said she recognised many people were looking forward to Christmas ""get-togethers"" as it had been a difficult year due to the pandemic.
""We hope that this clarity from ourselves today will give people that space to do a little bit of planning,"" added Mrs Foster.
The Republic of Ireland is due to set out its plans for socialising and travel over Christmas later this week.
Ms O'Neill said the executive also wanted to look to the Irish government for a ""common approach"" to managing the situation together.
""It's important to be honest - in a pandemic there's so little to be certain about but it's our intention to allow families to have some space over Christmas,"" she added.
Eleven further coronavirus-related deaths were reported in Northern Ireland on Tuesday, bringing the Department of Health's overall death toll to 947.
Six more deaths were reported in the Republic of Ireland, bringing its death toll to 2,028.
When a Christmas bubble is formed it must remain fixed and must not be changed; households within it have to be exclusive.
People can gather in private homes and overnight stays will be permitted.
People sharing a bubble can also meet up in places of worship or in an outdoor public space.
But people will not be allowed to meet with their Christmas bubble in hospitality settings or other entertainment venues.
You can meet people outside your bubbles, but this must be done in line with existing regulations.
At present, six people from two households in Northern Ireland can meet in a private back garden, while no more than 15 people can gather in a public space.
A support bubble counts as one household - so for this Christmas period this bubble can join with two other households.
Those households can be any size.
The executive has already said this will not be a normal Christmas, and recognises that that the arrangements will not work for everyone.
But the first and deputy first ministers said they presented an opportunity to allow people in Northern Ireland to have some more contacts with family and friends over the Christmas season.
The UK government has advised that in the two weeks that follow an individual's last meeting with a Christmas bubble, people should reduce their contact with others as much as possible.
Students planning to return to their family home for Christmas are not counted as people from a different household, according to Dr McBride.
""For students travelling home, that will be their household so they will become a member of that household,"" he said.
""So then they can combine with two other households.""
However, speaking on BBC's News NI's Coronavirus Catch-up, Dr McBride warned the the virus ""doesn't go away for Christmas"".
He urged students who had access to Covid-19 testing at their universities to avail of it and advised everyone travelling home for Christmas, not just students, to limit their contacts in the 10 to 14 days leading up to their journey.
He warned the public that older and extremely vulnerable people are ""at no less risk from this virus in the Christmas period than they were at any other period""."
"**There have been more than 1.6 million confirmed cases of coronavirus in the UK and about 60,000 people have died, government figures show.**
However, these figures include only people who have died within 28 days of testing positive for coronavirus and other measures suggest the number of deaths is higher.
**Find out how the pandemic has affected your area and how it compares with the national average:**
If you can't see the look-up click here.
After the first peak in April, cases started rising again in July, with the rate of growth increasing sharply in September and October, before falling again in the past two weeks.
On Wednesday, the government announced a further 16,170 confirmed cases.
It is thought the infection rate was much higher during the first peak in spring, but testing capacity at the time was too limited to detect the true number of daily cases.
The data for cases can also be broken down by region and comparing the change in those figures by week gives a sense of where there has been a recent increase in newly-reported infections.
Coronavirus infections in England have fallen by about a third during lockdown, according to a major study carried out by Imperial College London.
While some of the worst-hit areas saw the biggest improvements, cases have remained relatively high across England, according to the React-1 study, which is based on tests of more than 100,000 people between 13-24 November.
The latest figures from the Office for National Statistics (ONS), suggest about one in 185 (16,400 people) in Wales had the virus in the week ending 21 November.
In Northern Ireland rates are thought to be decreasing at around one in 145 people (about 12,700 people), while in Scotland, the figure was one in 115 (about 45,700 people).
The average number of daily deaths began to rise again in September, following the first peak in April.
On Wednesday, the government announced a further 648 deaths.
Of these, 555 deaths were in England, 51 in Wales, 38 in Scotland, and four in Northern Ireland.
Rules were amended over the summer to include deaths in the coronavirus total only if they occurred within 28 days of a positive test. Previously in England, all deaths after a positive test were included.
England has seen the majority of UK deaths from Covid-19. Using the 28-day cut-off, there have been more than 52,000.
Although hospital admissions for Covid-19 remain below the levels seen in the spring, there are big regional disparities.
The North West, North East and Yorkshire, and Midlands have seen the highest number of admissions but the situation in all three now appears to be improving.
Cases have risen across large parts of England, with other spikes in areas of Scotland, Wales and Northern Ireland.
The red areas on the map below are those currently seeing the highest number of cases per 100,000 people.
Restrictions have been tightened across the UK in an effort to tackle the number of rising cases.
In England, each local authority has been placed in one of the new three tiers following the end of the recent national lockdown.
Wales has announced a range of further measures, including a ban on pubs, restaurants and cafes serving alcohol, from Friday 4 December.
In Northern Ireland, a further two-week ""circuit-break"" lockdown began on 27 November.
Scotland has a five-tier system of alert levels with different measures in place in different parts of the country.
UK leaders have agreed to allow up to three households to meet indoors during a five-day Christmas period of 23-27 December.
When looking at the overall death toll from coronavirus, official figures count deaths in three different ways.
Government figures count people who died within 28 days of testing positive for coronavirus.
But there are two other measures.
The first includes all deaths where coronavirus was mentioned on the death certificate, even if the person had not been tested for the virus. The most recent figures suggest there had been almost 70,000 deaths by 20 November.
The second measure counts all deaths over and above the usual number at the time of year - that figure was more than 75,000 by 20 November.
The most recent figures available from the ONS are for the third week of November, which show there were 14,276 deaths registered in the UK.
Some 3,038 of these deaths involved Covid-19 - 199 more than the week before.
Deaths normally do rise at this time of the year, but the data from the ONS and its counterparts in Scotland and Northern Ireland show the second wave of the virus has pushed the death rate above the average seen over the past five years by about 21%.
Overall, the figures are still well below the peak of 9,495 deaths recorded in a week, reached on 17 April.
The ""R number"" is the average number of people an infected person will pass the disease on to.
If R is below one, then the number of people contracting the disease will fall; if it is above one, the number will grow.
The government's current estimate for the R number across the whole of the UK is 0.9 to 1.
The estimate for England is 0.9 to 1, while for Scotland it is 0.8 to 1. The estimate for Wales is 0.8 to 1 and in Northern Ireland it is 0.9 to 1.1.
The government has said in the past that the R number is one of the most important factors in making policy decisions."
"
In the past decade, since the release of the flawed 1998 study by Michael Mann, now known as MBH 98, the phrase “hockey stick” has been used to describe a certain shape of a graph. It has also become synonymous with poor data selection and  bad statistical procedure.
Yet again and again we see climate studies pushing this hockey stick shape as a way of saying we are “living in the worst time period of the data”.
Here, without statistics, without bristlecone pines, inverted lake sediments, midge larvae carcasses, larch trees in Yamal, or convoluted never before seen statistical methods, I present a directly measured data set that produces a real “hockey stick” shape.
Graph: It's worse than we thought
The data is directly measured and not a proxy, the plot is real. There’s no data adjustment or statistical manipulation. Care to know what it is?
From the website “Calculated Risk“

Here is the monthly Fannie Mae hockey stick graph …
 
Click on graph for larger image in new window.
Fannie Mae reported today that the rate of serious delinquencies – at least 90 days behind – for conventional loans in its single-family guarantee business increased to 4.45% in August, up from 4.17% in July – and up from 1.57% in August 2008.
“Includes seriously delinquent conventional single-family loans as a percent of the total number of conventional single-family loans. These rates are based on conventional single-family mortgage loans and exclude reverse mortgages and non-Fannie Mae mortgage securities held in our portfolio.”
Just more evidence of the growing delinquency problem, although these stats do include Home Affordable Modification Program (HAMP) loans in trial modifications.
Now that’s a hockey stick to be worried about.
It hardly is a surprise then that when we see that sort of graph of actual data in the American economy, we start to see graphs like this one depicting confidence in climate change as an important issue:

Source: Pew Poll,  story here
(h/t to WUWT reader Michael)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e91ce35e9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Chile is the first South American nation to sign, with complete bipartisan support within the country, a Free Trade Agreement with the United States. Now, why do Chilean workers support free trade policies? Because while the nation in the 1970s was initiating its free‐​trade development strategy it was also establishing a pioneering system of personal retirement accounts as the foundation of its Social Security policy. 



The connection between the two is important. All around the world, trade liberalization is cast as a battle between capitalists and workers, between “global elites” and the “common man.” In Chile, however, market‐​invested retirement funds mean that every worker is a capitalist and has a visible stake in an internationally competitive economy. In Chile, to be anti‐​globalization is to be both anti‐​capitalist and anti‐​worker.



A vast majority of Chileans benefits from free trade not just as consumers, but also as owners of the productive assets of the economy through their retirement accounts. Free trade is good for the economy, and what’s good for the economy is good for investors. Thus there is a virtuous cycle of trade liberalization that has so far thrived regardless of the political party in power.



Chile already has a 6 percent flat tariff rate that is low compared to the rates of most countries and, more importantly, it is applied equally to all imports. The flat tariff decision of the 1970s was critical. A differentiated tariff not only creates economic distortions that slow economic growth, but it continually generates special interest pressures and opportunities for corruption. With a flat rate, a politician can’t be bought on trade issues… because he has nothing to sell.



Before this latest deal, Chile signed an FTA with the European Union and another with Korea. It also has several bilateral free trade agreements with countries like Canada and Mexico. Complete free trade is at hand. The mere possibility of zero import tariffs is stunning in an economy that in the 1960s was one of the most protectionist in the world. In those days, Chile was a devoted follower of the misguided import‐​substitution proposals of the Santiago‐​based United Nations Economic Commission for Latin America. 



But in the mid‐​1970s the country’s trade policy changed radically. Not only did Chile completely dismantle the system of quotas and other trade barriers, but also under the so‐​called Chicago boys’ liberal economic policies, the low flat tariff policy was adopted. The end result of all these reforms: more than a decade of economic growth at an “Asian tiger” level of 7 percent a year that doubled the size of the Chilean economy and led, for the first time in history, to the highest income per person in the whole region.



Trade liberalization does not take place in a vacuum; the proper overall economic and cultural climate is essential. Social Security choice as implemented in Chile, between the government‐​run pay‐​as‐​you‐​go system and one of personal retirement accounts has solved the retirement crisis and delivered enormous benefits to workers. It also has made trade and economic liberalization more possible by linking the interests of workers to that of the overall economy.



U.S. Trade Ambassador Robert Zoellick, a real world hero of trade liberalization, courageously stated that “one of the nice things in this agreement is we have some additional access in terms of pension fund management with a Social Security system that I wish we could imitate.” 



I hope this FTA is only “the end of the beginning.” There are innumerable initiatives that could spring from greater trade integration. By a kind of intellectual osmosis, we Chileans can integrate into our own reality the basic economic and political concepts of a country “conceived in liberty” by its incomparable Founding Fathers‐​just as North Americans may benefit from learning about our culture and way of life, a process that, with 37 million people of Hispanic origin in the United States, is well under way.



My dream is an “American Community” of independent nations, cherishing their own cultural identities but joined together in a common market for trade and investment, and with free movement of people and ideas. An American Community would comprise 830 million people and a gross domestic product of $13 trillion.



I salute and join Walt Whitman who once said: “The spirit of the tariff is malevolent. It flies in the face of all American ideals. I hate it root and branch. It helps a few rich men to get rich, it helps the great mass of poor men to get poorer. I am for free trade because I am for anything that will break down the barriers between peoples. I want to see the countries all wide open.” 
"
"A fresh legal challenge to HS2 has been launched by the naturalist and broadcaster Chris Packham, arguing that the UK government’s decision to approve the high-speed rail network failed to take account of its carbon emissions and climate impact. Packham and the law firm Leigh Day said the Oakervee review, whose advice to proceed with HS2 in full was followed by Boris Johnson last month, was “compromised, incomplete and flawed”.  The crowdfunded legal challenge comes in the wake of a court of appeal ruling on Heathrow, which declared that the government’s planning statement allowing a third runway at the London airport was unlawful for not referencing the Paris climate agreement. Packham will likewise argue that the Oakervee report failed to quantify and address the full impact of HS2’s likely carbon emissions. The initial environmental statement for the high-speed rail network was made in 2013, before the government signed the Paris agreement and committed to achieving net zero carbon emissions by 2050. Packham said: “Every important policy decision should now have the future of our environment at the forefront of its considerations. But in regard to the HS2 rail project I believe our government has failed. I believe that essential submissions regarding environmental concerns were ignored by the review panel. “As a consequence, the Oakervee review is compromised, incomplete and flawed and thus the decision to proceed based upon it is unlawful.” Tom Short, a solicitor at the law firm Leigh Day, said the “environmental impacts relevant to the decision whether to proceed have not been properly assessed”. He added: “In a time of unprecedented ecological catastrophe, [Packham] is clear that the law, and moral logic, require the government to think again.” The Department for Transport said it was considering the claim and would respond in due course. A DfT spokesperson added: “We understand campaigners’ concerns, and have tasked HS2 Ltd to deliver one of the UK’s most environmentally responsible infrastructure projects. When finished, HS2 will play a key part in our efforts to tackle climate change, reducing carbon emissions by providing an alternative to domestic flights and cutting congestion on our roads.” HS2 Ltd will build a new high-speed rail line linking London with Birmingham, and later Manchester and Leeds. The Oakervee review, commissioned by Johnson after becoming prime minister, said the costs of the project could rise to more than £106bn. Campaigners say the line will damage or destroy almost 700 wildlife sites, including more than 100 ancient woodlands. HS2 disputes the figure and says only 62 ancient woodlands will be affected, and most of those would remain intact."
"**""At the moment we're not able to budget for Christmas or anything because we had to buy more all year round.""**
Kirsty Clayton, 24, from Milford Haven, is one of the 112,700 people in Wales out of work and on Universal Credit - almost double the 57,400 last October.
Like other claimants, mother-of-three Ms Clayton has received a Â£20-a-week top up during the pandemic.
But that is due to end in March and there are calls for it to be extended in the chancellor's Spending Review.
Neither Ms Clayton nor her partner are currently employed, and they are struggling to bring up three children aged two, one and three months.
She said she has had to buy three sizes of nappies and because she has had to buy more from local shops since the start of the pandemic, they are Â£4 more expensive per pack than supermarkets' own-brand versions.
Ms Clayton has been helped with parcels from Patch, a Pembrokeshire charity which donates food, gifts and toys at Christmas, and said she would ""struggle quite a bit"" if the extra Â£20 a week comes to an end as planned in March.
She explained: ""It's taken its toll on me. I've been depressed as well - post-natal depression - it's been really hard.""
There are calls, including from within his own party, for Chancellor Rishi Sunak to extend the payment beyond March when he announces his annual Spending Review later.
Like other charities in Wales, Patch has experienced a large increase in the numbers of people it is helping.
The charity, which gives food parcels with five days' worth of food to those in need, has handed out 70% more meals this year than it did last year.
Between January and the end of September, it provided 78,435 meals compared with 46,080 meals during the same period in 2019.
""Even though I have been doing this for more than 20 years, I am always shocked by the different needs, and this year, oh my goodness, I have never seen anything like it,"" said Patch co-ordinator Tracy Olin.
""The stories are completely different from anything I have seen before.""
Ms Olin spoke about a self-employed couple whose incomes had both stopped because of Covid and had been forced to use the foodbank.
""People have got used to that Â£20 and taking it away would be huge,"" said Ms Olin.
""These are the most vulnerable people. If I had my wages cut by Â£20 a week I would really notice a difference, but these people have so little income already, Â£20 a week is going to be devastating.""
Ms Clayton's Conservative MP, former Welsh Secretary Stephen Crabb, said the payment had been particularly important in Wales because wages are lower than the rest of the UK.
""Increasing the allowance by Â£20 has been so important to so many families, not just the unemployed who've lost their jobs but people who are working irregular hours,"" he said.
""I think the Treasury can afford to keep this uplift in payments - it will cost around Â£7bn next year.
""I think the idea that we would now reverse the Â£20 increase to Universal Credit next March, at a time when many families would be facing an increase in unemployment, for me that is just out of the question."""
nan
"Scotland might traditionally be known for its North Sea gas reserves but it also leads the way in renewable power. The current devolved Scottish government wants 100% of the nation’s electricity generation to come from renewables by 2020. But the wind industry that may power Scotland towards the target developed while part of the UK. How might renewables fare after independence? The short answer is that, in the event of a Yes vote, renewable energy would go from strength to strength. The rest of the UK would have no choice other than to co-operate with Scotland on energy matters. The negative attitude of the coalition government towards renewable energy has long been evident and also includes plans for a significant reduction in renewable energy subsidies. In the event of a No vote it is highly unlikely that the UK government would change its position and encourage investment in renewable power. Wind energy is doing well, for instance. According to RenewableUK wind energy set new UK records during August 2014, surpassing the amount of energy generated by coal power in the UK on five different days that month and overtaking nuclear generation on one occasion.  The latest government polling put public support for renewable energy sources at a very high 79%. But despite this the UK government continues to support unsafe and costly new nuclear stations at the expense of wind, solar or tidal power. Its position could only worsen in the event of a No vote, and would represent a disaster for Scotland’s renewables ambitions. A Yes vote, on the other hand, would surely place the future of renewable power in the hands of the Scots, who are committed to a future powered by wind and sea. The more optimistic reading of Scotland’s reserves, one that echoes the estimates of industry lobbyists Oil and Gas UK, would ensure energy security for Scotland. This would in turn provide revenue which could be invested in the further development of renewables. The more pessimistic version, where Scotland’s oil and gas reserves are declining, means developing renewables becomes an even more important aspect of energy security.  The Scottish government’s own report Energy Regulation in an Independent Scotland declares the country should position itself as the best place to generate renewable power and it seems set to follow up this plan in the event of a Yes. Whichever way you look at its oil reserves, Scottish renewables would grow stronger under independence. Scotland has the wind, and it knows how to use it. The renewables industry is clearly concerned about the effect of a Yes vote on subsidies currently regulated by UK bodies. However, Scotland can deal with regulatory uncertainty by relying on its clear long-term policy direction and the introduction of new subsidies that will focus on specific growth opportunities for renewables. Whatever the outcome of the referendum, the UK still has its climate change targets to meet – and wind energy from Scotland would help. As a result, it is highly likely that the rest of the UK would have no choice but to continue importing electricity from Scotland. This would place independent Scotland in a better position to negotiate an arrangement for an integrated energy market, in line with the stated policy goal of forming an effective energy partnership with the rest of the UK. In summary, a Yes vote would definitely be the better choice for meeting the aspirations of renewable energy generation in Scotland and to ensure security of supply in the long term. The rest of the UK would have no choice but to co-operate with an independent Scotland under these circumstances to meet its climate change targets and to avoid blackouts."
"The author of a groundbreaking report on the economic impact of climate change has called on Rishi Sunak to spend more than £8bn in his first budget next week to kickstart a “massive and long-term” boost to “zero-carbon infrastructure, new skills and sustainable innovation”. Lord Stern said the new chancellor had a unique opportunity to address regional inequalities and invest to meet the government’s target for net-zero emissions with measures already highlighted in the Conservative party manifesto. Stern, who runs the London School of Economics’ Grantham Research Institute on Climate Change, told Sunak to focus his efforts on sectors that are “difficult to decarbonise”, such as transport, property and industry. Sunak is known to be hurriedly rewriting his budget speech for 11 March and earmarking funds to tackle the coronavirus outbreak, possibly delaying measures to improve the UK’s infrastructure. But he is expected to signal extra spending in the regions over the life of the parliament to 2024, to support Britain reaching net-zero carbon emissions by 2050. The report recommends the government use £6.3bn committed for energy efficiency in the 2019 election manifesto to reduce energy waste in buildings, which are responsible for 17% of the UK’s greenhouse gas emissions. It also recommends that £1bn committed in the manifesto for vehicle charging points should be focused on rural parts of the UK that would otherwise miss out on the electric car revolution. Stern said £800m promised to limit emissions through initiatives to capture carbon and store it should be used as an incentive to generate private sector investment. “The budget should mark the start of a decade of massive investment in accelerating the transition of the UK economy to zero-carbon growth,” said Stern, whose report for former chancellor Gordon Brown in 2005 was one of the first to show the economic challenges from climate change and how they could be met in the UK and globally. “This would mean that by 2030 the UK could have higher living standards, and better health and wellbeing, underpinned by UK businesses innovating and adopting cutting-edge zero-carbon technologies and practices fit for the mid-21st century,” he added. Stern’s warning against inaction by the Treasury came as 101 climate campaigners and former government and Bank of England advisers wrote to the incoming central bank boss, Andrew Bailey, urging him to force firms to disclose their climate risks “as soon possible”. The central bank should also exclude fossil fuel assets from both future rounds of quantitative easing (QE) and the assets the Bank accepts as collateral, so as to “lead by example”. An earlier report by the Grantham Institute found that the bank’s £10bn purchases of corporate bonds, part of a £435bn stimulus programme, was heavily skewed towards oil and gas companies. Among the 101 signatories were Sir David King, a former government chief scientific adviser, ex-Citigroup chief economist and former Bank of England monetary policy committee member Willem Buiter and primatologist Jane Goodall. The signatories said they wanted Bailey to recognise the severity of the climate emergency by going beyond the initiatives put in place by his predecessor Mark Carney. Fran Boait, executive director of Positive Money, one of the sponsors of the letter, said: “The investments made by our financial system today determine whether we will be able to keep global temperature rises below the 1.5C upper safe limit. Finance is currently funding warming of more than 4C, which represents an existential threat not only to finance and the economy, but to life on earth. “With less than a decade to drastically cut emissions and avoid irreversible climate breakdown, Andrew Bailey must ensure that climate remains high on the Bank’s agenda. These steps are a necessary starting point.”"
"
Share this...FacebookTwitterNow that the science behind the threat of polar bear extinction has fallen to pieces too, it’s worth looking at how the German elite media has approached the story, at least those who have not chosen to ignore the inconvenient embarrassment altogether, as most have done.
Der Spiegel, to their credit, has given the story online, front page treatment here. But one notices that Der Spiegel couldn’t help making it sound like an intrigue involving environmentalists, power politics and oil companies lurking in the background pulling strings:
It is a mysterious story that a research suspense story needs: Powerful oil companies in the background, dead polar bears as icons of climate change – and a scientist under suspicion.”
Der Spiegel then goes on to explain why Monnett was suspended, first explaining how Monnett sees it:
In Monnett’s view, scientists like himself are standing in the way of the Obama-government to open up the ocean area off the coast of Alaska for oil drilling. That’s what the government of the state under Republican Sean Parnell wants. That’s what the oil companies, foremost Shell, want. And that’s what the White House wants.”
According to Der Spiegel, Monnett views himself as a lone victim of a conspiracy for having played a major role in getting the polar bear on the list of endangered species. Finally, past the half way point of the piece, Der Spiegel points out that Monnett’s scientific work was indeed sloppy and grossly lacked data:
Indeed the hated scientist had to admit last winter in a hearing that hardly any documentation for viewing the dead polar bears exists. There were no clear photos. The animals also did not show up in any official datasets of the expedition.”
Der Spiegel also noticed that the peer review was everything but rigorous, the paper sailed through the process with hardly any scrutiny:
In the publication of the Polar Biology article, it appears no one was disturbed by this. Even in-house reviewers in Monnett’s office as well as three anonymous peer reviewers of the journal simply waved the paper through with only slight modifications.”
Share this...FacebookTwitter "
"Algae isn’t just found in your garden pond or local river. Sometimes it explodes into vast “blooms” far out to sea, that can be the size of a small country. Such algal blooms can match even a rainforest at taking carbon out of the air. And then, in just a week or two, they are gone – sometimes consumed by viruses.  Given the scale of blooms and their vital role in both marine ecology and climate regulation we must know more about these viruses. Research conducted with our Weizmann Institute colleague Yoav Lehahn and others and published in the journal Current Biology, is the first attempt to quantify the affect of viruses on large scale algal blooms. Algae in this context refers to tiny sea organisms known as phytoplankton which exist right at the bottom of the marine food web, providing the ultimate source of all organic matter in the sea. They do this by consuming carbon dioxide during photosynthesis, “fixing” this carbon into organic matter (themselves) in the same way trees take carbon out of the air.  Therefore phytoplankton serve as a major sink, or “biological pump”, of atmospheric CO2. Some studies show that, although they account for less than 1% of the photosynthetic biomass on Earth, phytoplankton fix almost half of the world’s total organic carbon. Blooms in the ocean are controlled by two processes. First, the biological or ecological factors – the interplay between nutrients, predators and pathogens. More nutrients means more algae; more algae-eating fish, or algae-attacking viruses means fewer blooms. And then you have the physical processes – the stability of the upper ocean and the temperature and salinity of the water and the speed of its currents.  In our study, we developed an approach to trace and follow unique patches of plankton blooms in which the physical conditions are almost constant. Such patches can be viewed as unique ecosystems in which any changes can be attributed mainly to biological or ecological processes. We used satellite data over such a patch in the North Atlantic to track the whole life cycle of a bloom in a phytoplankton species called Emiliania huxleyi. We are able to do this as satellites are able to detect the presence in water of pigments such as chlorophyll used for photosynthesis. The physical properties of this patch were stable throughout the life of the bloom – the sea didn’t suddenly change temperature, get more salty or drift in an unusual direction. Therefore we know that most changes in the algae were caused by biological processes. Recent studies of marine microbiology showed viruses play a key role in regulating algae populations, killing off algal blooms in the same way viruses regulate human populations during an epidemic. Such processes were documented in the lab and in local measurements in confined zones in the ocean. But what was missing was real-world verification over large scale natural bloom. We wanted to find a mass viral-induced phytoplankton demise in nature. We set out to find a unique fingerprint of a virus-driven demise and to use this to measure the impact of viruses on large scale oceanic blooms. Combining satellite data with field measurements of viral activity during an algal bloom in the North Atlantic ocean allowed us to conclude that the demise of this specific bloom was due to viral infection.  We used this newly established method of following the complete life cycles of a distinct bloom to estimate the amount of carbon that is turned over by viruses that infect this bloom. We were able to calculate the life-span of this phenomenon and how it has been affected by viral infection. An algal patch with an area of around 1,000 km2 can “fix” around 24,000 tons of organic carbon throughout its life time. This is equivalent to a similarly sized patch of rain forest. And this is, of course, only one patch out of many.  The big impact viruses have on entire ecosystems was already well known. But now, for the first time, we can quantify their immense impact on open ocean blooms: in just two weeks such viruses can “consume” a huge algal bloom that harbours many tons of carbon. The exact fate of the organic carbon that is realised by viral attack is not clear. Most of it is probably recycled back into the atmosphere by bacteria that use the organic carbon as substrates for respiration (a process called The Viral Shunt).  Another option is that the viral infection release sticky molecules like sugars and lipids which make organic carbon sink faster to the ocean bed. If the latter scenario is true it will have a profound impact on the efficiency of CO2 “pumping” from the atmosphere to the deep ocean. This carbon will  have a better chance of being buried in the ocean sediment. The ability to quantify viral infections from space, and the rapid timing of this bloom demise on such large scales holds a promise for understating the fate of carbon in the oceans."
"

Everyone who reads _Science_ — the journal of the lobbying organization the American Association for the Advancement of Science (AAAS) — knows that it only accepts one side of the global warming story in its “Compass” and “Perspectives” sections, and in its more opinionated, mainline articles. Anyone who writes otherwise for those sections gets a quick rejection. That’s understandable because global warming is scheduled to pay U.S. scientists about $4.2 billion next year, and the AAAS is just doing its job keeping the customers happy.



But sometimes they go a little overboard in their one‐​sided zeal, particularly when they schedule so‐​called bombshell articles to coincide with the periodic meetings of the signatories to the United Nations’ Climate Change treaty, discussing implementation of the (dead?) Kyoto Protocol. The most recent case of this funereal dance just ended in Milan, Italy.



For Milan, _Science_ published, and then heavily publicized, an article by federal climatologists Tom Karl and Kevin Trenberth, entitled “Modern Global Climate Change.” This reveals that _Science,_ in its plumping for Kyoto, is now publishing material that is decades behind the global warming power curve. 



Karl and Trenberth repeat the usual United Nations saw that there’s “a 90% probability interval for warming from…1.7° to 4.9°C” in the next century.” In fact, the 21st century warming rate is now well‐​known to be confined to a much lower and smaller range, about 0.75 +/- 0.25°C per 50 years, and may be lower than that. 



You can’t even generate a constant rate of global warming unless carbon dioxide goes up exponentially. In other words, a constant increase in carbon dioxide must lead to a damped (slowing) response in warming. This has been known since 1872.



Karl and Trenberth give the impression that this exponential increase is happening. It’s not. But, they write: “Recent greenhouse gas emission trends in the United States are upward, as are global emission trends, with increases between 0.5 and 1% per year over the past few decades.”



The problem here is one of purposeful imprecision, as in “past few decades.” In reality, data from the Energy Information Administration show that there was some substantially exponential growth in emissions, but since 1980 it’s been much closer to a simple linear change. Twenty‐​four years of recent linearity comprises “a few decades,” doesn’t it?



This change in emissions is reflected in changes in the growth rate of atmospheric carbon dioxide, which stabilized nearly 30 years ago. That’s right. While all scientists have glibly assumed an exponential increase in atmospheric carbon dioxide, that stopped, in the statistical sense, three decades ago. But an exponential increase is required to generate a constant rate of warming.



What happened? Per capita emissions of carbon dioxide peaked around 1980 and have been in statistically significant decline ever since. 



What perpetuates the tired myth of exponentially increasing carbon dioxide? It’s the oft‐​repeated saw that “Everyone in the world aspires to a U.S. lifestyle.” Since we used to emit about 30 percent of the world’s industrial belching of CO2, the math becomes obvious if everyone emulates us.



People who assumed increases in per capita carbon dioxide were wrong 25 years ago and they are wrong now. But this is precisely what is input into every general circulation climate model, and these models serve as the basis for Karl and Trenberth’s projections for warming. They’ve been run with the wrong data for a quarter century! 



If you put in the right data, warming drops dramatically, to about 1.6°C in the next 100 years. A while back, in a statement he would probably like to have back, Robert Watson, then head of the U.N.‘s Intergovernmental Panel on Climate Change, allowed that such a small warming might actually be beneficial.



Why was everyone wrong? Well, it turns out that the world is largely emulating the United States. Per capita incomes are increasing. As they increase, per capita emissions drop because people can invest in more efficient technology. In what large nation did the drop first take place? The good ‘ole USA.



How on earth did _Science_ become so derriere in the face of so much reality? Perhaps that’s what happens when one’s political goals get in the way of one’s science.
"
"
Share this...FacebookTwitterTwo short items today. One a winter forecast for Central Europe and the other is a look back at Germany’s recent wet and “warm” summer.2011/2012 Winter forecast
Another forecast for Germany (Central Europe) for the 2011/12 winter is out. This one is from Dominik Jung, a young whippersnapper meteorologist for Germany’s leading daily tabloid Bild, read here in German in a piece called: Weather Expert Expects A Shivering Winter, h/t: Reader Ike
Jung believes the 2011/2012 Winter, i.e. December, January and February, will be colder than the mean temperature for the period of 1960-1990, which has been designated as being “normal”. Bild quotes Jung:
Already during the last three years it was up to 2°C colder than the average. If that happens again this year, which we believe it will, then it would be the fourth cold winter in a row and so a small sensation.”
Jung believes that it will be especially cold in Southern Germany with lots of snow and ice. I remind you that Jung is a warmist, and so his words need to be taken with caution when assessing the quality of his science. After all, warmists do believe CO2 is the major driver of climate, and that other factors like the sun and oceans are irrelevant.
Claim that German summer 2011 was too warm is “stupidity”
We just heard that Great Britain had its coolest summer in 20 years and how a number of butterflies died off as a result. Things were not much better in Germany, which just had one of its coolest summers in 20 years. But that didn’t keep the warmstream media from declaring it as “too warm”.
Readers Edition has a short piece here and brings our attention to a video clip of a discussion on German NDR public TV.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The video is some sort of talk show where guest meteorologist Stefan Kreibohm explains why Germany’s summer was a washout this year. At the 2.20 mark the question comes up on whether or not this was an unusual event and if this is the sort of thing we will all have to start getting used to in the future and if we are in a “death spiral”.
Quite surprisingly Stefan Kreibohm answers “no” and says:
It is simply nature’s mood. It happens. There have been summers where it hardly rained and there have been summers where it has rained a lot. This year we happened to have a summer where it rained a lot.”
and over the last 300 years, he adds:
There have always been warmer phases and colder phases.”
The lady then brings up an interesting point at the 3:31 mark and remarks that she perceived the summer to be cool, and so asks how come we are told it was warmer than normal, with a gentleman adding that in June parts of Germany even saw a frosty night. At the 4:05 mark Kreibohm explains:
There’s actually a dispute among meteorologists. The German Weather Service always uses the period of 1960 to 1990 as a reference. This was a time that was a little bit colder than today. So if you compare this summer with that, we see that it was warmer then normal. That’s nonsense – no one understands that. So if you tell someone the summer was too warm, it’s of course pure stupidity. If you compare it to the last 30 years, it was completely normal.”
Well shiver me timbers! A bit of reality here, and on German public television no less! Expect the producers to be reprimanded and Kreibohm never to be invited again.
 
Share this...FacebookTwitter "
"Brazil’s Atlantic forest – Mata Atlântica – is one of the world’s great biodiversity hotspots, rivalling even the Amazon. Running on and off for several thousand kilometres along the coast, the forest is home to 10,000 plant species that don’t exist anywhere else, more bird species than the whole of  Europe, and more than half of the country’s threatened animal species. Today, the ecosystem it sustains is under threat: trees have been cleared for farms, houses and roads, big cities such as Rio de Janeiro and São Paulo have grown in the region, and just 15% of the original forest remains. But it’s not all doom and gloom for the Atlantic forest. Myself and collaborators from Universities of São Paulo, Michigan, Toronto, and UNESP, have published a study in the journal Science which shows that paying farmers to conserve areas of forest within their property is good value for money. The key message from our work is that it is possible to protect native species, maintain a healthy ecosystem and potentially reduce poverty, all for less than US$200m each year. These results are interesting for at least two reasons. The first is very simple, our conclusions are not calamitous – instead of showing that it is the end of world as we know it, we show that human welfare and conservation needs can both be satisfied for a reasonably small amount of money. The headline number seems large, but it only represents less than 0.01% of Brazil’s GDP. The second reason is much more complicated, however. Brazil is heading for an election in October, and as it stands anything could happen. This is important as the current government is in the process of relaxing the Brazilian Forest Code which would, among other things, allow farmers to set aside a smaller proportion of their land to native habitat. This has sparked a fiery discussion among conservationists, scientists, politicians and farmers. Previously, the code required farmers living within the Atlantic forest to set aside 20% of their land for native habitat. Farmers were prevented from designating land they couldn’t use anyway such as particularly steep terrain or areas close to rivers, so many had to set aside more than 20% of their land. In theory, this was great for biodiversity; in practice it never worked as farmers didn’t respect the law, often because they couldn’t afford the economic costs of setting aside productive agricultural land for conservation. The other side of the coin is that the new code is unlikely to protect the Atlantic forest’s species. But, up until now, nobody knew how much habitat really was needed. What our results show is that at least 30% of the forest area needs to be set aside for conservation if we are to preserve a healthy ecosystem. This is great news for Brazilian scientists and conservationists, because now they can use a number to base their arguments while discussing the changes to the Forest Code, instead of saying the usual “more the merrier”. But the suggestion that more forest is needed to preserve biodiversity doesn’t mean the battle between farmers and conservationists has to continue. This is because the onus of increasing forest cover from 20% to 30% doesn’t have to solely fall on farmers. There are already some schemes that pay farmers to set aside part of their land for the protection or restoration of native habitats. These schemes are usually run by local governments or NGOs and the rationale is that they are paying people to protect crucial ecosystem services, such as carbon storage, watersheds (or water quality) and the functions provided by a healthy spread of plants and animals such as pollination, or pest control. We show that if the Brazilian government expands these schemes, then we can have both happy farmers and happy biodiversity. This does not mean that every single farm in the Atlantic forest would have to set aside 30% of its land for conservation. It also does not mean that we would protect all species from extinction, as some need 100% of pristine forest.  But if priority areas were restored to at least 30% native habitat cover, the price to pay would be less than 6.5% of what Brazil currently spends on agricultural subsidies. Farmers willing to set aside land for conservation would receive regular payments, local communities would receive the benefits of enhanced ecosystem services, and native species would be protected. Sounds like a good deal. Whoever wins Brazil’s election will have to deal with either angry farmers or angry scientists and conservationists. Our study thus reveals a promising light at the end of the tunnel – the suggestion that the new government might not need to make compromises; potentially this is a battle everybody can win."
"
Share this...FacebookTwitterh/t: Dirk H


Note how the reporter is upset about people going out in the storm when he himself is outside right smack in it. Then again, the winds are only 50 mph (and not the 500 mph the media was telling us yesterday :)). 
I noticed the report is from “The Weather Channel”. Isn’t that Heidi Cullen’s warmist outfit?
Share this...FacebookTwitter "
"
Guest Post by Willis Eschenbach
One of the arguments frequently applied to the climate debate is that the “Precautionary Principle” requires that we take action to reduce CO2. However, this is a misunderstanding of the Precautionary Principle, which means something very different from the kind of caution that makes us carry an umbrella when rain threatens. Some people are taking the Precautionary Principle way too far …

Figure 1. Umbrella Exhibiting an Excess of Precaution
The nature of the Precautionary Principle is widely misunderstood. Let me start with the birth of the Precautionary Principle (I’ll call it PP for short), which comes from the United Nations Rio de Janeiro Declaration on the Environment (1992). Here’s their original formulation:
“In order to protect the environment, the precautionary approach shall be widely applied by States according to their capability. Where there are threats of serious or irreversible damage, lack of full scientific certainty shall not be used as a reason for postponing cost-effective measures to prevent environmental degradation.”
This is an excellent statement of the PP, as it distinguishes it from such things as carrying umbrellas, denying bank loans, approving the Kyoto Protocol, invading Afghanistan, or using seat belts.
The three key parts of the PP (emphasis mine) are:
1)  A threat of serious or irreversible damage.
2)  A lack of full scientific certainty (in other words, the existence of partial but not conclusive scientific evidence).
3)  The availability of cost-effective measures that we know will prevent the problem.
Here are some examples of how these key parts of the PP work out in practice.
We have full scientific certainty that seat belts save lives, and that using an umbrella keeps us dry. Thus, using them is not an example of the PP, it is simply acting reasonably on principles about which we are scientifically certain.
There are no scientific principles or evidence that we can apply to the question of invading Afghanistan, so we cannot apply the PP there either.
Bank loans are neither serious nor irreversible, nor is there partial scientific understanding of them, so they don’t qualify for the PP.
The Kyoto Protocol is so far from being cost-effective as to be laughable. The PP can be thought of as a kind of insurance policy. No one would pay $200,000 for an insurance policy if the payoff in case of an accident were only $20, yet this is the kind of ratio of cost to payoff that the Kyoto Protocol involves. Even its proponents say that if the states involved met their targets, it would only reduce the temperature by a tenth of a degree in fifty years … not a good risk/reward ratio.
Finally, consider CO2. The claim is that in fifty years, we’ll be sorry if we don’t stop producing CO2 now. However, we don’t know whether CO2 will cause any damage at all in fifty years, much less whether it will cause serious or irreversible damage. We have very little evidence that CO2 will cause “dangerous” warming other than fanciful forecasts from untested, unverified, unvalidated climate models which have not been subjected to software quality assurance of any kind. We have no evidence that a warmer world is a worse world, it might be a better world. The proposed remedies are estimated to cost on the order of a trillion dollars a year … hardly cost effective under any analysis. Nor do we have any certainty whether the proposed remedies will prevent the projected problem. So cutting CO2 fails to qualify for the PP under all three of the criteria.
On the other side of the equation, a good example of when we should definitely use the PP involves local extinction. We have fairly good scientific understanding that removing a top predator from a local ecosystem badly screws things up. Kill the mountain lions, and the deer go wild, then the plants are overgrazed, then the ground erodes, insect populations are unbalanced, and so on down the line.
Now, if we are looking at a novel ecosystem that has not been scientifically studied, we do not have full scientific certainty that removing the top predator will actually cause serious or irreversible damage to the ecosystem. However, if there is a cost-effective method to avoid removing the top predator, the PP says that we should do so. It fulfils the three requirements of the PP — there is a threat of serious or irreversible damage, we have partial scientific certainty, and a cost-effective solution exists, so we should act.
Because I hold these views about the inapplicability of the precautionary principle to CO2, I am often accused of not wanting to do anything about a possible threat. People say I’m ignoring something which could cause problems in the future. This is not the case. I do not advocate inaction. I advocate the use of “no-regrets” actions in response to this kind of possible danger.
The rule of the no-regrets approach is very simple — do things that will provide real, immediate, low-cost, tangible benefits whether or not the threat is real. That way you won’t regret your actions.
Here are some examples of no-regrets responses to the predicted threats of CO2. In Peru, the slums up on the hillside above Lima are very dry, which is a problem that is supposed to get worse if the world warms. In response to the problem, people are installing “fog nets“. These nets capture water from the fog, providing fresh water to the villagers.
In India’s Ladakh region, they have the same problem, lack of water. So they have started building “artificial glaciers“.These are low-cost shallow ponds where they divert the water during the winter. The water freezes, and is slowly released as the “glacier” melts over the course of the following growing season.
These are the best type of response to a possible threat from CO2. They are inexpensive, they solve a real problem today rather than a half century from now, and they are aimed at the poor of the world.
These responses also reveal what I call the “dirty secret” of the “we’re all gonna die in fifty years from CO2” crowd. The dirty secret of their forecasts of massive impending doom is that all of the threatened catastrophes they warn us about are here already.
All the different types of climate-related destruction that people are so worried will happen in fifty years are happening today. Droughts? We got ’em. Floods? There’s plenty. Rising sea levels? Check. Insect borne diseases? Which ones would you like? Tornados and extreme storms? We get them all the time. People dying of starvation? How many do you want? All the Biblical Plagues of Egypt? Would you like flies with that?
Forget about what will happen in fifty years. Every possible climate catastrophe is happening now, and has been for centuries.
So if you are truly interested in those problems, do something about them today. Contribute to organizations developing salt resistant crops. Put money into teaching traditional drought resisting measures in Africa. Support the use of micro-hydroelectric plants for village energy. The possibilities are endless.
That way, whether or not the doomsayers are right about what will happen in fifty years, both then and now people will be better prepared and more able to confront the problems caused by the unpleasant vagaries of climate. Fighting to reduce CO2 is hugely expensive, has been totally unsuccessful to date, will be very damaging to the lives of the poorest people, and has no certainty of bringing the promised results. This is a very bad combination.
Me, I don’t think CO2 will cause those doomsday scenarios. But that’s just me, I’ve been wrong before. If you do care about CO2 and think it is teh eeeevil, you should be out promoting your favorite no-regrets option. Because whether or not CO2 is a danger as people claim, if you do that you can be sure that you are not just pouring money down a bottomless hole with very poor odds of success. That’s the real Precautionary Principle.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8f1dbead',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Excerpts: from the Sunday Times: Polar bear is a ‘new’ species
by Jonathan Leake
Polar bears may  have come into existence only 150,000 years ago, when trapped brown  bears had to adapt to an ice age
Kissing Cousins? Oreo the brown bear and Ahpun the polar bear play at the Alaska Zoo. Photo from the Alaska Daily News by BOB HALLINEN / Daily News archive 1998
Polar bears may have come into existence only 150,000 years ago, when  brown  bears were trapped by an ice age and had to adapt quickly to survive,  scientists have found.
The suggestion follows the discovery of the jawbone of an animal that  died up  to 130,000 years ago, making it the oldest polar bear fossil found. The  bone  has yielded new insights into the origins of Earth’s largest land  predator.
One is the possibility that polar bears owe their existence not only to  past  climate change, including ice ages, but have also survived at least one  long  period of global warming.
The bone was discovered at Poolepynten on the Arctic island of Svalbard  by  Professors Olafur Ingolfsson, of the University of Iceland, and Oystein  Wiig, of the University of Oslo.
…
In a paper they said: “Brown bears of the ABC islands may be descendants  of  ancient ursids [bears] that diverged from other lineages of brown bears  and  subsequently founded the polar bear lineage.” This view is expected to  get  support from new research, out this week, based on DNA extracted from  the  Poolepynten jawbone.
It means polar bears have already survived a global warming that  affected the  northern hemisphere from 130,000 to 115,000 years ago, when the  Greenland  ice sheet and the Arctic ice cap were smaller than now. Professor Chris  Stringer, of the Natural History Museum in London, an expert in ice  ages,  said: “Early polar bears would not have had all the specialisations of  modern animals and we know nothing about their behaviour.
“Living through a warm period back then does not mean they are resilient  to  climate change now.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8d8ce264',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**""It's going to look horrible."" The simple truth about the Spending Review according to a senior MP.**
The chancellor will bang the drum for his plans to keep people in jobs, or help find new ones. Rishi Sunak will take out the metaphorical megaphone to explain how he'll allocate billions of taxpayers' cash to spend on infrastructure in the coming months.
But the headlines of the Spending Review, when governments put their money where their mouths are, won't be in any rhetorical flourishes at the despatch box, nor likely in any surprise announcements kept back as goodies for the public.
What may shock, is the cold reality of the cost of the coronavirus, which will be laid bare in the tables and charts published at the same time, presenting to the country in the shape of statistics from the Office for Budget Responsibility how much damage the pandemic has really done to how we make a living.
Without poring over the spreadsheets, the ""horrible"" will mean a massive gap between what the government takes in tax and what is has been spending, a deficit more than ten times what it was last year.
There will be an estimate of the number of people who may end up unemployed, perhaps nearing three million before too long.
It's likely to mean a freeze on pay for much of the public sector; a cut, even if temporary to the amount of cash the UK spends on foreign aid; tight spending limits for government departments on their day-to-day spending and eye-watering levels of debt and borrowing.
One former Treasury minister, who is not prone to hyperbole (unusually for a politician you might wonder) described it as a ""multigenerational debt which will have implications for the rest of our lives in terms of what the British state can afford"".
We will on Wednesday, they suggest, ""learn a great deal about the problem"", what months of emergency spending has done to the economy. But before we go on, don't hold your breath to learn much about any solutions.
The chancellor and prime minister have decided politically that while budgets will be tight (and let's see the black and white to assess this for real) there can't be a return to the kind of squeeze of the Cameron and Osborne era.
No one in government would pretend in private there is any way to avoid tax rises at some point. But Mr Sunak is not going to announce any of that on Wednesday - any big ways of raising money to fill the hole won't come until the Budget next year at the earliest, and perhaps not until after that.
But Wednesday's review will sketch out the very, very serious challenge for the country's finances that is on the way.
Most importantly of course that will be reflected in the number of people who might lose their jobs with all the distress that entails, all the business that could be lost, and the impact on people's pay packets. But it also sets the backdrop for the decisions that our politicians have to make, and will be confronted with for many, many years to come.
It's notable that while there have been some skirmishes around the edges in the last nine months, there has been very little tension over the government and the Bank of England's central actions to write enormous cheques, and keep the signatures coming as the pandemic has progressed.
And it's far from over. But as time goes on the exit from the emergency leaves the government with extremely difficult political decisions.
There is no appetite to break any of the prime minister's expensive manifesto promises.
New Tory MPs, particularly in new Tory seats are chomping at the bit for evidence to show to their constituents they made the right decision.
One former minister said, ""our voters want something tangible they can see at the end of their street,"" and they want it fast.
But the chancellor also, according to his allies, says ""we have to be the party of looking after people's money - he says, if we lose that, why don't you just vote Labour?""
The argument works the other way too, to an extent. If the Tories are racking up levels of public spending that are previously unimaginable, the traditional gap in economic vision doesn't leave Labour with that much space.
How and when will either of the main parties try to confront what has really gone on as the cost of trying to deal with the pandemic has gone up and up and up?
Some ministers worry even many MPs haven't yet understood the real consequences for how we make a living - the damage the decisions made to protect the country during the emergency of the pandemic have had on the economy.
But Wednesday will be the first time, eight months in, when we will be confronted with the size of the likely bill. The argument about who and how to pay will dominate for many years to come."
"

One of the oft‐​encountered talking points offered by the Left is the extent to which the Bush administration has alternatively ignored, intimidated, and done violence to the scientific community. The picture being painted is that of a know‐​nothing Christian fundamentalist in the thrall of corporate America waging unremitting war against the Enlightenment.   
  
  
While there is enough truth to this charge to give it legs, the “science” lobby is scarcely blameless. For all the moral and ethical posturing surrounding the sanctity of “the scientific process” and the need to keep the same safe from assaults by power‐​hungry politicians and ignorant political mob action, climatologist James Hansen’s recent call to literally criminalize disagreement with him about climate change is a more radical assault on the the scientific process and the scientific method than anything forwarded by the Bush administration.   
  
  
Now, James Hansen would probably argue that he’s not interested in criminalizing disagreement _per se_ ; he’s interested in criminalizing dangerous, life‐​threatening speech that the speaker _knows_ is fraudulent. Perhaps. But exactly what is the nature of this special mind‐​reading power that allows James Hansen to determine that Rex Tillerson, head of ExxonMobil, believes X but says Y? Is it so beyond the realm of possiblity to think that Rex Tillerson actually _believes_ what he says (pace, say, commentary by our own Pat Michaels on the subject)? Or does James Hansen presume to know Pat Michaels’ true and secret thoughts as well?   
  
  
To the extent that James Hansen’s views are embraced by the self‐​appointed gendarmes of science, politicians are right to suspect that climate change alarmism is heavily influenced by the lust for power, the demands of ego, and the pursuit of political agendas that go far beyond a disinterested search for scientific truth. Moreover, one can’t help but wonder about the strength of an argument that requires the threat of force to silence critics.   
  
  
Call me an idealogue, but criminalizing skepticism about scientific theories is probably not the best way to facilitate the quest for scientific truth.
"
"The amount of water at the Earth’s surface is pretty constant, but in many parts of the developed world we are running out of the right sort of water, and our ability to access it. The severe water shortages experienced in California and the southwestern US, in Australia, and even parts of the UK show we need new methods for ensuring a clean water supply. One is to produce high quality water from wastewater, something that is improving all the time. While this could help relieve the strain on water supplies, public attitudes to the idea of using water that is recycled from sewage and other wastewater streams for drinking and domestic use is the more significant barrier. The treatment and reuse of “grey” water (waste from baths, showers, washing machines and so on) for non-drinking uses such as irrigation is already widespread. But as the demand for water grows and supplies continue to dwindle, more and more attention is being paid to “black” water – in simple terms, sewage.  Technological advances and environmental regulations have made the production of very high quality water from black wastewater streams not just feasible, but increasingly an economic and political necessity. The challenge facing water engineers now is arguably just as significant: convincing the public to accept sewage water recycled in this way for mainstream domestic consumption. Let’s be clear. Untreated sewage is dangerous stuff, responsible throughout history (and all-too-often still today for many communities worldwide) for more deaths, disease and misery than pretty much any other single cause.  Industrial wastewater treatment is rightly considered one of the wonders of the modern world. Customers of modern water utilities companies expect reliable, high-quality water supply and removal as a given, to the extent that the majority have no idea where their water comes from, or goes to. In practice, of course, wastewater discharged into the environment from one community has long become the source water for another community downstream – think Oxford, to Reading, to London in a chain along the river Thames. Urban myths about the number of people who have already tasted a Londoner’s tap water are deeply ingrained and somehow accepted. But when asked directly about the acceptability of recycled wastewater as a direct feed into potable supplies, attitudes harden. In an Oregon State University survey in 2008, while a majority supported a specific water recycling proposal in principle, the percentage of people strongly agreeing with potential applications dropped to as low as 13% for uses associated with human contact or consumption, from around 55% for other industrial and municipal uses.  In a 2013 poll for The Guardian newspaper, 63% of respondents claimed they would drink recycled sewage water, but the context was broader and the question more hypothetical than in the Oregon study.  This psychological factor is important: like the fly in your soup, we are put off when a problem is placed close at hand. The key is to add steps in the process – discharging treated wastewater to the river before abstracting it again for drinking. A 2012 Southern Water study suggests this approach would be acceptable, if the quality could be guaranteed. Recent evidence on the prevalence of antibiotic-resistant microbes in treatment plants highlights the need for ongoing technical development to combat emerging threats to health and environment. Other concerns lie around persistent organic pollutants such as pharmaceuticals, which may be concentrated by repeated recycling of black wastewater. In striving to introduce recycled water systems, water engineers face the challenge of tackling real and perceived threats to water quality, mistrust of commercial utilities and government authorities, and a deep-rooted fear of contaminated water. Ironically, climate change could be part of the answer. Wichita Falls, Texas, became in July 2014 the first place in the world to implement 50:50 mixing of directly recycled wastewater in domestic supplies. Residents are largely philosophical about their “potty water”, but then they’re experiencing the worst drought in 70 years with extreme restrictions on water use. In Wichita Falls, it’s state politicians and regulators rather than consumers that are the largest hurdles the scheme must jump. Water resource managers occupy a shifting landscape between technological capability, political precaution, and public attitudes which can swing strongly and quickly. Navigating this difficult terrain while introducing engineering answers that work is complex, but the evidence suggests that trust is key to public acceptance.  In California, Israel, Australia and Singapore, environmental concerns, price incentives, fines and even national security have been used to convince people of the need to adopt wastewater recycling. Information campaigns, celebrity endorsements, aggressive branding and collaboration with trusted independent organisations are designed to reduce the yuck factor.  In the final analysis however, necessity and urgency are the most effective levers of opinion, as Wichita Falls appears to prove. Perhaps the real challenge for water engineers is to find a way to secure the infrastructure for resilient, sustainable water supplies almost behind the scenes, ready to press the button when circumstances drive public and politicians to accept the unacceptable."
"**There have been more than 1.6 million confirmed cases of coronavirus in the UK and about 60,000 people have died, government figures show.**
However, these figures include only people who have died within 28 days of testing positive for coronavirus and other measures suggest the number of deaths is higher.
**Find out how the pandemic has affected your area and how it compares with the national average:**
If you can't see the look-up click here.
After the first peak in April, cases started rising again in July, with the rate of growth increasing sharply in September and October, before falling again in the past two weeks.
On Wednesday, the government announced a further 16,170 confirmed cases.
It is thought the infection rate was much higher during the first peak in spring, but testing capacity at the time was too limited to detect the true number of daily cases.
The data for cases can also be broken down by region and comparing the change in those figures by week gives a sense of where there has been a recent increase in newly-reported infections.
Coronavirus infections in England have fallen by about a third during lockdown, according to a major study carried out by Imperial College London.
While some of the worst-hit areas saw the biggest improvements, cases have remained relatively high across England, according to the React-1 study, which is based on tests of more than 100,000 people between 13-24 November.
The latest figures from the Office for National Statistics (ONS), suggest about one in 185 (16,400 people) in Wales had the virus in the week ending 21 November.
In Northern Ireland rates are thought to be decreasing at around one in 145 people (about 12,700 people), while in Scotland, the figure was one in 115 (about 45,700 people).
The average number of daily deaths began to rise again in September, following the first peak in April.
On Wednesday, the government announced a further 648 deaths.
Of these, 555 deaths were in England, 51 in Wales, 38 in Scotland, and four in Northern Ireland.
Rules were amended over the summer to include deaths in the coronavirus total only if they occurred within 28 days of a positive test. Previously in England, all deaths after a positive test were included.
England has seen the majority of UK deaths from Covid-19. Using the 28-day cut-off, there have been more than 52,000.
Although hospital admissions for Covid-19 remain below the levels seen in the spring, there are big regional disparities.
The North West, North East and Yorkshire, and Midlands have seen the highest number of admissions but the situation in all three now appears to be improving.
Cases have risen across large parts of England, with other spikes in areas of Scotland, Wales and Northern Ireland.
The red areas on the map below are those currently seeing the highest number of cases per 100,000 people.
Restrictions have been tightened across the UK in an effort to tackle the number of rising cases.
In England, each local authority has been placed in one of the new three tiers following the end of the recent national lockdown.
Wales has announced a range of further measures, including a ban on pubs, restaurants and cafes serving alcohol, from Friday 4 December.
In Northern Ireland, a further two-week ""circuit-break"" lockdown began on 27 November.
Scotland has a five-tier system of alert levels with different measures in place in different parts of the country.
UK leaders have agreed to allow up to three households to meet indoors during a five-day Christmas period of 23-27 December.
When looking at the overall death toll from coronavirus, official figures count deaths in three different ways.
Government figures count people who died within 28 days of testing positive for coronavirus.
But there are two other measures.
The first includes all deaths where coronavirus was mentioned on the death certificate, even if the person had not been tested for the virus. The most recent figures suggest there had been almost 70,000 deaths by 20 November.
The second measure counts all deaths over and above the usual number at the time of year - that figure was more than 75,000 by 20 November.
The most recent figures available from the ONS are for the third week of November, which show there were 14,276 deaths registered in the UK.
Some 3,038 of these deaths involved Covid-19 - 199 more than the week before.
Deaths normally do rise at this time of the year, but the data from the ONS and its counterparts in Scotland and Northern Ireland show the second wave of the virus has pushed the death rate above the average seen over the past five years by about 21%.
Overall, the figures are still well below the peak of 9,495 deaths recorded in a week, reached on 17 April.
The ""R number"" is the average number of people an infected person will pass the disease on to.
If R is below one, then the number of people contracting the disease will fall; if it is above one, the number will grow.
The government's current estimate for the R number across the whole of the UK is 0.9 to 1.
The estimate for England is 0.9 to 1, while for Scotland it is 0.8 to 1. The estimate for Wales is 0.8 to 1 and in Northern Ireland it is 0.9 to 1.1.
The government has said in the past that the R number is one of the most important factors in making policy decisions."
"
Share this...FacebookTwitterPerhaps the images of angry mobs in Greece and forecasts of a bitter cold winter for Europe are having a sobering effect on our CO2-drugged up politicians here in Europe.
And as other countries line up behind Greece on the path to discontent, perhaps its a good idea, after all, to stop raking honest citizens with the global warming scam.
At least EU Energy Commissar Günther Oettinger may be getting it.
The online Stern magazine reports here that Oettinger has a warning on the dangers of rising energy costs.
Higher energy costs do not only threaten the businesses here, but also the social peace when a part of the population is not able to afford paying the electric bills.”
Yeah, it can really piss people off when the power gets shut off in the middle of winter. Oettinger, at an international business conference last Saturday, added:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




I’m surprised at how thoughtlessly the development of the electric prices have been taken. This is one reason for the steady de-industrialisation – energy intensive production is being sent overseas.”
And most the production that is done overseas uses much more energy and entails higher greenhouse gas emissions. But who cares, Germany can pat itself on the back for its contribution in rescuing the planet from greenhouse gases.
We need to recall here that Oettinger is likely only speaking the words his audience wants to hear, and that tomorrow he’ll be at some environmental conference and will be spewing just the opposite-  on how “green” jobs are the future. It’s this kind of dishonesty and insincerity that really fuels the mobs out there. So let’s look at what Oettinger will say tomorrow.
Oettinger also provided a few other wise words during his rare few minutes of good sense. Stern writes:
For this reason Germany has to reconsider its subsidies in solar energy. ‘The Energy Feed-in Act was clever, but there comes a time when we have to wake up and realize that the sun shines brighter and longer in other countries,'”
Just come out and say it, Herr Commissar, solar energy was a flop.
 
Share this...FacebookTwitter "
"

News loves hurricanes. They usually form far, far away, providing at least a week of stories. And they often start with a bang. Down in the tropical Atlantic, young ones bomb out to amazingly low barometric pressures and outrageous sustained winds. Hurricane Ivan’s lowest pressure, for example, would cause the needle on you home barometer to spin around twice. The resultant “eyewall” winds were a 20‐​mile wide tornado.



It’s incredible stuff. But they usually weaken considerably by the time they get to the states, owing to our more northerly latitude and the fact that hurricanes don’t do well when much of their circulation is over land, which has to happen when they approach North America.



That doesn’t stop the hype machine. While we like to count up property damage and losses, no one mentions the fantastic revenue that these storms generate for the media, or that the constant drumbeat of Charley‐​Frances‐​Ivan, Charley‐​Frances‐​Ivan must have political repercussions.



And so, Tony Blair was just in Washington to visit John Kerry, where he conflated Hurricane Ivan with dreaded global warming. 



I like just about everything about Tony Blair. He’s smart, affable, and a real friend to a nation that needs some. But he’s way off on global warming, and advising Kerry to bail out his campaign with apocalyptic climate hype invites a grilling by the climate truth squad, a rather large body of weather nerds in a weather‐​fixated country.



Blair’s problem is that he listens to his science adviser, Sir David King, who is one of the most ill‐​informed hawks on climate change on this greening planet. King actually pronounced the goofy global warming flick “The Day After Tomorrow” as scientifically plausible, which should have completely blown away his credibility. Now he claims that this year’s hurricane activity is a product of global warming and that warming will make hurricanes worse.



Here’s the simplistic argument. Hurricanes require warm water. Global warming means more of that. Therefore, more hurricanes. 



The fact is that there’s plenty of warm water for hurricanes every year–virtually the entire tropical ocean is hot enough, and yet there are only about 10 per year in the Atlantic. The real research question on these storms is not why there are so many but, rather, why there are so few, given the massive expanse of warm water available to them. 



And here’s the real scientific inconvenience in Blair’s story. The planet warmed slightly–much less than forecast by people like King–in the last half of the last century, but while that happened, maximum winds in Atlantic hurricanes DECLINED significantly.



Yep. As shown by scientist Chris Landsea of the National Oceanic and Atmospheric Administration, maximum winds measured by hurricane‐​hunter aircraft over the last 50 years have declined significantly.



Further, there’s a logical (if lawyerly) argument that pins this salutary change on global warming. It goes like this: Atlantic hurricanes are much more delicate than their destruction suggests. One thing they cannot tolerate is a west wind blowing into them because it wrecks their symmetry. As a result, their maximum winds decline.



El Niño–another climate hype machine–generates precisely this type of wind over the Atlantic. That’s why, in El Nino years, the forecast is for a weak hurricane season.



In the latter part of the last century, there were an unusual number of El Niño years compared to previous decades. Some scientists (like David King) claim that global warming is increasing the frequency of El Nino. But if that’s the case, then global warming would be responsible for the decline in maximum hurricane winds.



How much could that be worth? The decline has been about 15 mph since 1950. That’s not a small number because the force of a hurricane’s wind goes up with the square of the velocity. In the high Category Three/​low Four range, this change reduces the power by 25 percent. Given that the U.S. experiences about 15 strong hurricanes every decade, and that the average cost is now about $5 billion for one of those hits, you could, if you buy the El Niño argument (I don’t but some others do), thank global warming saving about $13 billion per decade.



These numbers won’t stop the hype machine on hurricanes. But you’d think that Great Britain’s science adviser would have been sufficiently well informed that he would have kept his prime minister from asking John Kerry to sow the whirlwind.
"
"
Share this...FacebookTwitterI’ve been writing lately on the folly that is the Green Economy, citing a number of examples here in Germany, read here for example.
Walter Russel Mead at The American Interest has a must read on how the once much ballyhooed Green Jobs Initiative stands in the USA today. It isn’t pretty.
Feeding The Masses On Unicorn Ribs
by Walter Russell Mead
Besides healing the planet and returning the rising seas to their natural beds, then-Senator Obama promised that his administration would create beautiful green jobs: well paid, stable, abundant jobs, unionized, with full benefits and making the earth healthier and the American people richer. As President, he stayed on message: even after the truther-enabling “green jobs czar” Van Jones left the administration, green jobs have been one of the President’s signature policies for putting the American people back to work. Continue reading here…
Some excerpts from Mead’s article:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Obama promised to create 5 million green jobs within ten years.”
and
…New York Times has also figured it out that the administration’s green jobs initiative is an embarrassing mess.”
and
What worries me is that they didn’t understand that making something this bogus a central plank of his actual governing plan on an issue as vital as jobs would have serious costs down the road. Many liberals want green jobs to exist so badly that they don’t fully grasp how otherworldly and ineffectual this advocacy makes the President look to unemployed meat packers and truck drivers.”
Read the entire article for more.
Share this...FacebookTwitter "
"

“Donald Trump is undermining the rules‐​based international order.” _The Economist_ ’s headline last summer summarized a common refrain within America’s foreign policy establishment. Trump “wants to undo the liberal international order the United States built,” Thomas Wright of the Brookings Institution warned on Inauguration Day in 2017. Trump could “bring to an end the United States’ role as guarantor of the liberal world order,” Princeton professor G. John Ikenberry wrote.



Trump is certainly hostile to what he sometimes refers to as “globalism”: multilateralism, free trade agreements, international institutions, and any international legal regime that could impose constraints on U.S. power. He is antagonistic toward allies and treaties, withdrawing the U.S. from the Paris climate agreement, the Trans‐​Pacific Partnership (TPP), the Iran nuclear deal, the Intermediate Nuclear Forces Treaty (INF), the UN Educational, Scientific and Cultural Organization (UNESCO), and the UN Human Rights Council.



But those excoriating Trump for his disregard for rules and norms rarely mention similar, routine violations of this rules‐​based order by his predecessors. And while the foreign policy establishment is firm in its condemnation of Trump’s “turning away from global engagement,” as Richard Haass of the Council on Foreign Relations put it, their harshest criticisms seem reserved for those few sporadic instances in which Trump tries to jettison lengthy and failed military deployments, as in Syria and Afghanistan, or expresses insufficient enthusiasm for permanent overseas garrisons.





President Trump is not the first president to weaken the international liberal order.



The pundits, practitioners, and politicians that make up the foreign policy establishment have rarely respected the non‐​interventionist principles at the core of the United Nations, an institution exemplifying the liberal rules‐​based international order that the United States helped establish following World War II. Article 2(4) of the UN Charter says “All Members shall refrain in their international relations from the threat or use of force against the territorial integrity or political independence of any state…” According to the Charter, which American post‐​war planners helped write, the use of force is illegal and illegitimate unless at least one of two prerequisites are met: first, that force is used in self‐​defense; second, that the UN Security Council authorizes it.



This prohibition against war is not some trivial aspiration. Non‐​intervention is the centerpiece of international law and the United Nations has repeatedly sought to underline its significance. In 1965, the General Assembly declared “No state or group of states has the right to intervene, directly or indirectly, for any reason whatever, in the internal or external affairs of any state.” Again in 1970, it unanimously reaffirmedthe illegality of “armed intervention and all other forms of interference or attempted threats.” In 1981, the General Assembly further specified that the Charter’s “principle of non‐​intervention and non‐​interference” prohibited “any … form of intervention and interference, overt or covert, directed at another State or group of States, or any act of military, political or economic interference in the internal affairs of another State.”



The United States is currently engaged in active military hostilities in at least seven countries, namely Afghanistan, Iraq, Syria, Yemen, Somalia, Libya, and Niger. That tally doesn’t include drone strikes in Pakistan, combat operations in Kenya, Cameroon, and Central African Republic, or other interventions of unknown magnitude. The true number might be closer to 14 countries. The White House is also explicitly threatening U.S. military action to change the regime in Venezuela and against Iran for a host of spurious reasons. Not one of these cases meets the prerequisites for legal military intervention (a plausible self‐​defense case can be made for the war in Afghanistan, but it expired a long time ago).



 **No other state in the international system uses force more than** the U.S. has **.** Throughout the Cold War, the United States used military means to interfere in other countries about twice as often as did the Soviet Union. This doesn’t include interventions below the threshold of military action: from 1946 to 2000, Washington meddled in foreign elections more than 80 times (compared to 36 by the Soviet Union or Russia over the same period). Covert operations to overthrow democratically elected governments, as in Iran, Guatemala, and Chile, were a stapleof U.S. conduct in this period, and according to the Rand Corporation, “the number and scale of U.S. military interventions rose rapidly in the aftermath of the Cold War.” The Congressional Research Service lists more than 200 individual U.S. military interventions from 1989 to 2018, a rate that no other country even comes close to matching.



It’s hard for America to act as the guarantor of a rules‐​based order that it consistently violates. When President Obama condemned Russia’s annexation of Crimea in 2014, saying international law prohibits redrawing territorial borders “at the barrel of a gun,” it was somewhat awkward: The United States did exactly that in the 1999 Kosovo war, which lacked Security Council approval, and successive administrations have similarly supported Israel as it annexes and occupies territory in violation of international law. Secretary of State John Kerry castigated Russia’s territorial grab this way: “You just don’t in the 21st century behave in 19th century fashion by invading another country on completely trumped up pretext.” As it happens, that’s a rather apt description of the Bush administration’s brazenly illegal invasion of Iraq in 2003.



Washington often appeals to international law to justify military action against despots who commit atrocities, as it did when it secured UN Security Council approval in 2011 to bomb Libya. But even there, when the initial use of force was authorized, the Obama administration rapidly exceeded the mandate of the resolution by pursuing what amounted to a regime‐​change strategy. And such appeals to humanitarianism are highly selective: U.S. military power has also been used to assist Saudi Arabia, one of the world’s most regressive authoritarian regimes, commit war crimes and keep an impoverished and largely defenseless population in Yemen under siege.



America’s delinquency isn’t restricted to the use of force. Though 139 other countries have done so, Washington has refused to sign on to the Rome Statute, which established the International Criminal Court. And although the United States has badgered China for violating the UN Convention on the Law of the Sea, which defines maritime rights and responsibilities, the U.S. refuses to ratify the treaty itself. For all the talk of China’s unfair trade practices, the only country that receives more formal complaints about WTO violations than China is the United States—and China does a better job of complying once complaints are made.



The political establishment in Washington has always accepted this unique role for the United States. We’re the policeman of the world. We enforce the rules and therefore assert the right to violate them, even as we (often violently) deny others that same prerogative.



 **Any claim to special privileges rests to some extent on whether** the international community sees it as legitimate.The problem is that America’s increasing disregard for the rules has undermined its legitimacy and that of the order itself: More than any other single nation, its actions determine the basis of international norms. As U.S. foreign policy becomes more transparently lawless, the power of international law to constrain state behavior weakens accordingly. To legitimize the Russian annexation of Crimea, President Vladimir Putin actually citedthe “Kosovo precedent.” In 2016, Chinese officials dismissed U.S. criticisms of Beijing’s human rights record by citing the “notorious…prison abuse at Guantanamo.” The United States, Chinese diplomat Fu Cong told the UN Council on Human Rights, “conducts large‐​scale extra‐​territorial eavesdropping, uses drones to attack other countries’ innocent civilians, its troops on foreign soil commit rape and murder of local people. It conducts kidnapping overseas and uses black prisons.” And when American officials lambaste Iran for backing the Syrian regime of Bashar al‐​Assad despite his use of chemical weapons, Iranian officials frequently remind the world that the United States aided Saddam Hussein while he deployed chemical weapons on a much larger scale.



Our hypocrisy has always been a threat to our legitimacy, but in the past it was often managed with careful rhetoric and diplomatic maneuvers designed to conceal the discrepancy between our words and our deeds, to camouflage our violations in language that reinforced the order or appealed to higher values. Trump is distinct from his predecessors not because his foreign policy is a radical departure, but because he is carrying out similar policies without the moralistic righteousness of his predecessors .



Saving the liberal order means adhering to the UN Charter’s prohibition on the use of force except in self‐​defense or unless authorized by the Security Council. It means rolling back our global military footprint and adopting a more restrained foreign policy that at least approximates the manner in which we expect other nations to behave. It means recognizing that the United States is not exempt from the rules and norms it often punishes others for transgressing, and it means acknowledging that the foreign policy establishment has done at least as much damage to the rules‐​based order as has President Trump.
"
"**An outbreak of Covid-19 has taken place at a meat processing factory.**
Cornwall Council said it was aware of ""a number of confirmed cases"" at the Kepak meat processing factory in Bodmin.
Additional testing is now taking place on site for Kepak employees and the factory on the Cooksland Industrial Estate remains open.
Neither Cornwall Council nor Public Health England would say how many cases had been confirmed.
Rachel Wigglesworth, Cornwall Council's director of public health, said employees who needed to isolate had already been excluded from work and given support to isolate.
She said: ""We have been discussing their ongoing measures to ensure it is safe for staff to attend work as normal.""
Kepac said: ""The group is working tirelessly to protect its staff as well as ensuring the continuity of secure food supply during this pandemic."""
"Converting renewable energy into electricity is one thing; converting it into fuel is quite another. The vast majority of global energy demand is for fuel, and a renewable source could help us heat our houses and travel efficiently long into the future. It might even mean we could avoid the conflicts that will arise while competing for the last remaining fossil fuels. Today, we are a step further towards this goal after engineering the gut bacteria E. coli, most famous for the strain of it that causes food poisoning, to make it generate renewable propane. My colleagues and I detail our work in a study published in the journal Nature Communications. Scientific advances now mean we can make microbes churn out useful energy, by changing the way they process energy. These microbes can then convert the “renewable” sunlight (and carbon dioxide) into fuel, either directly or using sugar as an intermediate stop-over.  Although the technology for renewable conversion of solar energy into electricity already works well, this isn’t quite the same as being “renewable energy”. Approximately 85% of total energy demand is actually for fuels, as it is far easier to store energy in fuel rather than as electricity.  Industrial scale production of cheap renewable fuel therefore runs into a big problem. It needs to out-compete fossil fuels – an alternative technology that only needs to pump out the ready product. In searching for a renewable fuel process that could be economically sustainable we focused on propane, a bulk component of liquid petroleum gas. Propane is an attractive target for several reasons.  It’s a gas, which means you could immediately separate the finished product. The microbes who produced the propane would be left behind and the new fuel will escape as a gas. No need for a messy separation.  That said, propane also requires little energy to liquefy, thereby enabling the high-energy density storage that is required for cost-effective usage. There’s a reason your car’s gas tank is actually full of liquid – gas simply takes up too much room. The fact propane is already in commercial use also helps. It’s used as a fuel in rural areas or in industry, and sometimes also for transport. In Italy, for example, thousands of stations sell propane-containing mixtures under the label “Autogas”. You can’t make renewable propane through natural reactions – no organisms naturally pump out propane in the way humans breathe out CO2 or trees exhale oxygen. We therefore turned to synthetic biology, where biology meets engineering, in order to create such a capability. We chose E. coli because it is easy to engineer. Left to its own devices, E. coli takes glucose from its surroundings and breaks it down into smaller carbon molecules, electrons and “internal” chemical energy. These smaller parts are used only as building blocks for cellular growth – to reproduce. In the engineered cells, however, we hijack the assembly line for one of those building blocks known as “fatty acid synthesis”. Fatty acids are normally synthesised mainly in order to generate cell membranes but, by introducing a special enzyme, we can redirect it to instead release butyric acid, the precursor for propane. From there, only two more enzymes were needed in order to convert this smelly fatty acid into propane. All in all, this was achieved by introducing only five genes — a very, very tiny fraction of the more than 4,000 genes present in the entire genome of E. coli. Our work represents a proof-of-concept for renewable fuel development as we deliberately selected a process that considers all steps of the pathway from production to utilisation. All in order to maximise chances of commercial production. At the end of the day, that is what is most important – to enable sustainable and renewable conversion of sunlight and CO2 into fuel, with minimal impact on the environment."
"
Share this...FacebookTwitterAxel Bojanowski of the German flagship news magazine Der Spiegel blasted the UN for its Extreme Weather report saying the:
UN presentation was dubious: research results are ignored, the report remains secret.”
and
Only the summary of the report was officially made public on Friday. This is a document that politicians and lawyers of the community of countries negotiated this week in Kampala.”
Even though the sensationalist mainstream media have been writing about predictions of dire events coming in the future, Bojanowski writes:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Neither the new climate report nor its summary can be read as a sheer warning of increasing weather catastrophes. The real message is: We simply know too little about the most weather catastrophes in order to predict their development.”
There’s simply not enough data out there to draw conclusions. Der Spiegel also seems frustrated that politicians are not taking scientists seriously.
The presentation of the new climate report however shows the opposite, that it appears to not matter what scientists find out through their years of hard work – the message is always the same: ‘Everything is going to get worse.'”
With the conference in Durban coming up, little seems to have changed. Politicians automatically use every climate report for the sole purpose of driving the agenda, without even knowing what’s in it. Der Spiegel writes:
The climate report is being sold simply as a wake-up call – the work of the scientists is simply being ignored. ‘Unfortunately we still don’t have many answers to the questions of climate forecasts,’ says Lisa Schipper of the Stockholm Environment Institute, a lead author of the new IPCC Report, to SPIEGEL ONLINE. ‘There is no black and white scenario’.”
The circus that was Copenhagen is moving to Durban, and is about to start soon.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterDietmar Doering of the think tank Denken für die Freiheit (Thinking for Freedom) brings our attention to this exposé of green energy in Germany.Germany is a country completely obsessed with saving the planet from man-made CO2, and has been pursuing renewable energies with unmatched abandon, pouring tens of billions into green energy. Lots of countries use Germany as a model for the direction of the future of energy.
But before other states and countries decide to follow Germany’s lead, they need to take a closer look and to think again.
Is the German story a success? Far from it.
The reality, which is rarely revealed by the public media, is that the country is bordering on an environmental and energy debacle.
The normally very green German state television ARD presented a shocking piece on where the blind, ideological and zealous charge to renewable energy is taking the country.
ARD video here
The mad, blind rush to renewable energies
0:00 – 1:55: German pols are falling all over themselves, trying to take credit for the country’s booming renewables business. Billions of euros in subsidies are flowing everywhere and to anything that sounds renewable. It’s a runaway gravy train – one that is headed for a cliff. As the moderator says; “It’s an energy transformation gone amok”. The struggle to take credit for it likely will not last very long, once the damage gets tallied.
Thousands of micro hydro-power plants everywhere along rivers and streams are now chopping up the nation’s fish stock, mountains of corn for ethanol or driving up food prices, wind turbines are idled and don’t deliver power to the markets because of a lack of transmission lines – all generously subsidized. Professor Olav Hohmeyer of the University of Flensburg:
There’s no plan. There’s no idea of when, how much, and where the power is needed.”
Everything is just being thrown up randomly, without a plan. If it’s renewable, it gets built.
7000+ small-scale hydro power plants chopping up salmon and eel
1:55 mark – Germany has 7700 hydroelectric plants, most are very small scale and massively subsidized by the government. Germany’s vast network of rivers are now so dense with small-scale hydro plants that all paths for eel, salmon and other fish are cut off. A huge proportion of the fish and eel end up getting chopped up by the turbines. To solve the problem the government hired fishermen to try to help the fish bypass the turbines. With luck they are able to save one or two salmon…a week! “An enormous expense,” the ARD says. The fish that do make it upstream get chopped to pieces later on the way back down, see at 3:28 mark (warning – graphic!). Germany’s rivers have become the rivers of blood and chopped fish.
Oh! But isn’t that a small price to pay for rescuing the planet?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




According to Winfried Klein, 350 of these 7700 hydroplants produce 95% of the hydro power, while the other 7350 small-scale plants produce only 5% of the hydro power, but kill most of the fish and eel. Klein says at the 4:10 mark that if these 7350 micro-plants were shut down, the power grid would not even notice it. Klein adds:
But oh no! Every little puny project has to get subsidized because of ‘ecological reasons’.”
The ARD questions a top environment bureaucrat (Jürgen Becker – state secretary of the Federal Environment Ministry) at the 4:40 mark, but just the looks of the guy tells the whole story. Indeed they’ve known about the problem for years…but these fools have been drugged up on CO2 propaganda for too long, and are literally brainwashed and think the survival of the planet is at stake. Fish dying off is a small price to rescue it.
Of course efforts are being generously paid for preventing the turbine-fish-chopping problem. But bypasses consisting of 6-inch plastic pipes have saved very few fish, the video shows. For the mini hydroplant operator it’s however lucrative (5:54 mark). He gets paid 20,000 euros ($28,000) per year by the government for the next 20 years just for having the bypass tube (most fish never find it). “A lot of money for a senseless solution,” the ARD comments.
So if by now you’re thinking this is approaching Soviet-scale mass mismanagement, you’re not too far from the truth. This is what happens when a senseless hysteria grips a country and sends it charging blindly into the dark.
The ARD says at the 6:20 mark that the government is in a frenzy to switch over to renewable energy, no matter the cost.
Biogas – food for fuel
7:07 mark – Biogas is also massively subsidized in Germany, and consequently the country has become overgrown by millions of monoculture acres of cornfields. Dairy farmers can no longer afford to feed their cattle because feed prices and land lease prices have shot up. All land is now being committed to saving the planet. For biogas plant operators, it’s a real cash-cow. Initially the biogas plants were intended to convert waste materials into gas, but instead farmers twisted the arms of politicians and are now paid handsomely to dump mountains of corn into their biogas plants. Little wonder that food prices are surging worldwide today – that’s the price of rescuing the planet.
Paying windparks NOT to produce
10:00 mark – In Germany some wind power does not even make it to the consumer’s electrical outlet. As Germany approves every windpark at every and any location, many areas are not adequately connected to the grid, meaning they are turning for nothing, or not turning at all. For example windpark operator Reinhard Christiansen near the Danish border is often ordered to shut down his turbines even under ideal wind conditions. This is because the power grid gets overloaded and there is still no possibility of transmitting the wind power from northern Germany to the industrial and populated areas in the south.
ARD reports that Christiansen has had to shut down his park 70% more often than last year. But for Christiansen it doesn’t matter. The state jumps in and pays for the power that otherwise would have been produced – money for nothing! “Absurd,” says the ARD. What’s needed are huge transmission lines to transport the power to the south. But this will cost billions more and have to go through lots of private property. And the government never reckoned that citizens would protest vehemently.
ARD also reports at the 12:00 mark that offshore parks too “face a debacle” because “the government has no plan for the power system”. They’re building the windparks, but they’re forgetting the transmission lines to bring the green power to the markets. As it is, the gigantic offshore windparks risk becoming monuments dedicated to the colossal energy stupidity of the government. The government is simply subsidizing the windmills, whether they can be used or not.
For countries and states thinking about copying the German model, you may want to think again.
 
Share this...FacebookTwitter "
"

In less than two years, North Carolina’s governor and legislature have helped to revive the state’s economy. The economy is growing and adding jobs, improving the well‐​being of North Carolina residents.



Governor Pat McCrory took office in January of 2013, joining a Republican legislature. For the first time since Reconstruction, North Carolina’s executive and legislative branches were controlled by Republicans, and they had a large mandate for reform. In 2011, the state’s economy grew at an anemic 0.3 percent. It was well below the national rate of 1.6 percent and one of the lowest in the Southeast. The state’s growth lagged many of its peers in 2012 as well. Individuals wanted change.



The new government took action to repair the state. The biggest item on the agenda was tax reform. McCrory and the legislature’s plan passed one of the most impressive tax reform packages in any state in years.





Limiting the growth of spending and passing tax reform is putting the state on a path of fiscal responsibility.



First, the plan consolidated brackets and cut the individual income tax rate. The overhaul replaced three individual income tax rates ranging from 6.0 to 7.75 percent with a single rate of 5.8 percent. In 2015, the rate will be cut again to 5.75 percent. Cutting taxes returns money to the pocketbooks of individuals and small businesses in the state.



Larger businesses also gained from corporate tax reforms. The corporate income tax rate was cut from 6.9 to 6.0 percent in 2014, and is scheduled to fall to 5.0 percent in 2015. The rate will continue to drop over the next several years if budget targets are met.



The tax reforms also increased the income tax standard deduction, repealed the estate tax, and expanded the sales tax based to cover more services, eliminating favoritism among industries.



In totality, this package puts North Carolina on a pro‐​growth trajectory with a low, broad tax structure. The reforms will vault North Carolina from 44th to 17th in the Tax Foundation’s State Business Tax Climate Index. All told, these tax cuts reduced the burden of taxation on North Carolina residents by $700 million annually, or 3 percent of state tax revenues.



Tax reform did not stop there, with more tax cuts passed in 2014. The state eliminated the local privilege tax. The burdensome tax added unnecessary complexity to North Carolina’s tax code. Three hundred of North Carolina’s 540 cities charged the tax, but it was calculated differently across the state. Some localities assessed a flat fee, others charged a tax that varied by the business’s size or employment structure. Eliminating the tax freed North Carolina firms from needless paperwork allowing each firm to concentrate on their businesses core function.



To complement the tax reforms, the state has also controlled spending growth. Actual general fund spending increased at half the national average in 2014. The state also reformed unemployment insurance and Medicaid, and eliminated several thousand state employees too.



All of these reforms seem to be making a difference and North Carolina’s economy is responding. In 2013, North Carolina’s economy grew 30 percent faster than the national average. The state added jobs at a quicker rate than the national average too. During the same time period, private‐​sector jobs grew by 4 percent, compared to 3.4 percent nationally.



For his efforts, Governor Pat McCrory received an “A” in the Cato Institute’s newest edition of its “Fiscal Policy Report Card on America’s Governors.” The report card assigns grades of “A” to “F” to the nation’s governors based on their efforts to restrain government and cut spending. McCrory tied for the highest score of any governor.



Under the leadership of Governor McCrory and the state legislature, North Carolina is poised for economic success. Limiting the growth of spending and passing tax reform is putting the state on a path of fiscal responsibility.
"
"
Spurious Warming in the Jones U.S.  Temperatures Since 1973
by Roy W. Spencer, Ph. D.
INTRODUCTION
As I discussed in my last  post, I’m exploring the International Surface Hourly (ISH) weather  data archived by NOAA to see how a simple reanalysis of original weather  station temperature data compares to the Jones CRUTem3 land-based  temperature dataset.
While the Jones temperature  analysis relies upon the GHCN network of  ‘climate-approved’ stations whose number has been rapidly dwindling in  recent years,  I’m using original data from stations whose number has  been actually growing over time.  I use only stations operating over the  entire period of record so there are no spurious temperature trends  caused by stations coming and going over time.  Also, while the Jones  dataset is based upon daily maximum and minimum temperatures, I am  computing an average of the 4 temperature measurements at the standard  synoptic reporting times of 06, 12, 18, and 00 UTC.
U.S. TEMPERATURE TRENDS, 1973-2009
I compute average monthly temperatures in 5 deg. lat/lon grid squares,  as Jones does, and then compare the two different versions over a  selected geographic area.  Here I will show results for the 5 deg. grids  covering the United States for the period 1973 through 2009.
The following plot shows that the monthly U.S. temperature anomalies  from the two datasets are very similar (anomalies in both datasets are  relative to the 30-year base period from 1973 through 2002).  But while  the monthly variations are very similar, the warming trend in the Jones  dataset is about 20% greater than the warming trend in my ISH data  analysis.

This is a little curious since I have made no adjustments for  increasing urban heat island (UHI) effects over time, which likely are  causing a spurious warming effect, and yet the Jones dataset which IS (I  believe) adjusted for UHI effects actually has somewhat greater warming  than the ISH data.
A plot of the difference between the two datasets is shown next,  which reveals some abrupt transitions. Most noteworthy is what appears  to be a rather rapid spurious warming in the Jones dataset between 1988  and 1996, with an abrupt “reset” downward in 1997 and then another  spurious warming trend after that.

While it might be a little premature to blame these spurious  transitions on the Jones dataset, I use only those stations operating  over the entire period of record, which Jones does not do. So, it is  difficult to see how these effects could have been caused in my  analysis.  Also, the number of 5 deg grid squares used in this  comparison remained the same throughout the 37 year period of record (23  grids).
The decadal temperature trends by calendar month are shown in the  next plot.  We see in the top panel that the greatest warming since 1973  has been in the months of January and February in both datasets.  But  the bottom panel suggests that the stronger warming in the Jones dataset  seems to be a warm season, not winter, phenomenon.

THE NEED FOR NEW TEMPERATURE RENALYSES
I suspect it would be difficult to track down the precise reasons why  the differences in the above datasets exist.  The data used in the Jones  analysis has undergone many changes over time, and the more complex and  subjective the analysis methodology, the more difficult it is to ferret  out the reasons for specific behaviors.
I am increasingly convinced that a much simpler, objective analysis  of original weather station temperature data is necessary to better  understand how spurious influences might have impacted global  temperature trends computed by groups such as CRU and NASA/GISS. It  seems to me that a simple and easily repeatable methodology should be  the starting point.  Then, if one can demonstrate that the simple  temperature analysis has spurious temperature trends, an objective and  easily repeatable adjustment methodology should be the first choice for  an improved version of the analysis.
In my opinion, simplicity, objectivity, and repeatability should be  of paramount importance.  Once one starts making subjective adjustments  of individual stations’ data, the ability to replicate work becomes  almost impossible.
Therefore, more important than the recently reported “do-over”  of a global temperature reanalysis proposed by the UK’s Met Office  would be other, independent researchers doing their own global  temperature analysis.  In my experience, better methods of data analysis  come from the ideas of individuals, not from the majority rule of a  committee.
Of particular interest to me at this point is a simple and objective  method for  quantifying and removing the spurious warming arising from  the urban heat island (UHI) effect.  The recent paper  by McKitrick and Michaels suggests that a substantial UHI influence  continues to infect the GISS and CRU temperature datasets.
In fact, the results for the U.S. I have presented above almost seem  to suggest that the Jones CRUTem3 dataset has a UHI adjustment that is  in the wrong direction.  Coincidentally, this is also the conclusion of a  recent  post on Anthony Watts’ blog, discussing a new  paper published by SPPI.
It is increasingly apparent that we do not even know how much the  world has warmed in recent decades, let alone the reason(s) why.  It  seems to me we are back to square one.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8da3a513',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Here’s something rather astonishing.
The Institute of Physics, has made a statement about climate science.
 
The Institute of Physics is a scientific charity devoted to increasing  the practice, understanding and application of physics. It has a  worldwide membership of over 36,000 and is a leading communicator of  physics-related science to all audiences, from specialists through to  government and the general public. Its publishing company, IOP  Publishing, is a world leader in scientific publishing and the  electronic dissemination of physics.
IOP issued a no holds barred statement on Climategate to the UK  Parliamentary Committee. Here’s the key passages:

What are the implications of the disclosures for the  integrity of scientific research?
1. The Institute is concerned that, unless the disclosed e-mails are  proved to be forgeries or adaptations, worrying implications arise for  the integrity of scientific research in this field and for the  credibility of the scientific method as practised in this context.
2. The CRU e-mails as published on the internet provide prima facie  evidence of determined and co-ordinated refusals to comply with  honourable scientific traditions and freedom of information law. The  principle that scientists should be willing to expose their ideas and  results to independent testing and replication by others, which requires  the open exchange of data, procedures and materials, is vital. The lack  of compliance has been confirmed by the findings of the Information  Commissioner. This extends well beyond the CRU itself – most of the  e-mails were exchanged with researchers in a number of other  international institutions who are also involved in the formulation of  the IPCC’s conclusions on climate change.
3. It is important to recognise that there are two completely  different categories of data set that are involved in the CRU e-mail  exchanges:
· those compiled from direct instrumental measurements of land and  ocean surface temperatures such as the CRU, GISS and NOAA data sets; and
· historic temperature reconstructions from measurements of  ‘proxies’, for example, tree-rings.
4. The second category relating to proxy reconstructions are the  basis for the conclusion that 20th century warming is unprecedented.  Published reconstructions may represent only a part of the raw data  available and may be sensitive to the choices made and the statistical  techniques used. Different choices, omissions or statistical processes  may lead to different conclusions. This possibility was evidently the  reason behind some of the (rejected) requests for further information.
5. The e-mails reveal doubts as to the reliability of some of the  reconstructions and raise questions as to the way in which they have  been represented; for example, the apparent suppression, in graphics  widely used by the IPCC, of proxy results for recent decades that do not  agree with contemporary instrumental temperature measurements.
6. There is also reason for concern at the intolerance to challenge  displayed in the e-mails. This impedes the process of scientific ’self  correction’, which is vital to the integrity of the scientific process  as a whole, and not just to the research itself. In that context, those  CRU e-mails relating to the peer-review process suggest a need for a  review of its adequacy and objectivity as practised in this field and  its potential vulnerability to bias or manipulation.
7. Fundamentally, we consider it should be inappropriate for the  verification of the integrity of the scientific process to depend on  appeals to Freedom of Information legislation. Nevertheless, the right  to such appeals has been shown to be necessary. The e-mails illustrate  the possibility of networks of like-minded researchers effectively  excluding newcomers. Requiring data to be electronically accessible to  all, at the time of publication, would remove this possibility.
8. As a step towards restoring confidence in the scientific process  and to provide greater transparency in future, the editorial boards of  scientific journals should work towards setting down requirements for  open electronic data archiving by authors, to coincide with publication.  Expert input (from journal boards) would be needed to determine the  category of data that would be archived. Much ‘raw’ data requires  calibration and processing through interpretive codes at various levels.
9. Where the nature of the study precludes direct replication by  experiment, as in the case of time-dependent field measurements, it is  important that the requirements include access to all the original raw  data and its provenance, together with the criteria used for, and  effects of, any subsequent selections, omissions or adjustments. The  details of any statistical procedures, necessary for the independent  testing and replication, should also be included. In parallel,  consideration should be given to the requirements for minimum disclosure  in relation to computer modelling.
Are the terms of reference and scope of the Independent  Review announced on 3 December 2009 by UEA adequate?
10. The scope of the UEA review is, not inappropriately, restricted  to the allegations of scientific malpractice and evasion of the Freedom  of Information Act at the CRU. However, most of the e-mails were  exchanged with researchers in a number of other leading institutions  involved in the formulation of the IPCC’s conclusions on climate change.  In so far as those scientists were complicit in the alleged scientific  malpractices, there is need for a wider inquiry into the integrity of  the scientific process in this field.
11. The first of the review’s terms of reference is limited to:  “…manipulation or suppression of data which is at odds with acceptable  scientific practice…” The term ‘acceptable’ is not defined and might  better be replaced with ‘objective’.
12. The second of the review’s terms of reference should extend  beyond reviewing the CRU’s policies and practices to whether these have  been breached by individuals, particularly in respect of other kinds of  departure from objective scientific practice, for example, manipulation  of the publication and peer review system or allowing pre-formed  conclusions to override scientific objectivity.
How independent are the other two international data sets?
13. Published data sets are compiled from a range of sources and are  subject to processing and adjustments of various kinds. Differences in  judgements and methodologies used in such processing may result in  different final data sets even if they are based on the same raw data.  Apart from any communality of sources, account must be taken of  differences in processing between the published data sets and any data  sets on which they draw.

Clearly a sleeping giant has awakened.
Andrew Bolt muses:
This submission in effect warns that this recent warming may not be  unprecedented, after all, and those that claim it is may have been  blinded by bias or simply fiddled their results and suppressed dissent.
I’ll repeat: Climategate reveals the greatest scientific scandal of our  lifetime.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8e129716',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"**Personal protective equipment (PPE) stockpiles in England were inadequate for the Covid pandemic and price rises earlier this year cost taxpayers about Â£10bn, the spending watchdog has said.**
The National Audit Office said there had been a particular shortage of gloves and aprons.
The government said the NAO's report recognised that NHS providers had been able to get what they needed in time.
Almost Â£12.5bn was spent on 32bn items of PPE between February and July 2020.
During the same period in 2019, 1.3bn items were bought at a cost of Â£28.9m.
Each item had been ""substantially"" more expensive in 2020, because of very high global demand, the NAO said, from almost triple the cost for respirator masks to more than 14 times as much for body bags.
Had the government been able to pay 2019 prices, it would have spent Â£2.5bn on PPE in 2020.
In reality, it had spent Â£12.5bn, including hundreds of millions on ""unsuitable"" items that could not be used.
Some had ""passed its expiry date or did not meet current safety standards"", the watchdog said, with ""insufficient checks"" meaning Public Health England had had to recall eye protectors that did not meet standards.
In Parliament, on Wednesday, Labour leader Sir Keir Starmer accused Prime Minister Boris Johnson of ""wasting"" taxpayer's money on equipment that ""can't be used"".
But Mr Johnson replied ""99.5%"" of the 32 billion items of PPE bought between February and July 2020 ""conform entirely to our clinical needs"".
Earlier, the Department of Health and Social Care (DHSC) said ""only 0.49% of all the purchased PPE tested to date"" had not been fit for purpose.
NAO head Gareth Davies said: ""As PPE stockpiles were inadequate for the pandemic, government needed to take urgent action to boost supplies.
""Once it recognised the gravity of the situation... the price of PPE increased dramatically, and that alone has cost the taxpayer around Â£10bn.""
Before the Covid-19 pandemic, there were two emergency stockpiles of PPE:
But the NAO said: ""The EU exit stockpile held few items of PPE other than a large number of gloves.""
Meanwhile, the flu stockpile, as well as having shortages of some key items, did not include any gowns or visors despite the fact they had been ""recommended for inclusion in June 2019 by the New and Emerging Respiratory Virus Threats Advisory Group (Nervtag)"".
Public Health England told the NAO it had been analysing the market to work out which gowns to buy, when the pandemic had begun, which it said was the ""normal approach"" to find a lower price.
In mid-March, the government had still believed its two stockpiles would provide ""most of the PPE needed to manage a Covid-19 pandemic"" and so focused on distributing this PPE rather than buying more, the NAO reported.
The situation had become ""precarious"" in April and May, with stocks threatening to run out.
At one point, only 3% of the required number of gowns had been available.
But the nation did not at any point run out of any type of PPE.
The scramble for PPE in the early stages of the pandemic was not confined to the UK. Every healthcare system was desperate to secure protective equipment and prices soared. But the National Audit Office lays bare how the UK was at the back of the queue, having failed to spot the warning signs and how woefully inadequate the stockpiles were.
A failure to anticipate what might be needed for anything other than a flu pandemic in essence cost the taxpayer Â£10bn - the extra money needed to secure supplies such as gowns and visors during the Covid crisis.
The report highlights poor distribution of PPE with many staff saying they did not have the right equipment. The NAO notes starkly that health and care employers have reported more than 100 deaths among staff because of exposure to coronavirus.
An official inquiry, when it happens, will look hard at many aspects of the UK's preparedness and handling of the crisis and the PPE issue will be central. With a series of reports, the NAO has now done important groundwork but there is much still to find out.
A DHSC official said: ""As the NAO report recognises, during this unprecedented pandemic all the NHS providers audited 'were always able to get what they needed in time' thanks to the Herculean effort of government, NHS, armed forces, civil servants and industry"".
But the NAO heard feedback from care workers, doctors and nurses that showed ""significant numbers of them considered that they were not adequately protected during the height of the first wave of the pandemic"".
Employers have reported 126 deaths among health and care workers linked to exposure at work.
And there were concerns about training and whether the equipment was appropriately fitted, particularly from women and people belonging to ethnic minorities.
In a Royal College of Nursing survey of 5,000 NHS staff, 49% of respondents belonging to ethnic minorities said they had been adequately ""fit tested"" for a respirator, compared with 74% of white nurses.
The DHSC said it was ""listening to the reported practical difficulties with the use of some PPE experienced by women and black, Asian and minority ethnic (BAME) individuals, among others, and... taking action to make sure user needs are adequately addressed in future provisions"".
In a separate report, the Public Accounts Committee, a parliamentary body which works closely with the NAO, said it was ""concerned that the department had no plan before the pandemic for how it might increase critical care equipment in the event of an emergency"".
""This lack of preparedness was exacerbated by the fact that it did not know how many ventilators were available to the NHS to begin with,"" the PAC said.
But it added the government had managed to buy an additional 26,000 ventilators for use in the NHS, a ""significant achievement""."
"
One of the most ridiculous claims recently related to Menne et al 2010 and my surfacestations project was a claim made by DeSmogBlog (and Huffington Post who carried the story also) is that the “Urban Heat Island Myth is Dead“.
To clarify for these folks: Elvis is dead, UHI is not.
For disbelievers, let’s look at a few cases showing UHI to be alive and well.

CASE 1: I’ve measured it myself, in the city of Reno for example:

The UHI signature of Reno, NV  – Click for larger image
Read the story of how I created this graph here The procedure and raw data is there if you want to check my work.
I chose Reno for two reasons. It was close to me, and it is the centerpiece of a NOAA training manual on how to site weather stations to avoid UHI effects.

CASE 2: NOAA shows their own measurements that mesh well with mine:
To back that up, the NOAA National Weather Service includes the UHI factor in one of it’s training course ( NOAA Professional Competency Unit 6 ) using Reno, NV. 
In the PCU6 they were also kind enough to provide a photo essay of their own as well as a graph. You can click the aerial photo to get a Google Earth interactive view of the area. The ASOS USHCN station is right between the runways.

This is NOAA’s graph showing the changes to the official climate record when they made station moves:

Source for 24a and 24b: NOAA Internal Training manual, 2004-2007
Oops, moving the station south caused a cooling. Fixed now, all better.
What is striking about this is that here we have NOAA documenting the effects of an “urban heat bubble” something that DeSmog Blog says ” is dead”, plus we have NOAA documenting a USHCN site with known issues, held up as a bad example for training the operational folks, being used in a case study for the new USHCN2 system.
So if NOAA trains for UHI placement, I’m comfortable in saying that DesmogBlog claims of UHI being “dead” are pure rubbish. But let’s not stop there.

CASE 3: From an embattled scientist.
A paper in JGR that slipped in 2007 without much notice (but known now thanks to Warwick Hughes) is one from Phil Jones, the “former” director of the Hadley Climate Center in the UK. The paper is titled:  Urbanization effects in large-scale temperature records, with an emphasis on China
In it, Jones identifies an urban warming signal in China of 0.1 degrees C per decade.  Or, if you prefer, 1 degree C per century. Not negligible by any means. Here is the abstract:
Global surface temperature trends, based on land and marine data, show warming of about 0.8°C over the last 100 years. This rate of warming is sometimes questioned because of the existence of well-known Urban Heat Islands (UHIs). We show examples of the UHIs at London and Vienna, where city center sites are warmer than surrounding rural locations. Both of these UHIs however do not contribute to warming trends over the 20th century because the influences of the cities on surface temperatures have not changed over this time. In the main part of the paper, for China, we compare a new homogenized station data set with gridded temperature products and attempt to assess possible urban influences using sea surface temperature (SST) data sets for the area east of the Chinese mainland. We show that all the land-based data sets for China agree exceptionally well and that their residual warming compared to the SST series since 1951 is relatively small compared to the large-scale warming. Urban-related warming over China is shown to be about 0.1°C decade−1 over the period 1951–2004, with true climatic warming accounting for 0.81°C over this period. 
Even though Jones tries to minimize the UHI effect elsewhere, saying the UHI trends don’t contribute to warming in London and Vienna, what is notable about the paper is that Jones has been minimizing the UHI issues for years and now does an about face on China.
Jones may have tried to hide CRU data, but he’s right about China.

CASE 4: From “The Dog ate My Data” who writes:
 The Australian Bureau of Meteorology (BOM) blames Melbourne’s equal warmest overnight temperature of 30.6 degrees, on January 12 on the heat island effect. The previous time the city was that hot overnight was February 1, 1902.

 The Age newspaper cites a meteorologist at the bureau, Harvey Stern,
Melbourne recorded its equal warmest overnight temperature, 30.6 degrees, on January 12. The previous time the city was that hot overnight was February 1, 1902.
A meteorologist at the bureau, Harvey Stern, said that Melbourne suffered from a heat island effect, in which a city is warmer than the surrounding countryside.
This was the case especially at night, because of heat stored in bricks and concrete and trapped between close-packed buildings.
I am stunned if that is correct firstly because BOM isn’t blaming Global Warming and secondly that the urban heat island effect directly receives the blame. With faults in the 2007 IPCC’s AR4 now pouring out I guess it is not suprising that attributions of weather events are now, shall we say, possibly becoming more circumspect.

CASE 5: Heatzilla stomps Tokyo
From the website “science of doom” who writes:

New Research from Japan
Detection of urban warming in recent temperature trends in Japan by Fumiaki Fujibe was published in the International Journal of Climatology (2009). It is a very interesting paper which I’ll comment on in this post.
The abstract reads:
The contribution of urban effects on recent temperature trends in Japan was analysed using data at 561 stations for 27 years (March 1979–February 2006). Stations were categorized according to the population density of surrounding few kilometres. There is a warming trend of 0.3–0.4 °C/decade even for stations with low population density (<100 people per square kilometre), indicating that the recent temperature increase is largely contributed by background climatic change. On the other hand, anomalous warming trend is detected for stations with larger population density. Even for only weakly populated sites with population density of 100–300/km2, there is an anomalous trend of 0.03–0.05 °C/decade. This fact suggests that urban warming is detectable not only at large cities but also at slightly urbanized sites in Japan. Copyright, 2008 Royal Meteorological Society.
Why the last 27 years?
The author first compares the temperature over 100 years as measured in Tokyo in the central business district with that in Hachijo Island, 300km south.
Tokyo –               3.1°C rise over 100 years (1906-2006)
Hachijo Island –  0.6°C over the same period

This certainly indicates a problem, but to do a thorough study over the last 100 years is impossible because most temperature stations with a long history are in urban areas.
However, at the end of the 1970’s, the Automated Meteorological Data Acquisition System (AMeDAS) was deployed around Japan providing hourly temperature data at 800 stations. The temperature data from these are the basis for the paper. The 27 years coincides with the large temperature rise (see above) of around 0.3-0.4°C globally.
And the IPCC (2007) summarized the northern hemisphere land-based temperature measurements from 1979- 2005 as 0.3°C per decade.
How was Urbanization measured?

The degree of urbanization around each site was calculated from grid data of population and land use, because city populations often used as an index of urban size (Oke, 1973; Karl et al., 1988; Fujibe, 1995) might not be representative of the thermal environment of a site located outside the central area of a city.

What were the Results?

Mean temperature anomaly vs population density, Japan

The x-axis, D3, is a measure of population density. T’mean is the change in the mean temperature per decade.
Tmean is the average of all of the hourly temperature measurements, it is not the average of Tmax and Tmin.
Notice the large scatter – this shows why having a large sample is necessary. However, in spite of that, there is a clear trend which demonstrates the UHI effect.
There is large scatter among stations, indicating the dominance of local factors’ characteristic to each station. Nevertheless, there is a positive correlation of 0.455 (Tmean = 0.071 logD3 + 0.262 °C), which is significant at the 1% level, between logD3 and Tmean.
Here’s the data summarized with T’mean as well as the T’max and T’min values. Note that D3 is population per km2 around the point of temperature measurement, and remember that the temperature values are changes per decade:

The effect of UHI demonstrated in various population densities

Note that, as observed by many researchers in other regions, especially Roger Pielke Sr, the Tmin values are the most problematic – demonstrating the largest UHI effect. Average temperatures for land-based stations globally are currently calculated from the average of Tmax and Tmin, and in many areas globally it is the Tmin which has shown the largest anomalies. But back to our topic under discussion..
And for those confused about how the Tmean can be lower than the Tmin value in each population category, it is because we are measuring anomalies from decade to decade.
And the graphs showing the temperature anomalies by category (population density):

Dependence of Tmean, Tmax and Tmin on population density for different regions in Japan

Quantifying the UHI value
Now the author carries out an interesting step:
As an index of net urban trend, the departure of T from its average for surrounding non-urban stations was used on the assumption that regional warming was locally uniform.
That is, he calculates the temperature deviation in each station in category 3-6 with the locally relevant category 1 and 2 (rural) stations. (There were not enough category 1 stations to do it with just category 1). The calculation takes into account how far away the “rural” stations are, so that more weight is given to closer stations.

Estimate of actual UHI by referencing the closest rural stations – again categorized by population density

And the relevant table:


Temperature delta from nearby rural areas vs population density

Conclusion
Here’s what the author has to say:
On the one hand, it indicates the presence of warming trend over 0.3 °C/decade in Japan, even at non-urban stations. This fact confirms that recent rapid warming at Japanese cities is largely attributable to background temperature rise on the large scale, rather than the development of urban heat islands.
..However, the analysis has also revealed the presence of significant urban anomaly. The anomalous trend for the category 6, with population density over 3000 km−2 or urban surface coverage over 50%, is about 0.1 °C/decade..
..This value may be small in comparison to the background warming trend in the last few decades, but they can have substantial magnitude when compared with the centennial global trend, which is estimated to be 0.74°C/century for 1906–2005 (IPCC, 2007). It therefore requires careful analysis to avoid urban influences in evaluating long-term temperature changes.
So, in this very thorough study, in Japan at least, the temperature rise that has been measured over the last few decades is a solid result. The temperature increase from 1979 – 2006 has been around 0.3°C/decade
However, in the larger cities the actual measurement will be overstated by 25%.
And in a time of lower temperature rise, the UHI may be swamping the real signal.
The degree of urbanization around each site was calculated from grid data of population and land use, because city populations often used as an index of urban size (Oke, 1973; Karl et al., 1988; Fujibe, 1995) might not be representative of the thermal environment of a site located outside the central area of a city.

Case 6: California Counties by population show a distinct UHI signature.
My friend Jim Goodridge, former California State Climatologist identified the statewide UHI signature issues way back in 1996. This graph had a profound effect on me, becuase it was the one that really made an impact on me, switching my views to being skeptical. Yes, I used to be a warmer, but that’s another story.
Goodridge, J.D. (1996) Comments on “Regional Simulations of Greenhouse Warming including Natural Variability” . Bull, Amer. Meteorological Society 77:1588-1599.
Goodrich (1996) showed the importance of urbanization to temperatures in his study of California counties in 1996. He found for counties with a million or more population the warming from 1910 to 1995 was 4F, for counties with 100,000 to 1 million it was 1F and for counties with less than 100,000 there was no change (0.1F).

He’s been quietly toiling away in his retirement on his computer for the last 15 years or so making all sort of data comparisons. One plot which he shared with me in 2003  is a 104 year plot map of California showing station trends after painstakingly hand entering data into an Excel spreadsheet and plotting slopes of the data to produce trend dots.
He used every good continuous piece of data he could get his hands on, no adjusted data like the climate modelers use, only raw from Cooperative Observing Stations, CDF stations, Weather Service Office’s and Municipal stations.
The results are quite interesting. Here it is:

I’ll have more interesting revelations from Jim Goodridge soon.

Case 7: NASA JPL’s climatologist says UHI is an issue
This press release from NASA Jet Propulsion Lab says that most of the increase in temperature has to do with ubanization:
[NASA’s JPL Bill] Patzert says global warming due to increasing greenhouse gases is responsible for some of the overall heating observed in Los Angeles and the rest of California. Most of the increase in heat days and length of heat waves, however, is due to a phenomenon called the “urban heat island effect.”
Heat island-induced heat waves are a growing concern for urban and suburban dwellers worldwide. According to the U.S. Environmental Protection Agency, studies around the world have shown that this effect makes urban areas from 2 to 10 degrees Fahrenheit (1 to 6 degrees Celsius) warmer than their surrounding rural areas.
Patzert says this effect is steadily warming Southern California, though more modestly than some larger urban areas around the world. “Dramatic urbanization has resulted in an extreme makeover for Southern California, with more homes, lawns, shopping centers, traffic, freeways and agriculture, all absorbing and retaining solar radiation, making our megalopolis warmer,” Patzert said.

CASE 8: You can see it from space. NASA (not the GISS division) measures it. Here’s a report they presented at the last AGU meeting in December 2009. Gee, that curve below looks like Reno, NV, doesn’t it?
  
The urban heat island effect can raise temperatures within cities as much as 5 C higher than the surrounding countryside. New data suggests that the effect is more or less pronounced depending on the type of landscape — forest or desert — the city replaced. Credit: NASA
› Larger image


 
NASA researchers studying urban landscapes have found that the intensity of the “heat island” created by a city depends on the ecosystem it replaced and on the regional climate. Urban areas developed in arid and semi-arid regions show far less heating compared with the surrounding countryside than cities built amid forested and temperate climates.
“The placement and structure of cities — and what was there before — really does matter,” said Marc Imhoff, biologist and remote sensing specialist at NASA’s Goddard Space Flight Center in Greenbelt, Md. “The amount of the heat differential between the city and the surrounding environment depends on how much of the ground is covered by trees and vegetation. Understanding urban heating will be important for building new cities and retrofitting existing ones.”
Goddard researchers including Imhoff, Lahouari Bounoua, Ping Zhang, and Robert Wolfe presented their findings on Dec. 16 in San Francisco at the Fall Meeting of the American Geophysical Union.
Satellite imagery of suburban (top) and urban Atlanta shows the differences in daytime heating, as caused by the urban heat island effect. Credit: NASA Goddard’s Scientific Visualization Studio
› Larger image (suburban)
› Larger image (urban)

Yep, UHI is alive and well. Anybody with an automobile dashboard thermometer who drives a commute from country to city can easily measure UHI, and you don’t have to be a climate scientist to prove it to yourself.
UPDATE: For a primer on how UHI is not dealt with by NOAA and CRU, have a look at this Climate Audit post:
Realclimate and Disinformation on UHI


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e8e415395',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

According to David Rind from NASA’s Goddard Institute for Space Studies (GISS), 2005 is going to set the all‐​time record for global warmth. He told Juliet Eilperin of the _Washington Post_ (October 13, 2005) only a major volcanic eruption could intervene. But Eilperin also interviewed Oregon State Climatologist George Taylor, who told her that Goddard’s findings were “mighty preliminary.” 



That’s because there’s more than one history of global temperature. Three receive the most citations. NASA’s record begins in 1880, as does another history from the U.S. Department of Commerce, developed at the Department’s National Climatic Data Center (NCDC). But the most widely referenced history (and the one primarily used by the U.N.‘s Intergovernmental Panel on Climate Change (IPCC)) is compiled by the Climate Research Unit (CRU) at England’s University of East Anglia. It goes back to 1856.



The vast majority of the underlying temperature observations that go into each of these compilations is the same, but each organization has developed its own techniques for how the raw observations are geographically combined and adjusted for confounding factors such as urbanization, missing values, etc. As a result, annual values in each temperature history differ slightly.



So let’s take a look at where the average temperature in each stands through September 2005, and what the prospects are for setting a record for the year as a whole, given that there are still three months of data to be added.



In the table below are all of the relevant numbers.







The GISS anomalies are calculated from the average for the period 1951 through 1980, the NCDC anomalies are relative to the average from 1880 through 2004, while the CRU temperatures are the departures from the 1961–1990 mean. But this matters little in our analysis. In each history, the record‐​warm year is 1998. The September anomaly in the CRU data is not yet available but we have estimated it based upon the values reported by GISS and NCDC. 



The “Additional Monthly Anomaly” is the increment relative to the January‐​September average that each month in the period October through December must average in order to have 2005 become the warmest year. It is negative for the GISS temperatures because they are currently above the record value. 



The “Number of Observed Exceedences” is the number of times during the period of record that the average anomaly (relative to the first 9 months of the year) during last three months of the year reached or exceeded the value required to have 2005 set the record. 



The “Percent Chance” is “Number of Observed Exceedences” divided by the “Total Number of Observations”. 



This last column is where the rubber meets the road. Based upon the previous behavior of the climate system (as captured by the global average temperature in each compilation), there is a nearly three‐​in‐​four chance that 2005 will finish as the warmest year in the NASA GISS global temperature history, but less than a one‐​in‐​five chance that the NCDC record will be set, and virtually no chance that the CRU will report 2005 as the hottest year measured. Both Rind and Taylor are going to turn out to be correct. 



The significance of all of this depends on whom you talk to. The press, as already foreshadowed by Eilperin’s Washington Post article, will surely trumpet the record‐​setting values from the GISS data, while noting that the other datasets (probably) will have placed 2005 as the second warmest year on record. The various scientists interviewed will point out that this occurred even in the absence of a strong El Niño (the primary reason 1998 was so hot) and that this is further evidence that the earth is warming from an enhancing greenhouse effect. 



So, what else is new? We already know that the world is warming and that it will continue to do so for the foreseeable future (with or without any greenhouse gas emission controls). Record temperatures will continue to be set every couple of years or so. In fact, if it weren’t for the 1998 El Niño, a new record high global average temperature would have been established in 4 of the last 5 years (including 2005). The big news is that 2005 will further establish that the rate at which temperatures have been rising during the past 30 years or so has been remarkably constant with a value of about 0.17ºC per decade, and it shows no sign of speeding up. Climate models share this constancy of warming; they just predict different rates. Unless that behavior is wrong, the additional warming until 2100 will be about 1.6°C, near the low end of projections made by our friends at the United Nations, and, frankly, too small to worry about, given that the energy structure of our society is likely to change dramatically in 100 years’ time. We’ll bet that no one points that out in December, when the warmth‐​of‐​2005 stories will proliferate like Santas. 



References:



Goddard Institute for Space Studies Temperature Data  
http://​data​.giss​.nasa​.gov/​g​i​s​temp/



National Climatic Data Center Temperature Data  
http://​www​.ncdc​.noaa​.gov/​o​a​/​c​l​i​m​a​t​e​/​r​e​s​e​a​r​c​h​/​a​n​o​m​a​l​i​e​s​/​a​n​o​m​a​l​i​e​s​.html



Climate Research Unit Temperature Data  
http://​www​.cru​.uea​.ac​.uk/​c​r​u​/​d​a​t​a​/​t​e​m​p​e​r​a​ture/
"
"
Share this...FacebookTwitterNot even CO2 can stop it!
The German online Bild newspaper here projects another bitter cold winter ahead for Central Europe. Early November will be on the mild side, and the real cold will not arrive until the end of November – but then look out! The Bild article starts with:
Shiver-Alarm: In four weeks the Horror-Winter begins!
Bild quotes meteorologist Dominik Jung of www.wetter.net/:
This snowy winter will extend over three months and spread into the flatlands. And it is going to be bitter cold. For Germany the fourth colder-than-normal winter in a row lies ahead. The fourth below normal winter in a row would be a small sensation.”
Wetter.net predicts December in Germany should see a fair amount of snow with good chances for a white Christmas. Temperatures will be around normal.. January will see temperatures plummet and the month will be considerably colder than normal. Central Europeans can expect to see temperatures to fall as low as -25°C.
Whatever happened to the balmy winters that were predicted by the models of CO2-drugged up climatologists not long ago? Remember being told that snow in the future would be “rare and exciting” and how we’d all would have to move to Antarctica to escape the heat?
Jung also says February will also be colder than normal and that snow will fall until the end of the month, and possibly drag on into March. So forget about an early spring. Who knows, maybe we’ll be shoveling snow for Easter too.
Note that these forecasts predict snow and ice for the normally temperate lowlands. Just a few years ago the drugged-up climatologists were predicting the end of skiing in the Alps.
 
Share this...FacebookTwitter "
