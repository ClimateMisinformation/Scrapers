"As British high streets and farm fields lie under water this week, Boris Johnson has repeatedly been urged to put on his wellies, go out and listen to flood victims. So far though, his response has been more about tin ears than rubber boots: during Storm Dennis the prime minister was reportedly holed up in a 17th-century mansion in the Kent countryside. As even Nigel Farage and the Sun have pointed out, this is a pathetic failure of leadership. On a more sinister level, it is entirely consistent with a darkly elitist view of how to deal with the climate crisis. The question of climate priorities will grow increasingly important. Which communities will the government defend and which will they abandon? This threat to human civilisation ought to merit collaborative action at the highest global level. This seemed to be the case in the 1980s and 90s – those halcyon days of the “international community” – when world leaders stepped up to reverse the depletion of the ozone layer and put in place new United Nations structures to address the then nascent threats of global warming and biodiversity loss. Since then, decades of fossil fuel-funded denial and a shift in the political and economic landscape have taken their toll. Today, a growing number of governments prefer to stress how impotent they are in the face of market and natural forces, while multibillionaires – who are more powerful than ever – have started building apocalypse sanctuaries, applying for New Zealand citizenship or backing politicians who promise to erect higher physical and legal walls at their borders. Anything to keep out the weather and the climate-affected masses. Outright denial of climate science is now almost impossible. In the UK, the lengthening summer heatwaves and more intense winter deluges have seen to that. They have also demonstrated that walls and money are not enough. The environment secretary, George Eustice, admitted as much this week, when he said: “We’ll never be able to protect every single household just because of the nature of climate change and the fact that these weather events are becoming more extreme.” This raises the question of climate priorities, which will grow increasingly important as floods and heatwaves affect more people and property. Which communities will the government defend and which will they abandon? More importantly, how will it balance the resources for adaptation infrastructure (such as sea walls, flood barriers, drainage channels ) with those for mitigation (cutting emissions through forests, wetlands, regulation of petrochemical firms and a transition to renewable energy)? This is a choice between tackling the short-term local symptoms or the long-term global causes. Traditionally that has been the political dividing line between the right and the left. Today, it is the difference between climate apartheid – effectively excluding those affected on economic or racial grounds – and climate solidarity. The UK has steered a pragmatic course between these two extremes until now. Most politicians in this country recognise the most cost-efficient way of dealing with global warming is to cut emissions now to avoid far more expensive damages in the future. On the left and right, there has been a sense of social responsibility and national pride that Britain initiated the Industrial Revolution and should take the lead in clearing up the mess it left behind. But will that consensus hold as the domestic costs of climate action ramp up? Or will the free-market extremists in the government focus on protecting wealthy and economically productive regions rather than sharing the burdens and thinking about the future? This sodden week alone does not answer those questions, but it is worrying that we have an environment minister apparently ready to abandon some areas, and a prime minister who has cut himself off from those suffering the consequences. The country and the world needs a leader who steps out and steps up on climate. So far, Johnson has done neither. • Jonathan Watts is the Guardian’s global environment editor"
"
Share this...FacebookTwitterAnd that includes those of you who do not agree with me. Now here’s a humorous little clip that I think is a little relevant.

It’s always a good idea to have quality data and analyses before deciding on and implementing precautionary measures.
Have a good one, folks!
Share this...FacebookTwitter "
"

What a wonderful year, 1998! Global temperatures reached their highest value recorded in all three available records — surface, satellite, and weather balloon. Sayers of doom had pronounced dire and immediate consequences — so for once it was possible to check their models of misery against what actually happened when it really got warm. 



**El Niño vs. Greenhouse Warming**



Judging from the conflation of El Niño and human‐​induced global warming, you might think the two were one and the same, or maybe even, as Vice President Al Gore intimated, that one caused the other. 



Like many of his reaches, there was a bit of truth in the stretch, but only a bit. Global warming didn’t cause El Niño in any appreciable sense, but the two were related: It was a very good El Niño year, and it was a very, very warm year. 



El Niño is natural. Just because scientists discover something, or because we, as taxpayers, shell out tens of millions of dollars to research something, does not mean that something new has happened. Chemicals existed before chemists, DNA existed before its discovery won a Nobel Prize, and El Niño ebbed and flowed long before the first climatologist was born. 





No one knows why the trades suddenly slow or even reverse, piling warm water up against South America. Reid Bryson, the modern founder of the very true notion that climate does change in ways that are important to people, believes this reversal is mainly an effect of some other large‐​scale physical fluctuation. During an El Niño, a large portion of the tropical Pacific is much warmer than average — as much as 8oC (14.4oF) — and this heat eventually disperses through the atmosphere. 



Heat is energy, and an El Niño shows up both as warming and as motion. Its reach extends into the tropical Atlantic, where it suppresses — yes, suppresses — hurricanes. Rain, often absent for years, falls in the ultradeserts of Peru and Argentina. And the global temperature warms. 



When the trade winds return, the cold upwelling reappears. This is La Niña. It stands to reason that the more the cold water is suppressed, the greater the amount that eventually bubbles up, so a big El Niño warm spike may mean a big temperature fall in the months thereafter. 





We’re totally confident that 1998’s big warming spike was a result of El Niño, and not dreaded “global warming” — that is, a human product. We know because the stratosphere tells us so. 



The human version of global warming is caused by increasing amounts of “greenhouse” gases in the lower atmosphere. These compounds absorb the heating radiation that results from the sun’s warming of the earth’s surface, and reemit that radiation either downward, resulting in additional atmospheric warming, or out to space. If these compounds weren’t there, the radiation would pass directly outward. 





So what we should see from the increasing greenhouse effect is a lowering trend for stratospheric temperature. And El Niño should temporarily stop that trend, at least for a year or so. Figure 3 shows that 1998 was indeed one of the warmest years in the stratosphere in the last two decades and is testimony to El Niño as the cause. 



**Greenorama**



We were besieged with news reports about how El Niño (and, by not‐​so‐​subtle extension, global warming) would cause terrible agricultural disasters. Who can forget the miles of CNN footage showing tractors mired in the Georgia mud, or network reels of browned corn in Texas? 



Well, some folks did poorly, and some folks did well. That happens every year. About the best way we know of to settle the overall score is in the wheat, corn, and soybean markets. When there’s a big supply, the price goes down. Demand fluctuates some too, but a perpetually increasing population has a way of ensuring more mouths to feed. 



By late 1998, the price of U.S. wheat stood, after adjusting for inflation, at its lowest level since reliable records began in 1915. Fluctuations in America’s massive supply of agricultural products, more than anything, dictate the global price. 





Many agricultural economists and a few climatologists have made careers of studying the influence of global weather patterns on crop yields. Moisture at planting time and in the winter before harvest is the major determinant — by far — of winter wheat yield. Winter wheat is planted in the early fall, requires moisture to germinate, and then, when spring springs, is really poised to take advantage of wet soil. In addition, yields are positively influenced by above‐​normal winter temperatures. 



Undoubtedly, the climate of 1998 led to the record yields. But there was another factor as well: Increased carbon dioxide in the air increases yields and makes crops much more efficient in their use of moisture. As Sylvan Wittwer, former head of the Board on Agriculture of the U.S. National Research Council wrote,“Overall, it has been conservatively estimated that global crop productivity has risen by approximately 2.5 to 10 percent, and possibly as high as 14 percent from the current increase in atmospheric CO2 over pre‐​industrial levels.” 



**We’ve Seen Fire and We’ve Seen Rain**



Two other prominent newsmakers this year included the spate of overland fires in Florida during the early summer, and Mitch, a real son‐​of‐​a‐​gun of a flood, but really not much of a hurricane by the time it hit land. 





That didn’t stop everyone from blaming all this on El Niño and global warming, but the fact is that, in general, there is no relationship between summer dryness in Florida and the existence of an El Niño during the previous winter. That’s because El Niño makes it rain during the winter greening season. In the summer, there’s precious little documentation that El Niño does anything at all to Florida weather. Of course, we could blame Florida’s high temperatures this year on global warming, but that would mean ignoring the fact that changing the greenhouse effect warms up the coldest air masses a lot more than it heats up the warmer tropical ones. 





That was inflammatory nonsense. A 1974 hurricane named Fifi, which was also a Category 2 at landfall, took much the same track and killed 7,500. Janet and Edith had much more powerful winds and wreaked tremendous havoc. Perhaps the most interesting comparative aspect with Mitch is that a tropical storm named Claudette, in 1979, also produced 50 inches of rain and resulted in nine deaths (that’s about 9,990 fewer than Mitch caused) when it hit Texas. Perhaps infrastructure and poverty, not global warming, created the tragedy named Mitch. Maybe, just maybe, allowing us to save our money for investment in developing nations like Honduras and El Salvador is a better idea than taking it away in an attempt to stop something that would happen anyway. 



El Niño and hurricanes do share at least one common trait. They have been around for a long time and the biota of the world, thanks to the nature of evolution, likely take advantage of them. In California, rains of the magnitude that associate with El Niño are required to make the desert bloom. Just any old storm isn’t enough, even though the ground gets wet. In that environment, many seeds have to be scarred by the motion of overland movement of water before they’ll even germinate. 





Long before banging the climate‐​disaster gong became the key to career advancement, federal climatologist George Cry calculated the percentage of normal rainfall that comes from all tropical cyclones, including tropical depressions, storms, and hurricanes in the eastern United States. Figure 7 shows the result for September. 



In most of the areas with high values, American agriculture has adopted a double‐​crop system that plants one early, fast‐​maturing crop, and then replaces it with an October harvest crop, mainly soybeans. Late August and September rain can be very important determinants of final yield. It’s pretty clear that years in which amounts are below normal because of lack of tropical cyclones are those in which yields are in jeopardy. 



People adapt to their climatic environment. The biota of the world take advantage of change, and so does our agriculture: One of the biggest El Niños in recent centuries produced a glut of food. That’s the lesson of 1998. 



But the climate hype of 1998 also has portents. If this past year is any guide, when global warming becomes a major focus of the Y2K presidential campaign, the amount of distortion, exaggeration, scare stories and fear‐​mongering we’re sure to witness will be a real climate disaster. 



**References:**



Cry, G.W., 1967. Effects of tropical cyclone rainfall on the distribution of precipitation over the eastern and southern United States. ESSA Professional Paper, 1, U.S. Dept. of Commerce. Washington, D.C. 



Michaels, P.J., 1979. Atmospheric anomalies and crop yields in North America. Ph.D. Dissertation. University of Wisconsin. 



Wittwer, S., 1995. Food Climate and Carbon Dioxide. CRC Press, Boca Raton, Fla., pp. 236.
"
"

In the grand scheme of things, the Keystone XL pipeline is of little significance to anything tangible — including gas prices, jobs, and, yes, the environment. The price of gasoline is for the most part determined by global forces in the oil market, of which the Keystone XL oil will be but a drop.



The pipeline’s job‐​creation potential is largely ephemeral; although the construction of the pipeline will create tens of thousands of jobs, the operation of it thereafter is expected to create fewer than 100 permanent jobs. And least significant of all is its impact on climate change. If it were to operate at full capacity for the next 85 years, the consumption of oil delivered by the pipeline would lead to global warming of less than a hundredth of a degree — an amount that is scientifically undetectable and environmentally inconsequential.





The fight over the Keystone XL pipeline is, and always has been, nothing but a symbol.



The fight over the Keystone XL pipeline is, and always has been, nothing but a symbol — of dedication to environmentalism, for the Left; of resistance to excessive government interference, for the Right. Huge amounts of time and money have been spent — or, more accurately, wasted — arguing fruitlessly that it is something more concrete. Practically speaking, its implications are tiny.



This is not true, however, of the litany of carbon‐​dioxide‐​limiting regulations that President Obama has imposed through the EPA. These onerous regulations try to force a reduction in demand through increasing the price of energy derived from fossil fuels (that is, coal, oil, natural gas). They will infiltrate the daily lives of each American, making everything more expensive and potentially threatening the reliability of America’s energy supply. And for what?



As far as the environment is concerned, the EPA’s meddling will have no demonstrable effect. Even a complete cessation of all greenhouse‐​gas emissions from the U.S., starting now and lasting forever, would avoid only a fraction of a degree of global temperature rise. At the local level, where people interact with the climate, natural variability would swamp any effect that U.S. emission reductions may have on the daily weather.



And for all the talk of an increasing intensity and frequency of extreme weather events — hurricanes, tornadoes, droughts, floods, heat waves, cold snaps, and any other manner of unpleasant weather — there is little actual scientific research that either identifies much of a trend, or unequivocally links such weather events to climate change, much less traces a direct link to carbon‐​dioxide emissions.



So, when it comes to challenging Obama on climate policy, congressional Republicans should set their sights on issues with tangible and wide‐​ranging impacts, and be willing to trade off largely symbolic projects like the Keystone XL pipeline.



Keystone XL has little to offer besides a moral victory. Limiting the economic damage that the EPA regulations may inflict, on the other hand, will be a benefit to all Americans, for years to come.
"
"
I’m pleased to announce that the www.surfacestations.org project has reached a major milestone, with 67% of the 1221 USHCN network now surveyed.
819 of 1221 stations have been examined in the USHCN network. Of the 819, 807 have been assigned a site quality rating. In some of those cases we’ve found the stations closed, or we are waiting for supplemental information to enable assigning a rating.
The  Google Earth map below shows current coverage. We are in sight of the goal. However there are still some holes, especially in south Texas, Alabama, Idaho, Arkansas, Missouri and Illinois.
See this Google Earth generated image. The circles with question marks are stations left to be surveyed.

Click for a larger image
A Google Earth USHCN Station Rating Map (KML file used to generate the above image) is available – download  here
You can download the Google Earth application for free from this link
Sincere thanks to Gary Boden for this  contribution! This is a very useful tool to help locate stations as hi resolution lat/lon values and descriptions are available from each map icon. Of course, Google Earth will also plot driving directions too.
I’m hoping to reach a minimum of 75% before I start doing data analysis. I want to find more rural stations, with the hope of finding more of the better sited stations since the lions share is comprised of CRN3-5 stations.  I’m hoping those of you that live near some of these “holes” can help. if you can, please leave a comment below and I’ll help you locate stations. You’ll also need to visit the website www.surfacestations.org and register as a volunteer. It’s free and easy.
Here is what the current rating breakdown looks like:

click for a larger image
For those unfamiliar with the rating system, it is identical to the one used by NOAA/NCDC to select sites for their new Climate Refernece Network (CRN) They drew this rating scheme from a paper published by Michel Leroy, of MeteoFrance, that he devised for their meteorological network. Here are the details:
Climate Reference Network Rating Guide – adopted  from NCDC Climate Reference Network Handbook, 2002, specifications  for siting (section 2.2.1) of NOAA’s new Climate Reference  Network:
Class 1 (CRN1)- Flat and horizontal ground surrounded by a  clear surface with a slope below 1/3 (<19deg). Grass/low vegetation ground  cover <10 centimeters high. Sensors located at least 100 meters from  artificial heating or reflecting surfaces, such as buildings, concrete surfaces,  and parking lots. Far from large bodies of water, except if it is representative  of the area, and then located at least 100 meters away. No shading when the sun  elevation >3 degrees.
Class 2 (CRN2) – Same as Class 1 with the  following differences. Surrounding Vegetation <25 centimeters. No artificial  heating sources within 30m. No shading for a sun elevation  >5deg.
Class 3 (CRN3) (error >=1C) – Same as Class 2, except no  artificial heating sources within 10 meters.
Class 4 (CRN4) (error >=  2C) – Artificial heating sources <10 meters.
Class 5 (CRN5) (error  >= 5C) – Temperature sensor located next to/above an artificial heating  source, such a building, roof top, parking lot, or concrete surface.”
Here is how the survey status breaks down by state. States highlighted have less than 50% coverage and are in the need of the most help from volunteers.




State
Number of Stations
Survey Report Done
Percent Reported


Alabama
15
8
53%


Arizona
26
21
81%


Arkansas
15
7
47%


California
54
54
100%


Colorado
25
17
68%


Connecticut
4
4
100%


Delaware
5
4
80%


Florida
22
21
95%


Georgia
23
20
87%


Idaho
26
17
65%


Illinois
36
13
36%


Indiana
36
33
92%


Iowa
23
13
57%


Kansas
32
27
84%


Kentucky
13
7
54%


Louisiana
18
17
94%


Maine
12
10
83%


Maryland
17
9
53%


Massachusetts
12
12
100%


Michigan
24
19
79%


Minnesota
33
30
91%


Mississippi
32
25
78%


Missouri
25
11
44%


Montana
44
27
61%


Nebraska
45
27
60%


Nevada
13
13
100%


New Hampshire
5
4
80%


New Jersey
12
8
67%


New Mexico
28
17
61%


New York
59
28
47%


North Carolina
29
26
90%


North Dakota
24
15
63%


Ohio
26
15
58%


Oklahoma
45
36
80%


Oregon
41
28
68%


Pennsylvania
24
11
46%


Rhode Island
3
3
100%


South Carolina
29
20
69%


South Dakota
24
11
46%


Tennessee
15
12
80%


Texas
48
24
50%


Utah
40
24
60%


Vermont
7
6
86%


Virginia
19
7
37%


Washington
44
35
80%


West Virginia
13
6
46%


Wisconsin
22
13
59%


Wyoming
33
26
79%



















For those that wish to help here is what you need to do:
1. Visit  www.surfacestations.org and register as a volunteer. It’s free and easy.
2. Look over the the How To Guide for surveying a station. All you need is a digital camera, and optionally a portable GPS, but it is not mandatory. A GPS that can get you to a lat/lon you enter is helpful though.
3. Find a station that is unsurveyed by using either the Google Earth KML file download above, or by looking for stations with no entries yet in the Surfacestation image gallery database 
When you decide on stations to survey, drop a comment here to make sure we don’t get duplication of effort.

4. Locate the details on station that you want to survey. The KML file has popup ballons for each station that gives details, and you can get lat/lon from doing a right click and “properties” for a station in Google Earth.Google Earth can give you driving directions. Note that lat/lon values are not alway accurate. I’ve seen them spot on, and sometimes they are as much as a 1/2 mile off., but they’ll generally get you close.
You can also visit the NCDC MMS database here:  http://mi3.ncdc.noaa.gov/mi3qry/login.cfm and use the “guest login” button. Then do a search for the station name and match up with the city and the USHCN station # ID  in the Google Earth KML file balloon. Getting that USHCN ID# right is crucial, as some towns may have 2 or three COOP stations which are not part of the USHCN network. Once you find the right station, click on the link. Be sure to note iuf it says “current” or not.
Another clue to make sure you have the right station in the NCDC database is the “station type” field which will say something like “COOP-A, COOP, LAND SURFACE, A, A” If there is no “A” in the description, then it is not a climate station.
Also check the “Location tab” in the NCDC database, which will say something like like “fire station” or “sewage treatment plant”…you maye have to look down a few entries from the top. Once you have that, some Google web searches will often help you narrow down a likely street address if the Google Earth imagery doesn’t help you visualize the location.
The “Equpiment tab” is also useful, since it will tell you what to look for. Here is a photo link that has most of the usual components of a climate station hat will help you get an idea.
5. If you determine that the station is located at a private residence, you’ll need help locating the observer. For that you need to find the observer name. Thankfully these exist on the NCDC database also, as a signature on many of the B91 forms the observers send in. To find B91 forms with observer names, go to this url:
http://www7.ncdc.noaa.gov/IPS/coop/coop.html
Then narrow down the state and station name in the web form, and click through to see what B91 forms are available, if you don’t see any within the last 6 -12 months, chances are the station is closed (a growing problem).
Download one and you may see an observer name at the lower right. A web lookup for the name and address may lead you there. Most private observers are interested and helpful. Just be sure that you advise them that you only want to get photos of their station and immediate surroundings (6 photos minimum: NSEW at about 20-30 feet, and two overall wide shots showing the station in relation to it’s surrounding) and that you are not going to reveal their names, addresses or phone numbers in any way, or any other private info.
6. Plan your trip. If you have trouble, or need help locating a station, drop a  comment here.
7. Set your camera for 3.1 megapixels (2048×1536) for best results. Or use a photo editor program later to shrink the images to that size if you use a higher resolution. High resolution is good for long distance shots, such as are sometimes required when the station is at a fenced public facility like a water plant. You can then later crop out areas of the hi-res image. It’s like having an extra zoom level. All images should be 2 megabytes or less in size for uploading.
8. Fill out the station survey form (available here ) as best you can, making notes about the station. be sure to save it as a Adobe Acrobat PDF file, which is what is need to upload into the database. A free print to PDF application is available here at www.primopdf.com should you need one.
9. Navigate to the empty folder for the station you surveyed at the Surfacestation image gallery database and click on “add a photo” or “add items” on the left menu. Don’t try to do them all at once, as you may get a time out if your connection speed is slow. Doing 4 at a time really works well.  Here at this link is what a completed survey looks like after uploading.
10. Drop us note at info { at } surfacestations dot org to let us know! Or if you need help.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e997cc081',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Boris Johnson’s ambition to have some big ego project to memorialise him in the manner of great politicians and engineering projects of Victorian Britain would be better served by not building HS2 or indeed the barmy bridge from Scotland to Northern Ireland. All those billions should be invested in major flood protection projects across the country (Flood alerts at new high as storm lashes UK, 17 February).  Storms Ciara and Dennis over the past two weekends have shown the need for much-improved flood defences and river management systems, a need which will become increasingly critical as the climate emergency worsens and such events regularly occur in ever greater measure. It is appalling to see the destruction of people’s homes, businesses and communities by flooding and storms. It is shocking to see the distress that people are suffering from the repetition of these inundations when they have barely recovered from earlier ones. Some are suffering serious and long-term mental health problems as a consequence. The government must get its priorities right. Otherwise, by the time HS2 is completed, the communities that it will connect to in the north will be semi-permanently underwater and uninhabitable.Paul FaupelSomersham, Cambridgeshire • As the UK is battered by yet another large and destructive storm and so many of us have to battle to save our homes from flooding (in many cases for the second time in a week) it is perplexing that Extinction Rebellion, the one organisation trying to raise awareness of the causes, is considered to be equivalent to a terrorist threat. In contrast, the recently elected government, which has little if any credibility in responding to the climate emergency, considers that repeating imaginary figures as to their plans for the NHS, transport infrastructure, etc is acceptable. Maybe following all the climate catastrophes over the past weeks, there will finally be an urgent political awakening and necessary action to curtail the burning of fossil fuels. An increasing number of ordinary people are now experiencing first-hand what the alternative is. And it is bleak.Prof Michael SymondsLoughborough, Leicestershire • Given that running cold water over a plate smeared with marmalade has little effect, what about using it as the filler/sealant between sandbags? May be more effective than some of the failed flood defence schemes. Pleased to report that the Bathampton Meadows floodplain on which the Tory Bath and North East Somerset council proposed to build a massive car park is doing its job – flooded.Tim DaviesBatheaston, Somerset • Join the debate – email guardian.letters@theguardian.com • Read more Guardian letters – click here to visit gu.com/letters • Do you have a photo you’d like to share with Guardian readers? Click here to upload it and we’ll publish the best submissions in the letters spread of our print edition"
"

The nearly decade‐​old Regional Greenhouse Gas Initiative (RGGI) was always meant to be a model for a national program to reduce power plant carbon dioxide (CO2) emissions. The Environmental Protection Agency (EPA) explicitly cited it in this fashion in its now‐​stayed Clean Power Plan. Although the RGGI is often called a “cap and trade” program, its effect is the same as a direct tax or fee on emissions because RGGI allowance costs are passed on from electric generators to distribution companies to consumers. More recently, an influential group of former cabinet officials, known as the “Climate Leadership Council,” has recommended a direct tax on CO2 emissions (Shultz and Summers 2017).



Positive RGGI program reviews have been from RGGI, Inc. (the program administrator) and the Acadia Center, which advocates for reduced emissions (see Stutt, Shattuck, and Kumar 2015). In this article, I investigate whether reported reductions in CO2 emissions from electric power plants, along with associated gains in health benefits and other claims, were actually achieved by the RGGI program. Based on my findings, any form of carbon tax is not the policy to accomplish emission reductions. The key results are:



• There were no added emissions reductions or associated health benefits from the RGGI program.



• Spending of RGGI revenue on energy efficiency, wind, solar power, and low-income fuel assistance had minimal impact.



• RGGI allowance costs added to already high regional electric bills. The combined pricing impact resulted in a 12 percent drop in goods production and a 34 percent drop in the production of energy-intensive goods. Comparison states increased goods production by 20 percent and lost only 5 percent of energy-intensive manufacturing. Power imports from other states increased from 8 percent to 17 percent.



The regional program shifted jobs to other states. A national carbon tax would shift jobs to other countries. A better policy to reduce CO2 emissions is to encourage innovation rather than rely on taxes and regulation. The United States has already reduced emissions 12 percent from 2005 to 2015, more than any other developed country with a large economy, mainly through innovations in natural gas drilling techniques. There are many other opportunities to invest in innovation, for example, improved solar photovoltaic cells, more efficient batteries, small modular nuclear reactors, and nascent technologies that use fossil fuels without emitting CO2.



Ten northeast states joined together to form the RGGI to require power plants with a capacity of more than 25 megawatts to buy emission allowances for each ton of CO2 emissions. The states included Connecticut, Delaware, Maine, Maryland, Massachusetts, New Hampshire, New Jersey, New York, Rhode Island, and Vermont. The allowances were sold in quarterly auctions beginning in 2008. The initial plan was to gradually reduce the number of allowances available to achieve a 10 percent emission reduction by 2018. New Jersey dropped out of the plan in 2011. In 2013, RGGI, Inc. announced plans for a 45 percent reduction in the number of allowances available in auctions beginning in 2014, with an additional 2.5 percent reduction each year until 2020 (Brown 2013: 1). Consequently, allowance prices began to rise, and RGGI states are now negotiating an extension to 2030, with an additional 30 percent reduction in allowable emissions.



The program is touted by RGGI, Inc. as a market-based system. However, the program applies a minimum reserve price and a Cost Cap Reserve that kicks in additional allowances if an annual price cap is exceeded (Figure 1). The proposed agreement for 2030 also includes an Emissions Containment Reserve whereby states can withhold allowances if auction prices fall below a set target price. A true market-based cap and trade program would allow the market to set the price. Allowance prices averaged about $3/ton from 2008 to 2013 ranging from about $2 to $4. In 2014, there was a dramatic cut in the number of available allowances that forced prices to a high of $7.50/ton in 2015, tracking the Cost Cap Reserve target. Prices began to fall after the Clean Power Plan implementation was stayed by the Supreme Court, and hit $2.53/ton in June, 2017, compared to a reserve price of $2.15. The extension targets a $13/ton price in 2021, and $24/ton in 2030. Speculators have made up roughly one-quarter of allowance purchases, trading with compliance entities in a secondary market.





According to Hibbard et al. (2011:15), in a report for the Analysis Group, “Within the electric system, the impacts of these initial (RGGI) auctions show up during the 2009–11 period, as power plant owners priced the value of CO2 allowances into prices they bid in regional wholesale prices.” A flow diagram in that report (p. 22) shows how the auction costs flow from the electric generators to the electric distributors, and on to consumers, the same as a direct tax or fee would do.



In order to claim success for RGGI, the first cap and trade program in the United States, we need to consider some related issues:



1\. Can the measured emission reductions be accounted for by non-RGGI causes?



2\. Can the impacts on the economy be clearly broken down into statistically confirmable independent (RGGI inputs) and dependent variables (real GDP, or electric price changes)?



3\. Can the RGGI revenue expenditures be shown to have been necessary and to have had significant impacts?



4\. Were energy efficiency project claimed savings rigorously tested by weather-adjusted “before and after” meter readings?



RGGI fails to answer these questions. Unfortunately, the data needed for a robust statistical analysis (question 2) are not readily available and obtaining them is beyond the scope of this article. The other three are noted in the text that follows.



The change in electricity demand, by necessity, must consider the interplay of real economic growth, the details of that growth, changes in population, the impact of pricing, and changes in energy efficiency. The RGGI program has an impact on these parameters.



It is difficult to compare electric prices from state to state because of significant regional differences in power cost. Also, at roughly the same time RGGI started, many states began requiring increased use of energy sources like wind and solar in their Renewable Portfolio Standard (RPS) laws and set energy efficiency requirements.



A further complication is that a number of states deregulated the supply portion of electric bills allowing market competition just prior to the start of the RGGI program. All the RGGI states deregulated. Fortunately, there is a comparison sample of five non-RGGI states (Illinois, Ohio, Oregon, Pennsylvania, and Texas) that deregulated electric supply in a manner similar to the RGGI states, and also had significant RPS requirements. Both RGGI and non-RGGI states have wide variation in their RPS programs, which adds uncertainty. Increasing wind and solar power raises electric rates because they are premium-priced power sources. For example, the increase in Delaware’s electric prices by 9 percent is directly related to the RPS, which shows up on consumers’ Delmarva Power electric bills. Likewise, Maryland electric bills have increased 14 percent for the same reason, according to a report from the Maryland Energy Administration (Tung 2017: 17).1



Non-RGGI comparison states actually added more wind and solar generation than RGGI states: adding 5.5 percentage points to generation compared to 2.3 percentage points in the RGGI states. Even removing the large wind farm construction effort in Texas from the calculation, the non-RGGI comparison states still outperformed the RGGI states: adding 3.4 percentage points compared to 2.3. The cost of wind and solar power has averaged two to three times the megawatt-hour rate compared to existing conventional fuel sources. The price impact should be greater in the non-RGGI states. Despite this disadvantage, the non-RGGI states still had lower overall price increases.



Several states that offered limited deregulation were not included in the comparison, and New Jersey is not included as an RGGI state because it dropped out of RGGI in 2011, and California is not included because it began a carbon tax just a few years ago. The results shown in Figure 2 cover the period from 2002 to 2015 to capture the impact of the four policies taken together (deregulation, RPS, energy efficiency, and RGGI).





To more accurately isolate the impact of RGGI between 2007 and 2015, the weighted average of total electric revenue for the multi-state groups is used in Table 1, and shows RGGI prices rose 64 percent more than comparison states. The increase was split between direct RGGI cost pass-though and indirect cost. Direct emission allowance cost was $436 million in 2015, about half the price differential between RGGI and comparison states. The rest of the difference may be due to indirect RGGI costs. For example, when power is imported to Delaware and Maryland from the PJM Regional Transmission Organization, there are premium charges for transmission distances, transmission congestion, and capacity. An earlier study, “Cost Impacts of 2013 RGGI Rule Changes in Delaware” (Stevenson and Stapleford 2016: 2), demonstrates that RGGI allowances directly added $11 million a year to Delaware electric bills, while the indirect costs added another $28.5 million.





Prices in RGGI states rose concurrent with more energy-intense manufacturing segments of the economy leaving the RGGI states with slower overall real economic growth based on Regional Real Chained GDP (Table 2). Linking real economic growth to RGGI alone is fraught with problems: real economic growth rates in RGGI states between 2007 and 2015 varied widely from a negative 7.1 percent for Connecticut to a plus 11.9 percent for Massachusetts. Can we realistically claim RGGI helped Massachusetts but hurt Connecticut at the same time?





The comparison states’ economies grew 2.4 times faster than the RGGI states. Data from the U.S. Bureau of Economic Analysis show that the RGGI states lost 34 percent of energy-intensive businesses (primary metals, food processing, paper products, petroleum refining, and chemicals); the comparison states lost only 5 percent. The RGGI states lost 12 percent of overall goods production, while the comparison states grew by over 20 percent. We see this impact show up in industrial electric demand with the RGGI states falling 18 percent, while non-RGGI comparison states fell only 4 percent (Table 3).





Consideration also needs to be given to energy efficiency improvements as shown by the improvement in energy intensity (Table 4). RGGI states improved by 9.6 percent, while non-RGGI comparison states improved 11.5 percent. (Energy intensity improves when it goes down.)





According to RGGI, Inc. (2016), RGGI states are investing the RGGI revenue in energy efficiency projects, suggesting RGGI states should be improving energy efficiency faster than other states. Based on gains in overall energy intensity, this claim appears to be false. An explanation for this disparity may be that the funds are not going to energy efficiency, or that the energy efficiency projects may not be working well. Both effects are seen in Delaware where 35 percent of allowance revenue is assigned to the Department of Natural Resources & Environmental Control (DNREC), and the rest flows through a private, nonprofit organization known as the Sustainable Energy Utility (SEU). Delaware has received $100 million in RGGI revenue: $55 million remains unspent and another $22 million has gone to administrative overhead and fuel assistance, with just $23 million (23 percent) going for energy efficiency projects.2



The Maryland Energy Administration (2016) reported that only 25 percent of RGGI revenue was allocated to grants for energy efficiency projects, and that doesn’t take into account any money from the grants used for administration by the grantees.



Could the energy efficiency and renewable energy projects have been completed without the RGGI grants? The Maryland 2016 report, Appendix B, lists hundreds of projects receiving grants. Most of the renewable energy grants went to individuals or companies to install solar photovoltaic cells. The grants were small, running from $700 to $1,000 for residential systems that typically cost about $20,000. Solar projects receive federal tax credits, and the owners can sell renewable energy production credits to utilities that are required to buy them by state law, and receive full credit for every kilowatt-hour of energy produced from the local utility. Using a proprietary spreadsheet program, I find that the internal rate of return of a residential system falls from 10.6 percent with the state grant to 9.2 percent without the grant.3 Most of the projects would move forward without the RGGI revenue grants.



In a report for the Delaware Department of Natural Resources and Environmental Control, Small (2012: 3) found that the federally financed “Weatherization Assistance Program,” which receives 10 percent of RGGI revenue, was shut down for two years while all existing projects were reviewed and redone as needed after a federal audit found various quality control issues. This shows how state evaluation, measurement, and verification measures are not working.



The most rigorous test for energy efficiency projects is to check weather-adjusted meter readings before and after the project is implemented. I have found only one large-scale study by Alberini, Gans, and Towe (2013) that did this. The authors found Maryland homeowners who replaced their heat pumps with no incentives saved an average of 16 percent on electric usage. Meanwhile, homeowners receiving cash incentives of $300, $450, and $1,000 or more had energy savings of 6.2, 5.5, and 0 percent, respectively. The authors concluded on page 7 that “the survey responses provide suggestive evidence the ‘rebaters’ were disproportionately replacing ‘inadequate’ units, leading us to conjecture that the rebates are being used to defray the cost of more powerful units, or of units that end up being used more.”



Table 5 shows predicted changes in electricity demand in the RGGI states based on the 2007 demand adjusted for economic growth (7.2 percent from Table 4), population change (1 percent from U.S. Census data), loss of goods production (−12 percent from Table 2), and efficiency improvements (−9.6 percent from Table 4). The actual demand fell 11 million megawatt-hours, close to the projected 14 million.





Emissions were reduced about 40 percent from 2007 to 2015 from electric generating units in the RGGI states (Table 6). That compares to only about a 20 percent reduction in emissions for the country as a whole and the comparison states, suggesting RGGI has been a suc⁠cess. As raw percentages, this would be true, but the base emissions of the RGGI states are much lower than the total for the country, so a relatively small change can appear as a relatively large percent.





Table 7 shows high CO2 emission coal-fired generation _drops_ 16 percentage points in both RGGI and non-RGGI comparison states, and natural gas _rises_ virtually the same amount (10 for RGGI states versus 9 for non-RGGI states).





The non-RGGI comparison states actually added more wind and solar generation than the RGGI states (5.5 percentage points versus 2.3), even after allowing for a very large wind farm proliferation in Texas. Some RGGI auction revenue was invested in solar energy projects, but the RGGI, Inc. (2016) report identifies less than 100 MW of added solar capacity, which would account for only about 1 percent of the total wind and solar capacity added in the RGGI states according to generation data in the U.S. EIA _Electric Power Monthly_ .



Another way to sort out the impacts of the RGGI program on emissions reductions is to review regulatory and market impacts to the generation mix and emissions in detail. The impacts of exporting emissions through the increased importing of power must also be considered. If a comparison is made of the estimate of emission reductions using just factors common in all states, the comparison should isolate the impact of the RGGI program. The result of this comparison is discussed below and shows RGGI had no impact on emissions.



Delaware provides an early example of exporting emissions that can be found in a number of articles published in the _Wilmington News Journal_ beginning in January 2008. On December 17, 2008, Delaware participated in its first regional cap and trade auction. Three weeks later the Valero-owned Delaware City Refinery announced the shut-down of its electric generation at the plant. According to RGGI, Inc. (2009), CO2 emissions from the plant’s electric generation facility accounted for 17 percent of Delaware’s initial emission allocation. Valero had been gasifying petroleum coke, a waste product from the refinery, to fuel the power plant. Petroleum coke has emission rates similar to coal, but by gasifying it Valero reduced emissions of other air pollutants. So, three weeks into the RGGI program Delaware met its total 10 percent RGGI reduction goal. That isn’t the end of the story. Valero sold the facility to PBF Energy. PBF restarted portions of the power plant fueled with conventional natural gas. The petroleum coke was loaded onto ships and sent to China to be burned directly for electric generation without pollution controls.



The RGGI states export CO2 when they increase the import of electricity from other states. Between 2007 and 2015, the RGGI states doubled their imports (Table 8). Much of the imported power comes from the PJM transmission region. Adjusting for this factor decreases the RGGI state emissions reductions about 11 million tons.





CO2 emissions are down across the country. A number of major EPA regulations have been implemented since 2009. Electric power plants have seen the most impact from regulation including the Mercury & Air Toxics Standard (MATS), the Cross State Air Pollution Rule (CSAPR), the Carbon Pollution Standard for New Power Plants that established New Source Performance Standards (NSPS), and the Clean Power Plan (CPP), all aimed at reducing the use of coal and forcing the closure of older, smaller power plants that were not worth upgrading with expensive new filtration equipment, given the low cost of natural gas.



The question is how much of the improvement in power plant emission reduction was caused by EPA regulations. As shown in Figure 3, nominal natural gas prices dropped significantly starting about 2009, driven by an increase in supply from the deployment of hydraulic fracturing and horizontal well drilling technology in shale formations. The types of coal used for electric generation have no other significant uses, and price tends to be stable because electric demand does not vary much from year to year. Natural gas has a number of high volume uses, such as for industrial feedstock and as a primary fuel for heating. Heating demand can vary significantly from year to year. For example, very cold temperatures in the winter of 2014 caused a spike in demand and price. Lower overall natural gas prices played a major role in the switch from coal to natural gas for electric generation starting in 2009, and regulations impacted generation capacity starting in 2012.





Total electric generation was relatively constant since 2003, but increased almost 3 percent from 2009 to 2016 as the economy recovered from the recession (Figure 4). That increase in demand was met with wind and solar power growth driven by state Renewable Portfolio Standards along with federal and state subsidies. Coal-fired generation was relatively constant until 2008, but began to fall in 2009. The fall paralleled declining natural gas prices. Natural gas generation has been increasing at a relatively constant rate.





EPA regulations did impact coal-fired generation capacity as shown in Figure 5. The downturn in coal capacity coincides with new regulation implementation beginning in 2012. Lower natural gas prices obviously influenced the decisions to close down the coal-fired generation.





However, more important to coal-fired generation was the change in the capacity factor, that is, how often power plants ran in comparison to natural gas-fired power plants (Figure 6). The decline tracks the falling natural gas price curve that began in 2009.





With some certainty nationally, coal plant capacity reductions were caused by EPA regulations, and output reductions were caused by falling nominal natural gas prices. The impact of the two trends can be parsed. The computational details are provided in Stevenson (2017: 12). The result, both nationally and for the RGGI states, is an identical 28 percent from lost generation capacity, and 72 percent from lower natural gas prices. If the RGGI allowance program had a significant impact, it would have offset some of the impact of lower natural gas prices, because the allowance cost acts as an additional variable production cost, and would have shifted the ratio, but it didn’t. This result is not unexpected as RGGI allowance revenue only averaged 0.6 percent of electric revenue between 2007 and 2015 ($0.3 billion/$51 billion).



To complete the estimate of emissions from common factors, the changes in natural gas-fired and petroleum-fired generation need to be added. Table 9 shows that the total net estimated reduction in emissions for RGGI states, due to factors common to all states, was 59.7 million metric tons. That figure is slightly higher than the actual reduction of 57.2 million metric tons, which suggests that the actual reduction is accounted for without any significant additional contribution from the RGGI program.





According to RGGI, Inc. (2016), in its report titled _The Investment of RGGI Proceeds through 2014_ , 15 percent of RGGI revenue ($178.2 million) went to direct low-income electric bill assistance to 2.6 million households from the beginning of the RGGI auctions in 2008 through 2014. The RGGI funds, about $30 million a year, were added to the federal Low Income Home Energy Assistance Program (LIHEAP). According to the U.S. Department of Health and Human Services (2014: 10–11), the federal government provided $795 million to RGGI states in 2014. Thus, RGGI added less than 4 percent to LIHEAP ($30 million annual RGGI contribution/$795 million federal contribution).



RGGI allowance revenue totaled $1.8 billion through 2014. The allowance program added $0.85/megawatt-hour to electric bills between 2008 and 2014 ($294 million a year/348 million megawatt-hours demand a year). RGGI state residential electric demand has been fairly flat, and averaged 130.9 million megawatt-hours/year. According to the U.S. Census Bureau (2010), there were 17.3 million households in the RGGI states. Thus, residential electric demand averaged 7.6 megawatt-hours per year (130.9/17.3). The total cost of RGGI equaled $6.50/household ($0.85 × 7.6). This reduces the net contribution to low-income households to $5/year ($11.50−$6.50). Therefore, the net RGGI contribution to the federal LIHEAP was only 1.6 percent, an insignificant amount.



In this article, I investigate claims by the Acadia Center (Stutt, Shattuck, and Kumar 2015: 6) and RGGI, Inc. (2016) that the RGGI program has generated significant benefits. Using data from five comparison states with similar overall electricity policies, except for RGGI, along with looking at national trends, I find the RGGI, Inc. and Acadia Center claims to be misleading.



The Acadia Center claims that compared to other states RGGI states increased electric prices by half as much, had 3.6 percent more economic growth, and reduced emissions 16 percent more leading to greater health benefits from pollution reduction. In reality, from 2007 to 2015, net weighted average nominal electricity prices rose 4.6 percent in RGGI states compared to 2.8 percent in comparison states. Linking real economic growth to RGGI alone is fraught with problems. Real economic growth rates in RGGI states between 2007 and 2015 varied widely from a negative 7.1 percent for Connecticut to a plus 11.9 percent for Massachusetts. Also average RGGI revenue amounted to only 0.01 percent of the combined average real GDP of the RGGI states, so one wouldn’t expect much impact. Ignoring those difficulties, real economic growth was 2.4 times faster in comparison states than in the RGGI states. High RGGI state electric rates led to a 34 percent reduction in energy-intensive industries and a 12 percent drop in the goods production sector, while comparison states saw only a 5 percent drop in energy-intensive industries and a 20 percent gain in goods production.



This article finds there were no added reductions in CO2 emissions, or associated health benefits, from the RGGI program. RGGI emission reductions are consistent with national trend changes caused by new EPA power plant regulations and lower natural gas prices. The comparison requires adjusting for increases in the amount of power imported by the RGGI states, reduced economic growth in RGGI states, and loss of energy-intensive industries in the RGGI states from high electric rates.



The RGGI, Inc. report focuses on the impacts of spending the allowance revenue and suggests significant gains in energy efficiency, wind and solar investments, and assistance with low-income energy bills. Noticeably, RGGI, Inc. does not make claims of superior emission reductions or lower power prices. In reality, the spending of the allowance revenue had marginal impacts. All states have shown energy efficiency gains. The RGGI states saw a lower improvement in energy intensity at 9.6 percent compared to 11.5 percent for comparison states, so there appears to be no RGGI-related gain in overall energy efficiency. Wind and solar energy installation was slower in RGGI states, increasing by only 2.3 percentage points, while comparison states grew by 5.5 percentage points, more than twice as fast. RGGI grants for wind and solar power accounted for only about 1 percent of all the wind and solar power added by the RGGI states. The net fuel assistance help for low-income households, 15 percent of all households, added only 1.6 percent to the federal Low Income Home Energy Assistance Program, or less than $5/year. RGGI had no meaningful impact on lower-income families. Meanwhile, the other 85 percent of households saw an increase in electricity cost of $6.50/year directly caused by the RGGI allowance cost.



Alberini, A.; Gans, W.; and Towe, C. A. (2013) “Freeriding, Upsizing, and Energy Efficiency Incentives in Maryland Homes.” Fondazione Eni Enrico Mattei Working Paper No. 82. Available at https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2336044.



Brown, J. (2013) “RGGI States Propose Lowering Regional CO2 Emissions Cap 45%, Implementing a More Flexible Cost-Control Mechanism.” RGGI, Inc. Available at www.rggi.org/docs/PressReleases/PR130207_ModelRule.pdf.



European Commission (2016) “CO2 Time Series 1990–2015 per Region/Country.” _Emission Database for Global Atmospheric Research (EDGAR)_ , release version 4.3.2. Joint Research Centre (JRC)/PBL Netherlands Environmental Assessment Agency.



Hibbard, P. J.; Tierney, S. F.; Okie, A. M.; and Darling, P. G. (2011) _The Economic Impacts of the Regional Greenhouse Gas Initiative on Ten Northeast and Mid-Atlantic States._ Boston: Analysis Group.



Maryland Energy Administration (2016) _Maryland Strategic Investment Fund Report on Fund Activities FY2015._ Available at http://energy.maryland.gov/Reports/FY15_SEIF_Annual_Report.pdf.



PJM/EIS (2016) “Generation Attributes Tracking System.” _System Mix 2015_ . Available at https://gats.pjm-eis.com/GATS2/PublicReports/PJMSystemMix/FilterPJM.



RGGI, Inc. (2009) “RGGI Historical Emissions 2000 to 2008 Spreadsheet.” Available at http://www.rggi.org/historical_emissions.



______ (2016) “The Investment of RGGI Proceeds through 2014.” Available at www.rggi.org/docs/ProceedsReport/RGGI_Proceeds_Report_2014.pdf.



______ (2017) “Auction Results for Auctions 1–36.” Available at www.rggi.org/market/co2_auctions/results.



Shultz, G. P., and Summers, L. (2017) “This Is One Climate Solution That’s Best for the Environment and for Business.” _Washington Post_ (June 19).



Small, D. (2012) _Annual Report on the Weatherization Assistance Program_ . Wilmington, Del.: Delaware Department of Natural Resources & Environmental Control.



Stevenson, D. T. (2017) “Sorting Root Causes of Air Quality Improvements 2009 to 2015.” Caesar Rodney Institute Policy Paper (January). Available at https://criblog.wordpress.com/2017/02/12/sorting-root-causes-of-air-quality-improvements-2009-to-2015.



Stevenson, D. T., and Stapleford, J. E. (2016) “Cost Impacts of 2013 RGGI Rule Changes in Delaware.” Caesar Rodney Institute Policy Paper (August). Available at www.caesarrodney.org/pdfs/Cost_Impacts_of_2013_RGGI_Rule_Changes_in_Delaware.pdf.



Stutt, J.; Shattuck, P.; and Kumar, V. (2015) _Regional Greenhouse Gas Initiative Status Report, Part 1: Measuring Success._ Boston: Acadia Center (July).



Tung, M. B. (2017) “RPS Requirement and Aggregated Cost of RPS.” Presentation to the House Economic Matters Committee, Maryland Energy Administration, Baltimore, Md. (January 15). Available at http://news.maryland.gov/mea/wp-content/uploads/sites/15/2017/01/EconomicMattersPresentation1.23.2017.pdf.



U.S. Bureau of Economic Analysis (n.d.) “Real Chained GDP by State.” Interactive Tables available at www.bea.gov.



U.S. Census Bureau (2010) “Population and Housing Unit Estimate Tables by Year.” Available at www.census.gov/programs-surveys/popest/data/tables.2010.html.



U.S. Department of Health and Human Services (2014) _LIHEAP Report to Congress for Fiscal Year 2014._ Division of Energy Services. Washington: DHHS.



U.S. Energy Information Agency (EIA) (2016a) Detailed State Electricity Data: “Average Price by State by Provider (EIA-861)”; “Revenue from Retail Sales of Electricity by State by Sector by Provider (EIA-861)”; “Retail Sales of Electricity by State by Sector by Provider (EIA-861)”; “Net Generation by State by Type of Producer by Energy Source (EIA-906, EIA-920, and EIA-923)”; “U.S. Electric Power Industry Estimated Emissions by State (EIA-767, EIA-906, EIA-920, and EIA-923).” Available at www.eia.gov/electricity/data/state.



______ (2016b) _Electric Power Monthly_ (2007 and 2015): “Table 1.4.B Coal by State by Sector, Year-to-Date”; “Table 15B Petroleum Liquids by State by Sector, Year-to-Date”; “Table 1.7.B Natural Gas by State by Sector, Year-to-Date”; “Table 1.9.B Nuclear Energy by State by Sector, Year-to-Date”; “Table 1.10B Hydroelectric (Conventional) Power by State by Sector, Year-to-Date”; “Table 1.14.B Wind by State by Sector, Year-to-Date”; “Table 1.17.B Solar Photovoltaic by State by Sector, Year-to-Date”; Table 4.2 Total Average Cost of Fossil Fuels for Electric Utilities”; “Table 6.7A Capacity Factors for Utility Scale Generators Using Fossil Fuels.” Available at www.eia.gov/electricity/monthly.



1I use 2007 as the base year through 2015, unless otherwise noted. The reason for using 2007 is that RGGI auctions began in 2008, which was also the first year of the Great Recession.



2Calculation is based on information provided in an unpublished e-mail to a state senator of how DNREC spent RGGI allowance funds from 2014 to 2016, and from SEU Annual Reports (available at www.energizedelaware.org/sustainable-energy).



3I assume a 7,500 watt system @$2.85/watt cost with 20 year life, 9,000 KWh first-year generation reduced 0.5 percent per year, $0.1425/KWh electric rate rising 2 percent per year, $6 SREC value, and 30 percent federal investment tax credit.
"
"

What’s worse than a public policy debate that turns bitter and impolite? Well, for one, having the courts step into the marketplace of ideas to judge which side of a debate has the best “facts.”



Yet that’s what Michael Mann has invited the D.C. court system to do. In response to some scathing criticism of his methodologies and an allegation of scientific misconduct, the author of the infamous “hockey stick” models of global warming — because they resemble the shape of a hockey stick, with temperatures rising drastically beginning in the 1900s — has taken the global climate change debate to a record low by suing the Competitive Enterprise Institute, _National Review_ , and two individual commentators. The good Dr. Mann claims that some blogposts alleging his work to be “fraudulent” and “intellectually bogus” were libelous. (For more background on the matter, see this excellent summary by _NR_ ’s editor Rich Lowry; linking to that post is partly what led Mann to target CEI.)





Public figures must not be allowed to use the courts to muzzle their critics.



The D.C. trial court rejected the defendants’ motion to dismiss this lawsuit, holding that their criticism could be taken as a provably false assertion of fact because the EPA, among other bodies, have approved of Mann’s methodologies. In essence, the court seems to cite a consensus as a means of censoring a minority view. The defendants appealed to the D.C. Court of Appeals (the highest court in the District of Columbia).



Cato has now filed a brief, joined by three other think tanks, in which we urge the court to stay out of the business of refereeing scientific debates. (And if you liked our “truthiness” brief, you’ll enjoy this one.)



We argue that the First Amendment demands that failing to leave room for the marketplace of ideas to operate stifles academic and scientific progress, and that judges are ill‐​suited to officiate policy disputes — as history has shown time and again. The lower court clearly got it wrong here — and there are numerous cases where courts have more judiciously treated similarly harsh assertions for what they really are: expressions of disagreement on public policy that, even if hyperbolic, are among the forms of speech most deserving of constitutional protection.



The point in this appeal is that courts should not be coming up with new terms like “scientific fraud” to squeeze debate over issues impacting government policy into ordinary tort law. Dr. Mann is not like a corner butcher falsely accused of putting his thumb on the scale or mixing horsemeat into the ground beef. He is a vocal leader in a school of scientific thought that has had major impact on government policies.



Public figures must not be allowed to use the courts to muzzle their critics. Instead, as the U.S. Supreme Court has repeatedly taught, open public debate resolves these sorts of disputes. The court here should let that debate continue outside the judicial system.
"
"

Regular WUWT readers know of the issues related to Arctic Sea Ice that we have routinely followed here. The Arctic sea ice trend is regularly used as tool to hammer public opinion, often recklessly and without any merit to the claims. The most egregious of these claims was the April of  2008 pronouncement by National Snow and Ice Data Center scientist Dr. Mark Serreze of an ice free north pole in 2008. It got very wide press. It also never came true.
To my knowledge, no retractions were printed by news outlets that carried his sensationally erroneous claim.
A few months later in August, when it was clear his first prediction would not come true, and apparently having learned nothing from his first incident (except maybe that the mainstream press is amazingly gullible when it comes to science)  Serreze made another outlandish statement of “Arctic ice is in its death spiral” and” The Arctic could be free of summer ice by 2030″. In my opinion, Serreze uttered perhaps the most irresponsible news statements about climate second only to Jim Hansen’s “death trains” fiasco. I hope somebody at NSIDC will have the good sense to reel in their loose cannon for the coming year.
Not to be outdone, in December Al Gore also got on the ice free bandwagon with his own zinger saying on video that the “entire north polar ice cap will be gone within 5 years“. There’s a countdown watch on that one.
So it was with a bit of surprise that we witnessed the wailing and gnashing of teeth from a number of bloggers and news outlets when in his February 15th column, George Will, citing a Daily Tech column by Mike Asher, repeated a comparison of 1979 sea ice levels to present day. He wrote:
As global levels of sea ice declined last year, many experts said this was evidence of man-made global warming. Since September, however, the increase in sea ice has been the fastest change, either up or down, since 1979, when satellite record-keeping began. According to the University of Illinois’ Arctic Climate Research Center, global sea ice levels now equal those of 1979. 
The outrage was immediate and widespread. Media Matters: George Will spreads falsehoods Discover Magazine: George Will: Liberated From the Burden of Fact-Checking Climate Progress: Is George Will the most ignorant national columnist? One Blue Marble Blog: Double Dumb Ass Award: George Will George Monbiot in the Guardian: George Will’s climate howlers and Huffington Post: Will-fully wrong
They rushed to stamp out the threat with an “anything goes” publishing mentality. There was lots of piling on by secondary bloggers and pundits.
Feb 15th NSDIC Arctic Sea Ice Graph - click for larger image
Meanwhile, back at the ranch, I got interested in what was going on with odd downward jumps in the NSIDC Arctic sea ice graph, posting on Monday February 16th NSIDC makes a big sea ice extent jump – but why? Then when I was told in comments by NSIDC’s Walt Meier that the issue was “not worth blogging about” I countered with Errors in publicly presented data – Worth blogging about? 
It soon became clear what had happened. There was a sensor failure, a big one, and both NSIDC and Cryosphere today missed it. The failure caused Arctic sea ice to be underestimated by 500,000 square kilometers by the time Will’s column was published. Ooops, that’s a Murphy Moment.
So it is with some pleasure that today I offer you George Will’s excellent rebuttal to the unapologetic trashing of his column . The question now is, will those same people take on Dr. Mark Serreze and Al Gore for their irresponsible proclamations this past year? Probably not. Will Serreze shoot his mouth off again this year when being asked by the press what the summer ice season will bring? Probably, but one can always hope he and others have learned something, anything, from this debacle.
Let us hope that cooler heads prevail.
Climate Science in A Tornado

By George F. Will, Washington Post
Friday, February 27, 2009; A17

Few phenomena generate as much heat as disputes about current orthodoxies concerning global warming. This column recently reported and commented on some developments pertinent to the debate about whether global warming is occurring and what can and should be done. That column, which expressed skepticism about some emphatic proclamations by the alarmed, took a stroll down memory lane, through the debris of 1970s predictions about the near certainty of calamitous global cooling.
Concerning those predictions, the New York Times was — as it is today in a contrary crusade — a megaphone for the alarmed, as when (May 21, 1975) it reported that “a major cooling of the climate” was “widely considered inevitable” because it was “well established” that the Northern Hemisphere’s climate “has been getting cooler since about 1950.” Now the Times, a trumpet that never sounds retreat in today’s war against warming, has afforded this column an opportunity to revisit another facet of this subject — meretricious journalism in the service of dubious certitudes.
On Wednesday, the Times carried a “news analysis” — a story in the paper’s news section, but one that was not just reporting news — accusing Al Gore and this columnist of inaccuracies. Gore can speak for himself. So can this columnist.
Reporter Andrew Revkin’s story was headlined: “In Debate on Climate Change, Exaggeration Is a Common Pitfall.” Regarding exaggeration, the Times knows whereof it speaks, especially when it revisits, if it ever does, its reporting on the global cooling scare of the 1970s, and its reporting and editorializing — sometimes a distinction without a difference — concerning today’s climate controversies.
Which returns us to Revkin. In a story ostensibly about journalism, he simply asserts — how does he know this? — that the last decade, which passed without warming, was just “a pause in warming.” His attempt to contact this writer was an e-mail sent at 5:47 p.m., a few hours before the Times began printing his story, which was not so time-sensitive — it concerned controversies already many days running — that it had to appear the next day. But Revkin reported that “experts said” this columnist’s intervention in the climate debate was “riddled with” inaccuracies. Revkin’s supposed experts might exist and might have expertise but they do not have names that Revkin wished to divulge.
As for the anonymous scientists’ unspecified claims about the column’s supposedly myriad inaccuracies: The column contained many factual assertions but only one has been challenged. The challenge is mistaken.
Citing data from the University of Illinois’ Arctic Climate Research Center, as interpreted on Jan. 1 by Daily Tech, a technology and science news blog, the column said that since September “the increase in sea ice has been the fastest change, either up or down, since 1979, when satellite record-keeping began.” According to the center, global sea ice levels at the end of 2008 were “near or slightly lower than” those of 1979. The center generally does not make its statistics available, but in a Jan. 12 statement the center confirmed that global sea ice levels were within a difference of less than 3 percent of the 1980 level.
So the column accurately reported what the center had reported. But on Feb. 15, the Sunday the column appeared, the center, then receiving many e-mail inquiries, issued a statement saying “we do not know where George Will is getting his information.” The answer was: From the center, via Daily Tech. Consult the center’s Web site where, on Jan. 12, the center posted the confirmation of the data that this column subsequently reported accurately.
The scientists at the Illinois center offer their statistics with responsible caveats germane to margins of error in measurements and precise seasonal comparisons of year-on-year estimates of global sea ice. Nowadays, however, scientists often find themselves enveloped in furies triggered by any expression of skepticism about the global warming consensus (which will prevail until a diametrically different consensus comes along; see the 1970s) in the media-environmental complex. Concerning which:
On Feb. 18 the U.S. National Snow and Ice Data Center reported that from early January until the middle of this month, a defective performance by satellite monitors that measure sea ice caused an underestimation of the extent of Arctic sea ice by 193,000 square miles, which is approximately the size of California. The Times (“All the news that’s fit to print”), which as of this writing had not printed that story, should unleash Revkin and his unnamed experts.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e97d2a231',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Click for full PDF
From Dr. Roger Pielke Sr. Climate Science Weblog
There is a letter to the President published by the Cato Institute that headlines [thanks to ICECAPand Dr. Patrick J. Michaels to alerting us to it];
“Few challenges facing America and the world are more urgent than combating climate change.The science is beyond dispute and the facts are clear.” — PRESIDENT-ELECT BARACK OBAMA, NOVEMBER 19 , 2008
With all due respect Mr. President, that is not true.
The letter is signed by over 100 scientists.
Climate Science wants to comment on the specific statements of science in the letter which is reproduced below:
“We, the undersigned scientists, maintain that the case for alarm regarding climate change is grossly overstated. Surface temperature changes over the past century have been episodic and modest and there has been no net global warming for over a decade now.1,2 After controlling for population growth and property values, there has been no increase in damages from severe weather-related events.3 The computer models forecasting rapid temperature change abjectly fail to explain recent climate behavior.4 Mr. President, your characterization of the scientific facts regarding climate change and the degree of certainty informing the scientific debate is simply incorrect.”
Comments by Climate Science

“Surface temperature changes over the past century have been episodic and modest and there has been no net global warming for over a decade now.”

This is correct using the global average surface temperature. An effective analysis of this issue has been presented at the weblog http://rankexploits.com/musings/category/climate-sensitivity/. However, using the global average upper ocean heat content changes, the warming in the 1990s and early 2000s ended in 2003, so the more rigourous metric for global warming indicated “no net global warming” for 6 years.

After controlling for population growth and property values, there has been no increase in damages from severe weather-related events.

This is a correct statement which has been extensively discussed and summarized at http://sciencepolicy.colorado.edu/prometheus/category/climate-change; see also Chapter 2 in  Pielke, R.A., Jr. and R.A. Pielke, Sr., 1997: Hurricanes: Their nature and impacts on society.

The computer models forecasting rapid temperature change abjectly fail to explain recent climate behavior.

This is a robust conclusion both on the global scale (e.g. see) and on the regional scale (e.g see and see). 
The dismissive response on Real Climate and on Grist to this letter do not provide the objective scientific rebuttal to these science claims. This is unfortunate and is misleading policymakers, but, as we have learned and reported many times on at Climate Science and elsewhere (e.g. see and see), this is the way the IPCC and CCSP community deals with solid science that disagrees with their perspective. 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e97247a60',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

A few weeks ago, the Clinton administration finally got around to signing the Kyoto Protocol, the global warming treaty that obligates the United States to reduce greenhouse gas emissions by 7 percent below 1990 emission levels by the year 2012. Both supporters and opponents of the treaty agree that meeting that goal will require between a 30 and a 40 percent reduction in the greenhouse gas emissions that would otherwise occur about a decade hence. Beyond that, sorting out the scientific and economic argument between the two is difficult for a nonspecialist. Here’s the scorecard to date. 



Treaty supporters say that just about all the scientists engaged in global warming research now accept that the problem is real and must be addressed. Well, yes and no. Most (but by no means all) scientists engaged in the field agree that industrial emissions are probably affecting the climate. But the evidence is circumstantial. As the United Nations’ International Panel on Climate Change (IPCC) noted in its most recent (1995) report, the evidence thus far “cannot be considered as compelling evidence of cause‐​and‐​effect link between anthropogenic forcing and changes in the Earth’s surface temperature.” 



The “balance of evidence suggests” (in the words of the IPCC) that industrial emissions are the culprit, but that’s hardly conclusive. And the consensus about the matter is not as nearly universal as suggested. Seventeen thousand scientists (half of whom are trained in physics, geophysics, climate science, meteorology, oceanography, chemistry, biology or biochemistry) recently signed a petition written by Frederick Seitz, a past president of the National Academy of Sciences, declaring that there is no compelling evidence to justify reducing greenhouse gas emissions at all. 



Nor do scientists agree on how hot it would get if we did nothing. The IPCC’s “best guess” back in 1990 was that industrial greenhouse gas emissions would increase average global temperatures by 5.8 degrees (all temperature figures are Fahrenheit) by the end of the next century. In 1995 the IPCC adjusted its “best guess” down to 3.6 degrees. Three studies published this year (by Hansen, Dlogokencky and Myhre) suggest that the present “best guess” stands at about 2.25 degrees of warming by the end of the next century. Most experts doubt that that amount of warming would be particularly worrisome (indeed, we’re already about half way there temperature‐​wise, and the effect of this “global warming” has thus far proven underwhelming, to say the least). 



Killing the coal industry to reduce temperatures 1/​7th of 1 degree 50 years hence is justified by treaty advocates as a necessary “first step” of about 30 that must necessarily come. Treaty opponents do a quick cost/​benefit analysis and conclude that treaty advocates have lost their grip on reality. 



While the Kyoto Protocol envisions significant cuts in greenhouse gas emissions, scientists on all sides of the debate agree that its impact will be virtually undetectable. Tom Wigley, a highly respected senior scientist at the U.S. National Center for Atmospheric Research (and a scientist, moreover, usually thought of as in the alarmist camp), recently calculated that the Kyoto Protocol would only reduce temperatures by 0.13 degrees by 2050 if we accept the IPCC’s 1995 estimate of warming under a business‐​as‐​usual scenario. The Kyoto Protocol would have no meaningful impact on future climate change because, as along as we use fossil fuels, the question of global warming is not “if” but “when.” 



Finally, advocates of the treaty argue that its costs will be negligible, while opponents warn of a replay of the 1970s energy crises. The best evidence for each argument comes from studies issued by the Clinton administration (the study most optimistic about costs comes from the president’s Council of Economic Advisers (CEA), the most pessimistic from the Energy Information Administration). Both studies make valid projections. The differences are found in the assumptions. But it’s worth noting that the economic models relied on by the CEA reveal that, absent a truly global emissions trading system (a system that was hotly opposed by most nations at the recently concluded talks in Buenos Aires), America would be forced to abandon coal‐​fired electricity within the next decade to keep compliance costs from skyrocketing. 



Killing the coal industry to reduce temperatures 1/​7th of 1 degree 50 years hence is justified by treaty advocates as a necessary “first step” of about 30 that must necessarily come. Treaty opponents do a quick cost/​benefit analysis and conclude that treaty advocates have lost their grip on reality. 



And that, dear readers, is where we stand today. Misleading public characterizations of the global warming debate notwithstanding, the case for the Kyoto Protocol is pretty threadbare. Of course, the Senate is unlikely to ratify the treaty, a fact conceded by the administration in its decision not to send it up for a vote until at least 2001. If the past is any prologue, the case for ratification will continue to weaken. The question is, Will anyone be able to see through the political hot air to notice?
"
"Now that Japanese giants Toshiba and Hitachi have walked away from UK nuclear power projects that had previously been abandoned by others, it has forced the government to reassess the pro-nuclear bias of its energy policy. Greg Clark, the UK business secretary, has recognised that nuclear power is no longer cost competitive with renewable energy, but don’t expect any extra push into the cheaper technology.  There is easily enough solar and wind energy available to make up for the cancellation of the nuclear projects and to produce the low-carbon electricity required to make the UK’s 2030 carbon emissions targets achievable. Instead, however, the country’s incentives and regulations favour developing more power plants driven by natural gas. Having hacked back emissions from power by over two-thirds since 1990, progress with decarbonising the grid risks coming to an end.  According to the UK parliament’s Committee on Climate Change, the UK needs to cut power emissions from about 265g of carbon dioxide per kilowatt hour in 2017 to under 100g by 2030. The government had been substantially relying on nuclear power to do this, having originally identified eight sites as viable for new plants. Six projects were taken forward, including Hitachi and Toshiba plants in Wales and Cumbria respectively.  Yet despite much larger government incentives than those available for renewables, most private nuclear builders are now steering clear, having seen the problems with new plants in the likes of the US and France. The only two projects still on the slate are a joint venture by EDF of France and CGN of China – both foreign state-owned companies. They are building the UK’s first new plant in over two decades, Hinkley C in south-west England; while also planning a second, Bradwell B, in the south east.  Even before the latest announcement that Hitachi’s Wylfa plant in Wales was being suspended, the Committee on Climate Change was already saying the UK needed to build more renewable capacity to reach its carbon reduction targets. Now the problem is even worse.  In 2018, 19% of the UK’s electricity was generated by nuclear plants. With most existing plants due to retire over the next few years, I calculate this may now fall to 10% by 2030 when you factor in the new-build cancellations. Solar and wind generation could easily more than make up for this. For years, renewables’ share of generation has been steadily rising. It reached 30% in 2018 and is due to reach 35% in 2020. But with no new incentives for onshore wind and solar and only limited incentives for offshore wind, it looks likely to fall far short of its potential.  The government incentivises renewable energy projects through so-called contracts for difference (CFD) auctions in which the most competitive bidders are granted contracts to supply electricity at fixed prices. This year, it is set to auction some new offshore wind farm contracts. Only a trickle of projects that don’t receive such incentives go ahead, so the number of contracts on offer effectively dictates how much offshore wind capacity will be built.  With offshore wind currently providing about 7% of generation, I have heard from informal soundings that the new contracts will add less than 10% more. In other words, it would at best offset the decline in nuclear. With no further contracts in the pipeline at present, it suggests low-carbon power in the UK is at a standstill.  The reason why more renewables are not on the cards is because the Treasury is keen to limit energy incentives. It worries that the electricity price has been increasing – and hence the Treasury wants to strictly limit new incentives, the costs of which are added to electricity bills. This, however, ignores the fact that CFD prices will benefit from the falling cost of building offshore wind farms – the price has more than halved in three years. Nevertheless, the amount of money available to pay for the contracts is being limited to around half that being made available to owners of gas-fired power plants to supply capacity when the wind isn’t blowing. If all 27GW of offshore wind power schemes in various stages of planning got contracts, I calculate it would supply around one-third of the total electricity requirement. Coupled with the remaining nuclear power and the renewables that are already onstream, that would reach the 75% of power that the Committee on Climate Change reckons needs to be coming from these low-carbon sources by 2030 to achieve the emissions targets. This is not counting potential offshore wind resources which are not even being mobilised, plus large possibilities for onshore wind and solar. Instead, gas-fired power looks set to supply around half of UK electricity by 2030, compared to 40% at present. One government justification for being less generous to renewables is that unlike gas or nuclear, they do not represent “firm” power – in other words, they only generate when the wind is blowing or the sun is shining. 
Proponents of renewable energy counter that you can reduce the generating capacity required by increasing the use of batteries to store power on the grid and by incentivising consumers to, say, use more power overnight when demand is lower.  Yet one other option that attracts less attention is that you also get spare “firm” capacity from small gas engines or open-cycle turbines. These can be built quickly and would only be sparingly needed in a system mostly supplied by renewables.  Based on my calculations using Hinkley C and Wylfa, they cost around one-twentieth of the projected cost of new nuclear power. They are also nearly half the price of large gas-fired “CCGT” plants. Instead, however, the government spends the lion’s share of its incentives pot on large conventional power plants, many of which would operate whether they were subsidised or not.  The underlying obstacle seems to be political opposition within the Conservative Party. My understanding from renewable energy lobbying sources is that the government’s Department for Business, Energy and Industrial Strategy would like to promote renewable energy more, but is held back by the Treasury, which wants to leave it to “the market”.  The upshot is that government policy is offering large incentives to new nuclear, gas-fired power and also shale gas extraction – but, paradoxically, not many are actually being developed. Meanwhile the cheapest options – onshore wind, solar and offshore wind – are being discriminated against. The collapse of the UK’s nuclear power plans should be an opportunity to think again. How frustrating that decarbonising power is instead falling off the agenda."
"
Minus 13 degrees – the coldest it’s been in April
 From Weatherzone – Brett Dutschke, 
Wednesday April 29, 2009 – 14:58 EST
Charlotte Pass, 1,837m, Snowy Mountains of New South Wales, Australia
 
A new Australian record was set early this morning, a temperature of minus 13 degrees, at Charlotte Pass on the Snowy Mountains.
This is the lowest temperature recorded anywhere in Australia in April and is 13 below the average. Nearby at Perisher it dipped to minus 11 degrees and at the top of Thredbo it dipped to minus 10.
Across the border, on the Victorian Alps April records were broken at Mt Hotham where it chilled to minus eight degrees and Mt Buller and Falls Creek where it got as low as minus seven.
A few other locations set April low temperature records also. In Tasmania Lake Leake was as cold as minus six, Sheffield and Dover both reached minus one and Flinders island got to zero. Hobart had its coldest April night in 46 years, recording a low of 1.7 degrees, seven below average.
While much of inland NSW and Victoria will be colder tomorrow morning than it was this morning under clearer skies, the Alps should be a little warmer due to a rise in humidity.

Note, all temperatures in the story above are in Centigrade. Photo and map added by Anthony.
Here are the all-time highs and lows for the continent of Australia (source Perth Weather Center)
HIGHEST RECORDED TEMPERATURE:

 Oodnadatta, South Australia  50.7 C  (123.3 F) on the 2nd January, 1960

LOWEST RECORDED TEMPERATURE:

 Charlotte Pass, New South Wales  -23.0 C  (-9.4 F) on the 29th June, 1994

While this is certainly a significant new cold record this early in Australia’s fall going on winter, one must always remember that weather is not climate. – Anthony
(h/t to WUWT reader “Chuck”)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96bc62ff',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"To sort or not to sort, that is the question. Lots of people wonder whether it’s really worth their time and effort to separate, wash and store recyclable materials – especially if it takes more energy to recycle, or if the plastics sent for recycling end up in overseas landfill. The truth is, the issue is complex, and even experts can’t agree on the economic and environmental benefits of recycling.  There are four popular arguments, typically used by organisations and individuals to promote recycling: that it reduces landfill waste, that it saves public money, that it creates jobs and that it encourages consumers to reduce waste in the first place. Let’s consider each of these in turn.  Images of putrefying waste in landfill sites, generating greenhouse gas emissions and polluting the environment, are one of the most compelling reasons for recycling. The 1999 European Landfill Directive set targets to reduce biodegradable waste, and in response the UK government increased tax on landfill disposal, introducing an escalating duty,  which currently sits at £88.95 per tonne.  Then, in 2003, the Waste and Recycling Act established kerbside collection of recyclable materials. Rising levels of recycling and incineration, as well as the escalating landfill tax, have certainly reduced the proportion of waste dumped on landfill sites in the UK.  But the National Audit Office revealed that some of the plastics that residents separate for recycling are being exported overseas, to places such as Malaysia and Vietnam, where there are insufficient checks to ensure this material is actually recycled. The industry is also facing investigation for fraud and corruption, over these matters. So it could be that millions of tonnes of UK recycling is simply ending up in landfill in other parts of the world. 


      Read more:
      China bans foreign waste – but what will happen to the world's recycling?


 Part of the problem is that there are limited facilities to recycle mixed plastics in the UK. It costs a lot of money to separate and recycle different types of plastic, using specialist machinery. But there is infrastructure for plastic bottle recycling in the UK, which is why many council schemes historically only collected this type of plastic.  It costs a lot of money for local authorities to manage household waste. Disposal facilities owned by private companies, such as Veolia and SITA, charge local authorities gate fees per tonne of waste - around £107 per tonne for landfill and £86 per tonne for incineration. Local authorities in England produced 22.4m tonnes of waste in 2017, of which 45% was recycled – so that’s a lot of money saved.  Variations in collection systems across England and in material streams (such as paper, glass, cans) make it difficult to predict the cost per tonne of recycling, but it does cost significantly less than disposal, because the material can be sold. Aluminium cans are one of the more profitable materials in your waste and are sold by local authorities or waste contractors to be melted down and made into new cans.  In the UK, campaigns such as Recycle for London’s Nice Save use a moral message to emphasise the savings that local authorities can make when people recycle. But this is partly because laws such as the landfill tax have made recycling the cheaper option. The prices of different recyclable materials can fluctuate, which can limit the savings made. But this depends on the contracts local authorities strike with private waste management companies, and who takes on the risk.  In any case, UK citizens might wonder why taxpayers foot the bill for recycling, when in other parts of Europe producers are responsible. The government’s new waste strategy for England does include plans to extend responsibility for packaging to producers, by introducing a deposit scheme for bottles and asking producers to pay to cover the cost of recycling. But it’s not clear how this will work with existing private waste contracts. The charity Green Alliance claimed that recycling and reuse could create over 200,000 new jobs in the UK. Compared to disposal, recycling does create jobs, because waste sorted by consumers provides feed stock for an economy in global materials. How consumers sort their waste – whether in one box or separate boxes – leads to different supply chains and labour processes.  For example, if you’re putting all of your dry recyclables into one box, these materials will need to be taken to a special facility that employs people to sort them by hand, alongside machine processing. Many jobs in the recycling industry are low skilled and dirty work. They are often performed by migrant labour, or within the precarious informal economy overseas. Yet there are also mid-skilled, professional jobs – such as public and private waste service officers, who manage and supervise operations – and these opportunities will grow if the government creates incentives for producers to use recyclable materials, or invests in systems to promote reuse.  There are, of course, more effective ways of dealing with waste than recycling. Reusing, reducing and preventing waste – for example, by choosing products that are less packaged, refusing disposable coffee cups or buying secondhand – are all better options. Environmental organisations and influencers have targeted keen recyclers with this message, in the hope that they will take further steps to live more sustainably.  But there are only limited changes a person can make to their shopping habits, in a marketplace where packaging is embedded within infrastructures of provision. So responsible waste management is really a responsibility shared between governments, producers, local authorities, waste companies and citizens. In particular, the companies that create the materials that become household waste have huge power to reduce it. On the whole, it probably is worth the effort to sort your waste, despite some problematic practices, because recycling does drive down the amount of waste going to landfill and demand for recycling services will help drive improvements and oversight. There’s still a long way to go, before the UK can manage its waste sustainably as a society – and it’ll only get there if governments and citizens keep up their efforts to improve this process."
"
Another anemic solar cycle 23 sunspeck, could 19th century astronomers have seen it?
From Spaceweather.com

SUNSPOT 1016: A ring-shaped sunspot numbered 1016 has emerged near the sun’s equator. Its magnetic                polarity identifies it as a member of old Solar Cycle 23. Until                these old cycle sunspots go away, the next solar cycle will remain                in                abeyance.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96505207',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Mike Ronanye writes:
SWPC has just made a change in their solar cycle predictions in the middle of the month without any preannouncement. Both Sunspot and F10.7cm predictions were altered significantly. 



See the following links:
http://www.swpc.noaa.gov/SolarCycle/
The off-cycle update is in this week’s PDF report which contains the altered graphics:
http://www.swpc.noaa.gov/weekly/pdf/prf1747.pdf
You can see the last monthly summary here which I have been complaining reporting about, here:
http://www.swpc.noaa.gov/weekly/pdf/prf1745.pdf
This should have been the January 2009 summary but SWPC recycled the December 2008 summary.
I looked for but was unable to find any press releases. Please search for any additional information and post it here. If you downloaded any SWPC data or graphics hold on to it. I will be updating my SWPC Sunspot animation.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9859b2b9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The Ganges is a lifeline for millions of people who live within its catchment as a source of water, transport and food. During the Hindu pilgrimage known as Kumbh Mela the Ganges plays host to the largest human gathering on Earth as 120m people arrive to bathe in the river over 49 days. Despite its tremendous spiritual significance, the Ganges is also notorious for having some of the most polluted water in the world. For 79% of the population of the Ganges catchment, their nearest river fails sewage pollution standards for crop irrigation. Some 85% of the population live near water that isn’t safe for bathing and Allahabad – where Kumbh Mela takes place in 2019 – is one of those places.  Our own research suggests that as the number of people living in nearby cities increases, the problem with water quality in the Ganges worsens. Urban populations in the Ganges catchment contribute around 100 times more microbial pollution per head to the river than their rural counterparts. This means that untreated sewage discharged from a sewer appears worse for river water quality than sewage discharge where there are no sewers at all. When we examined 10 years of water quality data we found that the concentration of faecal coliforms – a common pollution indicator found in human faeces – increased when the density of people living upstream increased. This makes sense: more people means more poo.  But we also found that people living in cities in India contribute more pollution per person than those in rural areas – how much more depends on the population density. A person living in an area in India with 1,000 people per km², a density similar to central London, contributes on average 100 times more pollution to the nearest river than they would in an area with 100 people per km² – say, rural Devon in the UK. So why does it appear that a person living in an Indian city produces more sewage pollution than someone living in the countryside? Of course, people in the cities are unlikely to actually contribute significantly more faeces than those in rural communities. Instead, it’s probably sewers that are to blame. In cities, extensive sewage networks efficiently flush sewage to the river, whereas in rural areas more people defecate in the open or in pit latrines. This means faeces in rural areas are less likely to be washed into the river and the bacteria and viruses they carry are more likely to die in situ. As the population density of a place increases, sewers become more common. Sewage removal is essential for the protection of public health, but without effective treatment, as is typically the case in the Ganges catchment, it comes at the cost of increased river pollution and waterborne diseases for people living downstream. It’s therefore clear that water quality in the Ganges is a more complex and widespread problem than previously thought. We’d expected that cities, with their more advanced sewage management, would be better for the river. What we found was the opposite – more sewers without sewage treatment makes river pollution worse. The urgency to invest, not only in sewers, but in the treatment of sewage has never been greater – especially in the most densely populated areas. However, the Western approach of taking all waste to a central treatment plant is expensive and so may not be the best solution.  Onsite treatment technologies such as off-grid toilets or decentralised treatment plants are rapidly developing and may help improve river water quality sooner, enabling more and more people to celebrate Kumbh Mela safely."
"Conservation is often a conflict between the demands of development and a desire to do what is best for the environment. It’s rare that we get the chance to report a decision which was taken for the good of people that has also panned out well for nature’s ecosystems. However, that is just what our new research paper found. Saving energy from street lighting is not just a green option, it also makes good financial sense. Two solutions in particular include replacing old High-Pressure Sodium (HPS) lightbulbs with new and energy-efficient Light-Emitting Diodes (LEDs) and turning the lights out entirely during the latter part of the night when fewer people are around. In the UK, these changes in lighting technology have been gradually taking effect over the last decade or so. While these decisions were made for good reasons, we knew little about how they would influence nocturnal wildlife. Our team of experts at York and Newcastle universities were interested in finding out how moths might be affected by the switchoff and new LED lighting, as they play an important role as night-time pollinators of a wide range of flowers, and have declined in abundance by 40% in 40 years.  Light pollution, from street lights and other sources, has been suggested as a possible cause of this decline, though there are other factors such as climate change and habitat loss. Our previous research showed that under HPS street lighting left on all night, moths were distracted from visiting flowers and instead flew higher up, around street lights. This resulted in less pollen being carried by moths in lit areas, and a subsequent study by Swiss researchers demonstrated that this actually caused reduced fruit production. 


      Read more:
      Fatal attraction: how street lights prevent moths from pollinating


 In our study, we asked whether the disruption to nocturnal ecosystems from street lights might be eased or exacerbated by the introduction of new energy-efficient LED street lighting. Working on farmland in East Yorkshire in the UK, we set up a chain of mock street lights alongside hedgerows that would allow us to manipulate the type and duration of lighting. We compared older HPS lights to LEDS, and standard full-night lighting to part-night lighting, in which lights were turned off at midnight. All lighting was compared to an unlit control that replicated natural darkness. Under each lit and unlit treatment we placed several plants of White Campion (Silene latifolia), a common wildflower known to be pollinated by both bees and moths. We left each plant in the field for four days and nights before measuring what proportion of flowers had been pollinated, and the weight and number of seeds in every fruit. LEDs are rich in blue light which is attractive to moths but we found no difference in the rate of pollination between plants under LEDs and those under HPS lights. Our data did show that the differences between pollination under full-night lighting and in natural darkness were erased when lights were turned off at midnight.  Surprisingly, this wasn’t just a partial improvement. We found no significant difference between rates of pollination in part-night lighting treatments and in natural darkness, and this suggests that turning lights off at or after midnight may allow nocturnal ecosystems to function as normal in the second half of the night. These results are quite encouraging. Local authorities can save money and energy from street lighting and help nocturnal ecosystems recover from light pollution at the same time.  So there’s no evidence that the switch from HPS lights to LEDs increases the negative impacts of lighting on wildlife, and even better, switching to part-night lighting actually appears to reduce them. By switching lights off at midnight there is the potential to tackle two issues at once – reduce energy bills and the ecological impact of light pollution."
"

Some naive people might have been convinced that the U.S. House voted to wreck the American economy by endorsing cap and trade because it was the only way to save the world. But even many environmentalists had given up on the bill approved last Friday. It is truly a monstrosity: it would cost consumers plenty, while doing little to reduce global temperatures.   
  
  
But the legislation had something far more important for legislators and special interests alike. It was a pork‐​fest that wouldn’t quit.   
  
  
Reports the _New York Times_ :



As the most ambitious energy and climate‐​change legislation ever introduced in Congress made its way to a floor vote last Friday, it grew fat with compromises, carve‐​outs, concessions and out‐​and‐​out gifts intended to win the votes of wavering lawmakers and the support of powerful industries.   
  
  
The deal making continued right up until the final minutes, with the bill’s co‐​author Representative Henry A. Waxman, Democrat of California, doling out billions of dollars in promises on the House floor to secure the final votes needed for passage.   
  
  
The bill was freighted with hundreds of pages of special‐​interest favors, even as environmentalists lamented that its greenhouse‐​gas reduction targets had been whittled down.   
  
  
Some of the prizes were relatively small, like the $50 million hurricane research center for a freshman lawmaker from Florida.   
  
  
Others were huge and threatened to undermine the environmental goals of the bill, like a series of compromises reached with rural and farm‐​state members that would funnel billions of dollars in payments to agriculture and forestry interests.   
  
  
Automakers, steel companies, natural gas drillers, refiners, universities and real estate agents all got in on the fast‐​moving action.   
  
  
The biggest concessions went to utilities, which wanted assurances that they could continue to operate and build coal — burning power plants without shouldering new costs. The utilities received not only tens of billions of dollars worth of free pollution permits, but also billions for work on technology to capture carbon‐​dioxide emissions from coal combustion to help meet future pollution targets.   
  
  
That deal, negotiated by Representative Rick Boucher, a conservative Democrat from Virginia’s coal country, won the support of the Edison Electric Institute, the utility industry lobby, and lawmakers from regions dependent on coal for electricity.   
  
  
Liberal Democrats got a piece, too. Representative Bobby Rush, Democrat of Illinois, withheld his support for the bill until a last‐​minute accord was struck to provide nearly $1 billion for energy‐​related jobs and job training for low‐​income workers and new subsidies for making public housing more energy‐​efficient.   
  
  
Representative Joe Barton, a Texas Republican staunchly opposed to the bill, marveled at the deal‐​cutting on Friday.   
  
  
“It is unprecedented,” Mr. Barton said, “but at least it’s transparent.”



This shouldn’t surprise anyone who follows Washington. Still, the degree of special interest dealing was extraordinary. Anyone want to imagine what a health care “reform” bill is likely to look like when legislators finish with it?
"
"Flooding not only wrecks businesses, destroys homes and disrupts everyday life but also causes long-lasting and dangerous levels of stress, residents from flood-hit communities have said. Hundreds of homes have been flooded and six people are thought to have died across England and Wales after heavy downpours and successive storms further exposed the fragility of flood defences and the gravity of the climate emergency.  More flooding is expected as river levels continue to threaten to breach barriers. People in flood-hit areas are grappling with a seemingly relentless onslaught of extreme weather conditions, and are making preparations for the future while having to deal with the present consequences. “I am lucky, only one room of my house was flooded, but some people have had to leave everything,” Amanda Gillender, a 71-year-old painter from Eardisland on the River Arrow in Herefordshire, told the Guardian. The scale of the flooding in Gillender’s village has been unprecedented and there are record levels of water in rivers nearby. She fears life there may become untenable if the climate crisis is not taken seriously. But for now, her most immediate concern is dealing with her insurance claim as she assesses the damage to her 500-year old cottage. “The worst thing about flooding is the waiting and watching as the water rises foot after foot deep,” she said. “There’s nothing you can do. Once it has happened you’re completely isolated because you can’t go out and are left alone with the painful, disruptive aftermath.” Elsewhere, the cumulative stress is having a grave impact on communities. “There are thousands of small human stories of stress, worry and disappointment,” said Rachel Buchanan, from the Ludlow area in Shropshire. “We’re all looking at river level websites now before going out. It just makes everyone a little less willing to leave their home.” People are being prevented from getting to work and social events, caring for family members and attending to livestock, and are becoming ever more lonely, Buchanan believes. “You become slightly more isolated as the effects of the flooding encroach on everyday life,” she said. “A friend of mine who is a single mother simply won’t go out now. Small businesses are losing a day or a week of work and the roads are crumbling – it’s a mess.” All across the UK, flooding has become more frequent and the impact can be felt years later. While insurance agents visit to assess how much compensation is to be paid out, houses in areas not previously classified as being at risk of flooding are revalued. Greenhouse gas emissions from burning fossil fuels, forest destruction and other human activities are trapping heat and putting more energy into the climate system.  Hotter air means heatwaves are much more likely. For example, scientists now say the unprecedented heat and wildfires across the northern hemisphere in 2018 “could not have occurred without human-induced climate change”. In Australia, the scorching summer of 2016-17 in New South Wales was made at least 50 times more likely by global heating, linking it directly to climate change. Hotter air can also carry more water vapour, meaning more intense rain and more flooding.  Another important factor in the northern hemisphere is the impact of changes in the Arctic. The polar region is heating more rapidly, reducing the temperature difference with lower latitudes. There is strong evidence that this is weakening the planetary waves (including the jet stream) that normally meander over Europe, Asia and North America. When these waves stall, weather gets fixed over regions and becomes extreme. This has been linked to past floods in Pakistan, heatwaves in Russia and drought in California.  Most of the planet’s trapped heat goes into the oceans and rising sea temperatures mean more energy for hurricanes and typhoons. Record-breaking cyclones hit Mozambique last year. The deluge delivered in the US by Hurricane Harvey in 2017 was made three times more likely by climate change. Rising sea level also means storms cause more coastal damage. Natural variability would cause some extreme weather, even without global heating, but our impacts on the climate make such extremes more likely. Carbon Brief analysed more than 230 studies and found 95% of heatwaves were made more likely or worse by climate change. For droughts, 65% were definitely affected by our hotter world, while the figure for floods was 57%. With the ‘rapidly accelerating’ likelihood of 40C temperatures in UK, it is now undeniable that global heating is causing more extreme weather. “One of our neighbours lost £70,000 in value,” says Ian Young, a father of two who lives with his partner on the Isle of Man and who was affected by flooding last year. “We lost both our cars and did not receive market value for them. Our premiums went through the roof as they had to go down as at-fault claims even though the vehicles were parked on the driveway and we weren’t in them.” He and his family have been living in temporary accommodation since the beginning of October and work to repair his property, the ground floor of which was completely flooded, has only just begun. “It was absolutely horrific,” he said. “You spend years trying to build up your life centred around your home and garden and then you’re kicked out. We’ve lost a Christmas with our kids. They only believe in Father Christmas until they’re eight or nine. But it was a write-off not having our own home.” In Surrey, where the Thames flooded twice in 2014, residents are still dealing with the consequences. “I don’t think we ever fully recovered from the experience,” said the fluid dynamicist and racing boat maker Carl Douglas. “It knocked the momentum out of us, left us poorer and messed up our business.” Receiving an adequate payout from his insurers proved an “incredibly stressful” ordeal that he knows was not unique to himself, and in 2015 he developed circulatory problems that forced him to have a triple bypass operation. “The surgeon looked at me and said: ‘You look pretty fit for your age so I wouldn’t have expected you to need this procedure. But have you been under a lot of stress?’ ‘Oh yes,’ I replied,” Douglas said. In many cases, communities have pulled together to help those who are most affected. In Hebden Bridge in West Yorkshire, which has been hit by flooding over the last couple of years, neighbours have responded by helping older people move their possessions to safety, sharing route information on Facebook, offering spare bedrooms and helping to flood-proof homes. But people such as Rory Deighton, a 49-year-old NHS worker, are looking towards Westminster for real action. “If we get nothing again from a central government that ignores climate change, does not act on grouse shooting or discuss farming practices, and does not focus on how we can prevent flooding, then people will just ask: what’s the point?”"
"In these times of rising activism on climate change and other environmental threats, a new band of campaigners has joined the fight: street artists. And these artists are using the landscape, communities and social media to spread their message. Banksy, probably the most famous street artist in the world, recently made his views clear through a new piece in Wales featuring a boy under what looks like snow, but is actually pollution from an industrial bin.  Banksy has always been overtly political and controversial, but this was a clear environmental message in an area which is home to one of the largest steelworks in Europe. The image was displayed in the news and on social media across the globe. Through his use of a child, Banksy’s piece echoes the work of another street artist, Ernest Zacharevic, who reached international fame in 2012 with his two street murals little children on a bike and kid on a motorcycle located in the city of George Town, Malaysia. Like Banksy, Zacharevic’s more recent work was also inspired by environmental pollution. In 2015, a haze of pollution made the air in Malaysia almost unbreathable. Zacharevic’s inquiries led him to learn that that the smoke in the region came all the way from Indonesia, caused by unauthorised slash-and-burn techniques used by smallholder farmers to clear the forest to make room for palm oil plantations. The Lithuania-born artist began to take an interest in the palm oil industry and researched the issue for two years by engaging with local people in Indonesia. At the same time, NGO campaigns in the UK and elsewhere in Europe were trying to alert consumers to how the palm oil industry was destroying the environment and abusing human rights. They also began to collaborate with UK investors to engage with the urgent sustainability consequences of deforestation, land conflict and labour conditions, and to advocate for a sustainable palm oil industry. Zacharevic then partnered with the London-based Sumatra Orangutan Society and Indonesia-based Orangutan Information Centre to form the “Splash and Burn” campaign. Its aim was to make people aware of the social and environmental tensions caused by the current practices in the palm oil industry. In 2017, he discreetly invited a team of international street artists to join him in Indonesia to produce haunting public art pieces in remote villages, natural landscapes and towns. Famous figures in the street art scene were eager to participate in what they saw as a much needed demonstration of grassroots art activism. Here, for instance, is noted urban sculpturer Mark Jenkins: Through different creative and sometimes improvised techniques, the street art collective created awareness of the damage caused by unregulated deforestation. Their action was relayed through their Instagram accounts, personal websites, and online press.  Here’s a mural Zacharevic created of an orangutan being chased by fire: In 2018, Splash and Burn took a turn towards “land art” when Zacharevic and his team drew a giant SOS sign on a 124-acre former palm oil plantation in north Sumatra, Indonesia. The land had just been acquired by the environmentally conscious cosmetics company Lush, which raised funds to replant an indigenous forest. The artists also shot a short movie to raise global awareness and to connect artists with civil society organisations.  Street artists are becoming more and more internationally and officially recognised for their environmental work. In 2018, Hawaiian-born artist Sean Yoro, who goes by the artist name Hula, made the Forbes “30 under 30” list for his murals, mostly of female faces being submerged in water. His works raise the question of rising sea levels due to climate change. Street art has typically focused on megacities and urban festivals. But a generation of digitally ultra-connected artists has been encouraged to engage with grassroots campaigners and spread their brushes and spray cans elsewhere – to forests and seas – and to creatively question our relationships to the natural world. Street artists have recently been criticised for “selling out” to big companies for taking on commissioned work, without showing any critical awareness of the social impact of these big companies. Yet these examples of climate activist street art shows artists can actually bring an alternative and responsible message to the public through their work."
"
Share this...FacebookTwitterMilutin Milankovitch
Milankovic Cycles and Climate Change
Is it distance from the sun, or length of summer?
By Ed Caryl
A draft paper by Dr. James Hansen and Dr. Makiko Sato triggered a rebuttal by Dr. Martin Hertzberg on WUWT. The Hansen paper made a claim that weaker insolation in the Northern Hemisphere (NH) due to distance from the sun in NH winter should lead to cooling, but that this is offset by increasing CO2.
The Dr. Hertzeberg rebuttal claimed that the warming was due to the longer length of summer in the NH. Both are wrong!  Both are victims of Confirmation Bias, seeing only data that confirms their beliefs.
It seemed a little strange that the Hansen paper was written for a conference in Milankovic’s honor, yet the paper does not contain the famous diagram illustrating the Milankovic Cycle (or Milankovitch, both spellings are common in the literature), nor does the Hertzberg rebuttal. Here it is:

Figure 1. Source: Wikipedia Commons
The important part of this chart, and the reason both authors are wrong, is the black plot in the lower center of the chart, (that part of the plot is shown again below) the insolation per day at °65 N latitude. If the scale of this plot were anomaly, instead of absolute value, the anomaly would be slightly negative. The insolation change due to the length of summer and the distance from the sun are almost exactly canceling, and the insolation is not going to change very much for the next 20,000 years.

Figure 2. The blue dot is the current date. Source: Wikipedia Commons


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The position in our orbit where we are closest to the sun is called the parahelion point. The tilt of the earth’s axis is related to the parahelion by a relationship known as the “longitude of the parahelion”. The °360-°0 point is where the Vernal Equinox (1st day of Spring) and the parahelion coincide. If you divide the °360 by 4 (=°90) you see that the parahelion will be in spring from °0 to °90, summer from °90 to °180, autumn from °180 to °270, winter from °270 to °360. Right now we are at °283 and the parahelion happens on the 3rd of January.

Figure 3. Plot of the Longitude of the Parahelion point. Plotted from NASA data. The year 2000 is marked by the blue square. You can see that in the year 12,000, 10,000 years from now, we will be closest to the sun in NH mid-summer.
Will this lead to warming in mid-summer? No, because the earth reaches a point of minimum axial tilt at that time, and rapidly decreasing orbital eccentricity (as eccentricity reaches minimum, the orbit approaches a perfect circle) will make the reduced distance from the sun in summer much less important. See the next chart. Again, the current date is the blue squares.

Figure 4. Plot of Obliquity (Earth axial tilt) and orbital eccentricity (deviation from a circle). Plotted from NASA data obtained here.
When will the next ice age be?
One of the comments to the Hertzberg article in WUWT asked, “When will the next ice age will begin?” Answer, not for a while yet. From Wikipedia: “An often-cited 1980 study by Imbrie and Imbrie determined that, ‘Ignoring anthropogenic and other possible sources of variation acting at frequencies higher than one cycle per 19,000 years, this model predicts that the long-term cooling trend which began some 6,000 years ago will continue for the next 23,000 years.’[16]”
All these orbital cycles are much too long and gradual to account for late 20th Century warming. That warming is due to short-term solar output changes, solar magnetic field influence on galactic cosmic rays changing cloud cover, and ocean cycles.
You can see in Figure 2 that the next negative swing in NH insolation is over 50 thousand years in the future, and the next really negative swings are over 100 thousand years off. Even if we cool for the next 23,000 years, it should not get cool enough to trigger an ice age… unless the sun does something really strange.
Share this...FacebookTwitter "
"

_Washington Post_ political reporter Colby Itkowitz writes:



During floor debate ahead of a vote on the Green New Deal, Sen. Mike Lee (R‐​Utah) told his colleagues that if they really want to address environmental concerns they’ll encourage people to couple off and have more babies. … This recommendation, to add more people to the planet, doesn’t track with science or reason. A 2017 research article determined that one way an individual could contribute to eliminating greenhouse gases is to have one fewer child.



That’s the nut from her snide web article, “Sen. Mike Lee Says We Can Solve Climate Change with More Babies. Science Says Otherwise.” _Post_ national correspondent James Hohmann deemed the article noteworthy enough to be the “Hot [Read] on the Left” in his “Daily 202″ newsletter.



Problem is, Itkowitz seems to not understand the point Lee was trying to make. Instead, she “talks past” him — something all‐​too‐​common in politics, but not something reporters should do. Worse, if history is a guide, her view is more likely to prove a‐​scientific and unreasonable than his.



Here’s the story: Last Tuesday, the Senate held a floor debate and vote on the so‐​called “Green New Deal” — which is to say the Senate engaged in silly Republican grandstanding over a silly Democratic proposal. As part of the debate, Lee delivered a floor speech that featured everything from a picture of a machine‐​gun‐​firing, bazooka‐​toting Ronald Reagan riding a velociraptor waving an American flag (no, really), to references to _Star Wars_ tauntauns and the Hanna‐​Barbera Aquaman’s giant seahorse, to _Sharknado 4_ and Austin Powers’ Dr. Evil. That is, Lee met double‐​silliness with more silliness.



One can argue that this was inappropriate for a Senate debate, especially on a serious topic like climate change. But then, comedy can be an effective means to truth.



Toward that end, at the conclusion of his speech Lee offers a serious point. Itkowitz selectively quotes it; here’s the complete section:



The Green New Deal is not the solution to climate change. It’s not even part of the solution. It’s part of the problem. The solution to climate change won’t be found in political posturing or virtue signaling like this. It won’t be found in the federal government at all.  
  
You know where the solution can be found? In churches, wedding chapels, and maternity wards across the country and around the world. This, Mr. [Senate] President, is the real solution to climate change: babies. Climate change is an engineering problem — not social engineering, but the real kind. It’s a challenge of creativity, ingenuity, and technological invention. And problems of human imagination are not solved by more laws, but by more humans.  
  
More people mean bigger markets for innovation. More babies mean more forward‐​looking adults — the sort we need to tackle long‐​term, large‐​scale problems. American babies, in particular, are likely going to be wealthier, better educated, and more conservation‐​minded than children raised in still‐​industrializing regions. As economist Tyler Cowen recently wrote on this very point, “by having more children, you are making your nation more populous — thus boosting its capacity to solve [climate change].“  
  
Finally, Mr. President, children are a mark of the kind of personal, communal, and societal optimism that is the true prerequisite for meeting national and global challenges together. The courage needed to solve climate change is nothing compared with the courage needed to start a family. The true heroes of this story aren’t politicians or social media activists. They are moms and dads, and the little boys and girls they are — at this moment — putting down for naps, helping with their homework, building tree houses, and teaching how to tie their shoes.  
  
The planet does not need us to “think globally and act locally” so much as it needs us to think family and act personally. The solution to climate change is not this unserious resolution, but the serious business of human flourishing — the solution to so many of our problems, at all times and in all places: fall in love, get married, and have some kids.



No doubt, the Utah senator’s comments were, at least in part, his own virtue‐​signaling to his predominantly Mormon constituency. But he draws on an important economic idea: at the margin, human beings have a positive effect on the world. Human ingenuity, hard work, preferences, and values create goods, and among those goods can be improved environmental quality. Julian Simon popularized this idea in his 1981 book _The Ultimate Resource_ , and it has been restated in recent years by, among others, Cowen in his new book _Stubborn Attachments_ (reviewed by Pierre Lemieux here), Bryan Caplan in his 2011 book _Selfish Reasons to Have More Kids_ , and my Cato colleague Marian Tupy and BYU economist Gale Pooley in their 2018 paper on the “Simon Abundance Index.”



Itkowitz claims “science and reason” say different. To justify that, she links to a 2017 paper that actually _doesn’t_ determine “that one way an individual could contribute to eliminating greenhouse gases is to have one fewer child,” but rather examines Canadian high school science textbooks’ recommendations for reducing greenhouse gas emissions. A possible strategy — one that _none_ of the textbooks recommend, which the paper’s authors lament — is to “have one fewer child.” That paper, in turn, cites a 2009 paper that estimates the carbon emissions resulting from an offspring, including a share of the subsequent emissions of that offspring’s descendants. Not surprisingly, that’s a big number, dwarfing the carbon‐​reduction benefits of such common strategies as recycling, switching from gasoline‐​powered cars to hybrids and electric cars, and upgrading lightbulbs. (My takeaway from the 2017 paper is how pointless many of the commonly advocated carbon‐​reduction strategies are.)



The 2009 paper is a mathematical modeling exercise under various assumptions, resulting in different estimated marginal “carbon legacies.” But that doesn’t show Itkowitz is right and Lee’s being foolish because the paper ignores Lee’s point about the effects of population change on innovation and living standards.



From tin shortages in the ancient world, to William Strong Newberry’s 1875 (yes, **18** 75) warning that the world was running out of oil, to Paul Ehrlich’s _Population Bomb_, to Jimmy Carter’s moral equivalent of war, population growth has placed humanity on the Malthusian edge of poverty and privation — or so we keep being told. But we never fall off that edge. In fact, we keep moving away from it: we grow fatter (alas), longer‐​lived, and more comfortable. The reason is simple: more people means more innovation and resource availability, which means a higher standard of living rather than the opposite.



That doesn’t mean humanity is guaranteed to find some easy, innovative way to cut greenhouse gas emissions. As my Cato colleague Peter Van Doren noted, both Lee’s optimism and Itkowitz’s pessimism are attitudes (probably correlated to their politics) rather than testable, scientific hypotheses. That said, history suggests it’s more likely that humanity will find innovative ways to cut emissions, or geo‐​engineer around climate change, or accommodate change, than that reducing (or government constraining) population growth will save us from a much warmer world — or that there will be no future environmental quality innovations.



All that said, some legitimate criticisms can be made of Lee’s remarks. Government _can_ be a useful tool for addressing externalities, just as it can also be a terrible tool. And there are plenty more examples of people showing the “courage” to start families than there are of policymakers showing the courage to address difficult policy problems (e.g., entitlements insolvency, government debt, pork and rent‐​seeking, better childhood education…) Those criticisms would have made good reading, as would a broader discussion of Malthus and neo‐​Malthusianism, government intervention, and population change. Unfortunately, instead of that, readers got was the equivalent of the aforementioned Reagan–velociraptor picture.
"
"
Share this...FacebookTwitterAwhile back I did a story on Anders Levermann of the über-alarmist Potsdam Institute For Climate Impact Research (PIK), and reader Arnd B brought my attention to an article called: Our systems are especially vulnerable, which appeared in the online Frankfurter Allgemeine Zeitung (FAZ) in late December.

Hieronymous Bosch paints a scene of a Renaissance mountebank fleecing incredulous gamblers.
Levermann makes a number of interesting comments that provide insight on how the PIK views climate. Unfortunately, all his predictions are based on models, and ignore real-life observations.
Global warming could enhance cold weather
Levermann starts off saying the bitter cold and snow in Germany last month is a sure sign of “how out-of-whack the climate system is.” Levermann serves up the “science” that supports it:
The current cold weather in Europe is everything but evidence against climate change, rather it could even be enhanced by global warming. Colleagues have discovered the mechanism for this: Through the ice melt in the Kara Sea, high pressure zones can form, which then divert the Eurasian winds and lead to cold temperatures in Europe.”
It takes a real climate scientist to make such a profound discovery, and that with no data to back it up. Not only that, Levermann adds:
The more and faster we emit greenhouse gases, the more our climate gets knocked out of whack.”
At this point, I have to ask myself: “Just how gullible must the average FAZ reader be to take this seriously?
Extreme weather events prove manmade climate change
And as usual Levermann goes down the laundry list of last year’s weather events…floods in Pakistan, heat wave in Russia, mudslides in Brazil, etc., etc. and claims this is evidence supporting the man-made global warming link, and that it had all been predicted by models. Yet, Levermann forgets to mention that the accumulated cyclone energy (ACE) was near a record low last year, and that temperatures have not risen over the last decade – something his models have missed. Still he insists:
It’s now practically sure that in a rapidly warming world we have to expect more and stronger extreme events.”
PIK models can now see to the year 2200
Keep in mind that Anders Levermann is a lead author for the next IPCC Report on the subject of sea levels. The next report will deliver the latest “projections” based on various CO2 output scenarios. So where does Levermann say the globe is headed?
What we can already say, based on our latest studies, is: We currently find ourselves on the warmest possible future trajectory […] The temperature projection shows a warming of more than eight degrees Celsius in the year 2200.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Unfortunately for Levermann, there hasn’t been any warming in a decade, as the following HadCrut chart shows:
HadCrut shows cooling over the last decade. (WoodFor Trees)
Remember that he is a lead author on sea ice for the upcoming IPCC 5th assessment report. What kind of sea level projection do you think he’ll concoct with 8°C of warming? Expect the 5th assessment report to be worse in scientific quality than the 4th report of 2007. Sci-fi sequels tend to get worse and worse as they are taken over by B-rated directors.
Anders Levermann (Photo: PIK)
Ignoring real world observation and data, Levermann stares deeper into his crystal ball and sees only horrors. He wonders if man will be able to adapt to these rapidly changing climate conditions (the ones in his crystal ball). What are the limits of human adaptability, he asks? 4°C? 6°C?
Warming 50 times faster than the warming that ended the ice age
Claiming that his crystal ball sees 8°C of warming over the next 190 years, he says that this 4°C rise per century will be unprecedented. Levermann says the difference between an optimum and an ice age is about 5°C, and that it took 5000 years for the earth to make the 5°C climb out of the last ice age:
The transition from ice age to warm period lasted a good five thousand years. When man continues to emit greenhouse gases unabated, then we will reach the same warming 50 times faster than in the past.”
That’s assuming his models are correct. Looking at the above HadCrut chart, there has been no warming. And Levermann doesn’t mention that most of the temperature rise ending the last ice age took place in about 1000 years, and the temperature difference was more than 5°C. It was closer to 8°C. So he’s fudging there quite a bit.
He also ignores the huge temperature swings of up to 6°C which occurred in just a matter of a few decades during the Younger Dryas – all naturally.
Of course, Levermann doesn’t forget to play emotion-card Africa, and predicts dire scenarios for the poor continent.
It is probable that in such a situation, countries like Bangladesh and parts of Africa will have become uninhabitable. Whether the drinking water supply collapses because of drought, or sea water claiming the land, or because agriculture becoming impossible. Even without the extreme events, the United Nations estimates that the number of climate refugees will reach 90 million if the sea level rises 1 meter.
Here he ignores studies that show the African Sahara is shrinking and getting more desperately needed rainfall during this modern warm period. And he ignores that sea levels have decelerated over the last 5 years. And even if the sea level did rise 1 meter, something that only the most fanatic among us predict, it would not happen overnight. Most humans just don’t have the habit of standing still for 100 years and watching the water rise around them.
Finally, Levermann ends by saying:
The wall that we are racing towards is hidden in fog, but it is there!
At PIK it’s: If you can’t see it, then it’s proof it’s there. Just believe us.
Share this...FacebookTwitter "
"
Guest Post by Steven Goddard

Over the last year or so I have been taking an informal survey of a key news metric – Google news searches for the term “global warming.”  A year ago, the ratio of alarmist/skeptical articles was close to 100/1.  About six months ago, the ratio was 90/10, Two months ago it was 80/20, and today it hit 50/50 for the first time – including the lead skeptical story “A Cooling Trend Toward Global Warming“.  One thing that has changed is the rise of blogs written by informed citizens, complemented by the demise of corporate newspapers which make money from keeping people continually alarmed about one thing or another.
Congratulations to Anthony and all the readers for being a big part of this.  Democracy in it’s purest form – hope and change we can all believe in.
The top two items from Google news “global warming” search today.  The distribution of all stories through the first few search pages was similar in makeup as seen below:


The Tech Herald
A Cooling Trend Toward Global Warming
The New American – ‎1 hour ago‎
With the election of a president who is solidly in the global–warming-alarmist camp – and with many high-level appointees who are bona fide climate-change …
Global warming and climate change: facts and hype Examiner.com
UN global warming stand criticized Delta Farm Press
UN Con on Global Warming Nearly Foiled NewsMax.com
Opposing Views – Atlanta Journal Constitution
all 36 news articles »

New York Times
House Democrats release draft energy, climate bill
New York Times – ‎8 hours ago‎
By DARREN SAMUELSOHN AND BEN GEMAN, Greenwire Democratic leaders of the House Energy and Commerce Committee today unveiled a 648-page draft global warming …
House Democrats unveil sweeping plan to reshape energy in America MiamiHerald.com
Waxman’s clean energy draft includes cap-and-trade proposals Oil & Gas Journal
US lawmakers present draft bill on ‘clean energy’ AFP
iBerkshires.com


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96f29872',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Researchers have long known that man-made climate change will harm yields of important crops, possibly causing problems for the world’s food security. But new research shows air pollution doesn’t just harm crops indirectly through climate change; it seems to harm them directly. Pollution from soot and ozone has caused a major decrease of crop yields in India, with some densely populated states experiencing 50% relative yield losses. To ensure the world has enough food, we need to look directly at air pollution. Jennifer Burney and Veerabhadran Ramanathan from the University of California, San Diego systematically investigated the impact of air pollution and anthropogenic climate change on crops in India, where yields have levelled off or decreased in recent decades despite continued improvements in agricultural technology. Their study showed that overall air pollution has caused a third of loss in wheat yield and one fifth of loss in rice yield in India in 2010, using 1980 as a baseline. Their findings are published in the journal Proceedings of National Academy of Sciences. Many previous works have studied the impact of climate change on crop yield. However, this new study suggests that air pollution from ozone and soot caused far more loss of crop yield than climate change. From 1980 to 2010, the increase in temperature and change in precipitation as a result of anthropogenic climate change has caused a 3.5% decrease in wheat yield on a country level in India. However, air pollution has caused more than 32% decrease in wheat yield during the same period. Blame pollution not climate for declining wheat yields: Soot – or black carbon – is emitted mainly from burning plants and fossil fuels. It directly absorbs sunlight, reducing the amount of light available for crops to photosynthesise. Black carbon alone has caused more damage to Indian wheat yields than climate change. Ozone is a gas formed in the atmosphere through chemical reactions of precursors including nitrogen oxides (NOx) and volatile organic compounds (VOCs) in the presence of sunlight. NOx are mainly generated from fossil fuel combustion while VOCs are emitted from both natural sources and human activities.  Ozone damages crops by entering leaves during normal gas exchange. As a strong oxidant, ozone causes symptoms in crops such as yellowing, cell injury, tiny light-tan irregular spots, bronzing, and reddening. This directly affects the growth of crops and thus reduces their yield. Ozone is the key pollutant causing the yield loss of crops, for example wheat, which is very sensitive to ozone exposure. Ozone exposure could have an even bigger impact on yields of soybean, peanut and cotton. The picture is unlikely to improve any time soon. My colleague William Bloss, an expert in atmospheric chemistry, points out that background ozone levels have approximately doubled since the earliest measurements (performed near Paris in the 1870s). “Looking to the future”, he says, “models predict that ground level ozone will continue to rise in many areas of the world”. Ozone pollution will continue to be a major challenge for food security. It is important to note that there are significant regional variations in the crop yield loss in India, with some states seeing more than 50% losses in wheat yield, mainly due to air pollution. This has significant implications for other developing countries, in particular China.  Since the emission of soot and ozone precursors is significantly larger in China than in India, the impact of air pollution on Chinese agriculture is expected to be even larger than that in India. China is now the world’s largest food importer. Could air pollution in China have led to the thirst for food in the global market? Fortunately ozone and black carbon have short atmospheric lifetimes (unlike some greenhouse gases which can linger for decades or centuries). This means there is a strong, direct benefit to addressing such pollution, and it would be apparently relatively soon."
"In the UK, there are approximately nine million dogs and almost eight million cats – with around one in two households owning a companion animal. This large pet population is estimated to consume billions of tonnes of meat each year. So, given that more people are trying to do their bit to help save the plant and keep meat consumption to a minimum, it’s not surprising pet food has become the latest sector to think about its environmental credentials. Pet food trends tend to lag human dietary trends by around 12-18 months. And there are now many opportunities for dogs or cats to eat a vegetarian, vegan, gluten-free, low-allergenic or super-food diet. Then there is also a big market in “raw foods”, which have becoming increasingly popular. These are comprised solely of premium human-grade meats, raw fruits and vegetables – and are unrefined and minimally processed. The latest addition to the line-up is a high-protein pet food that meets the requirements for low environmental impact, as it is made from insects. Yora pet foods describes its “Green Insect Pet food” as: Made from 100% insect protein, grain-free, low allergenic potential, sustainable and an environmentally friendly delicacy for the modern canine household.  The domestication of canines has allowed their systems to adapt to be better at digesting starch –- found in grains, beans and potatoes –- than their wolf ancestors. This adaptation probably allowed the domestic dog to flourish on human grains and cereals. Their gut microbiome has also adapted to be better at breaking down carbohydrates and to some degree is able to produce amino acids normally sourced from meat. Dogs are true omnivores – they can survive on both plants and animals – unlike their carnivorous ancestors.  


      Read more:
      Vegan dogs: should canines go meat free?


 Our domesticated feline friends on the other hand, remain obligate carnivores – much like their larger and wilder ancestors. Cats still require many essential nutrients that can only be obtained from eating meat.  Responsible manufacturers of pet foods describe their products as “complete” if they meet all nutrient requirements for a dog or cat, according to established guidelines. Ideally, they register with the Pet Food Manufacturers Association to guarantee their product is in accord with certain standards.  Wet pet foods – such as tins, pouches, trays – are fed to approximately 41% of dogs and 77% of cats in the UK. Each is more often than not labelled as being a “meaty flavour” – such as beef, lamb, poultry, duck. The actual amount of meat in the feed varies according to the stated claim on the label – anything from 4% to 60% is common. These foods contrast steeply with the market-leading dry “kibbles” that are ultra-processed and ultra-refined – and account for 85% of all pet food sold.  The environmental impact of pet food in the US alone is estimated to be around 60m tonnes of CO₂-equivalent methane and nitrous oxide production per year – which is a huge amount. So could insect-based pet food be the answer? A first in the UK, Yora now offers a product with sufficient protein to satisfy our favourite companion animals and one that also has eco-credibility. Other manufacturers have also entered the fray with a few insect-based pet foods available online. On its website, Yora claims that the resources needed to produce just 10kg of protein from beef are 2,100 m² of land – which generates 1,500 kg of greenhouse emissions and uses 1,120,000 litres of water. Considering equivalent values to produce 60kg of insects used in its products are 45m² of land and 54,000 litres, then it’s clear that Yora could be on to something. But independent studies into such products are now needed to really conclude if the nutritional impact weighs up.  Of course, it isn’t just pet food that comes under question for it’s environmental credentials. With an expanding global population, nutritional scientists have, for many years, been considering how to produce sufficient quality protein from more efficient sources.  At the University of Nottingham, for example, academics are working on a range of projects evaluating the use of insects as both human food and animal feed. One of the major challenges though is what to feed the insects – plant and animal waste have been considered. It would defeat the object if they were fed foodstuffs more usually consumed by humans or farm animals – and kept in heated greenhouses.      Of course, cynics might say the answer is to reduce pet ownership altogether. But it’s important to not forget the positive impact that pets can have on people’s lives. Dog ownership increases activity levels and social interactions and lowers risk of premature death . Having a family pet also reduces the chances of a child in that house becoming asthmatic – by exposing their immature immune system to novel antigens at an early age. Research shows that owning a cat can also make you happier. Our feline friends can help to reduce stress levels – along with our blood pressure – and help to make us feel less lonely.   So despite the environmental impact of what they eat, the fact remains that pets are good for us. Perhaps now with increased choices on pet food, owners can make more informed, ethical decisions. And the industry could also help by labelling foods with an indication of how environmentally friendly a product is."
"
From NOAA News, Susan Solomon predicts the future with certainty. In other news, on the same day Caterpillar, Sprint, Texas Instruments, and Home Depot announce massive layoff plans to the tune of 50,000 people,  unemployed climate modelers get a government bailout today courtesy of our new president to the tune of 140 million dollars. That should be just enough to pay the electric power bill for the new supercomputer I’m sure NOAA will just “have to have” now to keep up with the new toy for the Brits at Hadley. (h/t to Ed Scott for the NOAA pr)
New Study Shows Climate Change Largely Irreversible
January 26, 2009
A new scientific study led by the National Oceanic and Atmospheric Administration reaches a powerful conclusion about the climate change caused by future increases of carbon dioxide:  to a large extent, there’s no going back.
The pioneering study, led by NOAA senior scientist Susan Solomon, shows how changes in surface temperature, rainfall, and sea level are largely irreversible for more than 1,000 years after carbon dioxide (CO2) emissions are completely stopped. The findings appear during the week of January 26 in the Proceedings of the National Academy of Sciences.
“Our study convinced us that current choices regarding carbon dioxide emissions will have legacies that will irreversibly change the planet,” said Solomon, who is based at NOAA’s  Earth System Research Laboratory in Boulder, Colo.
“It has long been known that some of the carbon dioxide emitted by human activities stays in the atmosphere for thousands of years,” Solomon said. “But the new study advances the understanding of how this affects the climate system.”
The study examines the consequences of  allowing CO2 to build up to several different peak levels beyond present-day concentrations of 385 parts per million and then completely halting the emissions after the peak. The authors found that the scientific evidence is strong enough to quantify some irreversible climate impacts, including rainfall changes in certain key regions, and global sea level rise. 
If CO2 is allowed to peak at 450-600 parts per million, the results would include persistent decreases in dry-season rainfall that are comparable to the 1930s North American Dust Bowl in zones including southern Europe, northern Africa, southwestern North America, southern Africa and western Australia.
The study notes that decreases in rainfall that last not just for a few decades but over centuries are expected to have a range of impacts that differ by region. Such regional impacts include decreasing human water supplies, increased fire frequency, ecosystem change and expanded deserts. Dry-season wheat and maize agriculture in regions of rain-fed farming, such as Africa, would also be affected.
Climate impacts were less severe at lower peak levels. But at all levels added carbon dioxide and its climate effects linger because of the ocean.
“In the long run, both carbon dioxide loss and heat transfer depend on the same physics of deep-ocean mixing. The two work against each other to keep temperatures almost constant for more than a thousand years, and that makes carbon dioxide unique among the major climate gases,” said Solomon.
The scientists emphasize that  increases in CO2 that occur in this century “lock in” sea level rise that would slowly follow in the next 1,000 years. Considering just the expansion of warming ocean waters—without melting glaciers and polar ice sheets—the authors find that the irreversible global average sea level rise by the year 3000 would be at least 1.3–3.2 feet (0.4–1.0 meter) if CO2 peaks at 600 parts per million, and double that amount  if CO2 peaks at 1,000 parts per million.
“Additional contributions to sea level rise from the melting of glaciers and polar ice sheets are too uncertain to quantify in the same way,” said Solomon. “They could be even larger but we just don’t have the same level of knowledge about those terms. We presented the minimum sea level rise that we can expect from well-understood physics, and we were surprised that it was so large.”
Rising sea levels would cause “…irreversible commitments to future changes in the geography of the Earth, since many coastal and island features would ultimately become submerged,” the authors write.
Geoengineering to remove carbon dioxide from the atmosphere was not considered in the study. “Ideas about taking the carbon dioxide away after the world puts it in have been proposed, but right now those are very speculative,” said Solomon.
The authors relied on measurements as well as many different models to support the understanding of their results. They focused on drying of particular regions and on thermal expansion of the ocean because observations suggest that humans are contributing to changes that have already been measured.
Besides Solomon, the study’s authors are Gian-Kasper Plattner and Reto Knutti of ETH Zurich, Switzerland, and Pierre Friedlingstein of Institut Pierre Simon Laplace, Gif-Sur-Yvette, France.
NOAA understands and predicts changes in the Earth’s environment, from the depths of the ocean to the surface of the sun, and conserves and manages our coastal and marine resources.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e994d102c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
It has been an interesting couple of days. Today yet another scientist has come forward with a press release saying that not only did their audit of IPCC forecasting procedures and found that they “violated 72 scientific principles of forecasting”, but that “The models were not intended as forecasting models and they have not been validated for that purpose.” This organization should know, they certify forecasters for many disciplines and in conjunction with John Hopkins University if Washington, DC, offer a Certificate of Forecasting Practice. The story below originally appeared in the blog of Australian Dr. Jennifer Marohasy. It is reprinted below, with with some pictures and links added for WUWT readers. – Anthony
 
J. Scott Armstrong, founder of the International Journal of Forecasting
Guest post by Jennifer Marohasy
YESTERDAY, a former chief at NASA, Dr John S. Theon, slammed the computer models used to determine future climate claiming they are not scientific in part because the modellers have “resisted making their work transparent so that it can be replicated independently by other scientists”. [1]
Today, a founder of the International Journal of Forecasting, Journal of Forecasting, International Institute of Forecasters, and International Symposium on Forecasting, and the author of Long-range Forecasting (1978, 1985), the Principles of Forecasting Handbook, and over 70 papers on forecasting, Dr J. Scott Armstrong, tabled a statement declaring that the forecasting process used by the Intergovernmental Panel on Climate Change (IPCC) lacks a scientific basis. [2]
What these two authorities, Drs Theon and Armstrong, are independently and explicitly stating is that the computer models underpinning the work of many scientific institutions concerned with global warming, including Australia’s CSIRO, are fundamentally flawed.
In today’s statement, made with economist Kesten Green, Dr Armstrong provides the following eight reasons as to why the current IPCC computer models lack a scientific basis:
1. No scientific forecasts of the changes in the Earth’s climate. 
Currently, the only forecasts are those based on the opinions of some scientists. Computer modeling was used to create scenarios (i.e., stories) to represent the scientists’ opinions about what might happen. The models were not intended as forecasting models (Trenberth 2007) and they have not been validated for that purpose. Since the publication of our paper, no one has provided evidence to refute our claim that there are no scientific forecasts to support global warming.
We conducted an audit of the procedures described in the IPCC report and found that they clearly violated 72 scientific principles of forecasting (Green and Armstrong 2008). (No justification was provided for any of these violations.) For important forecasts, we can see no reason why any principle should be violated. We draw analogies to flying an aircraft or building a bridge or performing heart surgery—given the potential cost of errors, it is not permissible to violate principles.
2. Improper peer review process. 
To our knowledge, papers claiming to forecast global warming have not been subject to peer review by experts in scientific forecasting.
3. Complexity and uncertainty of climate render expert opinions invalid for forecasting. 
Expert opinions are an inappropriate forecasting method in situations that involve high complexity and high uncertainty. This conclusion is based on over eight decades of research. Armstrong (1978) provided a review of the evidence and this was supported by Tetlock’s (2005) study that involved 82,361 forecasts by 284 experts over two decades.
Long-term climate changes are highly complex due to the many factors that affect climate and to their interactions. Uncertainty about long-term climate changes is high due to a lack of good knowledge about such things as:
a) causes of climate change,
b) direction, lag time, and effect size of causal factors related to climate change,
c) effects of changing temperatures, and
d) costs and benefits of alternative actions to deal with climate changes (e.g., CO2 markets).
Given these conditions, expert opinions are not appropriate for long-term climate predictions.
4. Forecasts are needed for the effects of climate change. 
Even if it were possible to forecast climate changes, it would still be necessary to forecast the effects of climate changes. In other words, in what ways might the effects be beneficial or harmful? Here again, we have been unable to find any scientific forecasts—as opposed to speculation—despite our appeals for such studies.
We addressed this issue with respect to studies involving the possible classification of polar bears as threatened or endangered (Armstrong, Green, and Soon 2008). In our audits of two key papers to support the polar bear listing, 41 principles were clearly violated by the authors of one paper and 61 by the authors of the other. It is not proper from a scientific or from a practical viewpoint to violate any principles. Again, there was no sign that the forecasters realized that they were making mistakes.
5. Forecasts are needed of the costs and benefits of alternative actions that might be taken to combat climate change. 
Assuming that climate change could be accurately forecast, it would be necessary to forecast the costs and benefits of actions taken to reduce harmful effects, and to compare the net benefit with other feasible policies including taking no action. Here again we have been unable to find any scientific forecasts despite our appeals for such studies.
6.  To justify using a climate forecasting model, one would need to test it against a relevant naïve model. 
We used the Forecasting Method Selection Tree to help determine which method is most appropriate for forecasting long-term climate change. A copy of the Tree is attached as Appendix 1. It is drawn from comparative empirical studies from all areas of forecasting. It suggests that extrapolation is appropriate, and we chose a naïve (no change) model as an appropriate benchmark. A forecasting model should not be used unless it can be shown to provide forecasts that are more accurate than those from this naïve model, as it would otherwise increase error. In Green, Armstrong and Soon (2008), we show that the mean absolute error of 108 naïve forecasts for 50 years in the future was 0.24°C.
7. The climate system is stable. 
To assess stability, we examined the errors from naïve forecasts for up to 100 years into the future. Using the U.K. Met Office Hadley Centre’s data, we started with 1850 and used that year’s average temperature as our forecast for the next 100 years. We then calculated the errors for each forecast horizon from 1 to 100. We repeated the process using the average temperature in 1851 as our naïve forecast for the next 100 years, and so on. This “successive updating” continued until year 2006, when we forecasted a single year ahead. This provided 157 one-year-ahead forecasts, 156 two-year-ahead and so on to 58 100-year-ahead forecasts.
We then examined how many forecasts were further than 0.5°C from the observed value. Fewer than 13% of forecasts of up to 65-years-ahead had absolute errors larger than 0.5°C. For longer horizons, fewer than 33% had absolute errors larger than 0.5°C. Given the remarkable stability of global mean temperature, it is unlikely that there would be any practical benefits from a forecasting method that provided more accurate forecasts.
8.  Be conservative and avoid the precautionary principle. 
One of the primary scientific principles in forecasting is to be conservative in the darkness of uncertainty. This principle also argues for the use of the naive no-change extrapolation. Some have argued for the precautionary principle as a way to be conservative. It is a political, not a scientific principle. As we explain in our essay in Appendix 2, it is actually an anti-scientific principle in that it attempts to make decisions without using rational analyses. Instead, cost/benefit analyses are appropriate given the available evidence which suggests that temperature is just as likely to go up as down. However, these analyses should be supported by scientific forecasts.
The reach of these models is extraordinary, for example, the CSIRO models are currently being used in Australia to determine water allocations for farmers and to justify the need for an Emissions Trading Scheme (ETS) – the most far-reaching of possible economic interventions.   Yet, according to Dr Armstrong, these same models violate 72 scientific principles.
********************
1. Marc Morano, James Hansen’s Former NASA Supervisor Declares Himself a Skeptic, January 27,2009. http://epw.senate.gov/public/index.cfm?FuseAction=Minority.Blogs&ContentRecord_id=1a5e6e32-802a-23ad-40ed-ecd53cd3d320
2. “Analysis of the U.S. Environmental Protection Agency’s Advanced Notice of Proposed Rulemaking for Greenhouse Gases”, Drs. J. Scott Armstrong and Kesten C. Green a statement prepared for US Senator Inhofe for an analysis of the US EPA’s proposed policies for greenhouse gases.  http://theclimatebet.com

Sponsored IT training links:
Get guaranteed success in 312-50 exam in first try using incredible 642-374 dumps and other 310-200 training resources prepared by experts.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e98f2bcc7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
\---   
  
Although it’s a favorite headline as people shiver during the coldest parts of the winter, global warming is almost assuredly _not_ behind your suffering (the “warming” part of global warming should have clued you in on this).   
  
But, some folks steadfastly prefer the point of view that all bad weather is caused by climate change.   
  
Consider White House Office of Science and Technology Policy (OSTP) head John Holdren. During the depth of the January 2014 cold outbreak (and the height of the misery) that made “polar vortex” a household name, OSTP released a video featuring Holdren telling us that “the kind of extreme cold being experienced by much of the United States as we speak, is a pattern that we can expect to see with increasing frequency as global warming continues.”   
  
At the time we said “not so fast,” pointing out that there were as many (if not more) findings in the scientific literature that suggested that either a) no relationship exists between global warming and the weather patterns giving rise to mid-latitude cold outbreaks, or b) the opposite is the case (global warming should lead to fewer and milder cold air outbreaks).   
  
The Competitive Enterprise Institute even went as far as to request a formal correction from the White House. The White House responded by saying that the video represented only Holdren’s “personal opinion” and thus no correction was necessary. CEI filed a FOIA request, and after some hemming and hawing, the White House OSTP finally, after a half-hearted search, produced some documents. Unhappy with this outcome, CEI challenged the effort and just this past Monday, a federal court, questioning whether the OSTP acted in “good faith,” granted CEI’s request for discovery.   
  
In the meantime, the scientific literature on this issue continues to accumulate. When a study finds a link between human-caused global warming and winter misery, it makes headlines somewhere. When it doesn’t, that somewhere is usually reduced to here.   




Case in point: Last week, _Washington Post_ ’s Capital Weather Gang published a piece by Jason Samenow that highlighted a pair of new findings that suggested that global warming was leading to more blizzards along the East Coast. The mechanism, favored by the global-warming-is-making-cold/blizzards-worse crowd is that Arctic warming, enhanced by melting sea ice there, is causing the curves (i.e., ridges and troughs) in the jet stream to become bigger, and thus slower. This “locks in” a particular weather pattern and can allow cold air to drop further southward as well as set up condition necessary for big snow storms. To us, this seemed more a case of natural variability than global warming, but we suppose beauty is in the eye of the beholder.   
  
But what you haven’t read in the _Washington Post_ (or anywhere else for that matter), is that an even newer paper has just been published by scientists (including Martin Hoerling) at NOAA’s Earth System Research Laboratory that basically demonstrates that global warming and Arctic sea ice loss should, according to climate models, lead to warmer winter temperatures, less temperature variability, and milder cold air outbreaks. This is basically the opposite conclusion from the one preferred and disseminated by Holdren et al.   
  
From the paper’s abstract:   




The emergence of rapid Arctic warming in recent decades has coincided with unusually cold winters over Northern Hemisphere continents. It has been speculated that this “Warm Arctic, Cold Continents” trend pattern is due to sea ice loss. Here we use multiple models to examine whether such a pattern is indeed forced by sea ice loss specifically, and by anthropogenic forcing in general. While we show much of Arctic amplification in surface warming to result from sea ice loss, we find that neither sea ice loss nor anthropogenic forcing overall to yield trends toward colder continental temperatures. An alternate explanation of the cooling is that it represents a strong articulation of internal atmospheric variability, evidence for which is derived from model data, and physical considerations. Sea ice loss impact on weather variability over the high latitude continents is found, however, characterized by reduced daily temperature variability and fewer cold extremes.



They were even more direct in paper’s conclusion:   




We…showed that sea ice loss impact on daily weather variability over the high latitude continents consists of reduced daily temperature variability and fewer cold extremes indicating that the enhanced occurrences of cold spells during recent winters (e.g., Cohen et al. 2014) are not caused by sea ice loss.



This is pretty emphatic. Global warming results in warmer, less variable winters in North America (Figure 1).   






  
  
_Figure 1. Modeled change in winter mean temperature (left), daily temperature variability (middle), and temperature on the coldest 10 percent of the days (right) as a result of decline in Arctic sea ice. (source: Sun et al., 2016)._   
  
Now, if only our government’s “top scientist” were paying attention.   
  
**Reference:**   
  
Sun, L., J. Perlwitz, and M. Hoerling, 2016. What Caused the Recent “Warm Arctic, Cold Continents” Trend Pattern in Winter Temperatures? _Geophysical Research Letters_ , doi: 10.1002/2016GL069024.


"
"
One of the claims about “global climate change” is that it will affect the normal ranges of flora and fauna of our planet. Well, with a very cold northern hemisphere this winter, that seems to happening. A bird not seen (as a mature adult) in Massachusetts since the 1800’s , an Ivory Gull, normally an inhabitant of arctic areas, has been spotted. Here are the details from the Plymouth, MA Patriot-Ledger. – Anthony

GULL-LOVER’S TRAVELS: Birdwatchers flock to Plymouth to spot rare specimen

PLYMOUTH — Jan 28th, 2009
The temperatures were in the single digits, but not low enough to keep the gawkers away. A celebrity was in town, behind the East Bay Grille, a visitor not seen in these parts in decades, if not longer.
But these weren’t paparazzi, and this wasn’t a Hollywood star. Rather, they were avid birdwatchers – about 20 in all – braving the frigid air as they scanned the bay and the edges of the breakwater with binoculars and spotting scopes.
And they would be rewarded, catching a glimpse of a glimpse of a rare, fully mature ivory gull. A birdwatcher reported seeing one in Plymouth last week, and another was spotted at Eastern Point Lighthouse in Gloucester. From Sunday through Tuesday, the avian visitor was a regular in Plymouth, much to the delight of birdwatchers, who came from near and far in hopes of adding the extremely rare bird to their life list.
Ivory gulls normally stay well above Newfoundland, living on Arctic ice where they follow whales and polar bears to feed on the scraps and carcasses they leave behind after making a kill.

Until this year, the last report of a fully mature ivory gull in Massachusetts was in the 1800s. Three immature birds were seen in the 1940s. In 1976, another immature bird had been spotted in Rockport.
Russell Graham of Dallas is flying in Friday for a three-day visit. He’s hoping the gull will still be in town when he arrives.
“The ivory gull is one of a handful of birds that every birder dreams of seeing but almost no one has.,” he said. “This isn’t a dream that’s confined to North America. There is also an immature bird in France that is causing the same reaction there. There are a couple of places where you can go in the summer and expect to see one but they are distant and expensive – Svalbard on Spitsbergen, Norway and Pond Inlet on Baffin Island, Canada.
“I never thought I would have the chance to see one and I can’t pass up this once-in-a-lifetime opportunity.”
If the gull is gone, Graham will consider a side trip to Nova Scotia, where two adult ivory gulls have been seen recently. “I’ll be keeping my fingers crossed,” he said.
John Fox of Arlington, Va., and his friend Adam D’Onofrio of Petersburg drove more than eight hours on Sunday to see the gull.
“No bird this morning,” Fox said a day later, shaking his head. “We left Virginia at three in the morning yesterday and arrived here 20 minutes too late.”
On Sunday morning, hundreds of people got to observe and photograph the gull as it fed on a chicken carcass someone put out on one of the docks in the parking lot. The bird stayed until 11 a.m., then flew across the harbor. It was not seen again for the rest of the day.
“We arrived at 11:20 and spent the rest of the afternoon in the parking lot, hoping it would return,” Fox said.
They stayed at Pilgrim Sands Motel and arrived at the parking lot early Monday morning for one more chance to see the ivory gull before returning to Virginia. Fox said it was his first time in Massachusetts. If he didn’t see the bird, he said, at least he could see Plymouth Rock before they left for home.
“That’s how it goes sometimes,” he said. “We don’t always see what we come for, but it’s nice to see some of the sights when you travel to a new area in hopes of seeing a rare bird.”
As Fox was planning his exit, a commotion caught his attention. One of the birders pointed toward the sky and said with a shout, “There it is.”
The pure white gull was flying toward the parking lot, silhouetted against a bright blue sky. Someone in the crowd announced for the record the gull had arrived at 7:45 a.m.
The bird flew in circles overhead, then landed on a snow bank in the middle of the parking lot. Cameras clicked and the birders “oohed and ahhhed” each time the ivory gull switched positions.
“Look how white it is,” someone said. “It’s got black feet, black eyes and a grayish-black beak,” said another.
The gull eyeballed the chicken carcass, still there from the day before, but it didn’t eat. Instead, it flew to the railing along the edge of the boat ramp and perched with a group of sea gulls. The photographers followed, changing positions to get the best lighting.
Fox stood with the group, talking with other birdwatchers, as the gull sat peacefully on the railing, observing all the people gathered around it. Was it worth the long drive up from Virginia?
“It sure was,” Fox said with a smile.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e98ddacfb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterWhy do I get the feeling this Accenture/Barclay’s report is going to have a serious backlash? Good if it does! The warmists outlets are already out there doing pre-emptive damage control, going into denial and wishful thinking.
A study carried out by consultants Accenture and Barclays Bank confirms that “climate protection” is going to cost a bundle and will involve “gigantic investments” if Europe’s target of reducing CO2 emissions 20% by 2020 is to be reached. The price tag for Europeans: 2.9 TRILLION euros, i.e. €2,900,000,000,000.00! With 450 million Europeans, that means €6,444.00 for every man, woman and child. The warmist klimaretter writes:
But at the same time, the conclusion that climate protection is expensive cannot be drawn from the calculations. The study does not analyse the costs the of climate protection, but only the necessary investments. Among these there are some that are economically attractive and thus will save money over the years.”
The question ought to be: “What are all these costs going to lead to?” The answer is: nothing. How about taking a look around and opening your eyes? Look at all the poverty out there that is screaming for investment. Look at the sorry state of many schools and hospitals in Europe.
Yet, instead of investing in these important things, the EU wants to blow the money on an energy system that no one really needs – one that is mandated by a fraud.
But the warmists are doing their damnedest to put a positive spin on it. Remember that protecting the climate is an abstract concept that exists only in Fairyland. The concept that we can “protect the climate” is a complete myth. As best I can tell, climate protection for warmists means the production of good weather. Good luck! Klimaretter writes:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The study shows foremost that climate protection is a big business opportunity, also for banking services because it will provide a large share of the needed capital. Therefore one has to view the necessary investments as a chance for the economy.
That this is going to be a big bonanza for the banks is no exaggeration. They are going to make a killing, but at the expense of the consumer. And saying these investments are a “chance” for the economy is really incorrect labelling. It’s going to be a “gamble” for the economy – Russian Roulette style. Klimaretter writes:
Climate protection is ‘one of the megatrends of the economy’.
They’re right about that. But that does not make it a trend that will lead to success. Having everyone go off the edge of a cliff as a trend does not make it a reasonable endeavour. Klimaretter then reports on Environment Minister Norbert Röttgen meeting with industry leaders of Allianz, Metro, Siemens and Viessmann, saying that the energetic renovation of buildings will have top priority and “the politicians must see to it that the renovation is equally attractive for property owners and users.”
More on Allianz in a day or two. In the meantime just keep in mind that this is all based on “purchased science”.  If they looked at the real science, all that capital could be directed to real problems.
======================================================
Sorry folks for the disappearing articles. It’s been a long day and I’ve hit some wrong buttons and so a really rough draft that was not meant to appear showed up as a new post. I’ll most likely post it tomorrow. – PG
Share this...FacebookTwitter "
"

One of the most interesting legal issues percolating in the American legal system is jury nullification. A poll by Decision Quest and the National Law Journal in 1998 found that three out of four citizens believed jurors should do what they think is right regardless of what the judge says the law is. In February 1999 the Washington Post found “a significant pattern of juror defiance” that has resulted in a sharp increase in the number of hung juries. Alarmed by these developments, trial judges are aggressively attempting to thwart jury nullification. The question now before the courts is, How far can a trial judge go before overstepping his or her authority and coercing a verdict? 



Over the past year a string of rulings have illustrated the problem. In April the Colorado Court of Appeals overturned a contempt of court conviction against a juror for her failure to reveal during voir dire that she did not intend to follow the trial judge’s instructions on the law. _People v Kriho_ , No. 97CAD700, 1999 WL 249143 (unpublished decision). In May a U.S. district court judge dismissed a deliberating juror for her unwillingness to follow the law and allowed the remaining jurors to convict the defendants on trial. _U. v Abbell_ (SD Fla) No. 93–0470-CR. In June the U.S. Court of Appeals for the Ninth Circuit reversed the convictions ofJohn Fife Symington III, the former governor of Arizona, because his Sixth Amendment right to an impartial jury was violated when the trial judge dismissed a holdout juror after eight days of deliberations. _U.S. v Symington_ , 1999 _Daily Journal_ DAR 6295, 1999 WL 415345-



The California Supreme Court will soon decide whether a trial court violated a defendant’s right to trial by jury by dismissing a deliberating juror because he did not intend to follow the law on a particular charge. People v Williams, No. S066106 (rev gr Feb. 18, 1998). With all of this appellate court activity, the issue seems destined to go to the U.S. Supreme Court for resolution 



A surprising number of lawyers hold ill‐​considered opinions about jury nullification. The conventional wisdom holds that jury nullification is absolutely improper. Thus, a trial judge should dismiss any juror who refuses to follow the court’s legal instructions. On first blush, that line of reasoning appears sound, but a closer examination will show that it is mistaken 



A leading case in this area of law, _U. v Thomas_ (1997) 116 FM 606, embodies the conventional wisdom on jury nullification, so it is useful to scrutinize the rationale of its holding Judge Jose A. eabranes opined that because “no juror has a right to engage in nullification,” trial courts have a duty to thwart nullification by dismissing any juror who refuses to follow the letter of the law. Although the judge acknowledged that nullification sometimes occurs, he said that it happens because the specter of nullification does not come “to the attention of a presiding judge before the completion of [the] jury’s work” 116 F3d at 616. Thus, he concluded, a trial judge has a responsibility to thwart such “misconduct” whenever the opportunity arises. 



There are several problems with Judge Cabranes’s analysis. First, his claim that jurors do not have the right to engage in jury nullification would have startled the framers of the Constitution. John Adams, who signed the Declaration of Independence and served as our second president, observed that “It is not only [the juror’s] right, but his duty … to find the verdict according to his own best understanding, judgment, and conscience, though in direct opposition to the direction of the court.” It is important to note that Adams was not just speaking for himself. Similar statements by Hamilton, Jefferson, and others show that Judge Cabranes’s view of jury service is at odds with the original understanding of that duty.



Second, the judge’s claim that “trial courts have [a] duty to forestall” jury nullification is weak. If such a duty truly existed, the law would give trial judges the discretionary power to direct verdicts for the prosecution. The same principle that denies judges the discretion to direct guilty verdicts should also operate to deny judges the discretion to dismiss deliberating jurors.



Third, Judge Cabranes seems to think that nullification sometimes occurs because “jurors are not answerable for nullification after the verdict has been reached.” That claim may be true, but it does not lend much support to his argument. After‐​all, the law could empower trial judges to enter a judgment of conviction notwithstanding the verdict. As is the case in civil trials, a JNOV would “cure” a nullification verdict and leave the jurors themselves “unanswerable” regarding the votes they cast in the jury room‐ The same principle that denies judges the power to grant JNOVs in criminal cases should also operate to deny judges the power to dismiss jurors who refuse to condemn the defendant in the circumstances of the case before them. 



Fourth, Judge Cabranes’s rule, which would permit trial judges to dismiss deliberating jurors, would yield highly unsatisfactory results. In the Thomas case, several jurors complained to the court that a verdict could not be reached because a lone holdout refined to follow the law. That juror was subsequently dismissed. The propriety of that dismissal became the central issue on appeal. Judge Cabranes’s opinion said that in such situations the trial judge has to proceed cautiously to determine if such complaints have merit. If the allegations are found to be true, the trial judge should dismiss the holdout juror for “misconduct.” ” Such an approach sounds reasonable to many lawyers, but consider the implications of such a procedure under some alternative fact patterns.



What if, after a full week of deliberations, a trial judge learns that two or three jurors have decided that they cannot in good conscience enforce the law against the defendant? Are we going to allow several alternates to take their places so that a conviction can be obtained? Take a defendant on trial for multiple charges. Wha~if two jurors believe that the defendant is indeed technically guilty on every count, but they disagree with the other jurors with respect to whether the defendant deserves to have the proverbial book thrown at hirn. If the “hard‐​line” jurors complain to the judge, should the trial court throw the “lenient” jurors off the case because they are unwilling to convict on every single count? According to Judge Cabranes’s reasoning the answer would be yes. Such meddling with the give‐​and‐​take of jury deliberations cannot possibly be reconciled with the defendant’s right to a trial by jury and to a unanimous verdict. Make no mistake, if trial courts begin to exercise power in this way, trial by jury will be so hamstrung as to be unrecognizable.



There is a better way to resolve such controversies. The key lies in the duty of jurors to deliberate. The duty to deliberate means that each juror must be willing to listen to the views of others with a disposition toward reexamining his or her own views.That is the duty that ought to concern the trial court. Should a judge receive a note that complains about a juror who refines to follow the letter of the law, the trial judge should resist the impulse to conduct an inquisition. Instead, the judge should calmly and respectfully ascertain whether the so‐​called holdout is willing to deliberate. Of course, a court’s instruction about the duty to deliberate should always be followed by the caveat that no juror should surrender a conscientious opinion solely because of the opinion of other jurors or for the mere purpose of returning a unanimous verdict. 



**Any juror who is unable or unwilling to deliberate** should be excused and replaced with an alternate. But a deliberating juror’s conscientious refusal to cast a vote for conviction should not be considered misconduct and thus a sufficient basis for removal. When a jury is deadlocked because one or more jurors cannot in good conscience vote to convict, the trial judge has only two options: send the jury back to continue deliberating or declare a mistrial. Unlike the holding in the Thomas case, this approach is attentive to the twin imperatives. of safeguarding the province of the jury from incursions fi‐​om the bench and protecting the defendant’s right to a unanimous verdict.



Jury nullification seems to be one of those topics where strong passions overcome reasoned discussion. Some lawyers focus obsessively on instances in which a jury or juror abused the system. Clearly, some cases of abuse exist. To maintain perspective, however, lawyers ought to recall some words of wisdom from judge David L. Bazelon, concurring in US. v Dougherty (1972) 473 F2d 1113, 1142: “Trust in the jury is … one of the cornerstones of our entire criminal jurisprudence, and if that trust is without foundation we must re‐​examine a great deal more than just the nullification doc‐ trine.” And since even conservatives rec‐ ognize that the Constitution placed its trust in juries, not judges, to determine criminal guilt, there’s a good chance that the hostile climate surrounding jury nullification may eventually subside. 
"
nan
"Games can help people engage with science outside of the traditional realm of research and academia. And using games in ecological research is on the rise, helping ecologists answer questions they’d never be able to in a laboratory experiment. This is particularly true when it comes to answering questions about evolution, such as: which traits help organisms maximise their chance of survival?  Natural selection operates over incredibly long timescales as individuals pass on their genes from generation to generation. With a game, we can speed up selection and test how different processes influence survival in just a few clicks. Our citizen science study enlisted ordinary people to help researchers find out which colours and patterns help crabs to camouflage. Digital games were deployed at the Natural History Museum, London, featuring images of common crabs defending themselves in different environments.  Camouflage is one of the most common defences in nature, but many species change colour and pattern as they grow older. Shore crabs (Carcinus maenas) are no exception. Juvenile shore crabs are known for their bright and varied colouration, which helps them camouflage in a range of habitats. But, despite this diversity when young, they all become greener with age. Why this happens remained a mystery – until now. A team of Finnish and British ecologists combined forces with game developers and science communicators at a firm called FoAM Kernow. Together they created a touch-screen game in which museum visitors searched for camouflaged crabs. A series of crabs were displayed against photos of different coastal habitats, challenging players to find them as fast as possible. At the end of each round, players could see how fast they found them and which crabs hid most effectively, with the research results shared with participants in real time. Photos of crabs found in rockpools, mudflats and mussel beds were displayed against each environment on a sleek screen during the museum’s Colour and Vision exhibition. Visitors to the exhibition performed the role of predators in the crabs’ environment – each on a mission to find crabs hidden in the different habitats. With thousands of “predators” playing the game, the team were able to see which colours and patterns helped the crabs hide most effectively. As Ossi Nokelainen, a researcher at University of Jyväskylälead and lead author of our study, explained: The citizen science game enables us to explore crab survival – something that would be really difficult to measure in the wild. This helps us to better understand why the crabs change appearance as they age when reared in controlled conditions. The time taken to find each crab – and whether it was found at all – allowed the researchers to see how effective each crab’s camouflage was. Across all environments green crabs were the hardest to detect. They were the most likely to survive and took the longest to find, offering clues to why crabs get greener as they grow. While baby crabs can be camouflage specialists – fine-tuning their appearance to match their surroundings – as they grow it pays to get greener and avoid predators across a wider range of environments. This shift from specialist to generalist camouflage may explain why many other species also start to look more similar as they age. Unlike many citizen science activities, the game shared live results with participants as they played, generating what is quite possibly the fastest public engagement with a research finding."
"
Love it or hate it, WUWT gets traffic.

This month was 1,478,801 page views. This is up significantly from both January (1,324,097) and February (1,168,852).
As always, my sincere thanks to the many readers, commenters (even the angry ones, you know who you are 😉 ), moderators, and guest contributors that keep WUWT fresh and interesting.
– Anthony
UPDATE: Since I had a question about it, the numbers and graph above are from my internal WordPress.com traffic counter and stat system. They are actual counted pages views, not estimates like some external web traffic analysers.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9716d680',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Japan’s boffins: Global warming isn’t man-made
Climate science is ‘ancient astrology’, claims report
By Andrew Orlowski The Register UK (h/t) from WUWT reader Ric Werme
UPDATE: One of the panelists (Dr. Itoh) weighs in here at WUWT, see below.
Exclusive Japanese scientists have made a dramatic break with the UN and Western-backed hypothesis of climate change in a new report from its Energy Commission.
Three of the five researchers disagree with the UN’s IPCC view that recent warming is primarily the consequence of man-made industrial emissions of greenhouse gases. Remarkably, the subtle and nuanced language typical in such reports has been set aside.
One of the five contributors compares computer climate modelling to ancient astrology. Others castigate the paucity of the US ground temperature data set used to support the hypothesis, and declare that the unambiguous warming trend from the mid-part of the 20th Century has ceased.
The report by Japan Society of Energy and Resources (JSER) is astonishing rebuke to international pressure, and a vote of confidence in Japan’s native marine and astronomical research. Publicly-funded science in the West uniformly backs the hypothesis that industrial influence is primarily responsible for climate change, although fissures have appeared recently. Only one of the five top Japanese scientists commissioned here concurs with the man-made global warming hypothesis.
JSER is the academic society representing scientists from the energy and resource fields, and acts as a government advisory panel. The report appeared last month but has received curiously little attention. So The Register commissioned a translation of the document – the first to appear in the West in any form. Below you’ll find some of the key findings – but first, a summary.
Summary
Three of the five leading scientists contend that recent climate change is driven by natural cycles, not human industrial activity, as political activists argue.
Kanya Kusano is Program Director and Group Leader for the Earth Simulator at the Japan Agency for Marine-Earth Science & Technology (JAMSTEC). He focuses on the immaturity of simulation work cited in support of the theory of anthropogenic climate change. Using undiplomatic language, Kusano compares them to ancient astrology. After listing many faults, and the IPCC’s own conclusion that natural causes of climate are poorly understood, Kusano concludes:
“[The IPCC’s] conclusion that from now on atmospheric temperatures are likely to show a continuous, monotonous increase, should be perceived as an unprovable hypothesis,” he writes.
Shunichi Akasofu, head of the International Arctic Research Center in Alaska, has expressed criticism of the theory before. Akasofu uses historical data to challenge the claim that very recent temperatures represent an anomaly:
“We should be cautious, IPCC’s theory that atmospheric temperature has risen since 2000 in correspondence with CO2 is nothing but a hypothesis. ”
Akasofu calls the post-2000 warming trend hypothetical. His harshest words are reserved for advocates who give conjecture the authority of fact.
“Before anyone noticed, this hypothesis has been substituted for truth… The opinion that great disaster will really happen must be broken.”
Next page: (at the Register)  Key Passages Translated

UPDATE: From Kiminori Itoh, Prof., Yokohama National University.
Hi everybody!
I am one of the five who participated to the article in the JSER journal, which may have seemed to you as a mystery from Japan. At first, I thank you for picking up our activity in Japan. I am a regular reader of several climate blog sites, and had been making some contributions mainly to Climate Science of Prof. Pielke. Actually, the information I gave in the article largely owes the invaluable information shown at this site WUWT as well as Climate Science and Climate Audit. Thus, I felt I should explain a bit about the article of JSER because, unfortunately, it is written in Japanese although it has partly been translated into English. 
Some readers of WUWT might remember my name; I had written a guest blog in Climate Science several months ago, when Roger kindly suggested me to introduce my new book “Lies and Traps in Global Warming Affairs.” Yes, I am regarded as one of the most hard-core AGW skeptics in Japan, although I myself regard me as a realist in this issue.
The article of JSER has been composed of discussions between the five contributors, made through e-mail for several months, and was organized by Prof. Yoshida of Kyoto University (an editor of the JSER journal). Our purpose was to invoke healthy discussions on the global warming issue in Japan. The JSER journal was selected as a platform for this discussion just because Prof. Yoshida has a personal interest in this issue and he is an editor of the journal. 
Thus, it is not correct if one thinks that the discussion represents the opinion of the journal’s editors or of the society JSER. In fact, none of the five contributors belong to the JSER, and Prof. Yoshida kept his attitude neutral in the article.
All the contributors are well-established researchers in different fields and each has characteristic personal opinions on the AGW issue. Only one (Dr. Emori, National Institute of Environmental Sciences, Japan) represents IPCC. Other members are more or less skeptical of the conclusions of IPCC. For instance, as translated into English, Dr. Kusano made a severe critique on climate models; he himself is a cloud-modeler, so that his critique seems plausible. Prof. Akasofu is well known as an aurora physicist, Prof. Maruyama is famous for his ideas in geophysics, and I myself have sufficient academic record in environmental physical chemistry (more than 160 peer review papers).
We know that our try this time is small one, and its impact has a limitation especially due to language problem. Nevertheless, we believe that the discussion was useful and informative for everyone interested in the controversies associated with the AGW issue. In March, another article will come also in the JSER journal because the discussion received much interest from the readers of the journal. 
Any comments and opinions are welcome and very helpful for us. 
Thank you again.
Based on Dr. Itohs comments, I’ve amended the headline to be more reflective of his first hand account on the report. – Anthony



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e986925e8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Lake Baikal, the world’s oldest, deepest lake, is feeling the temperature of human-induced climate change. Situated in southern Siberia, Baikal occupies one of the fastest warming regions on the planet and, as a result, the lake itself has got warmer, seasonal ice is present for a shorter period of time and has got thinner, and its waters have become stratified for longer periods. These changes have already had an impact on the lake’s microscopic life, including phytoplankton and zooplankton.  Now, our new research has provided the first evidence that some of the lake’s unique microscopic plants are being outcompeted by species not unique to the lake, most likely due to climate change. Although these ecological changes are so far confined to the south basin of Lake Baikal, they may act as an early warning signal of what might happen across the rest of the lake in the coming decades. To place modern ecological observations into a wider and longer-term perspective, we looked at the mud which accumulates at the bottom of the lake. This preserves an environmental history that, with careful collection and analyses, can be used to reveal changes in the lake’s ecology.  We were especially interested in investigating long-term trends in a key group of organisms called diatoms. These tiny algae are invisible to the naked eye, being only a fifth of the breadth of human hair.  Most of the energy in Lake Baikal’s food web ultimately comes from photosynthesis by these tiny diatoms. They are also especially useful in reconstructing long-term changes in the lake’s ecology because they have shells made of silica, the same material as glass, which allows their fossils to be preserved in the lake mud. As with most plants and animals found in Baikal, these diatoms are mainly endemic – that is, they are found nowhere else in the world. Back in 2006, one of us (Anson) predicted that climate change would lead to a decline in Baikal’s large, heavy, slow-growing, endemic diatom species, as they would quickly sink out of the photic zone as the lake became increasingly stratified. His work suggested that they would be replaced by smaller, lighter, faster growing species (both endemic and those found elsewhere), able to better tolerate more stratified water.  In the current study, our colleague Sarah Roberts set out to test this hypothesis as part of her PhD, by extracting “cores” from the mud at the bottom of the lake, to see what diatoms were like in previous years.   Our findings were surprising. In the south of Lake Baikal our data showed that a significant change in the diatoms occurred at the very start of the 1970s, at the same time as the lake began to warm and ice thinned. As predicted, this change was manifested by a decline in diatoms with thick, heavy shells, alongside an increase in faster-growing species with lighter shells, such as Synedra acus, a species also found growing in many other lakes worldwide.  What we didn’t expect was the decline in other lighter, endemic species such as Crateriportula inconspicua.  When the heavy diatoms decline in abundance, dissolved silica needed to make their glass shells is now available for other diatoms to use. But because S. acus can tolerate higher water temperatures with fast growth rates, it quickly outcompetes the other smaller, endemic diatoms.  Why is this important? Climate change is already messing with ecosystems in other large, ancient lakes, such as Lake Tanganyika in East Africa. What happens to plankton has a knock on effect up the food web, causing fish to struggle and also, ultimately, those humans who depend on the ecosystem for their livelihood.  The increasing dominance by non-endemic diatom species in Lake Baikal has the potential to disrupt the lake’s own food web, through changes to the types of zooplankton and other fauna that feed on Baikal diatoms, which may ultimately impact on the endemic fish species that feed on the zooplankton communities themselves.  Elsewhere in the lake, we are also seeing an increase in algal mats along the coastline, linked to untreated sewage from coastal settlements. Interacting stressors from both nutrient enrichment along Lake Baikal’s coastline, and increasing surface water temperatures and stratification from climate change, could have untold consequences on biodiversity in one of the world’s unique ecosystems."
"

State officials in Ohio have been complaining recently about potential revenue losses from the growth of untaxed Internet shopping. “We figure we’re losing over $200 million annually from direct marketing, catalog and Internet sales,” says Clare Long, Ohio’s deputy tax commissioner. Across the nation, departments like Long’s have been fighting for years to force out‐​of‐​state mail order companies to collect sales taxes. Now electronic commerce is in the cross hairs. What state officials propose is — you guessed it — more taxes. 



The officials are worried because an estimated 27 million U.S. households (a number that’s getting bigger every day) now use the Internet regularly. In the officials’ eyes, that’s too many consumers who might make purchases without the government’s getting a cut. Buyers are supposed to pay a “use tax” in lieu of a sales tax on all out‐​of‐​state purchases, but few volunteer. 



At first glance, the proposal sounds reasonable: why not tax identical items the same regardless of how they’re purchased? From the tax collector’s perspective, that makes sense. But in the real world, there are several reasons why allowing states to tax out‐​of‐​state electronic commerce is bad policy. 



First, there is no immediate danger of large revenue losses for traditional retailers, and by extension, for state tax authorities. Because they cater to a customer’s desire for a hands‐​on experience, local stores don’t charge for shipping and offer immediate gratification and so will probably always dominate retailing. What’s more, shopping is for many people a pleasurable social experience that cannot be duplicated online. Thus, Internet sales won’t destroy “real” retailers, just as catalog sales haven’t. 



National data support that conclusion. In an era of almost no inflation, state budgets grew by 5 percent in fiscal year 1997 and by more than 6 percent in fiscal year 1998. The last fiscal year ended with about $21 billion more in tax collections than originally anticipated. It appears that states will enjoy a sizable revenue windfall this year as well. If electronic commerce is undermining state revenues, it’s an undetectable trend. Electronic commerce certainly hasn’t slowed the flood of surplus money pouring into Columbus — expected to be around $400 million this year. 



Second, it’s not fair to force out‐​of‐​state firms to act as tax collectors when they don’t benefit from state services. When an Ohio business collects sales taxes, there is a clear linkage between the taxes paid, the services provided, and legislative representation. After all, local firms benefit from police and fire protection, roads and waste collection and other state services, so it is proper that they help cover those costs. And local firms can make their voice heard directly through lobbying and membership in groups like the Chamber of Commerce. 



Remote sellers, on the other hand, don’t enjoy any of those advantages. If the state wants more of the taxpayers’ money, it should collect it itself and not try to push the burden onto out‐​of‐​state businesses. 



Finally, differentiated tax rates create healthy competition that helps keep local rates under control. For example, some residents of Manhattan drive to Delaware to avoid sales taxes — an option that has undoubtedly curbed the profligate fiscal habits of New York politicians. Electronic commerce similarly guards against excessive taxation. When sales tax rates get too high, it’s important that Ohioans have a shopping alternative. 



The idea that government won’t find some way to keep the tax dollars flowing is laughable. So let’s be honest about what’s going on here: allowing states to tax out‐​of‐​state electronic commerce would be the equivalent of a tax increase. States would fatten already overflowing coffers without ever having to bring the issue to a vote at home. That’s a dream scenario for state legislators but a nightmare for taxpayers. 



If states are concerned about local retailers, they can effectively address the issue by moving tax rates downward. Minnesota policymakers have raised one such interesting possibility, proposing to eliminate the sales tax on certain products that are easily acquired online. Specifically targeted are intangible goods that can be downloaded, such as software, music and books. 



Sure, limiting states’ taxing authority can lead to unequal taxation. But such limitations are a crucial component of American federalism. Absent those restraints, confiscatory tax rates — which are the true injustice — would get worse. To improve its business climate Ohio should cut taxes, not scheme to collect more.
"
"
Here is a weather curiosity. We’ve been hearing a lot about snowfall in the northern hemisphere this year. In Oslo, they have given up on trying to pile it up so they have resorted to dumping it in the sea. If this happened in Seattle they’d probably get into a tizzy for polluting Puget Sound with fresh water snow. And it is not just Oslo, the problem seems widespread. Here are some other news stories in London, OT Geneva, Ohio Chardon, OH Wasatch, UT Chicopee, MA and Rochester, NY where they say the piles are making driving dangerous. In Wenatchee, WA they want to spray warm sewage water on the snow to melt it.  I know they could use the USHCN temperature sensor at the sewage treatment plant there to check the temperature to make sure conditions are right. Yeah, that’s the ticket! – Anthony

From Reuters Environment Blog by Alister Doyle
It looks more like an Ice Age than global warming.
There is so much snow in Oslo, where I live, that the city authorities are resorting to dumping truckloads of it in the sea because the usual storage sites on land are full.
That is angering environmentalists who say the snow is far too dirty – scraped up from polluted roads — to be added to the fjord. The story even made it to the front page of the local paper (’Dumpes i sjøen’: ‘Dumped in the sea’).
In many places around the capital there’s about a metre of snow, the most since 2006 when it was last dumped in the sea. Extra snow usually gets trucked to sites on land, where most of the polluted dirt is left after the thaw. Those stores are now full — in some the snow isn’t expected to melt before September.
But are these mountains of snow a sign that global warming isn’t happening?
Unfortunately, more snow might fit projections by the U.N. Climate Panel, which says that northern Europe is likely to get wetter and the south drier as temperatures rise this century.
“By the 2070s, hydropower potential for the whole of Europe is expected to decline by 6 percent, with strong regional variations from a 20 to 50 percent decrease in the Mediterranean region to a 15 to 30 increase in northern and eastern Europe.” it said in a 2007 report (page 60 of this link).
So people in northern Europe may have to buy more snow shovels than parasols to cope with global warming?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e98486bb9',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterAs winters get harsher and the snow piles up, more and more scientists are now warning of global cooling. Reader Matt Vooro has compiled a list (see below) of 31 prominent scientists and researchers who have words that governments ought to start heeding.
UPDATE: Another one for the list – Professor Paar, from Croatia’s Zagreb University
Cooling seems to be the trend. Photo source: NOAA. 
Lately, the clueless among warmist scientists, governments and the MSM have been running around in deep snow with their global warming blinders on, denying the cold around them. Governments, entrusted to serve the citizens, really ought to start listening up and planning accordingly.
———————————————————————————————–
Are we headed for global warming or cooling?
By Matt Vooro
For many years now a good number of non-AGW scientists, meteorologists, engineers, researchers and the like have looked at the possibilities of a cooling planet. I enclose some of the ones that I have noted  in my research. Indeed there is a significant number of scientists, academics, meteorologists and researchers who disagree with IPCC’s belief that the globe is very likely headed for unprecedented global warming due to man-made greenhouse gases.
The climate of this planet oscillates between periods of approximately 30 years of warming followed by approximately 30 years of cooling. Rather than 100 years of unprecedented global warming as predicted by IPCC, the global temperatures have leveled off and we seem to be heading for cooler weather.  
Lawrence Solomon in his article of 16 June, 2010 in the Toronto National Post commented on Professor Mike Hulme’s article about IPCC. The article can be found here at probeinternational.org. Hulme is a Professor of Climate Change in the School of Environmental Sciences at the University of East Anglia – the university of Climategate fame — is the founding Director of the Tyndall Centre for Climate Change Research and one of the UK’s most prominent climate scientists. Quoting Hulme, Solomon said:
The UN’s Intergovernmental Panel on Climate Change misled the press and public into believing that thousands of scientists backed its claims on manmade global warming, according to Mike Hulme, a prominent climate scientist and IPCC insider. The actual number of scientists who backed that claim was “only a few dozen experts” he states in a paper for Progress in Physical Geography, co-authored with student Martin Mahony.”
Professor Hulme’s paper can be found at fabiusmaximus
It would appear that IPCC underestimated the repetitive and significant impact of normal planetary cycles like the PDO, AO, AMO, NAO, ENSO, DEEP OCEAN CURRENTS [MOC], SOLAR CYCLES and UNEXPECTED PERIODS OF VOLCANIC ASH.
This is understandable as IPCC never had a mandate to study all causes of global warming but only the man induced component which seems to be dwarfed by natural planetary factors, which other scientists are now finding out. Read papers.ssrn.com/sol3/papers.
Here is a list of 31 different international climate scientists, academics, meteorologists, climate researchers and engineers who have researched this topic and who disagree with AGW science and IPCC forecasts, and are projecting much cooler weather for the next 1-3 decades.

The List

1. Don Easterbrook, Professor Emeritus, Dept. of Geology, Western Washington University.
Setting up of the PDO cold phase assures global cooling for next approx. 30 years. Global warming is over.  Expect 30 years of global cooling, perhaps severe 2-5°F.”
He predicts several possible cooling scenarios: The first is similar to 1945-1977 trends, the second is similar to 1880-1915 trends and the third is similar to 1790-1820 trends. His latest article states:
Expect global cooling for the next 2-3 decades that will be far more damaging than global warming would have been.”
Read here, here and here.
2. Syun Akasofu, Professor of Geophysics, Emeritus, University of Alaska, also founding director of ARC
He predicts the current pattern of temperature increase of 0.5C /100 years resulting from natural causes will continue with alternating cooling as well as warming phases. He shows cooling for the next cycle until about 2030/ 2040.
And again a new paper ON THE RECOVERY FROM LITTLE ICE AGE – Read here.
3. Prof. Mojib Latif, Professor, Kiel University, Germany
He makes a prediction for one decade only, namely the next decade [2009-2019] and he basically shows the global average temperatures will decline to a range of about 14.18 C to 14.28 C  from 14.39 C  [eyeballing his graphs].
He also said that “you may well  enter  a decade or two of cooling relative to the present temperature level”, however he did not indicate when any two decades of cooling would happen or whether the  second decade after the next decade will also be cooling. Read here and here.
4. Dr. Noel Keenlyside from the Leibniz Institute of Marine Sciences at Kiel University. The BBC writes:
The Earth’s temperature may stay roughly the same for a decade, as natural climate cycles enter a cooling phase, scientists have predicted.”
A new computer model developed by German researchers, reported in the journal Nature, suggests the cooling will counter greenhouse warming.”
Read here news.bbc.
5. Professor Anastasios Tsonis, Head of Atmospheric Sciences Group University of Wisconsin, and Dr. Kyle Swanson of the University of Wisconsin-Milwaukee. msnbc writes:
We have such a change now and can therefore expect 20 -30 years of cooler temperatures”
This is nothing like anything we’ve seen since 1950,”
Kyle Swanson of the University of Wisconsin-Milwaukee said. “Cooling events since then had firm causes, like eruptions or large-magnitude La Ninas. This current cooling doesn’t have one.”
Swanson thinks the trend could continue for up to 30 years.”
Also read The mini ice age starts here at dailymail.co.uk/.
6. William M Gray, Professor Emeritus, Dept of Atmospheric Sciences, Colorado State University
A weak global cooling began from the mid-1940’s and lasted until mid-1970’s. I predict this is what we will see in the next few decades.”
Read colostate.edu.
7. Henrik Svensmark , Professor DTU, Copenhagen. Henrik Svensmark writes:
Indeed, global warming stopped and a cooling is beginning. No climate model has predicted a cooling of the Earth, on the contrary. This means that projections of future climate is unpredictable.”
Read here.
8. Jarl R. Ahlbeck, D.Sc., AboAkademi University, Finland
Therefore, prolonged low solar activity periods in the future may cause the domination of a strongly negative AO and extremely cold winters in North America, Europe and Russia.”
Read here.
9. Dr. Alexander Frolov, Head of Russia’s state meteorological service Rosgidromet. The Daily Mail.co.uk quotes Frolov:
‘From the scientific point of view, in terms of large scale climate cycles, we are in a period of cooling.
‘The last three years of low temperatures in Siberia, the Arctic and number of Russia mountainous regions prove that, as does the recovery of ice in the Arctic Ocean and the absence of warming signs in Siberia.”
And writes:
Mr. Tishkov, deputy head of the Geography Institute at Russian Academy of Science, said: ‘What we have been watching recently is comparatively fast changes of climate to warming, but within the framework of an overall long-term period of cooling. This is a proven scientific fact’.” 
10. Mike Lockwood, Professor of Space Environmental Physics, University of Reading, UK. Read BBC News here:   
The UK and continental Europe could be gripped by more frequent cold winters in the future as a result of low solar activity, say researchers.”
11. Dr. Oleg Pokrovsky, Voeikov Main Geophysical Observatory: Ria Novosti writes:
There isn’t going to be an ice age, but temperatures will drop to levels last seen in the 1950s and 1960s.
Right now all components of the climate system are entering a negative phase.  The cooling will reach it’s peak in 15 years. Politicians who have geared up for warming are sitting on the wrong horse.
The Northeast Passage will freeze over and will be passable only with icebreakers.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Pokrovsky also claims that the IPCC, which has prophesized global warming, has ignored many factors. He also noted that most American weather stations are located in cities where temperatures are always higher.
We don’t know everything that’s happening. The climate system is very complex and the IPCC is not the final truth on the matter.”
Read here NoTricksZone.
12. Girma Orssengo, b.Tech, MASc, PhD 
These cool and warm PDO regimes correlate well with the cooling and warming phases of GMTA shown in Figure 3.
The model in Figure 3 predicts global cooling until 2030. This result is also supported by shifts in PDO that occurred at the end of the last century, which is expected to result in global cooling until about 2030 [7].”
Read WUWT and read here, and
In this article, a mathematical model was developed that agrees with observed Global Mean Temperature Anomaly(GMTA), and its prediction shows global cooling by about 0.42 deg C until 2030. Also, comparison of observed increase in human emission of CO2 with increase in GMTA during the 20th century shows no relationship between the two. As a result, the claim by the IPCC of climate catastrophe is not supported by the data.”
‘Fossil fuels allowed man to live his life as a proud human, but the IPCC asserts its use causes catastrophic.’ “
Read here at WUWT.
13. Nicola Scafetta, PhD.
Empirical evidence for a celestial origin of the climate oscillations and its implications
The partial forecast indicates that climate may stabilize or cool until 2030-2040.”
Read here
14. Dr William Livingston, astronomer  & solar physicist; and 15. Dr Matthew Penn – astronomer &  solar physicist
Astronomers Dr. William Livingston and Dr. Matthew Penn and a large number of solar physicists would say that now the likelihood of the Earth being seized by Maunder Minimum is now greater than the Earth being seized by a period of global warming.”
Read here: http://algorelied.com/?p=2706.
16. Joe d’Aleo – Executive Director of Certified Consultant Meteorologists. Read here:  Intellicast.com
Longer term the sun is behaving like it did in the last 1700s and early 1800s, leading many to believe we are likely to experience conditions more like the early 1800s (called the Dalton Minimum) in the next few decades. That was a time of cold and snow. It was the time of Charles Dickens and his novels with snow and cold in London.”
Also see various other articles about Global Cooling under ICE AGE at Ice Cap
17. Harry van Loon, Emeritus at NCAR and CORA, 18. Roland Madden, Senior scientist at NOAA, Deputy Head of Climate analysis, 19. Dave Melita, Head Meteorologist at Melita Weather Associates, and 20. William M Gray, Professor Emeritus, Dept of Atmospheric Sciences, Colorado State University
These scientists came to the same conclusions— the global warming trend is done, and a cooling trend is about to kick in.
Read here!
21. Dr. David Archibald, Australia, environmental scientist:
In this presentation, I will demonstrate that the Sun drives climate, and use that demonstrated relationship to predict the Earth’s climate to 2030. It is a prediction that differs from most in the public domain. It is a prediction of imminent cooling.”
See Warwick Hughes and David Archibald
22. Dr Habibullo Abdussamatov, Head of Space Research, Lab of Pulkov Observatory. See iceagenow.com:
In his presentation called The Sun Dictates the Climate, he indicated that there would be an ice age kind of temperatures in the middle of the 21st century. He showed a graph called The forecast of the natural climate change for the nearest 100 years and it showed the globa temperatures dropping by more than 1°C by 2055. According to him, a new ice age could start by 2014.”
And read here.
23. Dr Fred Goldberg, Swedish climate expert. People Daily:
We could have an ice age any time, says Swedish climate expert.”
and read: We could have an ice age any time, says Swedish climate expert
24. Dr. George Kukla, a member of the Czechoslovakian Academy of Sciences and a pioneer in the field of astronomical forcing, Read Ice Age Now:
In the 1970s, leading scientists claimed that the world was threatened by an era of global cooling.
Based on what we’ve learned this decade, says George Kukla, those scientists – and he was among them — had it right. The world is about to enter another Ice Age.”
25. Peter Clark, Professor of Geosciences at OSU: Read iceagenow.com: 
Sometime around now, scientists say, the Earth should be changing from a long interglacial period that has lasted the past 10,000 years and shifting back towards conditions that will ultimately lead to another ice age.”
26. James Overland, NOAA. Read physorg.com:
‘Cold and snowy winters will be the rule rather than the exception,’ said James Overland of the US National Oceanic and Atmospheric Administration.”
27. Dr. Theodore Landscheidt. Predicted in 2003 that the current cooling would continue until 2030 [Read here]:
Analysis of the sun’s varying activity in the last two millennia indicates that contrary to the IPCC’s speculation about man-made global warming as high as 5.8°C within the next hundred years, a long period of cool climate with its coldest phase around 2030 is to be expected.”
28. Matt Vooro, P. Eng. The icecap.us:
We seem to be in the same climate cycle that we were back in 1964-1976.The last two winters [2008, 2009] have been very similar to those we had back then with all the extra snow and cold temperatures. Once the extra warming effect of the current 2009/2010 El Nino is finished, watch for colder temperatures to return due to the impact of the negative PDO, AMO, AO, NAO, ENSO/La Nina, major volcanic ash and changing solar cycles.”
Good source of articles and data on global cooling, see: Isthereglobalcooling.com
29. Thomas Globig, Meteorologist, Meteo Media weather service. Read here at WUWT:
‘The expected cold for the next month will bring this down significantly by year end. ‘The year 2010 will be the coldest for ten years in Germany,’ said Thomas Globig from the weather service Meteo Media talking to wetter.info. And it might even get worse: ‘It is quite possible that we are at the beginning of a Little Ice Age,’ the meteorologist said. Even the Arctic ice could spread further to the south.”
30. Piers Corbyn, Astrophysicist. From http://wattsupwiththat.com/2010/12/27/piers-corbyn-goes-global-cooling/
Predicting in November that winter in Europe would be “exceptionally cold and snowy, like Hell frozen over at times,” Corbyn suggested we should sooner prepare for another Ice Age than worry about global warming. Corbyn believed global warming “is complete nonsense, it’s fiction, it comes from a cult ideology. There’s no science in there, no facts to back [it] up.”
31. Dr. Karsten Brandt, Director of donnerwetter.de weather service.
It is even very probable that we will not only experience a very cold winter, but also in the coming 10 years every second winter will be too cold. Only 2 of 10 will be mild.
Read here.
32. Joe Bastardi – Accuweather meteorologist and hundreds of other meteorologists (i.e. expert forecasters who outperform climatologists hands-down in seasonal forecasting).
http://www.accuweather.com/ukie/bastardi-europe-blog.asp
Other global cooling articles:
John Holdren Ice Age Likely
BBC News
Global Warming Debunked
Star Tribune
nationalpost.com
canadafreepress.com
news.yahoo.com
======================================================
Share this...FacebookTwitter "
"
This is from the Huffington Post. One can only hope that Kerry will follow through. For a quick primer on Kerry’s grasp of climate science, see this WUWT article: Kerry Blames Tornado Outbreak on Global Warming and a rebuttal Increasing tornadoes or better information gathering? I get a kick out of Kerry’s line “This has to stop”. Okay then, please debate Mr. Will, put a stop to it Mr. Kerry! –  Anthony
John Kerry
U.S. Senator from Massachusetts
Posted February 27, 2009 									| 04:47 PM (EST)
Facts Are Stubborn Things: George Will and Climate Change-
To paraphrase the conservative columnist’s favorite president, “There you go again, George.”
George Will has been one of my favorite intellectual sparring partners for a long time, a favorite more recently because he had the guts to publicly recognize the disaster that was George W. Bush’s presidency.
But in his latest Washington Post column, George and I have a pretty big loud disagreement.
Don’t get me wrong. I’m happy to see Will embracing the idea of recycling, but I’m very troubled that he is recycling errors of fact to challenge the science on global warming.
I’m even more troubled that Will used his February 15th column not only to cast doubt on sound science, but also to denigrate the work of two fine scientists.
Let’s be very clear: Stephen Chu does not make predictions to further an agenda. He does so to inform the public. He is no Cassandra. If his predictions about the effects of our climate crisis are scary, it’s because our climate is scary.
Likewise, John Holdren is a friend of mine and one of the best scientific minds we have in our country. Pulling out one minor prediction that he had some unknown role in formulating nearly three decades ago, as Will did in his February 15th column, and then using that to try to undo his credibility as a scientist may be a fancy debating trick, but it’s just plain wrong when it comes to a debate we can’t afford to see dissolve into reductio ad absurdum hijinx. (A side note: The incident in question occurred in 1980, which, as I recall, was just about the time Ronald Reagan made the claim that approximately 80 percent of our air pollution stems from hydrocarbons released by vegetation and that, consequently, we should “not go overboard in setting and enforcing tough emissions standards from man-made sources.”)
Dragging up long-discredited myths about some non-existent scientific consensus about global cooling from the 1970s does no one any good. Except perhaps a bankrupt flat earth crowd. I hate to review the record and see that someone as smart as George Will has been doing exactly that as far back as 1992. And it’s especially troubling when the very sources that Will cites in his February 15th column draw the exact opposite conclusions and paint very different pictures than Will provides, as the good folks at ThinkProgress and Media Matters for America have demonstrated so thoroughly.
This has to stop. A highly organized, well-funded movement to deny the reality of global climate change has been up and running for a long time, but it doesn’t change the verdict: the problem is real, it’s accelerating, and we have to act. Now. Not years from now.
No matter how the evidence has mounted over two decades — the melting of the arctic ice cap, rising sea levels, extreme weather — the flat earth caucus can’t even see what is on the horizon. In the old Republican Congress they even trotted out the author of Jurassic Park as an expert witness to argue that climate change is fiction. This is Stone Age science, and now that we have the White House and the Congress real science must prevail. It is time to stop debating fiction writers, oil executives and flat-earth politicians, and actually find the way forward on climate change.
This is a fight we can win, a problem we can overcome, but time is not on our side. We can’t waste another second arguing about whether the problem exists when we need to be debating everything from how to deal with the dirtiest forms of coal as the major provider of power in China to how to vastly increase green energy right here at home.
“Facts are stupid things,” Ronald Reagan once said. He was, of course, paraphrasing John Adams, who could have been talking about the science on global change when he said, “Facts are stubborn things.”
Stubborn or stupid — lets have a real debate and lets have it now.
I know George Will well, I respect his intellect and his powers of persuasion — but I’d happily debate him any day on this question so critical to our survival.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e97b846cc',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

People who don’t think the federal government sometimes exaggerates things a wee bit obviously did not survive the Y2K crisis. Or perhaps they merely sizzled away in the record heat endured by our fair Republic, as recently reported by the Department of Commerce, which has pronounced 1998–99 the warmest years for which we have adequate records — 1998 comes in as hottest and 1999 as second warmest. 



One Y2K lesson is that what is said in Washington isn’t necessarily what is, depending on what “is” means. In the case of the nation’s or the globe’s temperature, our government has chosen to trumpet one particular climate history out of several that are available. Not surprisingly, the government tells us about the hottest, while the rest are not remarkable at all. 



The heated pronouncement, which actually came from the National Climatic Data Center (NCDC), is not a result of cooking the books. Instead, it is a result of very selective reading. 



The particularly hot data set, known as the Historical Climate Network (HCN), comes from several hundred rural weather stations selected from the roughly 16,000 official sites that are available. But let’s consider two other data sets, one from NASA and the other from the very same NCDC. 



NASA’s record is compiled by James Hansen, director of the Goddard Institute for Space Studies. While the HCN has 1999 as the second‐​hottest year on record (records go back only to 1895), Hansen’s record ranks it as a very unremarkable 14th warmest, putting it nowhere close to the blazing year of 1934, when there was a real drought (as opposed to 1999, when less of the country than normal experienced extreme dryness). 



NCDC’s “other” history is known as “Climatological Division” (CD) record. It uses virtually all of the available data, aggregates them into 344 geographic units (CDs) that are thought to have some climate homogeneity, and then totals them, adjusting for the area occupied by each CD. In this record, both 1998 and 1999 temperatures fall beneath those of 1934. Instead of being second warmest, 1999 will be between fourth and eighth, depending on December data that have not yet been entered. 



The peculiar thing about the discrepancy between the CD and HCN records is that the former includes a few stations that are known to be artificially warming because cities have a way of growing up around their weather stations. Why the HCN should now be running warmer than this record is a mystery to everyone. But why the feds only trumpeted the HCN heat should be a mystery to no one. They are trying to whip up hysteria in order to shame the Senate into approving the United Nations’ Kyoto Protocol on climate change. Currently, 11 senators (of the required 67) support this economically disastrous treaty. 



The United Nations itself recently made a similar pronouncement about 1998–99 for global temperature. Their record goes to 1860. (Ask yourself how the U.S. record goes back only to 1895 while the U.N.‘s global history starts in 1860, and you will get an idea of how reliable are our historical statements about “global” warming.) 



Needless to say, the U.N.‘s story was blatted by every media network that serves it. But there are two other measures of global temperature that, like the NCDC and NASA records for the United States, were not so hot. 



The first is University of Alabama climatologist John Christy’s satellite history, which has been carefully corrected for instrument and orbit changes, that shows 1999 to be slightly cooler than the average for the 21 years in which the platforms have been taking our temperature. There are 12 warmer years and eight cooler ones in this history, which shows a slight warming trend only because of the big 1998 El Niño. (This means that the decade from 1998 to 2007 will very likely show a cooling trend.) 



The satellite temperatures are known to closely track those measured by weather balloons in the atmospheric layer from 5,000 to 30,000 feet — a zone forecast by computer models of global warming to be heating even more rapidly than the surface. This record extends back to 1958. Fifteen years were warmer than 1999 and 27 were cooler. 



The resolution of the difference between the U.N.‘s surface temperatures and those measured by the satellites and the weather balloons may spell the end of the global warming crisis. More and more, it appears that the reason they diverge is that warming is trapped largely in very cold airmasses in Siberia that don’t extend up to 5,000 feet, which is the altitude at which the balloon record begins. No one has yet to hear Russians clamoring for a return to the climate of the Stalin era. 



As Casey Stengel used to say, “You could look it up.” The NASA data are at www​.giss​.nasa​.gov/data, the NCDC CD history is at ftp​.ncdc​.noaa​.gov/pub, and the satellite record is at ftp://​vor​tex​.atmos​.uah​.edu/msu. 



Any way you look at it, governments have been less than truthful in telling the whole story about the heat of 1999. But what do you expect? After all, it is the year 2K.
"
"

The Dayton Peace Agreement, signed in Paris on Dec. 14, 1995, formally ended the civil war in Bosnia. Three years later, Dayton’s goal of creating a unified, multiethnic Bosnian state remains as elusive as ever. But that should have been expected. According to University of Chicago political scientist John Mearsheimer, “History records no instance where ethnic groups have agreed to share power in a democracy after a large‐​scale civil war. The democratic power‐​sharing that Dayton envisions has no precedent.” 



That’s not to say that Dayton hasn’t led to any success. The fighting has stopped, and so far more than 3,600 pieces of heavy weaponry have been removed. But those few successes reveal Dayton for what it really is: a complicated cease‐​fire, not a solution to Bosnia’s long‐​term problems. The country still is deeply fractured, divided into two semiautonomous “entities” separated by a monstrosity called the Inter‐​Entity Boundary Line. One entity, the Serb Republic, now almost is entirely Serb. The other, the Muslim‐​Croat Federation, is made up of rival enclaves that maintain a tense coexistence. Nearly 90 percent of the Serbs who lived in the Muslim‐​Croat Federation before 1992 were expelled or have left. 



The prospect for ethnic reintegration is not promising. By the end of 1997, only 19 percent of Bosnia’s 2.3 million refugees and displaced persons had returned home. Moreover, the total number of returnees in 1998 is expected to be only 11 percent of that of 1997. Even more telling, during the last three years only 55,000 Bosnians have returned to areas where they would be in the minority. At the same time, 80,000 Bosnians have moved from areas where they were in the minority to areas where they would be in the majority. That means there are 25,000 fewer Bosnians living in integrated communities today than when Dayton was signed three years ago. Moreover, 85 percent of Bosnians recently polled still say they will not vote for a candidate from another ethnic group. 



Nevertheless, the Clinton administration insists that the Dayton agreement will not be adapted to the reality that has existed in Bosnia for three years — de facto ethnic separation. That unwillingness to rethink Dayton is ill‐​conceived. 



But Bosnia’s costs are higher than many realize. It and other noncombat operations around the world have been diminishing U.S. national security by creating an operations tempo that undercuts U.S. military readiness. 



First, it ensures that billions of taxpayer dollars will be spent in vain trying to superimpose an imaginary Bosnia (united) over the real Bosnia (divided). The United States already is paying about half of the costs of the Bosnia peacekeeping operation, which includes 6,900 U.S. combat troops in Bosnia, plus 3,100 support personnel in Croatia, Hungary and Italy. By the end of fiscal year 1999, Washington will have spent $10.64 billion on the mission. 



But Bosnia’s costs are higher than many realize. It and other noncombat operations around the world have been diminishing U.S. national security by creating an operations tempo that undercuts U.S. military readiness. In fact, during the last decade, the U.S. Army has been used in 29 significant overseas operations, compared with 10 during the preceding 40 years. The strain of that pace has shown up in negative trend lines across all military services in a number of readiness categories. 



For example, to relieve the European‐​based units that have carried out most of the Bosnia mission so far, peacekeeping duties have been shifted to Ft. Hood’s First Cavalry Division, one of the premier U.S.-based combat divisions. As one staff member of the House National Security Committee observed: “The Army is disassembling one of its most ready, most fearsome war‐​fighting divisions. The action shows how the requirements of Bosnia are detracting from the military’s ability to do high‐​intensity conflicts.” 



Bosnia and other overseas operations also have caused the U.S. Air Force’s readiness to slip. The units that fly over Bosnia and the Persian Gulf have priority for plane rotation, support equipment and pilots. As a result, fighter squadrons based in the United States are at their lowest readiness level in years. In 1992, 86 percent of U.S.-based fighter jets were designated “mission capable.” In 1998 only 75 percent were. 



Even more worrisome, there is mounting evidence that peacekeeping and other noncombat operations have adversely affected retention of soldiers, sailors and pilots. The Pentagon reports that first‐​term soldiers assigned to peacekeeping in Bosnia generally reenlisted at the same rate as their counterparts stationed elsewhere in Europe last year. But soldiers stationed in Bosnia were offered a tax‐​exempt reenlistment bonus, which artificially inflated their retention rate. The gap in retention rates for midcareer soldiers stationed in Bosnia was more noticeable. They reenlisted at a rate 6.1 percent lower than that of their counterparts stationed elsewhere in Europe. 



Or take the Air Force. Since 1996, it has performed hundreds of peacekeeping missions in 11 countries, with the Bosnia operation being one of the largest. These mundane and repetitive missions have affected pilot morale negatively because there is no compelling national interest to keep them motivated. 



“We’re not really fighting the country’s wars; we’re just acting like the world’s policeman,” explains one pilot who is a veteran of both Bosnia and Saudi Arabia. This year, nearly 45 percent of eligible Air Force pilots did not renew their service contracts, up dramatically from 14 percent in 1994. Such an anemic retention rate cannot long be sustained without compromising U.S. military readiness. Last year the Air Force had 45 fewer pilots than needed. This year the number has grown to 700, and it’s expected to reach 2,000 by 2002. 



There also is increasing evidence that peacekeeping operations — as distinguished from actual national defense — deter prospective recruits from joining the military. And a strong economy, with plenty of private‐​sector jobs, has made it even tougher for the military to find recruits to replenish its shrinking ranks. 



For fiscal year 1998, both the Navy and the Air Force failed to meet their recruiting goals. The Army was more successful, but only because its recruiting target was lowered significantly. The Navy fell short of its annual recruitment target by 13 percent, and it recently was reported that the Navy has 18,022 fewer sailors at sea than it needs. 



The recruitment problem likely will worsen with the current demographic downturn in the prime recruiting pool: males between the ages of 18 and 21 who are physically fit high‐​school graduates and who scored in the upper half of the military’s standardized entry examination. That population currently consists of 15 percent fewer people than it did in the mid‐​1980s. 



Washington’s unwillingness to revise Dayton also may be paralyzing Bosnian reconstruction. In election after election Bosnian voters cast ballots for nationalist candidates to counterbalance the perceived political power of their ethnic rivals who, in turn, vote for nationalist candidates for the same reason. Such circular logic is built into Dayton because the agreement requires three ethnic groups, each of which fears the political ambitions of the others, to operate under the fiction of a unified state. The political obstructionism and stalemates brought on by upholding that fiction have crippled Bosnia’s efforts to emerge from a communist economy. In fact, three years and $4.35 billion in reconstruction aid later, Bosnia has yet to privatize any part of its economy. 



Ironically, because so much property in Bosnia still is government owned, NATO troops may be paying as much as $40 million a year to rent deployment and storage space from government‐​owned companies in Bosnia. That money is then pocketed by the nationalist party that happens to exercise control over the local or regional government and its institutions. What is puzzling about these payments is the obvious contradiction. NATO allies effectively are subsidizing the very nationalist political parties that Western officials consider the principal obstacles to peace in Bosnia. As the top Western diplomat in charge of implementing the Dayton agreement, Carlos Westendorp has asked, “How can they pay money to these people when we are supposed to be here promoting democracy?” 



A more prudent and viable U.S. policy now would be to convene a “Dayton II” conference that recognizes the reality that has existed on the ground since 1995: a three‐​way partition. That would allow Bosnian Croats, Muslims and Serbs to escape the current atmosphere of perpetual political confrontation and nationalist rancor and concentrate on rebuilding normal lives. The conference could be organized by the European Union, the Organization for Security and Cooperation in Europe, or the Dayton agreement’s Peace Implementation Council to work out the details of formalizing Bosnia’s divisions and to update Dayton’s arms‐​control, demilitarization and human‐​rights provisions accordingly. 



On the military side, arrangements could be made to replace NATO’s current 32,000-strong Stabilization Force with a European Force, or EFOR, to oversee the transition. The EFOR operation could be conducted with Western European Union troops with, perhaps, a prominent eventual role for the Southeast European Brigade, a new regional security initiative being developed by seven NATO and non‐​NATO countries in or near the Balkans. With a few exceptions — such as providing logistics support, cargo airlift and sealift, and space‐​based communications and intelligence — U.S. forces could be extracted from Bosnia before the Dayton agreement’s fourth anniversary in December 1999. 



No doubt critics will point out that this would allow separatism to prevail over multicultural cosmopolitanism. Ideally, the people of Bosnia should enjoy equal rights regardless of religion or ethnic background, and it is tragic that they have refused to uphold that principle. But a multiethnic Bosnia prospering in a climate of liberal toleration is not a realistic expectation; there is simply too much enmity and suspicion on all sides. Sometimes even an ugly divorce is preferable to preserving a futile and destructive marriage — especially when the union is forced. Most important, a negotiated partition is the last, best chance to create a relatively stable environment that will allow for the timely departure of U.S. forces from Bosnia.
"
"
Share this...FacebookTwitter25 Years Ago
Challenger 1986
The story of NASA’s (2nd) worst disaster. There was a “consensus” to launch.
 




<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->







They will never be forgotten.
Share this...FacebookTwitter "
"From the moment world leaders claiming to want to fight climate change arrived in private jets, the 2019 World Economic Forum in Davos attracted controversy. With global inequality growing and the threat of environmental destruction looming ever larger, the jets are getting larger and more expensive. The director of one private charter company says surging demand for his planes is partly down to “business rivals not wanting to be seen to be outdone by one another”. This contradiction between rhetoric and action goes far beyond the use of private aircraft. It reveals the broader problems of allowing billionaires to not only control a disproportionate amount of the world’s wealth but also shape its political and economic agenda. While recognising the need for change, their solutions will almost always lead to a defence of the status quo from which they profit so handsomely. Davos is just the tip of the iceberg of the more pervasive problem of “charitable billionaires”. On the surface, it would appear that having “the 1%” direct their wealth toward worthy causes is both laudatory and necessary. But such philanthrocapitalism promotes market-friendly policies and devalues the ability of democratic governments to provide social welfare or meet the needs of their citizens. As one critic noted:  the problems capitalist philanthropists claim to be solving are rooted in the same economic system that allows them to generate such enormous wealth in the first place. An obvious answer may then be to simply dismiss the World Economic Forum and Davos altogether. Stopgap solutions such as seeking to address economic insecurity through employee “mindfulness and well-being” could be mocked, at best.  However, the ethos of Davos still has much to offer. It represents an effort to bring together global leaders and influencers to address problems that go beyond national borders. If anything, it would appear that we need more of these trans-national spaces for discussing technological innovation and social transformation. For Davos to truly be effective though, it must stop serving as a space for elites to defend their status and power. It must no longer be a forum, for instance, for CEOs like Tim Cook of Apple to defend his company and his executive allies from the legitimate “big tech backlash”. Rather, it should be an opportunity to force billionaires to invest in ambitious progressive solutions for solving the very problems they are primarily responsible for causing. Climate change is a crucial place to start. At the moment, the most that pro-environmental voices such as New Zealand PM Jacinda Ardern can do is plead with elites to “get on the right side of history and embrace ‘guardianship’ of the earth”. Alternatively, Davos could be used to promote a more ambitious green agenda and directly challenge the power of the billionaire class and the politicians who continue to prop them up. Leading up to Davos, US congresswoman Alexandria Ocasio-Cortez made headlines by calling for a “Green New Deal”. It proposes to have the US become a global leader in  “decarbonising” its economy within ten years. Further, it will achieve these aims through massive public investment, a federal jobs guarantee, and a dramatic increase in the taxing of high income. The growing public support for this plan has also been a platform for progressive lawmakers to question the huge US military budget and tax cuts. Why, they ask, are war and wealth being prioritised over the housing, healthcare or the environment? This is a good beginning. Yet it would benefit from having the support of global movements and the capital of global elites. In an alternative world, Davos could be just such a venue for an international commitment to innovation and progressive change. Rather than just having countries fight to tax their richest citizens and corporations, they could also mandate that they have to invest a percentage of their profits in the ideas and agenda proposed by leading experts and activists around the world every year. In this case, it would be for helping to fund countries and communities to put in place a “green new deal”. The roots of such an alternative are already growing in events like the “World Social Forum” which is an attempt to bring together community and political leaders with leading thinkers to imagine a different “world” to the corporate-friendly one supported by the World Economic Forum. Critically, it advocates “glocal” solutions, customised to local conditions. Davos, in this regard, could be a yearly corrective where those most affected by elite policies could put forward the specific solutions for them to rectify it. For this to happen it would mean transforming the very ethos of Davos from one of “idea sharing” to that of democratic accountability and justice. It entails delinking social value and influence from economic wealth and the political influence it buys. Instead, Davos could be an annual opportunity for experts and “the people” to propose the best and most cutting edge ideas for redistributing this wealth to where it is truly needed and can do the most good. Scientists warn that we have only 12 years potentially left to avert a “climate change catastrophe”. It is why we need to create an alternative Davos “for the many, not the few” as soon as possible."
"

My new Cato Policy Analysis, “In Pursuit of Happiness Research: Is It Reliable? What Does It Imply for Policy,” was released today. If you’re wondering why we need long papers about the hazards of happiness research, look no further than Bill McKibben’s new essay in _Mother Jones_: 



According to new research emerging from many quarters, … our continued devotion to growth above all is, on balance, making our lives worse, both collectively and individually. Growth no longer makes most people wealthier, but instead generates inequality and insecurity. Growth is bumping up against physical limits so profound—like climate change and peak oil—that trying to keep expanding the economy may be not just impossible but also dangerous. And perhaps most surprisingly, growth no longer makes us happier.



There’s about five kinds of wrong in this one short passage. One of them is generated by the fact that McKibben is apparently ignorant of the most recent work on happiness — much of which directly contradicts his claim that we are not getting happier with growth. You’ll find the up‐​to‐​date scoop in my new paper. (Here’s a bite‐​sized taste.)   
  
  
If you’re worried about this whole business about measuring happiness and using the results to determine public policy, you’re not alone. Darrin McMahon in the elegant lead essay of this month’s _Cato Unbound_ casts a skeptical eye over the entire enterprise. But in today’s installment, Swarthmore psychologist Barry Schwartz, author of _The Paradox of Choice: Why More Is Less_ comes to the defense of the politics of happiness, and argues (in the McKibben vein), that more wealth can actually make us worse off. Is it true? Tune in to _Cato Unbound_ on Friday when Ruut Veenhoven, Director of the World Database of Happiness, will drop the latest data.   
  
  
Or, if you’re anxious, you can get plenty of secondhand Veenhoven in my paper.
"
nan
"Groundwater is the biggest store of accessible freshwater in the world, providing billions of people with water for drinking and crop irrigation. That’s all despite the fact that most will never see groundwater at its source – it’s stored naturally below ground within the Earth’s pores and cracks. While climate change makes dramatic changes to weather and ecosystems on the surface, the impact on the world’s groundwater is likely to be delayed, representing a challenge for future generations. Groundwater stores are replenished by rainfall at the surface in a process known as “recharge”. Unless intercepted by human-made pumps, this water eventually flows by gravity to “discharge” in streams, lakes, springs, wetlands and the ocean. A balance is naturally maintained between rates of groundwater recharge and discharge, and the amount of water stored underground. Groundwater discharge provides consistent flows of freshwater to ecosystems, providing a reliable water source which helped early human societies survive and evolve.  When changes in climate or land use affect the rate of groundwater recharge, the depths of water tables and rates of groundwater discharge must also change to find a new balance. The time it takes for this new equilibrium to be found – known as the groundwater response time – ranges from months to tens of thousands of years, depending on the hydraulic properties of the subsurface and how connected groundwater is to changes at the land surface. Estimates of response times for individual aquifers – the valuable stores of groundwater which humans exploit with pumps – have been made previously, but the global picture of how quickly or directly Earth’s groundwater will respond to climate change in the coming years and decades has been uncertain. To investigate this, we mapped the connection between groundwater and the land surface and how groundwater response time varies across the world. We found that below approximately three quarters of the Earth’s surface, groundwater response times last over 100 years. Recharge happens unevenly around the world so this actually represents around half of the active groundwater flow on Earth.  This means that in these areas, any changes to recharge currently occurring due to climate change will only be fully realised in changes to groundwater levels and discharge to surface ecosystems more than 100 years in the future. We also found that, in general, the driest places on Earth have longer groundwater response times than more humid areas, meaning that groundwater stores beneath deserts take longer to fully respond to changes in recharge. In wetter areas where the water table is closer to the surface, groundwater tends to intersect the land surface more frequently, discharging to streams or lakes.  This means there are shorter distances between recharge and discharge areas helping groundwater stores come to equilibrium more quickly in wetter landscapes.  Hence, some groundwater systems in desert regions like the Sahara have response times of more than 10,000 years. Groundwater there is still responding to changes in the climate which occurred at the end of the last glacial period, when that region was much wetter. 


      Read more:
      The global race for groundwater speeds up to feed agriculture's growing needs


 In contrast, many low lying equatorial regions, such as the Amazon and Congo basins, have very short response times and will re-equilibrate on timescales of less than a decade, largely keeping pace with climate changes to the water cycle.  Geology also plays an important role in governing groundwater responses to climate variability. For example, the two most economically important aquifers in the UK are the limestone chalk and the Permo-Triassic sandstone.  Despite both being in the UK and existing in the same climate, they have distinctly different hydraulic properties and, therefore, groundwater response times. Chalk responds in months to years while the sandstone aquifers take years to centuries. In comparison to surface water bodies such as rivers and lakes which respond very quickly and visibly to changes in climate, the hidden nature of groundwater means that these vast lag times are easily forgotten. Nevertheless, the slow pace of groundwater is very important for managing freshwater supplies.  The long response time of the UK’s Permo-Triassic sandstone aquifers means that they may provide excellent buffers during drought in the short term. Relying on groundwater from these aquifers may seem to have little impact on their associated streams and wetlands, but diminishing flows and less water could become more prevalent as time goes on.  This is important to remember when making decisions about what rates of groundwater abstraction are sustainable. Groundwater response times may be much longer than human lifetimes, let alone political and electoral cycles."
"

There’s a war on. And it cannot be successfully prosecuted without a delicate combination of U.S. leadership and international cooperation. Anything that threatens either threatens all. Unfortunately, two upcoming environmental issues do both.



Although it seems a lifetime, it was a bit over two months ago when European leaders couldn’t say enough bad things about President Bush, when he wisely said “no” to the United Nations’ Kyoto Protocol on global warming. That was during the last “Conference of the Parties” (COP) to the treaty, in Bonn, Germany. Bush knows the whole thing costs a fortune and, even if climate change is a big deal, the Protocol has no detectable effect on anything but our economy. Hopefully, the events of the last month provide a bit of needed perspective on how big this deal is.



In their obsession with pleasing radical greens (who just got clobbered in a Hamburg election), the euros modified Kyoto in Bonn. The purpose was to get the Japanese to go along, which will satisfy a requirement necessary to make it legally binding. In doing so, they made the climatically inconsequential treaty more irrelevant. They also left out a lot of details that need to be cleaned up at the next COP.



That COP begins on October 29 in, of all places, Marrakech. Last July, the United States intimated that despite our opposition to Kyoto we would attend and maybe pony up a new plan.



Let’s kill this meeting now, before it harms our wartime alliance. All it will do is re‐​open old wounds. And don’t even think about the security issues of getting a few thousand people from the hated West together in an Islamic nation, however friendly and moderate the current leaders might be.



The consequence of alliance‐​building is that we’ve been offering some pretty generous terms, such as debt‐​forgiveness and dropping of economic sanctions. We can only hope that our European allies didn’t make any demands about Kyoto. And if we don’t go to Marrakech, that will be a very good sign that they didn’t.



The second green threat to the war effort is on the home front. Career bureaucrats at the Environmental Protection Agency don’t seem to get it: This is not the time to saddle the nation with new rules that could constrict our energy security.



Right now, they’re busily working away on new regs to restrict a basket of three emissions: sulfur dioxide, nitrogen oxides and airborne mercury. The first two are substantially regulated already, and no one has been able to find one death from the last one. But we know that mercury is toxic, and despite the obvious lack of morbidity and mortality, so the logic goes: We must regulate.



The problem is that while sulfur and nitrogen oxides are already largely scrubbed from coal combustion, apparently the only way to keep the mercury out of the air is to stop burning it. Coal produces over 50 percent of our electricity. We have so much of it in the ground (despite attempts by the Clinton Administration to lock it up, as it did in Utah) that we are called the Saudi Arabia of coal.



In a war, prudence plans for the worst contingencies. One of those is that things can and do go bad, and that we could wind up with an embargo — in reality or de facto — on Middle Eastern oil. A supertanker is a wonderful target for, say, an airliner‐​cum cruise missile. Not likely, you say?



In 1973, oil was embargoed, fuel prices skyrocketed, and soon after, the economy went into a sharp and severe recession. Recall that the shock was so great that one of his first acts as president was for Jimmy Carter to declare that energy security was “the moral equivalent of war.” Now, consider prosecuting a real war in this environment.



In the worst‐​case scenario, if supplies are restricted, the United States can buy more oil — at potentially great expense — from non‐​Arab vendors. But how does it get shipped, once one supertanker blows up? In that world we can formulate alternative fuels out of the strategically inexhaustible coal. No one likes the idea. It’s dirty, but so is war, where a lot of things can happen that no one likes. Let’s not place this industry in harm’s way before such an effort might be required.



Yes, there’s only a small chance of this. But it is larger than the probability of the events of September 11.



The point is simple: It’s time to back off on things like Kyoto and regulations that have the potential to tie our hands in any way. Halfhearted war efforts are for losers.



Finally, for those concerned about ultimate environmental degradation, take solace. Big wars, however noble, leave behind big governments and even bigger alliances, which will be only too eager to impose all kinds of new regulations once the smoke clears away. The residuum from the last one is called the United Nations, which gave us the Kyoto Protocol.
"
"
Share this...FacebookTwitter
Only 8 days to go!
A new paper published today in Atmospheric Chemistry and Physics suggests that the relationship proposed by Henrik Svensmark is supported. Svensmark proposes that changes in the sun’s magnetic field modulate the density of Galactic Cosmic Rays (GCRs) which in turn seed cloud formation on Earth, which changes the albedo/reflectivity to affect Earth’s energy balance and hence global climate. Read more here at WUWT:
Cosmic rays linked to rapid mid-latitude cloud changes, B. A. Laken , D. R. Kniveton, and M. R. Frogley.
This will make the 3rd International Energy and Climate Conference that much more interesting as Svensmark himself will be one of the distinguished speakers, along with Fred Singer and others.

Roster of Speakers
Prof. Dr. Fred Singer, USA, Atmospheric Physicist, Chairman IPCC
Prof. Dr.Dieter Ameling, Former President of Business Union Steel and Chairman Stahl institute VDEh.
Prof. Dr. Robert Bob Carter, Australia, Geologist
Prof. Dr. Vincent Courtillot, France, Geophysicist
Prof. Dr. Karl-Friedrich Ewert, Germany, Geologist
Prof. Dr. Ian Plimer, Australia, Geologisist
Prof. Dr. Werner Kirstein, Germany, Dipl. Physicist & Geography
Prof. Dr. Horst Lüdecke, Germany, Press Spokesman for EIKE
Prof. Dr. Nir Shaviv, Israel, Astrophysicist
Prof. Dr. Henrik Svensmark, Denmark, Atmospheric Sciences
Prof. Dr. Jan Veizer, Canada, Paleo-geologist
Dr. Emmanuel Martin, France, Economist
Dr. Horst Borchert, Germany, Physicist
Dr. Lutz Peters, Germany, Author of Klima 2055
Dipl.-Ing. Michael Limburg, Germany, Vize President EIKE
Dipl. Meteorologist Klaus Puls, Germany
Günther Ederer, Germany, Business Journalist and Film Producer


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Climate conference location
Berlin, Germany
10117 Berlin Mitte, only a few steps away from the S-Bahn and Metro stations Friedrichstraße.
Conference dates and times
Starts Friday 3 December 2010 at 1 p.m. and ends at approx 8 p.m.
Starts Saturday 4 December 2010 at 8 a.m. and ends at approx 4 p.m.
Conference fees
120€ private participant (70€ day ticket)
250€ company representative
220€ for a student sponsored by company
Please book before 26 November 2010. After that a 30€ surcharge gets levied.
Info, registration, tickets
Email: info@berlinmanhattan.org
Fax: (+49) 30 69 20 800 39
More information here: 3rd International Energy and Climate Conference.

Share this...FacebookTwitter "
"

 **The oceans are heating up 40% faster than scientists realized** screamed _Business Insider_ last Saturday (January 12). Two days earlier _The New York Times_ broke the story with “the oceans are heating up 40 percent faster on average than a United Nations panel estimated five years ago.” It’s all from a January 10 article in _Science_ by Lijing Cheng, of the Chinese Academy of Sciences in Bejing, along with three American coauthors, titled “How Fast are the Oceans Warming?”   
  
  
Scary. Not. “40 percent” is a straw man.   
  
  
The subject of all this attention is the change in the heat content of the world’s oceans. This is obviously related to their temperature—something that has proven rather difficult to measure precisely on the centennial scale because of changes in measurement techniques and data sources. (Quants: heat (in joules) divided by the heat capacity (joules required to warm the ocean a degree) gives temperature change).   
  
  
At the outset, it’s important to note that this is not an original research article. It’s a “Perspectives” piece, kind of like a sciency op‐​ed that cites a collection of refereed publications (in this case, with a large number of self‐​citations) that determine the “perspective” of the writers. Quoting from Dr. Roy Spencer’s blog on January 16:   




For those who read the paper, let me warn you: The paper itself does not have enough information to figure out what the authors did…



Further, Spencer notes:   




One of the conclusions of the paper is that Ocean Heat Content (OHC) has been rising more rapidly in the last couple decades than in previous decades, but this is not a new finding, and I will not discuss it further here.   
  
  
Of more concern is _the implication that this paper introduces some new OHC dataset that significantly increases our previous estimates of how much the oceans have been warming._   
  
  
As far as I can tell, **this is not the case**.



The “United Nations panel” in the first paragraph is, of course, its Intergovernmental Panel on Climate Change (IPCC), and in their most recent (2013) science compendium they most certainly did **not** estimate that the heat content of the ocean is 40% less than what it is from Cheng et al’s “perspective.” In that report, they noted five different publications, but found problems with four of them and only conferred credibility upon the highest figure, published by Dominguez et al. in 2008. _The Cheng et al. study is only 11% higher than that, not 40%_. To repeat, the average of the five studies mentioned in the 2013 IPCC report is 40% below the new Cheng et al. figure, but the one that the IPCC found most credible in fact differs from Cheng et al. by only 11%.   
  
  
The 40% figure is therefore a straw man.   
  
  
It’s also noteworthy that the “40 percent” claim is nowhere in the _Science_ Perspective. It’s from a guest post by Cheng et al. in “Carbon Brief,” principally funded by the European Climate Foundation, which describes itself as “a major philanthropic initiative to help Europe foster the development of a low‐​carbon society and play an even stronger international leadership role to mitigate climate change.”   
  
  
_Another Perspective_   
  
  
It is obviously very important to understand historical changes in ocean heat content. Another way to do this would be with the new “reanalysis” data sets, which combine heretofore separate atmospheric observations in the past via a dynamic model. Obviously as one goes further back in time, important data, such as vertical weather balloon soundings drop out, as they did in the 1930s. One important note: the model is modulated with the changes in atmospheric radiation consistent with human emissions of greenhouse gases, ozone, and aerosols, as well as changes in solar radiation.   
  
  
(The relevant paper is by Patrick Layloyaux of the European Center for Medium‐​Range Weather Forecasts, the same people who produce the daily “Euro” model that mid‐​Atlantic forecasters love so much in snow situations. He has 14 co‐​authors, with the majority being from the ECMWF.)   
  
  
Here’s what the ECMWF simulates for the historical heat content (in Joules/​square meter) of the upper 300 meters (984 feet) of the globe’s oceans:   






  
  
  
_Oceanic heat content (joules/​square meter) or the upper 300 meters of the ocean. From Layloyaux et al., 2018._   
  
  
Somehow “ocean heat content as high as it was 75 years ago” isn’t quite so alarming. 
"
"Evidence of the devastating impacts of anthropogenic climate change are stacking up, and it is becoming horrifyingly real. There can be no doubt that the climate crisis has arrived. Yet another “shocking new study” led The Guardian and various other news media this week. One-third of Himalayan ice cap, they report, is doomed. Meanwhile in Australia, record summer temperatures have wrought unprecedented devastation of biblical proportions – mass deaths of horses, bats and fish are reported across the country, while the island state of Tasmania burns. In some places this version of summer is a terrifying new normal. The climate disaster future is increasingly becoming the present – and, as the evidence piles up, it is tempting to ask questions about its likely public reception. Numerous psychological perspectives suggest that if we have already invested energy in denying the reality of a situation we experience as profoundly troubling, the closer it gets, the more effort we put into denying it. While originally considered as a psychological response, denial and other defence mechanisms we engage in to keep this reality at bay and maintain some sense of “normality” can also be thought of as interpersonal, social and cultural. Because our relationships, groups and wider cultures are where we find support in not thinking, talking and feeling about that crisis. There are countless strategies for maintaining this state of knowing and not-knowing – we are very inventive. The key point is that it prevents us from responding meaningfully. We “succeed” in holding the problem of what to do about the climate crisis at a “safe” distance. As the crisis becomes harder to ignore – just consider the current batch of shocking reports – individually and culturally we will dig deeper to find ways to strategically direct our inattention. The standard narrative for a piece like the one I’m writing here, as a social scientist, is to now say something about how the crisis could be better communicated. The billion-dollar question, of course, is whether this most recent disaster can be used to motivate real change. No doubt it is important to keep this kind of commentary up. It is key that we consider how to give the climate crisis traction in a culture so accomplished at distancing us from uncomfortable realities.  But let’s be honest. No one really knows what works. We have never been here before. And I’m starting to think that more of this kind of analysis is, perversely, another example of distancing us from that crisis. Intellectualising terrifying climate crisis stories as an issue for “communicators” and “the public” is another way of detaching ourselves from their reality, from the relevance to me and you. So let’s cut through all that and stop invoking an imaginary audience. Many terrible things are happening as a result of climate change – their happening is being reported. How are you receiving it? How does it feel? Are you shocked, horrified, scared, bored, tired? What do you do with the terror? Do you compartmentalise it somewhere “safe”? Perhaps like me, you know you care. You attach importance to climate change, you want to act correctly, avoid risking other lives, damaging homes and habitats. Perhaps you know you are scared too – scared of contemplating what we have already lost or of what will happen as the crisis gets closer still. Scared of what you are being asked to give up. Add in some residual guilt and you might then engage in a defence of some kind, consciously or otherwise – telling yourself that others are more responsible, there is nothing we can do, everybody else seems to be carrying on as normal. As the crisis deepens, the walls close in, you might double down on those defences. So where do we go from here? How might this knowledge help us – you and me? We must make a commitment, but not of the kind you might imagine. The shocking reality of the climate crisis is making its way into the webs of everyday life, emotions, thought processes, relationships, hopes, dreams and fears. Perhaps we should commit to letting it, as an alternative to doubling down on our denial.  We can do this individually, but more important is collectively acknowledging our fears about actual and anticipated losses. Fears about the loss of species and habitats, but also our established ways of life. This leads to more constructive questions, about what we want to hang on to, what are our obligations? I don’t have ready answers to these questions, but I am still confident we can find ways to keep doing the things we really care about – for ourselves, each other, the places we live in. But we need to talk about these choices. Such a process is still miles apart from many “sustainability” agendas. Halting the climate crisis is still predominately framed as a matter for individual choice and change – use less plastic, cycle to work, fly less. But the behavioural response required is way more complicated than that. When it comes to the climate crisis, the personal is political. I am talking about a politics that grows from opposition and critique of our current systems. This is evident in young people organising school strikes and protesters willing to get arrested for their direct action. But we also need to pay more attention to what is lost, to who and what we care for, to other possible ways of being.  Some conservation scientists, at least, see recent cultural change as a hopeful sign of a growing sense of care and responsibility. So stop feeling guilty, it’s not your fault. Be attentive to what’s going on, so that you might notice what you care about and why. What are you capable of, and what might we be capable of together, when we aren’t caught between knowing and not knowing, denial and distress?  See what obligations emerge. There are no guarantees. But what else do we do?"
"

from the Calgary Herald: Canadian mini-satellite may solve carbon puzzle (h/t to WUWT reader “Freezedried”)
Tom Spears Canwestnews Service
Friday, February 27, 2009
While NASA lost a $285-million US satellite this week, a Canadian microsatellite that does the same job is chugging along happily in orbit –at 1/1,000th the cost.
The 30-centimetre-long University of Toronto satellite is searching for the “missing” carbon dioxide–the vast amount of Earth’s main greenhouse gas that somehow vanishes each year.
That’s what NASA’s OCO(orbiting carbon observatory) satellite would have done, if it had survived launch on Tuesday. The big difference: Canada built and launched its tiny version for $300,000.
The OCO launched but failed to reach orbit. (see WUWT story here)
The CanX-2 micro satellite, shown slightly smaller than actual size (10 x 10 x 34 cm) 
Details on the hardware are here
Meanwhile, the U of T’s CanX-2 is cruising 700 kilometres above Earth “and functioning really well,” after some glitches that followed its launch last April, said Ben Quine, the director of space engineering at York University–which made an instrument aboard the tiny CanX. Its job, like OCO’s, is to find Earth’s missing greenhouse gas.
“The measurement principle is almost exactly the same as the one for the OCO,”he said. “It’s very sad when you lose a spacecraft, but it also means that we are the only people in orbit with one-kilometre resolution on the ground.”
That means York’s Argus instrument can look at details below. A Japanese satellite does the same job, but can’t look at features less than 10 kilometres wide.
The problem is that where carbon dioxide comes from, and where it is sucked out of the atmosphere, remains poorly understood.
“Clearly, if we’re going to do something about climate change, we need to understand where CO2 is produced and particularly where it’s absorbed.That’s much less clear,” Quine said.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e97edcb62',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The Great Barrier Reef could be about to experience its most widespread outbreak of mass coral bleaching ever seen, according to an analysis from the US government’s National Oceanic and Atmospheric Administration. But the analysis, seen by Guardian Australia, says while bleaching could hit the entire length of the world heritage-listed reef, the impacts may not be as intense as previous major outbreaks.  Pockets of bleaching are being seen in areas including Lizard Island, north of Cooktown, and more than 1,100 kilometres south-east at Heron Island, off Gladstone. On Thursday, the Great Barrier Reef Marine Park Authority, conservationists and scientists warned that heat stress was building along most of the 2,300-kilometre reef along Queensland’s north coast. Temperatures would need to drop from current levels over the next two weeks if the reef was to avoid a third mass bleaching event in five years. Dr William Skirving, of Noaa’s Coral Reef Watch, prepared a briefing on the “status of heat stress on the GBR” on 19 February. The agency’s observations and model forecasts suggested that “2020 is likely to be the most extensive coral bleaching event that we have seen so far” on the reef. He told Guardian Australia: “We are pretty confident that it’s likely there will be bleaching right up and down the reef and you will find very few reefs that don’t have some bleaching.” While bleaching would be extensive, the intensity of the heat may not reach previous major bleaching events, Skirving said. In 2016 and 2017, back-to-back bleaching killed about half the reef’s corals. He said: “I’m expecting a bit of mortality but I don’t think this will get to the levels where we have seen large-scale mortality.” Corals bleach if they sit in unusually warm waters for long periods. The algae that provides food and the coral’s colour separate from the animal, leaving behind a visible white skeleton. Severe bleaching can kill some corals, and weaken others. Extreme heat can also kill corals almost immediately. Skirving’s analysis identified a key period from 26 February to 4 March when widespread bleaching was likely to hit. During this time, tides would be weak, meaning there was less mixing of the waters that help dissipate heat through the water column. At the same time, the unusually high ocean temperatures would remain. He said: “We will see temperatures rocket through through that period. But if it does get really bad is yet to be seen – it depends on rain and wind – and neither of those can be precisely predicted.” His analysis said only a significant weather event “such as a cyclone” during that eight-day period would save the reef from widespread bleaching. The world’s oceans have taken up about 93% of the extra energy caused by rising levels of greenhouse gases in the atmosphere, according to the UN’s climate science panel. According to the panel, it had high confidence that tropical coral reefs “are projected to reach a very high risk of impact at 1.2°C [of global warming], with most available evidence suggesting that coral-dominated ecosystems will be non-existent at this temperature or higher.” Prof Ove Hoegh-Guldberg, a marine biologist at the University of Queensland, whose early work helped to explain the link between coral bleaching and climate change, said in the past two weeks, “the risk factors for major bleaching events, like elevated water temperatures” had “increased dramatically”. Skirving said it appeared large-scale bleaching events were happening more frequently on the reef, and listed 1983, 1987, 1998, 2002, 2016, 2017 and “now likely 2020”. While reefs could “bounce back” from bleaching events, there was concern individual reefs would not have enough time to recover between each outbreak. Dr Lyle Vail, director of the Australian Museum’s Lizard Island Research Station, said on Friday there was already bleaching on some corals there. “Alarm bells are ringing, we’re in troubled waters,” he said. Lizard Island was badly hit by bleaching in 2016. Vail added: “The coral recovery was coming along nicely so it’s hard to see it bleaching again, fingers crossed for some cloud cover and rain to cool things down.” Several scientists working at Heron Island and the nearby One Tree Island have also been sharing images of corals beginning to bleach. Dr Tracy Ainsworth, a coral biologist at UNSW, has been with a team of scientists at a research station on Heron Island since 15 January. “We have been recording bleaching in the lagoon and reef flat and some exposed areas of the reef, and that’s concerning. A lot of the reef is fine though in the wave-exposed areas.” I've been on Heron Island for a month measuring community calcification and production with our team @CoralTrace @BillLeggat1 @jesseb3rg @thedivinggoose. Sadly, we've seen corals in the shallows begin to bleach and are monitoring the situation closely @UNSWScience @UON_research pic.twitter.com/su0vSPK0q1 Early signs of #coralbleaching at shallow sites on #MaggieIsland on Saturday. Seawater temperatures peaked at 31.7 C at the @aims_gov_au weather station in Cleveland Bay. #itsgettinghotinhere https://t.co/pCai9FmNvi pic.twitter.com/aBOp66d1VR Several colonies of mostly Acropora showing signs of #coralbleaching in the lagoon of #OneTreeIsland in the Southern #GreatBarrierReef. Observed during fantastic coral field work with Emily Howells and Andy Davis pic.twitter.com/jmUYLOgJMS"
nan
nan
"
Given the thousands of comments made here weekly, I’ve decided to add a new feature to WUWT: Quote of the Week. It will be posted on Sundays.

A commenter on WUWT summed up Earth Hour in a succinct  way:
I will be thinking about the 1.8 billion people on Earth who have no  access to electricity, and how insane they must think we are.
From commenter “007” on the WUWT Poll: What are you going to do for “Earth Hour”? thread.
Anyone that wants to submit a better feature logo that the simple one I cobbled together above is certainly welcome to do so. – Anthony
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e978cbff6',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"The sale of the most polluting fuels burned in household stoves and open fires will be phased out in England from next year to clean up the air, the government has said. Plans to phase out the sale of house coal and wet wood have been confirmed as part of efforts to tackle tiny particle pollutants known as PM2.5, which can penetrate deep into lungs and the blood and cause serious health problems.  Wood burning stoves and coal fires are the single largest source of PM2.5, contributing three times as much of the pollution as road transport, the Department for Environment, Food and Rural Affairs (Defra) said. Sales of two of the most polluting fuels, wet wood and house coal, will be phased out from 2021 to 2023, to give householders and suppliers time to move to cleaner alternatives such as dry wood and manufactured solid fuels. These produce less smoke and pollution, and are cheaper and more efficient to burn, officials said. The environment secretary, George Eustice, said: “Cosy open fires and wood-burning stoves are at the heart of many homes up and down the country, but the use of certain fuels means that they are also the biggest source of the most harmful pollutant that is affecting people in the UK. “By moving towards the use of cleaner fuels such as dry wood we can all play a part in improving the health of millions of people. This is the latest step in delivering on the challenge we set ourselves in our world-leading clean air strategy. “We will continue to be ambitious and innovative in tackling air pollution from all sources as we work towards our goal to halve the harm to human health from air pollution by 2030.” Sales of all bagged traditional house coal will be phased out by February 2021, and the sale of loose coal direct to customers via approved coal merchants will end by February 2023. Sales of wet wood in units of under two cubic metres will be restricted from February 2021, to allow for existing stocks to be used up. Wet wood sold in larger volumes will need to be sold with advice on how to dry it before burning from this date, the government said. Manufacturers of solid fuels will also need to show they have a very low sulphur content and only emit a small amount of smoke. Prof Stephen Holgate, special adviser on air quality at the Royal College of Physicians, said: “We know that air pollution causes significant health issues across the life course. It is key that the government does everything it can to improve the air we all breathe. Today’s announcement on domestic burning is a welcome step forward, and will in time, play a role in reducing the pollution associated with PM2.5.“Inhaling combustion particles from any source is harmful, but more so than ever when it’s directly within your home. Burning coal for heat and power has to stop and strong guidance is needed to insist that if wood is burnt in approved stoves, it is non-contaminated and dry.” John Maingay of the British Heart Foundation said: “Wood and coal burning accounts for 40% of harmful levels of background PM2.5 in the UK, and our research has shown that toxic PM2.5 can enter the bloodstream and damage our heart and circulatory system. “Phasing out sales of coal and wet wood is a vital first step towards protecting the nation’s health from toxic air … however, we must not stop there. Air pollution is a major public health challenge, and it requires an urgent and bold response.” • This article was amended on 24 February 2020 to clarify that the ban applies only in England, not to the whole of the UK."
" Labor has to take the initiative in defending Australia against the dangers of climate change because the summer of catastrophe has highlighted our national vulnerability and because business and the states are now demanding national leadership, according to Anthony Albanese. As revealed by Guardian Australia, the Labor leader will use a speech to a progressive thinktank on Friday to commit the ALP to adopting a net-zero target by 2050 if it wins the next federal election, without the use of carryover credits from the Kyoto period.  While Scott Morrison is holding off from making a commitment to carbon neutrality by 2050, partly because of an internal brawl within the Coalition and partly because the prime minister says Australia should not sign up to targets in the absence of costings, Albanese will say on Friday that adopting net-zero “should be as non-controversial in Australia as it is in most nations”. According to the Labor leader’s speech notes circulated in advance, Albanese will commit Labor to adopting “a real target, with none of the absurd nonsense of so-called carryover credits that the prime minister has cooked up to give the impression he’s doing something when he isn’t”. “That’s not acting. It’s cheating, and Australian’s aren’t cheaters. A Labor government will never use Kyoto carryover credits.” Albanese will point out – as some in the government have noted publicly in recent weeks – that Australia accepted the net-zero pathway when the Coalition signed and ratified the Paris agreement. “This is what the world agreed to in Paris – Australia included.” On Friday the finance minister, Mathias Cormann, confirmed the government “will be finalising a longer-term target in time for COP26” – the next round of climate talks in Glasgow in November. “But what we will do as part of our process is to ensure that the agenda we determine to achieve any such target is environmentally effective and economically responsible,” he said. Cormann accused Labor of committing to targets without setting out the cost, warning that Albanese is “making the same mistake” as former leader Bill Shorten. Earlier, Labor’s climate change spokesman, Mark Butler, told Radio National the opposition would set out a detailed policy about how to achieve targets and its cost “well before” the next election. Butler argued that the cost of reducing emissions should not be divorced from the cost of inaction and noted Melbourne University research had found actions to reduce emissions have a benefit cost ratio of 20 to one. He also noted the CSIRO had found a net zero emissions path “will deliver higher wages and lower energy bills” in modelling relied on by the New South Wales Berejiklian government when it set its target of zero emissions by 2050. In his speech, Albanese will say that “whether the current [federal] government accepts it or not, this goal is fast becoming the reality”. “All states and territories in Australia have already promised to operate in a carbon-neutral way by 2050. “The Business Council of Australia is calling for it. AGL, Santos, BHP, Amcor, BP, Wesfarmers, Telstra and others all agree. Seventy-three countries, including the UK, Canada, France and Germany, many with conservative governments, have already adopted it as their goal. Australia should too.” Albanese will argue the lesson of the summer is preparation can help avert further tragedy, “and only positive, forward-thinking leadership can steer us through”. Courtesy of bushfires that claimed 33 lives and destroyed 3,000 homes, caused more than 1m animal deaths and saw more than 12m hectares burned, Australians have learned “we’re now living in dangerous times”. Albanese will say climate change was a factor in the bushfires, and people touched by the tragedies of the summer now understood Australia had a lot to lose. “But the good news is we also have a lot to gain.” The Labor leader will argue taking action on climate change means “more jobs, lower emissions and lower energy prices”. “We should be a clean-energy superpower, harnessing the wind and sun to spark a new manufacturing boom and power generations of jobs; developing a hydrogen industry; creating manufacturing jobs here in Australia in new industries that provide well-paid jobs. “Instead we have the government talking nonsense that they themselves have dismissed previously.” The shadow cabinet took the decision to lock in behind a net-zero target before parliament resumed for 2020. The decision – Labor’s first significant move on climate policy post-election – comes as the opposition battles internal tensions about abatement targets and the future of coal. Guardian Australia revealed on Thursday that concerns were expressed during the shadow cabinet deliberation about the risks of setting a concrete target in the absence of a roadmap to get there. There were also concerns about how Labor would answer the inevitable questions about the cost of action. Labor has not yet taken a decision about what its interim emissions reduction target will be, and that decision looms as a future flashpoint. The Morrison government will blast Labor for committing to a target without a cost-benefit calculation. But on Thursday, Labor’s Senate leader, Penny Wong, told the ABC the Coalition was not being transparent about the costs of inaction. She said the path set by the Coalition was “a path which will impose, and is imposing, greater costs on Australians than the path of taking action”. “We saw [the costs] over these recent months. We’ve seen them in the tragedies we’ve seen. We’ve seen them in the costs of drought and bushfires and natural disasters. “But there’s more than the human cost. There is an economic cost, and what we know from what many people have told us is that it will cost us far more not to act than to act.”"
"Climate change is one of the few scientific theories that makes us examine the whole basis of modern society. It is a challenge that has politicians arguing, sets nations against each other, queries individual lifestyle choices, and ultimately asks questions about humanity’s relationship with the rest of the planet. The Intergovernmental Panel on Climate Change published its synthesis report on November 2, a document that brings together the findings from the IPCC’s three main working groups. It reiterates that the evidence for climate change is unequivocal, with evidence for a significant rise in global temperatures and sea level over the last hundred years. It also stresses that we control the future and the magnitude of shifting weather patterns and more extreme climate events depends on how much greenhouse gas we emit.  This is not the end of the world as envisaged by many environmentalists in the late 1980s and early 1990s, but it will mean substantial, even catastrophic challenges for billions of people. Greenhouse gases absorb and re-emit some of the heat radiation given off by the Earth’s surface and warm the lower atmosphere. The most important greenhouse gas is water vapour, followed by carbon dioxide and methane, and without their warming presence in the atmosphere the Earth’s average surface temperature would be approximately -20°C.  While many of these gases occur naturally in the atmosphere, humans are responsible for increasing their concentration through burning fossil fuels, deforestation and other land use changes.  Although carbon dioxide is released naturally by volcanoes, ecosystems and some parts of the oceans, this release is more than compensated for through the carbon absorbed by plants and in other ocean regions, such as the North Atlantic. Had these natural carbon sinks not existed, CO2 would have built up twice as fast as it has done. Records of air bubbles in ancient ice show us that carbon dioxide and other greenhouse gases are now at their highest concentrations for more than 800,000 years. The IPCC presents six main lines of evidence for climate change. We have tracked the unprecedented recent rise in atmospheric carbon dioxide and other greenhouse gases since the beginning of the industrial revolution. We know from laboratory and atmospheric measurements that greenhouse gases do indeed absorb heat when they are present in the atmosphere. We have tracked significant increase in global temperatures of 0.85°C and sea level rise of 20cm over the past century. We have analysed the effects of natural events such as sunspots and volcanic eruptions on the climate, and though these are essential to understand the pattern of temperature changes over the past 150 years, they cannot explain the overall warming trend. We have observed significant changes in the Earth’s climate system including reduced snowfall in the Northern Hemisphere, retreat of sea ice in the Arctic, retreating glaciers on all continents, and shrinking of the area covered by permafrost and the increasing depth of its active layer. All of which are consistent with a warming global climate. We continually track global weather and have seen significant shifts in weather patterns and an increase in extreme events. Patterns of precipitation (rainfall and snowfall) have changed, with parts of North and South America, Europe and northern and central Asia becoming wetter, while the Sahel region of central Africa, southern Africa, the Mediterranean and southern Asia have become drier. Intense rainfall has become more frequent, along with major flooding. We’re also seeing more heat waves. According to the US National Oceanic and Atmospheric Administration (NOAA) between 1880 and the beginning of 2014, the 13 warmest years on record have all occurred within the past 16 years. The continued burning of fossil fuels will inevitably lead to further climate warming. The complexity of the climate system is such that the extent of such warming is difficult to predict, particularly as the largest unknown is how much greenhouse gas we will emit over the next 85 years. The IPCC has developed a range of emissions scenarios or Representative Concentration Pathways (RCPs) to examine the possible range of future climate change. Using scenarios ranging from buisness-as-usual to strong longer-term decline in emissions, the climate model projections suggest the global mean surface temperature could rise by between 2.8°C and 5.4°C by the end of the 21st century. The sea level is projected to rise by between 52cm and 98cm by 2100, threatening coastal cities, low-lying deltas and small islands. Snow cover and sea ice are projected to continue to reduce, and some models suggest that the Arctic could be ice-free in late summer by the latter part of the 21st century. Heat waves, extreme rain and flash flood risks are projected to increase, threatening ecosystems and human settlements, health and security. These changes will not be spread uniformly around the world. Faster warming is expected near the poles, as the melting snow and sea ice exposes the darker underlying land and ocean surfaces which then absorb more of the sun’s radiation instead of reflecting it back to space in the way that brighter ice and snow do. Indeed, such “polar amplification” of global warming is already happening. Changes in precipitation are also expected to vary from place to place. In the high-latitude regions (central and northern regions of Europe, Asia and North America) the year-round average precipitation is projected to increase, while in most sub-tropical land regions it is projected to decrease by as much as 20%, increasing the risk of drought. In many other parts of the world, species and ecosystems may experience climatic conditions at the limits of their optimal or tolerable ranges or beyond. Human land use conversion for food, fuel, fibre and fodder, combined with targeted hunting and harvesting, has resulted in species extinctions some 100 to 1000 times higher than background rates. Climate change will only speed things up. The IPCC synthesis set in stark terms the global challenge of reducing greenhouse gas emissions. To keep global temperature rise below 2°C then global carbon emission must peak in the next ten years and from 2070 onward must be negative: we must start sucking out carbon dioxide from the atmosphere.   Despite 30 years of climate change negotiations there has been no deviation in greenhouse gas emissions from the business-as-usual pathway so many feel keeping the climate change to less than 2°C will prove impossible. The failure of the international climate negotiation, most notably at Copenhagen in 2009, set back meaningful global cuts in emissions by at least a decade.  Anticipation is building for the Paris conference in 2015 and there are some glimmers of hope.   China, now the largest greenhouse gas polluter in the world, has discussed instigating a regional carbon-trading scheme which if successful would be rolled out across the whole country. Meanwhile the US, which has emitted a third of all the carbon pollution in the atmosphere, has placed the responsibility for regulating carbon dioxide emissions under the Environment Protection Agency, away from political wrangling in Washington.  Support and money are also needed to help developing countries mitigate carbon emissions and adapt to inevitable climate change. Trillions of dollars will be invested in energy over the next 15 years to keep pace with increasing demand – what we must do is ensure that it is directed towards developing cheap, clean, secure energy production rather than exploiting fossil fuels. We must also prepare for the worst and adapt. If implemented now, much of the costs and damage that could be caused by changing climate can be mitigated. Climate change challenges the very way we organise our society. It needs to be seen within the context of the other great challenges of the 21st century: global poverty, population growth, environmental degradation, and global security. To meet these challenges we must change some of the basic rules of our society to allow us to adopt a much more global and long-term approach and in doing so develop a solution that can benefit everyone."
nan
"
The Zogby poll results mirrors the recent Gallup poll It’s the economy, stupid. Even so, with opinion on Cap and trade in the minority it seems plans are in place to move forward.
On Earth Day, Secretary Chu warmly embraced the administration’s cap-and-trade proposal, stating, “We must state in no uncertain terms we have a responsibility to our children to curb emissions from fossil fuels…”
Q. President Obama wants to impose cap-and-trade laws that would limit the total carbon dioxide emissions allowed to be released into the environment. These laws would turn carbon dioxide into a commodity allowing those that pollute less to sell credits to those that pollute more. These credits would be traded on commodities markets. According to congressional testimony given by the Director of the nonpartisan Congressional Budget Office, “decreasing emissions would also impose costs on the economy – much of those costs will be passed along to consumers in the form of higher prices for energy and energy intensive goods.” Some have estimated these costs to be $800 to $1300 more per household by 2015. Knowing this, do you support or oppose cap-and-trade laws?

Support                 30%
Oppose                  57%
Not sure                13%
Q. Which course of action should America take with regards to energy
policy?
Make energy cheaper by developing all sources of U.S. energy, including coal, nuclear power, offshore drilling and drilling in the Arctic National Wildlife Refuge                            54%
Reduce America’s production of fossil fuels that might cause global warming                                   40%
Not sure                                                           6%
The O’Leary Report/Zogby poll was conducted April 24-27 of 3,937 voters nationwide and has a margin of error of plus-or-minus 1.6 percentage points. Slight weights were added to party, age, race, gender, education to more accurately reflect the population. Margins of error are higher in sub-groups.
Brad O’Leary is publisher of “The O’Leary Report,” a bestselling author, and is a former NBC Westwood One talk show host. His new book, “Shut Up, America! The End of Free Speech,” is now in bookstores. To see more poll results, go to www.olearyreport.com.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e968b9be0',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterSteffen Hentrich of the German liberal institute presents a comparison of the safety of various sources of energy. Much of the media, many politicians and a host of activists have all told us that nuclear energy is too dangerous to be used by man.
Is this claim founded on solid data and facts, or is it run-away hysteria? You be the judge. Shown are the number of deaths per terawatt-hour of energy produced.

Source: http://nextbigfuture.com/2011/03/lowering-deaths-per-terawatt-hour-for.html
By far, probably to James Hansen’s glee, coal, with its dangerous mining work, is the most dangerous of all. Surprisingly, even hydro, wind, and solar power are far more hazardous than nuclear.
Photovoltaic power generation kills, per terawatt-hour, 11 times more people than nuclear. 
Using the arguments of the anti-nuclear activists, we can say there needs to be an immediate moratorium on windmills, photovoltaic panels and biomass plants. Compared to nuclear, they’re simply too deadly.
So what does all this mean? Hentrich writes:
With this in mind, nuclear energy alone becomes the scapegoat, and this illustrates the glaring denial of reality in politics and in the public. This also shows that a rapid stop of nuclear power generation will in no way reduce the risks involved in power generation.”
Indeed, stopping nuclear power makes the power generation industry much more dangerous, and not safer.
Share this...FacebookTwitter "
"Curious Kids is a series for children of all ages, where The Conversation asks experts to answer questions from kids. All questions are welcome: find out how to enter at the bottom of this article.  Why do we have different seasons at specific times of the year? – Shrey, age nine, Mumbai, India Over the course of a year, the Earth goes on a journey around the Sun. The reason we have seasons is because, during its journey around the Sun, the Earth is tilted.  The Earth’s tilt affects the amount of daylight each hemisphere gets, which in turn makes the temperature hotter or colder.  For example, if you live in the northern hemisphere – that’s north of the equator, like in Europe, USA, or India – then winter happens in December, January and February. That’s when the northern hemisphere is tilted away from the Sun, and the days are shorter.  For anywhere south of the equator, such as Australia or Latin America, it’s summer during these months. That’s because the southern hemisphere is tilted toward the Sun, and the days are longer.  Every season has a middle point. In summer and winter, these midpoints are called solstices. The summer solstice is the longest day, and shortest night, of the year. The winter solstice is the shortest day of the year, and the longest night.  In spring and autumn, the midpoints are called the equinoxes. At the spring and autumn equinoxes, day and night are the same length.  For thousands and thousands of years – right back to the Stone Age – people have known how to work out when the solstices and equinoxes happen throughout the year.  Indeed, they built hundreds of amazing stone circles – like the famous Stonehenge – all over Europe, which marked certain times of the seasons across the year.  These days, we even know how to calculate the seasons on other planets. For example, the next Spring equinox on Mars is on the 23rd March.  To understand how this works, imagine a small ball (representing the Earth) moving around a lightbulb (the Sun) in a circle. Let’s say the ball has a line drawn around the middle, representing the equator. If you have these things at home, you can try this yourself.  As the ball moves around the lightbulb, the half closest to the light will be lit, while the other half will be in darkness. One full circle around the lightbulb represents one full year on Earth.  As you move the ball around the lightbulb, try spinning it between your fingertips, so that the light always shines directly onto the equator.  If the Earth span like this, day and night would be the same length all year round, and there would be no seasons. Now, take that small ball and tilt it at an angle, so that the light from the bulb no longer shines directly on the equator. If you are doing this at home, it might help to colour in either the top or bottom half of the ball.  Now the hemispheres of the ball will get different amounts of light at any one time. The hemisphere tilted away from the bulb gets less light, and the hemisphere tilted towards the bulb gets more.  That means it’s “summer” in the hemisphere tilted towards the lightbulb, and “winter” in the hemisphere tilted away. Keeping the ball at the same angle, move it to the other side of the light bulb. The hemisphere that was tilted away from the bulb is now tilted towards it. So, the hemisphere that was in “winter” when you started moving the ball, is now in “summer”, and the hemisphere that was in “summer” is now in “winter”.  The same thing happens as the Earth moves around the Sun, which is what gives us different seasons at specific times of the year.  Remember, the decrease in sunlight and colder temperatures you get during winter is not because the hemisphere is further away, but because the sun is above the horizon for a much shorter time.  Hello, curious kids! Have you got a question you’d like an expert to answer? Ask an adult to send your question to us. You can: * Email your question to curiouskids@theconversation.com 

* Tell us on Twitter by tagging @ConversationUK with the hashtag #curiouskids, or

* Message us on Facebook. Please tell us your name, age and which town or city you live in. You can send an audio recording of your question too, if you want. Send as many questions as you like! We won’t be able to answer every question, but we will do our best. More Curious Kids articles, written by academic experts: Why has nobody found any life outside of Earth? – Anna, age 12, Sydney, Australia People say that everything is made of molecules. Are feelings made of molecules? Is sound made of molecules? – Claire, age six, Bristol, UK Do cats and dogs understand humans when they make miaowing or barking noises? – Mila, age 11 and Alex, age eight"
"

Pundits, politicians and the press have argued that global warming will bring disaster to the world, but there are good reasons to believe that, if it occurs, we will like it. Where do retirees go when they are free to move? Certainly not to Duluth. 



People like warmth. When weather reporters on TV say, “It will be a great day,” they usually mean that it will be warmer than normal. The weather can, of course, be too warm, but that is unlikely to become a major problem if the globe warms. Even though it is far from certain that the temperature will rise, the Intergovernmental Panel on Climate Change (the U.N. body that has been studying this possibility for more than a decade) has forecast that, by the end of the next century, the world’s climate will be about 3.6 degrees Fahrenheit warmer than today and that precipitation worldwide will increase by about 7 percent.



The scientists who make up this body also predict that most of the warming will occur at night and during the winter. In fact, records show that, over this century, summer highs have actually declined while winter lows have gone up. In addition, temperatures are expected to increase the most towards the poles. Thus Minneapolis should enjoy more warming than Dallas; but even the Twin Cities should find that most of their temperature increase will occur during their coldest season, making their climate more livable.



Warmer winters will produce less ice and snow to torment drivers, facilitating commuting and making snow shoveling less of a chore. Families will have less need to invest in heavy parkas, bulky jackets, earmuffs and snow boots. Department of Energy studies have shown that a warmer climate would reduce heating bills more than it would boost outlays on air conditioning. If we currently enjoyed the weather predicted for the end of the next century, expenditures for heating and cooling would be cut by about $12.2 billion annually.



The cost to Americans of building dikes and constructing levees to mitigate the damage from rising seas would be less than $1 billion per year, an insignificant amount compared to the likely gain of over $100 billion for the American people as a whole.



Most economic activities would be unaffected by climate change. Manufacturing, banking, insurance, retailing, wholesaling, medicine, educational, mining, financial and most other services are unrelated to weather. Those activities can be carried out in cold climates with central heating or in hot climates with air conditioning.



Certain weather‐​related or outdoor‐​oriented services, however, would be affected. Transportation would benefit generally from a warmer climate, since road transport would suffer less from slippery or impassable highways. Airline passengers, who often endure weather‐​related delays in the winter, would gain from more reliable and on‐​time service.



The doomsayers have predicted that a warmer world would inflict tropical diseases on Americans. They neglect to mention that those diseases, such as malaria, cholera and yellow fever, were widespread in the United States in the colder 19th century. Their absence today is attributable not to a climate unsuitable to their propagation but to modern sanitation and the American lifestyle, which prevent the microbes for getting a foothold.



It is actually warmer along the Gulf Coast, which is free of dengue fever, than on the Caribbean islands where the disease is endemic. My own research shows that a warmer world would be a healthier one for Americans and would cut the number of deaths in the U.S. by about 40,000 per year, roughly the number killed on the highways.



According to climatologists, the villain causing a warmer world is the unprecedented amount of carbon dioxide we keep pumping into the atmosphere. As high school biology teachers emphasize, plants absorb carbon dioxide and emit oxygen.



Researchers have shown, moreover, that virtually all plants will do better in an environment enriched with carbon dioxide than in the current atmosphere, which contains only trace amounts of their basic food. In addition, warmer winters and nights would mean longer growing seasons. Combined with higher levels of CO2, plant life would become more vigorous, thus providing more food for animals and humans. Given a rising world population, longer growing seasons, greater rainfall, and an enriched atmosphere could be just the ticket to stave off famine and want.



A slowly rising sea level constitutes the only significant drawback to global warming. The best guess of the international scientists is that oceans will rise about 2 inches per decade. The cost to Americans of building dikes and constructing levees to mitigate the damage from rising seas would be less than $1 billion per year, an insignificant amount compared to the likely gain of over $100 billion for the American people as a whole.



Let’s not rush into costly programs to stave off something that we may like if it occurs. Warmer is better; richer is healthier; acting now is foolish.
"
"

The administration continues to hype global warming despite the larger fish sizzling on Washington’s grill. Every month Vice President Gore holds a press conference with a federal climatologist in tow to show that yet another record has been set. But there’s a story behind this story that the 2,000 environmental journalists out there have somehow managed to miss, despite the fact that all the information they need is available from government sources on the Internet. 



The tawdry misuse of history began, as predicted by many of us, as few days (eight) after New Year’s as were required to pull together enough data to spell W-O-L-F. And seven monthly wolves later, it turns out that Gore is really not talking about the globe’s temperature after all, and the science he’s peddling hasn’t even been peer reviewed. 



Scientists suspected something was funny with the first federal pronouncement, in January, that said that temperatures in 1997 were the warmest ever measured. Other temperature histories, such as those of NASA, showed no such thing. Global satellite readings put 1997 smack in the middle of the range for the last two decades. 



It turns out that the new history cited by Gore was developed for political impact. It was developed, according to a paper released on June 9 by the National Climatic Data Center (NCDC), a unit of the Department of Commerce, because “climate change questions involve a body politic and busy elected officials [shouldn’t that be “one busy elected official”?] attuned to rapid information delivery.” Further, “Timely climatic information provided when there is a maximum of interest [read: when it’s hot] may be the best way” to communicate about climate. 



Scientists suspected something was funny with the first federal pronouncement…that said that temperatures in 1997 were the warmest ever measured…Global satellite readings put 1997 smack in the middle of the range for the last two decades. 



This “history” was never peer reviewed. It was sent around via e‐​mail by NCDC, which explained, “Our methodology was not documented in the open refereed literature. This [memorandum] is an attempt to provide that documentation.” 



Sending an e‐​mail to everyone is not quite the same as peer review. 



Nor was this history a record of global temperature after all. Instead, according to NCDC, it is an “index” that combines three different measures, kind of like putting fruit salad in a blender. 



The three measures were land surface temperatures, which by definition are hardly global; sea surface temperatures taken from ships; and data from a network of buoys whose deployment was begun in the mid‐​1980s. The last two measurements are very different from the first, and in order to create the desired fruit salad, NCDC adjusted the sea surface temperature data up by 25 percent after 1982. That certainly might make things appear to be a bit warmer in recent years! 



In point of fact, the sea surface temperature data are increasingly at odds with air temperatures taken over the ocean. No one knows the reason for this, but the air temperatures just happen to match up perfectly with those recorded by NASA’s satellites, which happen to match up perfectly with the Weather Bureau’s (what it was called before it became a “service”) weather balloons. None of those records shows a lick of global warming in the last 20 years. 



Parenthetically, we might note that recent reports about the satellite data being in error are themselves in error. Annual temperature averages taken by weather balloons look exactly like those measured by the satellites. So the satellite cannot be wrong unless, somehow, thermometers in the 1,125,000 weather balloons launched over the last 20 years have been making exactly the same mistakes in temperature measurement as the satellites. 



In order to reassure all the recipients of their e‐​mail that their new blended‐​index approach was a good idea, NCDC observed that the three records jammed together looked an awful lot like NASA scientist James Hansen’s global temperature history. “The match is very good,” they wrote. But the Hansen history does not remove the effect of urban warming, which is known to bias global temperatures by about 0.2 degree. No wonder it’s so hot. 



So NCDC had a choice: Either use sea surface temperature data that disagree with marine air temperatures and data from satellites and weather balloons, or use one of those three mutually agreeable records that all show no warming. Guess which choice they made for “busy elected officials”? 



This isn’t temperature measurement, it’s hot air.
"
"For decades now, GDP has been the standard measure of a nation’s well-being. But it is becoming clear that an economic boost may not be accompanied by a rise in individual happiness. While there are many reasons for this, one important factor is that as nations become richer, environmental features such as green space and air quality often come under increasing threat. The mental health benefits of access to parks or waterfronts, for instance, have long been recognised but more recently researchers have also started to look at the role air pollution can play in our general mental health and happiness. With more tangible outcomes such as health, cognitive performance or labour productivity, the adverse effects of poor air are significant and well-established. The link to infant mortality and respiratory disease is well known, and the World Health Organisation estimates that around 7m deaths are attributable to air pollution each year. But while many people will die and many more will acquire a chronic health condition, focusing on objective indicators such as these may still understate the true welfare cost. This is because there is now good evidence of a direct link between air quality and overall mental health and happiness. This evidence comes from a diverse array of studies in different countries and using different analytical approaches. The most compelling of these studies track the same people over time, and find that changes in the air quality in these people’s neighbourhoods are related to changes in their self-reported happiness. One particularly innovative study looked at what happened when large power plants in Germany were fitted with equipment designed to reduce emissions. Researchers had access to happiness data from a long-term survey of a panel of around 30,000 Germans, and categorised everyone by whether they lived upwind or downwind of a power plant (or nowhere near).  The research found that those downwind underwent a significant improvement in their happiness levels after the installation, while their upwind neighbours did not benefit. This sort of comparison – a natural experiment that would be impossible and perhaps unethical to replicate in a lab – helps to ensure that the improvement in happiness was due to the improvement in air quality as opposed to other factors. Economists and scientists are continually on the lookout for new ways to test the association. One example, recently published in Nature Human Behaviour, comes from China. Researchers looked at the sentiment expressed in 210m geotagged messages on the microblog platform Sina Weibo (a Chinese equivalent to Twitter). Given they knew where these tweets had been sent from, and how happy or sad they were, the researchers were then able to match the tweets to a daily local air quality index, providing a real-time connection between air pollution and happiness. Analysing data from 144 Chinese cities, they found that self-reported happiness was significantly lower on days with relatively higher pollution levels. This study adds to a pile of research which suggests that air pollution can be detrimental to happiness – but we still need more research on why this is. While health is undoubtedly a factor, we know from studies that control for health status that air pollution affects happiness over and above any indirect effects on physical condition. Some possible reasons for the direct link include aesthetics such as haze, smell and even taste, as well as anxiety about personal health or the health of others. Air pollution has also been a focus of several studies on cognitive impairment, but it is still too early to say if it really plays a role in brain health.  Improving the well-being of citizens remains an obvious and important aim of public policy. To date, the principal focus has been on material well-being but many social scientists and indeed policy makers now argue that we need to take account of how people think and feel about the quality of their life. This is not to ignore material factors like income or physical health. Rather, a comprehensive picture of societal well-being needs to integrate objective indicators with subjective measures like happiness. Doing so will help ensure that we take account of the total cost of environmental degradation such as air pollution. And we will all be better off as a result."
"
 Part II: Where does global warming rank among future risks to public health?

Guest essay by Indur M. Goklany
In Part 1, we saw that at present climate change is responsible for less than 0.3% of the global death toll. At least 12 other factors related to food, nutrition and the environment contribute more. All this, despite using the World Health Organization’s scientifically suspect estimates of the present-day death toll “attributable” to climate change,
Here I will examine whether climate change is likely to be the most important global public health problem if not today, at least in the foreseeable future.
This examination draws upon results generated by researchers who are prominent contributors to the IPCC consensus view of climate change.  I do this despite the tendency of their analyses to overstate the net negative impacts of climate change as detailed, for instance, here, here and here.
Specifically, I will use estimates of the global impacts of climate change from the British-government sponsored “Fast Track Assessments” (FTAs) which have been published in the peer reviewed literature. Significantly, they share many authors with the IPCC’s latest assessment. For example, the lead author of the FTA’s study on agricultural and hunger impacts is Professor Martin Parry, the Co-Chairman of the IPCC Work Group 2 during the preparation of the IPCC’s latest (2007) assessment.  This Work Group was responsible for the volume of the IPCC report that deals with impacts, vulnerability and adaptation.
I will consider “the foreseeable future” to extend to 2085 since the FTAs purport to provide estimates for that date, despite reservations.  In fact, a paper commissioned for the Stern Review (p.74) noted that “changes in socioeconomic systems cannot be projected semi-realistically for more than 5-10 years at a time.” [Despite this caution, Stern’s climate change analysis extended to at least 2200.]
In the following figure, using mortality statistics from the WHO, I have converted the FTAs’ estimates of the populations at risk for hunger, malaria, and coastal flooding into annual mortality. Details of the methodology are provided here.

In this figure, the left-most bar shows cumulative global mortality for the three risk categories in 1990 (the baseline year used in the FTAs). The four “stacked” bars on the right provide mortality estimates projected for 2085 for each of the four main IPCC scenarios. These scenarios are arranged from the warmest on the left (for the so-called A1FI scenario which is projected to increase the average global temperature by 4.0°C as indicated by the number below the stacked bar) to the coolest on the right (for the B1 scenario; projected temperature increase of 2.1°C).  Each stacked bar gives estimates of the additional global mortality due to climate change on the top, and that due to other non-climate change-related factors on the bottom. The entire bar gives the total global mortality estimate.
To keep the figure simple, I only show estimates for the maximum (upper bound) estimates of the mortality due to climate change for the three risk factors under consideration.
This figure shows that climate change’s maximum estimated contribution to mortality from hunger, malaria and coastal flooding in 2085 will vary from 4%-10%, depending on the scenario.
In the next figure I show the global population at risk (PAR) of water stress for the base year (1990) and 2085 for the four scenarios.

A population is deemed to be at risk if available water supplies fall below 1,000 cubic meters per capita per year.
For 2085, two bars are shown for each scenario. The left bar shows the net change in the population at risk due to climate change alone, while the right bar shows the total population at risk after accounting for both climate change and non-climate-change related factors. The vertical lines, where they exist, indicate the “spread” in projections of the additional PAR due to climate change.
This figure shows that climate change reduces the population at risk of water stress! This is because global warming will decrease rainfall in some areas but serendipitously increase it in other, but more populated, areas.
The figure also suggests that the warmest scenario would result in the greatest reduction in net population at risk.
[Remarkably, both the IPCC’s Summary for Policy Makers and the original source were reticent to explicitly point out that climate change might reduce the net population at risk for water stress. See here and here (pages 12-14 or 1034-1036).].  Thus, through the foreseeable future (very optimistically 2085), other factors will continue to outweigh climate change with respect to human welfare as characterized by (a) mortality for hunger, malaria and coastal flooding, and (b) population at risk for waters stress.
In the next post in this series, I will look at a couple of ecological indicators to determine whether climate change may over the “foreseeable future” be the most important problem from the ecological perspective, if not, as we saw here, from the public health perspective. 



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e969a1285',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The oil and gas industry has had a far worse impact on the climate than previously believed, according to a study indicating that human emissions of fossil methane have been underestimated by up to 40%. Although the research will add to pressure on fossil fuel companies, scientists said there was cause for hope because it showed a big extra benefit could come from tighter regulation of the industry and a faster shift towards renewable energy.  Methane has a greenhouse effect that is about 80 times more potent than carbon dioxide over a 20-year period and is responsible for at least 25% of global heating, according to the UN Environment Programme. In the past two centuries, the amount of methane in the atmosphere has more than doubled, though there has long been uncertainty about whether the source was biological – from agriculture, livestock or landfills – or from fossil fuels. There were also doubts about what share of fossil methane was naturally released and what share was from industry. Earlier estimates were based on intermittent, bottom-up monitoring of oil and gas companies and comparisons with geological evidence from the end of the Pleistocene epoch, about 11,600 years ago. For a more accurate comparison, a team at the University of Rochester in the US examined levels of methane in the pre-industrial era about 300 years ago. This was achieved by analysing air from that period trapped in glaciers in Greenland. The sample – made up of about a tonne of ice – was extracted with a Blue Ice Drill, capable of producing the world’s biggest ice cores. The findings, published in Nature, suggest the share of naturally released fossil methane has been overestimated by “an order of magnitude”, which means that human activities are 25-40% more responsible for fossil methane in the atmosphere than thought. This strengthens suspicions that fossil fuel companies are not fully accounting for their impact on the climate, particularly with regard to methane – a colourless, odourless gas that many plants routinely vent into the atmosphere. An earlier study revealed methane emissions from US oil and gas plants were 60% higher than reported to the Environmental Protection Agency. Accidents are also underreported. A single blowout at a natural gas well in Ohio in 2018 discharged more methane over three weeks than the oil and gas industries of France, Norway and the Netherlands released in an entire year. At the time, the company said it was unsure of the size of the leak. The immense scale was only revealed a year later when scientists analysed satellite data provided by the European Space Agency. Fracking also appears to have worsened the problem. Atmospheric methane had started to flatten off at the turn of the century, but rose again after a surge in fracking activity in the US and elsewhere. The industry, however, continues to claim that the energy source can be used as a “bridge fuel” because it has lower carbon emissions than oil or coal, but this fails to account for leaks and flares of methane and other gases during extraction. For almost three decades, world governments have met every year to forge a global response to the climate emergency. Under the 1992 United Nations Framework Convention on Climate Change, every country on earth is treaty-bound to “avoid dangerous climate change”, and find ways to reduce greenhouse gas emissions globally in an equitable way. Cop stands for conference of the parties under the UNFCCC. The UK will host Cop26 this November in Glasgow. In the Paris agreement of 2015, all governments agreed for the first time to limit global heating to no more than 2C above pre-industrial levels, and set out non-binding national targets on greenhouse gases to achieve that. However, these targets are insufficient, and if allowed to stand would lead to an estimated 3C of heating, which scientists say would spell disaster. For that reason, the Cop26 talks in Glasgow are viewed as the last chance for global cooperation on the emergency, with countries expected to come with tough new targets on emissions. The negotiations will be led by environment ministers and civil servants, aided by UN officials. Nearly every country is expected to send a voting representative at the level of environment secretary or equivalent, and the big economies will have extensive delegations. Each of the 196 nations on earth, bar a few failed states, is a signatory to the UNFCCC foundation treaty. The Cops, for all their flaws, are the only forum on the climate crisis in which the opinions and concerns of the poorest country carry equal weight to that of the biggest economies, such as the US and China. Agreement can only come by consensus, which gives Cop decisions global authority. Fiona Harvey Environment correspondent Growing calls for tighter controls will be strengthened by the new study. The lead author, Benjamin Hmiel, said the paper was cause for optimism because it showed that action on methane – which has a relatively short shelf life, persisting in the atmosphere for about nine years – could give a strong short-term boost to efforts to stabilise the climate. “Placing stricter methane emission regulations on the fossil fuel industry will have the potential to reduce future global warming to a larger extent than previously thought,” Hmiel said. “Methane is important to study because if we make changes to our current methane emissions, it’s going to reflect more quickly.” Other scientists who were not involved in the research concurred there were positive implications in the findings, but only if governments were able to rein in fossil fuel companies, which has not been the case until now. “This indicates that the fossil fuel sector has a much more polluting impact beyond being responsible for the overwhelming majority of carbon dioxide emissions. This is worrying and overall bad news,” said Dr Joeri Rogelj, a climate change lecturer at the Grantham Institute. The good news, Rogelj said, was that measures to prevent leaks, reduce flaring and switch to renewables would be more effective than expected. “What this study shows is that we can have a bigger impact on methane in the atmosphere than earlier thought. This allows us to set climate policy priorities right.” Dave Reay, the executive director of the Edinburgh Centre for Carbon Innovation, said one of the key messages from the study was that the old bottom-up method of measuring methane emissions was “woefully inadequate”. “We knew fossil fuel extraction – including fracking – was a major part of global methane emissions, but this impressive study suggests it is a far bigger culprit in human-induced climate change than we had ever thought,” he said. “If correct, gas, coal and oil extraction and distribution around the world are responsible for almost half of all human-induced methane emissions. Add to that all the carbon dioxide that is then emitted when the fossil fuels are burned, and you need look no further for the seat of the climate emergency fire.”"
"
Share this...FacebookTwitterEd Caryl has submitted another essay, which indicates that CO2 is not as strong a driver as many would like to have us believe.
CO2 is Cool!
By Ed Caryl
The global warming amount if CO2 doubles has been a bone of contention for the last 30 years. The IPCC has settled on figures in the range from 1.5 to 4.5 degrees, with a most probable rise of 3°C. One of the figures widely quoted is based on CO2 rising from 295 ppm in 1900, to 365 ppm in 2000, while temperature rose 0.57 degrees. (For those readers allergic to math formulas, apologies are offered, but keep reading, the result is what is important, and the math is done for you.) Because the CO2 affect is agreed by most to be logarithmic, the formula for the temperature rise if CO2 doubles, when based on CO2 concentration and temperature rise over time, is:
ln2/(ln(CO2 at end of period/CO2 at beginning of period))/(the change in temperature). (ln is the
natural logarithm). Substituting the numbers from above, we get:
 ln2/(ln(365/295)/0.57) = 1.85°C
 This formula was used to calculate the CO2 doubling-temperature (the temperature rise if CO2 in the atmosphere doubles) over a slightly longer period, from 1880 to 2010, using the Law Dome (Antarctic ice samples) and Mauna Loa Hawaii CO2 levels spliced together, and the GISS (Goddard Institute for Space Studies) global surface temperature figures.
The Law Dome and Mauna Loa CO2 data overlap from 1960 to 1978, and agree quite closely over those years, so splicing them seems quite valid. First, here (from the sources above) is the temperature and the CO2 plotted together:

Figure 1. Plot of GISS global temperature anomaly and atmospheric CO2.
The result of CO2 doubling using the above formula with the beginning and end numbers in Figure 1 is:
 ln2/(ln(389.78/290.7)/0.91 = 2.15°C. This is higher, but still less than the IPCC estimate of 3°C.
But what if we take different time periods for the calculation? A shorter period 50-year calculation results in a noisy chart because there is great variation in temperature from year to year, with negative as well as positive temperature changes, and in the years before 1970, the CO2 rise was very slow so the ratios are small numbers. Here is the plot of the CO2-doubling temperature rise using the above formula with a sliding 50-year window beginning with the period from 1880 to 1930 and ending with 1960 to 2010:

Figure 2. CO2 sensitivity over time using a 50-year window applied to the data in Figure 1. The red trace is a 10-year moving average on the calculated sensitivity. The black line is the linear trend.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The calculation results in large positive and occasionally negative numbers when a shorter period is used. Non-CO2 influences are visible, the warming in the late 30s and 40s, and the later cooling and warming again in the 50s 60s and 70s. These are visible in Figure 1, above. The Atlantic Multi-decadal Oscillation (AMO), the Pacific Decadal Oscillation (PDO), and others, cause these temperature cycles. These 60 to 70-year ocean temperature cycles will increase a sensitivity figure based on a 100-year window.
Volcanoes, La Niña, and El Niño events and above described ocean cycles, as well as the solar cycle, affect the plot because they perturb the system away from equilibrium. Anything that adds or subtracts from the temperature is indistinguishable from the affects of CO2. CO2 is not the only thing affecting temperature. Many people have forgotten this simple fact. The plot in Figure 2 is really the total climate sensitivity, not just from CO2.
The figure widely used for CO2 doubling is only stable if the window used is long. It is only accurate if other factors are not pushing the numbers up. If shorter periods are taken, wildly different numbers result, both positive and negative. In recent years, the numbers settle to just over 1.0°C because the CO2 differences over the 50-year windows are getting larger. What is the real Climate Sensitivity? It appears to be less than 1.0°C for CO2 doubling.
Keep in mind that these plots were generated using GISS global surface temperature data. This data has been criticized for including too many sites influenced by urban warming and for being adjusted upward in recent years, and downward for earlier years. This seems to be the case, as any upward bias would tilt the above plot in the upward direction, as is seen at the end of the plot.
In an effort to check for this, the same formula was used on UHA satellite temperature data for the last 31 years. If the window is the 30 years from 1979 to 2009, the result is 1.75°C, if from 1980 to 2010, the answer is 2.05°C. Again, a short window gives a noisy answer. If 2011 is cooler than 2009, the calculation will be less than 1.75°C. Caution must be used when selecting data for these calculations. As an example, a calculation using the window from 1980 to 2008 results in a negative –0.2°C sensitivity.
Here are the temperature and CO2 plots:

Figure 3. Satellite temperature and Mauna Loa CO2 plots.
The sensitivity was plotted using a sliding 20-year window on the 31 years of satellite data available. Here is that result:

Figure 4. CO2 sensitivity using satellite temperature data and a 20-year sliding window.
The plot is very noisy with two points going negative. We will really need 20 or 30 more years of satellite temperature data to see a valid result, but the 30-year window suggests that the sensitivity as measured by the satellite data will still be less than 2°. Keep in mind that many of the ocean cycles, such as the AMO and PDO, are on the order of 60 years in length, and recently have been in their positive phases. The satellite data is only half this long.
In summary, all these plots show that CO2 sensitivity is probably 1°C or less for a doubling of CO2. We will need a few more years of good temperature data to pin that down.
Meanwhile, all the people claiming sensitivities of 3° or more need to calm down. The above plots and calculations rule that out.
A CO2 climate sensitivity of 1°C for CO2 doubling is not very important. Also remember that this figure includes all the supposed positive feedbacks, because if they exist, they have had an influence on the temperature already. Keep in mind that CO2 will probably never double in the atmosphere for the simple reason that we will run out of easily available fossil carbon long before then. But this is fodder for the next article.
Share this...FacebookTwitter "
"
Old Radar Sites In Greenland Show Icecap Growth Over the Years
(And let’s not forget what we’ve learned about the temperature reporting from the DEW line Radar Stations – Anthony)
By Joseph D’Aleo, CCM, AMS Fellow
Though the ice may be melting around the edges of the Greenland Icecap in recent years during the warm mode of the AMO much as it did during the last warm phase in the 1930s to 1950s, snow and ice levels continue to rise in most of the interior. Johannessen in 2005 estimated an annual net increase of ice by 2 inches a year. 

(Above: Recent Ice-Sheet Growth in the Interior of Greenland, Ola M. Johannessen, Kirill Khvorostovsky, Martin W. Miles, Leonid P. Bobylev, Science Express on 20 October 2005 Science 11 November 2005: Vol. 310. no. 5750, pp. 1013 � 1016, DOI: 10.1126/science.1115356)
A Canadian Icecap emailer noted during the cold war there were two massive radar sites built on the Greenland icecap now abandoned. They are called Dye-2 and Dye-3. When built they sat high above the snow, recent pictures show how the snow is building up around them, proving the snow build-up in recent times. This demonstrates this snow accumulation over time.
Dye-2 and 3 were among 58 Distance Early Warning Line radar stations built by America between 1955-1960 across Alaska, Canada, Greenland and Iceland at a cost of billions of dollars. Their powerful radars monitored the skies constantly in case Russia decided to send bombers towards America. After extensive studies in late 1957, the USAF selected sites for two radar stations on the ice cap in southern Greenland. Dye-2 was to be built approximately 100 miles east of Sondrestrom AB and 90 miles south of the Arctic Circle at an altitude of 7, 600 feet, and Dye-3 was to be located approximately 100 miles east of DYE II and slightly south at an elevation of 8,600 feet.
The selected locations for the new radar sites were found to receive from three to four feet of snow fall each year. Since the winds were constantly blowing with speeds as much as 100 mph, this snow accumulation constantly formed large drifts. To overcome this potential problem, it was decided that the Dye sites should be elevated approximately twenty feet above the surface of the ice cap.
Dye 3 was built in 1960. From a distance the structure, with its onion-shaped dome, looks like a Russian orthodox church. Dye 3 was an ice core site and previously part of the DEW line in Greenland.  (The Distant Early Warning (DEW) Line: A Bibliography and Documentary Resource List Arctic Institute of North America, Page 23). As a Distant Early Warning line base, it was disbanded in years 1990/1991. The Dye 3 cores were part of the GISP (Greenland Ice Sheet Project initiated in 1971) and, at 2037 meters, was the deepest of the 20 ice cores recovered from the Greenland ice sheet as part of GISP. Samples from the base of the 2km deep Dye 3 and the 3km deep GRIP cores revealed that high-altitude southern Greenland has been inhabited by a diverse array of conifer trees and insects within the past million years. (Eske Willerslev, et al. (2007) Ancient Biomolecules from Deep Ice Cores Reveal a Forested Southern Greenland Science 317 111-114)
The first image below is  from 1972.

See larger image here.
Here it is in 2006.

See larger image here.
In looking back at the time the sites were abandoned, one console operator lamented “We were very busy during this time and I was sad to see it end. I remember thinking of all the waste,” he said. The site is slowly disappearing into the snow. Its outbuildings are no longer visible and drifting snow will consume it completely one day, but that day appears to be decades away.” Read more here.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99c3a0f3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Guest Post by Steven Goddard

The Third Little Show
Mad dogs and Englishmen go out in the midday sun The Japanese don’t care to, the Chinese wouldn’t dare to Hindus and Argentines sleep firmly from twelve till one But Englishmen detest-a siesta
– Noel Coward – 1931
Persistence is the British trait which kept the Shackleton crew alive and helped England withstand the Nazi’s throughout World War II.  It keeps the Catlin Crew going and kept Lewis Pugh relentlessly paddling his kayak over Arctic Ice towards the pole.   And it is the same trait which keeps the UK Met Office forecasting warm summers year after year.  The Met Office forecast 2007 to be the warmest year ever globally, and a hot summer in the UK. 
Instead it turned out to be a cool summer and the rainiest on record in England.  Similar story for summer 2008 and winter 2008-2009 .  Yet in fine British tradition the Met Office remains undaunted –
The coming summer is ‘odds on for a barbecue summer’, according to long-range forecasts. Summer temperatures across the UK are likely to be warmer than average and rainfall near or below average for the three months of summer.
Chief Meteorologist at the Met Office, Ewen McCallum, said: “After two disappointingly-wet summers, the signs are much more promising this year. We can expect times when temperatures will be above 30 °C, something we hardly saw at all last year.”
The last 30C day in London was July 26, 2006 – that was over 1,000 days ago.  But you have to admire their grit and determination to get the global warming message across to the ignorant British population.
“The definition of insanity is doing the same thing over and over and expecting different results.”
– Attributed to Albert Einstein

Darts anyone?


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9664a685',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterSome of us may be wondering whatever happened to the dana who we all love and miss so much. Well, Lubos Motl at the Reference Frame has a nice little update on the adventures of dana:H/t: Mindert Eiting
Why Dana 1981 Hasn’t Proved…
By Lubos Motl
Dana1981 is a 30-year-old Prius driver and the owner of several other alternative vehicles who has mistakingly received a bachelor degree in astrophysics and a master degree in physics, so he or she became a self-described environmental scientist who is “passionate” about the climate hysteria.
Clearly, such people shouldn’t be admitted as college students because they’re incapable of rational thinking. The presence of people like him dramatically cripples the intellectual atmospheres at the world’s universities…”
Read more!
PS: My advice to you dana is: I wouldn’t mess with Lubos, as he would certainly do physics circles and orbits around you. Dana, you’ve only proved one thing, and you may realize what that is when you get older.
Share this...FacebookTwitter "
"Curious Kids is a series for children of all ages, where The Conversation asks experts to answer questions from kids. All questions are welcome: find out how to enter at the bottom of this article.  Where did the first seed come from? – Alice, age six, Beverley, UK Hi Alice. This is a clever question. As I’m sure you know, plants use seeds to spread their young and make new plants. But plants haven’t always used seeds to do this. Seeds came together bit-by-bit over a really long time, as plants evolved. To understand how this happened, you need to know that all living things change slowly over time, to get better at surviving in their environment – this process is called evolution.  Here’s how it works: when a living thing has a feature which works well, it will be able to live longer and have more young. These young will probably have similar features, thanks to their parents.  Plants started using seeds to spread their young somewhere between 385m and 365m years ago. Before seeds existed, plants had other ways of doing this.  Back then, most plants used spores. Some plants today, such as algae, mosses and ferns, still do. You might have spotted the tiny brownish dots on the underside of fern leaves – these are spores.  Spores are different from seeds in a few ways. A spore is made of just one part – a single cell – while a seed contains many cells, each with different jobs to do.  Another difference is that spores only have one parent plant, while seeds have two.  This means that, after a seed starts sprouting, it can grow into a plant, just like its parents.  But spores have to work a bit harder: once they’ve travelled away from their parent plant, they grow into a little green plate of cells, which scientists call a “gametophyte”. Then, two gametophytes must join together, before they can grow into a plant.  It’s easier for gametophytes to join together when its wet – and that’s why plants that use spores usually need to grow in wet places.  For example, horsetails are a very ancient type of plant, which like to grow along lakes, rivers and ponds: they have very strange spores with four “legs” which help them to move and travel further away.  Scientists believe that an extinct seed fern, called Elksinia polymorpha, was the first plant to use seeds.  This plant had cup-like features, called “cupules”, that would protect the developing seed. These cupules grew along the plant’s branches.  Today, plants with seeds do things a little differently. There are two main types: “angiosperms” and “gymnosperms”.  Angiosperms are flowering plants – their seeds develop inside of fruit, like apples, tomatoes or even rose hips or holly berries.  Gymnosperms, such as pine trees, grow their seeds inside a hard cone.  Seeds have evolved because they are better at helping plants to survive than spores are. For example, seeds contain a food source to help the new plant grow.  They also have a hard coat, which helps them to live longer in different conditions: this means plants with seeds can life in lots of different places, from hot, dry deserts to cool, rainy places.  Seeds are so good at helping plants to spread their young that most plant species on Earth today use seeds.  Hello, curious kids! Have you got a question you’d like an expert to answer? Ask an adult to send your question – along with your name, age and town or city where you live – to curiouskids@theconversation.com. Send as many questions as you want! We won’t be able to answer every question, but we’ll do our best. More Curious Kids articles, written by academic experts: Why do spiders have hairy legs? - Audrey, age five, Melbourne, Australia Why do we have different seasons at specific times of the year? – Shrey, age nine, Mumbai, India How is water made? – Clara, age eight, Canberra, Australia"
"
Share this...FacebookTwitterHere’s one of my favorites – by Joe Walsh.
I’ve added the lyrics to the song – see below.
Some warmists tried to tell us yesterday that climate scientists lead humble, low-paid lives. Some probably do, but the ones we are familiar with don’t live bad at all. This is for them.

 
Warming’s been good to me so far
I have a data center
Know its high price
Don’t really work there
Man is that nice


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




I sponge off NASA
Fudge behind the wall
I have the government
Pay for it all
They say I’m crooked but it’s not on my dime
I’m just hiding the clues at the scene of the crime
Warming’s been good to me so far
My Teraflop
Runs a model and drive
I lost my data
Now I contrive
I have a big lab
Adjust in the back
I delete emails
In case I get hacked
I’m forging new records
My pols can’t wait
They send me money
And tell me I’m great
So I got me an offer
No cold records on the wall
Temperature keeps climbing
Doubt it will fall
Lucky I’m sane after all I’ve been through
(Everybody sing) It’s warm (He’s cool)
I can’t screw up but sometimes I still do
Warming’s been good to me so far
I fly to conferences
Sometimes I see Gore
It’s hard to believe
No one listens no more
It’s tough to cover
This fudging and shame
Everybody’s suspicious
Data’s the same
They say I’m crooked but it works every time
(Everybody sing) Oh yeah (Oh yeah)
I keep adjusting guess I’ll never know why
Warming’s been good to me so far…
Share this...FacebookTwitter "
"
Share this...FacebookTwitterReader Bernd Felsche reminds us of this excellent post by Christoper Hitchens here, which is indeed worth bringing up again today, particularly in light of the dark closed minds that have lately attempted to claim that no matter what the weather does today, man is the culprit.
An identical mindset infected so called leading intellectuals back during the Little Ice Age. As Dr. Baliunas explains, during the Medieval Warm Period, Europe enjoyed a warm climate, which allowed society to flourish.
But around the 15th century, the Little Ice Age began and the weather deteriorated. Agriculture suffered and people starved. The low-point was from 1550 to 1700. The intellectuals back then were sure it was man’s fault. A mass wave a institutionally legalized executions ensued.
Dr. Baliunas (emphasis added):
How unusual was this very intense period of the Little Ice Age? On the afternoon of August 3rd, 1562, a thunderstorm struck Central Europe across a front several hundred km long. After raging for several hours, the storm unleashed a terrific hail that continued until midnight. It destroyed crops. It destroyed vineyards, birds and unprotected horses and cows. Dirists noticed something we hear today. They said for 100 years such a storm had never been seen. The storm was deemed so unusual in this period of superstition, that it had to be unnatural; it had to be supernatural.
Thus superstition and witchcraft bred a precautionary response: Eradicate those responsible for the storm and the period of new storminess. Now it was well known that people could cook weather with the help of Satan. So thus did extreme conditions of the severest part of the Little Ice Age contribute to Europe’s most horrific period of mass executions a witch trials. This was completely legal and it was undertaken, administered by highly educated upper social strata. These were institutionally legalized executions for sorcery.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Sounds very familiar today, doesn’t it? How does it compare to PIK, RC and GISS? Watch this speech by Dr Baliunas. Humanity is always only a step away from with-burning and Holocausts. It has happened time and again. The only thing that saves us, is the strong rule of democracy. How often have we heard leading “intellectuals” question democracy lately? We see many similarities today in the thinking of the upper social strata. “Man is responsible” – especially the deniers.

Share this...FacebookTwitter "
"

Even yellow journalists know it’s a good idea to use the refereed scientific literature as the basis for science stories, so it was disconcerting to see a bona fide green journalist like the Washington Post’s Joby Warrick give a great deal of ink to a nonrefereed speech — not even a paper — delivered in San Francisco by federal climatologist Jonathan Overpeck. 



The reason for all the fuss soon became obvious. At the December meeting of the American Geophysical Union, Overpeck said that the so‐​called Medieval Warm Period was local, not global. In other words, the warming that was so substantial that it allowed the Vikings to colonize Greenland and North America was not created by a general planetary warming. That implies that the cooling that followed — known as the Little Ice Age — was similarly non‐​global; otherwise the Warm Period would have shown up, in comparison with temperatures in succeeding centuries as, well, global warming. 



Overpeck’s speech prompted handsprings of joy from our greener friends. Now, instead of saying that the decade of the 1990s (and, in particular, 1998) is the warmest in 600 years (which goes back to the beginning of the putative Warm Period), they can say it’s the warmest in 1,200 years. This story will be in print on or about January 4, 1999, and allows them to declare that the warm terror is here and higher taxes are needed pronto to stop the burning of fossil fuels. 



Others might say, “big deal, sure am glad that I haven’t spent a lick on heating oil and it’s almost Christmas. Think I’ll go and buy some stuff for the missus.” 



Like Tip O’Neill’s politics, climate is local. 



Still others may correctly deduce that Overpeck has created a big problem for those who warn of impending apocalypse. If he is right (a large IF), then regional climate naturally varies tremendously, whether or not the globe warms. In other words, climate changes so dramatic that they promoted the Viking exploration are simply the way of things. And ditto for their flipside — large regional coolings like the Little Ice Age. That event sent the Rhone Glacier in the Alps some 5,000 feet further downslope than it is today and prompted winter carnivals on the frozen Thames. 



Poignant testimony to the social consequences of this regional swing can be found in Kalaallit Nunaat (the politically correct term for Greenland these days), where masonry churches, once built in pastures, are now encased in ice. While KN’s climate clearly changed in ways that were tremendously important to society at the time of the Vikings, that apparently had nothing to do with global warming or cooling. 



Instead, Overpeck says, those changes occurred as purely internal oscillations of the climate system, with no external global change. If we accept that notion, what does it really mean? 



It means that large, regional climate changes have occurred and will occur whether or not the planet warms. That is the kind of change people and plants care about, because no one can sense the global temperature. Like Tip O’Neill’s politics, climate is local. So those who would seek to impose costs on society to prevent climate change had better demonstrate that warming the planet will make large regional excursions more, not less, likely. 



Recently, I explored this notion in a paper in the refereed journal Climate Research. Relying upon historical data (and explicitly ignoring computer models of climate because of their patent unreality), I found that temperature variability between seasons and between years has significantly declined in the second half of this century. And there have been a few warm years in that period, too. 



So when I looked at the variability as a function of the planet’s annual temperature, I found that the cool years were more variable and the warmer ones less. Conclusion? Warming the planet decreases variability on a year‐​to‐​year scale. Cooling the planet makes things more variable. 



That’s pretty good evidence that what human beings are doing to the climate makes things more predictable and equable than before. 



Want more? When the carbon dioxide concentration of the atmosphere was at its highest level since animals first appeared, the biggest animals in history roamed the earth: dinosaurs. Those beasts required a tremendous amount of vegetation to reach their enormous size. Carnivores, like T. Rex, were supported by the massive herbivores. How many tons of vegetation were ultimately required to feed them, considering it had to pass through huge lunks like Apatosaurus (that’s Brontosaurus to you intellectual dinosaurs)? The toasty earth had to have been greener than casino felt. 



What’s more, when the dinos were around, the climate was so stable that they were cold blooded! They’d probably still be here today, except for the fact that they went extinct when the earth got clobbered by a small asteroid. The asteroid raised a huge cloud of dust and killed them with global cooling, which made the climate more variable, resulting in an undependable food supply. 



Our greener friends might become extinct too, if they tout Overpeck’s findings as good news for their side.
"
"Although streaming remains the most popular way people listen to music, old formats like cassettes and vinyl have both seen an increase in sales. In fact, vinyl has seen a remarkable sales increase of 1,427% since 2007, selling around around 4m LPs in 2018 in the UK alone. Since the popularity of vinyl shows no signs of stopping soon, this means that more non-recyclable discs will be manufactured – which could have a negative impact on the environment. Although album covers are generally made of recyclable cardboard, records were originally made of shellac, before non-recyclable vinyl was used as a replacement. Shellac is a natural resin secreted by the female Kerria lacca bug, which was scraped from trees to produce gramophone records. Since shellac isn’t from fossil fuel-derived feedstock (chemicals, such as ethylene, used to make substances like plastic), its carbon footprint was lower than that of modern records.  Shellac records were brittle and prone to water and alcohol damage though, so PVC plastic records were developed to last longer. In ideal conditions (low oxygen, without movement), discarded PVC could take centuries to decompose. However, the environmental conditions of most landfill sites (which have varying soil acidity and temperatures) can cause discarded PVC albums to leach plasticisers (solvents added to plastics to make them more flexible and resilient). They may even outlive the site itself or escape into the environment as pollutants.  Modern records typically contain around 135g of PVC material with a carbon footprint of 0.5kg of CO₂ (based on 3.4kg of CO₂ per 1kg of PVC). Sales of 4.1m records would produce 1.9 thousand tonnes of CO₂ – not taking transport and packaging into account. That is the entire footprint of almost 400 people per year.   In the 80s, records were replaced by CDs, which promised durability and better sound quality. CDs were made of layered polycarbonate and aluminium, which has slightly less environmental impact than PVC, and are manufactured using less materials than records. However, CDs can’t be recycled because they’re made of mixed materials that are difficult and uneconomical to separate into their component parts for recycling. CDs were also encased in fragile polycarbonate cases, which, despite being a single material, aren’t widely recycled. They also aren’t as indestructible as many people first thought, which meant many ended up in landfills.    As new formats of music emerged throughout the years – first albums, then cassettes, CDs, and now streaming services – there was a cycle of waste and destruction as old technologies were replaced by new ones. But we didn’t move to CDs by choice – it was simply what companies manufactured at the time. While high-quality CDs could last for 50 to 100 years under ideal conditions, this isn’t true for many low-quality, cheap CDs. These were easily damaged by direct exposure to sunlight and heat, warped by fast-changing temperatures, gravity, scratches, fingerprints and smudges – subsequently resulting in them getting thrown out. Current digital technology gives us flawless music quality without physical deterioration. Music is easy to copy and upload, and can be streamed online without downloading. Since our digital music is less tangible than vinyl or CDs, surely it must be more environmentally friendly?  Even though new formats are material-free, that doesn’t mean they don’t have an environmental impact. The electronic files we download are stored on active, cooled servers. The information is then retrieved and transmitted across the network to a router, which is transferred by wifi to our electronic devices. This happens every time we stream a track, which costs energy. Once vinyl is purchased, it can be played over and over again, the only carbon cost coming from running the record player. However, if we listen to our streamed music using a hifi sound system it’s estimated to use 107 kilowatt hours of electricity a year, costing about £15.00 to run. A CD player uses 34.7 kilowatt hours a year and costs £5 to run.  So which is the greener option? It depends on many things, including how many times you listen to your music. If you only listen to a track a couple of times, then streaming is the best option. If you listen repeatedly, a physical copy is best; 
streaming an album over the internet more than 27 times will likely use more energy than it takes to produce and manufacture the same CD.  If you want to reduce your impact on the environment, then vintage vinyl could be a great physical option. For online music, local storage on phones, computers or local network drives keeps the data closer to the user and will reduce the need for streaming over distance from remote severs across a power-hungry network. In a world where more and more of our economy and social relations happen online, records, and other vintage music formats, buck that trend. Instead, the record revival shows us what we want to see in our media and material world more widely – experiences that hold their value and with loving care endure. Older music formats have a sense of importance and permanence attached to them, belonging to us in a way that our virtual purchases simply don’t.  It seems that whatever the format, owning copies of our favourite and most treasured music, and playing them over and over again, might just be the best option for our environment."
nan
"If you take an interest in ethical consumerism and plan to treat someone special this February 14, what dilemmas lie ahead? You might already be conscious of getting child labour and slave-free chocolate, a recycled card, even fair trade gold, and perhaps vintage or conflict-free diamonds if it’s a very special year. But what about your flowers?  This year one of us (Jill Timms) will spend her Valentine’s Day looking at sustainable supply chains in Lake Naivasha, Kenya, where hundreds of flower workers will be recovering from their busiest time of year.  Across the world, 250m rose stems will be produced for the day. Of those exported to the EU, 38% are from Kenya, where flower export values have trebled this decade. Governments in Ethiopia, Tanzania and more recently Uganda and Rwanda, are also pursuing expansion, with flowers now accounting for 10% of East African exports. That part of the world has a natural abundance of heat and space, and lots of available cheap labour. Flowers could help the regional economy to “bloom”. However, there are significant social and environmental challenges, such as the massive population growth around Lake Naivasha which contributes to pollution and has helped cut the lake’s volume in half.  Our own research project on sustainable flowers focuses on stakeholders from different parts of the supply chain. But you definitely have a role to play here too, and it begins with asking questions of the flowers you buy. Here are our top five: Geography matters. Some flowers travel by sea, some cargo plane and others in the hold of passenger jets, all with very different carbon footprints. For instance more than 90% of UK flowers are imported, mostly from the Netherlands, although Kenya and Columbia are increasingly important suppliers. Chemical sprays freeze flowers to extend life, and they often travel via the Dutch flower hub. Historically the Netherlands has been the industry powerhouse, but now works hard to retain this in the face of direct supermarket buying, growth in Chinese, East African and South American production, and criticism of the extra “flower miles” involved in transporting via Holland. So provenance is important, but you may struggle to know this. Flowers are not always labelled, labels don’t always specify origin or may list the Netherlands if bought at auction, and bouquets include flowers from multiple sources. Even when the origin is known, things can still be unclear as sustainability issues vary widely by country and flower. Of course, very short supply chains are possible for some varieties (the shortest being from your garden, if you have one). But this sort of localised growing does not satisfy the demand for volume, variety and year-round supply, or indeed guarantee sustainability in terms of energy, pesticide use and so on. In response to ethical concerns, “certification” schemes are becoming more common. Yet we find consumers, florists and even wholesalers are often unaware or misunderstand these, with Fairtrade still being the only one with wider recognition. We are working with bodies including the British Florist Association to educate florists about standards, and wholesalers like Fleurmetz to review how certification can be more visible. You can help by asking your florist if their flowers are certified. If they don’t know, ask to see delivery boxes.  In the UK, about 60% of flowers are bought from supermarkets, with the rest mostly from florists. Supermarkets have their pros and cons. Flowers tend to be better labelled, and they are more likely to cut out the auctions and buy direct from growers, which assures provenance and means they can influence standards. However the supermarkets might not share this information, and their demands on price, volume and the short time from field to market can put inordinate pressure on farms.  In contrast, the demise of the high street, Brexit uncertainty and increased online and supermarket competition, has led to “support your local florist” campaigns. Interestingly, some florists have responded by using sustainability as a selling point. Certifications can help you support farms that claim good practice, but could your purchase also promote development – a familiar argument for global trade? Of course it depends how it is done. For example, the Ethiopian government attracted lots of foreign investment in flower farming. However, incentives included controversial land use agreements that led to civil unrest in 2016, with several foreign-owned flower farms badly damaged or burnt to the ground. There is always a trade-off. Flowers grown in greenhouses in Holland use enormous amounts of energy, but travel less. Lake Naivasha roses enjoy natural heat and light, but are flown many miles and can be chemically treated to survive. So your priorities need to guide your purchase: environmental issues include carbon footprint, chemical use, ecological degradation and water use; social issues include health and safety standards, gender discrimination, precarious employment and land rights.  Accordingly you might choose locally-grown seasonal or organic  flowers, or seek growers who support community development or rights for women workers. Eco-florists such as Wild and Wondrous are raising awareness of alternative practices. Take in your own vase to avoid cellophane packaging or ask for reusable and recycled options like StemGem. When presenting your blooms, take inspiration from the #nofloralfoam campaign. Treat your flowers well by refreshing water and trimming, keep them out of heat and sunlight, then recycle as green waste to make their journey worthwhile. St Valentine’s is a day to express our love, so demonstrate yours for people and planet. The supply chains are complex, but our simple advice is to ask questions."
"
Share this...FacebookTwitterOh the irony!
Potsdam is the home of the alarmist Potsdam Institute for Climate Impact Research, headed by Professor Hans-Joachim Schellnhuber and Stefan Rahmstorf, who has been very busy lately trying to convince the rest of the world that the cold is due to the warming, at least that’s what his models are saying (now).
Snowiest December ever
Potsdam is now well on its way to recording its snowiest December since records began in 1893. The Meteorological Station Potsdam Telegrafenberg has been recording a wide variety of weather parameters since 1893, and looking at the records, no December compares to the one they are having now.
Daily snow cover in Potsdam: Source: http://saekular.pik-potsdam.de/2007_en/index.html
As Rahmstorf and Schellnhuber watch crews remove snow every day at the PIK, they really must be cursing all the white stuff that was never supposed to happen.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Some December data at the PIK – records smashed
On December 21 the station had a snow cover of 33 cm, a December record, smashing the old record of 23 cm set 97 years ago in 1913.
This December will have had snow cover 30 of 31 days (No thaw is in sight) – the 2nd most, just behind 1969 (see Figure below), which saw 31 days of snow on the ground. Currently there is 23 cm of snow cover, which was the old record.
Number of days with snow cover in December. Source; http://saekular.pik-potsdam.de/2007_en/index.html
The average so far for this December has been about 16 cm of snow cover. With the current forecast, that too will not change. If anything it may increase. That will make it an all-time record.
It’s been one of the coldest Decembers in Potsdam since records started. Temperatures for this December in Potsdam have averaged near -4°C so far.
The snow has not only been frequent in Potsdam, but all over Germany. Many cities are poised to set new records for most days with snow cover on the ground. Read here.
Share this...FacebookTwitter "
"If you’re committed to the Paris agreement – to keep the increase in global average temperature to well below two degrees above pre-industrial levels, and pursue efforts to limit the increase to 1.5 degrees – then at a minimum, logically, scientifically, you’re committed to net-zero carbon emissions by 2050. So far, at least 77 countries have committed to the target, as has every state and territory in Australia. The fact that prime minister Scott Morrison is pushing back hard against the calls for such a target sends yet another strong signal that his government still denies the need to tackle climate change. Sensing it must be seen to do something, but committed to doing nothing substantive, the government is arguing that investing in technology is the superior pathway to… to… to what? Are billions of dollars of public funds about to be allocated to a strategy that delivers on an unspoken goal? This passion for technology is newfound and insincere. In truth, our government has a long history of undermining climate technologies. In the three years to 2016, the government ripped just shy of $1bn from the Australian Renewable Energy Agency (Arena), the body charged with helping early stage technologies through to commercial launch. The funding of a feasibility study for a coal power station in Collinsville and the foreshadowed gift of $11m to extend the life of the 42 years old Vales Point coal power station in the Hunter, demonstrate just how reluctant the Coalition is to let go of last century’s energy technologies. One of the most promising and critical new technologies is the rapid maturation of the electric vehicle, but who can forget the government’s pushback against EVs during last year’s election? pic.twitter.com/GIvJffJ5EJ Last November I visited the Leilac zero carbon cement project Belgium – an exciting project given that cement is responsible for 7% of global emissions, more than twice as much as aviation. The new process captures most of the carbon dioxide that’s ordinarily released to the atmosphere during cement manufacture. The technology, which can be powered by renewable energy, was developed in Bacchus Marsh, Victoria and was lured to Europe on the back of a €12 million grant and a price on carbon. In the alternate universe where Arena and our carbon price weren’t smashed by ideological attacks, that world-changing technology would be proudly Australian made. While there’s plenty of valuable research and development in our future, especially for the difficult to decarbonise sectors of cement, steel and aviation, the truth is that we already have the technology to deal with around 70% of global emissions. The pathway is simple – electrify everything and swap fossil fuels for renewables. These technologies have come down in cost not because of boffins in laboratory coats, but because of innovation born of sustained deployment and ruthless competition. Mike and Annie Cannon-Brooke’s Resilient Energy Collective is a case study for how far we’ve come. In just a handful of weeks the group has put together an emergency power product for restoring power to bushfire affected communities. The solar-powered, battery-backed system can be installed in a single day, and will be rolled out to 100 communities in as many days. The energy supply companies partnering in the project are stunned that the infrastructure is being rolled out in hours not months. Community members are amazed that they’re using solar power at night. Likewise, Aemo, our grid operator, has just released a blueprint for reducing electricity sector emissions by 85%, using existing technologies and without compromising reliability. Industry is champing at the bit to implement such a plan — they just need a minister who believes in the end goal and is committed to resolving the roadblocks. In reality, the call for technology before action is a specious distraction designed to paper over the plan to take no action. The greatest proponent of the frame is Danish political scientist Bjorn Lomborg, one of a small cadre of almost respectable climate obfuscationists. In the lead up to the Copenhagen climate conference in 2009, Lomborg handpicked a panel of ancient Nobel laureates to rank 16 climate solutions. The four proposed carbon tax schemes were ranked dead last, and the top three projects deemed worthy of consideration were “marine cloud whitening”, energy research and development and “stratospheric aerosol insertion”. The top-ranked solution would involve a global fleet of “1,900 unmanned ships spraying sea water mist into the air to thicken clouds” and reflect the sun’s rays back into space. The third solution involves fleets of planes spraying sulphur dioxide into the sky. The chemical would mimic the effects of volcanoes “reacting with water to form a hazy layer … spread around the globe … scattering and absorbing incoming sunlight”. The first three years of the Coalition government focussed on tearing down climate policy. The next three used endless reviews that came to nothing – as intended. In July 2014, Tony Abbott finally made good on his promise to dismantle Australia’s carbon price mechanism, our most effective and efficient climate policy. In doing so, not only did he throw away the best tool we had, he cheated Australian farmers out of earning billions from exporting carbon credits to Europe. In 2015, Abbott managed to slash the renewable energy target – assisted in the background by Angus Taylor, the man now charged with reducing emissions – cutting future activity under the target by 40%. The only half decent action has been the emissions reduction fund, called a fig leaf of a policy by the party’s once and future leader Malcolm Turnbull in 2009, whereby taxpayers, not polluters, buy carbon offsets. To date, the ERF has bought just 50m offsets, which doesn’t even cover the increase in emissions from just the LNG sector during the last 5 years. Now the government is talking about a “technology investment target”, whatever that means. Will we be subjected to another barrage of lies that some magical technology exists to cut coal emissions? Remember CCS and HELE? Hopefully by now we all now know that “clean coal” is as real as healthy cigarettes. If Scott Morrison is genuine about climate action, then sure, he should start by restoring the billion dollars ripped out of Arena. In fact, let’s give them a few hundred million a year to help Australian ideas reach their potential and give us a whole new export sector to replace the inevitable decline in coal exports. We have the resources, people and smarts to position Australia for great success in a carbon-constrained global economy. At this point, the roadblocks to effective and affordable action are social and political, not technological. So here we are again. Another strategy to kick the can down the road. The Finkel review bought the government a year of doing nothing in 2017, as did the national energy guarantee in 2018. The hollow climate solutions package helped the government escape scrutiny in 2019, however the “Black Summer” and the approaching November’s COP26 conference in Glasgow – where countries are expected to lift their commitments in the direction of the Paris agreement’s goals – leave the government with nowhere to hide. Simon Holmes à Court is senior advisor to the Climate and Energy College at Melbourne University and sits on the board of the Smart Energy Council"
"
Share this...FacebookTwitterHat tip: Readers Edition.
Michael Krüger at skeptic site Readers Edition brings our attention to an interview (in German) with skeptic scientist Professor Werner Kirstein of the Institute for Geography at the University of Leipzig on MDR German Public Radio. (Expect MDR to receive hellfire and brimstone for daring to air such blasphemy).

In the interview Professor Kirstein is asked about the cold winters being a sign of warming. He said that it just doesn’t fit, and that years ago the same scientists predicted the opposite, and reminds us what climate experts said 3 or 4 years ago:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In Germany we will rapidly see no more snow in the winter.”
So much for that prediction.
Professor Kirstein also cautioned against placing too much emphasis on 2010 being among the hottest years on record, believing the claim is “a joke” and saying that determining a global average is a tricky business and in the end is only a theoretical value. The radio moderator then attempts to make Kirstein look like a denier in asking:
And so what you are saying is that there is no climate change?”
To which Kirstein answered that indeed there is – from natural causes, citing the extreme temperature variations of the past repeating ice ages. Kirstein disputes the notion that man-made CO2 is driving the climate today, saying CO2 has at best extremely little to do with temperature, and it will not do any good to reduce it. He views political attempts to reduce CO2 as an effort to steer the economy into a certain direction and as a way to earn money.
Near the end of the clip, Kirstein says flat out that it all comes down to a belief – a religion.
Transcript (in German) of the interview, Read here.
Professor Kirstein was also once featured at NTZ not long ago, Read here.
Share this...FacebookTwitter "
"It is not uncommon for advocates of a socialist alternative to capitalism to hear others say “it’s a nice idea but it does not work in practice”, or that the Soviet experience proved, once and for all, that capitalism is the best socio-economic model we can get.  Erik Olin Wright, who died on January 23 2019, begged to differ. Wright devoted his career as a sociology professor at the University of Wisconsin-Madison to breathing new life into the study of alternatives to capitalism. He did so with a consistent concern, namely to harness social scientific reasoning to construct a better socio-economic system. And yet, by the end of his life, Wright had reached a level of international acclaim that few Marxist theorists ever achieve. With the global financial crisis prompting a widespread search for alternatives to capitalism, and ideas he championed such as the universal basic income moving into the mainstream, Wright was a key figure behind the resurgent left on both sides of the Atlantic. His ideas are more relevant now than ever. In 2010, Wright poured his decades of research into one of his major works entitled Envisioning Real Utopias. In this immensely lucid, accessible and important book, Wright made his most comprehensive attempt at formulating a set of ideas and strategies that could liberate humanity from its current social, economic and political restrictions – what we might term an “emancipatory” social science. He constructed it around three main axes: a diagnosis of capitalism, a look at some alternatives to capitalism and a theory of transformation. Here, Wright wished to show the reader why a socialist alternative is not only desirable but also something achievable.  While there are different possible pathways towards the socialist transformation of society, Wright told us, we are best off conceptualising alternatives to the status quo on the basis of the “anti-capitalist potential” of things that actually exist. These “real utopias”, as he called them, include such organisations as workers’ cooperatives and even Wikipedia. They are “real” simply by virtue of existing. And they are “utopian” because the values upon which they rest, along with the practices they uphold, provide insights into an emancipatory alternative that has the potential to be brought into existence. But Envisioning Real Utopias is more than a book. It is, as I see it, akin to a manual for thinking about the practice of socialism. For Wright did not limit himself to demonstrating why capitalism ought to, and indeed can be, superseded by something better. He also highlighted how three core socialist values – equality, cooperation and freedom – can co-exist in practice. As such, he provided the reader with a guide for not merely thinking about emancipation but, also and crucially, for realising it.  Its realisation, Wright thought, would depend on the formation of institutions that can guarantee “social empowerment” or, to put it differently, individuals’ capacity to exert control, collectively, over the ownership and use of economic resources and activities. In a workers’ cooperative, for example, workers are effectively empowered by owning an equal share of the organisation and having an equal voice in decision making.  In the case of Wikipedia, one finds highly collaborative practices between a core of paid employees and volunteers. Decisions, such as what to include in different encyclopedic entries, are the result of a deliberative process between editors (which any member of the public can choose to become), without the need for a body specifically devoted to editorial control.  Wright acknowledged the fact that “social empowerment” could take several forms but insisted that, the higher its degree, the more an economy and society would be regarded as socialist.  His analytical framework can therefore be used to assess the degree of social empowerment that particular organisations have the potential to realise. This is in fact what Wright set out to do with worker cooperatives and Wikipedia. Despite their currently limited capacity to radically change things within a capitalist economy, both provide insights into the kind of institutions capable of guiding society towards greater empowerment.  It is worth remembering here that capitalism began its journey within the margins of the feudal economy – and what Wright provided us with are the analytical resources to understand how socialism could potentially emerge from within the margins of the capitalist economy. Those reading the book will also be in a position to appreciate something that characterised many of Wright’s works, namely the capacity to combine a passionate stance with intellectual rigour. He was indeed critical of capitalism and passionate about emancipation, but also firmly devoted to the task of formulating a robust critical social science. Whether Wright studied inequality, the basic income, or alternatives to capitalism, he would do so with admirable assiduity, clarity and precision. The greatest achievement of Envisioning Real Utopias is, as I understand it, its invaluable role in demonstrating that despite the reigning cynicism about the possibility for change, one need not seek refuge in a utopia in the clouds. Wright’s book is a stark reminder that elements of a vision for the future are often, if not always, found in the present. In his poignant and inspiring blog documenting the days before his death, Wright told us he did not fear death. He has now left us, but his work lives on – a work that inspires us not to fear capitalism’s death and to remain hopeful about socialism’s existence."
"Last year, humans emitted approximately 37 billion tonnes of carbon dioxide into the atmosphere – a disastrous and unsustainable figure. To avoid the worst effects of climate change we could capture some of that carbon as it is released by power plants and store it permanently below ground. Better still, some of that waste carbon dioxide could be converted into useful chemicals or fuel.  These processes are known respectively as “carbon capture and storage” and “carbon dioxide utilisation”, and
both require large amounts of raw materials. As an example, carbon capture can involve running emissions over some metal, which then reacts with (and hence captures) the CO₂ before turning it into a different substance that can be stored or reused. To make a dent in climate change, the amount of metal required would be huge. For example, if 1 gram of a metal, in a metal catalyst, could capture 100 grams of coal-based carbon dioxide emissions (an optimistic scenario), around 1.5m tonnes of this metal would reduce global emissions by just 0.4%. So, although keeping carbon out of the atmosphere is important, it is equally important that we do it in a green and sustainable manner. If large amounts of a metal are ever used to significantly decrease carbon emissions, it must have a sustainable supply so that reserves are not depleted.  Sadly, many technologies seem to be ultimately unsustainable. For instance, a recent study by a team of Japanese scientists, highlighted by the Royal Society of Chemistry, described how a catalyst based on the metal rhenium converts carbon dioxide into carbon monoxide. Carbon monoxide is useful as it can be used to form chemicals and fuels such as hydrogen and methanol. The catalyst is indeed extremely active and can work with carbon dioxide at very low concentrations, but the system is still not ideal. Rhenium is very rare: mostly found in Chile and Kazakhstan, it is estimated to have an abundance of less than 10 parts per billion in the Earth’s crust (equivalent to 0.000001%). To put that in prospective, aluminium is 8 million times more abundant and accounts for about 8% of the Earth’s crust. Rhenium itself is mostly used to make turbine blades in aircraft jet engines. If this metal was employed to tackle climate change globally, resources would decrease and its price would increase. This would have a knock-on effect on industrial manufacturing.  Its low abundance also means that producing this catalyst would be expensive. It is therefore unlikely that a global business model for worldwide rhenium based carbon dioxide utilisation would be pursued. In another study, an American research team created a ruthenium catalyst which could transform carbon dioxide from the air into the fuel methanol. However, ruthenium is also incredibly rare, and would likely encounter the same availability and cost problems. Fortunately, it is possible to develop catalysts that are more sustainable and environmentally friendly. This ties in with the principles of “green chemistry” which has been around since the 1990s and has gone from strength to strength.  I am one of numerous researchers across the globe using relatively abundant, and thus more sustainable, metals for carbon dioxide conversion. Colleagues and I recently developed an aluminium catalyst, for instance. It makes sense to use aluminium as it is one of the most abundant metals in the Earth’s crust and has shown promise in carbon dioxide utilisation.  This catalyst can convert carbon dioxide into cyclic carbonates, commercially valuable products used in batteries, pharmaceuticals and polymers. The catalyst can also be “regenerated” once its reactivity has gone and can be reused multiple times.  But it’s not always straightforward to use more abundant metals and I admit I myself have dabbled in using less sustainable metals. These include chromium, a toxic form of which was the subject of the film “Erin Brockovich”, and platinum, another metal estimated to account for less than 0.000001% of the Earth’s crust.  I used these scarce metals because sustainability is not always a substitute for reactivity. Fundamental chemical differences between rare and abundant elements means that simple substitution will not necessarily create a catalyst. For example, my colleagues found that chromium was more reactive in some cases than aluminium in forming cyclic carbonates.  Researching rare metals is still an interesting area to explore and will lead to new chemical discoveries that abundant metals could not produce. The impressive catalytic activity of the rhenium and ruthenium catalysts must not be ignored.  The massive problem of climate change however means that we have to be more realistic and considerate when it comes to designing catalysts for large-scale industrial application. This is by no means an easy feat. Of course, just using abundant natural materials will not necessarily make our methods greener. A true evaluation of sustainability is tough and involves a complex assessment of the entire process, including factors such as raw materials used, energy required, operation costs and carbon saved. Ultimately, we must divert more effort towards sustainable climate change reduction as soon as possible. As David Attenborough said at the recent COP24 summit in Poland: “If we don’t take action, the collapse of our civilisations, and the extinction of much of the natural world, is on the horizon”."
"I walked my kids to school recently through the pretty North East English village of East Boldon and saw a red cross on a beautiful old rowan tree. I thought it might be the diesel haze of nearby commuter traffic confusing my senses, but no. One email to my local councillors later and my worst fears were realised. The rowan tree, a species which feeds flocks of birds each autumn with its berries and whose only crime is to be old and gnarly, is for the chop – and the birds’ autumn lifeline with it.  In the email to my councillors I pointed out that two of the three trees slated for removal in that particular copse do not need to be cut down, they just need a bit of remedial work. One is a small tree smothered with ivy that simply needs to be stripped away.  The third is dead and I suggested in my email that the grounds team cut it into logs and scatter them around the small woodland – allowing them to rot for the benefit of fungi and insects, instead of their usual annihilation in an industrial grinder.  As a lecturer in ecology with 25 years of experience and over 30 peer reviewed papers published, I expected my suggestions to be taken seriously. While the councillor was prompt to reply and polite, it was made clear that grounds management is none of my business. From my experience in the UK, grounds management teams employed by local councils seem unresponsive to expert advice. They also appear unsure how to reconcile having wild areas which are safe for human use but which remain useful for wildlife.  The small patches of wild and semi-wild areas in cities have been mostly stripped of their value to wildlife by over-intensive grounds management. Anything that has the vague look of irregularity is removed. No wood is allowed to lie on the ground, it must be tidied away. A 2013 report by the Royal Society for the Protection of Birds reviews the dramatic loss of wildlife habitat in the UK in great detail, emphasising the need to improve the semi-wild urban areas we still have for wildlife.   The wildlife value of a particular tree species in cities is often disregarded when a decision is made to remove it. In parks, plant species which are exotic to the UK such as the New Zealand cabbage tree (Cordyline australis) are intentionally planted because no native wildlife can use them, so they are low maintenance. Pansies and other colourful flowering plants used in urban park borders are often useless for pollinators. This is due to modifications made during breeding of such plants which restricts access to the nectar producing organs and sometimes leads to lower rates of nectar production.  Petals of many ornamental plants are bred to enclose the central nectaries – nectar producing organs – and sexual parts of the flower that produce pollen and are of use to pollinators. This breeding gives the flower a beautiful ball shape but limits their use to insects. I suspect such plants are only put in urban parks in the first place to please the aesthetics of pensioners – a key voter demographic for local councils.  Leaves are swept up immediately before their nutrients can return to the ground and the insects that lay their eggs on them are doomed to certain death. Road verges are cut back to the bone several times each year and the clippings are left lying, minimising their use as a habitat for wild flowers.  It doesn’t help that grounds management is often subcontracted to private firms in the UK. In these cases, grounds management is more likely to be insensitive to expert advice as the function is out of the hands of the democratically controlled body and with a private company that needn’t care what the public thinks.  Only a few UK councils that I am aware of are making progress in wildlife-sensitive grounds management. Councils such as Burnley, Dorset, Devon, Cornwall, East Sussex and Bristol have developed comprehensive pollinator action plans and are taking measures like allowing grass to grow longer in public areas, planting native, bee-friendly flowers and mowing verges less frequently.  The irony of it all is that measures to improve public areas for wildlife essentially involve less effort overall. All that really needs to be done is to allow public areas to be a little more unkempt, because unkempt areas are what nature likes. If local councils allow this to happen, the public shouldn’t berate them. We must be sympathetic to having natural and semi-natural areas that don’t look like Tellytubby land.  But I am sceptical. Local councils in the UK seem a long way from taking wildlife seriously. Unfortunately, the focus on human ideas of “neatness” at present dominates any love for nature. I fear that green spaces in urban areas will continue their decline as wildlife habitats."
"

On April 7, I wrote about global warming “hotheads,” who dominate the science profession for a lot of very sound economic, incentive‐​based reasons.



We don’t shell out multimillion‐​dollar grants to people who say something isn’t a problem. Recipients of this largess peer‐​review each other’s papers. There’s a lot of incentive to give a bad review to a manuscript downplaying the issue and to give a great one to the paper describing an upcoming apocalypse.



Anyone who disagrees with this should spend some time reading the “climategate” e‐​mails purloined from the Climate Research Unit at the University of East Anglia in November 2009.



Transport yourself from the biased world of peer‐​reviewed science to the biased world of amateur climatology on the Internet. They really aren’t very different; they’re just symmetrically opposite.



In the Internet world where the flatliners live, there’s no such thing as global warming. Where the hotheads reside, it’s everywhere and all the time. Some flatliners even doubt the whole notion of the greenhouse effect — the recycling of infrared radiation by water vapor and carbon dioxide (and a few other things) — that keeps the lower atmosphere about 60 degrees warmer than it would otherwise be.



Both groups are delusional. Hotheads are _convinced, mind you_ that the Koch Brothers are behind the flatliners, and flatliners are _convinced, mind you_ that there’s a hothead conspiracy coordinated by George Soros. Both view each other as murderers of the world: Hotheads will kill the economy, while flatliners will destroy civilization.



Here is the flatliner Holy Picture:





This is the global surface temperature departure from the 1961–1990 average, also from the CRU at East Anglia. (Despite all the climategate hubbub, this is still the reference standard in the business). It’s obviously flat, giving rise to the flatliner mantra: “No warming in 14 years — the same time in which the greatest increases in atmospheric carbon dioxide occurred.” Or, put in more stark terms, “Global warming is a commie plot.”



The flatline argument happens to be popular because it is occurring now, and few people outside of Aspie climate guys like me can remember the weather more than a year or two back.



Here’s a little secret about global warming. The central tendency of computer models using input that pretty much mimics the observed changes in carbon dioxide is to produce a constant (not an increasing) rate of warming.



There are many reasons for this. The response of surface temperature to an increment of carbon dioxide is logarithmic. So for every part per million (ppm) increase, a little less warming is generated. But the actual increase in its atmospheric concentration is a low exponent. When we started monitoring it at Mauna Loa in 1957, CO2 was growing at about 0.75 ppm annually. Now it’s growing at around 2 ppm.



The addition of a logarithmic response to an exponential increase in the cause of something can in fact add up to a straight line, which is very obvious in a suite of the temperature projections made by our friends at the U.N.



There are two warming periods in our recent history. One, in the early 20th century, could not have been caused by carbon dioxide, because we simply hadn’t put very much in the air back then. The second one, which begins in the mid‐​1970s, is much more suspicious because it has been accompanied by a cooling of the stratosphere and is accentuated at high latitudes in the Northern Hemisphere and in the winter, which is what one would expect from increasing CO2.



Here’s the East Anglia history since then, with a straight line fit to the data:





The fit of a constant trend to the overall data is striking, despite the fact that indeed there is no net warming in the last 14 years. In fact, fitting any simple curve to the data does no better than the straight line.



So much for the flatliners. They have lived in fortuitous times. Stay tuned for an analysis of the lukewarmers.
"
"There aren’t many corners of the world left untouched by humanity. Recent research has highlighted that just 23% of the planet’s land surface (excluding Antarctica) and 13% of the ocean can now be classified as wilderness, representing nearly a 10% decline over the last 20 years. And more than 70% of what wilderness remains is contained within just five countries.  Researchers from the US and Australia recently produced a global map to illustrate this decline, made by combining data on things such as population density, night-time lights and types of vegetation. The problem with such an approach is that the question of where wilderness begins and ends is not as simple as it may first seem. The data used to map wilderness is often collected in different ways for different parts of the world. For example, some datasets map roads all the way down to farm and forest tracks, while others may only record primary road networks. The definition of how far land has to be from these roads to be classified as wilderness can also vary. Meanwhile, knitting all this data into a single map often leads to compromises that reduce its usefulness, such as not including any blocks of wilderness below a certain size. So while global maps are useful for drawing attention to the attrition of wilderness areas, only the greater detail of national and local maps can really help us understand and respond to the threats that face our remaining wild areas. Scotland is perhaps the country with the most detailed wilderness mapping in the world today. It has been mapped at global, continental, national, regional and local scales, each one showing progressively more detail, and higher levels of accuracy and reliability. The Scottish government has been able to use these maps to define what should count as protected “wild land” in the most effective way. Early maps showed most wilderness was in the uninhabited highlands and suggested there were almost no wild areas around the main cities of Glasgow and Edinburgh. But by zooming in and reducing the size threshold of what counted as wilderness, the government identified smaller areas of wild land nearer to cities that are just as important for recreation, and landscape, habitat and ecosystem conservation. China is following suit with a similar approach and using national level mapping to define wilderness areas and help develop a new national park system. The country can be neatly divided in two as highlighted by what’s known as the “Hu Line”, a simple straight line that connects Ai-hui in the north-east to Teng-Chong in the south-west. East of this line, the country is densely populated and intensively farmed. To the west, human population is sparse and the land remains largely wild. Chinese geographers are now developing methods to cope with this marked polarity in the distribution of the country’s wilderness. As with Scotland, they need to identify those smaller pockets of wild ecosystems that remain within the otherwise fragmented and developed landscapes of the east. One thing that wilderness maps are particularly good at illustrating is how wild land is being lost to the demand for food, fuel, water, timber and minerals as the human population increases. Maps show that this mainly happens through the road construction associated with logging, oil and gas and mineral extraction. Images of the ongoing fragmentation of the Amazon rainforest provide a good example of how roads, once constructed, open up the landscape for agriculture. Despite the problems of global wilderness maps, there have been some attempts to overcome the impact of cross-border assumptions and inconsistencies. The variations in wilderness quality have been consistently mapped across Europe as part of an EU project to develop a register of the EU’s remaining wilderness areas. One thing that this map highlights is just how common it is to find wilderness areas at more northern latitudes that are too cold and dry for agriculture or forestry and at high altitudes where the land is too rugged to work. So we shouldn’t be surprised to see a similar pattern on the global map.  The scale of these kind of maps affects both the patterns we see and how we understand wilderness destruction. This in turn influences how we might respond to and manage the threats to the world’s remaining wild areas. While global maps grab the headlines, they also risk masking the detail in the underlying causes and so have limited use. They may be great for highlighting the problem, but should only be a starting point to encourage us to look deeper and help us appreciate the underlying drivers of these lost wilds."
"In the past few years, there’s been a resurgence in the idea of foraging for food. The practice of hand gathering plants and animals for bait, money or the table has long taken place, but more recently top chefs have been popularising the idea, while urban foragers have told of the lengths they go to to find wild food in big cities.  But why, in an age where most things we want or need are only a few clicks away, do many seek the thrill of finding their own food? Why do local commercial gatherers choose to pursue these ancient livelihoods when there are less arduous alternative careers?  Humans by nature are hunter gatherers and have always collected food for sustenance. Over the centuries we have found many uses for different species collected both inland and along the seashore, including bait, medicines, fertilisers and soaps. Some research suggests that the Omega-3 fatty acids Homo sapiens gleaned from foraging shellfish on the seashore is what made us more “intelligent” than other human races.  There are clear benefits to wild harvesting in the modern world. Not only does it cut the forager’s own food costs but there may be health advantages too, not least from the exercise involved. Research has found that various activities, such as childhood beachcombing or practising mindfulness at the seaside, can add value to the coast as a “therapeutic landscape”.  However, there is still little published on the personal values of traditional activities such as foraging. A growing body of work on land-based cottage industries, such as wild blueberry and mushroom picking, has found that the wild product trade can empower communities by allowing them to develop new local industries that they can control. It has also been suggested that foraging should be considered an important ecosystem service, due to the cultural benefits that people gain freely from the land. But this research often does not consider the importance of foraging to individuals, and ignores the work of coastal gatherers who collect different things – seaweed, for example – which may be used for more than just food. While we are beginning to understand the benefits of people’s interactions with nature, one area that researchers are still looking in to is the meaning of the practices to those who forage. Despite numerous mentions in books and newspapers, there has been surprisingly little research into the non-monetary value of these gathering activities, particularly on the seashore. In countries where harvesting traditions are proudly practised, communities have begun to express the important meanings that lie behind their wild harvesting, in order to keep cultural practices alive and to protect their target species. The locals who forage on the Puget Sound in Washington, US, for example, have demonstrated that harvesting is key to the sense of place of some shellfish harvesters, and a sense of belonging for some plant and mushroom pickers. Carrying on these traditions brings together family heritage, personal experiences and social connections. And the very act of foraging deeply connects people to their environment, the traditions of the area, and the practices of their homelands too. In my previous work as a marine ecologist, I worked with bait collectors, cocklers, seaweed pickers, citizen scientists, natural historians, commercial fishermen and recreational anglers across the UK. Every person gathered something different, each passionate about what they did, and all with a story to tell. However, it seemed that the cultural and heritage reasons for foraging weren’t thought about until there was a threat of either wild stocks  declining or new management rules.   I am now looking at what gathering means to foragers in a modern world, and seeking participants to take part in the research. As other researchers have asked of other communities around the world, I am looking in to why people in the UK continue to forage from its seashores. Is there an unspoken, deep meaning to foraging that adds something to the gatherer’s well-being? Or is it simply something done for money or food? In a time when well-being and conservation policy is interested in both the sustainability of society and the environment the modern personal, familial and cultural values associated with the ancient activity of gathering should not be forgotten. We need to tell the stories of gatherers – be they commercial or recreational foragers, young or old."
"
Don sent me his AGU paper for publication and discussion here on WUWT, and I’m happy to oblige – Anthony
Abstracts of American Geophysical Union annual meeting, San Francisco  Dec., 2008
Solar Influence on Recurring Global, Decadal, Climate Cycles Recorded by Glacial Fluctuations, Ice Cores, Sea Surface Temperatures, and Historic Measurements Over the Past Millennium 
Easterbrook, Don J., Dept. of Geology, Western Washington University, Bellingham, WA 98225,
Global, cyclic, decadal, climate patterns can be traced over the past millennium in glacier fluctuations, oxygen isotope ratios in ice cores, sea surface temperatures, and historic observations.  The recurring climate cycles clearly show that natural climatic warming and cooling have occurred many times, long before increases in anthropogenic atmospheric CO2 levels.  The Medieval Warm Period and Little Ice Age are well known examples of such climate changes, but in addition, at least 23 periods of climatic warming and cooling have occurred in the past 500 years. Each period of warming or cooling lasted about 25-30 years (average 27 years).  Two cycles of global warming and two of global cooling have occurred during the past century, and the global cooling that has occurred since 1998 is exactly in phase with the long term pattern.  Global cooling occurred from 1880 to ~1915; global warming occurred from ~1915 to ~1945; global cooling occurred from ~1945-1977;, global warming occurred from 1977 to 1998; and global cooling has occurred since 1998.  All of these global climate changes show exceptionally good correlation with solar variation since the Little Ice Age 400 years ago.
The IPCC predicted global warming of 0.6° C (1° F) by 2011 and 1.2° C (2° F) by 2038, whereas Easterbrook (2001) predicted the beginning of global cooling by 2007 (± 3-5 yrs) and cooling of about 0.3-0.5° C until ~2035.  The predicted cooling seems to have already begun. Recent measurements of global temperatures suggest a gradual cooling trend since 1998 and 2007-2008 was a year of sharp global cooling. The cooling trend will likely continue as the sun enters a cycle of lower irradiance and the Pacific Ocean changed from its warm mode to its cool mode.
Comparisons of historic global climate warming and cooling, glacial fluctuations, changes in warm/cool mode of the Pacific Decadal Oscillation (PDO) and the Atlantic Multidecadal Oscillation (AMO), and sun spot activity over the past century show strong correlations and provide a solid data base for future climate change projections. The announcement by NASA that the Pacific Decadal Oscillation (PDO) had shifted to its cool phase is right on schedule as predicted by past climate and PDO changes (Easterbrook, 2001, 2006, 2007) and coincides with recent solar variations. The PDO typically lasts 25-30 years, virtually assuring several decades of global cooling.  The IPCC predictions of global temperatures 1° F warmer by 2011,  2° F warmer by 2038, and 10° F by 2100 stand little chance of being correct. “Global warming” (i.e., the warming since 1977) is over!

Figure 1.  Solar irradiance, global climate change, and glacial advances. Click to enlarge
The real question now is not trying to reduce atmospheric CO2 as a means of stopping global warming, but rather (1) how can we best prepare to cope with the 30 years of global cooling that is coming, (2) how cold will it get, and (3) how can we cope with the cooling during a time of exponential population increase?  In 1998 when I first predicted a 30-year cooling trend during the first part of this century, I used a very conservative estimate for the depth of cooling, i.e., the 30-years of global cooling that we experienced from ~1945 to 1977.  However, also likely are several other possibilities (1) the much deeper cooling that occurred during the 1880 to ~1915 cool period, (2) the still deeper cooling that took place from about 1790 to 1820 during the Dalton sunspot minimum, and (3) the drastic cooling that occurred from 1650 to 1700 during the Maunder sunspot minimum. Figure 2 shows an estimate of what each of these might look like on a projected global climate curve.  The top curve is based on the 1945-1977 cool period and the 1977-1998 warm period.  The curve beneath is based on the 1890-1915 cool period and 1915-1945 warm period.  The bottom curve is what we might expect from a Dalton or Maunder cool period.  Only time will tell where we’re headed, but any of the curves are plausible.  The sun’s recent behavior suggests we are likely heading for a deeper global cooling than the 1945-1977 cool period and ought to be looking ahead to cope with it.

Figure 2. Global temperature variation 1900 to 2008 with projections to 2100. Click to enlarge.
The good news is that global warming (i.e., the 1977-1998 warming) is over and atmospheric CO2 is not a vital issue. The bad news is that cold conditions kill more people than warm conditions, so we are in for bigger problems than we might have experienced if global warming had continued. Mortality data from 1979-2002 death certificate records show twice as many deaths directly from extreme cold than for deaths from extreme heat, 8 times as many deaths as those from floods, and 30 times as many as from hurricanes. The number of deaths indirectly related to cold is many times worse.
Depending on how cold the present 30-year cooling period gets, in addition to the higher death rates, we will have to contend with diminished growing seasons and increasing crop failures with food shortages in third world countries, increasing energy demands, changing environments, increasing medical costs from diseases (especially flu), increasing transportation costs and interruptions, and many other ramifications associated with colder climate. The degree to which we may be prepared to cope with these problems may be significantly affected by how much money we waste chasing the CO2 fantasy.
All of these problems will be exacerbated by the soaring human population.  The current world population of about 6 ½ billion people is projected to increase by almost 50% during the next 30 years of global cooling (Figure 2).  The problems associated with the global cooling would be bad enough at current population levels.  Think what they will be with the added demands from an additional three billion people, especially if we have uselessly spent trillions of dollars needlessly trying to reduce atmospheric CO2, leaving insufficient funds to cope with the real problems.

Figure 3. Global population.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a333224',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterThe chances are real, and both are threatening the planet Earth. Apophis and Yellowstone have been appearing in the media lately. The chances of a catastrophic event occurring in your lifetime are higher than you may think.
Illustration of an asteroid impacting the earth. (Source Wikipedia)
Apophis
Russian astronomers are predicting that the asteroid Apophis could collide with the planet earth on April 13, 2036, writes the online Voice of Russia.
Apophis’s length was earlier estimated to be 450 metres, but a better estimate based on spectroscopic observations at NASA’s Infrared Telescope Facility in Hawaii puts it at 350 metres. That’s still a big rock to be hit by.
‘Apophis will approach Earth at a distance of 37,000 – 38,000 kilometers on April 13, 2029. Its likely collision with Earth may occur on April 13, 2036,’ Professor Leonid Sokolov of the St. Petersburg State University said.”
According to Wikipedia, NASA has estimated the energy that Apophis would release if it struck Earth as the equivalent of 510 megatons on TNT. By comparison the impacts of the Tunguska event is estimated to be in the 3–10 megaton range. The 1883 eruption of Krakatoa was the equivalent of roughly 200 megatons, and the Chicxulub impact, believed by many to be a significant factor in the extinction of the dinosaurs, has been estimated to have released about as much energy as 100 million megatons. 
The bad news is that an impact by Apophis would destroy an area of thousands of square kilometres, and seriously disrupt the climate for a few years. The good news is that it would be unlikely to have long-lasting global effects. Also the chances of Apophis actually striking the earth are still remote.
Yellowstone
The other potential natural catastrophe is the Yellowstone super-volcano, reports National Geographic here. Yellowstone’s caldera covers a 40 by 60 kilometer swath of Wyoming, is an ancient crater formed after the last big blast, some 640,000 years ago. The magnitude of an eruption estimated by scientists would be 1000 times more powerful than the 1980 Mount St. Helens eruption of 1980, and would lead to dire consequences for the globe.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




See: “When Yellowstone Explodes“ in National Geographic magazine. Here’s how the last one looked:
Scientists calculate that the pillar of ash from the Yellowstone explosion rose some 100,000 feet, leaving a layer of debris across the West all the way to the Gulf of Mexico. Pyroclastic flows—dense, lethal fogs of ash, rocks, and gas, superheated to 1,470 degrees Fahrenheit—rolled across the landscape in towering gray clouds. The clouds filled entire valleys with hundreds of feet of material so hot and heavy that it welded itself like asphalt across the once verdant landscape.”
The Yellowstone crater today is rising at a record speed, forced up by a huge magma reservoir that is thought to be about 10 km below the surface, see below. It has risen 25 centimeters since 2004. NatGeo writes that roughly 3,000 earthquakes occur in Yellowstone each year.
But between December 26, 2008, and January 8, 2009, there were about 900 earthquakes, and the rate of rise then slowed for a time. Scientists believe the earthquakes may help to release pressure on the magma reservoir below the surface by allowing fluids to escape, and thus relieve some pressure.
Yellowstone super volcano. (Source: Wikipedia)
Yellowstone erupted 3 times in the last 2 .1 million years. The last eruption was about 640,000 years ago. German online FOCUS magazine writes:
 The Yellowstone volcano is considered to be dangerous because on a geological timescale, it is due to erupt.”
Worrisome odds
Yellowstone is not the only super-volcano threatening the planet. A FOCUS map shows 6 others. Although the chances are small that any one in particular will erupt soon – maybe 1 in a 1000, the odds increase to worrisome levels when all the catastrophe possibilities get factored in. If one identifies 10 potential catastrophic events, each with the odds of occurrence being 1 in 1000, then it means the odds of one happening reduce to 1 in a 100. That starts to get worrisome. It means there’s a pretty good chance one catastrophe will occur in the next 100 years.
People who were born just recently have a pretty good chance of witnessing such an event in their lifetime. And the longer the planet goes without a catastrophe occurring, the greater the chances become.
Share this...FacebookTwitter "
"The social media conversation over the climate crisis is being reshaped by an army of automated Twitter bots, with a new analysis finding that a quarter of all tweets about climate on an average day are produced by bots, the Guardian can reveal. The stunning levels of Twitter bot activity on topics related to global heating and the climate crisis is distorting the online discourse to include far more climate science denialism than it would otherwise.  An analysis of millions of tweets from around the period when Donald Trump announced the US would withdraw from the Paris climate agreement found that bots tended to applaud the president for his actions and spread misinformation about the science. The study of Twitter bots and climate was undertaken by Brown University and has yet to be published. Bots are a type of software that can be directed to autonomously tweet, retweet, like or direct message on Twitter, under the guise of a human-fronted account. “These findings suggest a substantial impact of mechanized bots in amplifying denialist messages about climate change, including support for Trump’s withdrawal from the Paris agreement,” states the draft study, seen by the Guardian. On an average day during the period studied, 25% of all tweets about the climate crisis came from bots. This proportion was higher in certain topics – bots were responsible for 38% of tweets about “fake science” and 28% of all tweets about the petroleum giant Exxon. Conversely, tweets that could be categorized as online activism to support action on the climate crisis featured very few bots, at about 5% prevalence. The findings “suggest that bots are not just prevalent, but disproportionately so in topics that were supportive of Trump’s announcement or skeptical of climate science and action”, the analysis states. Thomas Marlow, a PhD candidate at Brown who led the study, said the research came about as he and his colleagues are “always kind of wondering why there’s persistent levels of denial about something that the science is more or less settled on”. The researchers examined 6.5m tweets posted in the days leading up to and the month after Trump announced the US exit from the Paris accords on 1 June 2017. The tweets were sorted into topic category, with an Indiana University tool called Botometer used to estimate the probability the user behind the tweet is a bot. Marlow said he was surprised that bots were responsible for a quarter of climate tweets on an average day. “I was like, ‘Wow that seems really high,’” he said.  The consistent drumbeat of bot activity around climate topics is highlighted by the day of Trump’s announcement, when a huge spike in general interest in the topic saw the bot proportion drop by about half to 13%. Tweets by suspected bots did increase from hundreds a day to more than 25,000 a day during the days around the announcement but it wasn’t enough to prevent a fall in proportional share. Trump has consistently spread misinformation about the climate crisis, most famously calling it “bullshit” and a “hoax”, although more recently the US president has said he accepts the science that the world is heating up. Nevertheless, his administration has dismantled any major policy aimed at cutting planet-warming gases, including car emissions standards and restrictions on coal-fired power plants. The Brown University study wasn’t able to identify any individuals or groups behind the battalion of Twitter bots, nor ascertain the level of influence they have had around the often fraught climate debate. However, a number of suspected bots that have consistently disparaged climate science and activists have large numbers of followers on Twitter. One that ranks highly on the Botometer score, @sh_irredeemable, wrote “Get lost Greta!” in December, in reference to the Swedish climate activist Greta Thunberg. This was followed by a tweet that doubted the world will reach a 9-billion population due to “#climatechange lunacy stopping progress”. The account has nearly 16,000 followers. Another suspected bot, @petefrt, has nearly 52,000 followers and has repeatedly rejected climate science. “Get real, CNN: ‘Climate Change’ dogma is religion, not science,” the account posted in August. Another tweet from November called for the Paris agreement to be ditched in order to “reject a future built by globalists and European eco-mandarins”.[* See footnote]. Twitter accounts spreading falsehoods about the climate crisis are also able to use the promoted tweets option available to those willing to pay for extra visibility. Twitter bans a number of things from its promoted tweets, including political content and tobacco advertising, but allows any sort of content, true or otherwise, on the climate crisis. A Twitter spokesperson disputed the accuracy of the Brown University research. “Non-peer reviewed research using our public API can often be deeply flawed,” the spokesperson said, adding that “sweeping assessments” of users based on signals such as location and tweet content are routinely made by outside groups. “To be clear - none of these indicators are sufficient to determine if something is a bot. Looking for accounts that look similar to those disclosed as part of our archives is an equally flawed approach, given many of the bad actors mimic legitimate accounts to appear credible. This approach also often wrongly captures legitimate voices who share a particular political viewpoint that one disagrees with.” Research on internet blogs published last year found that climate misinformation is often spread due to readers’ perception of how widely this opinion is shared by other readers. Stephan Lewandowsky, an academic at the University of Bristol who co-authored the research, said he was “not at all surprised” at the Brown University study due to his own interactions with climate-related messages on Twitter. “More often than not, they turn out to have all the fingerprints of bots,” he said. “The more denialist trolls are out there, the more likely people will think that there is a diversity of opinion and hence will weaken their support for climate science. “In terms of influence, I personally am convinced that they do make a difference, although this can be hard to quantify.” John Cook, an Australian cognitive scientist and co-author with Lewandowsky, said that bots are “dangerous and potentially influential”, with evidence showing that when people are exposed to facts and misinformation they are often left misled. “This is one of the most insidious and dangerous elements of misinformation spread by bots – not just that misinformation is convincing to people but that just the mere existence of misinformation in social networks can cause people to trust accurate information less or disengage from the facts,” Cook said. Although Twitter bots didn’t ramp up significantly around the Paris withdrawal announcement, some advocates of action to tackle the climate crisis are wary of a spike in activity around the US presidential election later this year. “Even though we don’t know who they are, or their exact motives, it seems self-evident that Trump thrives on the positive reinforcement he receives from these bots and their makers,” said Ed Maibach, an expert in climate communication at George Mason University. “It is terrifying to ponder the possibility that the Potus was cajoled by bots into committing an atrocity against humanity.” * This footnote was added on 11 March 2020. Following publication, the owner of Twitter account @petefrt contacted us to say that the account had been suspended because Twitter said the owner appeared to be managing multiple accounts to achieve artificial amplification. However, on appeal, Twitter has conditionally unsuspended the account. The owner says @petefrt is their only account and has asked Twitter to detail any other accounts associated with that account so they may be deactivated."
"Even under the most conservative climate change scenarios, sea levels 30cm higher than at present seem all but certain on much of the UK’s coast by the end of this century. Depending on emission scenarios, sea levels one metre higher than at present by 2100 are also plausible.  The knee-jerk reaction to sea level rise has traditionally been to maintain the shoreline’s position at all cost, by building new flood defence structures or upgrading old ones. More than US$10 billion per year is already spent worldwide on “grey” infrastructure such as concrete walls and levies to protect against coastal flooding. Equally large are the costs incurred when coastal defences fail. The United Nations has called on governments to relocate public facilities and infrastructure from flood-prone areas, while the UK Climate Change Committee has urged the government to “set out how and when the hard choices that have to be made on the coast are going to happen”. The traditional approach of “grey” engineered sea defences locks society into ever increasing costs of replacement and maintenance. The alternatives are “nature-based solutions” to coastal flooding and erosion, which work with natural processes to reduce flood risk and incorporate ecosystems into flood defence. Rather than seeing the coast as a static line, these alternatives rethink the coastlines as zones with valuable habitats such as beaches, dunes and wetlands that act as carbon stores, places for recreation and natural buffers against the waves. Schemes such as the Wild Coast Project at Wallasea on the UK’s east coast have restored salt marshes where land had been reclaimed for agriculture years earlier. The tide and waves now regenerate salt marsh where it had been embanked and drained. If designed well, such schemes create new habitat which can reduce the height and intensity of storm surges and lower flood risk.  This technique for managing sea level rise can be thought of as allowing nature the space to create new coastal habitats within well-defined boundaries, akin to flooding a “sandpit”. In this sandpit, enough coastal space is vacated by humans to give natural processes room to respond to sea level rise by creating new wetlands further inland where once the terrain was dry. While such interventions do not enable full control over water levels and waves, they are designed to keep them at a safe distance from humans. Nature may be allowed to have some freedom to “play in the sandpit” created for it and people may not care what type of salt marsh or mudflat forms at Wallasea. But, as with “grey” infrastructure, humans ultimately build the sandpit by setting its boundaries.   The future of the world’s coastlines, however, is uncertain as the coast is inherently dynamic. Every wave and tide shapes the coast such that it determines how the next wave and tide can shape it. Though people may not notice it, the coast and the habitats which line it are never fixed and in fact change a great deal over a single human lifetime. People may create pockets of space for nature and think they are in control when in fact humans never were and it is doubtful they ever can be.  This is amply illustrated by the freshwater grazing marshes at Blakeney in Norfolk, on the UK’s east coast, where embankments were breached during the 2013 storm surge. A storm surge at sea forced salt water through an embankment into the Blakeney Freshes nature reserve, a unique freshwater wetland. Though unintended, salt water flooding of embanked areas like the Freshes can create a new ecosystem there and prevent flood waters from rising in adjacent areas, where people would have come to harm.  The Blakeney Freshes illustrate the importance of allowing sufficient space for nature to decide the boundaries of its “sandpit”. The more space given to it and the wider the buffer zone of coastal landforms, the lower the risk of flooding to areas that lie further inland.  As with weather forecasts, predicting how complex natural processes will interact at the coast is difficult – certainly over years and decades. It is time people stopped pretending that nature can be controlled at all, whether through “grey” or “green” engineering schemes. The best option is to watch and learn.  The means for monitoring exist, with ever-improving measurement technology, data transmission and high-resolution satellite imagery, such as the European Commission’s Copernicus programme. Responding to sea level rise may be as simple as allowing space for the flow of water and sediment during extreme events. Society can do so by relinquishing the need to control the process and restricting development near the coast, creating areas that nature can “claim” with whatever habitat it wishes to “build” there. As a salt marsh turns into a tidal flat, a freshwater field into a salty lagoon, there is the opportunity to stand back, watch and learn to better understand how and why those changes happen and how people can benefit from change rather than fight a losing battle to prevent it."
"Over the past two years, 37 experts from around the world have battled to develop a diet that is both sustainable and healthy. They integrated existing knowledge on the impact of diet on diseases, including cardiovascular disease, diabetes and cancer, with the impact of current food production systems on the environment.   The result of this “EAT-Lancet Commission” is a seminal report, published on January 16, on Food in the Anthropocene. The diet they came up with is likely to split opinion, as it would fundamentally impact on many people’s daily lives. When comparing it to what is typically consumed in the UK, for instance, there are some dramatic changes recommended.  The one which has hit most of the headlines is a reduction in the consumption of red meat (beef, pork and lamb) from approximately 12% of total energy intake to a mere 1%. Poultry does a little better, but still falls by more than half, along with major reductions in the intake of fish, eggs and dairy produce. The potential impact diets in North America (the largest consumers of meat and dairy products) is even greater.   These animal products are to be replaced largely by nuts, pulses and legumes. While total intake of grains (wheat, rice and corn) remains largely unchanged, refined products are to be replaced by wholemeal varieties. With veganuary fresh in the mind, such changes may not appear too radical. However, while social media may have us believe that vegetarianism, or even veganism, are becoming the norm, still only about 12% of the UK population follow a meat-free diet.  While there is little doubt that eating less red meat is good for your health, it is also important to recognise that for the poorest in society, meat and dairy products represent affordable sources of both calories and a range of micronutrients. In parts of the world where malnutrition is still common, increasing the availability of such products would certainly have significant health benefits.   Across much of the world, people aspire to the meat and dairy-rich diets that dominate most of high-income countries. This is seen most acutely in China, where traditional diets, rich in rice and other plant materials, are being rapidly replaced with meat dishes. The country now has more than 400m pigs – around half the global population. Changing such attitudes, and ensuring the most vulnerable people continue to obtain appropriate nutrition, is at least as big a challenge as manipulating agricultural production across the globe. Animal production is often perceived as an unsustainable use of natural resources. Many animals are fed on food suitable for human consumption, demand a high proportion of the planet’s water resources and then produce methane, one of the most potent greenhouse gases. It is hard to argue that the highest income countries should not reduce their meat and dairy.  However, other animals (including cows, sheep and goats) graze pasture that is unfit to grow human-edible crops, turning grass and other plants into high value protein. If the UK, for example, were to abandon such livestock production, are its residents really ready to see radical changes in a rural environment associated with its “green and pleasant land”? Could the uplands of Scotland, Wales and Northern England really be turned towards the production of peanuts and soya beans? The EAT-Lancet report cannot be ignored. It starkly points out the catastrophic consequences for the planet of continuing with “business as usual”. Yet it is important that, as agriculture becomes more efficient and sustainable, we still consider the health and well-being of the most vulnerable in society. This includes not only the one billion people who remain in danger of malnutrition in areas such as Sub-Saharan Africa and South Asia, but also many of the poorest people within the large urban conglomerates of our wealthy countries.  If adopted, the report suggests that the dietary changes recommended could save more than 11m lives each year. While most would welcome a longer and healthier life, these represent more mouths to feed in an ageing population. As people get older, their nutritional needs change and about a third of people living in care homes in the UK suffer from some degree of malnutrition. Ironically, this is often associated with a lack of high-quality protein, such as is found in meat and dairy products.  This perhaps raises even more profound questions over what we wish to achieve for the future of humanity."
"

Opinion studies indicate that many Americans are terrified of flying. But there’s good news from the Federal Aviation Administration for those nervous Nellies. Last year there were 14 million commercial airline flights carrying 615 million passengers. How many deaths? A big fat zero. The chances of dying by electrocuting yourself in the bathtub, being crushed to death by the airbag in your car or being hit on the head by a meteorite were about the same as by taking a ride on an airplane. 



America has become a nation of worrywarts. We worry about everything from global warming to the amount of fat in our breakfast cereal to the radiation emanating from our computers. A recent poll featured in USA Today confirms that we are concerned about the strangest things. More than half of Americans say that successful cloning is one of their biggest dreads for the 21st century. What is completely inexplicable is that 55 percent of Americans cited “technology” as one of their worries. How’s that for unfounded paranoia? Technology saves lives, it doesn’t cost them. 



Maybe our greatest risk in the modern age is that of worrying ourselves to death. The risks we now face in our daily lives are minuscule compared with the risks of earlier times. In fact, the world is a much safer place today than it was at any time in history. Many of the things we fret about today, such as flying on a commercial airline, are not dangerous at all. If you don’t believe me, consider the following statistics: 



  
  




  
  




  
  




What else do you worry about? War? Floods and tornadoes? AIDS? Nuclear accidents? Acts of terrorism? Sudden Infant Death Syndrome? Contaminated air and water? Heart attack? Believe it or not, the death rate from every single one of those menaces is down–in most cases way down. In the last 20 years modern medicine has made giant strides in treating and preventing heart disease. It remains a major killer only because, as life expectancies increase, we become more prone to this degenerative disease. 



Worrywarts have shifted their phobias to obscure and distant threats posed by things like global climate change, alien invasions, cloning and secondhand smoke. Most of the threats that were really serious and frightening earlier in this century are no longer problems at all. My parents often told gruesome tales of growing up in the 1930s when polio was still a dreaded killer. Everyone knew someone who had been paralyzed and confined to an iron lung or a wheelchair. The Centers for Disease Control tells us that there was not a single reported case of polio in the United States in 1997. 



One of the leading causes of death in America today is poverty. The best way to continue to reduce the risk of injury and early death is to promote economic growth and rising living standards through free‐​market capitalism. Paradoxically, many of the safety, health and environmental regulations that come out of the Environmental Protection Agency and the Occupational Safety and Health Administration do not reduce risk. They are so cost‐​ineffective that they make us poorer and thereby put people at greater risk of death than if we had never issued those regulations at all. Smart regulations save lives. Dumb regulations cost lives. 



Now, if after reading this you are still nervous about the sky falling and the world (or your own world) coming to an end, my advice is to board an airplane. These days, that’s probably the safest place you can be.
"
"
Set Phasers on Stun
March 29th, 2009 by Roy W.  Spencer, Ph. D.


I’ve been receiving a steady stream of e-mails asking when our latest work on  feedbacks in the climate system will be published. Since I’ve been trying to fit  the material from three (previously rejected) papers into one unified paper, it  has taken a bit longer than expected…but we are now very close to submission.
We’ve tentatively decided to submit to Journal of Geophysical Research (JGR)  rather than any of the American Meteorological Society (AMS) journals. This is  because it appears that JGR editors are somewhat less concerned about a paper’s  scientific conclusions supporting the policy goals of the IPCC — regulating  greenhouse gas emissions. Indeed, JGR’s instructions to reviewers is to not  reject a paper simply because the reviewer does not agree with the paper’s  scientific conclusions. More on that later.
As those who have been following our work already know, our main conclusion  is that climate sensitivity has been grossly overestimated due to a mix up  between cause and effect when researchers have observed how global cloud cover  varies with temperature.
To use my favorite example, when researchers have observed that global cloud  cover decreases with warming, they have assumed that the warming caused the  cloud cover to dissipate. This would be a positive feedback since such a  response by clouds would let more sunlight in and enhance the  warming.
But what they have ignored is the possibility that causation is actually  working in the opposite direction: That the decrease in cloud cover caused the  warming…not the other way around. And as shown by Spencer and  Braswell (2008 J. Climate), this can mask the true existence of negative  feedback.
All 20 of the IPCC climate models now have positive cloud feedbacks, which  amplify the small about of warming from extra carbon dioxide in the atmosphere.  But if cloud feedbacks in the climate system are negative, then the climate  system does not particularly care how much you drive your SUV. This is an issue  of obvious importance to global warming research. Even the IPCC has admitted  that cloud feedbacks remain the largest source of uncertainty in predicting  global warming.
Significantly, our new work provides a method for identifying which direction  of causation is occurring (forcing or feedback), and for obtaining a more  accurate estimate of feedback in the presence of clouds forcing a temperature  change. The method involves a new way of analyzing graphs of time filtered  satellite observations of the Earth (or even of climate model  output).
Well…at least I thought it was new way of analyzing graphs. It turns  out that we have simply rediscovered a method used in other physical sciences:  phase space analysis.  This methodology was first introduced by Willard Gibbs in 1901.
We found that by connecting successively plotted points in graphs of how the  global average temperature varies over time versus how global average radiative  balance varies over time, one sees two different kinds of structures emerge:  linear striations, which are the result of feedback, and spirals which are the  result of internal radiative forcing by clouds.
But such a methodology is not new. To quote from Wikipedia on the subject of  ‘phase space’:
“Often this succession of plotted points is analogous to the system’s  state evolving over time. In the end, the phase diagram…can easily elucidate  qualities of the system that might not be obvious  otherwise.”
Using a simple climate model we show that these two features that show up in  the graphs are a direct result of the two directions of causation: temperature  causing clouds to change (revealed by ‘feedback stripes’), and clouds causing  temperature to change (revealed by ‘radiative forcing spirals’).
The fact that others have found phase space analysis to be a useful  methodology is a good thing. It should lend some credibility to our  interpretation. Phase space analysis is what has helped us better understand  chaos, along with its Lorenz attractor, strange attractor, etc.
And the fact that we find the exact same structures in the output of  the IPCC climate models means that the modelers can not claim our interpretation  has no physical basis.
And now we can also use some additional buzzwords in the new article…which  seems to help from the standpoint of reviewers thinking you know what you are  talking about. The new paper title is, “Phase Space Analysis of Forcing and  Feedback in Models and Satellite Observations of Climate  Variability”.
It just rolls of the tongue, doesn’t it?
I am confident the work will get published…eventually. But even if it didn’t,  our original  published paper on the issue has laid the groundwork…it would just take  awhile before the research community understands the implications of that  work.
What amazes me is the resistance there has been to ‘thinking out of the box’  when trying to estimate the sensitivity of the climate system. Especially when  it has been considered to be ‘thinking in the box’ by other sciences for over a  century now.
And it is truly unfortunate that the AMS, home of Lorenz’s first  published work on chaos in 1963, has decided that political correctness is  more important than the advancement of science.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e976ea2e7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterI’m wrong with my last post. There is some climate material in those Wikileaks documents. I started sifting through and found this one. Nothing earth-shattering, but you see the deals being done behind the scenes.
May 8, 2009
Here we see China was never going to accept any targets on CO2 emissions. But they did promise to bring “action items”!
http://cablegate.wikileaks.org/cable/2009/05/09BEIJING1247.html
Climate Change
————–
8. (C) UK DCM Wood said the UK Environment and Science
Minister had recently had talks with Chinese officials on
climate change. In the lead up to Copenhagen, China would
not agree to targets on emissions but was willing to be
constructive and would come to Copenhagen with a package of
action items related to nuclear power, renewable energy and
reforestation. Wood said his impression was that China could
be induced to do more on climate change. “
They knew long before that Copenhagen was going to fail. 


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




There are likely more about climate in there. It’ll take an army to go through it. There are some real doozies in their on other topics of Iran, gas and oil projects, etc. I’d say a gold mine of info for many corporations too. This is going to be worse than I thought.
Expect lots of “action items” in Cancun to save face.
==================================================================
Another Wikilieaks climate document found: Jan 29, 2008
Merkel pushing for aggressive measures. Here we see that Merkel is a flaming warmist.
http://cablegate.wikileaks.org/cable/2008/01/08BERLIN122.html
13. (C) Chancellor Merkel and the rest of Germany’s political
leadership remain serious about pursuing aggressive
international measures to meet the challenges of global
warming.  Merkel has made climate change a priority of her
Chancellorship and enjoys the overwhelming domestic support
on this.  Merkel’s support for mandatory, targeted global
limits on greenhouse gas (GHG) emissions and an international
cap-and-trade regime reflects a deep-seated belief that only
drastic, concerted efforts on the part of the international
community can slow — and ultimately reverse — the human
contribution to global warming.  If anything, Steinmeier
supports tougher standards.  While the Germans have been
willing to consider alternative solutions, such as new
technologies for clean coal and renewables, fundamental
differences in our approaches to the issue of climate change
remain, and could lead to more public disagreement in the
future.  For example, while Germany will send a delegation to
the January 30 Major Economies Meeting (MEM), the German
Government remains skeptical about the value that the Major
Economies Process (MEP) adds to the UNFCCC track. The Germans
are particularly concerned about the need to avoid
duplication of effort in the various other climate
change-related forums, including the UNFCCC and the G-8.
TIMKEN JR 
Share this...FacebookTwitter "
"
Guest post by Steve Goddard
There is a considerable amount of misinformation propagated about the greenhouse effect by people from both sides of the debate.  The basic concepts are straightforward, as explained here.
The greenhouse effect is real.  If there were no greenhouse gases in the atmosphere, earth would be a cold place.   Compare Mars versus Venus – Mars has minimal greenhouse gas molecules in its’ atmosphere due to low atmospheric pressure, and is cold.  By contrast, Venus has a lot of greenhouse gas molecules in its’ atmosphere, and is very hot.  Temperature increases as greenhouse gas concentration increases.  These are undisputed facts.
Heat is not “trapped” by greenhouse gases.  The earth’s heat balance is maintained, as required by the laws of thermodynamics.

outgoing radiation = incoming radiation – changes in oceanic heat content
The image below from AER Research explains the radiative balance.

http://www.aer.com/scienceResearch/rc/rc.html
About 30% of the incoming shortwave radiation (SW) is reflected by clouds and from the earth’s surface.  20% is absorbed by clouds and re-emitted back into space as longwave (LW) radiation.  The other 50% reaches the earth’s surface and warms us.  All of that 50% eventually makes it back out into space as LW radiation, through intermediate processes of convection, conduction or radiation.  As greenhouse gas concentration increases, the total number of collisions with GHG molecules increases.  This makes it more difficult for LW radiation to escape.  In order to maintain equilibrium, the temperature has to increase.  Higher temperatures mean higher energies, which in turn increase the frequency of emission events.  Thus the incoming/outgoing balance is maintained.
It has been known for a long time that even a short column of air contains enough CO2 to saturate LW absorption.  This has been misinterpreted by some skeptics to mean that adding more CO2 will not increase the temperature.  That is simply not true, as higher GHG densities force the temperature up.  There is no dispute about this in the scientific community. See the graph below:

Click for larger image
As Dr. Hansen has correctly argued, increases in atmospheric temperature cause the ocean to warm up.  Thus changes the oceanic heat content become the short term imbalance in the incoming/outgoing equilibrium equation, which is not shown in the AER diagram.
The image below shows GHG absorption by altitude and wavenumber.  As you can see, there is a strong absorption band of CO2 at 600/cm.  That is what makes CO2 an important greenhouse gas.

http://www.aer.com/scienceResearch/rc/m-proj/lbl_clrt_mls.html

The important greenhouse gases are: H2O, CO2, O3, N2O, CO and CH4.  The reason why the desert can get very cold at night is because of a lack of water vapor.  The same is true for Antarctica.  The extreme cold in Antarctica is due to high albedo and a lack of water vapor and clouds in the atmosphere, which results in almost all of the incoming radiation returning immediately to space.
An earth with no CO2 would be very cold.  The first few tens of PPM produce a strong warming effect, and increases after that are incremental.  It is widely agreed that a doubling of CO2 will increase atmospheric temperatures by about 1.2C, before feedbacks.  So the debate is not about the greenhouse effect, it is about the feedbacks.
Suppose that the amount of reflected SW from clouds increases from 20% to 21%?  That would cause a significant cooling effect.  Thus the ability of GCM models to model future temperatures is largely dependent on the ability to model future clouds.  Cloud modeling is acknowledged to be currently one of the weakest links in the GCMs.  Given the sensitivity to clouds, it is perhaps surprising that some high profile climate scientists are willing to claim that 6C+ temperature rises are established science.
So the bottom line is that the greenhouse effect is real.  Increasing CO2 will increase temperatures.  If you want to make a knowledgeable argument, learn about the feedbacks.  That is where the disagreement lies.
“Lisa, in this  house we obey the laws of thermodynamics“
– Homer Simpson


Addenddum:

The GHG/stoplight analogy

Suppose that you have to drop your child at school at 8:00 and have to be  at work at 8:30.  There are 10 stoplights between the school and the office.   Your electric car has a fixed maximum speed of 30MPH.  It takes exactly 30  minutes to drive there.

If the city adds another stoplight (analogous to more CO2) the only way you  can make it to work on time is to run traffic lights and/or get the city to make  the traffic lights more efficient at moving cars (analogous to higher  temperature.)  The radiative balance has to be maintained in the atmosphere, so  the outgoing radiation has a fixed amount of time to escape, regardless of how  many GHG molecules it encounters.   Otherwise, Homer and your boss will be very  angry at you for violating the laws of thermodynamics.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9826bf2a',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterWhen it comes to renewable energy, you can call them “Jesus technologies”. These are technologies that went the way of the dinosaurs a long time ago due to their inefficiency and impracticality. But in order to serve a political purpose, they seem to keep getting resurrected every 30 years or so. I came up with “Jesus technology” from Bishop Hill’s  Caspar and the Jesus-Paper“, a paper that died often but kept coming back.
Sure in some cases these primitive technologies make sense, but for the wide-scale application in a power grid, they make little sense, cost the consumer dearly, and even put the energy supply at risk.
Klaus-Dieter Humpich of the Friedrich Naumann Foundation For Freedom has an excellent piece on renewable energy called The Dirty Secret of Wind and Solar. It’s in German, and so if you can read the language, it is worth taking the time to do so. What follows is a summary.
Humpich’s essay starts by reminding us that electrical energy is very difficult and enormously costly to store. Therefore, the wildly fluctuating supply of wind and solar energies requires having conventional back-up systems in place, ready to fire up or throttle down at a moment’s notice whenever the sun and wind intensities change. Humprich writes that solar and wind are referred to as “additive energy forms” in the energy business, and not as “alternatives”. Alternatives would suggest that they replace conventional fuels, which is not the case. They only add to conventional fuels, and hence they are called additive supplies.
Wind and solar are a nightmare to control
The problem with wind energy is that a wind generator’s output varies with the third power of the wind velocity, P = kV³. That means a wind generator produces only 1/8 of it’s rated energy if the wind speed is cut in half. So whenever the wind speed changes, the power grid must be compensated by conventional power plants that are on constant stand-by. On gusty days, as more wind parks get added to the grid, it becomes more and more of a nightmare to keep the grid stable. The result: you get a grid that behaves like a wild bronco. Humpich writes:
A power control engineer would say that these are real disturbances with steep gradients (e.g. changes in power output due to a wind gust through a wind park).”
The once easy-to-manage, steady, conventional-fuel power supply and corresponding consumption have since been intruded on by a third, highly unstable and unpredictable player.
Standby conventional power plants have low efficiencies
So when the wind suddenly dies down, reserve conventional plants have to jump in quickly, meaning they’ve got to be always on stand-by. These power plants thus rarely run at their peak efficiencies, and often at outputs well below their peak efficiency. The result? Little, if any, savings in fossil fuel consumption gets achieved. Now we know why the concept of wind being an alternative really isn’t so.
The energy that gets produced by a wind generator, is in part lost to reduced efficiencies by the standby conventional plants. All the investment and resources to install the massive system wind and solar park infrastructure has only lead to saving a fraction of what they originally were promised to save.
Humpich writes:
You always have to keep conventional power plants running alongside in order to keep the grid stable. That means it consumes fuel that does not even get used. Be it that the plant is running only at partial capacity – at a sub-optimal efficiency – or is “throttled”, which means the generated steam does not even get sent to the turbines to be converted into power but rather is simply sent back to the condenser.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Result: consumption of fuel – for nothing.
Mixed power grids and systems are less efficient
So why go through all the trouble if it isn’t worth it? It’s all in the bookkeeping. As long as the energy from renewable sources gets accounted as having replaced the equivalent in conventional energy, then it appears interesting and the business of CO2 emission certificates looks especially lucrative.
As Humprich explains, it’s a bit like a brochure for a new car claiming the car gets 45 mpg. But as we know, that number is only under certain ideal conditions. In reality, with all the stop-and-go driving in city traffic, etc., the car’s fuel efficiency turns out to be much less.
It’s the same concept with a grid that is powered only using steady conventional fuel. An efficiency close to that advertised by the “manufacturer'” can indeed be reached when operated near ideal conditions. But when you mix in wind and solar parks, the efficiency is spoiled – you’re in “city traffic”. It drops considerably.
So what exacly is the efficiency of a conventional power plant operating on a “mixed grid”? Does the generated wind and solar energy replace a corresponding amount of fossil fuel? The answer is of course “no”. To determine the exact amount, it is necessary to conduct comprehensive simulations or actual field measurements. Humprich provides an example. A combination natural gas fired/steam typically has an efficiency of 57%. But when it is used as a back up for wind and solar energy, it no longer operates under ideal conditions, and so the efficiency drops to a measly 36%.
Rotten in Denmark
Denmark is a country that has a large supply of renewable energy. And it is also long known that when storms rage over Denmark, its power grid has to be stabilized by conventional power plants in neighboring Germany and Sweden.
Today wind and solar energy are incresingly being stabilized by gas-fired steam power plants, and so  it means more business for the gas industry. Humprich writes:
Maybe that’s why the two leading propagandists for wind and solar energy today are representatives of gas. In the USA, in any case, the gas industry is the leading sponsor of the ‘climate industry’. But this is not reprehensible. If you wish to promote another product (natural gas) onto an established market (coal and nuclear), then a lot of arm-twisting is needed. In this respect, gas-guys like Schröder, Fischer and Co. become real vacuum cleaner salespeople, who happen to get get paid generaously for their sales pitches.”
Further Literature:
Kent Hawkins: Wind Integration Realities – Case Studies of the Netherlands and of Colorado, Texas, Master Resource.
C. le Pair & K. de Groot: The impact of wind generated electricity on fossil fuel consumption.
K. de Groot & C. le Pair: The hidden fuel costs of wind generated electricity.
Share this...FacebookTwitter "
"

The British government’s decision to begin extradition hearings against former Chilean dictator Gen. Augusto Pinochet has triggered a heated right‐​left quarrel in the United States and around the world. 



Pinochet’s conservative defenders argue that his military regime, although brutal at times, saved Chile from the scourge of communism, and that the retired, 83‐​year‐​old general should be allowed to return to his home in Santiago. 



Pinochet’s liberal detractors, on the other hand, compare him with Adolf Hitler and argue that he should be extradited to Spain and prosecuted for genocide and terrorism. Liberals also see the Pinochet case as an opportunity to move toward a universal system of criminal justice. 



Lost in the right‐​left controversy over Pinochet’s record is a more important issue. The Pinochet affair teaches us ominous lessons about what we can expect from the planned International Criminal Court (ICC), the global criminal justice organ approved last summer in Rome by 120 nations.  




The Pinochet affair is … an alarming harbinger of the ICC [and] … portends a very hazardous future. 



The first lesson of the Pinochet affair is that the ICC’s prosecutions will likely be ideologically based. Indeed, does anyone seriously believe that Mikhail Gorbachev will be arrested next time he travels to Western Europe and tried for the bloody war in Afghanistan or for the KGB’s cruel excesses during his six‐​year tenure? Was anyone startled by Italy’s decision not to extradite Abdullah Ocalan, the recently captured Marxist leader of the Kurdish Workers’ Party, to Turkey, where he is wanted as a terrorist? Or the German government’s decision not to demand Ocalan’s extradition to Germany where a number of his alleged murders took place? Last, was anyone surprised by the Spanish authorities’ decision to go after Pinochet, but at the very same moment, allow Cuban dictator Fidel Castro rome around Spain freely? 



The second lesson the Pinochet affair teaches us about the ICC is that its activities could threaten the fragile peace of transitional democracies. Chile and other new democracies around the world have withstood recent political and economic strains, but the arrest of Pinochet has reopened old wounds and many Chileans believe that terrorist violence could erupt if he is not allowed to return home. “I have already been threatened by ultra‐​left‐​wing terrorists,” says Sen. Hernan Larrain of the rightist Independent Democratic Union Party. His worst fear is that a British decision to go ahead with extradition will polarize the Chilean population, creating a climate for violence. Many people on the left also fear that terrorist violence could be aimed against them, since armed right‐​wing groups and former secret policemen are reportedly reorganizing. There is also widespread fear that the Chilean military will reassert control if the political situation deteriorates markedly. 



In addition to unsettling peace around the world, the ICC’s activities could make achieving peace in the first place more difficult. Consider the case of Palestine Liberation Organization leader Yasser Arafat, who was in the United States negotiating the Wye River Agreement the same week Pinochet was arrested in Britain. Would Arafat have negotiated at all if he thought the American government might arrest him when he arrived and try him for acts of PLO terrorism? Or would the recent Irish Peace Settlement have been negotiated if the Catholic or Protestant leaders faced criminal prosecution? 



The third lesson the Pinochet affair teaches us about the ICC is that its authority may be interpreted in expansive ways. For example, although Pinochet’s secret police are implicated in the deaths or disappearances of 3,000–4,000 people over 17 years (an average of about 250 people a year) he is being charged with the crime of genocide, defined by the 1948 Genocide Convention as systematic killing with “intent to destroy, in whole or in part, a national, ethnical, racial, or religious group.” Proponents of the genocide charge against Pinochet maintain that the elimination of political opponents through assassinations and imprisonment constitutes a kind of “ideological genocide.” So much for original intent. 



The Pinochet affair is thus an alarming harbinger of the ICC. Specifically, the planned court stands to produce an arbitrary and highly politicized “justice” and open a Pandora’s box of political folly and malleable laws. The Pinochet affair, in short, portends a very hazardous future.
"
"
Share this...FacebookTwitterThis is how ZDF German Public television envisioned how life in the year 2000 could appear back in 1972. Hat-tip Michael Miersch here.
Part 1 (Youtube video): Living and going to work (
The first part shows a man named Mr B living in a futuristic, electronic apartment. In the year 2000 people have to work only 25 hours per week, and can live with artificial hearts. Breakfast is prepared automatically and that all food is free of poisons because it is organically produced, free of pesticides and chemicals.
There are no printed newspapers – people get their news from a “printer” twice a day. Everyone lives in huge high rise apartment buildings with 2000 units, each equipped with international satellite TV that can be watched 24 hours per day. Shopping is done by radio-teleshopping; purchases are booked direct from his bank account.
Mr B communicates with friends using a “TV telephones” (smoking is still politically correct).
People don’t visit each other anymore – they converse via TV screen. Sociologists warn of the isolation of man by technology.
Mr B. does not use a conveying sidewalk to go the short distance to the train station when he goes to work, he walks. The air is now clean again because pollution was banned in 1990. Some even called for the death penalty for polluters. He works in another city 80 km away. No problem though, the jet-engine powered 500 km/hr commuter monorail train needs only 15 minutes. At the station he rents an electric city-car, which are readily available at all transportation hubs.
Part 2: At work in the year 2000, and the environmental hell of 1972.
The electric cars are automatically navigated. At work a massive network of people-conveyors take him through the huge maze of buildings. He works at a databank center that sells data to customers. All data is stored at a massive data storage centers and systems. For example, the databanks deliver critical data to politicians almost instantly so that they can always make the right decisions.
Everything is automated, and so Mr B has lots of time on his hands at work – no stress. That’s the way it is for millions of highly skilled workers like him, who only need to sit around and monitor the automated systems. As he sits around, he thinks about what he’ll do when he retires at the age of 50.
Retirement in the year 2000 is a problem too, as people have yet to figure out how spend all that free time on their hands. This is how ZDF imagined life could be in the year 2000 back in 1972.
At the 6 min mark, the show returns back to the reality of 1972. Here ZDF bemoans that 14 million cars jam the streets of West Germany. They pollute the air and threaten to choke the citizens in a sea of metal, exhust and noise. Cars are a symbol of freedom, but in reality they condemn the people to being stuck in traffic jams. The car – it kills 17,000 and injures 500,000 every year. Millions of tons of sulfur dioxide, carbon monoxide and lead are blown yearly into the atmosphere.
Air pollution in urban areas is already at the allowable limits. The SST pollutes the entire stratosphere and creates extreme noise – all to save 3 hrs of flight time. ZDF says we’ve poisoned the biosphere and food chain with our pesticides. Industry has polluted the water so much that clean water will be a luxury product in just a few years. Germany’s Lake Constanz will soon look like Lake Erie. It will take 50 to 100 years before Lake Erie returns to a natural condition.
Part 3: Man is destroying the planet – we have precious little time to save it


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Part 3 looks at life and the environment as it was in 1972. The film starts with:
Industrialization brought prosperity to many, but a threat to all. Industrialization favors the concentration of capital and assets in the hands of a few.
The clip then describes the growing gap between the haves and have-nots. Up to 800,000people (from 60 million) live in poverty. The film says that by1980, half a million students will jam into the universities, where only half will be able find housing, and classroom will be overfilled, leading to riots and civil unrest.
ZDF complained that the field of medicine is outdated, and Germany is falling behind. 30% of all hospital beds were made in the 1920s.
ZDF then asks a series of pessimistic questions. Will the political system be able to quickly enough make the decisions necessary to bring the country forward to meet the challenges of the year 2000?  How will the family survive? How will the workplace change? Automation threatens to turn workers into mere monitors.
For millions there isn’t going to be any work.”
Another problem is that the economy produces more food then what is needed. Europe destroys the surplus of food while people starve in other places on the globe. ZDF then juxtaposes this with military spending, which amounts to 600 billion German Marks annually. “For the first time in history, man is capable of destroying the planet.
The only choice is either we live together, or die together.”
ZDF then complains that technology is advancing too quickly; we can’t keep up and that we don’t really know what we’re getting ourselves into. Since WWII, millions of tons of concrete have been transformed into living units. Now everywhere the landscape is littered with high rise apartment buildings. No one thought about the impact on man and society. No research is being done to see where all this is taking us. The ZDF clip ends with a quote:
‘The capability of man to thoughtlessly destroy the environment is practically without limits,’ says an American scientist. Poiliticans face the challenge of stopping the destruction before it becomes too late. For that we have very little time.”
End
———————————————————————————-
Today we see that life is much better and different than what ZDF predicted in 1972. Lake Erie is also clean again. Sulphur dioxide and carbon monoxide have long since been replaced by life-giving CO2 as the big threat to humanity and nature. The dire prophesies never came true – even though there was a consensus among the “experts”.
HAPPY NEW YEAR EVERYBODY!
Share this...FacebookTwitter "
"
Earth Hour in California – Success or Bust?


Guest post by Russ Steele, NCWatch
At our house we set the timer to remind us to turn on all the visible out side lights.  We have multiple security lights on the garage and the barn that come on when the sun goes down. My friend George Rebane has evidence that he turned on his lights for Earth Hour at Ruminations. I should have done the same, but was working on a sea level issue in R and forgot. I am glad I set the timer to remind me to turn off the outside house lights at 9:30.
The real question is did it Earth Hour make a difference one way or the other?
Roger Sowell had a good idea, he download the the graph below from www.caiso.com, the California Independent System Operator.  CAISO is in charge of receiving power from power generating plants, and distributing the power throughout the state grid to the various end users.
California power use 3-28-09 from CAISO  - Click for a larger Graphic 
Now compare the graph from Saturday 3/28/09 to the one on Sunday 3/29/09 shown below, note the similar slopes during the same time period. Note that annotations were added by Anthony Watts on both graphs.
California power use 3-29-09 from CAISO  - Click for a larger Graphic 
Roger notes:
The light gray line is the forecasted power usage, shown in Megawatts.  The red line is the actual power consumed.  Around 1900 hours, 7 p.m., the load was approximately 24,000 MW.  By 8:00, the load increased smoothly to just over 26,000 MW.  Then the load began a steady decrease right on through the night, ending at around 22,000 MW at almost midnight. 
 
There was no apparent decrease in the power load throughout the state, from 8:30 to 9:30 p.m.  No step changes, nothing, nada, zip, zilch.
There you have it, scientific data showing that the Earth Hour was a total bust in California.  If you look close, you can see a little bump up above the forecast demand, which tracked very closely with actual power consumed prior to the witching hour 8:30 to 9:30. But, it is clear that power consumption did not drop, it stayed up. Maybe all those protesters forgot to turn off the lights.




			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e975adbdb',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Guest post by Steven Goddard
I have been noticing in recent weeks that NSIDC extent is much closer to their 1979-2000 mean than NANSEN is to their 1979-2007 mean.  This is counter-intuitive, because the NANSEN mean should be relatively lower than NSIDC – as NANSEN’s mean includes the low extent years of the 2001-2007 period.  Those low years should have the effect of lowering the mean, and as a result I would expect the NANSEN current extent to be equal to or above the 1979-2007 mean.
(For exclusive subsets A and, B where subset A has a mean value of 14 and subset B has a mean value less than 14, then the mean of the full set AB must also be less than 14.)

NANSEN shows extent more than 500,000 km2 below the 1979-2007 mean


NSIDC shows extent less than 200,000 km2 below the 1979-2000 mean

I overlaid the NANSEN graph on top of the NSIDC graph below, and it is easy to see how large the discrepancy is.  In fact, the NSIDC mean sits at about one standard deviation below the NANSEN mean – which makes little sense given their base time periods.  It should be the opposite way.

(Note – the NANSEN and NSIDC measuring systems are not identical, and I had to make a shift along the Y-axis to line them up.  However, the X and Y scales are identical for both graphs in the overlay image.)


NANSEN and NSIDC combined
As mentioned above, one might expect that the current NANSEN extent would actually be above the 1979-2007 mean.  But something odd happened with the NANSEN data on December 13, 2008.  Overnight it lost about 500,000 km2 of ice, as Anthony captured in the blink comparator below.

Is it possible that there is still an error in the NANSEN data?  The discrepancy in the offset from the mean vs. NSIDC is rather large – nearly large enough to place California inside.  What are your thoughts?
I asked Dr. Walt Meier from NSIDC his opinion, and he replied (as always) courteously and promptly.  His answers are below:

Nansen uses a different algorithm to calculate the sea ice extent. The algorithms differ in the way combine the raw data together to estimate extent. As long as one uses the same algorithm, the stories are all the same, but the details can differ, more so at certain times of year. When there is a diffuse, broken up ice edge and melt is starting is one such time.
I suspect the Bering Sea is probably the region resulting in most of the differences. While our algorithm shows the region has mostly “ice-covered” the ice cover there is very fragmented, broken-up, and thin.
….
The other thing that’s important to mention is that I was referring simply to discrepancy between how close the current lines are to climatology. However, there is also generally an “offset” between algorithm outputs – a bias or mean difference between the algorithms that is fairly consistent throughout the record. That is why NSIDC’s climatology is different than the Nansen climatology.
The important thing to remember is that there is a good consistent record from the passive microwave data as long as you consistently use the same algorithm and the same processing. But you can’t mix and match products.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96aa8eda',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterPDO how entering the cool phase. Will it cool the planet in the decades ahead?
Will the new 2011-2020 decade be warmer or cooler than the last one?
That’s what we’re betting on (FOR CHARITY).
This bet is also known as the Honeycutt Climate Bet for Charity, after Rob Honeycutt who first proposed this bet. You can find the entire background here.
First of all Charles Zeller has also pledged $5,000 for the warm side, and has already paid half of it to Doctors Without Borders. So whatever happens from now on, I will consider that this blog has led to some kind of succees in that it has played a role in leading to this generous donation. The money will no doubt alleviate much pain and suffering among those of us who happened to be born in unfortunate economic and social conditions. It may even save a life or more. So hats off to Charles.
If you wish to join the Climate Bet of the Decade, i.e. Honeycutt Climate Bet For Charity, click here to see how. It’s real easy: just leave a reader comment and I’ll put your name, e-mail address, and amount on a list. That’s it. You’ll appear in the next update.
Here’s the latest list. I have to admit that the warmists, though small in number, are a generous bunch.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




They have pledged over $14,000.00 so far. The coolists have also made a good number of pledges, and have in fact far surpassed all my earler expectations. No matter who wins, some good will surely come out of this.
The cool side: (I hope my math is correct).

And the warm side:

If you wish to up your bet, just say so. If you wish to cancel it, well there I can’t help you. Pledged is pledged! 🙂
Share this...FacebookTwitter "
"

‘Tis the season for Internet shopping! Well, sort of. It’s actually the season for all kinds of shopping, with record‐​breaking consumer spending this year. Online holiday sales are expected to top $2.3 billion, but that’s a mere snowflake in the overall blizzard of retail activity. 



Electronic commerce is catching on, however, and with more than 140 million people online, it’s no longer the exclusive domain of computer nerds. Thus, _Newsweek_ recently declared that “it’s beginning to look a lot like an e‐​Christmas.” On the same day, _U.S. News & World Report_ observed that “shoppers from east to west seem determined to avoid traffic jams at the mall, long lines at the post office, and last‐​minute dashes to the supermarket.” 



But Internet shoppers are also avoiding something else: sales taxes. And thanks to the newly enacted Internet Tax Freedom Act, “tax‐​free” will be the rule for at least three years. 



That leaves some observers and state officials decidedly short on holiday cheer. For example, a recent article by technology commentator James Ledbetter denounced restrictions on Internet taxation as “unfair” to those who shop in stores. Similarly, a wire service story accused Internet vendors of enjoying a “free ride” and warned that local retailing could cease to exist. Not to be outdone, Congress appointed a commission earlier this month to study the issue.  




Without doubt, limiting states’ taxing authority leads to unequal taxation. Nevertheless, such limitations are a crucial component of American federalism. 



Of course, not all Internet purchases are tax‐​free, just those made across state lines. Out‐​of‐​state Web vendors — like their catalog cousins — aren’t required to collect sales taxes except in states where they maintain a physical presence. In other words, electronic commerce is treated exactly the same as mail‐​order business. And the Supreme Court has held that states have no authority to tax mail‐​order sales outside their borders absent explicit congressional authorization. Instead, most states ask consumers to pay a “use tax” in lieu of a sales tax on all purchases. Few consumers volunteer. 



At first glance, the pro‐​tax case sounds reasonable: why should identical items be taxed differently depending on how they’re purchased? Theoretically, they shouldn’t be. But in the real world, there are several reasons why allowing states to tax out‐​of‐​state electronic commerce is bad policy. 



First, there is no immediate danger of large revenue losses for traditional retailers or, by extension, for state tax authorities. Local stores cater to a customer’s desire for a hands‐​on experience, offer immediate gratification, don’t charge for shipping, and so will probably always dominate retailing. What’s more, shopping is for many people a pleasurable social experience that cannot be duplicated online. Thus, Internet sales won’t destroy “real” retailers, just as catalog sales haven’t. 



Empirical evidence supports that conclusion. In an era of almost no inflation, state budgets grew by 5 percent in FY97 and by more than 6 percent in FY98. Over the past four years, state tax collections have exceeded expectations by about $25 billion. It appears that there will be a sizable revenue windfall this year as well. With revenues pouring in so rapidly, it’s just not credible that electronic commerce is undermining state tax collections. 



Second, differentiated tax rates create healthy competition that helps keep local rates under control. For example, some residents of Manhattan drive to Delaware to avoid sales taxes — an option that has undoubtedly curbed the profligate fiscal habits of New York politicians. Electronic commerce serves as a similar safety valve that guards against excessive taxation. 



If states and localities feel compelled to tax their citizens more heavily, they still have the tools to do it. There will always be income taxes, property taxes, gas taxes, hotel taxes and the like. Perhaps what states really seek isn’t equity or revenue security but rather a new source of funds that doesn’t require voter approval. 



Finally, there is something inherently unsettling about states’ exercising legal authority outside their jurisdictions. By what right can New York force a firm in Florida to act as its tax collection agent? Even if it were constitutionally permissible, it would set a dangerous precedent with enormous potential for conflict. 



Without doubt, limiting states’ taxing authority leads to unequal taxation. Nevertheless, such limitations are a crucial component of American federalism. Absent those restraints, confiscatory state tax rates — which are the true injustice — would worsen. To improve their business climate, states should cut taxes, not scheme to collect more. 



For its part, Congress should stubbornly refuse to bow to state demands for new taxing authority. The Internet Tax Freedom Act was a good start in ensuring that traditional principles of remote commerce apply to the online world; Congress should consider making it permanent. 
"
"What can be done to limit global warming to 1.5°C? A quick internet search offers a deluge of advice on how individuals can change their behaviour. Take public transport instead of the car or, for longer journeys, the train rather than fly. Eat less meat and more vegetables, pulses and grains, and don’t forget to turn off the light when leaving a room or the water when shampooing. The implication here is that the impetus for addressing climate change is on individual consumers. But can and should it really be the responsibility of individuals to limit global warming? On the face of it, we all contribute to global warming through the cumulative impact of our actions.  By changing consumption patterns on a large scale we might be able to influence  companies to change their production patterns to more sustainable methods. Some experts have argued that everyone (or at least those who can afford it) has a responsibility to limit global warming, even if each individual action is insufficient in itself to make a difference.  Yet there are at least two reasons why making it the duty of individuals to limit global warming is wrong. Climate change is a planetary-scale threat and, as such, requires planetary-scale reforms that can only be implemented by the world’s governments. Individuals can at most be responsible for their own behaviour, but governments have the power to implement legislation that compels industries and individuals to act sustainably. Although the power of consumers is strong, it pales in comparison to that of international corporations and only governments have the power to keep these interests in check. Usually, we regard governments as having a duty to protect citizens. So why is it that we allow them to skirt these responsibilities just because it is more convenient to encourage individual action? Asking individuals to bear the burden of global warming shifts the responsibilities from those who are meant to protect to those who are meant to be protected. We need to hold governments to their responsibilities first and foremost. A recent report found that just 100 companies are responsible for 71% of global emissions since 1988. Incredibly, a mere 25 corporations and state-owned entities were responsible for more than half of global industrial emissions in that same period. Most of these are coal and oil producing companies and include ExxonMobil, Shell, BP, Chevron, Gazprom, and the Saudi Arabian Oil Company. China leads the pack on the international stage with 14.3% of global greenhouse gas emissions due to its coal production and consumption.  If the fossil fuel industry and high polluting countries are not forced to change, we will be on course to increase global average temperatures by 4°C by the end of the century. If just a few companies and countries are responsible for so much of global greenhouse gas emissions, then why is our first response to blame individuals for their consumption patterns? It shouldn’t be – businesses and governments need to take responsibility for curbing industrial emissions.  


      Read more:
      Climate action must now focus on the global rich and their corporations


 Rather than rely on appeals to individual virtue, what can be done to hold governments and industries accountable?  Governments have the power to enact legislation which could regulate industries to remain within sustainable emission limits and adhere to environmental protection standards. Companies should be compelled to purchase emissions rights – the profits from which can be used to aid climate vulnerable communities.  Governments could also make renewable energy generation, from sources such as solar panels and wind turbines, affordable to all consumers through subsidies. Affordable and low-carbon mass transportation must replace emission-heavy means of travel, such as planes and cars. More must also be done by rich countries and powerful industries to support and empower poorer countries to mitigate and adapt to climate change. All of this is not to say that individuals cannot or should not do what they can to change their behaviour where possible. Every little contribution helps, and research shows that limiting meat consumption can be an effective step. The point is that failing to do so should not be considered morally blameworthy.  In particular, individuals living in poorer countries who have contributed almost nothing to climate change deserve the most support and the least guilt. They are neither the primary perpetrators of global warming nor the ones who have the power to enact the structural changes necessary for limiting global warming, which would have to involve holding powerful industries responsible.  While individuals may have a role to play, appealing to individual virtues for addressing climate change is something akin to victim-blaming because it shifts the burden from those who ought to act to those who are most likely to be affected by climate change. A far more just and effective approach would be to hold those who are responsible for climate change accountable for their actions."
"

What’s worse than a public policy debate that turns bitter and impolite? Well, for one, having the courts step into the marketplace of ideas to judge which side of a debate has the best “facts.” Yet that’s what Michael Mann has invited the D.C. court system to do. In response to some scathing criticism of his methodologies and an allegation of scientific misconduct, the author of the infamous “hockey stick” models of global warming—because they resemble the shape of a hockey stick, with temperatures rising drastically beginning in the 1900s—has taken the global climate change debate to a record low by suing the Competitive Enterprise Institute, National Review, and two individual commentators. The good Dr. Mann claims that some blogposts alleging his work to be “fraudulent” and “intellectually bogus” were libelous. The D.C. trial court rejected the defendants’ motion to dismiss this lawsuit, holding that their criticism could be taken as a provably false assertion of fact because the EPA, among other bodies, have approved of Mann’s methodologies. In essence, the court seems to cite a consensus as a means of censoring a minority view. The defendants have appealed to the D.C. Court of Appeals (the highest court in the District of Columbia). Cato has filed a brief, joined by three other think tanks, in which we urge the court to stay out of the business of refereeing scientific debates. We argue that the First Amendment demands that failing to leave room for the marketplace of ideas to operate stifles academic and scientific progress, and that judges are ill‐​suited to officiate policy disputes—as history has shown time and again. The lower court clearly got it wrong here—and there are numerous cases where courts have more judiciously treated similarly harsh assertions for what they really are: expressions of disagreement on public policy that, even if hyperbolic, are among the forms of speech most deserving of constitutional protection. The point in this appeal is that courts should not be coming up with new terms like “scientific fraud” to squeeze debate over issues impacting government policy into ordinary tort law. Dr. Mann is not like a corner butcher falsely accused of putting his thumb on the scale or mixing horsemeat into the ground beef. He is a vocal leader in a school of scientific thought that has had major impact on government policies. Public figures must not be allowed use the courts to muzzle their critics. Instead, as the U.S. Supreme Court has repeatedly taught, open public debate resolves these sorts of disputes. The court here should let that debate continue outside the judicial system.
"
"Every country in the world is failing to shield children’s health and their futures from intensifying ecological degradation, climate change and exploitative marketing practices, says a new report. The report says that despite dramatic improvements in survival, nutrition, and education over the past 20 years, “today’s children face an uncertain future”, with every child facing “existential threats”. “In 2015, the world’s countries agreed on the sustainable development goals (SDGs), yet nearly five years later, few countries have recorded much progress towards achieving them,” says the report by a commission of 40 child and adolescent health experts from around the world. “Climate change, ecological degradation, migrating populations, conflict, pervasive inequalities, and predatory commercial practices threaten the health and future of children in every country,” it says.  The commission, convened by the World Health Organization (WHO), the United Nations children’s agency, Unicef, and medical journal the Lancet, calls for radical changes to protect children’s health and futures from the intensifying climate emergency. It also highlights the threat of predatory commercial practices, linking children’s exposure to marketing of fast food and sugary drinks to an 11-fold increase in childhood obesity, from 11 million in 1975 to 124 million in 2016. The report includes an index of 180 countries that compares data on survival, wellbeing, health, education and nutrition; as well as sustainability, with a proxy for greenhouse gas emissions, and equity, or income gaps. Norway, South Korea, the Netherlands, France and Ireland are found to be the best countries for a child to flourish in his or her early years. The Central African Republic, Chad, Somalia, Niger, and Mali are the bottom five in the list, based on the same ranking. But when performance is compared taking per capita carbon emissions into account, Burundi, Chad and Somalia are best performers, while the US, Australia and Saudi Arabia are among the bottom 10 countries. “When authors took per capita CO2 emissions into account, the top countries [on the child flourishing ranking] trail behind: Norway ranked 156, the Republic of Korea 166, and the Netherlands 160,” the report says. “Each of the three emits 210% more CO2 per capita than their 2030 target.” “The only countries on track to beat CO2 emission per capita targets by 2030, while also performing fairly (within the top 70) on child flourishing measures are: Albania, Armenia, Grenada, Jordan, Moldova, Sri Lanka, Tunisia, Uruguay and Vietnam,” the report says. The UK is ranked among the top 10 countries when it comes to child flourishing, but placed 133rd on “delivering on emissions targets”; it is “currently on track to emit 115% more CO2 than its 2030 emissions target”. Experts behind the report agree that “while the poorest countries need to do more to support their children’s ability to live healthy lives, excessive carbon emissions – disproportionately from wealthier countries – threaten the future of all children”. Stefan Peterson, Unicef’s chief of health, said children living in the poorest countries are facing the brunt of a changing climate, despite having a tiny carbon footprint. “These children face enormous challenges to their health and wellbeing, and are also now at the greatest disadvantage due to the climate crisis,” he said. “We need sustainable gains in child health and development, which means that big carbon emitters need to reduce their emissions for all children to thrive, poor and rich.” The report says: “If global warming exceeds 4C by the year 2100 in line with current projections, this would lead to devastating health consequences for children, due to rising ocean levels, heatwaves, proliferation of diseases like malaria and dengue, and malnutrition.” Anthony Costello, professor of global health and sustainable development at University College London, said the commission was calling for a radical rethink on global child health. “Climate change threatens our children’s future so we must stop carbon emissions as soon as possible,” he told the Guardian. “Our new index shows that not a single country performed well on both child development and emissions indicators. “We also call for greater regulation of marketing of tobacco, alcohol, formula milk, sugar-sweetened beverages and gambling to children, and of social media companies which target children through secret algorithms and the inappropriate use of their personal data.”  The report says children are at risk from harmful marketing. “Evidence suggests that children in some countries see as many as 30,000 advertisements on television alone in a single year, while youth exposure to vaping (e-cigarettes) advertisements increased by more than 250% in the US over two years, reaching more than 24 million young people.” Industry self-regulation has failed, said Costello, adding that in Australia, for instance, “children and adolescent viewers were still exposed to 51 million alcohol ads during just one year of televised football, cricket and rugby”. “The reality could be much worse still,” he said. “We have few facts and figures about the huge expansion of social media advertising and algorithms aimed at our children.” The commission calls on governments to put measures in place “to ensure children receive their rights and entitlements now and a liveable planet in the years to come”. “We live in an era like no other. Our children face a future of great opportunity, but they stand on the precipice of a climate crisis … our challenge is great and we seem to be paralysed,” it says."
nan
"The UK’s accounting watchdog has launched a major review into whether companies and their auditors are adequately reflecting the financial risks of the climate crisis in their accounts. The Financial Reporting Council, which sets reporting standards for all listed companies in the UK, plans to use the review to make sure companies are being clear with investors about their exposure to climate risks. The review could lead to tougher disclosure rules for UK listed companies and more scrutiny on the work of accounting firms – including KPMG, EY, Deloitte and PwC – in helping investors to identify climate risks. Sir Jon Thompson, the FRC’s chief executive, said company reports and accounts were “essential to understanding how the corporate world is responding to the challenge of climate change”. “Not only do boards of UK companies have a responsibility to report their impact on the environment and the risks of climate change to their business, but investors expect them to operate sustainably,” he said. “Auditors have a responsibility to properly challenge management to assess and report the impact of climate change on their business.” The FRC will also examine whether companies have adopted the recommendations put forward by the the Taskforce on Climate-related Financial Disclosures (TCFD), which was set up to highlight the financial exposure of companies to the risk of climate chaos. Mark Carney, the outgoing Bank of England governor, warned companies late last year to use their next two annual financial reports to road-test how they represent climate risks in their accounts. He said that while many of the largest banks and energy companies had made progress in how they report their climate risks, this progress is “uneven across sectors”. Investors are calling for better climate disclosure standards as they face increasing pressure to prove they are investing responsibly. Under a new voluntary City code, which came into effect this year, pension funds and asset managers must publish annual reports showing they have taken environmental, social and governance (ESG) issues into account when investing. The FRC, which governs the code, said it would help “align the approach of the whole investment community”. The FRC plans to select a sample of company financial reports from across different industries to assess to what extent they comply with reporting requirements in relation to the climate emergency. It will investigate whether auditors have assigned enough resources to analysing climate risks and whether the impact of a climate crisis has been appropriately reflected in company reports and accounts. The first set of accounts to face the FRC’s scrutiny under its new investigation will be taken from this year, which will be published from early 2021."
"

 _Global Science Report_ _is a weekly feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   
  
As economic heavyweights assembled for their annual summit held by the World Economic Forum (WEF) in Davos, Switzerland, they were greeted by a call for $700 billion/yr of increased spending out to the year 2030 to “to close the green investment gap worldwide, leading to sustainable economic growth that attains global climate change goals.” They were told that this goal can be reached through an additional $36 billion/yr investment from the world’s governments (on top of the $96 billion/yr currently spent) that will “spur up to US$ 570 billion in private capital needed to avoid devastating climate impacts on economy.”   
  
This call was made by the WEF’s own Green Growth Action Alliance as it released its first Green Investment Report at the outset of the Davos conference.   
  
The Green Growth Action Alliance justified the call for the extra spending this way:   




Such investments are urgently needed to avoid the potentially devastating impacts of climate change and extreme weather events as witnessed in many parts of the world in 2012. Scientists agree that extreme weather has become the “new norm” and comes at a huge, and rising, cost to the global economic system. Without further action, the world could see a rise in average global temperatures by 4ºC by the end of the century. According to scientists, this could lead to further devastating impacts, including extreme heat waves, more intense tropical storms, declining global food stocks and a sea-level rise affecting hundreds of millions of people.



Using a poor excuse to call for a bad idea doesn’t seem much like progress.   
  
The science of global warming re extreme events is hardly compelling. The data noise, generated from both natural processes and from other human influences, largely overwhelms any anthropogenic greenhouse effect signal in most cases.   
  
However, compelling evidence is emerging that the magnitude of the climate sensitivity—that is, how much warming we should expect from a doubling of atmospheric carbon dioxide concentration—has been overestimated. Even if there was good scientific evidence that higher temperatures lead to a more “extreme” climate (there’s just about as much evidence for the opposite), an overestimate of the sensitivity would lead to an overestimate of extremes.   
  
And these overestimates are being used by the Green Growth Action Alliance to oversell the need to do something about climate change.   
  
In fact, there are much more pressing needs.   




For example, according to the International Energy Agency (IEA), there are currently 1.3 billion people globally without access to electricity. The IEA recognizes that getting these folks hooked up is imperative for economic growth:   




Energy alone is not sufficient for creating the conditions for economic growth, but it is certainly necessary. It is impossible to operate a factory, run a shop, grow crops or deliver goods to consumers without using some form of energy. Access to electricity is particularly crucial to human development as electricity is, in practice, indispensable for certain basic activities, such as lighting, refrigeration and the running of household appliances, and cannot easily be replaced by other forms of energy. Individuals’ access to electricity is one of the most clear and un-distorted indication of a country’s energy poverty status.



It would seem to us, that getting electricity to those without is a better way to achieve the Green Growth Action Alliance’s goal of “driving development and well-being” than is “reducing greenhouse gas emissions.”   
  
And the best way to get (cheap, reliable) electricity to large numbers of people is through electricity systems that are powered by greenhouse gas-emitting fossil fuels. This is not to say that there are not instances where boutique energy sources such as solar may provide a better solution, but just that those instances are minor compared to the magnitude of the task—which makes doubly bad Green Growth Action Alliance advice to shift substantial capital from fossil fuel projects to help fund its green solutions.   
  
The bottom line is this: Fossil-fuel energy leads to more people with electricity which leads to more economic growth which leads to richer, more stable, more resilient and more environmentally-friendly societies with greater wellbeing.   
  
Don’t get us wrong, we are all for market-driven innovation, but in Davos, the urgency for such innovation is being overhyped, and the situation is made worse by the urging for public sector spending in order to fuel it.


"
"Jeff Bezos, the Amazon founder and Washington Post owner, announced on Monday that he was donating $10bn to save the Earth’s environment – barely a month after it was revealed Amazon threatened to fire employees who spoke out about the company’s role in the climate crisis. The new Bezos Earth Fund will start distributing the money this summer, the multi-billionaire said in an Instagram post to his 1.4 million followers. “I want to work alongside others both to amplify known ways and to explore new ways of fighting the devastating impact of climate change on this planet we all share,” Bezos said in the post. “This global initiative will fund scientists, activists, NGOs – any effort that offers a real possibility to help preserve and protect the natural world. We can save Earth. It’s going to take a collective effort from big companies, small companies, nation states, global organizations and individuals.” The announcement appears to contradict Amazon’s actions towards employees speaking publicly about the climate crisis. In January, the Guardian revealed that several workers who called for stronger climate action by the company were warned to be quiet or face dismissal. Bezos, the world’s richest man with a personal net worth of $129.9bn according to Forbes, has clashed with US president Donald Trump, a frequent climate change denier, on many occasions, notably after the US withdrew from the Paris climate agreement. “Anybody today who is not acknowledging that climate change is real – that we humans are affecting the planet in a very significant and dangerous way – those people are not being reasonable,” Bezos told Amazon’s Smbhav summit for small and medium-sized businesses in India last month."
"A supercomputer designed to improve extreme weather and climate forecasting is to receive £1.2bn from the UK government towards its development. The technology will be managed by the Met Office, with more sophisticated rainfall predictions and improving forecasting at airports among its aims.  Data collected by the powerful device will also be used to predict storms more accurately, select the most suitable locations for flood defences and forecast changes to the global climate. The supercomputer is expected to be the most advanced of its kind dedicated to weather and climate in the world. The Met Office’s current supercomputer, which is due to reach its end of life in late 2022, is among the world’s 50 most powerful computers, and contains enough storage to hold more than 100 years of HD films. “This investment will ultimately provide earlier, more accurate warning of severe weather, the information needed to build a more resilient world in a changing climate and help support the transition to a low-carbon economy across the UK,” said Prof Penny Endersby, the Met Office’s chief executive. “It will help the UK to continue to lead the field in weather and climate science and services, working collaboratively to ensure that the benefits of our work help government, the public and industry make better decisions to stay safe and thrive.” The government hopes the technology will help communities better prepare for weather disruption such as that from recent storms Dennis and Ciara. The supercomputer itself is expected to cost £854m, with remaining funds going towards investment in the Met Office’s observations network and programme offices, over a 10-year period from 2022 to 2032. “Over the last 30 years, new technologies have meant more accurate weather forecasting, with storms being predicted up to five days in advance,” said Alok Sharma, the business and energy secretary and Cop 26 president. “Come rain or shine, our significant investment for a new supercomputer will further speed up weather predictions, helping people be more prepared for weather disruption from planning travel journeys to deploying flood defences.”"
"
Share this...FacebookTwitterMeteorologist Karsten Brandt of donnerwetter.de projects more cold winters ahead. (Photo: Donnerwetter.de)
So forget the Met Office and PIK, who have proven themselves to be quite the laughing stocks. All that good money flowing into these institutions, and such rubbish coming out.
Meteorologist Karsten Brandt of German weather service company donnerwetter.de provides us with an outlook for 2011 and the next 10 years ahead. Read here (in German).
More cold winters over the next 10 years
Lately we’ve been hearing a lot about the North Atlantic Oscillation (NAO) going negative, which appears to be linked to quiet solar activity. (On the other hand we’ve been hearing from AGW true believers that the negative AO is caused by sea ice changes, which are caused by Arctic warming, which is caused by man-made trace CO2 emissions).
Clearly though, the NAO plays an important role in Europe’s weather. Knowing what the NAO will do for the next 10 years allows you to make a predictions for Europe’s climate ahead. Karsten Brandt writes:
It is even very probable that we will not only experience a very cold winter, but also in the coming 10 years every second winter will be too cold. Only 2 of 10 will be mild.
Dr Brandt is not some lone guy out there making this kind of blasphemous prediction. A large number of meteorologists and climatologists are projecting the same, e.g. like Accuweather’s Joe Bastardi and warmist climatologist Mojib Latif. Even the Potsdam Institute for Climate Impact Research is joining the chorus, but claiming the cold is due to warming.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




2011 in Germany about 1°C cooler 
Over the short-term Dr Brandt says that making a reasonable forecast for the next 2 months is not that hard, and that one can generate a pretty good idea of how the rest of the year will develop. Last year much of western Europe had a cooler year. What ‘s the outlook for Germany for 2011? Brandt writes:
Compared to the last 20 years, 2011 will turn out with a high probability to be too cold. Instead of 11°C in the west, or 10°C in the north and east, we expect only 8-10 °C, i.e. 1°C colder than the last years. Especially the cold start in the year and the suspected once again colder end of the year will pull the year’s temperature down.
Hopefully municipalities will procure the necessary wintertime snow-clearing equipment and stockpile sufficient amounts of sand and road salt so that next winter citizens will not have to endure the horrific traffic conditions that they are now experiencing over much of Europe. And you home centers ought to think about ordering snow shovels next fall.
But don’t despair, Brandt says the chances for a warm spring and a warm summer month aren’t bad.
Finally, here’s what the National Weather Service is showing:

Share this...FacebookTwitter "
"When you hear about businesses with a high environmental impact or activities with a high carbon footprint, you are probably more likely to imagine heavy machinery, engines and oil rather than hairdressing. Yet hairdressing, both as a sector and as an individual activity, can have a massive carbon footprint.  Hairdressing uses high levels of hot water, energy and chemicals. Similarly, in our homes, heating hot water is typically the most energy intensive activity. For the cost of a ten-minute shower that uses an electric immersion heater, you could leave a typical television on for 20 hours.  So while it helps to turn lights and appliances off, the real gains in terms of reducing energy usage are in slashing our use of hot water. A quarter of UK emissions are residential and, of those, the vast majority come from running hot water. The longer it runs and the hotter it is, the more energy intensive (and costly) it is.  Most people use too much shampoo and wash their hair too often. A daily routine of shampooing your hair twice followed by a wash out conditioner uses annually about 14,222 litres of water and 1252kWh of energy, costs about £245, and has a carbon footprint of 500kg of carbon dioxide equivalents (CO₂e).  On the other hand, if you shampoo your hair twice a week (supplementing that with a dry shampoo if needed) and use a leave-in conditioner, you will use annually just 613 litres of water and 55 kWh of energy, produce a carbon footprint of 25kg of CO₂e, and cost yourself about £27 a year.  Research has also revealed how shampoo can contribute to pollution. Maybe this in part explains why sales of shampoo have fallen over the past few years in the UK – with many people choosing to wash their hair less often.  Washing your hair less doesn’t just save you money, it’s also much better for your hair condition. It can also help to limit the ageing effects of over exposure to hot water and chemicals on your skin.  My latest research project looks at the issue of sustainability across the hairdressing sector. Not only is the hair sector a high user of resources, but hairdressers probably talk to more people than any other occupation – and are in a great position to pass on advice about lower resource hair care.  From speaking with hairdressers, it seems that ever since the episode of Blue Planet II in which David Attenborough explained how a whale mother was still carrying her dead baby which, it was claimed, had been poisoned by plastics (though scientists working on the show have confirmed there was no actual evidence to prove this) salons have been seeing a massive increase in clients wanting to know that their hairdresser is doing their bit.  Our research has found that many hairdressers are keen to make changes that are better for the environment. The opportunity to present their industry as part of the solution rather than part of the problem is very attractive to hairdressers, as it boosts their sense of professional identity and pride in offering a well informed service.  A large focus of the project has been on equipping hairdressers with the skills and knowledge required for them to talk to their clients about sustainable hair care. There are many products out there that are better for the environment, not because they have “organic” or “eco” on the label, but because they reduce the need for hot water.  Dry shampoo is a great example. It is fast, convenient, and great at festivals and on the move. It also makes hair easier to style, is cheap and avoids the need for any hot water. Similarly, leave-in conditioner avoids the need for an extra rinse and again makes hair easier to style. It is also fantastic at giving body to fine hair, and saves water, energy, money and time.  Our ecohair project, run in association with the Vocational Training Charitable Trust and the Hair and Beauty Industry Authority, provides a sustainable stylist certificate at no cost, once hairdressers have completed the training programme. The salon owner can also obtain a sustainable salon certificate to let customers know these things are important to their business.  Getting certified as a sustainable salon has numerous benefits, and not just in terms of reputation. Adopting the changes as part of the scheme saves the typical salon 286,000 litres of water, 24150 kWh of energy and £5,300 a year.  And with new research showing the increased threat of climate change and the need for urgent behavioural change, it is great that simple alterations to our hair care routines – and where we choose to get our hair cut (you can find sustainable salons here) – can make such a difference to the planet we call home."
"
Share this...FacebookTwitterAlthough the Potsdam Institute For Climate Impact Research (PIK) gives the impression that it is a climate research facility, it also appears to have become an institute for formulating novel economic policy.
H/t: reader Ike
The PIK, commissioned by the German Federal Ministry for the Environment, Nature Conservation and Nuclear Safety, has produced and released a NEW SYNTHESIS REPORT that claims Europe can revitalize its economy by tackling the climate challenge, namely by raising the European climate target for emissions reductions from 20% to 30%. The report is titled:

A New Growth Path for Europe – Generating Prosperity and Jobs in the Low Carbon Economy”
Tipping point to prosperity at 30%
The PIK seems to be claiming there is an economic tipping point to prosperity at 30% emissions reduction. The onlineDie Zeit writes on the new PIK report:
Europe should reduce its greenhouse gas emissions by 30% instead of only 20% by 2020, which is the current plan. This is how the continent could overcome its economic stagnation.
If they stick to the 20% target, ‘then it would be like someone stuck in a hole who is digging deeper’.”
Based on climate-economic models!
The PIK claims a 30% reduction by 2020 would lead to higher growth and increased employment. These projections are based on “new model results”. (PIK models have an incurable habit of producing exactly what the PIK wants to see).
In the coming decade, Europe will need to accept the challenge of increasing economic growth while reducing both unemployment and greenhouse gas emissions. New model results show that these three goals can actually reinforce one another.”
Yet, there must be something terribly wrong with their models because a slew of European governments have been recently forced to do just the exact opposite, due real-life economics, and scale back subsidies to money-losing  green energy sources – especially solar.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




But the PIK has never been deterred by the harsh truths of reality, and claims:
Clear policies associated with a decisive move to a 30% target can be doubly beneficial for the climate and the EU economy.”
Their “new model results” also say their new plan would:
• increase the growth rate of the European economy by up to 0.6% per year.
• create up to 6 million additional jobs Europe-wide.
• boost European investments from 18% to up to 22% of GDP.
• increase European GDP by up to $842 billion (2004 dollars).
• increase GDP by up to 6% both in the old (EU15) and new (EU12) member states.”
For the first time in the academic modelling field
The above projections all sound so rosy. So just exactly what kind of brilliant mastermind plan did the PIK use to produce such rosy projections? (Hang on to your chair!):
For the first time in the academic climate modeling field, the present study has taken a state-of-the-art model of climate economics and enhanced it along those lines. The enhanced model includes:
• the fact that investments depend on subjective expectations, not on correct previsions of whatever future possibilities may arise.
• the fact that higher investments trigger higher learning-by-doing, thereby reducing unit costs.
• the resulting existence of different possible equilibria with different growth paths.
The new simulations show that 30% is achievable and can be economically beneficial by shifting the European Economy into a new, more advantageous equilibrium – a path of low-carbon growth.”
I don’t know about you, but I’d be a little wary of the “first-time in the academic climate modeling field” point, especially in a field as complex as economics. Thinking that these things work without glitches after just one tune-up sets a new standard in naiveness.
Apparently I’m not the only one who’s palm over face on this. Even Germany’s greenie Environment Minister Norbert Röttgen is not touching this with a 10-foot pole. According to Die Zeit:
Even Environment Minister Norbert Röttgen has placed little emphasis on the presentation of the new study: Instead of travellingng to Brussels himself, he sent his secretary Katherina Reiche.”
Like the old promises of government central planning, PIK’s plan is a roadmap to a disaster.

Share this...FacebookTwitter "
"
Share this...FacebookTwitterYesterday I wrote about the latest issue of FOCUS news magazine HERE.
I just picked up the new issue, out today, and read it. It’s an unmistakable departure from usual doom and catastropheism, which had generated so much of the fear and urgency needed to justify the rush to reckless policy making and profiteering.
FOCUS shows that the urgency is undue and that it’s time to get back to rationality, and away from all the mass hysteria that has taken policymaking on a ride on the crazy train.
Part 1: It’s getting warmer – that’s good!
In its first part, FOCUS describes the Sahara region as it was thousands of years ago – an area rich in wildlife and plants where humans settled and even built a 500 meter long by 5 meter wide protective wall 1000 years before Christ. Then came a climate catastrophe (naturally of course), in the form of cooling and drying. The once green paradise dried up into a desert wasteland. Today the Sahara desert, thanks to warming, is greening up again. Says Stefan Kröpelin, geo-archaeologist of the University of Cologne, who has been researching the region for 30 years now:
At the southern edge, vegetation in most places has been moving northwards since the end of the 1980s. Global warming here has been a blessing. If the trend continues at this pace, the Sahara will be green again in a few hundred years.”
The Sahara is just one example of the advantages of global warming. FOCUS also writes:
More and more renowned scientists are saying climate change does not lead to only catastrophes, rather it also brings with it rich advantages for both man and nature. But this will hardly play a role in Cancun. Politicians, scientists and media are too fixated on the problems of the future – from sea level rise, to storms, to the spread of tropical diseases (see page 86).
Studies worldwide show that many of the widespread horror scenarios are baseless.”
FOCUS continues to believe the science underpinning the theory that global temperatures will rise 2 – 4°C by the end of the century, and this being due to man’s activities. Here, FOCUS naively ignores the impacts of oceanic and solar cycles. But even so, it has, at least in this article, truly departed from the planet-is-going-to-hell-in-a-hand-basket narrative. That’s huge progress for traditional German journalism – make no mistake about it.
And what does Stefan Kröpelin say about climate models for the future?
I trust the data from the earth’s history more than any climate model.”
Josef Reichholf, Professor Emeritus of Ecology and Evolutionary Biology at the University of Munich speaks on biodiversity.
The earth’s history shows that warm periods are characterized by high levels of biodiversity. In general, the rule is: the warmer it is, the more biodiversity you get.”
FOCUS then writes about the warm and cold cycles the earth has experienced, in particular the ice ages that have occurred every 100,000 years and the massive ice sheets that once covered many parts of the globe. This part really puts the earth and climate in the right perspective. Readers see that things have been far more extreme than the puny half-degree fluctuations we are biting our nails over today. The earth does change naturally, and often dramatically.
FOCUS also writes about the Holocene optimums and minimums, and the challenges and benefits that man derived from them. Climate has always been changing. The Vikings even settled in Greenland in the year 982, FOCUS points out.
FOCUS also puts CO2 in the spotlight and discovers that it is not that “climate-killing” gas everyone has been making it out to be. Indeed, the gas is actually a fertilizer for plants. It makes the planet greener. It boosts agricultural yields, which means more food to feed the world.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




CO2 is often called a climate poison or climate killer, but: ‘It is an essential building block for photosynthesis, and thus the basis for all life,’ says Hans-Joacheim Weigel, Director of the Johann Heinrich von Thünnen Institute for Biodiversity.”
Part 2: The 7 scourges of the end time
FOCUS then looks at the 7 climate scourges alleged will occur as the world ends, or something, and debunks them one by one. Here I think FOCUS did a good job – a must read. Here I summarize with just a few key words.
FOCUS (in a few words):
1. Islands will sink: “Satellite photos show they are not”
2. Disappearing glaciers. FOCUS writes: “not a new phenomena”
3. Melting of the ice caps: “Highly improbable – very high temperatures would be needed”
4. Gulf Stream collapse: “NASA shows it’s not true”
5. More storms: “Chris Landsea shows it won’t be so.”
6. Polar bears will die: “They are growing in number and are highly adaptable – no big problem.”
7. Tropical diseases will spread: “DDT ban was a cause. And Paul Reiter says it’s BS.”
Part 3: Who’s who in climate science?
One interesting section of the FOCUS report was the part on who’s who in climate science? They list 11 names:
1. Rejendra Pachauri: “calls to resign”
2. Michael Mann: “doubts on his methodology”
3. James Hansen: “warning of AGW since the 1980s”
4. John Christy, Atmospheric Sciences:
Analyzes climate data from satellites at the University of Alabama. Is against alarmist statements from other scientists who warn of catastrophic temperature increases and sea level rises, and considers climate protection measures unnecessary.”
5. Stephen McIntyre, mining specialist:
The Canadian analyzed Mann’s hockey stick curve together with economist Ross McKitrick, who both found deficiencies in the methodology, which put the curve’s shape into question, as well as the claim that never in the last 1000 years has it been warmer than today. McIntyre became known when Mann refused to reveal his data. He is active with his blog Climate Audit.”
6. Stefan Rahmstorf: “A lead author of the IPCC 4AR report and many publications”
7. Björn Lomborg: “Recommends adaptation”
8. Mojib Latif: “Projected a pause in global warming”
9. Richard Lindzen:
Researches at MIT and one of the most prolific skeptics. He says the earth is never in equilibrium, and for that reason natural changes such as ocean currents or atmospheric cycles can explain the warming. For this reason it is senseless to attempt to fight climate change.”
10. Nicholas Stern: “Inaction is more expensive than action.”
11. Paul Crutzen: “Advocates geo-engineering.”
So much for the ballyhooed consensus and settled science. This FOCUS article shows that the science is hotly disputed, and more importantly, that the catastrophe scenarios are hysteria, and that warmer climates would bring real benefits.
For German journalism, this to me represents a watershed event. This piece breaks a lot of taboos in Germany. The reactions indeed will be worth following. Expect hellfire and vitriol from the greenshirts.
I wish I had time to write more, but this I think is a good overview of the extensive 14-page FOCUS piece. Go out and buy it.
Share this...FacebookTwitter "
"

The lead story on the June 29 MSNBC News was that there were terrible floods in the United States and–interspersed in the middle of the story–that global warming is going to be even worse than we thought. This was one‐​sided, emotional science reporting even worse than I thought possible. 



The story was rooted in a recent study by Tom Wigley, introduced as “a respected climatologist.” Wigley’s study was financed by the Pew Foundation, which is running a multi‐​million‐​dollar campaign to hype global warming. 



Wigley says that sulfate aerosols will be legislated out of existence faster than previously thought. He champions the theory that sulfates reflect away the sun’s radiation, conveniently explaining why the planet has warmed so little despite the claims of warming doomsayers and their computers. 



Without sulfate aerosols, computer models indicate our hemisphere should have already warmed about 2.3 degrees Celsius as a result of the greenhouse effect. The observed warming this century is a scant 0.65 degrees. If the sulfate hypothesis fails, the argument devolves into what the “skeptics” have said for decades: the earth simply isn’t going to warm all that much. 



Having held a doctorate in climatology for two decades, I feel confident in saying that every one of my colleagues who has expressed an opinion to me dislikes Wigley, mainly because he seems arrogantly dismissive of some facts when they get in the way of his theories. He actively discourages the airing of points of view that conflict with his. 



In October 1994, at a global warming meeting called by Rep. John Dingell (D‐​Mich.), Wigley was confronted with the reality that satellites had found no warming. He merely waved his hands and said, “Oh come now, that’s just the satellite data.” Oh come now, Tom, it’s just the only global measure of temperature that exists! 



The sulfate aerosol theory is politically correct, because some explanation is needed for why the early climate models flopped so badly. They served as the basis for the United Nations climate treaty, recently modified in Kyoto to force the United States to reduce its greenhouse gas emissions by as much as 45 percent in 8.5 years. Even though the treaty would devastate the U.S. economy, Wigley thinks it is not enough, saying we need “nine more Kyotos.” 



Needless to say, Tom is very big with the folks like the Pew Foundation and the United Nations, both of which seem to care not a whit about the destruction of the American economic miracle as long as dreaded global warming is banished. 



He co‐​authored a famous 1996 paper that showed that, from 1963 through 1987, the behavior of the atmosphere did in fact increasingly resemble that of one in which greenhouse warming was being counteracted by sulfate cooling. But the data available for his study actually began in 1957 and ended in 1995. When all of the data on the critical region of the atmosphere was looked at, there was no change whatsoever! Wigley has never given a satisfactory explanation of why he ignored all the data. Should he write a response to this paper, I reserve the right to reply. 



Wigley does not like to be confronted with this in public. Resources for the Future, a Washington outfit big in the global warming game, recently held a forum featuring Wigley but none of his critics. When pressed, Wigley said that his critics did not belong on the stage with him because they generally did not publish their work in peer‐​reviewed literature. Hogwash. Wigley’s critics are among the most published in the business. 



Arizona State’s Robert Balling may be the most prolific living climatologist. Only a handful of papers have mathematically searched for predicted greenhouse signals–and several are mine. The five most prominent critics have published nearly a thousand articles, mostly in peer‐​reviewed journals. Critics include a member of the National Academy of Sciences, the former head of the American Physical Society, heads of major research laboratories and former presidents of other scientific and professional societies. The fact that they even appear in the literature at all, given the thousands of people who lose some of the $2.1 billion that we spend each year on global change research if it all goes kerblooey, is testimony to the cogency of their arguments. Maybe that’s why Wigley doesn’t want any opposition. 



Sulfates don’t do a good job of explaining the failure of the models noted above. NASA scientist James Hansen, who essentially ignited the greenhouse issue 11 years ago with his flamboyant congressional testimony, has become very skeptical about sulfates. University of Washington scientist Peter Hobbs found that sulfates off the East Coast are overwhelmed in their own plume by black carbon particles that absorb radiation and cancel sulfate cooling. And throughout the eastern United States, where sulfates have been in decline for the last 30 years, the temperature hasn’t budged during the entire century. 



Any or all of these observations could have been offered by “respected climatologists” if MSNBC had bothered to do a little legwork. The real “story behind the story” is why they didn’t.
"
"

And yet to play out, let’s also not forget Al Gore’s 2008 prediction: “Entire north polar ice cap will be gone in 5 years”
-Anthony
By Dennis  Avery  in the Canada Free Press
“2008 will be the hottest year in a  century:” The Old Farmers’ Almanac, September 11, 2008, Hurricanes, Arctic  Ice, Coral, Drinking water, Aspen skiing
We’re now well into the earth’s third straight harsher  winter-but in late 2007 it was still hard to forget 22 straight years of global  warming from 1976-1998. So the Old Farmer’s Almanac predicted 2008 would be the  hottest year in the last 100.
But  sunspots had been predicting major cooling since 2000, and global temperatures  turned downward in early 2007. The sunspots have had a 79 percent correlation  with the earth’s thermometers since 1860. Today’s temperatures are about on a  par with 1940. For 2008, the Almanac hired a new climatologist, Joe D’Aleo, who  says the declining sunspots and the cool phase of the Pacific Ocean predict  25-30 years of cooler temperatures for the planet.
“You could potentially sail, kayak or even  swim to the North Pole by the end of the summer. Climate scientists say that the  Arctic ice . . . is currently on track to melt sometime in 2008.” Ted  Alvarez, Backpacker Magazine Blogs, June, 2008.
Soon after this prediction, a huge Russian icebreaker got  trapped in the thick ice of the Northwest Passage for a full week. The Arctic  ice hadn’t melted in 2007, it got blown
into warmer southern  waters. Now it’s back. (Reference)
Remember too the Arctic has its own 70-year climate cycle.  Polish climatologist Rajmund Przbylak says “the highest temperatures since the  beginning of instrumental observation occurred clearly in the 1930s” based on  more than 40 Arctic temperature stations.
(This uneducated prediction may have been the catalyst for Lewis Pugh and his absurd kayak stunt that failed miserably – Anthony)
“Australia’s Cities Will Run Out of Drinking  Water Due to Global Warming.”
Tim  Flannery was named Australia’s Man of the Year in 2007-for predicting that  Australian cities will run out of water. He predicted Perth would become the  “first 21st century ghost city,’ and that Sydney would be out of water by 2007.  Today however, Australia’s city reservoirs are amply filled. Andrew Bolt of the  Melbourne Herald-Sun reminds us Australia is truly a land of long droughts and  flooding rains.
“Hurricane Effects Will Only Get Worse.” Live Science,  September 19, 2008.
So wrote the on-line  tech website Live Science, but the number of Atlantic hurricanes 2006-2008 has  been 22 percent below average, with insured losses more than 50 percent below  average. The British Navy recorded more than twice as many major land-falling  Caribbean hurricanes in the last part of the Little Ice Age (1700-1850) as  during the much-warmer last half of the 20th century.
“Corals will become increasingly rare on reef  systems.” Dr. Hans Hoegh-Guldberg, head of Queensland University (Australia)  marine studies.
In 2006, Dr.  Hoegh-Guldberg warned that high temperatures might kill 30-40 percent of the  coral on the Great Barrier Reef “within a month.” In 2007, he said global  warming temperatures were bleaching [potentially killing] the reef.
But, in 2008, the Global Coral Reef  Monitoring Network said climate change had not damaged the “well-managed” reef  in the four years since its last report. Veteran diver Ben Cropp said that in 50  years he’d seen no heat damage to the reef at all. “The only change I’ve seen  has been the result of over-fishing, pollution, too many tourists or people  dropping anchors on the reef,” he said.
No More Skiing? “Climate Change and  Aspen,” Aspen, CO city-funded study, June, 2007.
Aspen’s study predicted global warming would change the climate  to resemble hot, dry Amarillo, Texas. But in 2008, European ski resorts opened a  month early, after Switzerland recorded more October snow than ever before.  Would-be skiers in Aspen had lots of winter snow-but a chill factor of 18 below  zero F. kept them at their fireplaces instead of on the slopes.
*Sources:
Predictions  of 25-30 year cooling due to Pacific Decadal Oscillation:  Scafetta and West,  2006, “Phenomenological Solar Signature in 400 Years of Reconstructed Northern  Hemisphere Temperature Record,” Geophysical Research Letters.
Arctic Warmer in the 1930s:  R. Przybylak,  2000, “Temporal and Spatial Variation of Surface Air Temperature over the Period  of Instrumental Observation in the Arctic,” International Journal of Climatology  20.
British Navy records of Caribbean  hurricanes 1700-1850:  J.B. Elsner et al., 2000, “Spatial Variations in Major  U.S. Hurricane Activity,” Journal of Climate 13.
Predictions of coral loss:  Hoegh-Guldberg et al., Science, Vol.  318, 2007. Status of Coral Reefs of the World 2008, issued by the Global Coral  Reef Monitoring Network, Nov., 2008.
Aspen climate change study:  Climate Change and Aspen: An  Assessment of Potential Impacts and Responses, Aspen Global Change Institute,  June, 2007.
(1) Reader  Feedback | Click  here to get Canada Free Press in your email
Dennis T. Avery, is a senior fellow with  the Hudson Institute in Washington.  Dennis is the Director for Global Food  Issues ([url=http://www.cgfi.org]http://www.cgfi.org[/url]).  He was formerly a senior analyst for the Department of State. 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a091411',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter""I see lots of cold and snow in the warm times ahead!"" Britons stop believing. Welcome back to reality. (Photo source Wikipedia)
Britons are finally waking up from that global warming spell cast upon them years earlier.
Hat Tip Dirk H
What happens when the predictions of mountebanks (my favorite word lately) don’t come true, and the opposite happens?
People stop believing. That’s what’s happening now in Great Britain.
The online Daily Mail reports today on how the number of sceptic Britons has doubled in the last 4 years.
Obviously they don’t believe the junk science that insists global warming leads to more extreme cold and snow. Sorry, but many people aren’t that stupid to buy it. So take that crap alarmist science and use it for the next time you have to go.
This new scepticism in Britain is based on real-life observation, with people comparing the horoscopes of snowless winters heard earlier to the reality that is observed in real life today.
And for those who still believe in AGW, many now don’t think that it’s a real problem. The Daily Mail writes:
Fewer than half those polled – 46 per cent – are ready to use their cars less, and only 47 per cent are prepared to take public transport more often. Fewer than a quarter – 23 per cent – are willing to fly less.”
Share this...FacebookTwitter "
"

From the blog of Roger Pielke Sr. http://climatesci.org/
Erroneous News Article In The Times
Filed under: Climate Science Reporting — Roger Pielke Sr. @ 7:00 am

Thanks to Andrew  Forster of Local Transport Today in the UK for alerting us to the erroneous news article from the Times on  December 27 2008 titled
The  war on carbon – Arguments of 2009: Can Copenhagen save the planet?
An excerpt reads,
“The stakes at Copenhagen could not be much higher. Global surface  temperatures have risen by a tolerable three quarters of a degree celsius over  the past century, but the rate of increase is accelerating. The Kyoto Protocol  has had negligible impact on greenhouse gas emissions, and projections for the  mean global temperature rise in the next century range from 1.1 to 6.4 degrees.  Whether fast or very fast, the Earth is heating up.
There will be continued argument about the science of climate change over  the next 12 months, but not, except on the conspiratorial fringe, about the  threat. Climate change is real and worsening, and there is an overwhelming  likelihood that much of it is man-made.”
This is a erroneous report on the climate system! The rate of increase is  NOT accelerating. There is absolutely no question that global warming has  stopped for at least 4 years (using upper ocean data) ; e.g see
Pielke Sr., R.A., 2008: A broader view of the role of humans in the climate  system. Physics Today, 61, Vol. 11, 54-55.
http://www.climatesci.org/publications/pdf/R-334.pdf
and over 7 years using lower tropospheric data; e.g. see
Figure 7 TLT in http://www.ssmi.com/msu/msu_data_description.html.
With respect to the surface temperature trends [which have a warm bias in any  case, as we have documented in our peer review papers; e.g. see], a good set  of analyses on this subject has been posted over the last few years at http://rankexploits.com/musings/ [you should scroll back over the last several months to view; it is an excellent  comparison with model predictions]. As discussed on that website, even with the  warm biased global average surface temperature trends, the models have  over-predicted warming. The GISS data itself even shows recent cooling in the  ocean sea surface temperatures [see their figure for Monthly-Mean Global Sea  Surface Temperature; http://data.giss.nasa.gov/gistemp/2008/ where it has cooled since 2002.
The writers of the Time article, and other journalists who write  similar misinformation, damage the liklihood of responsible environmental  actions as a result of their overstatement and erroneous communication to the  public and policymakers of climate science.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99f86964',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"London mayor Sadiq Khan is to spend £50m on a Green New Deal for London as part of a pledge to make the UK capital carbon neutral by 2030. The mayor, who faces the electorate in May, said the announcement highlights his commitment to tackling the climate crisis and improving London’s dangerous levels of air pollution. “I’m unapologetic at how ambitious my plans are for a Green New Deal for London because we can’t afford not to be ambitious when it comes to saving our planet,” Khan told the Guardian. In May’s election he faces a challenge from Tory candidate Shaun Bailey, former Conservative cabinet member Rory Stewart – who is standing as an independent – as well as candidates from the Green party, the Liberal Democrats and the Women’s Equality party. But announcing the new fund, which officials said comes from higher council tax and business rates, Khan insisted May’s election was a “two horse race” between him and Bailey – and made a direct pitch for green votes. “The Tory candidate and I are the only candidates who can win this election,” he said. “Which is why I’m making a direct appeal today to Londoners who have previously supported the Green party to lend me their vote on 7 May so that I can stand up for our shared values and take action on climate change.” The mayor has introduced an ultra-low emissions zone (Ulez) in central London which is due to expand to the north and south circulars in 2021. However, other cities – including Manchester and Birmingham – have announced ambitious plans to improve air quality and encourage cycling, and Khan has been criticised for not moving quickly enough. But the mayor insisted he was determined to make London a leading green city. “These issues are personal to me. I don’t want my children to grow up in a world where our very way of life is threatened by the climate crisis. And as someone who suffers from adult onset asthma, I understand the price we pay for failing to clean up our toxic air.” Green groups welcomed the new fund but said Khan must go further if he is re-elected in May. Jenny Bates, campaigner at Friends of the Earth, said the climate and nature crisis was the biggest threat the world faced, adding air pollution in London was “a health scandal”. “While the mayor has made good strides on these issues, much more needs to be done to cut emissions from all sectors. This means expanding the Ulez to the whole of London, not just to the north and south circular roads. We also need more infrastructure to make cycling and walking safer and easier for everyone, along with clear action to cut traffic levels which a London-wide pay-as-you-go driving system would help deliver.” Bates also criticised the mayor for approving the four-lane Silvertown road tunnel in east London which she said would make “already illegally bad air breathed in some areas even worse”. “There is no place for large scale road-building in London given the climate and air pollution imperatives, nor for any airport expansion at City airport or Heathrow.” Doug Parr, policy director at Greenpeace, said it was right that all candidates for London mayor prioritised the climate emergency. “We will see what all parties have to offer but this is a helpful pledge to increase the priority that climate protection receives in a city of global significance such as London,” he added. City Hall officials said the new fund had not been allocated to specific programmes yet but proposals being considered included making homes energy efficient, creating new green spaces and speeding up the installation of solar panels in the capital."
"
Share this...FacebookTwitterNot only Europe is getting walloped by winter, so are some regions in China, where the most snow has fallen in 30 years.
The early snowstorms have isolated herdsman living in remote areas, see map, and Chinese relief services dispatched assistance.
According to China.org.cn:
Snow storms has hit four towns in Xing’an Prefecture, a pasture region about 1,500 km northeast of the regional capital, Hohhot, since last Saturday.
Snow has accumulated up to 30 cm deep in most parts of the region and a meter in some areas.
The snow was 40 days earlier than its usual arrival time and was the heaviest in 30 years. At least 700 heads of livestock are believed to have died in the storm.
Winter arrives early in Northeast China
And more here: Winter in China
UPDATE: Record cold in the UK. h/t: M White
Share this...FacebookTwitter "
"
Share this...FacebookTwitterThe online leftist Die Tageszeitung (TAZ) from Berlin has an essay by Stefan Rahmstorf, who attacks Dr Fritz Vahrenholt, head of RWE Innogy, for his comments in a December essay that casts doubt on man-made climate change appearing in the somwhat more conservative Die Welt from Hamburg.Interestingly, instead of appearing in the Science section, Rahmstorf’s rant appears in the Debate section. Is that a sign?
First Rahmstorf, praises Germany for carrying out a “scientific, factual discussion of climate science”, unlike in the USA, where he says:
It’s different in the USA: There the conservative Tea Party movement has proclaimed that man-made climate change is made up, and large parts of the economy are lobbying using dubious ‘climate sceptic’ propositions.”
Rahmstorf is visibly worried that this “dubious climate scepticism” may be spreading to Germany, and goes after RWE Vahrenholt. In his Die Welt essay, Vahrenholt assigns the blame for the cold winters  – writing:
It’s the sun, stupid!”
Rahmstorf, however does not believe the sun has an impact on the earth’s climate, and thinks it’s all due to a few molecules of CO2. And so Rahmstorf attributes the recent cold winters to miscalibrated human perception:
The winter appears cold because we had gotten used to the mild winters.”
and misleadingly reframes Mojib Latif’s 2008 predictions of cooling:
No serious scientist doubts global warming, and certainly not Mojib Latif, whose quote has nothing to do with the cold winters. It’s old and stems from his model projections of a temporary cooling, which in the meantime we know failed to materialise.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Latif made his cooling projections in 2008, and so are not that old (he predicted warm-winters back in 2000). And who can say that the cold winters aren’t related to his projections of cooling? Latif’s projections look to be true, and likely will be true for the new, current decade.
Rahmstorf then quotes, defends and explains Kevin Trenberth’s infamous “it’s travesty we can’t account for the missing heat” statement, saying it was taken out of context and that Trenberth meant something else.
Rahmstorf acknowledges the solar correlation with regards to the Russian heat wave and flood in Pakistan, but claims that this correlation is very weak, and quotes Trenberth again:
‘Without global warming, these events would not have happened’.”
Now that’s the “scientific, factual discussion” that PIK scientists and the government like to praise. Of course, anyone with even a rudimentary knowledge of science knows that blaming a couple of isolated weather events on global warming is preposterous. But what is preposterous in climate science passes as “scientific, factual” at the PIK.
Rahmstorf takes offence to the harsh criticism that Vahrenholt fired at PIK science, and thinks sceptics give the sun too great of a role in climate change. He says Vahrenholt is silent about the fact…(emphasis added)
…that also during the largest solar minimum of the “Little Ice Age” during the so-called Maunder Minimum of the late 17th century the global temperature was only a few tenths of a degree cooler than before and after, and that our model reproduces the temperature back then very well, otherwise we would have not used it for our future projections.”
A few tenths of a degree Celsius? That’s the difference between having vineyards in England and a frozen Thames? PIK science is moving beyond preposterous.
Rahmstorf ends with a comment on climate debate:
Those who wish to sow doubt on the urgency of climate protection, really have to work hard to twist the facts. However, the climate crisis can be overcome only by having an honest debate.”
Share this...FacebookTwitter "
"

Guest Post by Steven Goddard



Recreational cycling during the summer of 2007

The UK Climate Impacts Programme (UKCIP) is a government funded organization with the following scientifically neutral mission statement on their home page “The UK Climate Impacts Programme (UKCIP) helps organisations to adapt to inevitable climate change. While it’s essential to reduce future greenhouse gas emissions, the effects of past emissions will continue to be felt for decades.“
On their headline messages page they have a list of global warming predictions and supporting evidence.  In this article we will examine some of their claims and evidence.
Claim: Summers will continue to get hotter and drier…

 Evidence: Total summer precipitation has decreased in most parts of the UK, typically by between 10 and 40% since 1961.

According to the UK Met Office, the summer of 2007 was the wettest summer on record.  Summer, 2008 was the wettest on record in Northern Ireland, and broke many local rainfall records in England.  The last hot day in London (30C or 86F) was on July 27, 2006.  London is normally one of the UK’s warmest locations in summer, and it has been 915 days since London has seen any “hot” weather.

Claim: Winters will continue to get milder and wetter…

 Evidence: Average winter temperature for all regions of the UK has risen by up to 0.7 °C since 1914..

The Met office reported last month: “Temperatures from the Met Office have revealed that the UK has had the coldest start to winter in over 30 years.” 
This month, the Met Office reported: “The British Isles has experienced almost a fortnight of freezing conditions. Temperatures as low as -9 °C have been fairly common throughout southern areas of the UK, with temperatures struggling to rise above freezing in some places.“
This winter has not only been unusually cold, but it has also been unusually dry in the UK.

Recreational boating during the winter of 2008-2009
Claim: Some weather extremes will become more common, others less common…

Evidence: The average duration of summer heatwaves has increased in all regions of the UK by between 4 and 16 days since 1961.
Evidence: The average duration of winter cold snaps has decreased in all regions of the UK by between 6 and 12 days since 1961.
Evidence: There has been a trend towards heavier winter precipitation for most parts of the UK since 1961.







As mentioned above, there have been no hot days in the UK for nearly three years.  The current winter has been one of the coldest and driest in recent memory.




Claim: Sea level will continue to rise…

Evidence: Global average sea level rose by between 10 and 20 cm during the twentieth century.
Evidence: The temperature of UK coastal waters has increased by between 0.2 and 0.6 °C per decade since 1985.


It is somewhat surprising that a scientific organisation would use this information in support of global warming.  Sea level has been rising nearly continuously since the end of the last ice age, 15,000 years ago.  The average sea rise rate has been about 80cm/century, 4X-8X higher than UKCIP’s reported current levels.

From: http://www.globalwarmingart.com/wiki/Image:Post-Glacial_Sea_Level_png
Additionally, there has been little change in sea level rise rates over the last 100 years.

From: http://www.globalwarmingart.com/images/thumb/0/0f/Recent_Sea_Level_Rise.png/700px-Recent_Sea_Level_Rise.png
Regarding their discussion of UK sea temperatures since 1985, there hasn’t been much glacial activity in the UK over the last 25 years and it is unlikely that UK ice sheet melt is adding much to sea level.  Their reported UK SST changes are more likely due to ocean circulation patterns like the AMO.  Current SST anomaly maps show ocean temperatures around the UK near or below normal.  And according to the University Of Colorado, global sea level has scarcely risen since 2005.

From: http://sealevel.colorado.edu/current/sl_noib_global_sm.jpg
One might think that taxpayer funded organisations like UKCIP would be required to keep their public statements a bit more up to date and accurate.





			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99384947',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter**  BREAKING NEWS!   **  BREAKING NEWS! **  BREAKING NEWS! **
Stunning!
Another huge slab of the Climate-Berlin-Wall has fallen. It’s a climate skeptic jail-break! I imagine the Climate-Politburo members must be quivering and trembling in their bunkers in Potsdam by now.
Big hat-tip to NTZ reader Ike.
A leading German news magazine has decided to depart from the dogma of angst and catastrophe and bring up climate science issues that, up to now, have been strictly taboo here in the Vaterland. Tomorrow FOCUS magazine will come out with its newest issue titled:
Prima Klima! Umdenken:Wieso die globale Erwärmung gut für uns ist. (Best Climate! Change of Thinking: Why global warming is good for us.)
Change of Thinking – yes! And the timing couldn’t be better.
Folks, this is the first time in a long time that a major German news magazine has decided to do a little investigative reporting, instead of relying on the press releases from the Palaces Of Panic like the Potsdam Institute of Climate Impact Research, NOAA, Alfred Wegener Institute, etc., and seriously look into this controversial global issue. Game over comrades!
When the global warming hoax collapses in Germany, then Europe follows right behind it – and then, of course, the rest of the world. Germany is that one domino. This represents a major setback for the warministas. Indeed it would be interesting to know what went on in the FOCUS editorial offices.
Perhaps the normally über-alarmist FOCUS has already gotten tired of the winter and longs for the warmer days. I can’t explain why they are coming out with such an issue – especially during Cancun. Whoa! That’s all I can say.
Here’s what tomorrow’s issue will feature:
78   Warm times are good times. Harvest yields increase, Forests grow, deserts shrink
86   Which impacts of climate change are proven?  Which are not?
90   The “Who is who“ in climate science


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Promo video (in German, see English text below):
http://www.focus.de/magazin/videos/focus-titel-prima-klima_vid_21483.html
The video begins with:
This week in the coming FOCUS: Best Climate. Change in thinking – global warming is good for us. FOCUS editor Dr. Christian Pandler (sp?) researched the current topic and reports on it in the new issue.
Editor Dr. Pandler (a bit paraphrased):
In the new FOCUS issue, we take a look at the question of climate change. This week the world climate conference is taking place in Cancun, where world leaders are going to debate over how to combat warming. Our question: Is global warming actually bad? Does it entail only disadvantages and only catastrophic consequences? Up to now, people have only focused on what will be bad. The question is what could be good? No one has really looked at this. It’s taboo in Germany.
We know from history that warm periods were good periods for us. Cold periods were bad periods. We know that 20,000 years ago Europe was a frozen wasteland where nobody lived. That was a real climate catastrophe. For example we had a warming 10,000 years ago, which led to a greening of the Sahara. Then there was cooling which led it to be parched again. Now it’s warming, and there are lots of signs that show it is greening up again. For the people in Africa, it is absolutely a positive development. If it continues that way, it could once again become green with a variety of wildlife, rivers and lakes and so on. This is a consequence that hardly has been discussed.
We’ve spoken to scientists who are there on site. One researcher in particular has gone there every year for 30 years and photographed how the Sahara is gradually getting greener.”
Go out and reward FOCUS by buying this issue – get an additional copy for a friend too. Thank the editor for having the guts to do this.
In the meantime, my advice to that brave editor at FOCUS: Put on your bullet-proof political vest and find the deepest possible bunker. The greenshirts are sending over the B-52s! Achtung!
This is going to be something to relish.
Ironically this comes out precisely when the science is showing signs that cooling is coming instead, and so FOCUS may be only getting false hopes up. Lol! You just can’t make this stuff up. It makes my day.
Share this...FacebookTwitter "
"

“Shut down the World Bank!” reads one of the many placards being carried through Washington D.C. As the international institution holds its annual spring meeting, thousands of activists are venting their anger about everything from world poverty to environmental degradation. While they are correct to point an accusatory finger at the World Bank for making those problems worse, their well‐​publicized efforts will do little good. The bank has become adept at co‐​opting key elements of its opposition in order to keep billions in U.S. government money flowing in its direction. 



Since first being stung by environmentalist criticism in the early 1980s, the World Bank has repeatedly promised to reform itself. Despite those promises, the bank has neither fundamentally reformed its lending practices nor radically changed the kinds of projects that receive its funding. Bank projects around the world remain environmentally destructive and socially disruptive. By its own account, 2.6 million victims of World Bank lending in poor countries are having their property confiscated, their homes destroyed, and their livelihoods ruined. 



When the bank hears public criticism of its destructive environmental record, it typically issues a press release announcing renewed commitment to old promises made in prior years’ press releases. After two decades of this behavior, there is little evidence of tangible improvement. In a recent review, the bank judged that 25 percent of its projects in 1997–98 had an unsatisfactory outcome, even by its own rather generous standards. It also concedes that only 54 percent of its projects completed could be judged “sustainable,” even though sustainable development is now supposed to be a main purpose of the bank. 



Environmental groups also have a difficult time identifying reform successes. A 1999 environmentalist report labeled the World Bank’s reform program a “failure,” noting that it had not produced more environmentally sound projects or a greater level of bank accountability to the public. According to the report, endorsed by the Environmental Defense Fund, Friends of the Earth, Greenpeace and the Sierra Club, “an evaluation of the World Bank Group’s portfolio shows that it does not promote environmental protection in its operations and loans.” 



The bank’s most noticeable improvement in recent years has been in its ability to weather criticism, exposure and political opposition. It has actively courted the support of environmental advocacy groups and has rhetorically embraced their “sustainable development” creed. The bank cooked its books to make it appear that environmental spending had gone up and provided generous grant funding to nongovernmental organizations (NGOs) in order to make them more dependent on its continued existence. It has invited green organizations to its lavish headquarters to solicit their advice and to enter into partnerships that improve the bank’s image. Despite its documentation of egregious abuses at the World Bank, the leadership of the anti‐​bank movement does not advocate — as it sometimes did — the only real solution to an institution that has demonstrated zero capacity for genuine reform: elimination. 



Why has the bank’s opposition gone soft? The World Bank, recognizing that environmentalist opposition in the early 1990s threatened its very existence, made a concerted effort to meet this challenge. An internal World Bank report identifies the key to its survival continuing “to build a public constituency for the Bank’s policies and programs, and for development assistance in general.” Central to its strategy was a systematic effort to convert the environmental movement to an accommodationist stance toward the bank. The greens must advocate incremental changes to the institution, not its elimination. 



Beginning in the early 1990s, the bank contacted its most influential environmental critics and invited them to “participate” in the bank’s work. According to the bank, this activity has caused “a reduction in NGO criticism” and a recognition on the part of NGOs that they and the bank share common interests. The bank claims that a chief benefit of appeasing the environmental movement is to “improve the overall climate of opinion around the Bank’s work.” Most important, environmental groups tempered their criticism of the bank during key moments when its budget was being debated in Congress. 



The environmental groups are now committed to working with and improving the World Bank. Implicitly, they believe that World Bank planners, who once ignored environmental considerations with tragic consequences, can now be trusted to implement major development decisions in the Third World. They suggest that the bank can become an instrument of “sustainable development.” 



The World Bank’s latest reform effort is eerily reminiscent of earlier efforts that environmental groups denounced as inadequate and diversionary. But this strategy has been effective enough to secure funding prospects on Capitol Hill. If the anti‐​bank protestors really want to “Break the Bank,” as their signs say, they will work harder to ensure that their own organizations do not compromise on this objective.
"
"
 
Guest post by Frank Lansner, civil engineer, biotechnology.
(Note from Anthony – English is not Frank’s primary language, I have made some small adjustments for readability, however they may be a few  passages that need clarification. Frank will be happy to clarify in comments)
It is generally accepted that CO2 is lagging temperature in Antarctic graphs. To dig further into this subject therefore might seem a waste of time. But the reality is, that these graphs are still widely used as an argument for the global warming hypothesis. But can the CO2-hypothesis be supported in any way using the data of Antarctic ice cores?
At first glance, the CO2 lagging temperature would mean that it’s the temperature that controls CO2 and not vice versa.

Click for larger image Fig 1. Source: http://www.brighton73.freeserve.co.uk/gw/paleo/400000yrfig.htm
But this is the climate debate, so massive rescue missions have been launched to save the CO2-hypothesis. So explanation for the unfortunate CO2 data is as follows:
First a solar or orbital change induces some minor warming/cooling and then CO2 raises/drops. After this, it’s the CO2 that drives the temperature up/down. Hansen has argued that: The big differences in temperature between ice ages and warm periods is not possible to explain without a CO2 driver.
Very unlike solar theory and all other theories, when it comes to CO2-theory one has to PROVE that it is wrong. So let’s do some digging. The 4-5 major temperature peaks seen on Fig 1. have common properties: First a big rapid temperature increase, and then an almost just as big, but a less rapid temperature fall. To avoid too much noise in data, I summed up all these major temperature peaks into one graph:

Fig 2. This graph of actual data from all major temperature peaks of the Antarctic vostokdata confirms the pattern we saw in fig 1, and now we have a very clear signal as random noise is reduced.
The well known Temperature-CO2 relation with temperature as a driver of CO2 is easily shown:

Fig 3.
Below is a graph where I aim to illustrate CO2 as the driver of temperature:

Fig 4. Except for the well known fact that temperature changes precede CO2 changes, the supposed CO2-driven raise of temperatures works ok before temperature reaches max peak. No, the real problems for the CO2-rescue hypothesis appears when temperature drops again. During almost the entire temperature fall, CO2 only drops slightly. In fact, CO2 stays in the area of maximum CO2 warming effect. So we have temperatures falling all the way down even though CO2 concentrations in these concentrations where supposed to be a very strong upwards driver of temperature.
I write “the area of maximum CO2 warming effect “…
The whole point with CO2 as the important main temperature driver was, that already at small levels of CO2 rise, this should efficiently force temperatures up, see for example around -6 thousand years before present. Already at 215-230 ppm, the CO2 should cause the warming. If no such CO2 effect already at 215-230 ppm, the CO2 cannot be considered the cause of these temperature rises.
So when CO2 concentration is in the area of 250-280 ppm, this should certainly be considered “the area of maximum CO2 warming effect”.
The problems can also be illustrated by comparing situations of equal CO2 concentrations:

Fig 5.
So, for the exact same levels of CO2, it seems we have very different level and trend of temperatures:

Fig 6.
How come a CO2 level of 253 ppm in the B-situation does not lead to rise in temperatures? Even from very low levels? When 253 ppm in the A situation manages to raise temperatures very fast even from a much higher level?
One thing is for sure:
“Other factors than CO2 easily overrules any forcing from CO2. Only this way can the B-situations with high CO2 lead to falling temperatures.”
 
This is essential, because, the whole idea of placing CO2 in a central role for driving temperatures was: “We cannot explain the big changes in temperature with anything else than CO2”.
 
But simple fact is: “No matter what rules temperature, CO2 is easily overruled by other effects, and this CO2-argument falls”. So we are left with graphs showing that CO2 follows temperatures, and no arguments that CO2 even so could be the main driver of temperatures.
– Another thing: When examining the graph fig 1, I have not found a single situation where a significant raise of CO2 is accompanied by significant temperature rise- WHEN NOT PRECEDED BY TEMPERATURE RISE. If the CO2 had any effect, I should certainly also work without a preceding temperature rise?!  (To check out the graph on fig 1. it is very helpful to magnify)
Does this prove that CO2 does not have any temperature effect at all?
No. For some reason the temperature falls are not as fast as the temperature rises. So although CO2 certainly does not dominate temperature trends then: Could it be that the higher CO2 concentrations actually is lowering the pace of the temperature falls?
This is of course rather hypothetical as many factors have not been considered.

Fig 7.
Well, if CO2 should be reason to such “temperature-fall-slowing-effect”, how big could this effect be? The temperatures falls 1 K / 1000 years slower than they rise. 
However, this CO2 explanation of slow falling temperature seems is not supported by the differences in cooling periods, see fig 8.
When CO2 does not cause these big temperature changes, then what is then the reason for the  big temperature changes seen in Vostok data? Or: “What is the mechanism behind ice ages???”
 
This is a question many alarmists asks, and if you can’t answer, then CO2 is the main temperature driver. End of discussion. There are obviously many factors not yet known, so I will just illustrate one hypothetical solution to the mechanism of ice ages among many:
 
First of all: When a few decades of low sunspot number is accompanied by Dalton minimum and 50 years of missing sunspots is accompanied by the Maunder minimum, what can for example thousands of years of missing sunspots accomplish? We don’t know.
 
What we saw in the Maunder minimum is NOT all that missing solar activity can achieve, even though some might think so. In a few decades of solar cooling, only the upper layers of the oceans will be affected. But if the cooling goes on for thousands of years, then the whole oceans will become colder and colder. It takes around 1000-1500 years to “mix” and cool the oceans. So for each 1000-1500 years the cooling will take place from a generally colder ocean. Therefore, what we saw in a few decades of maunder minimum is in no way representing the possible extend of ten thousands of years of solar low activity.
It seems that a longer warming period of the earth would result in a slower cooling period afterward due to accumulated heat in ocean and more:

Fig 8.
Again, this fits very well with Vostok data: Longer periods of warmth seems to be accompanied by longer time needed for cooling of earth. The differences in cooling periods does not support that it is CO2 that slows cooling phases. The dive after 230.000 ybp peak shows, that cooling CAN be rapid, and the overall picture is that the cooling rates are governed by the accumulated heat in oceans and more.
Note: In this writing I have used Vostok data as valid data. I believe that Vostok data can be used for qualitative studies of CO2 rising and falling. However, the levels and variability of CO2 in the Vostok data I find to be faulty as explained here:
http://wattsupwiththat.com/2008/12/17/the-co2-temperature-link/

Sponsored IT training links:
Pass PMI-001 exam fast using self study 70-290 guide and 350-029 tutorial.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e98b94e93',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The Victorian Liberal Katie Allen has declared the world is approaching an “iPhone moment” when it comes to new technology lowering greenhouse gas emissions, and Australia needs to be part of the revolution, rather than being a technology “taker”. Allen has joined fellow Liberal Trent Zimmerman in noting the Coalition’s decision to sign the Paris agreement means Australia has already committed to achieving carbon neutrality by mid-century. But she said the government should not nominate a specific date to hit the milestone until it had developed a policy roadmap.  She told Guardian Australia the smart phone had provided the requisite technological step change in communications, but there was unlikely to be a single technology in energy to drive the transformation to low emissions. Allen said there was “a bit of an arms race” in low emissions technology, and she nominated batteries, green hydrogen and “new nuclear” – modular reactors – as, potentially, part of the transformation. “I believe we need to be open-minded because it is hard for governments to pick commercialised and scalable winners,” she said. Asked whether she supported adopting a target of net zero by 2050, Allen said: “I want to see the roadmap, and how we would get there.” She noted the states had already made commitments to carbon neutrality, and that would be an important part of the picture, and said she was “very interested in an economically sound approach”. Allen’s comments join Zimmerman’s call for the government to look “very seriously” at adopting a net zero target. On Sunday night, fellow moderate Jason Falinski also told Sky News the government needed to set a roadmap for the transformation, and he would like Australia to hit net zero before 2050 “if we can”. On Tuesday The Australian reported that Scott Morrison may adopt a technology investment target as part of an Australian government push to avoid setting a more explicit net zero emissions target at the next UN climate summit in Glasgow in November. The Business Council of Australia estimates at least $22bn of investment in new technology every year and a doubling of current renewable energy generation capacity within the next 20 years will be needed to meet a net zero emissions target by 2050. Moderate Liberals have returned to the new political year attempting to build traction internally for the government to increase ambition in climate action. But the push has triggered resistance from Nationals. At the weekend, Matt Canavan, who quit his cabinet position to support Barnaby Joyce’s failed bid to return to the Nationals leadership, declared a net zero target “fantastical”. “I haven’t looked at the modelling or costs and benefits of net zero emissions closely because it just seems so fantastical to me,” Canavan told Sky News. “It seems like the kind of things that governments say, because they’re not doing much today but they’d like to try and hoodwink people that they might do something in 30 years’ time.” The Nationals leader, Michael McCormack – who remains under pressure in his role – told the ABC on Sunday net zero was a bad idea. “I think if you go down that path, what you’re going to do is send factories and industries offshore, send manufacturing jobs offshore,” McCormack said. “That’s not the Australian way. Regional Australia is more than doing its fair share, its fair share as far as making sure that we have lower emissions.” Asked if he accepted warnings from the Intergovernmental Panel on Climate Change that the emissions target was needed to limit global warming, McCormack said the “IPCC is not governing Australia”. “The Liberals and Nationals are – we took our climate policies, we took our emissions reduction policies to the election last May and we were re-elected. “The Australian people have spoken – we’re not run by international organisations, we’re run by Australians, we’re run by Scott Morrison and we’re run by myself. And we’re run by the Liberals and Nationals.” On Monday Morrison stepped around the divisions within his own ranks by repeating the same formulation he has given on net zero since Liberals and Nationals began debating the proposition in public. The prime minister told reporters: “I don’t sign up to anything when I can’t look Australians in the eye and tell them what it costs. “How many jobs it’s going to cost them. What it means for their industries. What it means for rural and regional parts of the country. Whether it means they’d have to pay higher taxes. “And none of that information is before me that would enable me to give any such commitment – and I haven’t.”"
"Watching starling murmurations as the birds swoop, dive and wheel through the sky is one of the great pleasures of a dusky winter’s evening. From Naples to Newcastle these flocks of agile birds are all doing the same incredible acrobatic display, moving in perfect synchrony. But how do they do it? Why don’t they crash? And what is the point?  Back in the 1930s one leading scientist suggested that birds must have psychic powers to operate together in a flock. Fortunately, modern science is starting to find some better answers. To understand what the starlings are doing, we begin back in 1987 when the pioneering computer scientist Craig Reynolds created a simulation of a flock of birds. These “boids”, as Reynolds called his computer-generated creatures, followed only three simple rules to create their different patterns of movement: nearby birds would move further apart, birds would align their direction and speed, and more distant birds would move closer.  Some of these patterns were then used to create realistic looking animal groups in films, starting with Batman Returns in 1992 and its swarms of bats and “army” of penguins. Crucially this model did not require any long-range guidance, or supernatural powers – only local interactions. Reynolds’s model proved a complex flock was indeed possible through individuals following basic rules, and the resulting groups certainly “looked” like those in nature. Scientifically-accurate bat behaviour in Batman Returns. From this starting point an entire field of animal movement modelling emerged. Matching these models to reality was spectacularly achieved in 2008 by a group in Italy who were able to film starling murmurations around the rail station in Rome, reconstruct their positions in 3D, and show the rules that were being used. What they found was that starlings sought to match the direction and speed of the nearest seven or so neighbours, rather than responding to the movements of all of the nearby birds around them. Simple rules, complex patterns When we watch a murmuration pulsate in waves and swirl into arrays of shapes it often appears as if there are areas where birds slow, and become thickly packed in, or where they speed up and spread wider apart. In fact this is largely thanks to an optical illusion created by the 3D flock being projected onto our 2D view of the world, and scientific models suggest that the birds fly at a steady speed. Thanks to the efforts of computer scientists, theoretical physicists and behavioural biologists we now know how these murmurations are generated. The next question is why do they happen at all – what caused starlings to evolve this behaviour?  One simple explanation is the need for warmth at night during the winter: the birds need to gather together at warmer sites and roost in close proximity just to stay alive. Starlings can pack themselves into a roosting site – reed beds, dense hedges, human structures like scaffolds – at more than 500 birds per cubic metre, sometimes in flocks of several million birds. Such high concentrations of birds would be a tempting target for predators. No bird wants to be the one that a predator picks off, so safety in numbers is the name of the game, and swirling masses create a confusion effect preventing a single individual being targeted. However, starlings often commute to roosts from many tens of kilometres away, and they burn up more energy on these flights than could be saved by roosting in marginally warmer places. Therefore the motivation for these colossal roosts must be more than temperature alone.  Safety in numbers could drive the pattern, but an intriguing idea suggests that flocks may form so that individuals can share information about foraging. This, the “information centre hypothesis”, suggests that when food is patchy and hard to find the best long-term solution requires mutual sharing of information among large numbers of individuals. Just as honeybees share the location of flower patches, birds that find food one day and share information overnight will benefit from similar information another day. Although larger numbers of birds join roosts when food is at its scarcest, which seems to provide some limited support for the idea, it has thus far proven extremely difficult to properly test the overall hypothesis. Our understanding of moving animal groups has expanded enormously over the past few decades. The next challenge is to understand the evolutionary and adaptive pressures that have created this behaviour, and what it might mean for conservation as those pressures change. Possibly we can adapt our understanding and use it to improve the autonomous control of robotic systems. Perhaps the rush-hour behaviour of the automated cars of the future will be based on starlings, and their murmurations."
"Antibiotic resistance in bacteria is spreading rapidly worldwide and has even been called a global threat to humans as serious as climate change. Excessive and imprudent use of antibiotics is usually blamed, but the rate and extent of the spread can’t be explained by overuse alone. Antibiotic resistance has now been found in remote parts of the world where humans and antibiotics are scarce or absent. Recently we published a study in Environment International that reported antibiotic resistant genes in Svalbard in the High Arctic. Antibiotic resistance itself is natural, but continued human exposure to antibiotics has selected for progressively stronger bacteria. These bacteria often acquire antibiotic resistance genes that allow them to make powerful defence proteins. As more antibiotics are used new forms of resistance evolve, including multiple drug resistance in pathogens, creating a health crisis.       For our research, we extracted bacterial DNA from 40 soil cores at eight locations along Kongsfjorden on the west coast of Spitsbergen on the Arctic Ocean, 300km from the North Pole. This location is fairly unusual in the Arctic for its vibrant wildlife populations and neighbouring fjord which does not freeze. We screened the Arctic soil DNA and quantified 131 antibiotic resistance genes associated with nine major antibiotic classes. These include aminoglycosides, carbapenem, macrolides and β-lactams – many of which are used to treat human and veterinary infections.  Some of the detected genes, some of which are almost certainly not “local” to the Arctic, can confer resistance to multiple drugs. These “foreign” genes were found in highest concentrations near fresh water sources – areas where wildlife tend to congregate. One antibiotic resistance gene we found is called “blaNDM-1”. This gene confers resistance in bacteria to carbapenem antibiotics, one of our last resort remedies to infectious disease. We found this gene in soils near a small Arctic lake in 2013, but it was first detected in a hospital patient in India in 2007, and was later found in surface waters in urban India in 2010. Although not explicitly from India, BlaNDM-1 was first identified on one side of the world and migrated to the other in less than three years, ending up in a place where few humans reside and where antibiotics are scarce. How did it get there? Places with inadequate sanitation often have higher levels of resistance genes and bacteria in the surrounding water and soil. Local overpopulation, poorly controlled antibiotic use and incomplete wastewater treatment means resistance is more readily selected in these places. Local water and food supplies become contaminated with faecal matter, providing a pathway for resistance genes and their bacterial hosts to be ingested by humans and wildlife – hitching a ride from one region of the world to another. It’s unclear whether it’s a single migration via a bird or a human, or a chain reaction of exposures. By whatever pathway, resistance genes are moving fast and to places where antibiotics are not present. As antibiotic resistance migrates, it also changes as it passes through animals and the wider environment. This complicates tracing these genes from their origins to where they end up. In the case of blaNDM-1, it was initially detected as a single gene in a few places, but there are now numerous variants which have been found in tens of countries, now including the Arctic. If resistance genes migrate via humans or wildlife to other locations, especially places with inadequate local sanitation, such genes and bacterial hosts might be selected in subsequent human and wildlife populations. This is what we think is happening worldwide. These genes move around the world with people and other animals, seeding new places with resistance potential. Reducing antibiotic use is critical to tackling bacterial resistance. In the UK, this is outlined in the five-year action plan announced by the health secretary, Matt Hancock. However, reducing local antibiotic use may have a limited effect on global resistance unless environmental pathways are not given greater consideration. Improving sanitation and water quality worldwide must be part of the fight against antibiotic resistance. There is one positive note from our High Arctic study, however. Although we detected genes like blaNDM-1, we also found other interesting and unexpected antibiotic resistance genes. We found one gene that codes for multiple drug resistance in Tuberculosis bacteria in almost all of our soil cores. This gene was not explicitly related to faecal matter, which means it is almost certainly natural to this remote environment. 


      Read more:
      It’s the age of the antibiotic revolution, not apocalypse


 This might sound like bad news, but it suggests where this specific resistance gene might have originated. Understanding more about the origins of this gene might help us better understand resistance in tuberculosis. That said, focusing efforts on developing new arsenals of antibiotic drugs may not be enough. It additionally would be wise for wealthier countries to help poorer ones improve water quality and sanitation, even if it is only providing toilets to reduce open defecation. Smarter use of antibiotics in agriculture and medicine is a necessary step for tackling resistance, but only a comprehensive approach will reduce the evolution and spread of antibiotic resistance."
"Life in the sea isn’t easy. Talk to most people about the ocean and they are likely to imagine a tropical scene with a stretch of golden sand and warm, clear water. The reality is often quite different – the marine environment can be a surprisingly cold place. Water conducts heat far more effectively than air, which means that submerged animals quickly lose their body heat. It’s also harder to warm up again than on dry land, where animals often have the option of basking in the sun or on hot rocks. Finally, many aquatic animals use gills to get oxygen – great for breathing, but essentially another source of heat loss due to all the of water flowing across them and sucking away warmth.  All this contributes to making it much harder for aquatic animals to regulate their body temperature. So surely it would make sense to find more marine animals in warmer waters rather than in colder ones? Not necessarily. A new study published in Science by a team of US researchers led by John Grady reports higher levels of marine biodiversity in polar waters than tropical ones – but only for some types of animals. This goes against the longstanding idea that species richness is always highest in the tropics. The theory was that tropical waters provide a less thermally-extreme environment than polar ones – compare, for example, the Caribbean with Antarctica. Tropical areas often provide greater stability and productivity, giving benefits such as a predictable environment and plenty of food. As a result, scientists typically report high levels of biodiversity in warmer waters, with a multitude of species making the most of these favourable areas.  However, in this latest research, Grady and colleagues argue that rather than grouping all marine species together, we should instead consider them separately based on how they regulate their body temperature. There are two main types of “thermal strategy”. Some animals are not able to generate enough heat to warm their own body tissues, so their internal temperature is determined by conditions outside their bodies. These animals are known as “ectotherms”, and typically include reptiles, amphibians, fish (including most sharks), and invertebrates. In comparison “endotherms”, including humans, other mammals and birds, warm their body tissues by having high metabolisms to burn off food, which generates heat inside their bodies. These categories are similar to “cold blooded” and “warm blooded”, but more scientifically accurate. An ectothermic lizard might be considered cold blooded on a cool evening, but (since their body temperature matches the environment) it rapidly becomes warm on a hot, sunny day. To avoid this kind of confusion, scientists prefer to refer to the thermal strategy, rather than actual blood temperature. When marine species are separated into these categories of “ectotherm” and “endotherm”, Grady and colleagues show a pattern of marine biodiversity that both supports and contradicts the previous theory. The reptiles, fish and invertebrates rely on their environment for heat, so follow the established prediction of being generally found in warmer climates with particularly high levels of species diversity in the tropics.  But the heightened metabolism and body temperature of the endotherms opens up new possibilities. Unrestrained by the need for a warm environment, birds and mammals can successfully exploit colder habitats. They are also able to move more quickly in cold water than their ectothermic prey, which can become sluggish at cooler temperatures.  Consequently, endotherms seem to buck the expected trend and instead show higher species richness in polar waters. As the new study points out, seals and their relatives are “virtually absent from tropical waters” yet are common nearer the poles, while of all 89 species of cetacean (whales, dolphins and porpoises) only dolphins have “truly diversified in the warm tropics”. This seems to make sense. But, of course, there is more to it than thermal strategy. Apart from a considerable difference in temperature, the tropics and poles also differ in other ways. For example, differences in levels of dissolved oxygen, nutrients, algae growth, light, and so on. As a result, some areas and species are the “exception to the rule”. Regardless, categorisation by thermal strategy does show some interesting patterns of species diversity, and accounts for animals like whales and seals largely avoiding the warmest waters. But this advantage of environmental independence comes at a price. To maintain their heat-generating metabolism, endotherms have a continuous need for lots of high-quality food. For instance an ectothermic lemon shark has to eat about 2% of its body weight each day, whereas an endothermic common dolphin might have to eat 12% or more. To be fair, there are a lot of factors to consider beyond thermal strategy, but that’s still quite a difference.  In a changing climate, the energy requirements of endotherms could spell trouble. With the world warming fastest towards the poles, many scientists predict that polar species will face the biggest struggle as their habitats are dramatically altered, and they cannot retreat any further north or south to seek cooler areas. As the oceans warm, perhaps the high metabolisms and body temperatures of marine mammals will become a burden rather than a benefit."
"

Art courtesy Dave Stephens
Foreword by Anthony Watts: This article, written by the two Jeffs (Jeff C and Jeff Id) is one of the more technically complex essays ever presented on WUWT. It has been several days in the making. One of the goals I have with WUWT is to make sometimes difficult to understand science understandable to a wider audience. In this case the statistical analysis is rather difficult for the layman to comprehend, but I asked for (and got) an essay that was explained in terms I think many can grasp and understand. That being said, it is a long article, and you may have to read it more than once to fully grasp what has been presented here. Steve McIntyre of Climate Audit laid much of the ground work for this essay, and from his work as well as this essay, it is becoming clearer that Steig et al (see “Warming of the Antarctic ice-sheet surface since the 1957 International Geophysical Year”, Nature, Jan 22, 2009) isn’t holding up well to rigorous tests as demonstrated by McIntyre as well as in the essay below. Unfortunately, Steig’s office has so far deferred (several requests) to provide the complete data sets needed to replicate and test his paper, and has left on a trip to Antarctica and the remaining data is not “expected” to be available until his return.
To help layman readers understand the terminology used, here is a mini-glossary in advance:
RegEM – Regularized Expectation Maximization
PCA – Principal Components  Analysis
PC – Principal Components
AWS – Automatic Weather  Stations
One of the more difficult concepts is RegEM, an algorithm developed by Tapio Schneider in 2001.   It’s a form of expectation maximization (EM) which is a  common and well understood method for infilling missing data. As we’ve previously noted on WUWT, many of the weather stations used in the Steig et al study had issues with being buried by snow, causing significant data gaps in the Antarctic record and in some burial cases stations have been accidentally lost or confused with others at different lat/lons. Then of course there is the problem of coming up with trends for the entire Antarctic continent when most of the weather station data is from the periphery and the penisula, with very little data from the interior.
Expectation Maximization is a method which uses  a normal distribution to compute the best probability of fit to a missing piece  of data.  Regularization is required when so much data is missing that the EM  method won’t solve.  That makes it a statistically dangerous  technique to use and as Kevin Trenberth, climate analysis chief at the National Center for Atmospheric Research, said in an e-mail: “It is hard to make data where none exist.” (Source: MSNBC article) It is also valuable to note that one of the co-authors of Steig et al, Dr. Michael Mann, dabbles quite a bit in RegEm in this preparatory paper to Mann et al 2008 “Return of the Hockey Stick”.
For those that prefer to print and read, I’ve made a PDF file of this article available here.
Introduction
This article is an attempt to describe some of the early results from the Antarctic reconstruction recently published on the cover of Nature which demonstrated a warming trend in the Antarctic since 1956.   Actual surface temperatures in the Antarctic are hard to come by with only about 30 stations prior to 1980 recorded through tedious and difficult efforts by scientists in the region.  In the 80’s more stations were added including some automatic weather stations (AWS) which sit in remote areas and report the temperature information automatically.  Unfortunately due to the harsh conditions in the region many of these stations have gaps in their records or very short reporting times (a few years in some cases).  Very few stations are located in the interior of the Antarctic, leaving the trend for the central portion of the continent relatively unknown.  The location of the stations is shown on the map below.

In addition to the stations there are satellite data from an infrared surface temperature measurement which records the temperature of the actual emission from the surface of the ice/ground in the Antarctic.  This is different from the microwave absorption measurements as made from UAH/RSS data which measure temperatures in a thickness of the atmosphere.  This dataset didn’t start until 1982.
Steig 09 is an attempt to reconstruct the continent-wide temperatures using a combination of measurements from the surface stations shown above and the post-1982 satellite data.  The complex math behind the paper is an attempt to ‘paste’ the 30ish pre-1982 real surface station measurements onto 5509 individual gridcells from the satellite data.  An engineer or vision system designer could use several straightforward methods which would insure reasonable distribution of the trends across the grid based on a huge variety of area weighting algorithms, the accuracy of any of the methods would depend on the amount of data available.  These well understood methods were ignored in Steig09 in favor of RegEM.
The use of Principal Component Analysis in the reconstruction
Steig 09 presents the satellite reconstructions as the trend and also provides an AWS reconstruction as verification of the satellite data rather than a separate stand alone result presumably due to the sparseness of the actual data.  An algorithm called RegEM was used for infilling the missing data. Missing data includes pre 1982 for satellites and all years for the very sparse AWS data.  While Dr. Steig has provided the reconstructions to the public, he has declined to provide any of the satellite, station or AWS temperature measurements used as inputs to the RegEM algorithm.  Since the station and AWS measurements were available through other sources, this paper focuses on the AWS reconstruction.
Without getting into the detail of PCA analysis, the algorithm uses covariance to assign weighting of a pattern in the data and does not have any input whatsoever for actual station location.  In other words, the algorithm has no knowledge of the distance between stations and must infill missing data based solely on the correlation with other data sets.  This means there is a possibility that with improper or incomplete checks, a trend from the peninsula on the west coast could be applied all the way to the east.  The only control is the correlation of one temperature measurement to another.
If you were an engineer concerned with the quality of your result, you would recognize the possibility of accidental mismatch and do a reasonable amount of checking to insure that the stations were properly assigned after infilling.  Steig et. al. described no attempts to check this basic potential problem with RegEM analysis.  This paper will describe a simple method we used to determine that the AWS reconstruction is rife with spurious (i.e. appear real but really aren’t) correlations attributed to the methods used by Dr. Steig.  These spurious correlations can take a localized climactic pattern and “smear” it over a large region that lacks adequate data of its own.
Now is where it becomes a little tricky.  RegEM uses a reduced information dataset to infill the missing values.  The dataset is reduced by Principal Component Analysis (PCA) replacing each trend with a similar looking one which is used for covariance analysis.  Think of it like a data compression algorithm for a picture which uses less computer memory than the actual but results in a fuzzier image for higher compression levels.

While the second image is still visible, the actual data used to represent the image is reduced considerably.  This will work fine for pictures with reasonable compression, but the data from some pixels has blended into others.  Steig 09 uses 3 trends to represent all of the data in the Antarctic.  In it’s full complexity using 3 PC’s is analogous to representing not just a picture but actually a movie of the Antarctic with three color ‘trends’ where the color of each pixel changes according to different weights of the same red, green and blue color trends (PC’s).  With enough PC’s the movie could be replicated perfectly with no loss.  Here’s an important quote from the paper.
“We therefore used the RegEM algorithm with a cut-off parameter K=3. A disadvantage of excluding higher-order terms (k>3) is that this fails to fully capture the variance in the Antarctic Peninsula region.  We accept this tradeoff because the Peninsula is already the best-observed region of the Antarctic.”

Above: a graph from Steve McIntyre of ClimateAudit where he demonstrates how “K=3 was in fact a fortuitous choice, as this proved to yield the maximum AWS trend, something that will, I’m sure, astonish most CA readers.”
K=3 means only 3 trends were used, the ‘lack of captured variance’ is an acknowledgement and acceptance of the fuzziness of the image.  It’s easy to imagine that it would be difficult to represent a complex movie image of Antarctic with any sharpness from 1957 to 2006 temperature with the same 3 color trends reweighted for every pixel.  In the satellite version of the Antarctic movie the three trends look like this.

Note that the sudden step in the 3rd trend would cause a jump in the ‘temperature’ of the entire movie.  This represents the temperature change between the pre 1982 recreated data and the after 1982 real data in the satellite reconstruction.  This is a strong yet overlooked hint that something may not be right with the result.
In the case of the AWS reconstruction we have only 63 AWS stations to make the movie screen, by which the trends of 42 surface station points are used to infill the remaining data.  If the data from one surface station is copied to the wrong AWS stations the average will overweight and underweight some trends. So the question becomes, is the compression level too high?
The problems that arise when using too few principal components
Fortunately, we’re here to help in this matter.  Steve McIntyre again provided the answer with a simple plot of the actual surface station data correlation with distance.  This correlation plot compares the similarities ‘correlation’ of each temperature station with all of the 41 other manual surface stations against the distance between them.  A correlation of 1 means the data from one station is exactly equal to the other.  Because A -> B correlation isn’t a perfect match for B->A there are 42*42 separate points in the graph.  This first scatter plot is from measured temperature data prior to any infilling of missing measurements.  Station to station distance is shown on the X axis.  The correlation coefficient is shown on the Y axis.

Since this plot above represents the only real data we have existing back to 1957, it demonstrates the expected ‘natural’ spatial relationship from any properly controlled RegEM analysis.  The correlation drops with distance which we would expect because temps from stations thousands of miles away should be less related than those next to each other.  (Note that there are a few stations that show a positive correlation beyond 6000 km.  These are entirely from non-continental northern islands inexplicably used by Steig in the reconstruction.  No continental stations exhibit positive correlations at these distances.)  If RegEM works, the reconstructed RegEM imputed (infilled) data correlation vs. distance should have a very similar pattern to the real data.  Here’s a graph of the AWS reconstruction with infilled temperature values.
Compare this plot with the previous plot from actual measured temperatures.  Now contrast that with the AWS plot above.  The infilled AWS reconstruction has no clearly evident pattern of decay over distance.  In fact, many of the stations show a correlation of close to 1 for stations at 3000 km distant!  The measured station data is our best indicator of true Antarctic trends and it shows no sign that these long distance correlations occur.  Of course, common sense should also make one suspicious of these long distance correlations as they would be comparable to data that indicated Los Angeles and Chicago had closely correlated climate.
It was earlier mentioned that the use of 3 PCs was analogous to the loss of detail that occurs in data compressions.   Since the AWS input data is available, it is possible to regenerate the AWS reconstruction using a higher number of PCs.  It stood to reason that spurious correlations could be reduced by retaining the spatial detail lost in the 3 PC reconstruction.  Using RegEM, we generated a new AWS reconstruction using the same input data but with 7 PCs.  The distance correlations are shown in the plot below.

Note the dramatic improvement over that shown in the previous plot.  The correlation decay with distance so clearly seen in the measured station temperature data has returned.  While the cone of the RegEM data is slightly wider than the ‘real’ surface station data, the counterintuitive long distance correlations seen in the Steig reconstruction have completely disappeared.  It seems clear that limiting the reconstruction to 3 PCs resulted in numerous spurious correlations when infilling missing station data.


Using only 3 principal components distorts temperature trends
If Antarctica had uniform temperature trends across the continent, the spurious correlations might not have a large impact in the overall reconstruction.  Individual sites may have some errors, but the overall trend would be reasonably close.  However, Antarctica is anything but uniform.  The spurious correlations can allow unique climactic trends from a localized region to be spread over a larger area, particularly if an area lacks detailed climate records of its own.  It is our conclusion is that is exactly what is happening with the Steig AWS reconstruction.
Consider the case of the Antarctic Peninsula:

The      peninsula is geographically isolated from the rest of the continent
The      peninsula is less than 5% of the total continental land mass
The      peninsula is known to be warming at a rate much higher than anywhere else      in Antarctica
The      peninsula is bordered by a vast area known as West Antarctica that has      extremely limited temperature records of its own
15 of      the 42 temperature surface stations (35%) used in the reconstruction are      located on the peninsula

If the Steig AWS reconstruction was properly correlating the peninsula stations temperature measurements to the AWS sites, you would expect to see the highest rates of warming at the peninsula extremes.  This is the pattern seen in the measured station data.  The plot below shows the temperature trends for the reconstructed AWS sites for the period of 1980 to 2006.  This time frame has been selected as this is the period when AWS data exists.  Prior to 1980, 100% of AWS reconstructed data is artificial (i.e. infilled by RegEM).

Note how warming extends beyond the peninsula extremes down toward West Antarctica and the South Pole.  Also note the relatively moderate cooling in the vicinity of the Ross Ice Shelf (bottom of the plot).  The warming once thought to be limited to the peninsula appears to have spread.  This “smearing” of the peninsula warming has also moderated the cooling of the Ross Ice Shelf AWS measurements.  These are both artifacts of limiting the reconstruction to 3 PCs.
Now compare the above plot to the new AWS reconstruction using 7 PCs.

The difference is striking.  The peninsula has become warmer and warming is largely limited to its confines.  West Antarctica and the Ross Ice Shelf area have become noticeably cooler.  This agrees with the commonly-held belief prior to Steig’s paper that the peninsula is warming, the rest of Antarctica is not.
Temperature trends using more traditional methods
In providing a continental trend for Antarctica warming, Steig used a simple average of the 63 AWS reconstructed time series.  As can be seen in the plots above, the AWS stations are heavily weighted toward the peninsula and the Ross Ice Shelf area.  Steig’s simple average is shown below.  The linear trend for 1957 through 2006 is +0.14 deg C/decade.  It is worth noting that if the time frame is limited to 1980 to 2006 (the period of actual AWS measurements), the trend changes to cooling, -0.06 deg C/decade.

We used a gridding methodology to weight the AWS reconstructions in proportion to the area they represent.  Using the Steig’s method, 3 stations on the peninsula over 5% of the continent’s area would have the same weighting as three interior stations spread over 30% of the continent area.  The gridding method we used is comparable to that utilized in other temperature constructions such as James Hansen’s GISStemp.  The gridcell map used for the weighted 7 PC reconstruction is shown here.

 
Cells with a single letter contain one or more AWS temperature stations.  If more than one AWS falls within a gridcell, the results were averaged and assigned to that cell.  Cells with multiple letters had no AWS within them, but had three or more contiguous cells containing AWS stations.  Imputed temperature time series were assigned to these cells based on the average of the neighboring cells.  Temperature trends were calculated both with and without the imputed cells.  The reconstruction trend using 7 PCs and a weighted station average follow.

The trend has decreased to 0.08 deg C/decade.  Although it is not readily apparent in this plot, from 1980 to 2006 the temperature profile has a pronounced negative trend.
Temporal smearing problems caused by too few PCs?
The temperature trends using the various reconstruction methods are shown in the table below.  We have broken the trends down into three time periods; 1957 to 2006, 1957 to 1979, and 1980 to 2006.  The time frames are not arbitrarily chosen, but mark an important distinction in the AWS reconstructions.  There is no AWS data prior to 1980.  In the 1957 to 1980 time frame, every single temperature point is a product of the RegEM algorithm.   In the 1980 to 2006 time frame, AWS data exists (albeit quite spotty at times) and RegEM leaves the existing data intact while infilling the missing data.
We highlight this distinction as limiting the reconstruction to 3 PCs has an additional pernicious effect beyond spatial smearing of the peninsula warming.   In the table below, note the balance between the trends of the 1957 to 1979 era vs. that of the 1980 to 2006 era. In Steig’s 3 PC reconstruction, moderate warming that happened prior to 1980 is more balanced with slight cooling that happened post 1980.  In the new 7 PC reconstruction, the early era had dramatic warming, the later era had strong cooling.  It is believed that the 7 PC reconstruction more accurately reflects the true trends for the reasons stated earlier in this paper.  However, the mechanism for this temporal smearing of trends is not fully understood and is under investigation.  It does appear to be clear that limiting the selection to three principal components causes warming that is largely constrained to a pre-1980 time frame to appear more continuous and evenly distributed over the entire temperature record.



Reconstruction

1957 to 2006 trend


1957 to 1979 trend (pre-AWS)


1980 to 2006 trend (AWS era)



Steig 3 PC 

+0.14 deg C./decade


+0.17 deg C./decade


-0.06 deg C./decade



New 7 PC 

+0.11 deg C./decade


+0.25 deg C./decade


-0.20 deg C./decade



New 7 PC weighted

+0.09 deg C./decade


+0.22 deg C./decade


-0.20 deg C./decade



New 7 PC wgtd   imputed cells

+0.08 deg C./decade


+0.22 deg C./decade


-0.21 deg C./decade




Conclusion
The AWS trends which this incredibly long post was created from were used only as verification of the satellite data.  The statistics used for verification are another subject entirely.  Where Steig09 falls short in the verification is that RegEM was inappropriately applying area weighting to individual temperature stations.  The trends from the AWS reconstruction clearly have blended into distant stations creating an artificially high warming result.  The RegEM methodology also appears to have blended warming that occurred decades ago into more recent years to present a misleading picture of continuous warming.  It should also be noted that every attempt made to restore detail to the reconstruction or weight station data resulted in reduced warming and increased cooling in recent years.  None of these methods resulted in more warming than that shown by Steig.
We don’t yet have the satellite data (Steig has not provided it) so the argument will be:
“Silly Jeff’s you haven’t shown anything, the AWS wasn’t the conclusion it was the confirmation.”
To that we reply with an interesting distance correlation graph of the satellite reconstruction (also from only 3 PCs).  The conclusion has the exact same problem as the confirmation.  Stay tuned.

(Graph originally calculated by Steve McIntyre)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e97a0fe00',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The unprecedented, devastating bushfires that engulfed Australia – from even before our summer began – have forever disrupted our usually laconic and relaxed memories of Christmas and New Year. Those memories are instead marked by anguish, anxiety, grief and consternation about our future.  We turned to fire and emergency chiefs, tireless local volunteers, our national broadcaster and the spirit of community to find leadership in this crisis. And very late, but at last, we turned to scientists; fire, flood and climate experts; and vitally, Indigenous knowledge, to start to rebuild a sense of a path forward for our nation. Consistent with the rapidly changing expectations of global investors, we’ve also turned to business for leadership. It’s no coincidence the Edelman trust barometer data for 2020 reveals that around the world, trust in government leaders has continued to collapse, while our highest trust is reserved for scientists and people in our local community.  Significant increases have been observed in our expectations of CEOs: 77% of people agree that the CEO of the company where they work should speak out on climate change and 74% believe that CEOs should take the lead on change rather than waiting for the government to impose it. As a company director returning to work in preparation for the half-year results period, the change in sentiment about action on climate change among many colleagues and employees has been palpable. This is both at a personal level, but also after the increasing calls from corporate regulators in relation to additional fiduciary duties of directors and executives regarding the complex and interrelated climate risks now faced by our organisations. Pleasingly, a considerable number of Australian CEOs have announced commitments to net zero carbon by 2050, and we’ve seen global organisations such as Microsoft pushing harder to account for their carbon impact in the past. I am cautiously optimistic about the impact of good businesses stepping up on climate change action. But it is tinged with the frustration of knowing that Australia could have been a global leader in decarbonising the economy had we supported business leaders calling for a legal and policy framework for reducing our carbon emissions, while protecting our economy and its most significant industries. Fifteen years ago a group of Australian business leaders from insurance, banking and energy made the case for early climate action supported by science, economics and stewardship for our country. The Australian Business Roundtable on Climate Change released The Business Case For Early Action in 2006 showing significant reductions in greenhouse gas emissions could be achieved at an affordable cost to the Australian economy. Many of us have continued the advocacy since. Particularly the insurance and finance sectors which are at the front line of exposure, as departing governor of the Bank of England, Mark Carney, continues to point out. Australia has always been a resource nation. We pay tribute to those key vital industries that have in the past shaped us as a country. But at our core, we are a resourceful nation. We have the know-how, the science and the business acumen to thrive in a low-carbon economy – and as Professor Ross Garnaut points out, be a superpower at that. It’s time to realise this vision. If we delay further we cede that opportunity to other nations. This is why the climate change bill being introduced by independent MP Zali Steggall and the crossbench comes at a crucial time. It provides an internationally proven framework that will ensure certainty so we can manage risk, as well as actively pursue new areas of opportunity in a low-carbon economy. This model has been proven and enacted in the United Kingdom, Germany, France and New Zealand. It was introduced in the UK in 2008 and adopted under a conservative government. It is sensible. It is pragmatic. It is responsible. The 2020 bill proposes: 1. An independent climate change commission to oversee the transition.2. An economy-wide net-zero target by 2050.3. Economy-wide emissions budgets and national plans for reducing greenhouse gas emissions.4. A national climate risk assessment and plans for adapting to changing climate. 5. Transparent monitoring, reporting on progress and plans. This is our moment in history to come together and demonstrate to Canberra that as a nation, we need an end to the partisan politics. We need a goal we can all work towards together. When business first called for action almost 20 years ago, time was somewhat on our side. We now have no time to waste. Beyond politics and ideology, and in collaboration with others, business leaders have the opportunity to show strong stewardship and care for a dynamic, sustainable future for our country. As business leaders – whether CEOs or nonexecutive directors – supporting a non-partisan, sensible piece of legislation is not only what our community, employees and stakeholders expect, it’s also good for business. Let’s stand up and help parliament take the next step we need for a prosperous economy to 2050.  • Sam Mostyn is a non-executive director on a number of ASX-listed boards, is a sustainability adviser and has long engagement working with civil society on climate, gender and Indigenous issues."
"
Share this...FacebookTwitterPada saluran yang indah ini kita akan sedikit berbagi menurut anda seperti apa buku petunjuk bermain judi bandarq yang benar dan aman khususnya untuk anda yang sedang pemula, bermain judi bandarq itu tidak semudah yang anda bayangkan hanya dengan menyembunyikan taruhan maka anda bakal menang begitu saja, terdapat beberapa hal penting yang harus anda pelajari beserta baik agar bisa bermain dengan profesional. Berikut merupakan panduan sederhana bermain pertaruhan bandarq. Simak baik-baik ulasannya di bawah ini.
# Langkah pendaftaran
Langkah pertama yang harus anda lakukan adalah dengan mencari distributor judi online terlebih lewat yaitu agen judi domino online yang ada pada internet, gunakan cara yang sudah kami tuliskan pada website ini untuk memperoleh agen judi domino online yang terpercaya. Setelah kamu menemukan agen judi yang bsia dipercaya silahkan lakukan pendaftaran terlebih dahulu, pati semua form dengan betul dan valid terlebih untuk isian pada nomor perkiraan hingga kontak yang dikau miliki, kemudian setelah tersebut silahkan login dengan account yang sudah anda buat tadi, lakukan transfer deposit sesuai dengan jumlah minimal maupun maksimal yang dikasih oleh situs. Setelah tersebut silahkan lanjut pada tahap berikutnya.
# Tahap pengenalan permainan judi bandarq
Kemudian tahap kedua adalah tahap pengenalan judi domino online, pertama-tama anda pahami dulu apa itu tiket judi domino online, kartu judi ini biasa disebut dengan permainan judi gaple atau dadu bernomor kalau di daerah anda. Di setiap permainan judi kartu domino dilakukan 6-8 orang di satu meja dengan nilai taruhan yang beragam, mulai dari dari 2000 rupiah terlintas ratusan bahkan jutaan yen. Permainan ini menggunakan relevansi 52 kartu acak yang masing-masing player akan jadi secara acak, tujuan dari permainan ini adalah player yang mendapatkan kombinasi poin paling besar maka dialah yang akan menjadi pemenangnya.
# Tahap pengenalan sebutan dalam permainan judi bandarq
Kemudian tahap berikutnya member akan sedikit mengulas kurang lebih pengertian dari istilah yang digunakan dalam permainan betting domino, pertama adalah pengenalan di meja sendiri. Ada istilah check, raise, fold, call, hingga all tersebut. pengertian check sendiri menjajal kartu artinya tidak berbuat taruhan, Raise menaikkan taruhan lebih tinggi, fold bukan mengikuti permainan dan mempersembahkan kartu ke bandar, all in melakukan taruhan semata dana yang kita punya dan untuk call swasembada adalah memanggil taruhan pantas dengan jumlah taruhan sebelumnya. Kemudian pembahasan mengenai tiket spesial, dalam permainan tersebut ada 4 kartu spesial yang bisa anda miliki, jika anda bisa memperoleh salah satu dari 4 kartu spesial ini bisa dipastikan anda menang. Slip spesial mulai dari slip 99/qiu, kartu murni kecil/besar, kartu kembar/balak dan slip 6 dewa.
# Tara bermain judi domino online
Tahap pertama anda bakal duduk berjumlah 8 orang-orang, pembagian kartu seperti suksesi jam, pada pembagian pertama anda hanya akan nampi 3 kartu, analisa tiket anda apakah nantinya dapat berkombinasi bagus atau bukan, jangan terkecoh dengan slip bagus sebelum kartu ke empat muncul. Lakukan judi bola sesuai dengan keyakinan serta keputusan anda, setelah tersebut kartu ke empat bakal muncul, jika kartu kamu bagus misalnya 99/qq tidak sungkan untuk all in.
Nah itulah cara permainan judi domino yang simple dan mudah untuk para pemain pemula
Share this...FacebookTwitter "
"

I am rather concerned about my elderly father‐​in‐​law, who lives in northern Virginia. I just visited him, as Washington’s temperatures bubbled into the high 90s. On his television, the summer’s first heat‐​related fatalities were being reported. I noticed that his house seemed unusually warm, and I went over to the window to turn on his air conditioner. “Don’t bother,” he told me, “it’s not working.” Without air conditioning, my father‐​in‐​law and millions of elderly citizens just like him are at grave risk in this weather. After I write this column, I’m calling Sears and having a new machine delivered pronto. 



“This weather” has been in the news for over a week now. The last week in July is, statistically speaking, normally the hottest all year in the eastern United States. Although the news may be all atwitter about the temperatures, they’re actually pretty much what you’d expect on sunny days at the end of July. 



The average July high temperature along most of the East Coast from New York on down is around 90 degrees, with the southern portion a bit above and the northern a bit below. Sometimes a sea breeze gets into the Big Apple, but Philly, Baltimore, Washington, and the like are far enough inland that they simply bake. 



Given that the last week of July is the warmest in the month, temperatures in the lower 90s should be the rule, not the exception. But these “normal” values are composed of 30‐​year averages. Some days that form that average were sunny, some were cloudy, some had rain and some were in‐​between. It stands to reason that a bright sunny day is going to be warmer than the average–so that 95 degrees is pretty “normal” as long as it doesn’t cloud up. 



This is the threshold temperature at which elderly deaths begin to take off. And, true to form, we’ve seen the usual spate of stories trying to conflate this mortality, this summer’s temperatures and global warming caused by pernicious economic activity. 



Let’s get one thing straight. There is no warming trend in U.S. summer temperatures over the last 80 years. It did warm a bit from 1900 to 1930, but that change surely wasn’t because of a greenhouse effect; we hadn’t put much new carbon dioxide in the air by then. Further, current planetary temperatures measured by satellites and weather balloons are considerably below their average for the last two decades. 



In addition, heat‐​related mortality is going down. In 1995, Chicago saw several hundred deaths in a July heat wave. But there were 885 heat‐​related deaths in the Second City in 1955. Want to see true carnage? Go back to 1900, when 10,000 Americans perished in the heat. (The globe was one degree cooler then!) 



What’s the difference here? Two words: air conditioning. 



Air conditioners use more electricity than any other home appliance. On a hot day, they create such demand for electricity that, sometimes, the power fails. After this, the county coroner isn’t far around the corner. In fact, it was a power failure that magnified the 1995 Chicago tragedy. Normally in a heat wave, the poorer South Side experiences more deaths than the North Side. But a power outage in the affluent side of town resulted in a pretty equal distribution of fatalities across income classes. 



In this summer’s heat, Mayor Richard Daley has been exhorting citizens who feel they cannot afford to run their air conditioners to take advantage of a federal program designed to subsidize payments in just that eventuality. Somehow I do not believe that every 80 year old has gotten this message and fear that some will die today. 



Which brings us back to global warming. It should be self‐​evident that the very technology that enhances the greenhouse effect–the production of electricity–is what saves our lives in the heat of a normal summer. Thousands more would die, as did in 1900, without air conditioning in a world where the enhanced greenhouse effect and dreaded global warming did not exist. 



The risk of power failure can be averted by installing new generation capacity. But every time a new power plant is proposed, someone squawks “global warming.” When lack of power causes an outage on a hot day, that well‐​intended protest becomes a lethal weapon. 



Therefore, it is somewhat ironic that all proposals to fight global warming drastically raise the price of energy and power. The Kyoto Protocol on climate change requires us to reduce our emissions of greenhouse gases (read: use of energy) by 30 to 45 percent by 2008 compared with where we would be if we just went on as we are. If the price of electricity more than doubles (a likely scenario according to most experts), how many more of our elderly will hesitate to turn on the air conditioner until it is too late? The Kyoto Protocol is a killer.
"
nan
"Most of Poland’s wild boars are being culled to stop the spread of African swine fever, a deadly viral disease that endangers farm pigs.  The Polish Hunting Association was mandated to organise large-scale hunts to kill more than 200,000 wild boars by the end of January, reducing its population by almost 90%. The government had been planning to erect a 1200 km-long fence along the eastern border of Poland to stop infected wild boars from moving westward, but the costly project was abandoned.  The war against African swine fever lifts some of the hunting restrictions, allowing licensed hunters to kill pregnant sows and organise hunts in national parks and nature reservations. These drastic measures have sparked protests. Even some of the hunters have refused to take part in the slaughter of wild boars.  Wildlife biologists also warn that mass hunts are not only pointless, but can even spread the deadly virus through human-mediated contact with infected blood. A letter from over 1000 scientists urges Polish government to seek out other solutions, highlighting the importance of following biosecurity guidelines.  So: why are wild boars scapegoated?  While the disease takes its toll on the wild boar population across Eastern Europe, the authorities are trying to prevent an epidemic in another species: domestic pigs. Due to high mortality rates, African swine fever poses high risk to the European pork industry. Every outbreak in pig farms is followed by gassing all livestock, a quarantine period, and harsh trade restrictions. It is the potential economic loss that drives the political war against this deadly disease, for which there is no vaccine. The current European epidemic started in 2007, when a ship containing infected pork meat entered the harbour of Poti in Georgia. From there, African swine fever spread in the Caucasus, to Chechnya and Russia. When the epidemic reached the eastern borders of the EU in 2014, with cases of infected wild boars in Lithuania, Poland, Latvia and Estonia, wild boars became an international concern. The virus initially occurred in Sub-Saharan Africa, where it affected wild pigs, bushpigs, and warthogs. It was first described in 1921, recording an  earlier diagnosis from 1910, when domestic pigs brought by European settlers became infected. African swine fever started to spread across British East Africa, mostly in what is now Kenya, with the colonial traffic in commodities.  The first outbreak in Europe was reported in 1957 in Lisbon, Portugal. By the 1990s, the Iberian Peninsula had suffered major economic losses caused by the disease decimating its swine population. Pork prices dropping drastically. Eventually, Portugal and Spain took radical steps towards eliminating the virus and achieved it by culling nearly all of their farm pigs and wild boars. This seems to be the model of containing the most recent outbreaks that the Polish authorities are following. But it’s not the appropriate response. The viral cycle in Africa and Iberia involves a soft tick that transmits the virus between wild and domestic pigs, but this particular type of parasite is absent Eastern Europe. Biologists are still searching for an insect mechanical vector of the disease in this geographical context.  But the virus can be also transmitted between swines through direct contact, bodily fluids, and ingestion of infected meat. That is why highly mobile wild boars are believed to carry the virus, and figure as the main culprits of the epidemic. The disease can be transmitted by infected boars, but also through their carcasses and contaminated meat, putting hunting practices and meat consumption under suspicion. The two incursions of the virus to Europe occurred through improper disposal of food waste. But even before the introduction of African swine fever to Europe, wild boars had a bad reputation. In rural areas, wild boars are notoriously blamed for crop damage. They have also become a more frequent sight in European cities, often causing human-wildlife conflicts as they raid garbage, and pose a risk of attacks or road collisions.  Since the 1990s, the wild boar population has been rising rapidly across Europe due to mild winters and industrial agriculture privileging energy-rich corn and soy production (the same crops that enjoy EU subsidies). These favourable conditions have even altered the reproductive patterns of the species, allowing for more frequent pregnancies and larger litters. Human-induced conditions, such as the elimination of natural predators, climate change and agricultural mono-cultures aided wild boars to thrive.  Meanwhile, scientists have linked the most recent cases of African swine fever (reported in the Czech Republic, Hungary, Belgium and China) to human rather than wildlife mobility. Such long-distance jumps in the distribution of the virus suggest that humans play a significant role in spreading African swine fever.  As the first large-scale hunts in Poland began in mid-January, anti-hunting activists claimed that hunters were carrying traces of blood on their shoes and cars, rendering the whole operation counterproductive and dangerous. Even though wild boars are typically considered invasive pests, many Poles have started to defend the swines against such drastic eradication plans. Activists are not only disrupting hunts, but also taking to the streets, where the wild boar is becoming a symbol of political resistance. Following earlier protests against logging in the Białowieża Primeval Forest, the environmental policy of the Polish government is increasingly coming under public scrutiny."
"

The U.S. Senate will almost surely approve permanent normal trade relations (PNTR) this summer, paving the way for President Clinton to cast a favorable U.S. vote for China’s entry to the World Trade Organization before the end of the year.



Both China and the United States will benefit from more Open trade. New wealth will be created as firms, investors and workers profit from their comparative advantages and as consumers are better served. Along with the increased volume of trade will be an increased demand for financial and other services.



Western banks and non‐​bank financial institutions stand to benefit handsomely from their specialized knowledge and technological advantages. And, since every market exchange is two‐​sided and requires consent, gains for Western firms imply gains for China. PNTR and WTO accession, therefore, are good for China and for its trading partners. 



If the PRC meets investors’ expectations about greater market access and more secure property rights, foreign investment in China will continue to expand, and the flow of new capital will create opportunities for millions of Chinese to increase their standard of living. The Chinese people will demand banking, investment, insurance and other financial services‐​from both foreign and domestic firms‐​and they will become increasingly intolerant of government policies that limit their investment choices and “cannibalize” their savings. 



The dismal condition of China’s state‐​owned banks, which Are burdened with the nonperforming loans made to state‐​owned enterprises (SOEs), cannot be improved without the discipline of foreign competition. Just as non‐​state industrial firms have been allowed to grow spontaneously, private banks and financial institutions must have the freedom to develop if China is to improve its allocation of scarce capital and achieve healthy long‐​term growth. 



What China needs is not “socialist capital markets” but Real capital markets with private owners who can specialize in Risk taking, who can freely capitalize expected future profits Into their present values by selling shares on organized stock exchanges and who are held fully responsible for losses. That means prices, including interest rates, must be determined by the free market, not by the Chinese Communist Party (CCP). 



If the mainland is to become a major player in the global economy, it must stand by its commitment to liberalize its financial sector upon accession to the WTO. That means, according to the Accession Agreement, that, within five years after China accedes to the WTO, U.S. and foreign banks should have full access to the local currency market: They should be able to deal directly with Chinese citizens and business firms in renminbi, they should be able to establish branches throughout China, and they should receive national treatment (i.e., the same treatment as domestic banks). 



Any backsliding from those commitments will send a signal to the global financial community that China is not to be trusted. Capital will leave the mainland and flow to places where it is protected by the rule of law. 



Granting China PNTR and bringing it into the WTO will encourage Beijing to conform to international norms and move closer to the rule of law. But no one should be under the illusion that the adjustment process will be easy or fast. 



Hong Kong and Taiwan can show China the way toward the free market and a more open society, and the Internet and international trade can show the Chinese people that freedom is valuable in its own right, in addition to being a means to greater wealth. But, in the end, the Chinese people themselves must determine the course of their nation by the choices they make. 



China’s financial future will depend on the steps that are taken in the next several years to restructure state‐​owned banks and enterprises. Allowing greater foreign competition will play an important role in transforming the financial landscape, but ultimately the only way to efficiently allocate capital and de‐​politicize investment decisions is to privatize banks and SOEs. 



The growing enthusiasm of Chinese President Jiang Zemin For equity markets should not be misconstrued. He and his supporters have not suddenly become capitalists. They simply recognize that tax revenues are not sufficient to bail out insolvent banks and firms. Oddly, they see equity markets as a socialist tool for raising capital to “revitalize” state‐​owned banks and SOEs. 



The plan is to allow some islands of private ownership in A sea of state ownership. The state would retain majority ownership and control of the key financial and industrial firms it puts on the market. Those who invest in such firms will have attenuated private property rights and trade in pseudo, not real, financial markets.



Nevertheless, getting China’s leaders to start thinking in terms of equity markets, talking about the need for more flexible interest rates and allowing some private ownership are steps in the right direction. 



Once West meets East in the global capital markets, things will start to change‐​perhaps faster than anyone can imagine. Stock exchanges in Shanghai and Shenzhen will be expanded to include index funds, Chinese citizens will eventually be allowed to move their savings into a portfolio of international investments and Chinese capitalists will be trading over the Internet 24 hours a day. The pace of change, of course, will depend on the extent of competition that the CCP allows and, hence, on the political climate. If the global market proves more powerful than the party, change will accelerate.



The reality is that China’s current financial markets are strictly limited, the renminbi is not convertible on the capital account, SOEs are crowding out the non‐​state sector in the quest for investment funds, and foreign banks and non‐​bank financial firms are still waiting patiently to enter the Middle Kingdom.



Those constraints on capital freedom have led to rampant corruption. Below‐​market pricing of loans by state‐​owned banks has created opportunities for side payments as a means of deciding who gets the scarce state funds; exchange and capital controls have led to attempts to circumvent the law by bribery and favors. The success of those attempts is revealed by the fact that, from 1991 through 1998, more than US$100 billion illegally left the mainland for safe havens in Hong Kong and elsewhere. That figure shows up in the errors and omissions component of China’s balance of payments, according to Dong Fu, an economist at the Federal Reserve Bank of Dallas, and amounted to nearly 40 percent of total foreign direct investment in China during that period.



If China wants capital to freely come and stay, it must be free to leave. More importantly, Beijing must establish sound constitutional protection for private investors. That is the challenge for the next decade. By granting China PNTR and by allowing it to enter the WTO, the U.S. Congress will help China meet that challenge and help create real, not pseudo, financial markets for the future.
"
"
Map from the University of Alabama-Huntsville. Each contour represents 0.2 degree C per decade warming or cooling between Dec. 1979 and Nov. 2008
From the USA Today Weather Blog
This has been in my inbox for a couple of weeks, so on a  fairly quiet day for weather, I thought I’d put this out there. John Christy of the  University of Alabama-Huntsville reported earlier this month that the  Earth’s climate change over the past 30 years has been rather uneven: It’s  gotten much warmer in the Arctic and, at the same time, cooler in the  Antarctic.
Christy and his colleague Roy Spencer, who are known in some quarters as global  warming skeptics, use data from satellites to measure the temperature of the  Earth. The more well-known NASA GISS and  National Climatic Data  Center data sets primarily measure surface temperatures.
Overall, Christy found that Earth’s atmosphere warmed an average of about  about 0.72 degree F in the past 30 years, according to NOAA and NASA satellites.  More than 80 percent of the globe warmed by some amount. However, while parts of  the Arctic have warmed by as much as 4.6 degrees F in 30 years, Christy says  that much of the Antarctic has cooled, with parts of the continent cooling as  much as the Arctic has warmed (see map, above; click to enlarge).
“If you look at the 30-year graph of month-to-month temperature anomalies,  the most obvious feature is the series of warmer-than-normal months that  followed the major El  Nino Pacific Ocean warming event of 1997-1998,” says Christy. “Right now we  are coming out of one La Nina Pacific Ocean cooling event and we might be  heading into another. It should be interesting over the next several years to  see whether the post La Nina climate ‘re-sets’ to the cooler seasonal norms we  saw before 1997 or the warmer levels seen since then,” he says.
He adds that most of the warming found in the satellite data has taken place  since the beginning of the 1997-98 El Nino, and that Earth’s average temperature  showed no detectable warming from December 1978 until the 1997 El Nino.
Meanwhile, the Washington  Post reported yesterday that the USA “faces the possibility of much more  rapid climate change by the end of the century than previous studies have  suggested, according to a report led by the U.S. Geological Survey.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a221b04',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
More anecdotal weather news of a colder and more brutal winter.

From the Juneau Empire, Juneau Alaska
Bitter cold moves in to Interior – Temperatures could drop to 50 below zero in parts  of Alaska
Meanwhile, in other news: Roofs collapsing due to record snows in Spokane, WA

FAIRBANKS – Bitterly cold weather slid over from Canada and settled into  Interior Alaska with forecasters saying temperatures could continue to slide to  nearly 50 degrees below zero in coming days.
Over the weekend, the mercury at Fairbanks International Airport dropped to  39 degrees below zero. Areas in the Interior outside the city were even colder;  46 below on the Yukon Flats, 41 below in Fort Yukon and 44 below in Central,  according to the weather service.
Rick Thoman, lead forecaster at the National Weather Service office in  Fairbanks, said temperatures rose a few degrees on Sunday, but that was it.
“The temperature will probably continue to go up and down randomly,” he said.  “With no clouds and no wind on the valley floor, temperatures are pretty much  probably going to be stuck.”
Fairbanks had experienced a relatively mild winter prior to Christmas. It had  only dropped to 30 below once, in early December.
The howling winds and frigid weather were too much for several mushers,  including four-time Iditarod winner Jeff King and his dog team, who pulled out  of the Gin Gin 200, a 200-mile race along the Denali Highway.
For the men, Brent Sass came in first, ahead of more well-known mushers such  as four-time Yukon Quest and two-time Iditarod champion Lance Mackey who was  fourth.
Mackey, resting Monday at the lodge in Paxson, said it was blowing so hard  and the teams were getting so turned around by the wind that it almost made him  laugh.
“It was almost comical. Your sled was going sideways down the road,” he said.
Further down the trail, when temperatures dipped to 50 below, it wasn’t so  much fun, he said.
“There were a lot of people not wanting to put their teams through that,”  Mackey said. “It is all about the dogs in a situation like this… They get  hardened by this stuff. That is why we do it.”
For the women, defending champion Jodi Bailey of Chatanika came in first.
Several mushers pulled out of the race from Paxson to the MacLaren River  Lodge.
“It was a real challenge this year,” Bailey wrote on a friend’s Facebook  page. “Winds like a banshee, and killer cold, wow am I glad to be back in  Paxson!!!”
According to the race Web site, temperatures at the MacLaren River Lodge were  between 35 and 40 below. It was reportedly 10 to 15 degrees colder on the lower  portions of the trail during the second portion of the race.
In Southeast Alaska, at least 20 inches of snow fell in Ketchikan, forcing  the shut down of the Ketchikan International Airport for a few hours. The  airport shut down at about 1:30 p.m. Sunday due to the heavy snow.
“We’ll stay here all night and dig out,” airport manager Mike Carney said.
The airport reopened Monday and normal operations resumed.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99e844af',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterLast Saturday polar bear poster child Knut died unexpectedly, much to the horror of viewers and fans internationally. Knut was only 4 years old. Polar bears normally live to be more than 30.
Of course, everyone is asking why. What was the cause of death?
Media reports say Knut had some sort of brain ailment. Activists are blaming it all on the Berlin Zoo.
German tabloid BILD claims that offcial cause of death was drowning. But Wolfgang Röhl at German blog achgut.com doesn’t buy it. He writes:
Please – since when do polar bears drown? No, there has got to be something else behind all of this. Professor Stefan Rahmstorf of Potsdam Institute for Climate Impact Research, you’re on!”
Yes, the “experts” can clear up all of this – with an authoritive peer-reviewed paper, of course.  The consensus is there after all; the science is done. Everybody today knows what every (bad) thing is caused by.
Share this...FacebookTwitter "
nan
"Nine years ago, Britain generated nearly 75% of its electricity using natural gas and coal. In 2018, this dropped to under 45% – a remarkable transition away from fossil fuels in under a decade.  As energy efficiency improved, demand fell, and the UK generated less electricity than at any point since 1994. Our own analysis below looks at the past year, using similar data for Great Britain (as Northern Ireland has a separate power system), and we include net imports from France, the Netherlands and Ireland as an overall part of electrical generation. Here are a few things we found:  In 2018, Britain was coal-free for a record 1,898 hours – that’s up from just 200 hours in 2016. Coal generation fell for the sixth year in a row, and the country now has substantial periods without coal power (the longest stretch was just over three days straight). For comparison, the 5% of electricity generated from coal was a broadly similar level to the combined total of solar and hydro (see table at end of the article). Wind increased its output to 17% of the total, and combined with solar these two renewables generated more electricity than nuclear – another significant milestone. However, low levels of coal generation averaged across the year mask its importance at times when the electrical demand is particularly high. For example, over the week of the Beast from the East cold snap in February 2018, the gas system experienced significant stress and coal stepped in to provide nearly a quarter of Britain’s electricity. As coal generation is set to be phased out by 2025, the electrical system needs to continue to find alternative power sources to cope during extreme weather events.  Our analysis shows that annual renewable generation has increased by 27 terawatt hours (TWh) over the three years since 2015. This is particularly impressive considering the Hinkley Point C nuclear plant will produce a similar annual amount of electricity but will take three times as long to build (from contract signing). But what about the decade ahead? Could Britain repeat its success since 2010 and reduce its coal and natural gas generation by a further 30 percentage points? Under this scenario, the country would then generate just a sixth of its electricity from fossil fuels.  It’s definitely possible, but the next decade will be more challenging for two main reasons: the demand for electricity is expected to rise rather than fall, and incorporating ever greater levels of variable renewable generation will need additional flexibility. To achieve this, new renewable generation – new solar panels, new turbines, new hydro, tidal, marine and biomass generation – will have to replace an estimated 100 TWh per year (about four Hinkley Point Cs) from fossil fuels. That would require a build programme that was broadly 50% greater than the previous nine years.  Given the continued development of offshore wind in particular, this seems challenging but achievable. Solar and wind prices keep falling, which will help. Indeed, the UK’s business and energy secretary Greg Clarke recently said that “it is looking likely that by the mid 2020s, green power will be the cheapest power. It can be zero subsidy”. However, at some point over the next decade, electrical demand will stop falling as electric vehicles gain market share from fossil fuel vehicles, and electrical heating for homes becomes more popular. As an indication of the scale of the transport demand, in 2017 UK cars and taxis travelled 254 billion miles. If all those journeys were taken in electric vehicles about as efficient as the latest Hyundai or Tesla then total electrical demand would increase by a quarter (over 80 TWh).  These vehicles would need the equivalent of three Hinkley Point Cs to charge them over the year. This is also a similar level to current generation from renewables. The UK also needs to consider how to fill the gap that would be lost from fuel duty, which is forecast to raise around £28 billion this financial year. If charging these vehicles adds to electrical demand at peak times, there would be substantial new infrastructure costs (more pylons, stronger electrical sub-stations). If Britain adopts a smarter system, fleets of electric vehicles could provide network support by changing their times of charging or even providing electricity back to the grid. This could provide a massive new form of flexibility that is needed to accommodate greater levels of weather dependent renewable generation. This is not an easy task, though, and needs better communication between vehicle, owner and power companies. Overall, 2018 saw steady progress for low carbon generation, including record months for wind, biomass and, mid-heatwave, solar: Looking to 2019, with more renewable capacity being installed, it is possible that solar could overtake coal, and renewables could generate more than nuclear for every single month. They could also generate more than coal and gas combined over a month for the first ever time. If any of these do happen, it will be yet another indication of the speed at which Britain’s electricity system is changing.  The electrical generation data is from Elexon and National Grid. Data from other analyses (such as BEIS or DUKES) will differ due to methodologies and additional data, particularly by including combined heat and power, and other on-site generation which is not monitored by Elexon and National Grid.
Renewables in this analysis = wind + solar + hydro + biomass. "
"Despite overwhelming public opposition and a longstanding ban, fox hunting shows no signs of abating in the UK. The 2018 hunt season alone saw 550 reports of illegal hunting, though these figures only represent known incidents.  In 2014 it was found that 250,000 fox hunters attended Boxing Day hunts across the UK. In 2019, so far, at least 21 foxes have been killed by the hunt and 151 incidents of illegal hunting have been reported since the season began on November 1. The Hunting Act, which prohibited hunting foxes and wild mammals with dogs, was approved by the UK’s parliament in 2003 with 362 MPs in favour and 156 against. The following year it became law. In 2017, the British people were surveyed on whether they continue to support the ban on fox hunting and the result was resounding – the highest margin ever recorded on the matter - 85% thought fox hunting should remain prohibited. So if the ban is entering its 15th year, why is fox hunting still happening? This question is answered in the Hunting Act itself, particularly the manner in which it “outlaws” fox hunting. Article 1 states that a “person commits an offence if he hunts a wild mammal with a dog”. But the provision continues: “Unless his hunting is exempt.” Herein lies the deceit of the Hunting Act, for it lists a total of nine reasons a hunt may flout the general ban. One of the more commonly invoked exemptions maintains that it is legal to hunt foxes if they pose a danger to livestock, game, crops or fisheries. As such, fox hunting advocates would have us believe that Roald Dahl’s tale of Fantastic Mr Fox and his endeavours to outwit farmers is all too common a curse in rural communities.  This remains nothing more than a smokescreen to defy the ban. Research has shown that foxes naturally control rabbit populations that if left unchecked, would cause significant economic harm to farmers. The UK government’s Department for Environment, Food and Rural Affairs (DEFRA) also advises against controlling foxes, and instead favours strengthening protection around livestock to guard against natural predation. Another commonly used exemption exploits a loophole around flushing foxes out to help birds of prey hunt. This has seen fox hunters disguising their true intentions by taking birds of prey along with them without ever letting them loose.  There is also the dubious practise of “manufactured” trail hunting in which hounds are supposed to follow an artificial scent trail with no animal chased or killed. In reality, hunt organisers use actual fox scent and lay routes deliberately close to where foxes are known to live, meaning they quickly become the subject of a hunt. Trail hunting is again an attempt to hide the true intentions of those that wish to continue fox hunting. Monitoring and gathering accurate information on all this to help prosecute offenders is a dangerous task, with members of the public often exposed to insults, intimidation and threats from hunters. The inadequate Hunting Act and the nefarious practises of hunt organisers mean fox hunting endures in England and Wales. Scotland too, offers no refuge for foxes and the Protection of Wild Mammals Act 2002 provides similar loopholes that allow hunting to continue. Setting aside the cruelty of fox hunting, evidence from the Breeding Bird Survey suggests red fox numbers have declined by 41% since 1995. Introducing a complete hunting ban is more essential than ever to protect the UK’s foxes. The Hunting Act has humans as its focus by specifying how people can bend the law’s provisions to their circumstances. Despite its prevalence in much of environmental law, this human-centric idea is entirely the wrong approach. Any future legislative efforts need to place foxes, and other mammals, at the centre of legislation. Foxes must be protected for their own right, and a blanket ban on hunting, absent any exemptions, is the only way to safeguard populations. Severe penalties must also be included, to ensure that those already willing to flout the law will rethink their actions. The likelihood of such a move materialising during this parliament is slim, however. Prime Minister Theresa May offered a free vote to repeal the Hunting Act during the 2017 election but withdrew the pledge after her disastrous election result. It’s essential that campaigns for stronger anti-hunting laws highlight how widespread resistance to diluting the ban is. The failures of the existing ban endanger foxes and betray the wishes of a majority of the public. Any update to the Hunting Act must crack down on those who think they are above the law."
nan
"
The hits just keep on coming. 1,672,437 page views this month, up from 1,478,801 page views in March.

After posting last months stats, there was some discontent by some angry and somewhat incredulous bloggers that it might be an April Fools joke of some sorts. Sorry, no such luck.
But what was humorous, was one particular blogger (Joe Romm) who said:
It is absurd to publish one’s page views to 7 significant digits without caveats — even 2 is stretching it. 
I got a huge chuckle out of that. So just to show that the stats are indeed real, and accurate down to that 7th digit (since they come from the WordPress internal traffic counter), I’m expanding this month’s report.
Joe, this report’s for you. Here is a screencap showing my WordPress internal report page, with all 7 digits, sans caveats. No caveats are needed since the WordPress numbers are actual, not estimated, and not drawn from router statistics.
click for a larger image
Maybe Joe will reciprocate and post a screencap his own internal stats page.
The graph on this page above differs a bit from the headline graph (which is graphically edited to move the title inwards and downwards to fit nicely on the page) because it was snapped after 00GMT (5PM PST) and it shows the new month of May stats also, which are of course quite low by comparison.
So the numbers are real, from WordPress.com (where my blog is hosted) and exact to the 7th digit, despite angry accusations otherwise.
In that blog post, Joe Romm also showed an Alexa graph to prove his point, saying:
Interestingly, there is one independent source that suggests Watts’ page views and mine are in fact the same (and hence possibly around 1.4 million).  If you go to the Web traffic ranking and comparison site Alexa, go to page views, and type in wattsupwiththat.com, you’ll get this graph:

So at best I am just negating the disinformation Watts is spreading.  Sigh.  And lest there be any doubt, WattsUpWithThat is in fact an extremist anti-scientific denialist website, as his recent posts make clear.
Gosh, all that angry prose from a traffic report? I don’t much care whether WUWT beats Climate Progress in traffic or vice versa, but I do care when the proprietor suggests that by printing my own internal stats page I’m spreading disinformation. Of course, I don’t expect an apology, but I wanted WUWT readers to know.
OK, following Joe’s lead, lets look at Alexa this month.
Pageviews: we are still about the same in April according to Alexa.

Though over the longer 6 month term, WUWT is slow and steady while Climate Progress runs hot and cold.

Joe didn’t show the other interesting comparative statistics from Alexa though, so I thought WUWT readers might enjoy seeing them.
Here is Traffic Rank over 6 months:

And here is Reach:

But having been labeled a “spreader of disinformation”, please, don’t take my word for it, visit Alexa and see for yourselves. Here is the link.
Some people might wonder why I post my web stats each month. Some might say I’m tooting my own horn, well maybe a bit, I’m admittedly proud of WUWT.
But there’s a bigger reason. WUWT has thousands of visitors, many of whom are regulars who see this blog as something they relate to and grow with. Many visit daily or even more often. There’s a sense of pride and ownership with all of you, too. Without its readers, content contributors, and those tireless volunteer moderators that keep the snark under control, WUWT would be nothing. I try not to lose sight of that. This is my monthly reminder to myself that this is a team effort.
As this blog expands its reach, it seems only fair that my readers also get to share in seeing the growth. I listen to my readers, who are often more insightful than I am, and they offer wonderful suggestions and topics.
I thank you all for making WUWT one of top science blogs on the Internet today.
And thanks to you to Joe, for bringing up this topic last month.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e963b5251',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterAs usual, there have been more horror reports in the media today about the Fukushima crisis in the German media. German Zettels Raum blog brings us back to reality today.
Leading the way with the horror stories was Der Spiegel, which had the headline here at its online site:
Increased Radioactivity – Nuclear Power Plant Fukushima completely evacuated”.
So terrible has the media been with reporting on the nuclear crisis in Japan that Zettels Raum was compelled to write:
You always have to remember that when dealing with the media, journalistic research is something comparable to a chimpanzee dealing with the question of how one can prove the existence of anti-matter.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




It’s that bad. While media outlets like Der Spiegel propagated information distorted into horror stories, here’s what Japanese NHK said of the situation, 7:13 pm Japan time (emphasis added).
TEPCO: Black smoke rises from No.3 reactor
The Tokyo Electric Power Company, or TEPCO, says black smoke was seen rising from the No.3 reactor building at the quake-damaged Fukushima Daiichi nuclear plant at around 4:20 PM on Wednesday. TEPCO told reporters that it received a report 1 hour later that the smoke had gradually cleared.
The company said that the level of radiation near the main gate of the plant, 1 kilometer west of the No.3 reactor, was 265.1-microsieverts-per-hour at 5 PM. They added there had been no major change in the levels after the smoke was observed. On Monday afternoon, gray smoke was seen rising from the same reactor building. TEPCO said that the plumes turned white before disappearing.
The power company evacuated workers from the control room of the No. 3 reactor, as well as firefighters from Tokyo and Yokohama preparing for a water-spraying operation. The firefighters had to abandon their planned water spraying operation for the day.
The evacuation took place at only one of the control rooms of the 6 reactors. Click here to see a graphic on radiation levels at the plant. Here you can see that radiation levels are trending down.
Share this...FacebookTwitter "
"
I thought about writing a year end recap, but then I saw my traffic count for the month at 00 GMT (4PM PST), and thought that would do just as well at telling the story for this year. After a slight dip in October and November, WUWT has reached a new high at nearly 900,000 page views this month.

Click for a larger image
Not bad for a 12 month growth. My hit counter, as of this writing, stands at:
6,840,995 hits 
In December 2007, I hadn’t even broken 500,000.

Thanks to each and every one of you for visiting, contributing, and commenting. Thanks especially to the moderating team who keeps the temperature of this blog down whilst I think up new topics.
Here were the top 7 most popular posts in 2008, in case you missed them:
Top Posts
January 2008 – 4 sources say “globally cooler” in the past 12 months 140,090 views
A look at temperature anomalies for all 4 global metrics: Part 1 64,508 views
Where have all the sunspots gone? 59,144 views
Sudan hit by Apollo Asteroid 36,543 views
UAH: Global Temperature Dives in May 35,521 views
Solar Cycle 24 has officially started 34,877 views
Arctic sea ice back to its previous level, bears safe; film at 11 27,091 views


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9992e40d',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Nothing has been more embarrassing to the administration’s global warming apocalyptics than a few satellites, first launched in 1978, that resolutely refuse to find any warming of the lower atmosphere, which all the administration’s modelers and all the administration’s men predict should be heating with reckless abandon. 



When the satellite data were first published eight years ago by Roy Spencer and John Christy, of NASA and the University of Alabama respectively, the record was only 10 years long; the global warmers chanted in unison that the record was too short and therefore only a blip. Steve Schneider, who was the federal guru of climate change (he’s now with population apocalyptic Paul Ehrlich at Stanford), told _Science_ that “the next ten years will tell the story.”



They have, but the administration hasn’t listened. The story is that there’s a slight (but statistically significant) _cooling_ __ trend in the satellite data. It’s not surprising — but it _is_ scientifically dismaying — that the administration wants that trend to stop. So when the White House held its big global warming show last fall, up popped the leader of one of the nation’s most prominent environmental organizations (hint: it’s the one that destroyed the nuclear power business, thereby causing the greenhouse increase to begin with!) to declaim, “We have got to _do something_ about the satellite.” 



Climate watchers have been wondering how long it would take the $2.1 billion the federal government spends each year on global change research to “do something.” It did on February 23, when a California rocket scientist sent a manuscript to _Nature_ magazine, claiming he had found the error in the satellite data and that the atmosphere was actually warming up after all. (Sorry, can’t mention the author. _Nature_ has a hard‐​and‐​fast rule about blabbing to the press, and they’d have to pull the paper.)



The scientist calculated how much the solar wind — a stream of high‐​energy particles that exerts a slight force on everything in the solar system — would slow the satellites. In slowing down, the satellites fall a bit toward the earth’s surface, and they “see” a smaller area from which to take their measurements. To the satellites, a reduced area would appear colder than it really is. 



The satellite data could show a spurious cooling trend that exactly matched the balloon temperatures only if there were an exactly similar bias in the balloon readings. That’s simply not the case.



Adjusting for that induces a compensating warming in the satellite data of about 0.12 degrees (C) per decade. When that warming is coupled with the currently observed cooling in the record (0.04 degrees per decade), the result is a slight warming trend of 0.08 degrees every 10 years. That is still way below where it’s supposed to be (the computer models that served as the basis for the Kyoto global warming treaty predicted about 0.35 degrees per decade by now), but at least it’s a warming. 



The rocket scientist then sent his results not only to _Nature_ but to everyone else of scientific note who carries the administration’s precipitation. If you don’t believe there’s a cabal in the science community cheering for apocalyptic global warming, you ought to see the e‐​mail list. The only ones on it who weren’t cheerleaders happen to be Messrs. Spencer and Christy, whose satellite was gored. 



Included on the list were the Big Agency science administrators, who dutifully made sure Al Gore got a copy. He was reportedly ecstatic. And _Nature_ itself, whose editorial stance increasingly resembles the vice president’s view, helped things along ASAP. Send a paper to _Nature_ showing that global warming may not be such a big deal, and they’ll take six months to review it before, in all probability, sending a terse rejection letter. But this one got turned around in 10 days! 



Balance need not apply. Spencer and Christy asked the rocket scientist if they could publish a companion response but were told no, it would take too long. What’s the rush?



The rush is that the apocalyptics want to drop this news on the public like they did the “Ozone Hole over Kennebunkport” in 1992. That’s when Gore, still in the Senate, trumpeted some NASA chemical data indicating that a Northern Hemisphere ozone hole was imminent (the late winter ozone depletions are largely confined to Antarctica for good physical reasons). By stampeding the Senate into panic, Gore rammed through a 99‐​to‐​1 vote for an accelerated ban on certain industrial refrigerants. Only two weeks later, NASA had data showing that their initial pronouncement was a gross exaggeration, and the ozone hole never appeared. NASA wasn’t forthcoming until after the Senate acted.



If Spencer and Christy are allowed to respond concurrently, they will blow the paper to kingdom come. That’s because the satellite temperatures match up perfectly with a totally independent measure, taken twice a day by weather balloons as they ascend through the lower atmosphere. The satellite data could show a spurious cooling trend that exactly matched the balloon temperatures only if there were an exactly similar bias in the balloon readings.



That’s simply not the case. Or did thousands of folks launching millions of weather balloons over the last 20 years somehow decide to fudge the numbers so they would match up perfectly with those of satellites that didn’t show global warming? Talk about a massive conspiracy of right‐​wing airheads!
"
"

Seven years ago, on March 13, 1993, the eastern third of the United States was pasted by a big low‐​pressure system that was (erroneously) called the “Storm of the Century.” It dumped two inches of snow on the Gulf Coast beaches of Florida and two feet of snow in a wide swath from southwestern Virginia through New York and was followed by record cold. The next day, Kevin Trenberth, a federal climatologist with the U.S. National Center for Atmospheric Research, went on _Meet the Press_ to say that global warming, El Niño, and the 1991 Mount Pinatubo volcano were all involved. Never mind that Pinatubo caused a well‐​known global cooling. 



A look out today’s window reveals a remarkably different picture of blue skies, daffodils and softball practice. 



Given the choice, which Ides of March do you prefer? 



The fact is that we have just completed the warmest winter in the 105‐​year record for the continental United States, and few are complaining. But because it is good news, the same federal climatologists who glibly pulled the greenhouse effect trigger on the 1993 snowstorm can’t bring themselves to tell why we are so happy: global warming. Instead, they subsequently trotted out a scare story about how the warm winter is causing a drought. 



Even the most rudimentary statistical analysis of winter temperatures reveals that our coldness is determined by the number and severity of incursions of miserable, deadly, frigid air, minted in northwestern Canada, Alaska, and Siberia and shipped south. Winters in which there is little of this activity, such as the totally delightful ones of 1931–32 (87 degrees Fahrenheit in Roanoke, Virginia, on February 11, 1932) or 1999–2000, tend to record above‐​average temperatures. 



Federal climatologists could have looked it up in the January 24 issue of _Climate Research,_ which proved that the greenhouse warming in the last half of the 20th century is largely confined to those same cold air masses. That is predicted by greenhouse effect theory but rarely mentioned in the rush to gloom. In the Northern Hemisphere winter half‐​year, which itself contains over two‐​thirds of the surface temperature warming, these frigid‐​air killers of the homeless are warming at a rate 10 times greater than the average warming everywhere else! The paper proved the greenhouse effect was the finger on this trigger by showing that the more cold air there is, the more it warms up. 



Further proof, ironically, is given by satellite and weather balloon data that show no warming (after allowing for the now‐​departed 1998 El Niño) since their records became concurrent in 1979. These instruments are best at measuring the atmospheric slice from roughly 5,000 to 30,000 feet, but the cold Siberian and North American air is usually more shallow than that. 



Because people saving the planet do not brook embarrassment with decorum, please don’t ask them whether this disparity between the surface and lower‐​atmospheric temperature was forecast by the climate models cited as evidence for a global‐​warming disaster. 



Droughts are caused by deficits in water balance as evaporation exceeds precipitation. In the winter, over most of the nation, temperatures and the sun’s angle are so low that there’s very little evaporation, which is why, even in a dry January, most of the soils in the eastern half of the nation are saturated. The recent warm winter was only three degrees above the long‐​term average, or the average temperature difference between, say, March 5 and March 15. Evaporation rates remained low because the mean temperature is still way below the summer peak. So warm winter or not, winter temperatures do not cause droughts. 



Federal behavior is not hallmarked with consistency. If global warming is caused by burning of fossil fuels (it probably is), and if it is a terrible problem (I’ll bet not), then the only way it can be stopped is by raising energy prices through the roof. How high? Today’s $2.00 per gallon gasoline hasn’t dented consumption enough to cool the mean temperature of the planet 1/1,000 of a degree, even if spread over a year. In other words, a two‐​term Gore administration at that price would produce a change in temperature of less than 1/100 of a degree. But with the softballs flying by my window on this fine March day, who on earth would want to do such a thing anyway?
"
"
NASA’s updated data appears to suggest the annual rate of global polar ice loss  has actually decreased
 
Greenland’s Riviera – their green southwest. Will another Maunder minimum
grip the region in cages of ice again, or will bells ring in the portside squares,
as they did in the 1300’s before that cooling came, and ships sailed the fiords?
(Source: NASA)
Excerpt:
Washington Post correspondant Juliet Eilperin, in her 12-26-08 report entitled “New climate change estimates more pessimistic,” dutifully surveys the latest bleak findings of the climate change community. Her primary source is a recently released survey comissioned by the U.S. Climate Change Science Program – expanding on the findings of the 2007 4th IPPC Report on Climate Change. Apparently this “new assessment suggests that earlier projections may have underestimated the climatic shifts that could take place by 2100.” One of Eilperin’s primary examples of alarming new data is reported as follows:
“In one of the reports most worrisome findings, the agency estimates that in light of recent ice sheet melting, global sea level rise could be as much as 4 feet by 2100. The IPCC had projected a sea level rise of no more than 1.5 feet by that time, but satellite data over the past two years show the world’s major ice sheets are melting much more rapidly than previously thought. The Antarctic and Greenland ice sheets are now losing an average of 48 cubic miles of ice a year, equivalent to twice the amount of ice that exists in the Alps.”
Three years ago what NASA quantified as an alarming loss of  annual ice loss from Greenland was easily demonstrated at that time to be an  insignificant loss, and today NASA’s updated data appears to suggest the annual  rate of global polar ice loss has actually decreased since then.
http://ecoworld.com/blog/2008/12/26/pessimistic-reporting-optimistic-data/


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a4b0434',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Your ‘order’ is built on sand. Tomorrow the revolution will ‘rise up again, clashing its weapons,’ and to your horror it will proclaim with trumpets blazing: I was, I am, I shall be! The final written words of Polish revolutionary Rosa Luxemburg still resonate 100 years since her death. Murdered by right-wing paramilitaries on January 15, 1919, her fate in Berlin foreshadowed the brutality of the following two decades. The German Revolution she fought for was stamped out in the chaotic aftermath of World War I. But did Luxemburg’s legacy die with it?  Luxemburg’s colossal influence on the left is still celebrated today in Germany and around the world. Her conviction that democratic socialism could flourish was rooted in her meticulous analysis of how capitalism worked and she was convinced that brutality was an inevitable feature of capitalism. Through its need for new resources and territory, Luxemburg believed capitalism would end in collapse and misery.  As her final words suggest, she saw the uncertainty of her day as an opportunity to create a fairer world. With the rise of right-wing strongmen in Donald Trump and Jair Bolsonaro and the ongoing crisis of climate change, we should heed her words today. Luxemburg was born on March 5, 1871 in Zamość, a city in Russian-occupied Poland. Her parents knew from her earliest years that her congenital hip dysplasia would not stop her from pursuing a passion for justice.  She studied philosophy and economics and in 1898 was awarded a PhD in law from the University of Zurich in Switzerland, where she was also exiled and in hiding from the Russian police. Her thesis charted industrial development in Poland from the Napoleonic Wars to the latter years of the Russian Empire. Luxemburg wrote constantly, including passionate letters to close friends and fellow feminists Clara Zetkin and Luisa Kautsky. With their correspondence she fought the loneliness of her several imprisonments between 1904 and 1906 and during World War I.  She questioned everything – challenging Karl Marx, some of his theories and her male comrades who prevaricated on war, monarchy, bureaucracy and imperialism – all of which she vehemently opposed. Luxemburg preferred organising within the labour movement to party politics and she stood against the political class who voted for war in 1914, seeing it as a betrayal of internationalism and the common interest of workers around the world. What is Luxemburg’s contribution to our understanding of the world today? Threatened by environmental destruction, violence against women, gross inequality, insecure and exploitative work and the rise of the far-right, her diagnosis of global capitalism is perhaps more relevant than ever. 


      Read more:
      Beyond Rosa Luxemburg: five more women of the German revolution you need to know about


 For her, economic expansion and the resulting devastation of the environment was not a defect of global capitalism, but an inherent feature of a destructive system. In The Accumulation of Capital she explained that by definition, capital needed to conquer, absorb and destroy non-capitalist economies and territories to survive. This is evident in the recent decision of Brazil’s new far-right president, Bolsonaro, to “integrate the Amazon region into the Brazilian economy”. This would expand the authority and reach of powerful agribusiness corporations into the Amazon Rainforest – threatening the rights and livelihoods of indigenous people and the ecosystems their lives are entwined with. Luxemburg criticised Marx for not having paid enough attention to these external contradictions in economic growth. A socialist revolution was, for Luxemburg, the only way to stop the engulfing of non-capitalist life into capitalism.  She taught that war, colonialism and unsustainable extraction from nature are products of global capitalism. The result is the loss of irreplaceable natural wealth and people struggling for food, water and shelter in the developing world.  


      Read more:
      Capitalism is killing the world's wildlife populations, not 'humanity'


 Luxemburg also criticised economic growth based on financial speculation and profit making in global stock markets. She argued that such a model is prone to crisis, as the 2008 crash demonstrated, which creates unemployment and job precarity that cannot be easily solved. The economy loses the capacity to give employment to every adult with the capacity to work. Many of Luxemburg’s contemporaries, such as Eduard Bernstein, trusted that credit would alleviate capitalism’s tendencies towards crises. However, in Reform or Revolution Luxemburg argued that credit could only postpone and even intensify crisis.  Economic crises that result in this way allow the far right to stoke division within communities by turning widespread economic insecurity into a stick to beat refugees and immigrants with. So can we save Earth from the global expansion of capital and the fascism that emerges from it? The hundredth anniversary of Luxemburg’s assassination should make us reflect on her foresight. As Luxemburg herself saw it, the choice is “socialism or barbarism”."
"

Given his very bad temper, folks have been wondering when Al Gore and his environmental soulmates at the White House were going to get nasty with people who don’t share their view of global warming. Well, the time is now, and it looks like another Scorched Earther. 



Judging by Mr. Gore’s heated rhetoric lately, he sees people who disagree with him as demonic beings who’ll be doing the scorching. _U.S. News & World Report_ quotes him as saying, “I really can’t think of a clearer demonstration of the contrast between Democratic policies and Republican policies than what happened under Scar compared to what happened under Simba.” 



For the few of you who have not seen Disney’s “Lion King,” Scar is the evil leader who takes over the pride. A terrible drought ensues. The women are enslaved. The “Circle of Life” (Elton John’s catchy theme song) is destroyed. For Gore, those are Republican values. When the good Simba returns after a few years away (could this be analogous to Gore at Harvard?), the rains return and balance is restored. Democrats, you see, can change the climate. 



I note parenthetically that drought in central Africa is often related to El Niño, that Gore has been trying to blame El Niño on global warming (scientifically, the exact opposite may be true) and … , well, you get the way his mind works. His global warming world has always been a struggle between good and evil, between New Age and the free market. More than 10 years ago, he wrote this about global warming: ” ‘Evil’ and ‘Good’ are not terms used frequently by politicians [pleeze, Al!]. Yet I do not see how this problem can be solved without reference to spiritual values.” 



Al’s world view is enthusiastically shared by Dirk Forrister, a rock‐​hard Gore man who heads the White House Office of Global Climate Change. Recently he told a Washington, D.C., meeting of the prestigious Energy Institute that critics who disagree with the official view of global warming are “clowns.”  




Science is “frivolous,” to be dismissed quite casually when it turns out to be inconvenient. 



Half an hour later, Forrister blew up when confronted with the most recent scientific findings, which provide compelling and conclusive evidence that folks who have beaten the apocalyptic global warming drum for the last decade have been just plain wrong. 



In 1990 the United Nations Intergovernmental Panel on Climate Change (IPCC) offered a “best estimate” prediction of warming over the next century of 3.2 degrees Celsius. By 1995, thanks in part to incessant attacks by so‐​called skeptics, the warming estimate was lowered to 2.0°C. 



Three months ago Department of Commerce researcher Ed Dlugokenky published a paper in _Nature_ demonstrating that atmospheric methane — an important man‐​created greenhouse gas — is likely to show very little change in the next century. That forces the warming estimate down to about 1.75°C. Forrister called this observation “frivolous.” 



At the same time, Norwegian researcher Gunnar Myhre discovered that the **direct heating effect of carbon dioxide has been overestimated** , something the “skeptics” had maintained had to be true because the planet has warmed so little. His work was published in _Geophysical Research Letters_. That drops the warming estimate to 1.5°C. Forrister called this “frivolous.” 



A popular climate model from 10 years ago that served as much of the basis for the infamous U.N. Climate Treaty and the subsequent Kyoto Protocol (currently 0 for 95 in the Senate) said that, over the past decade, the globe should have warmed about 0.45°C. The observed temperature as measured at the surface, inflated by an urban (warm) bias, shows warming of just 0.11°C. Weather balloon thermistors and barometers, two independent instruments, showed **cooling** , as do the satellites, even after correction for recently discovered orbital drift. Forrister shouted “frivolous.” 



NASA scientist James Hansen has recently argued that the reason dramatic warming didn’t show up as he had forecast was because the soil and vegetation are taking up carbon dioxide at an increasing rate. That makes the planet greener, not browner (sorry, Carol!). Accounting for Hansen’s work published in the _Proceedings of the National Academy of Sciences_ , lowers 21st‐​century warming to about 1.25°C. Forrister called this “frivolous.” 



Tom Wigley of the National Science Foundation has just published a paper in _Geophysical Research Letters_ showing that if every nation met its commitments under the Kyoto Protocol, planetary cooling would be an undetectable 0.07°C by 2050, compared to what the temperature would be if we did nothing. My own research, recently published in _Climate Research_ , shows that the largest warmings occur in the coldest winter air masses rather than in the summer. “These are all frivolous arguments,” Forrister said. 



En coda, Forrister was asked if there would even be a Kyoto Protocol if the climate modelers had told us 10 years ago that it would only warm 1.0–1.5°C over the next century. After a long pause, he said (as best as I can recall), “I don’t know. Maybe yes, maybe no.” 



Having thus opened Pandora’s floodgates, Forrister was asked if the new findings might not make it appropriate for the Senate to pass a resolution forcing the president to withdraw the United States from the U.N. treaty, which allows such an option. “Frivolous!” he shouted. “You just can’t go making frivolous arguments like that!” 



Thus the new White House policy: those who do not agree with their (now thoroughly discredited) view of global warming are evil and will scorch the earth. Science is “frivolous,” to be dismissed quite casually when it turns out to be inconvenient. Stay tuned. Things can only get worse.
"
"Taking up almost the entire southern tip of India, Tamil Nadu is the country’s second-largest economy. Its delta region is considered to be the “rice bowl” of the state, also producing coconuts, bananas, nuts, spices and sugar cane. On 16 November 2018, Cyclone Gaja struck Tamil Nadu’s coastal areas, devastating local agriculture and infrastructure, and destroying thousands of homes.  Despite the region being prone to extreme weather and residents receiving some advance warning, locals reported that emergency responders only managed to reach many of the remote villages a week later. Perhaps few in the West are aware of the extent of the damage and distress this violent cyclone caused. When it comes to major storms and environmental disasters, the world’s media tend to focus more keenly when developed countries are affected. The devastation caused by extreme weather in Tamil Nadu remains a damning indictment on India’s ability to effectively address such emergencies, whether that be taking preventative measures or actually coping with the aftermath with a proper disaster management plan. Cyclone Gaja saw 45 deaths reported, crops destroyed and livestock killed. One farmer committed suicide after his small coconut plantation – his main source of income – was destroyed. People were traumatised in coastal districts that were unprepared for winds of speed of 160km/h – a category 2 tropical cyclone according to the Saffir-Simpson hurricane wind scale. Millions of trees were uprooted, agricultural land devastated, transportation blocked by debris, communications downed and there were power outages for eight weeks. Three months on and there is still only a limited power supply to some remote areas. It was this very same southern state which faced Cyclone Ockhi in 2017, Cyclone Vardah in 2016, man-made flooding in Chennai in 2015, and of course, the Boxing Day Indian Ocean tsunami in 2004.
Around 300 Tamil Nadu fishermen missing after Cyclone Ockhi have never been found. India is part of the Sendai Framework, an organisation that helps participating countries to adopt disaster risk reduction as a key goal in achieving a sustainable society. The country only released its first national disaster management plan in 2016, despite all of the states on the Bay of Bengal having a history of extreme weather events. Developing countries often faces barriers to creating a joined-up response between national, regional and local emergency plans. Tamil Nadu’s coastal areas are highly populated with people who depend on the sea for their living. Many live in small huts and makeshift houses that are easily destroyed. Often they are ill-informed about the hazards of living in places that are subject to volatile weather, but have nowhere else to go. On top of this, poor emergency planning and communications, insufficient coastal defence investment and failure to learn from previous cyclones have all led to a kind of paralysis in creating effective disaster response strategies. Cyclone Gaja stands as a warning to international disaster organisations; they must prepare the countries they work with more thoroughly for future weather disasters. They need to make clear that taking measures to limit the extent of disaster cannot be voluntary, but mandatory. It will take months to clear the debris and repair the infrastructure, and years to rehabilitate entire villages across Tamil Nadu. It is time to establish a proper framework that helps developing countries to facilitate an effective response to an emergency, crucially with the help of other, more developed nations.  It’s also time to think about alternative options such as natural coastal defences and wetland adaption, such as creating salt marshes and growing mangrove trees and sea grasses which can diffuse the energy of coastal flooding caused by storm surges or flash floods. It’s 14 years since the 2004 tsunami struck this part of the country where 10,000 people lost their lives, and there are still serious gaps in the region’s disaster response methods. Developed countries should understand the need for developing countries to be economically and technologically equipped for extreme events. For many, the issue is lack of funding for investment in coastal defences, but the Indian government needs to make this a key priority. Such extreme weather phenomena are likely to intensify as the effects of climate change – global warming and rising sea levels for example – escalate."
"
Guest post by Steven Goddard




The BBC ran an article this week  titled “Acid oceans  ‘need urgent action‘” based on the premise: 

The  world’s marine ecosystems risk being severely damaged by ocean acidification  unless there are dramatic cuts in CO2 emissions, warn  scientists.

This sounds very alarming, so being diligent  researchers we should of course check the facts.  The ocean currently has a pH  of 8.1, which is alkaline not acid.  In order to become acid, it  would have to drop below 7.0.  According to Wikipedia “Between 1751 and 1994 surface ocean pH is estimated to have decreased from  approximately 8.179 to 8.104.”  At that rate, it will take another 3,500  years for the ocean to become even slightly acid.  One also has to wonder how  they measured the pH of the ocean to 4 decimal places in 1751, since the idea of  pH wasn’t introduced until 1909.

The BBC article then asserts: 

The  researchers warn that ocean acidification, which they refer to as “the other CO2  problem”, could make most regions of the ocean inhospitable to coral reefs by  2050, if atmospheric CO2 levels continue to increase.

This does indeed sound alarming, until you consider that  corals became common in the oceans during the Ordovician Era – nearly 500 million years ago – when atmospheric CO2 levels were about 10X  greater than they are today. (One might also note in the graph below that there  was an ice age during the late Ordovician and early Silurian with CO2 levels 10X  higher than current levels, and the correlation between CO2 and temperature is  essentially nil throughout the Phanerozoic.)





http://ff.org/centers/csspp/library/co2weekly/2005-08-18/dioxide_files/image002.gif
Perhaps  corals are not so tough as they used to be?  In 1954, the US detonated the  world’s largest nuclear weapon at Bikini Island in the South Pacific.  The bomb  was equivalent to 30 billion pounds of TNT, vapourised three islands, and raised  water temperatures to 55,000 degrees.  Yet half a century of rising CO2 later,  the corals at  Bikini are thriving.  Another drop in pH of 0.075 will likely have less  impact on the corals than a thermonuclear blast.  The corals might even survive  a rise in ocean temperatures of half a degree, since they flourished at times  when the earth’s temperature was 10C higher than the present.

There seems to be no shortage of theories about how rising CO2 levels will  destroy the planet, yet the geological record shows that life flourished for  hundreds of millions of years with much higher CO2 levels and temperatures.   This is a primary reason why there are so many skeptics in the geological  community.  At some point the theorists will have to start paying attention to  empirical data.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e989122c8',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

On St. Patrick’s Day, we wear green and celebrate the culture of Ireland. I’ll be down at the pub tomorrow, but I’ll be toasting Ireland’s success at attracting greenbacks — all that investment flowing into the Emerald Isle and the resulting prosperity.



Ireland has boomed in recent years, and it now boasts the fourth highest gross domestic product per capita in the world. In the mid‐​1980s, Ireland was a backwater with an average income level 30 percent below that of the European Union. Today, Irish incomes are 40 percent above the EU average. 



Was this dramatic change the luck of the Irish? Not at all. It resulted from a series of hard‐​headed decisions that shifted Ireland from big government stagnation to free market growth. After years of high inflation, double‐​digit unemployment rates, and soaring government debt that topped 100 percent of GDP, Irish policymakers began to cut spending in the late 1980s in a desperate bid to recover financial stability.



Irish government spending fell from more than 50 percent of GDP in the 1980s to 34 percent by 2005. For Europe that is a triumph of restraint, given that the average size of government across 25 EU countries today is 47 percent of GDP. 



And Ireland has steadily reduced its tax rates. The top individual income tax rate was cut from 65 percent in 1985 to 42 percent today. The capital gains tax rate was cut from 40 to 20 percent in 1999. 



However, the key to Ireland’s success has been its excellent tax climate for business. In 1980, Ireland established a corporate tax rate for manufacturing of just 10 percent. That low rate was subsequently extended to high‐​technology, financial services, and other industries. More recently, Ireland established a flat 12.5 percent tax rate on all corporations — one of the lowest rates in the world, and just one‐​third of the U.S. rate.



Low business tax rates have helped Ireland attract huge inflows of foreign investment. Given the country’s modest size, it boosts a high‐​tech industry second to none. Intel, Dell, and Microsoft are among the island’s biggest exporters. Ireland also hosts booming insurance, banking, money management, and pharmaceutical industries.



The Irish model of rock‐​bottom business taxation has been hugely influential. In recent years, corporate tax rates have been slashed across Europe. According to KPMG, the average rate in the EU has fallen from 38 percent in 1996 to 26 percent in 2006. 



Inspired by the Celtic Tiger, many Eastern European nations have gone one step further and installed both low corporate taxes and simple, flat‐​rate taxes on individuals. According to my colleague Dan Mitchell, there are now 13 nations in the “flat tax club,” including Estonia, Russia, and Slovakia. 



The average corporate and individual rates in the flat tax nations are 19 and 18 percent, respectively, and these countries are growing strongly. Ireland and some of the newer “tiger” economies are putting to rest the notion that luck, natural resources, or other uncontrolled factors are the source of growth.



It’s become fashionable to argue that increased government spending on education is the key to success for countries like Ireland. I’m skeptical. For one thing, booming economies today can attract high‐​skill workers from global labor markets. In Ireland, brain drain has been replaced by brain gain as smart people from across Europe are drawn into the country’s growing industries.



Economic growth is spurred by attracting entrepreneurs and investment capital. Countries do that by establishing the rule of law, stable money, open borders, and low taxes. Let’s call these the “rainbow” factors, since Irish legend says that there is a pot of gold at the end of the rainbow.



Consider Hong Kong, which was once a barren outpost with seemingly few natural advantages. It followed the rainbow and found a pot of gold in just a few short decades.



You may recall that the Irish leprechaun is a sneaky character who tries to hide the pot of gold. Those are the politicians who spend their time trying to undermine the free market. Leprechauns, such as Venezuela’s Hugo Chavez, may be buoyed for short periods by high oil prices or other unique factors. But natural resources are usually pots of fool’s gold because of the bad governance they encourage.



The good news is that with the competition spurred by globalization, the leprechauns are on the defensive As more countries follow the path of the trailbrazing Irish, the relationships between the rainbow factors and growth become ever more clear. 



Now if only we could chase the leprechauns out of this country and cut our corporate tax rate, we’d be enjoying Irish‐​level growth rates by next St. Paddy’s Day.
"
"

Last December representatives of 160 nations gathered in Kyoto, Japan, to hammer out an agreement on emissions of greenhouse gases. But before the U.S. Senate ratifies the Kyoto treaty, it’s important to point out that much discussion of the “greenhouse effect” is little more than hot air. 



The Kyoto agreement rests on forecasts of future greenhouse gas emissions. But energy use, which requires the burning of fossil fuels, depends on economic growth and prosperity. Economists aren’t soothsayers: they often over‐ or underestimate growth, and they can’t be expected to discern how technological advances will change fossil fuel consumption.



Indeed, many climate models — which remain unreliable — predict that most of the climate change will occur many decades from now — the forecasted increase of 4.5 degrees Fahrenheit won’t happen for a century. It’s impossible to have any confidence in forecasts of the technology, population or energy sources of 2098. We can predict, however, that future generations will have better technology at their disposal, that they will be wealthier, and that they will live longer. They will certainly be in a better position to deal with adverse climate changes than we are today.



The Clinton administration had difficulty deciding what it could accept at Kyoto. Its quandary was magnified by the projected failure of the United States to reduce emissions to 1990 levels by the year 2000. Rather than cutting them, a booming economy appears likely to boost emissions of carbon dioxide by at least 15 percent in this decade. Cutting emissions enough to prevent climate change, which might require slashing emissions by some 60 percent, seems out of reach. Avoiding a warmer world would require a radical curbing of emissions by all countries, which in turn would lead to a worldwide slowdown in growth, perhaps even a depression that might make the 1930s look like Disneyland on a good day.



The Kyoto agreement is futile. Even Bert Bolin, the former chairman of the United Nations’ body of experts on global warming, says that the present plan would, if fully implemented, cut warming a quarter century from now “by less than 0.1 degree C, which would not be detectable.” We are plunging into a treaty that creates gigantic obligations without examining its costs and benefits. Congress has demanded that the Clinton administration provide estimates of the costs, but none have been forthcoming.



For most of the world, warming over the next century would cost only a little or would be an actual benefit. The few regions that actually would be harmed by warming should have help.



There is no need to rush into a treaty that would have little benefit but great cost. If climate change becomes a real problem, many steps can be taken that wouldn’t cripple our economy. Ocean scientists have shown, for example, that if the seas were “fertilized” with iron filings, phytoplankton (algae) would bloom and absorb vast quantities of carbon dioxide. The minuscule plants are nutritionally starved for iron and, when provided with that metal, multiply rapidly, absorbing large amounts of carbon. Some experts estimate that iron supplements might offset 15 to 20 percent of man‐​made carbon dioxide over the next few decades.



In addition, harvesting and replanting timber could sequester much carbon. Forest researchers have concluded that an active program of cropping and replanting fast‐​growing forests, then turning the lumber into housing and other long‐​term products, together with reforestation, could offset 12 to 15 percent of human greenhouse gas emissions. Seeding the ocean with iron and properly managing forests could by themselves do as much to slow climate change as capping greenhouse gas emissions at 1990 levels. Furthermore, scientists may develop other strategies in the future that don’t require shrinking our economy. 



The administration is under tremendous pressure to act immediately. To retain credibility with environmentalists, politicians, other countries and the mass media, it must take steps to reduce carbon dioxide emissions even if the limitations would have no benefit and would potentially impose high costs. To succeed in this high‐​wire act, President Clinton will probably propose new regulatory steps and mandates, such as higher fuel efficiency standards for new cars, more stringent restrictions on appliances, strict insulation levels for new buildings and more spending on mass transportation. Most of those regulations would be phased in slowly — that is, after President Clinton leaves the White House and many current members of Congress retire. The actual legislation required to meet the goal might even await a future Congress. Whatever difficulties present themselves, the administration will negotiate a formula that will allow it to claim that the entire world is cutting greenhouse emissions.



In short, this unnecessary measure would devastate our economy. For most of the world, warming over the next century would cost only a little or would be an actual benefit. The few regions that actually would be harmed by warming should have help. Delaying action by 20 to 30 years is the only prudent, “no regrets” policy. Technology will advance. Incomes in Third World countries will grow. The world will be more capable of coping with change. Except for measures that make sense with or without global warming — like ending subsidies for energy and energy use — Congress should resist any attempts to limit greenhouse gas emissions.
"
"
Share this...FacebookTwitterA mountebank fleecing incredulous gamblers (Hieronymus Bosch - Wikipedia)
How many times has the demise of the planet or humanity been predicted? How many charlatans have passed through the revolving doors of history? You’d need the resources of the Census Bureau to tally that up.
A couple of days ago I presented yet another such prediction from a scientist at the Potsdam Institute For Climate Impact Research (PIK).
The planet is going to hell in a hand-basket they keep telling us.  But when you boil it down, they all have one thing in common: They’ve been wrong every time. 
The charlatans of the past have been replaced by a new generation of charlatans – government-funded climate scientists who are paid to get the masses to stampede in panic into the arms of government programs, all aided and abetted by the chatterbox media.
It turns out that life as a whole on the planet has never been better.
One media outlet steps up
In Germany we’re finding out that not all the media accept what is written down on the press releases. There are a few journalists who actually do good research and the hard work of digging. German news magazine FOCUS has an uplifting and positive article about how things, by almost every measure, are far better today than anyone expected.
 The FOCUS report written by veteran journalist Michael Miersch takes a look at some of the charlatans of the past and a bit into the psychology of why end-of-the-world scenarios find such mass appeal among the gullible, who later wonder what happened when things turn out differently.
This is not to say all the world’s problems have been solved. But those problems are under much better control today then they were say 50, 100 or 200 years ago. Here are just a few of the observations made by FOCUS.
Human prosperity
The world’s population has grown 6-fold since 1800, and at the same time life expectancy has doubled. Between 1955 and 2005 inflation-adjusted average personal income has tripled for the average person on the globe.”
Many of the poor indeed have gotten much richer.
Agriculture
How often do we hear about the threats of industrial agriculture devouring land to feed the exploding masses of people? Guess what? Modern agriculture protects wildlife and forests. FOCUS:
With the crop yields of 1961, farmers would have needed 32 million square km of cropland to have fed the 6 billion persons on the globe in 2000. Instead they have been able to harvest the necessary amount of crops on just 15 million acres. That means an area almost the size of South America was spared the plow. Forests and savannahs were thus saved.
All thanks to modern agricultural technology, which today continues to develop nicely. Yet, today, many greens are busily bemoaning the very agriculture that has rescued millions of sq km of forests and wildlife from primitive manaul agricultural practices. Worse, they also want us to fuel our cars with bio-diesel, which would require the extra deforestation of millions of sq km.
And so how do today’s results compare to the projections made by “intellectuals” like Paul Ehrlich? Clearly he has earned a top spot in that elite Club of Rome Crackpots, along with Al Gore, James Hansen and others. And another thing:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Inflation-adjusted prices for food have fallen 75% since 1950.
In fact, food has become so bountiful that bureaucrats are now whining about the social problems of obesity.
Today there are two things that could reverse the tremendous progress that agriculture has made – the Green Movement (think bio-diesel and organic farming) and global cooling. Guess which side (skeptic or alarmist) wants to see both?
Good news are never welcome by alarmist malcontents, who are psychologically sustained by daily doses of misery, pessimism and promises of catastrophe. This keeps Institutes like the PIK running.
On the other hand, optimists who maintain a positive, stay-the-course outlook for the future are viewed by the malcontents as simple-minded, naive and ignorant. Funny how the optimists always turn out to be right.
Miersch writes:
Completely contrary to what we hear day in and day out from the newspapers and TV, whether it’s war, hunger, illiteracy, political suppression, or environmental pollution, all of the world’s evils are shrinking.
That’s especially obvious with air pollution, which has reduced to such an extent in western countries that control freaks and tree huggers are now forced to call the non-pollutant CO2 a pollutant just to have something to do.
On poverty, peace and prosperity
First let’s recall, that all of Eastern Europe and much of South America were governed by dictators less than 50 years ago. Thankfully, they’ve fallen and things have gotten much better with free markets in control. FOCUS writes:
The United nations has determined that poverty receded more in the second half of the 20th century than in the 500 years before it.
Miersch then drives it home, using Germany as an example:
Germans who are now retiring belong to the first generation that has gotten to know peace, freedom, and prosperity as permanent fixtures in their lives. This has never happened in their history.
Isn’t apocalypse tough? But looking at the German media today, you’d think times couldn’t be worse – the planet is threatened by an imminent climate catastrophe that is “hidden in fog – but it’s there!”  
The future
So what lies in the future now that we have seen that every apocalyptic warning heard earlier in history has ended up being just fly-crap in the wind? The business of apocalypse is a big industry and involves lots of money – so don’t expect the end-of-world-charlatans to go away. There’s more money in it today than ever.
Being wrong every time isn’t going to deter today’s modern charlatans. They have a whole new line-up of catastrophes in their bag of tricks: climate change, biodiversity, ocean acidification, species extinction, to name some. And, there are plenty of malcontents out there who want to hear more, more, more.
But I suspect, like the earlier scares of the past, we’ll soon be able to put those on the list of seriously endangered species as well. Here today, extinct tomorrow.
Share this...FacebookTwitter "
"Mining giant Glencore has predicted its carbon footprint will shrink by almost a third by 2035, but will not set climate targets for the company. The company expects its total carbon emissions to fall by 30% in the next 15 years as it gradually produces less coal due to the “natural depletion” of its coal reserves. Glencore promised investors last year it would cap its coal production, and said on Tuesday that production was expected to fall as its Colombian mines close. It may produce less coal from mines in Australia and South Africa, too. The Switzerland-based miner has also vowed to disclose its contribution to the climate crisis and show investors how it plans to align its business with the Paris climate goals from April this year. Glencore has yet to set a target to reduce its full climate impact because it does not include the so-called scope 3 emissions, which would take account of the emissions produced from using the coal it sells. Ivan Glasenberg, the chief executive of Glencore, described climate targets like BP’s aim to be a “net zero” company by 2050 as “wishy-washy” because the goals are “a long way out”. “As we rebalance our portfolio towards commodities supporting the transition to a low-carbon economy, we expect the intensity of our scope 3 emissions to decrease,” a statement from Glencore said. “Starting in 2020, we will start disclosing our longer-term projections for the intensity reduction of scope 3 emissions, including mitigation efforts.” The mining company’s existing targets cover its direct greenhouse gas emissions, and the emissions produced from its operations. Glencore has cut these emissions by 10% between 2016 and 2020 after promising to reduce them by 5%. The company pledged to set out new longer-term targets that “support the Paris goals” later this year. Glencore is the first big mining company to spell out how emissions from the coalmining sector are likely to fall over the next decade as big economies begin to choose gas over coal-fired electricity, and miners shift their attention to commodities that can help to support a low-carbon future. Glencore said it weighted its spending last year towards “energy transition materials” such as copper, cobalt and nickel, which are used to make batteries and electric vehicles. Demand for these materials is expected to grow rapidly as global economies use more renewable energy to power homes and cars. BloombergNEF estimates that coal’s role in the global power mix will fall from 37% today to 12% by 2050. Glasenberg said the company was also “spending a lot of money on carbon capture” technology that could help to reduce the climate impact of coal by trapping the emissions before they can contribute to global heating."
nan
"
Share this...FacebookTwitterLast week it was reported almost everywhere by the German media that North Atlantic currents were the warmest in 2000 years and melting the Arctic at an unprecedented rate.
Labrador and North Atlantic currents (Chart from US Coast Guard)
These media reports were based on a study published in Science. The authors write in the abstract (emphasis added):
Here, we present a multidecadal-scale record of ocean temperature variations during the past 2000 years, derived from marine sediments off Western Svalbard (79°N). We find that early–21st-century temperatures of Atlantic Water entering the Arctic Ocean are unprecedented over the past 2000 years and are presumably linked to the Arctic amplification of global warming.”
I’m not sure what they mean by “early 21st century”. Perhaps the years 2000 to 2007? The study says the waters are about 2°C warmer. Here it has to be noted that they are presuming, i.e. postulating. The CO2 link here is a bit of wild speculation.
Here’s how Der Spiegel puts it in a report titled Atlantic Current Is Heating Up The Arctic:
The experts suspect that the accelerated reduction in sea ice and the measured warming of ocean and atmosphere in the Arctic over the last decades, among other factors, was the result of an enhanced transfer of warmth from the Atlantic. The Fram Straits is even about 1.4° C warmer than during the Medieval Climate Optimum, a time when temperatures in Europe were significantly increased.”
And Der Spiegel writes:
‘Cold sea water is decisive in the formation of sea ice, which in turn cools because it reflects sunlight,’ says Thomas Marchitto of the University of Colorado in Boulder. The melting is accelerating by itself.”
Media reports like the one in Der Spiegel of course emphasized the supposed vicious circle of the melting Arctic ice dynamic: more melting leads to more warming, which then accelerates the process – all unleashed of course by man-made CO2.
If anything they are, perhaps unwittingly, admitting that the Arctic sea ice reduction of the 2000s can be traced back to ocean currents.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




And as things stand right now, just the opposite is occurring. The Arctic is NOT melting, it’s freezing up again – recovering from its low in 2007, and quite impressively.
The Global Rumblings website here reports that 70 trillion cubic feet of ice have been added to the Arctic core since January 2009. That translates to 2000 cubic km – enough to cover Manhattan with 20 miles of ice (or 32,000 Manhattans with 1 meter of ice).
The US Navy PIPS 2.0 graphic shows ice thickness. The following comparator shows how it’s the Arctic that has gone green.

Source: Global Rumblings
Some will say that PIPS is not a reliable indicator of Arctic ice thickness, and so cannot be used reliably. But you can put that rumour to rest, see PIPS WUWT.
So why is the Arctic thickening and regrowing, and no longer melting at an unprecedented rate as claimed by the media?
This could have to do with the Labrador Current, which flows southward between Greenland and Labrador. Reports say it is slowing down. That means cold water is not getting transported out of the Arctic. A Der Spiegel article just 2 weeks ago titled “Feared Atlantic current is now weakening” suggests that this current is at its weakest level in 1800 years. What is it caused by? According to scientists, Der Spiegel says:
As a cause for the change, scientists suspect climate change. The coincidence that this has happened during the warming of the last decades allows this to be the conclusion, they believe. But the knowledge about ocean currents still has many holes says Wallace Broecker of Columbia University in USA – a pioneer in ocean research.”
Changes in the atmosphere controls the ocean currents? Right. And as usual, they’re sure – yet admit there are many holes in the knowledge and so they are not sure.
Meanwhile, the ice keeps growing.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterJapan has decided to back out of carbon trading, likely from intense pressure from industry. This will no doubt disappoint many in organized crime, read here and here.
According to the online Environmental Leader:
A September survey of Japan’s largest business lobby group, found that 95% of companies surveyed oppose carbon trading, citing competition from countries like India and China that are not bound by similar pollution limits, reported Bloomberg.”
Japan’s government also said it won’t support an extension of  the Kyoto Protocol after its greenhouse-gas emissions targets expire in 2012, calling the treaty “outdated” because it only regulates 27 percent of global emissions, and doesn’t include the U.S. and China, reported Bloomberg.”
Meanwhile the Financial Times Deutschland here reports the same in a piece called: CO2 Trade No Longer In Vogue. The FTD blamed pressure from industry for leading to Japan’s decision to back out of the scheme, and added:
The decision is a major setback for the once praised system that was touted as a way to achieve environmental protection using  market economy means.”
Turns out that the Japanese have determined that it’s not very market economy-friendly after all.
The USA, under its newly elected Congress, have also signalled it will reject any trading scheme. Japan meanwhile is planning hefty taxes on carbon fuels and is considering energy feed-in traiffs like those used in Germany for supporting renewable energy.
Share this...FacebookTwitter "
"Is your morning coffee an espresso or a skinny latte? Is it from a darkly roasted French or Italian blend? If it’s a high quality brew, it’s almost certainly made with beans from the Arabica species (Coffea arabica), which is known for its finer flavours. Examples would be Javan coffees, Ethiopian sidamo, and the expensive Jamaican blue mountain. If you’ve stirred together an instant blend, it’s probably from a different species, Robusta (Coffea canephora), known for its harsher taste. But there are more than 100 species of coffee in the wild. All produce similar beans that you could make a recognisable coffee drink from. Robusta is sometimes openly mixed with Arabica in commercial products – and is often secretly used to adulterate “100% Arabica” products, too. A third species, Coffea liberica, native to west and central Africa, is widely grown for local use in tropical countries, but is not globally traded because of its more bitter taste.   A fourth species Coffea eugenoides bred with Robusta to give rise to Arabica, a crossbreed. Another 38 closely related species are known or assumed to have fertile pollen transfer with commercial coffees. There are a further 82 species which are more distantly related to the commercial breeds, but scientists could interbreed them with commercial coffees in a lab. All these coffee relatives can help enhance the genetic diversity of commercial coffee species, making them more adaptable to changes in their environment. Climate change is threatening global coffee yields as changing temperatures and rainfall patterns affect plant growth. The changing climate may also be leaving plants more vulnerable to disease. All major commercial coffee growing countries have been badly affected by the fungal disease “coffee leaf rust”, which spread across Africa and into Asia during the early 20th century, then to South America, becoming entrenched globally by the turn of the millennium. The Central American coffee rust outbreak that began in the 2011-2012 harvest season affected 70% of farms in the region, resulting in over 1.7m lost jobs and US$3.2 billion in damage and lost income. 


      Read more:
      Climate change is causing havoc for global coffee yields


 Robusta varieties used for the instant blends have been key to developing resistance to coffee leaf rust in Arabica varieties through cross breeding. As climate change and disease risks escalate, wild coffee species offer a crucial resource for maintaining the world’s coffee supply. Arabica has tightly limited geographic ranges in which it grows well and Robusta, while resistant to leaf rust, is vulnerable to other diseases. A recent study led by the UK’s Kew Royal Botanic Gardens set the value of this variety in context: over 60% of coffee species are threatened with extinction. The authors explained that wild relatives of coffee are already used as local substitutes for globally traded commercial crops. They offer different climatic tolerance ranges and disease resistance traits that can help ensure global coffee production continues to meet demand.  But coffee species are particularly vulnerable to extinction because they occur in a small numbers of small geographic ranges – such as pockets of wild Arabica populations between certain altitude ranges in the Ethiopian highlands. Wild coffee species – and wild varieties of the commercial species – are almost all in decline due to competition for land use and overharvesting of the coffee plant for timber or firewood. A number of wild coffee relatives haven’t been spotted for many decades and may be extinct.  One species, the cafe marron, from the remote island of Rodrigues in the Indian Ocean, was known from only one sighting in 1877. A century later, a schoolboy drew an “unusual” tree while exploring and showed it to a teacher. They recognised it as a surviving cafe marron. The sole surviving specimen of that wild coffee has inspired wider forest conservation on Rodrigues. It is also being cultured in lab collections at Kew. Sadly, there may be less hope for other species. Coffee seeds don’t store well, unlike wild relatives of other crops such as wheat or maize. So we can’t rely on storage in seed banks to conserve coffee diversity and resilience. Freezing plant matter in labs or growing samples in test tubes might be an alternative, but not one that has been explored beyond existing commercial strains. Preserving different coffee varieties in botanic gardens isn’t really viable for protecting genetic diversity either. Coffee species readily fertilise each other, “contaminating” the resource you’re trying to conserve. While some experts suggest we preserve coffee diversity in collections, the Kew Gardens study argues that the sustainability of coffee depends on conservation of these species where they grow, in protected areas and working with communities throughout their native distribution in Africa and Asia. Conserving genetic diversity should be included in existing approaches for sustainable coffee production, such as Fair Trade and Rainforest Alliance certifications. Ensuring the continuity of the coffee trade means protecting the ecosystems coffee comes from and the livelihoods of people across the bean to coffee cup economy. We can also expect new flavours and even coffees with naturally low or zero caffeine content. Naturally caffeine-free Indian Ocean island cafe marron anyone?"
"

“No matter how much you pay with a carbon levy, virtually nothing is received climatically… No matter the level of domestic action that we take, it will pale in comparison to the rapid expansion of carbon dioxide emissions in other parts of the world.”



How much global warming will result from U.S. emissions over the course of this century, and how much of that could be prevented by a carbon tax? These two questions have the same simple answer— _virtually none_. One or two tenths of a degree a century out with–and without–a carbon tax makes the whole climate debate a peculiar exercise.



The Intergovernmental Panel on Climate Change (IPCC) estimates that the earth’s average temperature will increase somewhere between 1.1°C and 6.4°C over the 21st century, depending on the assumed pathway of anthropogenic emissions (both greenhouse gases and aerosols) and the actual (but unknown) climate sensitivity.



A temperature rise towards the low end of this range is not worth worrying too much about (the ‘lukewarming’ position), while a rise near the higher end of the range is potentially much more problematic (the alarmist position). And while lukewarmers and alarmists stray apart when it comes to the amount of climate change they are expecting, _they are bound together by the fact that there is practically nothing that can be done to change the situation, either way_. Why? They use the same math.



But you won’t hear many alarmists admitting to that fact—if they did, you would never have heard of the terms like “cap‐​and‐​trade” or “carbon tax.” Instead, you’d be much more familiar with words like “planning” and “adaptation.”



 **How Much U.S.-Side Global Warming?**



Lest alarmists protest, let’s work through the numbers to see just how much “global warming” is being caused by U.S. economic activity.



In other words, how much of the IPCC’s projected 1.1°C to 6.4°C of warming will the U.S. be responsible for in the next century? The answer is about 0.08°C of the low end estimate and about 0.35°C of the high end estimate (according to an IPCC‐​like analysis*). Using the IPCC’s mid‐​range scenario, carbon dioxide emissions from the U.S. contribute about 0.19°C of the total 2.96°C global temperature rise.



Yep, that is it. For all the incessant talk as to how the highly consumptive U.S. lifestyle—from SUVs, to air conditioners, to big screen TVs and huge portion sizes—is leading climate catastrophe, the sum total of our contribution to “global warming” this century will amount to the neighborhood of _about 0.2°C_. Not five degrees. Not two degrees. But about _two‐​tenths of a degree Celsius_. And even this number may be on the high side if the climate sensitivity is lower than about 3°C (see here for more on recent findings concerning the climate sensitivity).



So all the U.S. carbon dioxide emissions restriction tactics—EPA regulations, cap and trade schemes, carbon taxes, efficiency programs, guilt‐​inducing ad campaigns, etc.—are aimed at chipping away at this already tiny 0.2°C. Big deal.



When considering any of these options, you have to ask yourself (or your representatives in Congress) how much are you willing to pay—in dollars or inconvenience, or both—to avert some portion of this 0.2°C of global temperature increase and its accompanying inconsequential and impossible to measure climate change?



 **Avertable Climate Change**



No matter how much you pay with a carbon levy, virtually nothing is received climatically.



Consider the effect of the Waxman‐​Markey Climate Bill that was passed by the U.S. House of Representatives back in the summer of 2009. That cap‐​and‐​trade scheme was designed to step down U.S. carbon dioxide emissions ultimately by 83% by the year 2050. This would have taken a monumental effort that was sure to be disruptive in any number of ways.



The net climate result? Instead of 0.19°C of warming coming from the U.S. by the year 2100 (assuming the IPCC mid‐​range scenario), our contribution would have been reduced to 0.08°C—for a net “savings” of about 0.11°C of “global warming”. (See my analysis here.) This amount is of virtually no environmental consequence and was repeatedly cited as one of the reasons that this legislation died in the Senate.



Much the same holds true for the present day fad for a carbon tax. The talk of a carbon tax—or more rightly a carbon _dioxide_ tax—was bolstered recently by superstorm Sandy and its aftermath (widely, but wrongly, blamed on anthropogenic climate change). A tax on carbon dioxide emissions would be felt from the gas station to the grocery store and everywhere in between as virtually every aspect of our modern life benefits from cheap carbon dioxide emitting, fossil‐​fuel produced energy.



A carbon tax has become so trendy that even “no new tax pledge” champion Grover Norquist briefly flirted with it before quickly reconsidering.



And for good reason. For about the only thing that a carbon (dioxide) tax in the U.S. will _not_ do, is produce a detectable mitigation of anthropogenic global warming and any associated effects.



The U.S. Energy Information Agency recently projected the impacts on carbon dioxide emissions in the U.S. out to the year 2035 resulting from a carbon dioxide tax of $15/​per ton emitted (beginning in 2013 and increasing by 5% per year out to 2035) and for a tax of $25/​ton of CO2 (beginning in 2013 and increasing by 5% per year out to 2035). The EIA projections are shown in Figure 1. I have continued the same emissions reductions pathway out from 2035 until the year 2100—admittedly, this is a shot in the dark, but at least is gives us something to work with.









Figure 1. Energy Information Agency estimates for the future course of U.S. carbon dioxide emissions resulting from a $15/​ton tax on carbon dioxide emissions (solid blue line) and a $25/​ton tax on carbon dioxide emissions (solid red line), 2010–2035. I have extended these projections to the year 2100 (dotted lines).



When I substitute these carbon tax pathways for U.S. carbon dioxide emissions for the one already included in the IPCC mid‐​range scenario, I calculate that the amount of “global warming” contributed by the U.S. drops from 0.19°C by the year 2100 to 0.13°C and 0.08°C for the $15/​ton and $25/​ton carbon tax respectively (Figure 2).









Figure 2. Amount of total global warming (red bars) and the U.S. contribution to the total global warming (blue bars) over the 21stcentury under three different scenarios. BAU= business‐​as‐​usual as portrayed by the IPCC A1B mid‐​range emissions scenario; $15/​ton CO2=$15/ton tax on U.S. carbon dioxide emissions as prescribed by the EIA to the year 2035 and extended to 2100; $15/​ton CO2=$15/ton tax on U.S. carbon dioxide emissions as prescribed by the EIA to the year 2035 and extended to 2100.



A global warming “savings” of 0.06°C to 0.11°C across this century is of no scientific consequence, while a tax of carbon dioxide emissions of $15 to $25 per ton is sure to be of significant personal consequence (and, my guess, only very temporary, i.e., until the next election cycle).



 **Conclusion**



Any tax on carbon dioxide is clearly a case of not getting what you pay for. You will pay a lot, and receive nothing in return, or at least nothing that you will ever realize, or that could be proven.



What is working against any form of a carbon tax is that the U.S. plays only a minor role in the future course of global warming driven by anthropogenic activities. The rest of the world—primarily the developing countries like China and India—is where the rubber meets the road for climate change. No matter the level of domestic action that we take, it will pale in comparison to the rapid expansion of carbon dioxide emissions in other parts of the world.



Instead of trying to make an expensive repair a very small and inconsequential leak, I would think that our attention ought to be directed at determining just how big the coming flood may be, and make our plans accordingly.



 **Appendix: Methodological Note**



I have used the Model for the Assessment of Greenhouse‐​gas Induced Climate Change (MAGICC) for my analysis of the effect of U.S. emissions on projected global temperature rise. MAGICC is sort of a climate model simulator that you can run from your desktop (available here). It was developed by scientists at the U.S. National Center for Atmospheric Research.



There are many parameters that can be altered when running MAGICC, including the climate sensitivity (how much warming the model produces from a doubling of CO2 concentration) and the size of the effect produced by aerosols. In all cases, I’ve chosen to use the MAGICC default settings, which represent the middle‐​of‐​the‐​road estimates for these parameter values (e.g., climate sensitivity equals 3.0°C).



I’ve had to make some assumptions about the U.S. emissions pathways as prescribed by the original IPCC scenarios in order to obtain the baseline U.S. emissions (unique to each scenario) to which I could apply the various emissions reduction schedules. The most common IPCC definition of its scenarios describes the future emissions, not from individual countries, but from country groupings. Therefore, I needed to back out the U.S. emissions.



To do so, I identified which country group the U.S. belonged to (the OECD90 group) and then determined the current percentage of the total group emissions that are being contributed by the United States—which turned out to be about 50%. I then assumed that this percentage remained constant over time. In other words, that the U.S. contributed 50% of the OECD90 emissions in 2000 as well as in every year between 2000 and 2100.



Thus, I am able to develop the future emissions pathway of the U.S. from the group pathway defined by the IPCC for each scenario (in this case, the B1, the A1B and the A1FI scenarios). The Waxman‐​Markey and carbon tax reductions were then applied to the projected U.S. emissions pathways, and the new U.S. emissions were then recombined into the OECD90 pathway and into the global emissions total over time.



It is the total global emissions that are entered into MAGICC in order to produce global temperature projections. My results are largely insensitive to minor changes in these assumptions.
"
"While Europe was in the early days of the Renaissance, there were empires in the Americas sustaining more than 60m people. But the first European contact in 1492 brought diseases to the Americas which devastated the native population and the resultant collapse of farming in the Americas was so significant that it may have even cooled the global climate.  The number of people living in North, Central and South America when Columbus arrived is a question that researchers have been trying to answer for decades. Unlike in Europe and China, no records on the size of indigenous societies in the Americas before 1492 are preserved. To reconstruct population numbers, researchers rely on the first accounts from European eyewitnesses and, in records from after colonial rule was established, tribute payments known as “encomiendas”. This taxation system was only established after European epidemics had ravaged the Americas, so it tells us nothing about the size of pre-colonial populations. Early accounts by European colonists are likely to have overestimated settlement sizes and population to advertise the riches of their newly discovered lands to their feudal sponsors in Europe. But by rejecting these claims and focusing on colonial records instead, extremely low population estimates were published in the early 20th century which counted the population after disease had ravaged it. On the other hand, liberal assumptions on, for example, the proportion of the indigenous population that was required to pay tributes or the rates at which people had died led to extraordinarily high estimates. Our new study clarifies the size of pre-Columbian populations and their impact on their environment. By combining all published estimates from populations throughout the Americas, we find a probable indigenous population of 60m in 1492. For comparison, Europe’s population at the time was 70-88m spread over less than half the area. The large pre-Columbian population sustained itself through farming – there is extensive archaeological evidence for slash-and-burn agriculture, terraced fields, large earthen mounds and home gardens.  By knowing how much agricultural land is required to sustain one person, population numbers can be translated from the area known to be under human land use. We found that 62m hectares of land, or about 10% of the landmass of the Americas, had been farmed or under another human use when Columbus arrived. For comparison, in Europe 23% and in China 20% of land had been used by humans at the time. This changed in the decades after Europeans first set foot on the island of Hispaniola in 1492 – now Haiti and the Dominican Republic – and the mainland in 1517. Europeans brought measles, smallpox, influenza and the bubonic plague across the Atlantic, with devastating consequences for the indigenous populations. Our new data-driven best estimate is a death toll of 56m by the beginning of the 1600s – 90% of the pre-Columbian indigenous population and around 10% of the global population at the time. This makes the “Great Dying” the largest human mortality event in proportion to the global population, putting it second in absolute terms only to World War II, in which 80m people died – 3% of the world’s population at the time. A figure of 90% mortality in post-contact America is extraordinary and exceeds similar epidemics, including the Black Death in Europe – which resulted in a 30% population loss in Europe. One explanation is that multiple waves of epidemics hit indigenous immune systems that had evolved in isolation from Eurasian and African populations for 13,000 years. Native Americas at that time had never been in contact with the pathogens the colonists brought, creating so-called “virgin soil” epidemics. People who didn’t die from smallpox, died from the following wave of influenza. Those who survived that succumbed to measles. Warfare, famine and colonial atrocities did the rest in the Great Dying. This human tragedy meant that there was simply not enough workers left to manage the fields and forests. Without human intervention, previously managed landscapes returned to their natural states, thereby absorbing carbon from the atmosphere. The extent of this regrowth of the natural habitat was so vast that it removed enough CO₂ to cool the planet. The lower temperatures prompted feedbacks in the carbon cycle which eliminated even more CO₂ from the atmosphere – such as less CO₂ being released from the soil. This explains the drop in CO₂ at 1610 seen in Antarctic ice cores, solving an enigma of why the whole planet cooled briefly in the 1600s. During this period, severe winters and cold summers caused famines and rebellions from Europe to Japan.  The modern world began with a catastrophe of near-unimaginable proportions. Yet it is the first time the Americas were linked to the rest of the world, marking the beginning of a new era. We now know more about the scale of pre-European American populations and the Great Dying that erased so many of them. Human actions at that time caused a drop in atmospheric CO₂ that cooled the planet long before human civilisation was concerned with the idea of climate change.  Such a dramatic event would not contribute much to easing the rate of modern global warming, however. The unprecedented reforestation event in the Americas led to a reduction of 5 parts per million CO₂ from the atmosphere – only about three years’ worth of fossil fuel emissions today."
"When Luxembourg announced recently that all public transport in the country will be free from next year, this radical move was received with astonishment. After all, most nations would surely shy away from putting such strain on public finances and from antagonising those taxpayers who don’t use public transport.  But supporting public transport is almost always good for the environment. So, if the finances add up, does this mean that the case for free public transport is a no-brainer? Economists like me view subsidies (or taxes) on specific goods as ways to better align people’s decisions with what is best for society as a whole. The key question is whether free public transport is a good way of achieving this. When thinking about whether to buy any item such as a book or an apple, we usually compare how much we enjoy using this item with what we must pay for it. In most cases, if the item is supplied within a competitive market, the price that we pay for something largely reflects society’s cost of producing it, such as the use of natural resources or labour.  This is not the case for driving a car, however. In addition to our own private costs for petrol and wear and tear, every car ride imposes costs on other people by polluting the air and congesting the roads. Few of us would want to fully account for these social costs when deciding whether to use the car to do the school run or the groceries. Therefore, people will often find that the benefit of another car ride exceeds the private cost, even when social costs – that pollution and congestion – exceed any social benefit. In other words, people will use their cars too much from society’s point of view. The same reasoning applies for a person’s choice between private and public transport. If I think about whether to take the car to get to work, I will compare the benefits and costs to me with the next best alternative, which may be to take the bus or train.  But my use of public transport affects other people much less than if I travelled by car: per user, public transport causes much less additional road congestion and air pollution than a car. Yes, if too many people take the bus it may get overcrowded, but once a specific service is consistently over capacity, the bus operator can add more services. But as most people base their decisions on their own cost on benefits rather than those they impose on other people, the decision between public and private transport will typically be biased against public transport. The economic idea of subsidising public transport is to level the playing field between these options. If the subsidy is equal to the difference in other people’s cost of me driving the car versus taking the bus, my decision on the mode of transport will be aligned with society’s best interest. So, are the environmentalists right after all? Let’s have a look at Luxembourg. Public transport in the small, wealthy country is already dirt cheap – a two-hour ticket with unlimited journeys is just €2 – but road congestion is still among the worst worldwide. It seems Luxembourgish commuters are still choosing to spend hours on a congested road, even though they could easily afford the train.  Partly this is because, in general, individual traffic is more convenient than public transport, as car drivers can travel independently of timetables, train lines or bus routes. Therefore, a denser network or more frequent timetable may be a more effective way of getting people out of their cars than an even higher subsidy. Furthermore, when cheap public transport induces commuters to leave their cars at home, roads get less congested. However, this may make driving into the city more attractive for people who otherwise would have stayed at home, or more people may choose to live on the outskirts rather than in the city centre if commuting gets more convenient or cheaper. This demonstrates a fundamental dilemma of transport policy: as soon as traffic problems are relieved, even more people will want to travel. Therefore, those who are sceptical of entirely free public transport do have a point. An alternative way of levelling the playing field between car driving and public transport without inducing even more people to travel is to increase the petrol tax. Indeed, petrol prices in Luxembourg are markedly lower than in neighbouring Germany, Belgium and France, which may well contribute to Luxembourgers’ reliance on cars. In times of ever more alarming news about global warming, every car that won’t be driven as a result of free public transport is an achievement. However, an optimal policy needs to carefully balance subsidies for public transport use with petrol taxes and investments in the public transport network. 


      Read more:
      Luxembourg's free public transport sounds great, but it won't help people get from A to B


"
"The flow of the Colorado River is dwindling due to the impacts of global heating, risking “severe water shortages” for the millions of people who rely upon one of America’s most storied waterways, researchers have found. Increasing periods of drought and rising temperatures have been shrinking the flow of the Colorado in recent years and scientists have now developed a model to better understand how the climate crisis is fundamentally changing the 1,450-mile waterway.  The loss of snow in the Colorado River basin due to human-induced global heating has resulted in the river absorbing more of sun’s energy, thereby increasing the amount of water lost in evaporation, the US Geological Survey scientists found. This is because snow and ice reflect sunlight back away from the Earth’s surface, a phenomenon known as the albedo effect. The loss of albedo as snow and ice melt away is reducing the flow of the Colorado by 9.5% for each 1C of warming, according to the research published in Science. The world has heated up by about 1C since the pre-industrial era and is on course for an increase of more than 3C by the end of the century unless planet-warming emissions are drastically cut. For the Colorado this scenario means an “increasing risk of severe water shortages”, the study states, with any increase in rainfall not likely to offset the loss in reflective snow. The magnitude of the Colorado’s decline as outlined in the Science paper is “eye popping”, according to Brad Udall, a senior scientist at Colorado State University and an expert on water supplies in the west who was not involved in the research. “This has important implications for water users and managers alike,” Udall said. “More broadly, these results tell us that we need to reduce greenhouse gas emissions as soon as we possible can. “We’ve wasted nearly 30 years bickering over the science. The science is crystal clear – we must reduce greenhouse gas emissions immediately.” The Colorado rises in the Rocky Mountains and slices through ranch lands and canyons, including the Grand Canyon, as it winds through the American west. It previously emptied into the Gulf of California in Mexico but now ends several miles shy of this due to the amount of water extraction for US agriculture and cities ranging from Denver to Tijuana. The river’s upper basin supplies water to about 40 million people and supports 16m jobs. It feeds the two largest water reserves in the US, Lake Powell and Lake Mead, with the latter supplying Las Vegas with almost all of its water. Snowpacks that last into late spring have historically fed streams that have nourished the Colorado River, as well as reducing the likelihood of major fires. As the climate heats up, the river is evaporating away and the risk of damaging wildfires is increasing. The climate crisis is compounding existing threats to the river, which include intensive water pumping for agriculture, water use by urban areas and the threat of pollution from uranium mining. Lake Mead, the vast reservoir formed by the Hoover dam, has dropped to levels not seen since the 1960s. A 19-year drought that racked stretches of the river almost provoked the US government to impose mandatory cuts in water use from the river last year, only for seven western states to agree to voluntary reductions. The problems are set to become more severe, however, as the climate becomes hotter and drier at a time when demand for water from expanding cities in the American west increases."
"
Al Gore Leaves The Light On For Ya
From Nashvillepost.com
By Kleinheider
The “312” is his address – 312 Lynnwood Blvd. Nashville
Even during Earth Hour. President of the Tennessee Center For Policy Research Drew Johnson takes a Saturday drive by Al Gore’s during the time most environmentalists went dark:
I pulled up to Al’s house, located in the posh Belle Meade section of Nashville, at 8:48pm – right in the middle of Earth Hour. I found that the main spotlights that usually illuminate his 9,000 square foot mansion were dark, but several of the lights inside the house were on.
In fact, most of the windows were lit by the familiar blue-ish hue indicating that floor lamps and ceiling fixtures were off, but TV screens and computer monitors were hard at work. (In other words, his house looked the way most houses look about 1:45am when their inhabitants are distractedly watching “Cheaters” or “Chelsea Lately” reruns.)
The kicker, though, were the dozen or so floodlights grandly highlighting several trees and illuminating the driveway entrance of Gore’s mansion.
I [kid] you not, my friends, the savior of the environment couldn’t be bothered to turn off the gaudy lights that show off his goofy trees.
More here
Here’s a look at Al Gores Nashville mansion:
Gore's Mansion in Nashville 

Vice President Al Gore has purchased this home, in Nashville’s exclusive Belle Meade section, for a reported USD2.3 million. The deed for the Colonial-style home, which sits on 2.09 acres of some of the city’s most expensive land, was signed on June 17, 2002. Gore and his wife, Tipper, will keep other homes in Tennessee and Virginia. It was published February 28, 2007 that research group in Tennessee, where the former vice president lives, claims that Mr Gore’s 20-room, eight-bathroom home in Nashville consumes more electricity in a month than the average American household uses in a year.
Photo and description Source: Daylife
You can see it here on Google Maps
From an aerial view looking south you can see what could be a handful of solar panels, though the orientation is puzzling if that is what they are. Update: in comments it it pointed out that they may also be skylights, which seems more probable. So it appears there are no solar panels on Mr. Gore’s home. Note the SUV fleet.
From Microsoft Live Earth - click image for an interactive view
Here is a view looking east:
From Microsoft Live Earth - click image for an interactive view
UPDATE: The photos above don’t show solar panels, however an alert commenter found this photo showing the placement on the one flat section of roofing shown in the aerial views above:
Solar panels are seen on the roof of the home of former Vice President Al Gore in Nashville, Tenn. , Thursday, June 7, 2007. Gore, the environmental activist stung by criticism over his house's energy efficiency, said Friday that renovations are nearly complete to make it a model ""green"" home. Earlier this year, a conservative group criticized Gore, citing electric bills that were far more than the typical Nashville home. Utility records showed the Gore family paid an average monthly electric bill of about $1,200 last year for its 10,000-square-foot home. Source: AP
The 34 panels look to be between 200 and 250 watts each, for a total capacity at full sun of 6.8 to 8.5 kilowatts for the system.They will provide an offset, but will not fully replace energy consumption there. Given the 10,000 sq foot size and the pool, this is an undersized installation for the home. Some ground based panels would have helped.
– Anthony

Sponsored IT training links:
We offer guaranteed success in OG0-093 exam using latest 1z0-007 dumps and 70-272 sample tests



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e977d2c01',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterSweden’s meteorological agency reports that ice coverage on the Baltic Sea is greater than it’s been in nearly a quarter century. Read more here.
About 250,000 square kilometres of the Baltic Sea are now covered in ice according to the Swedish Meteorological and Hydrological Institute (SMHI).
The last time so much of the Baltic was frozen was the winter of 1986-87, when ice covered nearly 400,000 square kilometres of the sea’s surface.
SMHI warns that ice coverage on the Baltic could expand further in the coming days, possibly setting a new record.
The area of interest is in the circle.
Most Baltic Ice in 25 years.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterI found this comment from a reader who calls him(her)self “wordsmith”, posted under my “Contact” sidebar. Because it’s Christmas Eve and things are a little hectic, I thought maybe some readers could give me a hand and help out this poor warmist. Here’s his(her) comment:
It’s amazing to me when I hear Climate Change skeptics talk about the misinformation from those who believe it is happening, that it’s primarily caused by man, and that the consequences will be significant. What would be the motivation for so many scientists, the vast majority of scientists, far more educated and intelligent than either of us, to spread misinformation? Do you believe they’re just stupid? More so than you? Is there some economic benefit for them? Do you discount what all scientists say? Also, give me a break–an engineering degree does not in any way qualify you to be a climate expert. I work with a lot of engineers and you don’t have to be that bright to get an engineering degree, especially from some no-name university. MIT, maybe, but not the ones you attended.
He seems annoyed that I’m expressing myself freely at this blog.
Share this...FacebookTwitter "
"

I’m willing to wager two things. First, I’ll bet that anyone who said global warming is an overblown bunch of hooey had a terrible time at this year’s holiday cocktail parties. Second, I’ll take even money that the 10 years ending on December 31, 2007, will show a statistically significant global cooling trend in temperatures measured by satellite. 



Those two bets are related. Our greener friends were in high attack mode on the party circuit last month as TV newscasts presented a relentless nightly stream of reports on how 1998 was the warmest year since gosh knows when. NBC got British scientist Phil Jones to say that it was the warmest year of the millennium, even though the thermometer was invented only about three‐​quarters of the way through it. The figures he used were estimates based mainly on tree ring measurements prior to 1900 (that’s 900 of the last 1000 years), and those estimates are known to miss an enormous amount of true temperature variation. 



As far as 1998 was concerned, the wire stories correctly noted (for a change) that all three true temperature histories — from ground‐​based thermometers, weather balloons in the lower atmosphere and satellite soundings of the lower layers — showed the same thing: record temperatures. The satellite history is 20 years long, the weather balloon history is 42, and reliable global ground‐​based temperature data really extend back only about 55 years. But they all were higher than kites. 



The many hours spent explaining to folks that 1998 blew the top off the record because heat from the big El Niño burped its way out to space could easily have been saved if I’d just had a vest‐​pocket temperature history handy. To save you a similar investment of time, there’s one included with this article. It’s intended to wrap around your business card (or if you’re not working, a credit card) for easy access at the next party. 



Brandish the card while you’re being jabbed into the corner by a sharpened carrot stick (or, God forbid, a Ranch‐​sauced bit of broccoli) by some punk wearing a Phish shirt. What he’ll see is absolutely no warming trend whatsoever from when the satellite measurements began (January 1979) through the end of 1997, followed by one warm year in 1998.  




Our greener friends were in high attack mode on the party circuit last month as TV newscasts presented a relentless nightly stream of reports on how 1998 was the warmest year since gosh knows when. 



The stunning blip in 1998 is just that — a blip. Close examination (data shown here run through November 1998) shows that temperatures have dropped back down to the levels typical of 1979–97. 



Those who follow global warming know that two California scientists recently found a problem with the satellite temperatures. The data shown on the chart have been corrected by NASA scientist Roy Spencer. 



Last year was so warm that it induces a statistically significant warming trend in the satellite data. Thus the second bet: Starting with 1998, there will almost certainly be a statistically significant cooling trend in the decade ending in 2007. 



In part, rapid cooling in late 1998 explains the record level of hysteria accompanying the global warming stories in the last half of December. Anyone pushing for climate legislation knows that it’s now or never, because the warming is over. 



If the Kyoto protocol doesn’t pass in the heat of this particular moment, it never will. So maybe readers will want to make copies of the little wallet card and send a few to their friends here in Washington. Or perhaps if you come to town for a little social gathering, you can pull it out and show it around. And, while we’re at it, any takers on my wagers?
"
" When Josep Borrell, the EU’s newly appointed foreign policy chief, recently caused outrage by dismissing young climate activists as flaky sufferers of “Greta syndrome”, he made not just a serious error of judgment but a serious mistake in macroeconomics. It was a mistake that is symptomatic of the dire state of European economic debate after a decade of austerity and schwarze Null (balanced budget) ideas.  “The idea that young people are seriously committed to fighting climate change – we could call it the ‘Greta syndrome’ – allows me to doubt that,” Borrell said, before going on to question their naivety about the cost of tackling the climate crisis. “I would like to know if young people demonstrating in Berlin … are willing to lower their living standards to offer compensation to Polish miners, because if we fight against climate change for real they will lose their jobs and will have to be subsidised.” For Borell, greening the European economy is a zero-sum game, in which paying for the move to a low-carbon economy must come out of the pockets of German taxpayers. Borrell’s views, however unpalatably delivered, are perfectly aligned with the flawed macroeconomics of the much-vaunted European Green Deal. The clue is in the name. The European Green Deal is the European commission’s proposed €1tn plan to finance the transition away from fossil fuels to decarbonising Europe’s economy. But the commission quietly dropped the word “new” from original US plans for a green new deal, which of course echo Franklin D Roosevelt’s Depression-era economic New Deal. Losing that “new” is a signal that the commission does not seek system change through ambitious green macroeconomics and tough regulation of carbon financiers. Rather, it takes a politics as usual, third-way approach that seeks to nudge the market towards decarbonisation. The macroeconomics of the European Green Deal remains trapped in the black zero logic of austerity. Instead of ambitious green fiscal activism, it mostly reshuffles existing European funds through a logic of seed funding to mobilise private sector money. Public money will be used to take risk out of private business activities and finance a “just transition” mechanism that promises to protect groups like Polish miners after their coal mines close through retraining and reskilling programmes. But there is little guarantee that European taxpayer money will reach Polish miners. It will probably go into the pockets of decarbonisation “barons”: clever local elites who will funnel transition money to their businesses, just as land barons siphoned most of the subsidies originally intended for small farmers under the common agricultural policy. Take Romania. Mining unions there complain that measures intended to “reskill’ miners, tested in the Valea Jiului region in Transylvania for the past 15 years, solely benefitted decarbonisation firms. Their connections to Romania’s political elites allowed them to capture the “market” for reskilling services, but private investment and jobs in new economic sectors never actually materialised. In dismissing green macroeconomics, the European commission puts its hopes on private finance. The logic is that the state won’t have to pay if the private sector will, provided there is nudging from public funds to “derisk” green investments. Here, the commission seems to have powerful allies, such as institutional investors with trillions ready to be greened. Larry Fink, the head of BlackRock, one of the world’s largest asset managers, recently noted that “we are on the edge of fundamentally reshaping finance” by taking decarbonisation seriously. The turn to green finance is a welcome step given that BlackRock and other global investors have so far behaved more like greenwashing carbon financiers than responsible climate investors: talking green while consistently blocking climate shareholder resolutions. But the danger is that the public money the commission plans to put into greening the European economy will instead merely subsidise greenwashing. Think of it as a two-step strategy through which carbon financiers can turn climate into a profitable business. The first step involves shaping the rules of the game, such as the “green list” of assets (or “green taxonomy”) currently being negotiated by the EU institutions. The EU taxonomy of sustainable activities has important advantages over the private environmental ratings (known as ESG ratings) currently used by private finance to identify green assets. Drawing on a broader range of views, including climate experts, the EU taxonomy sets a public standard of green that makes it more difficult for carbon financiers to purchase green ratings privately. Done properly, it could become a global standard for measuring and regulating the environmental performance of global finance. But the EU list risks being watered down in the ongoing political negotiations over the exact details of what constitutes “green” activities. Already, furious lobbying has led to the inclusion of a category of “enabling” activities under the auspices of “pathways to green”. These could easily become loopholes for activities that are more brown than green. The incentive for carbon financiers is to stick the label green everywhere they can in preparation for the second step: persuading European regulators to promote (de-risk) green assets. Meanwhile the commission refuses to talk about – let alone regulate – “brown finance”. Yet the strict regulation of brown finance could be a powerful tool for financing the European Green Deal. The commission could impose penalties on brown assets, either through taxation (a green FTT) or regulation, thus accelerating the switch to green assets. Those outraged by Borrell’s dismissive remarks about Greta Thunberg’s generation should note that the real political battle is to ensure that the European Green Deal does not morph into the first greenwashed social pact between regulators and carbon financiers, between Brussels and local elites, exporting greenwashed finance standards to the rest of the world. Climate activists should be pushing for a complete green economic agenda that recognises the critical role of green fiscal activism in organising the transition to low-carbon. It also means protecting public finances from carbon financiers, ensuring instead that private finance becomes the first lever in the climate fight. • Daniela Gabor is professor of economics and macrofinance at UWE Bristol "
"

NEW 4/10/09: There is an update to this post, see below the “read the rest of this entry” – Anthony
Guest Post by Richard Lindzen, PhD.
Alfred P. Sloan Professor of Meteorology, Department of Earth, Atmospheric and Planetary Science, MIT 

This essay is from an email list that I subscribe to. Dr. Lindzen has sent this along as an addendum to his address made at ICCC 2009 in New York City. I present it here for consideration. – Anthony
Simplified Greenhouse Theory
The wavelength of visible light corresponds to the temperature of the sun’s surface (ca 6000oK). The wavelength of the heat radiation corresponds to the temperature of the earth’s atmosphere at the level from which the radiation is emitted (ca 255oK). When the earth is in equilibrium with the sun, the absorbed visible light is balanced by the emitted heat radiation.
The basic idea is that the atmosphere is roughly transparent to visible light, but, due to the presence of greenhouse substances like water vapor, clouds, and (to a much lesser extent) CO2 (which all absorb heat radiation, and hence inhibit the cooling emission), the earth is warmer than it would be in the absence of such gases.

The Perturbed Greenhouse
If one adds greenhouse gases to the atmosphere, one is adding to the ‘blanket’ that is inhibiting the emission of heat radiation (also commonly referred to as infrared radiation or long wave radiation). This causes the temperature of the earth to increase until equilibrium with the sun is reestablished.
For example, if one simply doubles the amount of CO2 in the atmosphere, the temperature increase is about 1°C.

If, however, water vapor and clouds respond to the increase in temperature in such a manner as to further enhance the ‘blanketing,’ then we have what is called a positive feedback, and the temperature needed to reestablish equilibrium will be increased. In the climate GCMs (General Circulation Models) referred to by the IPCC (the UN’s Intergovernmental Panel on Climate Change), this new temperature ranges from roughly 1.5°C to 5°C.
The equilibrium response to a doubling of CO2 (including the effects of feedbacks) is commonly referred to as the climate sensitivity.
Two Important Points
1. Equilibration takes time.
2. The feedbacks are responses to temperature – not to CO2 increases per se.
The time it takes depends primarily on the climate sensitivity, and the rapidity with which heat is transported down into the ocean. Both higher sensitivity and more rapid mixing lead to longer times. For the models referred to by the IPCC, this time is on the order of decades.
This all leads to a crucial observational test of feedbacks!


The Test: Preliminaries
Note that, in addition to any long term trends that may be present, temperature fluctuates on shorter time scales ranging from years to decades.

Such fluctuations are associated with the internal dynamics of the ocean- atmosphere system. Examples include the El Nino – Southern Oscillation, the Pacific Decadal Oscillation, etc.
These fluctuations must excite the feedback mechanisms that we have just described.
The Test
1. Run the models with the observed sea surface temperatures as boundary conditions.
2. Use the models to calculate the heat radiation emitted to space.
3. Use satellites to measure the heat radiation actually emitted by the earth.
When temperature fluctuations lead to warmer temperatures, emitted heat radiation should increase, but positive feedbacks should inhibit these emissions by virtue of the enhanced ‘blanketing.’ Given the model climate sensitivities, this ‘blanketing’ should typically reduce the emissions by a factor of about 2 or 3 from what one would see in the absence of feedbacks. If the satellite data confirms the calculated emissions, then this would constitute solid evidence that the model feedbacks are correct.
The Results of an Inadvertent Test
From Wielicki, B.A., T. Wong, et al, 2002: Evidence for large decadal variability in the tropical mean radiative energy budget. Science, 295, 841-844.
Above graph:
Comparison of the observed broadband LW and SW flux anomalies for the tropics with climate model simulations using observed SST records. The models are not given volcanic aerosols, so the should not expected to show the Mt. Pinatubo eruption effects in mid-1991 through mid-1993. The dashed line shows the mean of all five models, and the gray band shows the total rnage of model anomalies (maximum to minimum).
It is the topmost panel for long wave (LW) emission that we want.
Let us examine the top figure a bit more closely.

From 1985 until 1989 the models and observations are more or less the same – they have, in fact, been tuned to be so. However, with the warming after 1989, the observations characteristically exceed 7 times the model values. Recall that if the observations were only 2-3 times what the models produce, it would correspond to no feedback. What we see is much more than this – implying strong negative feedback. Note that the ups and downs of both the observations and the model (forced by observed sea surface temperature) follow the ups and downs of temperature (not shown).
Note that these results were sufficiently surprising that they were confirmed by at least 4 other groups:
Chen, J., B.E. Carlson, and A.D. Del Genio, 2002: Evidence for strengthening of the tropical general circulation in the 1990s. Science, 295, 838-841.
Cess, R.D. and P.M. Udelhofen, 2003: Climate change during 1985–1999: Cloud interactions determined from satellite measurements. Geophys. Res. Ltrs., 30, No. 1, 1019, doi:10.1029/2002GL016128.
Hatzidimitriou, D., I. Vardavas, K. G. Pavlakis, N. Hatzianastassiou, C. Matsoukas, and E. Drakakis (2004) On the decadal increase in the tropical mean outgoing longwave radiation for the period 1984–2000. Atmos. Chem. Phys., 4, 1419–1425.
Clement, A.C. and B. Soden (2005) The sensitivity of the tropical-mean radiation budget. J. Clim., 18, 3189-3203.
The preceding authors did not dwell on the profound implications of these results – they had not intended a test of model feedbacks! Rather, they mostly emphasized that the differences had to arise from cloud behavior (a well acknowledged weakness of current models). However, as noted by Chou and Lindzen (2005, Comments on “Examination of the Decadal Tropical Mean ERBS Nonscanner Radiation Data for the Iris Hypothesis”, J. Climate, 18, 2123-2127), the results imply a strong negative feedback regardless of what one attributes this to.
The Bottom Line
The earth’s climate (in contrast to the climate in current climate GCMs) is dominated by a strong net negative feedback. Climate sensitivity is on the order of 0.3°C, and such warming as may arise from increasing greenhouse gases will be indistinguishable from the fluctuations in climate that occur naturally from processes internal to the climate system itself.
An aside on Feedbacks
Here is an easily appreciated example of positive and negative feedback. In your car, the gas and brake pedals act as negative feedbacks to reduce speed when you are going too fast and increase it when you are going too slow. If someone were to reverse the position of the pedals without informing you, then they would act as positive feedbacks: increasing your speed when you are going too fast, and slowing you down when you are going too slow.

Alarming climate predictions depend critically on the fact that models have large positive feedbacks. The crucial question is whether nature actually behaves this way? The answer, as we have just seen, is unambiguously no.
UPDATE: There are some suggestions (in comments) that the graph has issues of orbital decay affecting the nonscanner instrument’s field of view. I’ve sent a request off to Dr. Lindzen for clarification. – Anthony
UPDATE2: While I have not yet heard from Dr. Lindzen (it has only been 3 hours as of this writing) commenter “wmanny” found this below,  apparently written by Lindzen to address the issue:
“Recently, Wong et al (Wong, Wielicki et al, 2006, Reexamination of the Observed Decadal Variability of the Earth Radiation Budget Using Altitude-Corrected ERBE/ERBS Nonscanner WFOV Data, J. Clim., 19, 4028-4040) have reassessed their data to reduce the magnitude of the anomaly, but the remaining anomaly still represents a substantial negative feedback, and there is reason to question the new adjustments.”
I found the text above to match “wmanny’s” comment in a presentation given by Lindzen to Colgate University on 7/11/2008 which you can see here as a PDF:
http://portaldata.colgate.edu/imagegallerywww/3503/ImageGallery/LindzenLectureBeyondModels.pdf
– Anthony
UPDATE3: I received this email today  (4/10) from Dr. Lindzen. My sincere thanks for his response.
Dear Anthony,
The paper was sent out for comments, and the comments (even  those from “realclimate”) are appreciated.  In fact, the reduction of the  difference in OLR between the 80’s and 90’s due to orbital decay seems to me to  be largely correct.  However, the reduction in Wong, Wielicki et al (2006) of  the difference in the spikes of OLR between observations and models cannot be  attributed to orbital decay, and seem to me to be questionable.  Nevertheless,  the differences that remain still imply negative feedbacks.  We are proceeding  to redo the analysis of satellite data in order to better understand what went  into these analyses.  The matter of net differences between the 80’s and 90’s is  an interesting question.  Given enough time, the radiative balance is  reestablished and the anomalies can be wiped out.  The time it takes for this to  happen depends on climate sensitivity with adjustments occurring more rapidly  when sensitivity is less.  However, for the spikes, the time scales are short  enough to preclude adjustment except for very low sensitivity.
That said,  it has become standard in climate science that data in contradiction to alarmism  is inevitably ‘corrected’ to bring it closer to alarming models.  None of us  would argue that this data is perfect, and the corrections are often plausible.   What is implausible is that the ‘corrections’ should always bring the data  closer to models.
Best wishes,
Dick

Sponsored IT training links:
Best quality 70-448 prep material is available for download. Pass the real exam using JN0-350 guide and E20-361 lab tutorial.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e973bbaff',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
nan
"
A snowmobiler negotiates the streets of Crosby, North Dakota. Photograph courtesy of the Crosby Journal.

Guest Post by Harold Ambler
Snow, wind, and cold have assaulted North Dakota yet again in the past 24 hours. In Bismarck Friday morning the temperature was 12 below zero with a new inch or two of snow expected following Thursday’s more significant storm.
According to USA Today, snow in the southern part of the state was bad enough Thursday that snowplow operators were pulling off the road, blinded by the whiteout conditions. A foot of snow was common in the heaviest band.
The National Weather Service predicts a high temperature of 3 degrees Fahrenheit Friday in Bismarck, as well as additional snow. As of Thursday, three-quarters of the state’s roads were still snow-covered, in whole or in part, from the storm that just ended the day before.

Howling winds and copious snow have combined to leave austere scenes like this in Cavalier County, North Dakota. Photograph courtesy of the ND Department of Emergency Services.

More than once during the winter, the Department of Transportation has issued a no-travel advisory, most recently on February 10.
Cecily Fong, spokeswoman for the state’s Department of Emergency Services, said that the winter got off to a bad start on November 4. “That first storm was definitely a blizzard with blowing and drifting snow,” she said.  Since then, according to Fong, several counties have seen more than 400 percent of normal snowfall.
December was a record breaker for Bismarck, as it was at many other locations around the state. In Bismarck, the total for the month was 33.3 inches, the greatest amount ever received in a single month.
Those were early days, it turned out. Frequent storms, followed by howling northwest winds and record-breaking cold, have made it a winter to remember. On January 15, the morning low at the Bismarck airport was 44 below zero, the coldest ever for the date, and one degree shy of the all-time coldest reading for a state known to be less than balmy.
By the end of January, many counties had more than 400 percent of normal snow totals on the ground, and Governor John Hoeven had declared a state of emergency. 
“There has been a repeated pattern,” said Fong,  ”where the county will come and plow a road and then two days later, without any additional snow, the road becomes impassable again.” Relatively speaking, the people in Bismarck have gotten off light. Divide County, in the state’s northwest corner, has received 500 percent of normal snowfall.
Steve Andrist, who has lived most of his life in Divide county and is the publisher of the weekly Crosby Journal, commended the street department. “There has never been more than a day or a day and a half where the roads were

Roads that were cleared once, and twice, have needed to be cleared a third time in various locations throughout the state. Photograph courtesy of the ND Department of Emergency Services.

impassable,” he said.
After a lifetime living so near the Canadian border, did the last few months really amount to anything? “This winter got my attention,” he said. “The thing that’s different about this one is the volume of snow. It’s so much more than we anticipated. As far as snow and moving it, and moving it again, and having to move it again a third time, this has been very unusual.”
On February 19, the governor asked the federal government to provide emergency assistance for snow removal. “We’ve got roads that aren’t being plowed,” Fong said, “just because the funds aren’t available to do it.”
Although the spring melt is weeks away, Fong said that flooding is already a concern. “We don’t know where, and we don’t know when, but we’re keeping our eyes on it.”


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e97fec37c',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Plastic pollution has become pervasive, pernicious and persistent. In 2016, global plastic production was around 335m tonnes and it continues to be released into our rivers and oceans on a catastrophic scale, despite recently becoming one of the world’s most publicised and well documented environmental issues. The so called “Blue Planet” effect, which emerged after David Attenborough’s TV show highlighted the issue of plastic pollution in the UK, has seen the introduction of small but significant changes in behaviour and legislation. Awareness campaigns and the growth of numerous grassroots organisations, particularly on a local scale, are making significant contributions to resolving some of the issues. But much of this activity is based around macroplastics, larger pieces of pollution that we know more about and that more easily attract public action through activities such as beach cleaning. A major part of the plastic problem is microplastics, tiny pieces less than 5mm in width whose impact on the environment is much less well understood and that are harder for the public to do something about. With that in mind, I’ve launched a citizen science project that aims to harness the public’s concerns about microplastics in order to gather the data that scientists need. There have been piecemeal attempts to resolve some of the microplastic issues, such as the bans on microbeads in cosmetics introduced in a number of countries. But the key challenges highlighted by a 2015 report on microplastics from the GESAMP group of international scientists – increasing scientific knowledge, adapting our behaviour and changing public perception – still exist. There are probably two reasons for this. First, the problem is vast and complex, and there are only a small, albeit growing, number of scientists looking into it. Second is the fact that ocean microplastics are harder to see than other forms of pollution and so the problem is more easily ignored. Getting people to stop using plastic drinking straws is one thing. But getting them to think or do something about the potential impact of tiny pieces of plastic that hide in the sand on beaches around the world is much more difficult. If we want to meet the objectives of the GESAMP report, we can only do it with with public engagement and participation. Citizen science potentially provides a win-win opportunity to meet those challenges. In early 2018, my colleagues and I at the University of Portsmouth along with the conservation charity I founded, Just One Ocean, developed a citizen scientist microplastic research programme. The aim was to gather data to help evaluate the environmental impacts of visible microplastics (between 1mm and 5mm in width) on the coastal environment. For each location studied, we wanted to know how many microplastics there were, how they were spread and what other characteristics they had.  


      Read more:
      Explainer: what is citizen science?


 Citizen science is growing but still viewed with some scepticism by many professional scientists. So citizen projects need to be carefully planned and carried out to ensure data is reliably accurate and consistent, and isn’t prejudiced by the biases of participants. Projects also need to be able to attract volunteers and consider issues of time, effort, cost and accessibility for volunteers. With these in mind we made the task simple, short and easy. Volunteers are provided with clear written information backed up by video guidance, while images of the data they collect provides a verification process. In trials undertaken on the south coast of the UK with nearly 100 volunteers, this method captured substantial amounts of scientific data that was consistent and accurate enough to prove citizen science could be used to study microplastics. The pilot also improved the knowledge and skills of the participants and encouraged the local community to take ownership of the problem.  Though we are still improving the programme, the success of the trials was enough for us to launch “The Big Microplastic Survey” as a global research project – and since July 2018 we’ve signed up participants from 42 countries. As well as individuals, this has included universities in India, Australia and South Africa, environmental agencies in remote locations such as St Helena, the Falklands and Ascension Island, and small scientific communities in Malaysia and Indonesia. Volunteers are able to undertake a simple microplastic survey in around 30 minutes using the sort of equipment you will find in most households. They take samples of sand from a specified area and use a density separation process with sea water to remove the microplastics. They then record the data, take a photograph of the microplastics and send us the results. We haven’t set an end date for the project and, in the longer term, our ambition is to enable volunteers to upload data in real time onto a geographic database that will be made freely available for researchers. We hope it will not only increase our scientific understanding of the problem, but also engage with hundreds of people. The challenges we face from plastic pollution are significant, but so are the potential contributions that ordinary people all over the world can make – a situation that scientists should embrace. There is a lot of interest in microplastics and people want to be involved, but up until now they did not know how to be. This project provides them with the opportunity they might have been waiting for."
"Ponds are taken for granted. Perhaps it’s because most of us have seen them – and on occasion, fallen into them – and think they’re only good for goldfish. Ponds may be the number one habitat for children’s “minibeast” hunts, but we are supposed to grow out of them in adulthood.  As James Clegg, a 20th-century British naturalist wrote, ponds are  a field particularly suited to the activities of the amateur, whose humble pond-hunting, if carried out systematically and carefully, may well result in valuable contributions to science. But all-too often, ponds are missed out of conservation strategies which are instead fixated on larger lakes and rivers. This is a serious omission – ponds are the most common and widespread habitat for all plants and animals across the continents and islands of Earth, from Antarctica to the tropics. Perched on the surface of Alpine glaciers or waiting out desert droughts to refill with the rains, deep in equatorial forest or amid the city sprawl. They could well be found on Mars.  The past 20 years have seen a blossoming of research into ponds, led in the UK by the Freshwater Habitats Trust and, internationally, the European Pond Conservation Network. These organisations bring together researchers and practitioners to help conserve pond biodiversity. Their work has revealed that ponds are biodiversity hotspots in the landscape, disproportionately rich in species when compared to rivers, streams and lakes and home to many rare specialists, such as fairy and tadpole shrimps.   Ponds benefit humans by slowing down water run-off that can cause flooding and mopping up excess nutrients – a great example of what are now recognised as “small water bodies” that enrich and enliven a landscape. But, globally, ponds may also be important in influencing atmospheric carbon by storing and releasing it, given the intensity of geochemical processes and the sheer number of ponds around the world. However, just how fast ponds can bury carbon is poorly understood. Measuring the rate at which ponds can store carbon is tricky, primarily because the age of many ponds is unknown. To get precise measurements of carbon burial rates we exploited an unusual opportunity using some small, lowland pools whose age is known to the exact day. The ponds were dug out in 1994, at Hauxley Nature Reserve in north-east England. Their original purpose was to follow the colonisation of plants and invertebrates. Two decades later they had accumulated a layer of sediment, dark and rich in organic debris, distinctly different to the underlying clay. We used sediment cores and dug out all of the sediment from some ponds, to measure the organic carbon that had accumulated. The amount of carbon in the cores was scaled up to the amount dug up from other ponds to reflect the total volume of sediment. The ponds’ burial rates for organic carbon ranged from 79 to 247g per square metre per year, with a mean of 142g. These rates are high – much higher than the rates of 2-5g attributed to surrounding habitats such as woodland or grassland. Small ponds occupy a tiny proportion of the UK’s land area – scarcely 0.0006% – compared to grassland at 36% or 2.3% for ancient woodland. But the rate of carbon burial we found would result in ponds burying half as much as the vastly greater expanse of grassland. However, the role ponds play in the carbon cycle is complicated. Some ponds may be significant sources of greenhouse gases, such as permafrost thaw ponds in the Arctic which release even more carbon as the tundras they’re found in warm. Our Hauxley ponds can switch back and forth from being a net sink to a net source of carbon as they dry out or re-flood. Nevertheless, our ponds have accumulated plenty of carbon over their 20 years and provided a home to a wealth of animals and plants. Nothing was done to engineer carbon burial in our ponds – there was no artificial enhancement of productivity to maximise carbon capture. They are small, shallow, lowland ponds among the intensively farmed landscapes typical of much of the temperate climes. Similar ponds and tiny wetlands are dotted throughout the local landscape, primarily scraped out for wildlife conservation.  These lowland ponds are easy to create, even in a back garden. They can be small and temporary – clean water is the key – and the value of their wildlife is now firmly understood. No longer overlooked, the importance of ponds in the carbon cycle and in fighting climate change is becoming apparent."
"

Kalamazoo State Hospital
In an effort to add to the surfacestation.org survey coverage, I’ve been looking at a number of stations from the aerial vantage points available in Google Earth and Microsoft Live Maps. I particularly look for the stations that have Stevenson Screens, as those are the most visible and easy to spot from these online resources.
I was disappointed to learn though that the USHCN station at Kalamazoo State Hospital (a state psychiatric hospital) has been closed. It was probably due to recent construction of new wards as seen here:
Click for a live interactive aerial view
While I was scouring online image databases looking for a surviving photo that would show the placement of the Stevenson Screen, I stumbled across this photo on Flickr taken from afar and this strange comment about it:

Flickr caption reads: I took this photo from well away from the grounds of the State Hospital, so as to avoid violating state law.
“Apparently, it is illegal to take photographs on the grounds of the state hospital as a protection to those who are patients there.  I received a lengthy explanation of this law from the Michigan State Police officer who was on patrol at the hospital the morning I visited.  He advised me that the hospital office has quite a collection of confiscated cameras.”
I did visit another state mental hospital in Napa, CA and found this placement of the MMTS:

You can see a full set of pictures, at the surfacestations.org image database.
So WUWT readers, here is the challenge against very unlikely odds:
Find a surviving photo of the USHCN weather station at Kalamazoo State Hospital. 
Good hunting.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9a64e2ca',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"Climate change is already affecting many aspects of our everyday lives, including what we eat, where we live and the buildings we inhabit. And dangerous industries that make our way of life possible, such as agriculture, construction and fishing, are becoming riskier than ever as a result of changing weather. As extreme natural events become more common, it is increasingly important these industries adapt for the future.  Stronger winds, more frequent storms and increased flooding obviously make life more difficult for anyone who works outdoors. Workers also might potentially face increased levels of heat and pollution exposure, which could be harmful to their health. But climate change won’t just potentially make existing workplace hazards worse – it will create new ones. In the construction sector, new building materials and practices may come with unanticipated health and safety consequences. The industry also faces the challenge of designing and constructing buildings and infrastructure that can withstand more stress from a changing climate. Parts of the world most affected will need infrastructure built to higher standards. In areas like California, where problems with drought and wildfires have grown dramatically in recent decades, resilient construction guidelines have emerged.  In the Netherlands and Japan, for example, many car parks and spaces beneath buildings can store floodwater. Residential buildings have also been adapted to float on stilts if floodwaters rise. Buildings are also constructed to be more sustainable, with green roofs, living walls, and better energy efficiency to lessen environmental impact. Developing other innovative design strategies and techniques will help ensure building safety, but it’s important that worker safety is kept in mind. Finding novel solutions to more extreme weather will be crucial to increase safety in the fishing industry. Improved GPS, tracking, and better storm forecasting will help fishermen navigate more precisely, and better identify dangerous waters and conditions.  Extreme weather also means more days’ work will be lost because of poor and dangerous conditions for fishermen globally. But if more ships stay in port because of extreme weather, this will have economic consequences on anticipated revenue and incomes. To limit this, the industry needs to increase training and preparedness to operate in bad weather, as well as investing in improved technological support, such as satellite-based geographic information systems and long-range weather-forecasting.  The industry is already responding with efforts to improve scientific advice and data collection to predict weather systems and extreme events, and to better understand trends in fish stock populations and distributions. There are also attempts to improve vessel safety, port resilience, and reduce the vulnerability of freight while at sea. Warmer seas may also cause some fish species such as cod to move from their traditional habitats, often further from key fishing ports. This creates problems for smaller fishing vessels that aren’t designed to navigate deeper and stormier waters. Better sat-nav systems are therefore essential to help these vessels operate safely. Increased levels of fish farming, meanwhile, will need to be rolled out in a way that minimises pollution caused by intensive rearing.  


      Read more:
      Climate change threatens global fish stocks


 The farming sector is at growing risk from greater flooding, not only because of extreme rainfall but because of growing soil erosion, which reduces the ground’s ability to absorb water, worsening flood risk. Lowland and coastal areas in particular might be more susceptible or even completely lost to saltwater flooding.  Agricultural workers also face the threat of more heat-related illnesses, as well as greater exposure to pesticides and contaminated and polluted air. But the extent of this impact will be hard to forecast until we know more about the extreme weather patterns that may become the norm. It’s unlikely many of these problems can be halted, but it may be possible to develop new crops more suited to a changing climate, or pest and disease resistance. Researchers are already developing new strains of staples (such as maize) to adapt to these changes. Other new technologies, such as satellite imagery and drones can identify drought in crops, and target irrigation where it is most needed. Farmers are also being asked to change the way they use the land in order to reduce the environmental impact of agriculture. Extreme weather and climate change is becoming the norm, and these events are part of a long-term shift that will continue. The solutions to our situation will be in technical and engineering innovations, and in changing the ways we influence nature. We know these problems will only grow in their magnitude and effect, so it is essential we adapt for things we can no longer avoid."
"In a move to protect its ski slopes and growing economy, Utah – one of the reddest states in the nation – has just created a long-term plan to address the climate crisis.  And in a surprising turnaround, some of the state’s conservative leaders are welcoming it. “If we don’t think about Utah’s long-term future, who will?” Republican state house speaker Brad Wilson said at a recent focus group to discuss the proposals. At the request of the Republican-dominated state legislature, a University of Utah economic thinktank produced the plan to reduce emissions affecting both the local air quality and the global climate. Project lead Thomas Holst, an energy analyst, never expected to be at the helm of an effort like this. A few years ago, the Utah legislature passed a resolution urging the EPA to “cease its carbon dioxide reduction policies, programs, and regulations until climate data and global warming science are substantiated”. But now the perspectives of some state lawmakers – and of Holst, who spent most of his career in the oil and gas industry – have shifted. “The economist Adam Smith talked about an invisible hand that guides the economy, and in this particular case, the cost of renewable energy, whether it’s wind or solar, has gone down so rapidly and made itself so market efficient versus fossil fuels, that there is a change, and the change can’t be ignored,” Holst said. “So now is the opportunity for a state like Utah which is rich in both renewables as well as fossil fuels to embrace that change and get out ahead of it.” Other red states and municipalities are slowly starting to address global heating. After banning the words “climate change” from state environmental agencies, Florida now has a chief resilience officer tasked with preparing for sea level rise. After a year of disastrous flooding, Nebraska lawmakers advanced a bill to develop a climate change plan for a full legislative debate.  Utah prides itself on being business friendly – and it has a rapidly growing tech sector concerned about environmental issues, as well as booming tourist economy that revolves around the ski industry and public lands. The Utah plan, known as the Utah Roadmap, began, like a number of recent environmental initiatives, with young people clamoring for action. High school students drafted a resolution that recognized the impacts of the climate crisis and encouraged emissions reductions, and persuaded two Republican lawmakers to sponsor it. Environmental advocates say it was the first measure of its kind to pass in a red state. The legislature followed up with state money for experts to provide policy recommendations. Another factor that has primed Utah leaders to address the climate crisis is the state’s unique air quality issues. The majority of the population lives in mountain valleys where in winter, temperature inversions can trap air pollutants, often reaching levels that impact health, particularly among children and the elderly. “It cuts across political lines. [Clean air] is not a partisan issue in our state,” said Utah speaker Wilson. He said there is not the same kind of consensus on climate change in the legislature, but “there is absolutely overlap between air quality concerns we have and reducing greenhouse gas emissions”. Natalie Gochnour, the head of the economic policy institute that drafted the Utah Roadmap, said its proponents managed to turn a hyper-partisan issue into a shared priority by emphasizing the local impacts of the climate crisis. Research suggests that framing policy around economic benefits and sustainability allows local leaders to respond to climate change without getting caught up in political divisions. “That tends to pull some of the politics out of it – not for everybody – but for many. I think enough to create momentum on Capitol Hill,” Gochnour said. Clean air concerns are also the reason officials are pushing Utah gas refineries to produce cleaner gasoline, and when the Trump administration announced plans to roll back clean car standards, Utah’s bipartisan clean air caucus held a press conference urging Congress to resist the move. Holst, the roadmap project lead, acknowledged that blue coastal states have taken the initiative on ameliorating climate change, but he sees potential for Utah. “Is there an opportunity for a red state to take a leadership role? We believe that there is. And by composing a road map, by encouraging our legislative leaders to embrace this, we believe that there can be a change, and that Utah will be willing to take a leadership role,” he said. Utah’s per capita carbon emissions are higher than most states, in part because it’s nearly twice as reliant on coal, but utilities serving Utah customers plan to close many of their coal power plants by 2030, converting to wind, solar, natural gas, and possibly hydrogen. Republican state lawmaker Melissa Garff Ballard has an ambitious plan to make Utah a source of hydrogen power serving the western US. Among the Utah Roadmap’s top priorities is to reduce CO2 emissions by half over the next decade – a challenge for a state with a growing population. The plan suggests focusing on energy-efficient buildings and clean transportation options. It recommends expanding Utah’s network of charging stations, incentivizing the purchase of electric vehicles, and involving auto dealers in strategies to increase the zero-emissions vehicle supply. Business leaders have told Holst they are drafting a document that would pledge to move forward with the Utah Roadmap’s recommendations. “What I’m interested in is a viable future for the state of Utah,” Republican state representative Stephen Handy said. “There are still a number of Utah legislators who don’t want to look at the science that’s very obvious on climate change, but we’ve come a long way.” This article was amended on 19 February 2020 to clarify Brad Wilson’s comments on clean air."
"Despite some positive climate action, new fossil fuel infrastructure is still being built and deployed. Dozens of new coal power plants are currently planned or under construction, for instance, while petrol car sales will nearly hit 100m in 2019. But what if all that ceased tomorrow? It turns out that if we built no more fossil fuel infrastructure and instead replaced existing infrastructure at the end of its productive life with a zero carbon alternative we could limit peak temperature rise to 1.5°C – as long as we start now. Colleagues and I recently analysed what would happen to global emissions if fossil fuel power plants, cars, ships, aircraft and industrial infrastructure were all phased out. Our results are published in Nature Communications. This may be a hypothetical scenario, given a full phase out is unlikely to happen any time soon, but calculating what would happen in such a scenario gives us a better idea of the challenge ahead.   In our optimistic scenario, the process of replacing all the fossil fuel infrastructure with zero carbon alternatives (or not replacing it at all) began from the end of 2018. Doing this we found that the chance of keeping global temperature rise to below 1.5°C was 64%. Delaying a fossil phase out until 2030 would make this a lot less likely, even if the phase out rate was sped up. A coal power plant is typically operated and emits carbon dioxide (CO₂) for about 40 years. So, every new coal plant built in the recent past or today carries a climate change commitment. For instance, Drax, the UK’s largest power plant,  used to burn coal exclusively and has probably warmed the planet by a few ten-thousandths of a degree over its lifetime. That isn’t much by itself, but such warming all adds up. We call this the “committed warming” from fossil fuel infrastructure. Drax now predominately burns wood pellets, and is one example of how fossil fuel infrastructure could be replaced. Our analysis produces a scenario that reduces CO₂ emissions to nearly zero over 40 years. This compares with the recent IPCC special report on 1.5°C, which concluded that reducing emissions to net zero over 35 years was required to get even a 50% chance of limiting global warming to 1.5°C. The additional, narrow window of five years to get to net zero can be explained by different approaches. Some of this difference is accounted for by the timing of emissions phase out. As every year of procrastination brings the date by which we’ll have to reach net zero emissions forward by two years, even delaying until after 2020 rather than 2018 means that a 40 year phase out would have to happen in 36 years to achieve the same outcome. We also used historical temperature observations to guide the future climate response to emissions. Our projection of additional warming for each additional tonne of CO₂ is a little lower than the IPCC’s best estimate, although well within the “likely” range.  Alongside power stations, cars, ships and planes, we also applied the “asset lifetime” assumption to meat cattle. Cows produce a lot of methane, so if we ate them all over the next three years without breeding any more, we could certainly reduce our greenhouse gas emissions considerably while having a gluttonous time doing so. Non-livestock emissions are more tricky and harder to mitigate – we still need to eat, and grow crops using fertilisers – but we assumed that we get more savvy at doing so, eventually reaching zero emissions by 2100.  Our study also comes with a number of important caveats. For instance, we did not delve into whether coal power will be replaced with solar panels or wind turbines, and we weren’t concerned with the exact sort of electric vehicle that will replace the petrol car. What matters to us is that these replacements are zero carbon, or that fossil fuel infrastructure is not replaced at all.  In many cases, zero carbon alternatives do not exist or might be difficult to scale up (for example in aviation). We also assumed that expected lifetime of fossil fuel infrastructure would not increase and that the surviving infrastructure would not be used more intensively (no rebound effects). Lifecycle effects are not included either: with our current high-carbon industry, manufacturing a wind turbine costs energy and creates emissions, even if the energy produced from it does not. In the fossil phase out scenario, we project temperatures would peak at anywhere between 0.1℃ and 0.8℃ above today – this shows the level of uncertainty in how the climate will respond, mostly related to how much warming is masked by air pollution in the present day. Because air pollution reflects sunlight and can brighten clouds, it reduces some of the warming we would otherwise experience. If fossil fuels are phased out, this air pollution will be removed, unmasking the warming. Importantly however, the CO₂ and fossil methane emissions would also reduce at the same time, meaning there would not be a sudden spike in warming from phasing out fossil fuels.  The good news is that the optimistic scenario we looked at in our research found that current global fossil fuel infrastructure is not yet at the point where it will definitely take us over 1.5°C. However, as every year’s delay makes staying below 1.5°C less and less likely, urgent action is still needed.  Indeed, building more of this infrastructure carries an economic cost. If more fossil fuels plants “lock in” more warming, then remedial action would involve premature retirement of these “stranded assets”, or costly negative emissions technologies to take carbon out of the air. We must do our best to avoid fossil fuel lock in and bring forward alternative technologies as quickly as possible."
"

Human life expectancy is rocketing upwards, crops are growing ever bigger, and nutrition is improving dramatically. As we stand witness to the greatest democratization of wealth in human history, how many times do we have to read that environmental disaster is upon us? This time the story is that the North Polar summer icecap is melting, and that computer models prove it is a result of all that pernicious industrial prosperity.



That’s what the casual reader must take from the December 3 issue of Science, and the same day’sWashington Post, citing the work of University of Maryland’s Konstantin Ya Vinnikov. Who claims that the well‐​known (but slight) recession of Arctic sea ice that has occurred in the last five decades is caused by human‐​induced climate change. The reason, Vinnikov says, is that computer models using greenhouse gas warming and sulfate aerosol cooling melt only about as much summer arctic sea ice as has already been lost. When run without human influence, the computer models melt this much ice only 2 percent of the time. We don’t worry about winter ice because everything is frozen up there in that season.



To be fair to the Post, science writer Curt Suplee went out of his way to point out the highly controversial nature of the finding, and that there’s a reasonable argument that Vinnikov’s finding isn’t as solid as it is cracked up to be. But Suplee didn’t have the space to go into the gory details. The problem lies in the “logic” of the study, in which a computer model is used to “prove” something. That requires that the model be correct. But every global warming model has gotten the behavior of 80 percent of the lower atmosphere wrong over the last quarter century, predicting warming where there was none. Because the atmosphere is a stirred fluid, correctly predicting the warming that occurred in the remaining 20 percent can only mean that the right answer was arrived at for the wrong reason. 



Welcome to 21st century science! Perhaps, instead of comparing two computer models, it might be better to see if warming is human induced by looking at the actual temperature history.



Let’s stipulate that the decline in Arctic sea‐​ice is real. But is it a consequence of the largely unknown behavior of the world’s largest natural highball glass? The Arctic Ocean contains the largest mass of floating ice on earth. Lest folks worry that melting this ice will inundate Miami, they might pour a scotch and soda and watch the ice melt. The “sea level” remains the same, even as the drink goes stale. I’ll drink to the notion that a lot of the observed melting is simply a continuation of a long‐​term process that initially had nothing to do with people.



We have a pretty decent history of Arctic temperatures, thanks to the Cold War. That history begins in 1958–about the time we realized we had to know a lot about the Arctic atmosphere, owing to a long‐​standing dispute with our nuclear‐​armed adversaries across the pole. The temperature history has been summarized and is continually updated by Department of Commerce scientist James Angell. What it shows certainly complicates Vinnikov’s computer‐​based analysis.



As is plain and clear, there is no warming at all in the first three decades of these measurements. Then a trend sets in, beginning in 1988, a mere decade ago. How can one melt the ice from 1950 through 1988 when there is no regional warming? In fact, it looks like the ice was in the process of melting long before the initiation of putative human greenhouse warming. Even worse, the period from 1950 to 1975 was one of cooling in the Northern Hemisphere, and still the ice melted.



A more logical explanation than what appears in Science is that the ice has been responding to a rapid and dramatic 2 degree centigrade warming of the Arctic that took place from 1920 to 1940. In other words, it takes many decades for the sea and the ice to adjust to past thermal imbalance. Is this Arctic warming really such a terrible thing? Scientists are pretty sure that the earth was about 1.5ºC warmer 4000 to 7000 years ago than it is now (although there is uncertainty as to why). That means that the summer Arctic ice had to have receded considerably beyond its current position. That era, known as the “hypsithermal period,” was also called the “climatic optimum” in previous generations of textbooks (written before the current “hysterical period”), because it accompanied the rise of agriculture and civilization. 



Maybe it’s not an accident after all that crops are growing better than ever and people are getting rich.
"
"
Share this...FacebookTwitterWorld leaders don't discuss climate? Is it a non-issue?
I’ve been keeping an eye on the Wikileaks disclosure of classified documents, which has deeply embarrassed a wide collection world political leaders, and particularly the US government in general.
I was hoping that among all of these confidential documents, some would be protocols, messages or reports on discussions on climate policy. So I did some trolling in the Internet, hoping to find more on this, but came up with nothing. Admittedly my search was not the most thorough of searches.
Seems very odd that this “vital” issue, one we are told our very survival hinges on, is nowhere mentioned in any of these hot documents. Do you mean to tell me world leaders never talk about it behind the scenes? I have a truly difficult time believing that. Call it the deafening silence.
Either:
a) World leaders, in reality, couldn’t care less about the topic, and thus all the talk about rescuing the climate is just a facade issue in front cameras and mikes to distract the public, meaning the leaders never really talk about it behind the scenes, and so no climate-related protocols exist. This seems highly unlikely to me.
or
b) The leaked documents were selectively disclosed to simply to target and cause personal embarrassment to some world leaders, while others were kept under wraps, meaning climate-related documents were sifted away by Wikileaks or by the media outlets with whom they were entrusted (e.g. the Guardian and Der Spiegel). Selective release with the intent to produce a designed reaction would be immoral, irresponsible, arrogant and a complete abuse of journalistic power.
The danger and the probability here is that Wikileaks, or the media outlets with whom the documents have been entrusted, have carefully cherry-picked the documents in order to engineer and produce a desired political reaction. This is what makes the release of the documents so disturbing, dangerous, and untrustworthy. We probably are getting only the picture that the disclosers want us to see.
I’m not into conspiracy theories, but I do smell a rat. We’ve seen this pattern of behavior time and again in climate science journalism.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




If anyone knows of the existence of any climate-related documents that are already out in the public domain, we would all like to see them. All the data is needed to make an accurate picture of the situation.
================================================
UPDATE 1: h/t Ron de Haan: Wikileaks co-founder on Climategate E-mails:

At 4-min mark:
UK Intelligence tried to frame us as a conduit for the FSB because they didn’t like the truth of what was in those e-mails.”
They’re both hiding things.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterOr to call it what it is: theological propaganda.
Google is also responding to the growing skepticism in climate science by starting “an effort to foster a more open, transparent and accessible scientific dialogue aimed at inspiring pioneering use of technology, new media and computational thinking in the communication of science to diverse audiences.” They’ll start by focusing on communicating the science on climate change.
Read here
That means they are going to help ramp up climate propaganda. According to their press release at Google blog, they’ve started the effort by selecting 21 mid-career Ph.D. scientists who have “the strongest potential to become excellent communicators.” A list of the 21 selected members can viewed at the website.
The selected fellows will participate in a workshop at Google headquarters in California. Google writes:
Following the workshop, fellows will be given the opportunity to apply for grants to put their ideas into practice. Those with the most impactful projects will be given the opportunity to join a Lindblad Expeditions & National Geographic trip to the Arctic, the Galapagos or Antarctica as a science communicator.
Expect to see more calving ice footage – dubbed with dire messages of planetary destruction. Folks, they’ve been crowing this 20 years.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




This is not the Internet giant’s first effort to try to sway public opinion concerning climate change. Last year it launched Google Earth map “which shows how the world would be affected by a global average temperature increase of 4C in a bid to rebuild public trust in climate science,” the UK Telegraph writes here.
Thankfully, trust is real hard to rebuild once lost. And it doesn’t get rebuilt by returning (again) to the old scare-mongering and serial exaggerating. If they truly wish to win back the trust of the public, then they have to start being honest. But they can’t afford that because that would mean the end of their Utopian pipe dream and scam.
And check out this jewel of propaganda made with massaging, über-alarmist Al Gore.

Share this...FacebookTwitter "
"
Share this...FacebookTwitterWhen you have an institution that can longer function because of incompetence, corruption, or whatever human element, it is impossible to repair it without first replacing the bad personnel behind the problem. Anything else is like trying to cure someone with a bad liver by treating everything else except the liver itself.That’s the case at the Met Office. If you truly want to reform it, then it has to be purged of its rotten apples. Piers Corbyn reports on the UK parliament transport select committee, click here, into the December cold & snow, and concludes:
THE MET OFFICE’s submission is, I would say: a Mubarak-style, bunkerish, self-serving, denial of reality.”
He then lists the points why, among them is a comment on the usefulness of seasonal forecasts, which, as we know, their own have been completely wrong. Piers writes (emphasis added):
They say ‘accurate regional forecasts on a monthly scale have proved to be useful. Perhaps they are talking about someone else’s forecasts (eg WeatherAction’s). The observed FACT is Met Office seasonal forecasts have demonstrably negative skill. They have consistently – with zero success in all the last 6 unusual (extreme) seasons – misled the public, emergency services and Councils and led to deaths on unsalted roads consequent on ill-advised Council’s believing MetOffice warmist winter forecasts.”
Piers then writes on the Met Office’s commitment in developing forecasting science, calling it “as delusional as Colonel Gaddafi”. To top it all off, the Met Office then has the temerity to expect the British taxpayers to cough up many more millions for super-computers to “improve their forecasts”.
Everybody knows that no matter how good your computers are, if you feed them with garbage, then you get garbage out. I’d rely on Piers and his laptop long before I’d call the clowns at the Met Office.
In its submission, the Met Office does promise to take the steps needed for generating accurate monthly forecasts for the public, adding:
The extent and speed of this development is, of course, dependent on the availability of resources – particularly in supercomputing power to enable modelling to incorporate new science and understanding…..”.
Never let a crisis go to waste.
Indeed the Met Office needs a good house-cleaning. They are attempting to screw the public yet again. And unless the house does indeed get cleaned, more Bitish lives will be put at risk.
Share this...FacebookTwitter "
"

Congratulations to the first winners of the Cato on Campus Op‐​Ed Contest: Mytheos Holt and Šimon Franěk! Their op‐​eds, both dealing with environmental policy, tied for the December 2008 Cato on Campus Op‐​Ed Contest.   
  
  
Mytheos is a junior at Wesleyan University. Citing Cato Senior Fellow in Environmental Studies Patrick Michaels regarding the dangers of excessive environmentalism, Mytheos’s op‐​ed, “Burning the Greenbacks to Save the Greenhouse,” was published by the California Independent Voter Project.   
  
  
Šimon is an 18‐​year‐​old student at Gymnasium Kladno in the Czech Republic. Citing Patrick Michaels to refocus the global climate debate, Šimon’s op‐​ed, “Climate Change vs. Liberty in Europe,” calls attention to the politics behind global warming and the need to consider the ramifications of policy decisions based on global warming.   
  
  
Mytheos and Simon were each sent autographed copies of Patrick Michaels’ books _Meltdown: The Predictable Distortion of Global warming by Scientists, Politicians and the Media _and _Climate of Extremes: Global Warming Science they Don’t Want You to Know_ (just released in January). Their op‐​eds will also be considered for the Cato on Campus Op‐​Ed of the Year, with a chance of receiving a full scholarship to Cato University.   
  
  
Submit your work to one of three Cato student contests.
"
nan
nan
nan
nan
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   




The role that anthropogenic “global warming” from the emissions of greenhouse gases from the combustion of fossil fuels plays is debatable—both in timing and magnitude. Almost certainly its influence is present and detectable in the U.S. annual average temperature record, but beyond that simple statement, not a whole lot more can be added with scientific certainty.



We now stand nearly a year later with more evidence of proof and point.   
  
  
Through November of this year, the U.S. average temperature is only 0.53°F above the 20th century mean temperature (the default baseline used by NCDC). Last year the annual temperature was 3.24°F above it.   






Figure 1. Average January‐​November temperature in the contiguous United States from 1895–2013 as compiled by the National Climatic Data Center (source: NCDC, Climate at a Glance).   
  
  
With the cold start to December across the country, the annual temperature for 2013 has an increasingly good shot at coming in below the 20th century average. For this to happen, the U.S. temperature for December would have to average about 27.6°F. For the first 12 days of the month, the average has been 28.4°F, and the forecast is for continued cold, so getting to the needed temperature is not out of the question.   
  
  
If 2013 does come in below the 20th century average, it would be the first year since 1996 to have done so, and would end a 16‐​year long run of above average annual temperature for the U.S. You can follow the chase here.   
  
  
But even if the rest of the month is not quite cold enough to push the entire year into negative territory, the 2013 annual temperate will still be markedly colder than last year’s record high, and _will be the largest year‐​over‐​year decrease in the annual temperature on record_ , underscoring the “outlier” nature of the 2012 temperatures.   
  
  
Will 2013 mark the end of the decade and a half period of abnormal warmth experience across the U.S. that was touched off by the 1998 El Niño event, and a return to conditions of the 1980s and early‐​to‐​mid 1990s? Or will 2013 turn out to just be a cold blip in the 21st century U.S. climate?   
  
  
In either case, 2013 shows that the natural variability of annual temperatures in the U.S. is high (as is decadal and multi‐​decadal variability, see Figure 1)—an important caveat to keep in mind when you face the inundation of every‐​weather‐​event‐​is‐​caused‐​by‐​human‐​global‐​warming hysteria.   
  
  
Stay tuned!   
  
  
_The Center for the Study of Science would like to thankRyan Maue of WeatherBELL Analytics for his summary of December temperatures and the expected temperatures for the rest of the year._
"
"In my 2010 book, Crisis Economics, I defined financial crises not as the “black swan” events that Nassim Nicholas Taleb described in his eponymous bestseller but as “white swans”. According to Taleb, black swans are events that emerge unpredictably, like a tornado, from a fat-tailed statistical distribution. But I argued that financial crises, at least, are more like hurricanes: they are the predictable result of builtup economic and financial vulnerabilities and policy mistakes. There are times when we should expect the system to reach a tipping point – the “Minsky Moment” – when a boom and a bubble turn into a crash and a bust. Such events are not about the “unknown unknowns” but rather the “known unknowns”. Beyond the usual economic and policy risks that most financial analysts worry about, a number of potentially seismic white swans are visible on the horizon this year. Any of them could trigger severe economic, financial, political and geopolitical disturbances unlike anything since the 2008 crisis. For starters, the US is locked in an escalating strategic rivalry with at least four implicitly aligned revisionist powers: China, Russia, Iran and North Korea. These countries all have an interest in challenging the US-led global order and 2020 could be a critical year for them, owing to the US presidential election and the potential change in US global policies that could follow. Under Donald Trump, the US is trying to contain or even trigger regime change in these four countries through economic sanctions and other means. Similarly, the four revisionists want to undercut American hard and soft power abroad by destabilising the US from within through asymmetric warfare. If the US election descends into partisan rancour, chaos, disputed vote tallies and accusations of “rigged” elections, so much the better for rivals of the US. A breakdown of the US political system would weaken American power abroad. Moreover, some countries have a particular interest in removing Trump. The acute threat that he poses to the Iranian regime gives it every reason to escalate the conflict with the US in the coming months – even if it means risking a full-scale war – on the chance that the ensuing spike in oil prices would crash the US stock market, trigger a recession, and sink Trump’s re-election prospects. Yes, the consensus view is that the targeted killing of Qassem Suleimani has deterred Iran but that argument misunderstands the regime’s perverse incentives. War between US and Iran is likely this year; the current calm is the one before the proverbial storm. As for US-China relations, the recent phase one deal is a temporary Band-Aid. The bilateral cold war over technology, data, investment, currency and finance is already escalating sharply. The Covid-19 outbreak has reinforced the position of those in the US arguing for containment and lent further momentum to the broader trend of Sino-American “decoupling”. More immediately, the epidemic is likely to be more severe than currently expected and the disruption to the Chinese economy will have spillover effects on global supply chains – including pharma inputs, of which China is a critical supplier – and business confidence, all of which will likely be more severe than financial markets’ current complacency suggests. Although the Sino-American cold war is by definition a low-intensity conflict, a sharp escalation is likely this year. To some Chinese leaders, it cannot be a coincidence that their country is simultaneously experiencing a massive swine flu outbreak, severe bird flu, a coronavirus outbreak, political unrest in Hong Kong, the re-election of Taiwan’s pro-independence president, and stepped-up US naval operations in the East and South China Seas. Regardless of whether China has only itself to blame for some of these crises, the view in Beijing is veering toward the conspiratorial. But open aggression is not really an option at this point, given the asymmetry of conventional power. China’s immediate response to US containment efforts will likely take the form of cyberwarfare. There are several obvious targets. Chinese hackers (and their Russian, North Korean, and Iranian counterparts) could interfere in the US election by flooding Americans with misinformation and deep fakes. With the US electorate already so polarised, it is not difficult to imagine armed partisans taking to the streets to challenge the results, leading to serious violence and chaos. Revisionist powers could also attack the US and western financial systems – including the Society for Worldwide Interbank Financial Telecommunication (Swift) platform. Already, the European Central Bank president, Christine Lagarde, has warned that a cyber-attack on European financial markets could cost $645bn (£496.2bn). And security officials have expressed similar concerns about the US, where an even wider range of telecommunication infrastructure is potentially vulnerable. By next year, the US-China conflict could have escalated from a cold war to a near hot one. A Chinese regime and economy severely damaged by the Covid-19 crisis and facing restless masses will need an external scapegoat, and will likely set its sights on Taiwan, Hong Kong, Vietnam and US naval positions in the East and South China Seas; confrontation could creep into escalating military accidents. It could also pursue the financial “nuclear option” of dumping its holdings of US Treasury bonds if escalation does take place. Because US assets comprise such a large share of China’s (and, to a lesser extent, Russia’s) foreign reserves, the Chinese are increasingly worried that such assets could be frozen through US sanctions (like those already used against Iran and North Korea). Of course, dumping US Treasuries would impede China’s economic growth if dollar assets were sold and converted back into renminbi (which would appreciate). But China could diversify its reserves by converting them into another liquid asset that is less vulnerable to US primary or secondary sanctions, namely gold. Indeed, China and Russia have been stockpiling gold reserves (overtly and covertly), which explains the 30% spike in gold prices since early 2019. In a sell-off scenario, the capital gains on gold would compensate for any loss incurred from dumping US Treasuries, whose yields would spike as their market price and value fell. So far, China and Russia’s shift into gold has occurred slowly, leaving Treasury yields unaffected. But if this diversification strategy accelerates, as is likely, it could trigger a shock in the US Treasuries market, possibly leading to a sharp economic slowdown in the US. The US, of course, will not sit idly by while coming under asymmetric attack. It has already been increasing the pressure on these countries with sanctions and other forms of trade and financial warfare, not to mention its own world-beating cyberwarfare capabilities. US cyber-attacks against the four rivals will continue to intensify this year, raising the risk of the first-ever cyber world war and massive economic, financial and political disorder. Looking beyond the risk of severe geopolitical escalations in 2020, there are additional medium-term risks associated with climate change, which could trigger costly environmental disasters. Climate change is not just a lumbering giant that will cause economic and financial havoc decades from now. It is a threat in the here and now, as demonstrated by the growing frequency and severity of extreme weather events. In addition to climate change, there is evidence that separate, deeper seismic events are under way, leading to rapid global movements in magnetic polarity and accelerating ocean currents. Any one of these developments could augur an environmental white swan event, as could climatic “tipping points” such as the collapse of major ice sheets in Antarctica or Greenland in the next few years. We already know that underwater volcanic activity is increasing; what if that trend translates into rapid marine acidification and the depletion of global fish stocks upon which billions of people rely? As of early 2020, this is where we stand: the US and Iran have already had a military confrontation that will likely soon escalate; China is in the grip of a viral outbreak that could become a global pandemic; cyberwarfare is ongoing; major holders of US Treasuries are pursuing diversification strategies; the Democratic presidential primary is exposing rifts in the opposition to Trump and already casting doubt on vote-counting processes; rivalries between the US and four revisionist powers are escalating; and the real-world costs of climate change and other environmental trends are mounting. This list is hardly exhaustive but it points to what one can reasonably expect for 2020. Financial markets, meanwhile, remain blissfully in denial of the risks, convinced that a calm if not happy year awaits major economies and global markets. • Nouriel Roubini is a professor at NYU’s Stern School of Business and was senior economist for international affairs in the Clinton White House’s Council of Economic Advisers. He has worked for the IMF, the US Federal Reserve and the World Bank. © Project Syndicate"
"

LONDON (Reuters) – Next year is set to be one of the top-five warmest on  record, British climate scientists said on Tuesday.
The average global temperature for 2009 is expected to be more than 0.4  degrees celsius above the long-term average, despite the continued cooling of  huge areas of the Pacific Ocean, a phenomenon known as La Nina.
That would make it the warmest year since 2005, according to researchers at  the Met Office, who say there is also a growing probability of record  temperatures after next year.
Currently the warmest year on record is 1998, which saw average temperatures  of 14.52 degrees celsius – well above the 1961-1990 long-term average of 14  degrees celsius.
Warm weather that year was strongly influenced by El Nino, an abnormal  warming of surface ocean waters in the eastern tropical Pacific.
Theories abound as to what triggers the mechanisms that cause an El Nino or  La Nina event but scientists agree that they are playing an increasingly  important role in global weather patterns.
The strength of the prevailing trade winds that blow from east to west across  the equatorial Pacific is thought to be an important factor.
“Further warming to record levels is likely once a moderate El Nino  develops,” said Professor Chris Folland at the Met Office Hadley Center.  “Phenomena such as El Nino and La Nina have a significant influence on global  surface temperature.”
Professor Phil Jones, director of the climate research unit at the University  of East Anglia, said global warming had not gone away despite the fact that  2009, like the year just gone, would not break records.
“What matters is the underlying rate of warming,” he said.
He noted the average temperature over 2001-2007 was 14.44 degrees celsius,  0.21 degrees celsius warmer than corresponding values for 1991-2000.
(Reporting by Christina Fincher; Editing by Christian Wiessner)


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99d79ecd',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Guest Post by Steven Goddard
January, 1790 was a remarkable year in the northeastern US for several reasons.  It was less than one year into George Washington’s first term, and it was one of the warmest winter months on record.  Fortunately for science, a diligent Philadelphia resident named Charles Pierce kept a detailed record of the monthly weather from 1790 through 1847, and his record is archived by Google Books.  Below is his monthly report from that book.
JANUARY 1790 The average or medium temperature of this month was 44 degrees This is the mildest month of January on record. Fogs prevailed very much in the morning but a hot sun soon dispersed them and the mercury often ran up to 70 in the shade at mid day. Boys were often seen swimming in the Delaware and Schuylkill rivers. There were frequent showers as in April some of which were accompanied by thunder and lightning The uncommon mildness of the weather continued until the 7th of February.
Compare that to January, 2009 with an average temperature of 27F, 17 degrees cooler than 1790.  One month of course is not indicative of the climate, so let us look at the 30 year period from 1790-1819 and compare that to the last 10 “hot” years.
From Charles Pierce’s records, the average January temperature in Philadelphia from 1790-1819 was 31.2F.  According to USHCN records from 2000-2006 (the last year available from USHCN) and Weather Underground records from 2007-2009, the average January temperature in Philadelphia for the last ten years has been 29.8 degrees, or 1.4 degrees cooler than the period 1790-1819.  January, 2009 has been colder than any January during the presidencies of Washington, Adams, Jefferson, or Monroe.  January 2003 and 2004 were both considerably colder than any January during the terms of the first five presidents of the US.  Data can be seen here.
According to several of the most widely quoted climate scientists in the world, winters were much colder 200 years ago than now – yet the boys swimming in the Delaware in January, 1790 apparently were unaware.
Another interesting fact which can be derived from Charles Pierce’s data, is that January temperatures cooled dramatically during the period 1790-1819 – as can be seen in the graph below.  The cooling rate was 13F/century.  What could have caused this cooling?  We are told by some experts that variations in solar activity can only affect the earth’s temperature by a few tenths of a degree.  CO2 levels had been rising since the start of the industrial age.  The downward trend is fairly linear and does not show any sharp downward spikes, so it is unlikely to be due to volcanic activity.  What other “natural variability” could have caused such a dramatic drop in temperature?

Looking at the sunspot records for that period, something that clearly stands out is that solar cycle 4 was very long, and was followed by a deep minimum lasting several decades.  Perhaps a coincidence, but if not – Philadelphia may well be in for some more very cold weather in coming winters.

Source for graph:
http://www.springerlink.com/content/k37032647541h753/fulltext.pdf


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99659303',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
What Lunar Orbiter 1 saw as it looked back at Earth on August 23, 1966. Climate studies of Earth will benefit by a look back in time thanks to decades old view from the Moon. Credit: LOIRP/NASA
From Space.com: Old Moon Images Get Modern Makeover
WOODLANDS, Texas — Think of it as a space age twist to that adage: Something old, something new…something borrowed, something blue.
Back in 1966 and 1967, NASA hurled a series of Lunar Orbiter spacecraft to the moon. Each of the five orbiters were dispatched to map the landscape in high-resolution and assist in charting where best to set down Apollo moonwalkers and open up the lunar surface to expanded human operations.
Imagery gleaned from the Lunar Orbiters over 40 years ago is now getting a 21st century makeover thanks to the Lunar Orbiter Image Recovery Project (LOIRP). 
By gathering the vintage hardware to playback the imagery, and then upgrading it to digital standards, researchers have yielded a strikingly fresh look at the old moon. Furthermore, LOIRP’s efforts may also lead to retrieving and beefing up video from the first human landing on the moon by Apollo 11 astronauts in July 1969.
Digital domain
Dennis Wingo, LOIRP’s team leader, detailed the group’s work in progress during last week’s 40th Lunar and Planetary Science Conference.
Teamed with SpaceRef.com, LOIRP’s saga is one of acquiring the last surviving Ampex FR-900 machinery that can play analog image data from the Lunar Orbiter spacecraft. Wingo noted that the work is backed by NASA’s Exploration Systems Mission Directorate, the space agency’s Innovative Partnership Program, along with private organizations, making it possible to overhaul old equipment, digitally upgrade and clean-up the imagery via software. 
LOIRP is located at NASA’s Ames Research Center at Moffett Field, Calif. There, project members are taking the analog data, converting it into digital form and reconstructing the images.
By moving them into the digital domain, Wingo said, the photos now offer a higher dynamic range and resolution than the original pictures, he added.
“We’re going to be releasing these to the whole world,” Wingo said. 
Use of the refreshed images, contrasted to what NASA’s upcoming Lunar Reconnaissance Orbiter (LRO) mission is slated to produce, has an immediate scientific benefit. That is, what is the frequency of impacts on the Moon’s already substantially crater-pocked surface?
“We’ll be able to get crater counts,” Wingo told SPACE.com. “LRO imagery of the same terrain imaged decades ago will provide a crater count over the last 40 years.” 
Frozen in time
There’s also a more down to Earth output thanks to LOIRP scientists.
They have used a Lunar Orbiter 1 image of the Earth for climate studies, basically a snapshot frozen in time that shows the edge of the Antarctic ice pack on August 23, 1966. 
The team is working with the National Snow and Ice Data Center in Boulder, Colorado to correlate their images of the Earth with old NASA Nimbus 1 and Nimbus 2 spacecraft imagery that flew at about the same time — in the mid-1960s — as the Lunar Orbiter 1. Nimbus satellites were meteorological research and development spacecraft.
Wingo said that the original Nimbus images may have been recorded on an Ampex FR-900 – so by processing the original Nimbus tapes there is a very good chance that they can provide NASA with polar ice pack data from ten years earlier.
Lessons learned
One treasure hunt outing by LOIRP may lead to finding what some term as “lost” Apollo 11 slow scan tapes, Wingo said. 
“We don’t think they are lost. People have been looking for the wrong tapes,” he said, explaining that they were recorded on Ampex FR-900 equipment — not on another type of recorder as previously thought.
Wingo said those Apollo tapes are stored at the Federal Records Center, labeled and ready for a look see.
“We think for the 40th anniversary of Apollo we may be able to get the original slow scan tapes,” Wingo said. If so, the hope is to recover them and give the public a higher-quality, never-before-seen view of human exploration of the Moon.
There is a lesson learned output from LOIRP. 
In the beginning, very few people thought this could be done…but now they have seen the results,” Wingo said.
It is not enough to have 100 year recording medium, Wingo explains. Without the retention of the specific era equipment that images are archived on, it will be impossible for future generations to recover older NASA or other satellite data, he advised.
This is a general issue, not specific to the Lunar Orbiter program. The retention of critical hardware should be a requirement for flight efforts. The original historic Apollo 11 slow scan images have been lost due to inattention to this critical detail, Wingo concluded.
(h/t to Gary Boden)
UPDATE: Dennis Wingo responded in comments, and offers this LA Times story on the real trials and tribulations of this project. 
http://www.latimes.com/news/nationworld/nation/la-na-lunar22-2009mar22,0,931431.story
We owe Mr. Wingo and his team, and especially Nancy Evans, a debt of gratitude for preserving space history against the odds. – Anthony
 


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96e2e2c7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Nearly three‐​fourths (71%) of Americans believe that political correctness has done more to silence important discussions our society needs to have. A little more than a quarter (28%) instead believe that political correctness has done more to help people avoid offending others.



The consequences are personal—58% of Americans believe the political climate today prevents them from saying things they believe. Democrats are unique, however, in that a slim majority (53%) do not feel the need to self‐​censor. Conversely, strong majorities of Republicans (73%) and independents (58%) say they keep some political beliefs to themselves.





Most Americans (59%) think people should be allowed to express unpopular opinions in public, even those deeply offensive to other people. Forty percent (40%) think government should prevent hate speech in public. Nonetheless, an overwhelming majority (79%) agree that it is “morally unacceptable” to engage in hate speech against racial or religious groups. Thus, the public appears to distinguish between _allowing_ offensive speech and _endorsing_ it.





Despite this, the survey also found Americans willing to censor, regulate, or punish a wide variety of speech and expression they personally find offensive:



An overwhelming majority (82%) of Americans agree that it would be difficult to ban hate speech because people can’t agree what speech is hateful and offensive. Indeed, when presented with specific statements and ideas, Americans can’t agree on what speech is hateful, offensive, or simply a political opinion:





African Americans and Hispanics are more likely than white Americans to believe:



However, black, Hispanic, and white Americans agree that free speech ensures the truth will ultimately prevail (68%, 70%, 66%). Majorities also agree that it would be difficult to ban hate speech since people can’t agree on what hate speech is (59%, 77%, 87%).





Two‐​thirds (66%) of Americans say colleges and universities aren’t doing enough to teach young Americans today about the value of free speech. When asked which is more important, 65% say colleges should “expose students to all types of viewpoints, even if they are offensive or biased against certain groups.” About a third (34%) say colleges should “prohibit offensive speech that is biased against certain groups.”



But Americans are conflicted. Despite their desire for viewpoint diversity, a slim majority (53%) also agree that “colleges have an obligation to protect students from offensive speech and ideas that could create a difficult learning environment.” This share rises to 66% among Democrats, but 57% of Republicans disagree.





More than three‐​fourths (76%) of Americans say that recent campus protests and cancellations of controversial speakers are part of a “broader pattern” of how college students deal with offensive ideas. About a quarter (22%) think these protests and shutdowns are simply isolated incidents.



However, when asked about specific speakers, about half of Americans with college experience think a wide variety should not be allowed to speak at their college:



Excluding a speaker who would disrespect police, Democrats are about 15 to 30 points more likely than Republicans to say each of these speakers should not be allowed to speak.





Two‐​thirds (65%) say colleges need to discipline students who disrupt invited speakers and prevent them from speaking. However, the public is divided on how: 46% want to give students a warning, 31% want the incident noted on the student’s academic record, 22% want students to pay a fine, 20% want students suspended, 19% favor arresting students, and 13% want students fully expelled.  
Democrats take a softer while Republicans take a harder approach to handling disruptive college protestors. Nearly two‐​thirds (64%) of Democrats say colleges should listen to and address the students’ concerns, compared to 36% of Republicans. Conversely, Republicans are two to six times as likely as Democrats to support some sort of punishment for the students, such as suspending or expelling them (47% vs. 15%), noting the incident on the students’ records (41% vs. 22%), or having police arrest the students (32% vs. 7%).





Most people support the heckler’s veto. A majority (58%) say colleges should cancel controversial speakers if administrators believe the students will stage a violent protest otherwise. Democrats and Republicans again disagree: Democrats say universities should cancel the speaker (74%) and Republicans say they should not cancel the speaker (54%) if the students threaten violence. 



The survey finds that many microaggressions that colleges and universities advise faculty and students to avoid aren’t considered offensive by most African Americans and Latinos. The percentage who say these microaggressions are _not offensive_ are as follows:



The one microaggression that African Americans (68%) agree is offensive is telling a racial minority, “you are a credit to your race.” Latinos are evenly divided.





A majority (66%) of Americans have heard of safe spaces, but half or less are familiar with other social justice terms and phrases popular on college campuses today, including: cultural appropriation (50%), trigger warnings (49%), “check your privilege” (48%), microaggressions (43%), and “mansplaining” (41%).



In contrast, strong majorities of current college students and graduate students are familiar with all of these words and phrases: safe spaces (86%), cultural appropriation (76%), trigger warnings (75%), “check your privilege” (77%), microaggressions (66%), and “mansplaining” (69%).



Nearly two‐​thirds (65%) of the public say colleges shouldn’t advise students about offensive Halloween costumes and should instead let students work it out on their own. A third (33%) think it is the responsibility of the university to advise students not to wear costumes that stereotype racial or ethnic groups at off‐​campus parties.



A majority of African Americans (56%) believe universities should intervene and advise against offensive costumes. Conversely, a strong majority (71%) of white Americans and a majority of Latinos (56%) believe that college students should discuss offensive Halloween costumes among themselves without administrator involvement.





Only 20% of current college and graduate students believe their college or university faculty has a balanced mix of political views. A plurality (39%) say most college and university professors are liberal, 27% believe most are politically moderate, and 12% believe most are conservative.



Democratic and Republican students see their college campuses differently. A majority (59%) of Republican college students believe that most faculty members are liberal. In contrast, only 35% of Democratic college students agree most professors are liberal.





A slim majority (51%) of Americans oppose, while nearly as many (48%) support, the idea of a confidential reporting system at colleges and universities in which students could report people who make offensive comments about a person’s race, gender, sexual orientation, age, or disability status.



This “bias reporting system,” as it’s often referred to, is highly popular among current students. More than two‐​thirds (68%) of college students and graduate students support it, while less than a third oppose (30%).



Americans tend to oppose firing people for their beliefs or expression. However, Democrats and Republicans differ on what beliefs or expressive acts they believe are fireable offenses:





A majority of Republicans (63%) agree with President Trump that journalists today are an “enemy of the American people.” Conversely, most Americans (64%), as well as 89% of Democrats and 61% of independents, do not view journalists as the enemy.





Despite this, Republicans (63%) agree with most Americans (70%), including Democrats (76%) and independents (71%), that government should not have the power to stop news stories even if officials say they are biased or inaccurate.



Most Americans believe many major news outlets have a liberal bias, including the _New York Times_ (52%), CNN (50%), and MSNBC (59%).1 Fox News, on the other hand, is perceived to have a conservative bias (56%). Americans are divided about whether CBS is balanced (42%) or has a liberal bias (40%). Local news stations are a rare trusted source. A majority (54%) say their local TV station provides balanced news coverage without bias.





Majorities of Democrats believe most major news organizations are balanced in their reporting, including CBS (72%), CNN (55%), the _New York Times_ (55%), as well as their local news station (67%). A plurality (44%) also believe the _Wall Street Journal_ is balanced. The two exceptions are that a plurality (47%) believe MSNBC has a liberal tilt and a strong majority (71%) say Fox has a conservative bias.



Republicans, on the other hand, see things differently. Overwhelming majorities believe liberal bias colors reporting at the _New York Times_ (80%), CNN (81%), CBS (73%), and MSNBC (80%). A plurality also feel the _Wall Street Journal_ (48%) has a liberal bias. One exception is that a plurality (44%) believe Fox News has a conservative bias, while 41% believe it provides unbiased reporting.





The public distinguishes between a business serving people versus weddings:



Few support punishing businesses who refuse service to same‐​sex weddings. Two‐​thirds (66%) say nothing should happen to a bakery who refuses to bake a cake for a same‐​sex wedding. A fifth (20%) would boycott the bakery. Another 22% think government should sanction the bakery in some way, such as by fining the bakery (12%), requiring an apology (10%), issuing a warning (8%), taking away their business license (6%), or sending the baker to jail (1%).





Nearly two‐​thirds (61%) of Hillary Clinton’s voters agree that it’s “hard” to be friends with Donald Trump’s voters. However, only 34% of Trump’s voters feel the same way about Clinton’s. Instead, nearly two‐​thirds (64%) of Trump voters don’t think it’s difficult to be friends with Clinton voters.





Most Americans (59%) say people should be allowed to express unpopular opinions in public, even those that are deeply offensive to other people. A substantial minority (40%), however, say government should prevent people from engaging in hate speech against certain groups in public.





Racial minorities support government banning public hate speech, including 56% of African Americans and 58% of Latinos. Conversely, a majority of white Americans (66%) oppose banning hate speech. 





While solid majorities of Republicans (72%) and independents (60%) oppose government banning hate speech, Democrats stand out with a slim majority in support (52%). However, African American and Latino Democrats largely drive these numbers. A majority (55%) of white Democrats say government should allow public hate speech, but majorities of black Democrats (59%) and Hispanic Democrats (65%) say it should prevent such speech in public.



Thus, the Democratic Party is divided on matters of free speech. White Democrats are more likely to oppose government regulations on speech while black and Hispanic Democrats are more likely to support it.



Current college and graduate students diverge from Americans who have already graduated from college. About half (49%) of current students say government should ban hate speech while the same proportion (49%) say it should not. In contrast, among college _graduates,_ 64% say hate speech should be legal and a third (36%) say it should not.



Using a political typology to identify ideological groups,2 we find that Libertarians (82%) are the most opposed to hate speech laws, followed by Conservatives (75%) and a slim majority (53%) of Liberals. However, nearly two‐​thirds of Populists (64%) say government should prevent hate speech in public.



Altogether, Hispanic and black Americans, Democrats, women, Populists, and college students are most supportive of the government prohibiting public hate speech. Whites, Republicans, independents, men, and Libertarians are most opposed.



Although most Americans say government should not prevent people from engaging in public hate speech, most think hate speech is morally unacceptable. Nearly 8 in 10 (79%) say that it is “morally unacceptable” to “say things that might be offensive to racial or religious groups.”



This indicates that Americans make a distinction between allowing speech and endorsing that speech. Most think that speech that is offensive or insulting toward minority groups should be legally permitted, but that it is still wrong.



More than 8 in 10 Americans (82%) believe that it’s hard to ban hate speech “because people can’t agree what speech is hateful.” Seventeen percent (17%) disagree.



As a later section will show, Americans are sharply at odds over what speech they would personally define as hateful, offensive, or neither. For instance, a majority of Democrats (52%) believe saying that transgender people have a mental disorder is hate speech. Only 17% of Republicans agree. On the other hand, 42% of Republicans believe it’s hateful to say that the police are racist, while only 19% of Democrats agree.



Majorities across partisan groups, demographic groups, college students, and non‐​college students alike agree that hate speech is hard to define and thus may be hard to regulate.



Most Americans (75%) are aware that making racist statements in public is legal under the First Amendment. However, a substantial minority—24%—think hate speech is currently prohibited by law.



Unsurprisingly, those with less education are more likely to think that hate speech is currently illegal. About a third (32%) of those with high school degrees or less think hate speech is illegal, compared to 19% of college graduates and 13% of those with post‐​graduate degrees.



When asked if Americans might favor banning hate speech against particular groups of people, Americans still oppose such laws. There is, however, relatively more support for banning offensive and insulting speech against African Americans (46%). After that, about 4 in 10 would support banning offensive speech about Jewish Americans (41%), immigrants (40%), armed service members (40%), Hispanics (39%), Muslims (37%), the police (37%), gays, lesbians, and transgender people (36%), and Christians (35%). About a third (32%) would support banning insulting speech about white people.



Interestingly, Democrats favor hate speech protections for some groups more than others. Majorities of Democrats support making it illegal to say offensive or insulting things in public about African Americans (61%) and Jewish Americans (53%). Compared to Republicans, Democrats tend to be more supportive of hate speech laws across the board. Nearly half support hate speech laws for immigrants (49%), gays, lesbians and transgender people (48%), Latinos (46%), and Muslims (45%). About 4 in 10 support such laws for military members (42%), Christians (39%), the police (38%), and a third (33%) support such laws for white Americans.





In contrast, majorities of Republicans tend to more consistently oppose hate speech laws for all the groups included on the survey, with about 3 in 10 in support. However, Republicans are relatively more likely to support banning hate speech against military service members (36%) and the police (36%) but less likely to support such laws for Muslims (25%) and LGBT people (24%).



Racial minority groups are more likely than whites to support hate speech laws for groups across the board, but particularly members of their own racial/​ethnic group. Nevertheless, blacks and Hispanics are more supportive than white Americans of laws banning offensive speech about white Americans as well.



African Americans are most likely to favor a law that bans hate speech against African Americans (62%). Fewer support banning hate speech against Hispanic (53%) and white (41%) Americans.



Latinos are most likely to favor a law that bans hate speech against Latinos (65%). A majority (59%) also favor making offensive speech against African Americans illegal and 47% favor banning hate speech against white Americans.



Whites are comparatively less likely to support banning hate speech against particular racial/​ethnic groups. Nevertheless, whites are most likely to favor a law that bans hate speech against black Americans (39%). A little more than a quarter support banning offensive speech about Latino (28%) and white (26%) Americans.



Hispanic (51%) and black (40%) Americans are also more likely than white Americans (32%) to support making it illegal to say offensive or disrespectful things about the police. This is surprising given that surveys have long shown that African Americans and Latinos view the police more negatively.3 The data reveal that both groups tend to more consistently support laws that restrict offensive public speech about any group, not just some groups.



As one 26‐​year‐​old Hispanic female further explained, “we are all human beings, we must all respect each other equally.” Similarly, a 31‐​year‐​old black male in the survey explained that he supported hate speech laws not only for African Americans “but it should be for everybody because it will stop the hate.”



Women are more likely than men to support hate speech laws for different racial, religious, and other groups—particularly for African Americans.



A majority of women (57%) favor a law that would make it illegal to say offensive or insulting things about African Americans in public while 43% oppose. In contrast, only 36% of men would similarly favor this law while 64% would oppose it.



Majorities of women oppose similar laws for other groups. However, they are about 15 points more likely than men to favor banning hate speech against immigrants (46% vs. 33%), gays, lesbians, and transgender people (45% vs. 27%), the police (45% vs. 28%), Hispanics (45% vs. 30%), Muslims (44% vs. 30%), Jewish people (45% vs. 35%), and Christians (43% vs. 27%).



Besides slurs and biological racism, Americans are strikingly at odds over what speech and ideas constitute hate.4





First, majorities agree that calling a racial minority a racial slur (61%), saying one race is genetically superior to another (57%), or calling gays and lesbians vulgar names (56%) is not just offensive—but is hate speech. Interestingly a majority do not think calling a woman a vulgar name is hateful (43%), but most would say it’s offensive (51%). Less than half believe it’s hateful to say that all white people are racist (40%), transgender people have a mental disorder (35%), America is an evil country (34%), homosexuality is a sin (28%), the police are racist (27%), or illegal immigrants should be deported (24%). Less than a fifth believe it’s hateful to say Islam is taking over Europe (18%) or that women should not fight in military combat roles (15%).



Liberals and conservatives significantly diverge over what speech they define as hateful, offensive, or simply an opinion. (See Appendix B).



Majorities of Americans agree with liberals that slurs and biological racism are hateful. However, majorities do not agree with liberals that it’s hateful to say “transgender people have a mental disorder” (35% of all Americans vs. 59% of liberals) or to call women a vulgar name (43% vs. 54%).



Strikingly, majorities of conservatives don’t think any of these ideas are “hateful” although most consider them “offensive” or hateful. In fact, conservatives are about 40 points less likely than liberals to think that saying transgender people have a mental disorder (17% vs. 59%) or saying racial slurs (43% vs. 81%) are hateful. While strong majorities of conservatives agree these are at least _offensive_ or hateful, they are less likely to equate these phrases and ideas with hate specifically.



Liberals are also more likely than conservatives to view a variety of political opinions and speech as either _offensive_ or hateful.





Liberals are more than 40 points more likely than conservatives to think it is offensive or hateful for a person to say that homosexuality is a sin (90% vs. 47%), women shouldn’t fight in military combat roles (87% vs. 47%), illegal immigrants should be deported (80% vs. 36%), or Islam is taking over Europe (79% vs. 33%). Not even a majority of conservatives find these statements to be offensive or hateful.



Notice that two of these, women fighting in combat roles and deporting illegal immigrants, are _policy positions_ that a substantial number of Americans hold. Yet, to merely express these as political positions would also be viewed as highly offensive to a large share of the population.



Furthermore, President Trump has explicitly advocated for deporting illegal immigrants during his 2016 presidential campaign.5 Thus, a large share of Americans not only disagree with his policy position but also find it highly offensive if not hateful. 



Majorities of conservatives did not find any of the statements included on the survey hateful. However, they were _more likely_ than liberals to find several statements hateful. First, conservatives are about twice as likely as liberals to think it’s hateful to say the police are racist (39% vs. 17%). Second, conservatives are somewhat more likely to believe it’s hateful to say that America is an evil country (39% vs. 29%). Third, conservatives are somewhat more likely than liberals to think it’s hateful to say that all white people are racist (44% vs. 35%).



Most Americans (68%) do not think it’s morally acceptable to use physical violence against Nazis, while 32% say it is morally acceptable.6



However, strong liberals stand out with a slim majority (51%) who say it’s moral to punch Nazis in the face. Only 21% of strong conservatives agree. The survey found liberals were more likely to consider upsetting and controversial ideas “hateful” rather than simply “offensive.” This may help partially explain why staunch liberals are more comfortable than the average American with using violence against Nazis.



Strong liberals’ approval of Nazi‐​punching is not representative of Democrats as a whole. A majority (56%) of Democrats believe it is not morally acceptable to punch a Nazi. Thus, tolerance of violence as a response to offensive speech and ideas is found primarily on the far Left of the Democratic Party.





Approval for punching Nazis also varies with age and race. Millennials (42%) are nearly twice as likely as people over 55 (24%) to say violence is morally justified. African Americans (45%) are also 17 points more likely than whites (28%) and 10 points more likely than Latinos (35%) to say punching Nazis is morally acceptable. Nevertheless, majorities of each of these groups say physical force is not justified, even against a Nazi.



Nearly two‐​thirds (64%) of Americans oppose a law that would make it illegal to deny that the Holocaust happened. About a third (35%) would support banning Holocaust denial. These results put Americans at odds with a number of European countries that have outlawed denying the historicity of the Holocaust.7



Support for banning Holocaust denial varies with ideology. A plurality (50%) of strong liberals support such a law, followed by 43% of liberals, 33% of moderates, 30% of conservatives, and 26% of strong conservatives.



A slim majority (54%) of Americans oppose a law that would ban making sexually explicit statements in public, while 45% would oppose.



Although majorities of Democrats (52%), Republicans (55%), and independents (57%) all oppose such a law, certain demographics would support it.



Women (54%) are more than 20 points more likely than men (36%) to support banning sexually explicit public statements. Hispanics (55%) and African Americans (50%) are also somewhat more likely than white Americans (41%) to support such a ban.



Church attendance also predicts support for banning sexually explicit public statements. A slim majority (52%) of regular churchgoers support such a law, but support declines as church attendance declines. A majority (55%) of those who seldom attend church and nearly two‐​thirds (63%) of those who never attend oppose a ban.



Libertarians (67%) and Liberals (64%) are most opposed to banning sexually explicit language in public. 8 Conservatives also marginally oppose (54%). But Populists stand out, with a majority (57%) who say we should outlaw explicit statements. (See Appendix A).



Americans oppose legal restrictions on hate speech, Holocaust denial, and sexually explicit public statements. However, nearly two‐​thirds (62%) would support a law making it illegal to call for violent protests. A little more than a third (37%) would oppose this law.



Outlawing public calls to violently protest is not controversial. Solid majorities of partisans and demographic groups support prohibiting this type of public speech.



Nearly 6 in 10 liberals (59%) favor a law that would require people to refer to transgender persons by their preferred gender pronouns, not their biological sex. This is in sharp contrast to what Americans overall support. Nearly two‐​thirds (62%) oppose a law requiring people use certain pronouns for transgender people while 37% would support it. Moderates (60%) and conservatives (82%) are highly opposed to such laws, including 59% of conservatives who _strongly_ oppose.





These results are relevant to the cities and states that are moving to fine or jail businesses and landlords who refuse to use transgender people’s preferred pronouns. For instance, California enacted a new law that punishes long‐​term nursing home care staff who refuse to use a resident’s preferred name or pronouns.9 Or in New York City, new regulatory guidance subjects landlords and businesses to fines for refusing to use transgender employees’, customers’, or tenants’ preferred pronouns.10 Americans overall, however, do not support these laws.



While Democrats are more supportive of censorship when it comes to hate speech, Republicans disdain criticizing patriotic symbols like the American flag.



A majority (53%) of Republicans favor stripping a person of their U.S. citizenship if they burn the American flag, while 47% would oppose. These results fit with President Trump’s tweets soon after his presidential election victory in which he called for a “loss of citizenship” to punish flag burning.11





While aligned with Trump, Republicans are out of step with the mainstream: 61% of Americans don’t think we should strip people of their citizenship for flag burning. Thirty‐​nine percent (39%) think revoking a person’s citizenship is a reasonable response to flag burning.



A strong majority of Democrats (71%) and independents (61%) oppose such a proposal. Nevertheless, a non‐​insignificant minority of Democrats (28%) and independents (38%) support stripping citizenship from a flag burner.



Latinos align most with Republicans on this issue: 49% agree flag burners should have their citizenship revoked. Latinos are 22 points more likely than African Americans (27%) and 10 points more likely than white Americans (39%) to support such a policy.



Support for revoking citizenship steadily declines with education. While nearly half (48%) of those with high school degrees or less agree with President Trump, only 29% of college graduates and 20% of post‐​grads agree.



Although the Supreme Court has ruled that flag burning is protected speech under the First Amendment, a majority (58%) of Americans still favor a law banning it while 42% oppose.



Majorities of Republicans (72%) and independents (60%) also favor making it illegal to burn or desecrate the flag. Democrats stand out, with a slim majority (53%) who oppose a flag burning ban.



Hispanic Americans are most in favor (63%) of a ban on flag burning, followed by white Americans (58%). African Americans are divided, with 50% in favor and 49% opposed. Women are also more likely than men to support such a ban (65% vs. 50%).



Libertarians (56%) and Liberals (62%) stand out in opposition to a flag burning ban. 12 In contrast, nearly three‐​fourths of Conservatives (74%) and Populists (69%) support it. (See Appendix A).



In this section, the survey report investigates the public’s assumptions about how free speech operates. We find that Americans believe free speech has both benefits and costs. First, nearly two‐​thirds (67%) think that “freedom of speech ensures the truth will ultimately win out” and 58% say free speech does more to protect minority viewpoints. But also, most believe that speech can turn violent: 53% say hate speech _is_ an act of violence and even more say that hate speech _leads to_ violence against minority groups (70%). Ultimately, a majority (56%) think it’s possible to both ban hate speech and still protect free speech.



There are wide racial and partisan divides over how people think free speech operates. Democrats, African Americans, and Latinos are more likely than Republicans and white Americans to believe that hate speech is violent and allows majority views to crowd out minority viewpoints, that supporting a racist’s free speech right is as bad as being a racist, that people who offend others with their ideas have bad intentions, and that we can simultaneously ban hate speech and protect free speech.





Americans provide a strong endorsement of free speech with 67% who agree that “free speech ensures the truth will ultimately win out.” About a third (32%) do not believe that truth can prevail with the free exchange of ideas.



This concept is non‐​controversial with strong majorities of political partisans and demographic groups who share this belief. However, strong liberals (42%) are more likely than moderates (31%) and strong conservatives (25%) to lack this confidence in free speech.



Most Americans (58%) believe that free speech does more to protect minority viewpoints rather than those of the majority. However, African Americans stand out, with 59% who believe free speech does more to protect majority opinions, rather than views held by a minority of individuals. Nearly two‐​thirds (64%) of white Americans believe free speech primarily protects minority views. Latinos are evenly divided on this question.





Majorities of Democrats (53%), independents (57%), and Republicans (66%) agree that free speech does more to allow for and protect minority views.



However, the Democratic Party is divided. Six in 10 black Democrats believe free speech allows the majority to crowd out minority views, while 6 in 10 white Democrats believe it primarily protects minority views. Latino Democrats are divided with 51% who think free speech primarily protects majority opinions.



College protests of controversial speakers across the country have elevated an idea that deeply offensive speech is like an act of violence.13 A slim majority of Americans appear to endorse these sentiments: 53% say that “hate speech is an act of violence.” Another 46% do not believe that hate speech is violence.



Equating speech with violence is highly controversial and sharply divides Americans by political ideology, race, gender, and age.



While two‐​thirds (66%) of Democrats say hate speech is violence, 58% of Republicans say hate speech is _not_ violence. Independents are split, with 51% who disagree hate speech is tantamount to violence.





African Americans (75%) and Latinos (72%) are nearly 30 points more likely than white Americans (46%) to believe hate speech is violence. Instead, a slim majority (53%) of white Americans believe it is not.



While nearly two‐​thirds (63%) of women believe hate speech is violence, a majority (56%) of men disagree.



Americans under 30 (60%) and seniors (57%) are also more likely than middle‐​aged Americans (35–64) to believe hate speech is violence (49%).



These differences may partially explain why Democrats, students, African Americans, Latinos, and women are more supportive of hate speech laws. Equating hate speech with violence provides a greater justification for restricting it.



One reason why Americans may believe hate speech is violence is that a majority (70%) believe that “hate speech leads to violence against minority groups.” This is a view shared by a majority of partisans and racial/​ethnic groups. Nevertheless Democrats (89%), African Americans (85%), and Hispanic Americans (79%) are more likely to believe this than independents (60%), Republicans (54%), and white Americans (66%).



A variety of campus protestors and social justice activists have argued that society can prohibit hate speech while still protecting Americans’ First Amendment rights to free speech. As Scott Crow, a former Antifa organizer put it, “hate speech is not free speech.“14 Similarly, a widely circulated Wellesley College newspaper staff editorial argued that “shutting down rhetoric that undermines the existence and rights of others” is “not a violation of free speech” because such rhetoric is “hate speech.“15



The survey finds that a majority (56%) of Americans agree with the idea that “society can prohibit hate speech and still protect free speech.” Forty‐​three percent (43%) disagree that society can simultaneously prohibit hate speech and protect free speech.





The idea that society can have both hate speech bans and uphold the First Amendment divides partisans and demographic groups. A majority of Democrats (64%) and independents (54%) think it’s possible. A slim majority (52%) of Republicans think it’s not.



Strong majorities of African Americans (69%), Latinos (71%), and women (64%) believe society can both protect free speech and ban hate speech, but white Americans and men are evenly divided. Current college students and graduate students (62%) are also more likely than college graduates (47%) to believe this can be done.



Nearly two‐​thirds of African Americans (65%) and Latinos (61%) agree that “supporting someone’s right to say racist things is as bad as holding racist views yourself.” About a third (34%) of white Americans agree. This suggests that Americans of color may not believe people are reasoning in good faith when they say we should _allow_ speech even if we strongly _disagree_ with it.





This perspective was on full display at the College of William and Mary when student protestors recently prevented an invited ACLU affiliate from speaking at an event, “Students and the First Amendment.” Protestors explained this was in retaliation for the ACLU’s defense of white nationalists’ free speech rights.16 The Black Lives Matter of William and Mary student group wrote on their Facebook page, where they live streamed their shut down of the event: “We want to reaffirm our position of zero tolerance for white supremacy no matter what form it decides to masquerade in.“17 From these students’ perspective, the ACLU supporting someone’s right to say racist things was as bad as being a racist organization.



Most Democrats (53%) also believe supporting a racist’s free speech rights is as bad as holding racist views. However, the Democratic Party is divided by race. While 72% of black Democrats and 65% of Latino Democrats believe this, only 42% of white Democrats agree. Instead, a majority (57%) of white Democrats don’t believe supporting a racist’s right to free speech is the same as supporting racism. Majorities of independents (57%) and Republicans (66%) agree.



Men and women also disagree about whether supporting the right to speak is the same as endorsing its content. Nearly two‐​thirds (65%) of men don’t believe supporting free speech rights is the same as supporting the speech’s content. But a slim majority (51%) of women believe that it is.



A slim majority (51%) of current college students and graduate students believe a person doesn’t deserve the right of free speech if they don’t respect other people. In contrast, a majority (55%) of Americans overall don’t think a person should lose their free speech rights even if they don’t respect others.



There is also a wide race gap here. Six in 10 African Americans (59%) and Hispanics (62%) believe people don’t deserve the right of free speech if they don’t respect others, compared to 36% of white Americans. Instead a majority (62%) of white Americans think even disrespectful people should retain their free speech rights.



A majority (58%) of Americans believe people “usually have bad intentions when they express offensive opinions.” Forty‐​one percent (41%) disagree that people who offend others with their ideas usually have nefarious motives.



Democrats (69%) are 22 points more likely than Republicans (47%) to believe that people have bad intentions when they express offensive opinions. Instead, most Republicans (52%) think people may mean well even when they share an opinion others find offensive.



Latinos (75%) and African Americans (70%) are also about 20 points more likely than white Americans (52%) to think people usually have bad intentions when expressing offensive ideas.



Populists and Liberals are the most likely to believe (67%) that people who express offensive opinions have nefarious motives.18 Libertarians are the polar opposite, with 67% who do _not_ think offensive ideas imply hurtful intentions. Conservatives are evenly divided.



On the campaign trail, then‐​candidate Donald Trump contended: “I think the big problem this country has is being politically correct.“19 A strong majority of Americans (70%) agree with this sentiment. Even though the survey did not attribute the quote to President Trump, fully 90% of Republicans and 78% of independents agree. Democrats are evenly divided.



Why do many people believe political correctness is a problem? Why do others believe it is necessary? Nearly three‐​fourths (71%) of Americans say that political correctness has done more to silence important discussions our society needs to have. Conversely, a little more than a quarter (28%) think that political correctness does more to help people avoid offending others.



Strong majorities of white Americans (74%), African Americans (64%), and Latinos (58%) agree that political correctness has silenced necessary conversations. Overwhelming majorities of Republicans (89%) and independents (80%) also agree.



Far fewer Democrats believe political correctness has done more to silence necessary discussions (50%) than reduce offense. Liberal Democrats are driving these numbers. More than two‐​thirds (68%) of strong liberals believe political correctness primarily helps reduce offense. In stark contrast, nearly 9 in 10 strong conservatives (87%) say it primarily silences conversations society needs.



Most Americans self‐​censor their political opinions because they’re afraid they might offend someone. Nearly 6 in 10 (58%) report that the “political climate” these days prevents them from saying what they believe “because others might find them offensive.” Four in 10 don’t feel the need to censor their opinions.



The political climate appears to favor liberal Democrats, as they are among the few groups who feel they _do not_ need to censor their opinions. However most other political and demographic groups do self‐​censor.



Strong liberals are the most comfortable sharing their true beliefs (69%). Far fewer strong conservatives (24%) and moderates (41%) agree. Similarly, Democrats (53%) are more likely than Republicans (26%) and independents (39%) to feel they can express their opinions. Instead, nearly three‐​fourths (73%) of Republicans and 58% of independents are afraid to share some of their true beliefs because of the political climate.



Why are Republicans more afraid than Democrats to share their views in this “political climate” given that Republicans currently control both Congress and the White House? Perhaps political power does not solely determine the political climate. Cultural sources of power, such as media, academia, and entertainment may matter more. The survey found that Americans believe most large media outlets, like the _New York Times_ (52%) and CNN (50%), have a liberal bent. A plurality (45%) also believe college faculties are mostly liberal. These institutions may shape the political environment such that liberals feel more comfortable sharing their political views.



But perhaps, one might argue, liberals feel more comfortable sharing their political opinions because their views are less offensive. However, the survey found several instances where conservatives are more offended than liberals by political views more commonly held among liberals. For instance, conservatives are about twice as likely as liberals to say calling the police racist is hate speech (39% vs. 17%). Conservatives are also somewhat more likely to believe it’s hateful to say that America is an evil country (39% vs. 29%). Conservatives are also more offended than liberals by flag burning and NFL players refusing to stand for the national anthem.



There are certain topics that Americans feel less inclined to discuss with others in their social surroundings, such as over dinner with co‐​workers or with classmates.



In such an environment, less than half of Americans would be “very willing” to discuss gay and lesbian issues (45%), race relations (45%), women’s issues (48%), and foreign policy (48%).20 Only about half would be similarly willing to discuss issues related to immigration (51%), the police (51%), abortion (52%), or poverty (53%). Americans are somewhat more willing to discuss the environment (64%), health care (61%), education (60%), crime (58%), and gun issues (56%).



There are some issues Democrats feel more comfortable discussing than Republicans and vice versa. Compared to Republicans, Democrats are more likely to say they’d be very willing to discuss women’s issues (57% vs. 41%), gay and lesbian issues (52% vs. 37%), poverty (57% vs. 47%), race relations (50% vs. 40%), and the environment (69% vs. 62%). Conversely, Republicans feel relatively more comfortable than Democrats talking about crime (63% vs. 54%) and gun issues (60% vs. 52%). Across the board, however, Democrats are more willing than Republicans to discuss major policy issues.



The survey also asked respondents to use their own words to describe political beliefs they hold, but feel unable to share because of the political climate. Even though Democrats are more likely than Republicans to feel comfortable sharing their opinions, Americans of all political stripes have views they censor. A sampling of these opinions can be found in the box “Dangerous Ideas vs. Approved Beliefs” below.



Liberals, particularly those in conservative areas, feel they can’t express secular beliefs, their dislike of Donald Trump, support for immigration, gun control, police reform, ending the Drug War, and LGBT rights, and a belief that racism continues in America today.



Conservatives, particularly those in liberal areas, feel they can’t share their religious beliefs, support for Trump, patriotism, a belief that racial minorities receive special privileges in society, opposition to illegal immigration, affirmative action, same‐​sex marriage, and abortion, and support for the border wall, gun rights, free speech, deportation of unauthorized immigrants, and more rigorous security screening for Muslims entering the United States.



Notably, liberals also self‐​censored conventionally conservative sentiments. These included: indifference to identity politics, a belief that racial minorities receive favoritism, support for free speech, and opposition to “PC culture” and removing Confederate statues.







Even though Democrats are more likely than Republicans to feel comfortable sharing their opinions, Americans of all political stripes have political views they feel can’t be expressed. The survey asked people to use their own words to describe what views they feel can’t be shared. Location matters a lot. Liberals in conservative areas and conservatives in liberal cities both self‐​censor.21



Nearly two‐​thirds (61%) of Americans believe that “people often call others racist or sexist to avoid having to debate with them.” More than a third (37%), however, say “people usually only call someone out for racism or sexism when they deserve it.”





A slim majority (51%) of Democrats believe that calling out racism or sexism is typically justified and not an avoidance tactic. In sharp contrast, about three‐​fourths (76%) of Republicans and two‐​thirds (65%) of independents believe it’s primarily used as a tool to stifle debate.



A majority (58%) of African Americans believe that a person called out for racism or sexism usually deserves it, while 41% think that such labels are often used to avoid discussion. Whites (66%) and Latinos (55%) are 14–25 points more likely to believe these labels are primarily used to suppress debate.





Nearly two‐​thirds (61%) of Clinton voters agree that “it’s hard to be friends with people who voted for Donald Trump” while 38% disagree. Trump voters don’t feel a similar animus toward Clinton voters. Instead, a majority (64%) of Trump voters do not think that it’s hard to be friends with Clinton voters while 34% believe it is difficult.





Two‐​thirds of Americans (66%) say colleges and universities aren’t doing enough today to teach young Americans about the value of free speech. This is a view shared by 51% of current college and graduate students, while 46% think colleges are doing enough.



When asked which is more important, 65% say colleges should expose students to “all types of viewpoints even if they are offensive or biased against certain groups.” About a third (34%) say colleges should “prohibit offensive speech that is biased against certain groups.”





Strong liberals (52%), African Americans (54%), and Latinos (54%) stand out with slim majorities who believe it’s more important for colleges to prohibit offensive and biased speech on campus. Conversely, majorities of regular liberals (66%), conservatives (73%), and white Americans (73%) think colleges need to expose students to a wide variety of perspectives even if they are offensive or prejudiced.



But Americans are conflicted. While most say colleges need to prioritize viewpoint diversity, a slim majority (53%) also agree colleges have “an obligation to protect students from offensive speech and ideas that could create a difficult learning environment.” Problems arise, as evidenced earlier in the report, when students disagree about what speech is offensive and would create a difficult learning environment.



Americans are divided by race, party, gender, and education. Nearly three‐​fourths of Latinos and African Americans (74%) agree colleges need to protect students from offensive ideas that could disrupt the learning environment. Less than half (44%) of white Americans agree. While a solid majority of Democrats (66%) believe colleges have this obligation, majorities of Republicans (57%) and independents (51%) do not believe colleges should do this.





Men and women are also divided. A majority of men (56%) don’t think colleges should protect their students from offensive ideas while 64% of women think colleges should.



With more education, Americans become more averse to colleges shielding students from offensive speech even if it risks disrupting the learning environment. Six in 10 Americans (61%) with high school degrees or less think colleges should protect students from offensive ideas, compared to 44% of those with college degrees and 37% of post‐​graduates.



Although Americans say it’s more important for colleges to expose students to a variety of diverse viewpoints, even offensive ones, many are willing to shut down speech they personally find offensive. About half of Americans who have college experience don’t think a wide variety of speakers should be allowed to speak at their university.22





An overwhelming share (81%) of respondents with college experience agree that campus speakers who advocate for violent protests shouldn’t be allowed to speak at their university. Nearly two‐​thirds (65%) oppose a speaker who would reveal the names and identities of unauthorized immigrants attending the college. A solid majority (57%) would also oppose allowing any speaker who says the Holocaust did not occur. About half would oppose allowing a speaker who says all white people are racist (51%), that Muslims shouldn’t be allowed to come to the U.S. (50%), that transgender people have a mental disorder (50%), or that gays and lesbians should receive conversion therapy (50%). Nearly half would support cancelling a speaker who says all Christians are backward and brainwashed (49%), who publicly criticizes or disrespects the police (49%), who defends the police stopping African Americans at higher rates than other groups (48%), or says the average IQ of whites and Asians is higher than African Americans and Hispanics (48%), says all illegal immigrants should be deported (41%), or says men on average are better at math than women (40%). (Results are similar among Americans without college experience who were asked if the aforementioned speakers should be allowed to speak in their local community. (See Appendix C.))



The reader may notice that most of these hypothetical speakers are taken from real‐​world examples of controversial campus speakers or other public figures who could be invited to speak on a college campus. (Note that several of these campus speakers were not shut down because of controversial ideas they planned to include in their speech but for things they have said in the past.) If campus presidents agreed to cancel speakers that large numbers of their student body and faculty found offensive, these results imply they would have to prohibit a wide range of speakers including:



Major differences emerge between Democrats and Republicans in their willingness to allow controversial and offensive speakers speak on campus. Even on issues in which one might expect Republicans to be more offended, they were less likely than Democrats to support cancelling the speaker. Majorities of Democrats would not allow, while Republicans would allow, a speaker who:



There is also a wide racial gap between white Americans and black and Hispanic Americans in allowing these speakers to come to campus. Majorities of black and Hispanic Americans would not allow, while white Americans would allow, a speaker who:



Majorities of black, white, and Hispanic Americans all oppose allowing a speaker who would reveal the names of unauthorized immigrants on campus, deny the Holocaust, or call for violent protests.





Men and women are similarly divided, with majorities of men supportive of nearly all these speakers being allowed to speak on campus and women opposed. Young Americans are also more averse to allowing these speakers to speak at their college or university, compared to older Americans.



Taken together, Republicans, white Americans, men, and older people are more supportive than Democrats, African Americans, Latinos, women, and younger people of allowing these campus speakers to speak at their college or university. Why are these latter groups more supportive of censoring speech? Perhaps because they are more likely to believe that colleges have an obligation to protect students from offensive ideas.



About two‐​thirds (64%) of current college and graduate students say that if their college or university hosted a speaker who believes some races are superior to others, they would not attend the speech. Sixteen percent (16%) say they would attend the speech. Many would also take action: 43% would attend the speech and ask the speaker tough questions, 39% would hold a counter‐​event in a different location, 26% would hold a protest outside of the speech location.



Notably, few students would try to forcibly shut down the speech by shouting loudly so the speaker cannot speak (7%) or by forcibly removing the speaker from the stage (7%).33 Although most wouldn’t use shouting or physical force to stop an offensive speech, more than a third (36%) would sign a petition to get the speech cancelled beforehand.



Democratic and Republican students say they’d handle the situation differently. Democratic students are more likely than Republicans to say they’d hold a counter‐​event in a different location (50% vs. 33%), protest outside (38% vs. 15%), or sign a petition beforehand to get the speech canceled (48% vs. 22%). On the other hand, Republican students are somewhat more likely to say they’d attend the speech and ask tough questions (53% vs. 44%) or simply attend the speech (25% vs. 15%).



More than three‐​fourths (76%) of Americans say that recent student protests and cancellations of controversial speakers on college campuses are part of a “broader pattern” of how college students respond to controversial ideas. About a quarter (22%) believe these protests and cancellations are isolated incidents, not indications of a broader pattern.



This perception is not controversial. Strong majorities of current students and non‐​students alike believe recent shut downs of campus speakers tell us something broader about how students deal with offensive ideas.



Nearly two‐​thirds (65%) of Americans say that colleges and universities should discipline college students who disrupt invited campus speakers and prevent them from speaking.



Republicans are most likely to support disciplining students (83%); 67% of independents agree. Democrats on the other hand are evenly divided over whether colleges should punish students who shut down speakers (50%). White Americans (71%) are also more likely than Latinos (51%) and African Americans (49%) to support disciplining these students.



When asked how specifically colleges and universities should handle disruptive college protestors, Americans are less resolute. A plurality (50%) say that first, colleges and universities should listen and address the students’ concerns. After that, 46% want colleges to give students a warning, 31% say colleges should note the incident on the students’ records, 22% say students should pay a fine, 20% say colleges should suspend students for 30 days, 19% want the police to arrest the students, 13% want colleges to completely expel the students, 11% want to suspend students for a semester. Only 6% say colleges should do nothing.





Democrats take a softer while Republicans take a harder approach to handling disruptive college protestors. Nearly two‐​thirds (64%) of Democrats say colleges should listen to and address the students’ concerns, compared to 36% of Republicans who agree. Conversely, Republicans are two to six times as likely as Democrats to support some sort of punishment for the students, such as noting the incident on the students’ records (41% vs. 22%); supporting suspending or expelling the students (47% vs. 15%); or having police arrest the students (32% vs. 7%).



Ultimately 75% of Republicans would impose at least one of the listed punishments, compared to less than half (42%) of Democrats. Most Democrats would rather listen and address the students’ concerns or give them a warning. Given that research shows most of academia leans left of center, this might help explain why few universities have punished students who have shut down controversial campus speakers.34



Most Americans would accede to the heckler’s veto. A solid majority (58%) of Americans think college administrators should cancel controversial invited campus speakers if students threaten to stage a violent protest. Four in 10 think colleges should move forward with the invited speaker regardless.



Democrats and Republicans disagree about how to respond to threats of student violence: 74% of Democrats think colleges should cancel such controversial speakers while 54% of Republicans think colleges should not cancel the speech.



A slim majority of men (51%) believe colleges should resist student threats. Conversely, more than two‐​thirds (67%) of women think colleges should cancel speakers if students threaten violent protest.



  
  
  
  
  
  
  




A slim majority (51%) of Americans oppose while nearly as many (48%) support the idea of a confidential reporting system at colleges through which students could report people who make offensive comments about a person’s race, gender, sexual orientation, age, or disability status.



This “bias reporting system,” as it’s often described, is highly popular among current students. More than two‐​thirds (68%) of current college students and graduate students support it while less than a third oppose (30%). However, 63% of those who have already graduated from college oppose a system to allow students to report bias on campus.



A bias reporting system is highly divisive along partisan and demographic lines. Solid majorities of Democrats (60%), African Americans (67%), Latinos (59%), and women (54%) support it. Conversely, majorities of Republicans (64%), white Americans (57%), and men (58%) oppose it.



The survey finds that many microaggressions that colleges and universities advise faculty and students to avoid aren’t considered offensive by most people of color.35 The survey included a variety of statements that major universities have identified should be avoided because the colleges contend they “communicate hostile, derogatory, or negative messages to target persons based solely upon their marginalized group membership.“36 However, most African Americans and Latinos do not find most of these statements offensive.





Strong majorities of African Americans and Latinos say the following statements are _not offensive_ :



Seventy percent (70%) of Asian Americans do not think it’s offensive to ask an Asian person, “where are you from?” (The sample size for Asian Americans is small and thus their responses are not shown separately for each of these microaggressions.)37



The one microaggression that African Americans (68%) agree is offensive is telling a racial minority “you are a credit to your race.” Latinos are evenly divided on this question.



There may be other microaggressions not included on the survey that these groups find derogatory. However, African Americans and Latinos do not find most of the key microaggressions identified in academic training manuals insulting.



Two years ago at Yale, a controversy erupted over a series of emails about offensive Halloween costumes. A resident advisor and Yale lecturer pushed back against an email from college administrators advising students not to wear offensive Halloween costumes. The advisor emailed her students and expressed confidence in students’ capacity to discuss offensive Halloween costumes among themselves without administrators getting involved. Many students interpreted her email as an endorsement of offensive costumes, rather than of freedom of expression and the ability of people to discuss and resolve offense without oversight. What do Americans think?



The survey finds that nearly two‐​thirds (65%) of Americans agree that “college students should discuss offensive costumes among themselves without administrators getting involved.” A third (33%) say “college administrators have a responsibility to advise college students not to wear Halloween costumes that stereotype certain racial or ethnic groups at off‐​campus parties.”



A significant racial divide emerges about how to handle offensive Halloween costumes. A majority (56%) of African Americans feel college administrators should intervene and advise students against offensive costumes. Conversely, a strong majority (71%) of white Americans and a majority of Latinos (56%) believe that college students should discuss offensive Halloween costumes among themselves without administrator intervention.



A majority (54%) of college and graduate students agree that students should discuss offensive costumes without intervention from school authorities. However, students (45%) are 12 points more supportive than Americans overall (33%) of administrators advising about offensive costumes.



About two‐​thirds to three‐​fourths of college students and graduate students are familiar with the new language of social justice terms and phrases that have emerged on college campuses. However, most Americans overall are unfamiliar with these words and phrases. The one exception is “safe spaces,” which two‐​thirds of the general public and 86% of current students have heard something about them.





Most Americans (55%) and current college and graduate students (55%) say college newspapers should not need approval from college administrators before printing controversial news stories and editorials. However, nearly two‐​thirds of African Americans (63%) and a majority of Hispanic Americans (54%) think student papers should get approval before printing controversial stories. In contrast, 61% of white Americans don’t think student papers should need approval.



Similar majorities of Democrats (56%), independents (55%), and Republicans (54%) oppose requiring that student papers get permission before printing controversial stories. However, Democrats are divided along racial lines. More than two‐​thirds (68%) of white Democrats do not believe such permission should be necessary while 65% of black Democrats and 57% of Hispanic Democrats believe it should be.



Men and women are also divided. Nearly two‐​thirds of men (63%) do not believe controversial news stories in student papers should need approval while 51% of women think they should.



 **The Faculty** Only 20% of current college students and graduate students believe their college or university faculty has a balanced mix of political views. A plurality (39%) of current students agree that most college and university professors are liberal. Twenty‐​seven percent (27%) believe most are politically moderate, and 12% believe most are conservative.



Democratic and Republican college students see their campuses very differently. A majority (59%) of Republican college students believe that most faculty members are liberal. In contrast, Democratic college students are 25 points less likely to believe that most of the faculty is liberal (35%). Democratic students are also about twice as likely as Republican students to think their professors are moderate (32% vs. 16%) or conservative (14% vs. 9%).



 **The Students** Current students believe that most of their campus’ student body is liberal. Fifty‐​percent (50%) believe that most students at their college or university are liberal, 21% believe most are moderate, 8% believe most are conservative, and 19% believe there is a balanced mix of political views. Democratic and Republican students largely agree on the ideological composition of their campus student body. 



In sum, there is a widespread perception that most faculty and students in colleges are liberal. These results matter because if universities become political echo chambers, it could lead to the exclusion of non‐​conforming political views, self‐​censorship, and less rigorous academic inquiry. Without a free exchange of ideas, there may be less thorough checking of academic work and the quality of research may decline. By extension, the public may lose confidence in the process of academic inquiry and become skeptical of its results.



Although many Americans favor silencing offensive speakers on college campuses and in local communities, most oppose firing people for their political beliefs or expression.



Nearly two‐​thirds (61%) of Americans oppose firing NFL (National Football League) players who refuse to stand for the national anthem before football games in order to make a political statement. These results stand in contrast to President Trump’s urging NFL teams to fire players who refuse to stand for the anthem. A little over a third (38%) of Americans align with Trump and support firing these players.38





Conservative Republicans stand out with their support for firing NFL players who refuse to stand for the national anthem. Nearly two‐​thirds (65%) of Republicans say NFL players should be fired for this reason. Only 19% of Democrats and 35% of independents agree. Punishing NFL players for their political speech distinguishes political Conservatives from Libertarians. Using a political typology to identify these ideological groups, the survey finds that Conservatives (62%) are the only political group to support firing NFL players. Conversely, 60% of Libertarians, 85% of Liberals, and 62% of Populists all oppose firing players. 39



People who are older, with less education, and living in smaller towns and rural communities are most likely to support punishing players who refuse to stand for the national anthem.



A majority (57%) of Americans over 65 think such players should be fired while 71% of Americans under 30 think they should not. Those without college degrees (44%) are more likely than college graduates (32%) and those with post‐​graduate degrees (26%) to support punishing NFL players who engage in this form of political protest. Americans living in rural communities are divided equally over whether teams should fire NFL players who refuse to stand for the national anthem. Conversely, those living in large urban centers solidly oppose (69%) such firings.



Majorities across racial groups oppose firing NFL players who kneel during the national anthem before football games. However African Americans (88%) are about 30 points more likely than Hispanic Americans (60%) and white Americans (55%) to oppose. 



Not wanting to fire NFL players because of their political expression doesn’t mean that most people necessarily agree with the content of that expression. As surveys have long found, including this one, the public opposes desecrating or disrespecting patriotic symbols, like the American flag. It’s likely such views extend to the national anthem as well. Thus, many appear to make a distinction between allowing expression and endorsing its content. Americans can be tolerant of players’ refusing to stand for the national anthem, even if they don’t agree with what the players are doing.



Most Americans (55%) don’t think a business executive should be fired from their job if they burn an American flag as part of a weekend political protest. However, a majority (54%) of Republicans think an executive should be fired for flag burning on the weekend. A plurality (50%) of Hispanics agree with Republicans that such an employee should be fired. In contrast, majorities of Democrats (61%), independents (57%), white Americans (56%), and African Americans (57%) don’t believe this should be a fireable offense.



A slim majority (53%) of Americans say that business employers should not discipline their employees for posting controversial or offensive opinions on social media accounts like Facebook. Forty‐​six percent (46%) think businesses should.





Democrats stand out with 58% who say businesses _should_ discipline their employees for offensive Facebook posts. In contrast, 60% of Republicans and 62% of independents think employees shouldn’t be punished at work for what they write online.



There is also a racial divide. A majority (59%) of African Americans think employees should be subject to discipline at work for their social media posts, while 56% of whites think they should not. Latinos are evenly divided.



Majorities of Americans don’t want to fire people from their jobs because of their political beliefs. But, the public is most likely to support firing an executive who believes that African Americans are genetically inferior (46%). About a quarter to a third support firing business executives who believe that all white people are racist (35%), believe transgender people have a mental disorder (30%), believe men are better at math than women (26%), believe psychological differences help explain why there are more male than female engineers (25%), or believe homosexuality is a sin (22%).



Besides a belief in biological racism, majorities of Democrats and Republicans oppose firing business executives for these other beliefs. Nonetheless, Democrats are considerably more likely than Republicans to support doing so. Democrats are about three times more likely than Republicans to support firing an executive if they believe transgender people have a mental disorder (44% vs. 14%) or believe homosexuality is a sin (32% vs. 10%). Democrats are twice as likely as Republicans to support firing an employee if they believe psychological differences help explain why there are more male engineers (34% vs. 14%), or that men are better at math than women (35% vs. 17%). Democrats and Republicans are more similar in their support for firing executives who believe all white people are racist (40% vs. 33%).





We find that the more strongly a respondent identifies as liberal the more supportive they are of firing people for each of these beliefs. However, the more strongly a respondent identifies as conservative the more likely they are to support firing a person for burning an American flag or firing an NFL player for refusing to stand for the national anthem. Thus, Americans become more likely to support firing people for offensive beliefs and expressions the more ideological—either liberal or conservative—they become.





Some of these results are surprising given that they test the boundaries of tolerable beliefs in the workplace. For instance, one might have expected that a belief in biological racism would be grounds for firing a business executive in charge of fostering merit and talent among all employees. Nevertheless, most Americans oppose firing someone for this belief.



Furthermore, few Americans wish to fire executives for their beliefs about homosexuality or differences between men and women. These results imply that high‐​profile firings in recent years of Silicon Valley executives and employees for these reasons, such as Brendan Eich at Mozilla or James Damore at Google, do not reflect the demands of the public at large.



Early in his presidential tenure, Donald Trump tweeted that the national news media is “fake news” and that it is an enemy of the American people.40 Nearly two‐​thirds (64%) of Americans do not agree with President Trump that journalists today are an “enemy of the American people.” Thirty‐​five percent (35%) side with the president.



However, nearly two‐​thirds (63%) of Republicans agree that journalists are an enemy of the American people. Such a charge is highly polarizing: 89% of Democrats and 61% of independents disagree.





Although Republicans think that the national news media is a threat, they don’t believe government ought to regulate news stories, even if biased or inaccurate. Strong majorities of Republicans (63%), independents (71%), and Democrats (76%) agree that “government should not be able to stop a news media outlet from publishing a story that government officials say is biased or inaccurate.”



Among all Americans, 70% say government should not shut down news stories regardless of whether officials think the story is inaccurate. A little more than a quarter (29%) think government should have the authority to stifle stories authorities say are inaccurate or biased.





While Republicans stand out with their negative view of the media, Democrats have uniquely positive evaluations of it. A slim majority (52%) of Democrats say the national news media is doing a good or even excellent job “holding government accountable.” In contrast, only 24% of independents and 16% of Republicans agree.



Among all Americans, only a third (33%) agree the news media is doing its job holding government accountable. More than two‐​thirds (67%) say it is not. Even more Republicans (84%) and independents (75%) share such negative views of the media.



The more a person identifies as liberal the more likely they are to say the media is doing a good job. Among strong liberals, 59% say the national news media is doing a good or excellent job holding government accountable. In contrast, 87% of strong conservatives say it’s doing a poor or fair job.



Why do Republicans lack confidence in the national news media while Democrats view it positively? Perhaps because most Americans perceive a liberal bias among most major news organizations.41





Fifty‐​two percent (52%) of respondents say that the _New York Times_ allows a liberal bias to color its reporting. Fifty percent (50%) feel CNN also succumbs to a liberal media bias. Fifty‐​nine percent (59%) say that MSNBC also has a liberal bias. Of all the top news organizations included on the survey, only Fox News was perceived to have a conservative bias (56%).



Americans feel their local news stations and broadcast news channels do a better job than cable news in providing balanced reporting. A majority (54%) say their local news station is balanced, without a liberal or a conservative bias. A plurality (42%) also believe that CBS is balanced. Nevertheless, respondents were four times as likely to say CBS has a liberal bias than a conservative bias (40% vs. 10%), and almost twice as likely to say their local station has a liberal bias (23% vs. 14%).



Majorities of Democrats believe most major news organizations are balanced in their reporting, including CBS (72%), CNN (55%), the _New York Times_ (55%), as well as their local news station (67%). A plurality (44%) also believe the _Wall Street Journal_ is balanced. The two exceptions are that a plurality (47%) believe MSNBC has a liberal bias (37% believe it’s unbiased) and a strong majority (71%) say Fox has a conservative bias.



Republicans, on the other hand, see things differently. Overwhelming majorities believe liberal bias colors reporting at the _New York Times_ (80%), CNN (81%), CBS (73%), and MSNBC (80%). A plurality also feel the _Wall Street Journal_ (48%) has a liberal tilt. Only when evaluating their local TV news station do most Republicans, but not a majority, perceive balanced reporting (42%). Similar to Democrats’ perceptions of MSNBC, a plurality of Republicans (44%) believe Fox News has a conservative bias; 41% believe it provides unbiased reporting.





The news outlets that Republicans find most objective are their local news station (42%), Fox (41%), and the _Wall Street Journal_ (28%). The media organizations Democrats find most objective include CBS (72%), their local news station (67%), CNN (55%), and the _New York Times_ (55%).



Who cares more about protecting religious liberty in the United States? It depends on whose liberty is at stake. Republicans tend to care more about protecting the conscience of religious bakers, florists, and other wedding‐​related businesses who refuse service to same‐​sex weddings. On the other hand, Democrats care more about ensuring Muslims have the right to build mosques in their communities.



Americans make a distinction between requiring businesses with religious objections to serve gay and lesbian _people_ and providing custom services to same‐​sex _weddings._



While 50% of Americans say businesses with religious objections should be required to provide services to gays and lesbians, only 32% think a baker should be required to bake a special‐​order cake for a same‐​sex wedding. Instead 68% say a baker should not be required to bake a custom wedding cake if doing so violates their religious convictions.



Majorities of Democrats say a business should be required to provide service to both LGBT people (73%) and bake a custom cake for same‐​sex weddings (52%), even if doing so violates the business owner’s religious beliefs. Conversely, majorities of Republicans say business owners should not be required to provide services in either situation, either to LGBT people (77%) or for same‐​sex weddings (87%).



Most Americans (73%) do not view baking a special‐​order wedding cake for a same‐​sex wedding as an endorsement of same‐​sex marriage. About a quarter (26%) do view it as an endorsement. However, Republicans (41%) are 28 points more likely than Democrats (13%) to view baking the cake as an endorsement of the marriage.



Evangelical Protestants are also more likely to believe (42%) that baking a custom cake for a same‐​sex wedding would be an endorsement of that wedding. In contrast, about a quarter of Mainline Protestants (26%), Catholics (27%), or other religious groups (28%) view it as an endorsement. Only 14% of non‐​religious people agree.



These data suggest that one reason Americans may disagree about requiring businesses service same‐​sex weddings is they don’t agree on what providing those services means. For some Americans, it would require them violate their conscience, while it would not appear that way to others.



What should happen to a religious baker who refuses to bake a special‐​order cake for a same‐​sex wedding? Most Americans (66%) say nothing should happen to the baker. Alternatively, a fifth (20%) would support a boycott of the bakery and 22% would support some kind of government punishment including: issuing a fine (12%), requiring an apology (10%), issuing a warning (8%), revoking their business license (6%), or sending the baker to jail (1%). Another 6% support suing the baker for damages.





Strong liberals stand out with a majority (58%) who favor some form of government punishment for a baker who refuses to bake the cake. In contrast, 22% of moderates and only 4% of strong conservatives support some form of government sanction against the baker or bakery.



An overwhelming majority of Americans oppose requiring churches and religious organizations perform same‐​sex wedding ceremonies if doing so violates their religious beliefs. This is non‐​controversial, with strong majorities of Democrats (73%), independents (81%), Republicans (91%), evangelical Protestants (92%), and non‐​religious people (72%) in agreement.



A slim majority (52%) of Americans say that local government officials should be required to perform same‐​sex wedding ceremonies, even if doing so violates that official’s religious convictions. Nearly as many (47%) say these officials should not be required to perform these ceremonies.



Partisans are sharply divided. Nearly 7 in 10 (69%) Democrats say local officials should be required to perform same‐​sex wedding ceremonies. In contrast, 68% of Republicans say such officials should not be required to do this. Independents are divided, with a slim majority (51%) who say officials should perform the ceremonies.



Most Americans (69%) would oppose a law that would ban the building of mosques in their community while 28% would favor. Although a slim majority (51%) of Republicans also oppose such a law, they are the most likely group to support it (47%). Far fewer Democrats (14%) and independents (28%) would also support a ban on building mosques in their communities.





The question distinguishes Libertarians from Conservatives. Using a political typology to identify ideological groups,42 we find that Libertarians (76%) are 25 points more likely than Conservatives (51%) to oppose a ban on building mosques. Eighty‐​nine percent (89%) of Liberals and 67% of Populists also oppose such a law.



The Cato 2017 Free Speech and Tolerance survey asked the following three questions to identify clusters of like‐​minded respondents based on their answers to questions about the proper role of government involvement in economic affairs and in promoting traditional values.



Respondents were divided into five groups, based on whether they wanted more or less government involvement in economic affairs and promoting traditional values. Here are the five groups defined:





The Cato Institute 2017 Free Speech and Tolerance Survey was conducted by the Cato Institute in collaboration with YouGov. YouGov collected responses August 15 to 23, 2017, from 2,547 Americans 18 years of age and older who were matched down to a sample of 2,300 to produce the final dataset. The survey included oversamples of 769 current college and graduate students, 459 African Americans, and 461 Latinos. Results have been weighted to be representative of the national adult sample. The margin of error for the survey, which adjusts for the impact of weighting is +/- 3.00 percentage points at the 95% level of confidence. The margin of error for current college and graduate students is +/- 5.17; for African Americans it is +/- 6.69; for Hispanics it is +/- 6.68; for whites it is +/- 4.13. This does not include other sources of non‐​sampling error, such as selection bias in panel participation or response to a particular survey. 



Data on the moral acceptability of punching a Nazi come from a Cato Institute/​YouGov survey conducted August 21 to 22, 2017, of 1,141 respondents, with a margin of error of +/- 4.5 percentage points, which adjusts for the impact of weighting.



YouGov conducted the surveys online with its proprietary Web‐​enabled survey software, using a method called Active Sampling. Restrictions are put in place to ensure that only the people selected and contacted by YouGov are allowed to participate.



The respondents in each survey were matched to a sampling frame on gender, age, race, education, party identification, ideology, and political interest. The frame was constructed by stratified sampling from the full 2013 American Community Survey (ACS) sample with selection within strata by weighted sampling with replacements (using the person weights on the public use file). Data on voter registration status and turnout were matched to this frame using the November supplement of the Current Population Survey (CPS), as well as the National Exit Poll. Data on interest in politics and party identification were then matched to this frame from the 2007 Pew Religious Life Survey. The matched cases were weighted to the sampling frame using propensity scores. The matched cases and the frame were combined and a logistic regression was estimated for inclusion in the frame. The propensity score function included age, gender, race/​ethnicity, years of education, non‐​identification with a major political party, census region, and ideology. The propensity scores were grouped into deciles of the estimated propensity score in the frame and post‐​stratified according to these deciles. The weights were then post‐​stratified to match the election outcome of the National Exit Poll, as well as the full stratification of four‐​category age, four‐​category race, gender, and four‐​category education.
"
"For the past few million years the world’s oceans have existed in a slightly alkaline state, with an average pH of 8.2. Now, with carbon emissions escalating, there is more CO₂ in the world’s atmosphere. This dissolves in the oceans, altering the chemistry of the seawater by lowering the pH and making it more acidic – up to 30% more in the past 200 years. This growing acidification of the oceans is becoming a serious problem for the production of shellfish around the world. Shellfish are creatures which produce calcium carbonate shells and skeletons, such as mussels, oysters and corals. They create their protective shell structures through a process known as biomineralisation – producing hard minerals such as calcium carbonate by filtering calcium and carbonate from the water. If the amount of carbonate available in the oceans is reduced by acidification, it limits the ability of these creatures to create shells. But now coastal acidification is happening close to land in regions where freshwater run-off can release sulphate soils and excess carbon, which also lowers pH and carbonate available for producing shells. This is being exacerbated by flooding and rises in sea levels caused by climate change. Recent studies reported these implications for the Sydney rock oyster in New South Wales, Australia. Historically, oyster production in the region has seen a decline in larger “plate-grade” oysters and an increase in smaller oysters. This can be due to a number of reasons which are physical, biological and economic, including pressures on farmers to harvest oysters early to avoid high winter mortalities in cold dry weather. But our recent study suggests that coastal acidification in Australia is damaging oysters’ ability to grow properly as well. The change in shell growth mechanisms could have implications for the future, such as producing thinner shells which are prone to fracture, causing potential risk of shell damage during culture and harvesting. The situation in New South Wales is not an isolated case. In Washington state in the US, acidification caused by deeper, colder seawater with high levels of CO2 rising to the surface has caused malformations in oyster larvae and loss of hatchery seed production. Read more: Explainer: why ocean acidification is the evil twin of climate change  A report by one shellfish hatchery detailed the impact on shell formation in oyster larvae under these detrimental conditions. Oyster farms in Washington, have put measures in place to sustain oyster shellfisheries under increasingly acidic conditions. This includes treating hatchery water to increase pH, making more carbonate available for early larvae shell formation, and growing oyster seed in different locations to ensure their survival for future production.  In Scotland, a country famous for its high-quality shellfish, acidification is less of an imminent threat. There are no sulphate soils or deeper water with high levels of CO2 rising to the surface, as can be found in Australia and America. But as coastal acidification is made worse by climate change – in particular freshwater run-off from increased rainfall and sea-level rise – this could have a serious effect on commercial shellfisheries all over the world, including Scotland. The changes in seawater chemistry associated with freshwater run-off include lowered salinity and pH, and carbonate availability. This, coupled with increasing temperatures, adds pressures to shellfish farmers producing mussels and oysters. I have previously reported on the effect of experimentally induced high CO2 acidification on mussels, where shells showed reduced growth and became more brittle. Shellfish are predicted to produce thinner shells which may also be more prone to fracture throughout harvesting, transportation or when another animal attempts to eat them. The industry needs to consider ways to reduce this risk. In Washington producers have adjusted the carbonate chemistry in oyster hatcheries to develop larvae before release into farms where acidification has the potential to reduce early shell development. In New South Wales, the Department of Primary Industries has done studies on the Sydney rock oyster to examine the potential for selective breeding to develop resilient strains that can cope better with more acidic seawater conditions. Researchers have reported on the potential for selective breeding for disease resistance and faster shell growth, which could create acidification resilience in the oyster. Our next step working with Australia’s DPI is to examine these selectively bred oysters to understand the potential for combating the acidification problem. It is important for the Scottish shellfish industry to understand the risks posed by climate change already playing out in Australia and the US. With climate change in the future threatening freshwater and CO2-induced ocean acidification in UK waters, the country could suffer the same fate."
nan
"If we don’t make a fundamental change to the way we are living, the world faces
the destruction of entire eco-systems, flooding of coastal areas, and ever more extreme weather. Such was the stark warning in  a recent Intergovernmental Panel on Climate Change (IPCC) report. The task is enormous.  One way to approach it is to look back to a time when scientific thinking did manage to initiate revolutionary changes in our outlook. In the 17th century, the philosopher Francis Bacon called for a “great fresh start” in our thinking about the natural world, and helped usher in the scientific revolution that replaced the staid thinking of the time. We could do worse than follow his example once again – this time in our social and political thinking – if we are to tackle the biggest challenge of our era. In his key work Novum Organum, Bacon identified “four idols” of the mind – false notions, or “empty ideas” – that don’t just “occupy men’s minds so that truth can hardly get in, but also when a truth is allowed in they will push back against it”. A true science, he said, should “solemnly and firmly resolve to deny and reject them all, cleansing our intellect by freeing it from them”.  Bacon’s idols – listed below – are no longer part of standard scientific thinking, but they are still in place within our moral and political thought, and provide a useful model for understanding the challenges we face and how we might respond to them. For Bacon, these “have their foundation in human nature itself … in the tribe or race of men”. Human understanding, says Bacon, “is like a false mirror, which … distorts and discolours the nature of things by mingling its own nature with it”.  Bacon was referring to our understanding of the world around us. But his point applies to our morality too. As the philosopher Dale Jamieson has argued, our natural moral understanding is too limited to grasp the moral consequences and responsibility that comes with a problem like climate change, in which diffuse groups of people cause a diffuse set of harms to another diffuse set of people, over a diffuse range of time and space. Since the “idols of the tribe” are natural and innate, they are tricky to shift. As Jamieson argued, one way to combat them is for individuals to mindfully cultivate green virtues, such as rejecting materialism, humility about your own importance, and a broad empathy with your ecosystem. “Everyone has a cave or den of his own,” Bacon wrote, “which refracts and discolours the light of nature.” The cave is the knowledge set, specific to each individual, as a result of their upbringing and learning.  This has become even more splintered in recent years, as people follow their own silos of information online. For instance, although most in the UK think that rising global temperatures are the result of man-made emissions, a sizeable minority (25%) do not. On the day of the recent IPCC report, much of the UK press ran as their main story a drunken kiss between two contestants on a reality TV show. To combat the idols of the cave we must ensure that, through education, the media and culture, the scientific consensus behind climate change is well known.  For Bacon, these arose “from consort, intercourse, commerce”. Everyday language, he argued, diminishes our understanding of the world by promoting concepts “imposed by the apprehension of the vulgar” over those of “the learned”.  The language that dominates contemporary political and economic discourse similarly diminishes our relationship with the natural world. The emphasis is on profit, consumption and continuous growth, rather than well-being and sustainability.  Consequently, our economic system is not well geared towards the environment.  “Donut Economics”, and the “post-growth” movement are useful proposals for reframing our economic systems and combating Bacon’s idols of the market. At a global political level, the UN’s 17 Sustainable Development Goals provide a basic political vocabulary for tackling climate change. These “are idols which have immigrated into men’s minds from the various dogmas of philosophies[…]representing worlds of their own creation”. They are preconceived dogmas – of a religious, political or philosophical kind – that undermine clear, evidence-based thinking about the world. In contemporary politics, preconceived dogma – often in the form of vested interests – continues to exert a hold on our response to climate change. For instance broadcasters routinely invite climate change deniers (often industry-funded) to debate points of scientific evidence, on the grounds of “balance”.  To combat the idols of the theatre, we need a recognised global hub where relevant information from expert bodies can be assessed and translated into actions. This would be the modern equivalent of the French mathematician Marin Mersenne in the 17th century, whose wide range of contacts (from Hobbes to Pascal to Descartes to Galileo), allowed to him act, as Peter Lynch puts it, like “a one-man internet hub” for the emerging scientific revolution. To tackle climate change, we urgently need a far-reaching restorative project, of similar scale and scope to the scientific revolution. Such change can sometimes seem remote and difficult to conceive. Yet, as Bacon himself put it:  By far the greatest obstacle to the progress of science – to the launching of new projects and the opening up of new fields of inquiry – is that men despair and think things impossible."
"After a summer of catastrophic bushfires, the most brutal evidence of the impacts of climate change, the government has managed to move the debate towards the pros and cons of setting a long-term net zero emissions target for 2050.  While #Scottyfrommarketing copped flack for his lack of empathy or response to the summer of fires, you have to admit, he’s done an amazing job of shifting the debate away from the Coalition’s failure to reduce emissions or prepare for the fires they were warned of by 24 former fire and emergency chiefs.  As Australia’s forests (and international credibility) burned, Scott Morrison took a match to common sense and got away with it. Apparently, drilling for more gas reduces emissions, and we need to debate what happens in 2050 rather than what the government must do today.  It’s an impressive achievement. To be clear, I’m not opposed to setting targets for net zero by 2050. I’m glad that the Liberal premiers of New South Wales, South Australia and Tasmania have torched the idea that committing to zero emission reductions is “virtue signalling” or, as the Business Council of Australia said in the lead up to the last election, that a similarly ambitious target was “economy wrecking”. After a decade of nonsense from so many conservatives, it’s a relief to hear the rhetorical support for big cuts to emissions from the political parties and business groups that have done so much to stifle climate action in Australia. But we must beware the siren song of ambitious climate action in the future. As former resources minister Matt Canavan helpfully reminded us this week: “It’s exactly the same as to say: look, I’m going to lose 10kg in 10 years’ time but I’m not going to do anything about it today.” I couldn’t have said it better myself. Maybe I’m being cynical, but Senator Canavan might be on to something. There is a real risk that Gladys Berejiklian, Jennifer Westacott and other conservative voices calling for a net zero target for 2050 are simply trying to avoid making hard decisions today. But luckily there’s a simple way to test their commitment; by starting the transition today. A Chinese proverb says while the best time to plant a tree was 20 years ago, the second best time is right now. It’s true we don’t know today exactly how we will make steel or travel between continents without fossil fuel in 2050. But that uncertainty should drive us to do simple things we do know. The less oil we waste moving inefficient cars around our cities, the more oil that can be burned flying people to Europe. Similarly, the less coal we waste making electricity with highly subsidised power stations, the more coal that can be burned to make steel. Neither techno-optimism nor techno-pessimism is a substitute for the rapid roll-out of changes we know must be made. It’s not hard: Stop increasing emissions. Don’t build new coal-fired power stations that will pollute for decades. Stop the expansion of industries whose business case relies on increasing fossil fuel consumption. Don’t build new coal mines, open new gas fields, or expand logging of the only viable carbon capture and storage device known today – the tree. Start investing in things that reduce emissions now. Renewable energy and storage are a good start. Energy efficiency measures are great too. (Both measures save money, so won’t be a “cost to the economy”.) Invest in science and future technology, but don’t bet the planet on it. We’ve been told that so-called carbon capture and storage (of the non-tree variety) is just “10 years away” for the last 20 years. Likewise, cheap safe nuclear and “green” hydrogen. After 30 years of hope it’s time to start doing things that work. Now. Stop the accounting tricks. Pretending that dodgy “carryover credits” are a substitute for burning less coal will do nothing to stop climate change. Which brings me to the “net” in net zero emissions. Some emissions of greenhouse gasses are inevitable. Not only do cows burp but some people do as well. As of today, making milk, producing steel, and flying planes all generate lots of emissions. And while technological solutions are possible, it’s not inevitable that those solutions will be quick enough, or cheap enough, to reduce emissions to zero by 2050. The first response to the reality that some products will generate emissions for decades to come, is to buy less of those products. The second is to get cracking with installing the simple stuff that we already know works, in the sectors where it’s easiest to do so. But it’s the third response that puts the “net” into net zero targets. It’s possible to sequester huge amounts of carbon dioxide in trees and soil. It might even be possible one day to capture C02 from the smokestack of power stations and bury it. Maybe. But after billions of dollars and decades of government-funded research, the best carbon capture and storage device known is still the tree. Installing solar, wind and batteries is far cheaper, and the technology exists now. Because some “negative emissions” are possible, we don’t have to get all of our manufacturing or farming emissions to zero. But because there’s a limit to the amount of negative emissions we can generate, we have to do everything we can to drive down emissions in the sectors where it’s easiest, such as electricity. Then, there’s international offsets. Another way for Australia to be net zero by 2050 is to continue our pollution-as-usual path and buy huge amounts of offsets from other countries. But as Canavan rightly reminds us: “… we are being told that we need all countries to hit net zero by 2050 to save the planet. Who are we going to buy carbon emissions from if all other countries have stopped emitting too? We can’t buy carbon credits from Mars.” So, with this sage counsel ringing in our ears, those of us who take the science of climate change seriously must ensure we can walk and chew gum at the same time. Long-term targets are only as valuable as the urgent actions we attach to achieve them. And short-term action won’t drive big change unless we bring powerful new voices into the conversation. The best way to hit a net zero target would have been to stop building coal-fired power stations, coalmines and gas fields 20 years ago. But the second best way is to stop building them today, to boost our renewable energy targets today, and to introduce energy efficiency standards for buildings and vehicles today. We’ve got 30 years to solve the hard problems, but no time to avoid the easy solutions. • Richard Denniss is chief economist at the Australia Institute"
"

It seems that ever since 1776, European elites have been grousing about the symbols of American culture that are “forced” upon them. First it was the ideas of our revolution, such as liberty and democracy. Today it’s Hollywood movies, EuroDisney and — horror! — McDonald’s hamburgers. So perhaps it’s no surprise that some European Union politicians seem positively gleeful at the prospect of exporting to the United States that great staple of European life: ubiquitous taxation. 



At issue is who will collect the value‐​added tax (VAT) when a European consumer downloads a digital product or service from a U.S.-based company. The VAT — which can add as much as 25 percent to the cost of a product — is usually charged at the point of entry on “tangible” products shipped from the United States. But since products such as software and music can be delivered directly over the Internet, there aren’t any packages for EU tax collectors to inspect at the border. Thus, when French President Jacques Chirac downloads a game called, say, “Jerry Lewis’ Championship Boxing,” he’s responsible for paying the VAT himself. There’s no practical way to track Chirac’s purchase, however, so the odds of his paying the tax voluntarily are essentially zero. 



Ever alert to the scourge of untaxed commerce, EU politicians have hit upon a solution to their problem: have the Americans harvest the loot! 



That beggar‐​thy‐​neighbor approach to tax collection has been endorsed by the European Commission, which proposed on June 7 that U.S. companies be required to collect a VAT on all sales of digital products to Europe. If the proposal becomes law — which could happen later this year — all U.S. business will have to register with the tax agency of at least one EU member country, ascertain the location and tax status of each and every customer, transmit tax payments electronically to the relevant tax authority, and submit to audits and “due diligence examinations” to make sure no one is cheating. In return for acting as Europe’s tax collectors, American businesses would receive half‐​price coupons for EuroDisney (just kidding, they wouldn’t get anything). 



But as the history‐​minded among us have noted, America once fought a war to escape burdensome taxes imposed from across the Atlantic. In the days before the Internet, America’s political leaders uniformly denounced “taxation without representation” as an intolerable evil. 



Even given today’s less‐​principled political climate, it’s difficult to see what Washington has to gain by going along with Europe’s scheme. Under the current rules, European consumers have an incentive to shop VAT‐​free from U.S. companies, which also makes the United States an attractive market for e‐​commerce investors. European companies rightly complain about that disparity, but too bad: let Europe solve its own tax problems. If political leaders there weren’t reflexively opposed to tax cuts, they could simply exempt digital products and services from the VAT. 



Enforcement will be difficult without Washington’s help, but Europe’s tax collectors are determined to try. U.S. businesses with a branch office in Europe, for example, could probably be forced to comply. Another option being considered is “blacking out” the Web sites companies that refuse to register for VAT collection. But consider that despite having highly centralized systems of Internet service provision, authoritarian governments (such as China) have been unable to control access to dissident Web sites. The Internet is simply too massive and decentralized to police effectively. 



That enforcement problem is why Europe is calling for increased “international collaboration” on tax collection, meaning that the IRS would monitor U.S. companies’ compliance with EU tax law. And there’s no guarantee that Congress won’t play along: the digital‐​VAT already has allies among U.S. state and local politicians who would love to see broader taxation of the Internet. After all, if American businesses can be compelled to collect taxes on their sales to foreign consumers, surely those businesses can be told to collect taxes on sales of products destined for other states? Two wrongs, the politicians hope, will in this case make a right. 



Ordinary Americans, as they pause this summer to consider the reasons our ancestors took up arms against the British 224 years ago, aren’t going to buy it, and Congress should take heed. “Taxation without representation” remains an intolerable evil that neither time nor technology has diminished.
"
nan
"**Back to the future. Next week England will return to the tier system, where the country is carved up into different categories.**
Areas where there are higher levels of coronavirus will be under tighter restrictions. Parts of the country where it is less prevalent will have looser limits.
It is the same model that didn't do enough to slow the winter surge of the disease last time. But second time round, it's different in some important ways.
First off, the strictest regime, tier three, will overall be tougher than the last time round.
Swathes of the country are still going to stuck under a pretty restrictive system. (You can read all about exactly how the tiers will work here.)
Second, gyms and businesses that carry out personal care like hairdressers will stay open everywhere. Stricter yes, but way short of a rerun of the spring lockdown.
Third, the decision on which set of instructions a region will have to follow will be taken by the government and its scientists centrally.
For all of the warm words about the importance of local leaders, and local authorities, there is no desire in central government to repeat the very messy political process of trying to get regional buy-in when decisions are being made about moving in and out of the different categories.
That might be a more straightforward way of making the choices. But it could make it harder to get local support for measures being taken.
Although it's worth noting the chancellor has already changed his mind about extending national financial support which was such a bone of contention last time.
And in theory, areas could move more quickly in and out of the different tiers too - with possible switches every fortnight.
First time round the government struggled to communicate how the system would work cleanly, but this is going to be the way things are run until March so ministers want to get it right.
But the political test of how bumpy it could be won't come until Thursday when ministers reveal the answer to the question every MP and member of the public wants to know - what will the rules be in the place they live.
The Tory backbenches and the opposition this time are more demanding of the government in terms of explaining its decision-making.
But the extent of the potential push back will depend to a large extent on just how much of the country is asked to keep coping largely behind closed doors.
Downing Street has repeatedly made it clear that more of England will be in the toughest tier than in the first phase of this system, but won't be drawn yet beyond that.
The prime minister's wi-fi might have held up through his press conference on Monday, unlike the House of Commons earlier (although the PM protested the problems were on the Commons' end) but it's not clear yet how his arguments for the revised tier system will hold up too.
And don't forget the relative success of the new system depends in large part on all of us - how the public responds to the new system of rules and regulations, and how patient we are all willing to be.
P.s. Remember all four parts of the UK are following different approaches right now. Northern Ireland is about to tighten up further. Wales has not long left its circuit break lockdown, and Scotland has a new system of five tiers.
That patchwork is partly why it is 'possible, but not definite', according to government insiders tonight that the four parts of the country will agree how to manage Christmas on Tuesday.
The four administrations have been trying to figure out a way of allowing a bit more flexibility around families spending time with each other at Christmas.
But there are considerations around how long it should be, how to manage transport, how many households it should include, even what really constitutes a household.
There doesn't seem any doubt that an agreement will be reached. It seems (for once perhaps?) the delay is because of the complexities not a conflict.
Conversations continue, and there should be a deal of some sort before too long."
"

The busted forecast for last week’s East Coast snowstorm points to a very troubling aspect of modern life: We now believe the output of computers more than we trust our own eyeballs. 



At noon on January 25, the most sophisticated weather‐​forecasting model in human history predicted a total snowfall for Washington, D.C., of somewhat less than an inch in the succeeding 36 hours. All the human forecasters I know went along. 



The computer model, named for the Greek letter eta (pronounced “ay‐​ta”), which describes its mathematical coordinate system, forecast a storm far out to sea, sparing the I-95 megalopolis that stretches from Richmond to Boston. Instead, eta confined its significant snow to a sliver of southeastern Virginia, before shoving everything north, east and offshore. 



Fear in the forecasting community was palpable as minute‐​by‐​minute updates from our national radar network painted an army of green and yellow monsters marching north and west toward our nation’s capital. By 9 p.m. they were closer than most of the Confederates ever got, and still the forecast was for a minor dustup. It wasn’t until eta, whose major update cycle is 12 hours, was run again, that forecasters decided disaster was at hand. 



Why didn’t we believe our eyes when the model was clearly busting? The truth is, as models become more sophisticated, forecasters are increasingly reluctant to abandon them, even in the face of contrary evidence. But, “the computer eta my forecast” is an insufficient excuse. 



Would that this were the case merely for the 48‐​hour forecast. Unfortunately, it appears that the same pathology has infected the 48‐​year projection. Just like weather forecast models, our climate simulations have become increasingly sophisticated in recent years. And just like the situation with the recent snow, there has been incontrovertible and advancing evidence, this time over the course of the last two decades, that climate simulations are making a disastrous error, and it has taken forever for forecasters to acknowledge it. 



Every computer model predicts that the entire troposphere, or roughly the bottom 40,000 feet of the atmosphere, should be warming rapidly. Most models even predict that much of the warming accelerates with height. 



But, for over 20 years now, we have two independent measures of temperature — satellites and weather balloons — that show no net warming at all from 5,000 feet skyward. In 1996, we had 17 years of satellite data, and yet the “Policymakers Summary” (the only part that gets read) of the U. N. Intergovernmental Panel on Climate Change report contained not one mention of the word “satellite.” That is the report usually cited as the “consensus of scientists.” 



If that was the 1996 consensus of my profession, it was largely a consensus of ostriches, not scientists. How long could we continue to sweep under the computer the fact that somehow we got 88 percent of the atmosphere wrong? 



Not very long. Here is what the National Research Council now says about our ability to model climatic behavior: “It is clear,” they recently wrote, “that reconciling the discrepancy … is not simply a matter of deciding which [climate model] is correct.… In the long term it will require major advances in our ability to interpret and model [atmospheric behavior].” In other words, our models don’t work. Here, for once, eyeballs have triumphed over the computer. 



Most people use weather forecasts to plan the future. The Kyoto Protocol on climate change was a response to a climate forecast. This onerous document would require the United States to drastically reduce the energy use that has propelled us on our gee-doesn’t-your-401 k -look‐​great way for the last two decades. Everyone (except, maybe, President Clinton, judging from his State of the Union Address) believes it will cost us a fortune. 



When it finally became clear that Washington was going to get buried in the last snowstorm, the federal government shut itself down. Now that we have admitted that our climate forecasts for the next century aren’t worth much at all, can we please shut down the Kyoto Protocol on climate change?
"
"
Share this...FacebookTwitterWe were told that the mild winters we experienced in Europe were due to global warming. Now, suddenly, we are getting hit with yet another nasty cold winter.
Why? Guest writer Juraj Vanovcan presents his observations and interesting evidence that it has nothing to do with CO2. He presents what I think is an astonishing finding near the end.
===============================================================
Predicting The European Climate From The CET Record – Lesson Learned
By guest writer Juraj Vanovcan
This post was inspired by the article Negative AO bringing cold winters back to Europe.
Recalling the summers and winters of the early 1980s, it becomes obvious to me that it is the prevalence of air circulation that determines if a season is warm or cold. The very mild winter of 2006/2007 in Central Europe was characterized by a sustained flow of warm Atlantic air over the European continent, while the cold and snowy 2005/2006 winter received a lot of Arctic air entering the mid-latitudes from the North.
Air circulation is governed by pressure differences and basically this is what North Atlantic Oscillation is all about.
 
Fig 1 Example of positive NAO (Source: JISAO webpage).
I compared the NAO index with the European long-term climate record. Checking the Central European Record (CET) shown, one sees there is an obvious correlation between NAO and winter temperatures. The dark blue line is CET, orange/light blue is NAO.

Fig 2 North Atlantic Oscillation index compared with CET winter record, 1860-2010 (CET graph source: http://climate4you.com/ ).
As observed above, the NAO oscillates in an 80-year long sine wave cycle. The first period with mild winters happened in the 1920s, which of course we do not remember. The second positive phase began in 70s and mild winters in Central Europe become frequent since late 80s. It also seems that the current period with prevalent NAO-positive years has ended; the recent string of cold winters in North-Western Europe suggests this as well.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




On the other hand, CET summers do not show correlation with NAO in the early part of 20th century. Comparing the CET summer anomalies with AMO index (detrended North Atlantic SST) however gives reasonable correlation again. Summers in the period 1930-1950 were often as warm as the recent ones, and the extremely hot and dry summer of 1946 in Central Europe with its catastrophic impact on crops has since never been repeated. However, warm summers in this period were combined with cold winters, like those of 1939/40/41.
Fig 3 Atlantic Multidecadal oscillation index compared with CET summer record, 1860-2010
It is worth noting that while the NAO peaked circa 20 years before the warm AMO phase centered on 1940, their warm phases were much closer to each the other in the later part of the century. It means that while Europe experienced cold summers and mild winters in 1920s and warm summers and cold winters in the 1940s, the last 20 years saw both warm summers as mild winters. This is also probably the reason why 2000-2010 decade is slightly warmer than 1940-1950.
Based on the observed SST and OHC record for North Atlantic, it seems however that AMO had peaked around 2005 and it is now heading down. This time, both NAO and AMO being in their negative phases will mean miserable summers and cold winters. Such shift in temperature trend is already being observed in the whole northern extratropics record.

Fig 4 Northern hemisphere north of 30N, HadCRUT3 data
IPCC attributed the post-1975 warming phase almost solely to anthropogenic reasons, namely to increased “greenhouse effect” caused by increase of CO2 molecules from 3.5 to 4 per other 10,000 molecules in the atmosphere.
We can conclude, however, that at least for Europe, observed warming is fully explainable by natural variations, two of which – AMO and NAO – had their positive phases overlaid during the last 20 years. Neither CO2, aerosols nor greenhouse effect theory are needed.
For those still seeking the anthropogenic signature in recent warming, here is a comparison of 1890-1920 warming trend compared against 1980-2010 warming trend in the winter CET record. The running 10-year mean is strikingly similar, following even minor dips and upticks.
Fig 5 CET winter record with late century warming superimposed on early century warming 
Extrapolations into the future may be tricky, especially when pulling some 20-year trend into year 2100, which seems to be a favorite practice in modern climatology. Observing the European climate record of the early part of 20th century and understanding its causes gives us much more predictive skill when forecasting climate for the next decades. All climate indicators today point to cooling, and not only in the European region.
Juraj Vanovcan (juraj.vanovcan@gmail.com)
26 November 2010
Share this...FacebookTwitter "
"

Either global warming is the greatest crisis ever to confront humankind, or it is a lefty plot completely manufactured by scientists and politicians in pursuit of research funding and control over our lives. That’s about the way it plays out in the media, on blogs and in conversations on the Metro. Anyone out front on this issue is either an apocalyptic or a denier, virtuous or vile.



Similarly, one camp maintains that temperatures are rising dramatically with unspeakable portents, while the other thinks what has happened is entirely a result of undefined internal oscillations in the earth‐​sun climate system, and that there is virtually no human component to climate change. This group is especially fond of the lack of statistically significant surface warming since 1995. Since 1997, temperatures really flatlined.



There’s a third way, which suffers from the problem that it is subtle, neither black nor white, and doesn’t do well in sound bites. It’s a “lukewarm” synthesis, arguing indeed that humans have something to do with the rise in surface temperature measured since the mid‐​1970s, but that it is hardly the end of the world as we know it. This view claims to accommodate the seemingly odd behavior of temperature in the last 15 years.



Each of these positions — let’s call them hothead, flatline, and lukewarm — are testable against observed history and theory. To keep some interest in this occasionally boring topic, I’m going to examine them sequentially, starting with the hotheads.



The hothead argument is that we have already set the planet on the road to climate calamity, and that we must promptly reduce the atmospheric concentration of dreaded carbon dioxide‐ the main global warming emission — to levels seen decades ago.



Before we started torching carbon stored in forests and then carbon stored underground as coal and oil, the carbon dioxide concentration of our atmosphere was about 280 parts per million (ppm). It’s now around 390, and headed for a nominal doubling to 600ppm between 2070 and 2090 if the world continues its current rate of development and does not find an effective (meaning neither solar nor wind) and politically acceptable (meaning not nuclear, at least for now) alternative for hydrocarbon fuels.



The high priest of the hotheads is NASA’s James Hansen, who preaches that, unless we dial back to 350ppm, we will lose, within a hundred years’ time, the vast majority of Greenland’s ice, which will raise sea levels about 20 feet. Hansen has testified that he thinks this could happen within a hundred years.



The hothead theory is that the ice on that gigantic island is much less stable than previously thought, and that with a tad more warming, lakes will form in the summer, drain thousands of feet down to the bedrock, and lubricate the flow to the ocean. It quickly melts, submerging a lot of Florida and Manhattan. The Washington Monument becomes an island.



The reason that glaciers flow to begin with is because the bottom is liquid. It’s quite unclear that simply adding more water will have much effect. Recent studies indicate that when the lakes drain suddenly into the ice, the acceleration of flow is not sustained. But that’s today; what about in the future?



One way to project the future with confidence is to look to history, when it was warmer. Danish colonists established a series of weather stations on the Greenland coast, with reliable records that go back over 225 years. They unequivocally show that — from 1920 through 1960 — there was substantially more warming than has been observed in recent decades. If hothead theory is correct, there should have been a detectable jump in sea level during that period, but there was none.



Further, there is very strong evidence that the integrated warming — that’s temperature times time — was much greater for _millennia_ after the end of the recent ice age around 10,800 years ago. Assuming that humans will find something better to power the world with than carbon dioxide‐​emitting fossil fuels in the next one or two hundred years, that total warming back then was greater or equal to what we are likely toinflict on Greenland.



In those millennia — which are only the blink of a geologist’s eye ago — trees used to grow where there is now only barren tundra. When they died, they were preserved in the acidic bogginess, so we can tell exactly when they were alive with carbon dating. It’s very clear that the forest in Eurasia used to extend all the way to the Arctic Ocean during that warm period.



Plant ecologists know that the northern limit of the forest is determined by the mean July temperature. The dead trees tell us it was as much as 13 degrees F warmer than the 20th century average.



The author of that work, Glen MacDonald of UCLA, has noted that the only way to get that region so warm is with a massive influx of Gulf Stream water from the Atlantic. The only “gate” for that is the channel between Greenland and Scandinavia, which means that Greenland (at least the eastern half) would have been pretty balmy compared to today.



And Greenland still retained the lion’s share of its ice cap.



Even so, the Arctic Ocean was likely to have been largely ice‐​free during the summer during much this time — from 6,000 to 8,000 years ago — as noted by theUniversity of Stockholm’s Martin Jacobsson in a 2010 edition of the scientific journal _Quaternary Science Reviews_. The Geological Survey of Norway foundsomething similarin 2008. Not only did Greenland’s ice survive — so did the polar bear.



So it appears that the ice that the hotheads skate on is pretty thin. Are the flatliners doing any better? We’ll have a look in Part 2.
"
"
Part I: Ranking global warming among present-day risks to public health.

Guest essay by Indur M. Goklany
There seems to be no limit to the hyperbole surrounding climate change – and that’s no hyperbole. Numerous politicians have informed us over the years that climate change is one of the most important problems facing mankind.  In fact, U.N. Secretary General Ban Ki-moon has called it the defining challenge of our age.”
But is it?
I answer this question in a paper just published in the refereed section of Energy & Environment.
A 2005 review article in Nature on the health impacts of climate change estimated that 166,000 deaths were “attributable” to climate change in 2000. This estimate was derived from a World Health Organization (WHO) sponsored study that even the study’s authors acknowledge may not “accord with the canons of empirical science” (see here). But I will accept this flawed estimate as gospel for the sake of argument.
In the year 2000, however, there were a total of 56 million deaths worldwide. Thus, climate change may be responsible for less than 0.3% of all deaths globally (based on data for the year 2000). This places climate change no higher than 13th among mortality risk factors related to food, nutrition and environment, as shown in the following table.
Specifically, climate change is easily outranked by threats such as hunger, malnutrition and other nutrition-related problems, lack of access to safe water and sanitation, indoor air pollution, malaria, urban air pollution. And had I included other risks to public health beyond environmental, food and nutritional factors (e.g., HIV/AIDS, TB, various cancers, etc.) then climate change would have ranked even lower than 13th.
With respect to biodiversity and ecosystems, today the greatest threat is what it always has been – the conversion of land and water habitat to human uses, i.e., agriculture, forestry, and human habitation and infrastructure. See, e.g., here.
Climate change, contrary to claims, is clearly not the most important environmental, let alone public health, problem facing the world today.
But is it possible that in the foreseeable future, the impact of climate change on public health could outweigh that of other factors?
I will address this question in subsequent blogs.



Risk factor

Ranking


Mortality   (millions)


Mortality   (%)



Blood   pressure
1
7.1
12.8


Cholesterol
2
4.4
7.9


Underweight   (hunger)
3
3.7
6.7


Low fruit   & vegetables
4
2.7
4.9


Overweight
5
2.6
4.6


Unsafe water,   poor sanitation
6
1.7
3.1


Indoor smoke
7
1.6
2.9


Malaria

1.1
2.0


Iron   deficiency
8
0.8
1.5


Urban air   pollution
9
0.8
1.4


Zinc   deficiency
10
0.8
1.4


Vitamin A   deficiency
11
0.8
1.4


Lead exposure
12
0.2
0.4


Climate   change
13
0.2
0.3


Subtotal
27.6
49.4


TOTAL from all causes
55.8
100.0



Priority ranking of food, nutritional and environmental problems, based on global mortality for 2000. Source: I.M. Goklany, Is Climate Change the “Defining Challenge of Our Age”? Energy & Environment 20(3): 279-302 (2009), based on data from the World Health Organization. Note that malaria isn’t ranked in this table because deaths due to malaria were attributed by WHO to climate change, underweight, and zinc and vitamin A deficiencies.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e96d06e75',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"The world’s largest financier of fossil fuels has warned clients that the climate crisis threatens the survival of humanity and that the planet is on an unsustainable trajectory, according to a leaked document. The JP Morgan report on the economic risks of human-caused global heating said climate policy had to change or else the world faced irreversible consequences.  The study implicitly condemns the US bank’s own investment strategy and highlights growing concerns among major Wall Street institutions about the financial and reputational risks of continued funding of carbon-intensive industries, such as oil and gas. JP Morgan has provided $75bn (£61bn) in financial services to the companies most aggressively expanding in sectors such as fracking and Arctic oil and gas exploration since the Paris agreement, according to analysis compiled for the Guardian last year. Its report was obtained by Rupert Read, an Extinction Rebellion spokesperson and philosophy academic at the University of East Anglia, and has been seen by the Guardian. The research by JP Morgan economists David Mackie and Jessica Murray says the climate crisis will impact the world economy, human health, water stress, migration and the survival of other species on Earth. “We cannot rule out catastrophic outcomes where human life as we know it is threatened,” notes the paper, which is dated 14 January. Drawing on extensive academic literature and forecasts by the International Monetary Fund and the UN Intergovernmental Panel on Climate Change (IPCC), the paper notes that global heating is on course to hit 3.5C above pre-industrial levels by the end of the century. It says most estimates of the likely economic and health costs are far too small because they fail to account for the loss of wealth, the discount rate and the possibility of increased natural disasters. The authors say policymakers need to change direction because a business-as-usual climate policy “would likely push the earth to a place that we haven’t seen for many millions of years”, with outcomes that might be impossible to reverse. “Although precise predictions are not possible, it is clear that the Earth is on an unsustainable trajectory. Something will have to change at some point if the human race is going to survive.” The investment bank says climate change “reflects a global market failure in the sense that producers and consumers of CO2 emissions do not pay for the climate damage that results.” To reverse this, it highlights the need for a global carbon tax but cautions that it is “not going to happen anytime soon” because of concerns about jobs and competitiveness. The authors say it is “likely the [climate] situation will continue to deteriorate, possibly more so than in any of the IPCC’s scenarios”. Without naming any organisation, the authors say changes are occurring at the micro level, involving shifts in behaviour by individuals, companies and investors, but this is unlikely to be enough without the involvement of the fiscal and financial authorities. Last year, analysis compiled for the Guardian by Rainforest Action Network, a US-based environmental organisation, found JP Morgan was one of 33 powerful financial institutions to have provided an estimated total of $1.9tn (£1.47tn) to the fossil fuel sector between 2016 and 2018. A JP Morgan spokesperson told the BBC the research team was “wholly independent from the company as a whole, and not a commentary on it”, but declined to comment further. The metadata on the pdf of the report obtained by Read said the document was created on 13 January and that the author of the file was Gabriel de Kock, an executive director at JP Morgan. The Guardian has approached the investment bank for comment. Pressure from student strikers, activist shareholders and divestment campaigners has prompted several major institutions to claim they will make the climate more of a priority. The business model of fossil fuel companies is also weakening as wind and solar become more competitive. Earlier this month, the influential merchant bank Goldman Sachs downgraded ExxonMobil from a “neutral” to a “sell” position. In January, BlackRock – the world’s biggest asset manager – said it would lower its exposure to fossil fuels ahead of a “significant reallocation of capital”. Environmental groups remain wary because huge sums are invested in petrochemical firms, but some veteran financial analysts say the tide is changing. The CNBC money pundit Jim Cramer shocked many in his field when he declared: “I’m done with fossil fuels. They’re done. They’re just done.” Describing how a new generation of pension fund managers was withdrawing support, he claimed oil and gas firms were in the death knell phase. “The world has turned on them. It’s actually happening kind of quickly. You’re seeing divestiture by a lot of different funds. It’s going to be a parade that says, ‘Look, these are tobacco. And we’re not going to own them,’” he said. “We’re in a new world.”"
"It can be easy to lose hope in the age of what some call the planet’s sixth mass extinction. It is clear that there are no easy solutions to ongoing biodiversity loss, and while there are some inspiring local successes, we have so far failed to change the trajectory of environmental destruction that has historically accompanied economic expansion. Meanwhile, the main underlying driver of biodiversity loss, our consumption of natural resources, is only projected to increase. But we are optimistic that things can improve. Though biodiversity in Europe is still declining, three major trends give us hope for our home continent.  People are increasingly recognising how their lifestyle choices impact the environment, and this is slowly driving a cultural shift towards sustainability. Among those aged 18-35 across the world, climate change and the destruction of nature is already considered the most serious global issue and, as this generation becomes increasingly influential, that mindset might cascade into multiple biodiversity successes. It is already affecting consumption choices, with “green” products and lower-impact diets on the rise to the benefit of both biodiversity and human health. This rising focus on sustainability is also delivering notable policy successes. One example is last year’s European parliament vote to ban many single-use plastics, following widespread concern about their impacts on wildlife.  Perhaps most importantly, questions are increasingly being asked about how we can deliver societal prosperity on a finite planet, hence the widespread acclaim of alternative narratives that aren’t based on growth, such as Doughnut Economics. There are even small signs that these alternatives might be growing in policy influence, such as members of the European parliament organising last year’s Postgrowth Conference in Brussels. Meat consumption is a major threat to biodiversity globally, and in Europe a sixth of all land is devoted to pasture. One exciting driver of the reduced meat consumption outlined above is the emergence of ambitious meat substitutes which could both significantly reduce the amount of land needed for farming, and reduce other pressures on biodiversity such as pollution.  A key strength of meat substitutes is that they don’t rely on people sacrificing the “experience” of meat. This means they can work alongside demand reduction efforts to limit agricultural consumption.  A study by Impossible Foods in the US found that someone replacing half of their beef consumption with one of the company’s plant-based alternatives could lead to a 12% reduction in their total agricultural footprint. On top of that, the company claims its meatless burger scored as highly as actual beef in its own blind taste tests.  As culture changes, and as tasty meat substitutes becomes more popular, demand for land-intensive red meat might start to dwindle. This in turn could free up areas of land for nature restoration and rewilding.  Around 50m hectares of land has has been abandoned across Europe in the past 40 years. That’s an area the size of Spain. Alongside improved wildlife legislation, this has stimulated the extraordinary comeback of bears, wolves and lynx across the continent.  But abandoning farms and leaving them to nature does have its downsides. Abandonment is a leading driver of the loss of certain components of biodiversity (such as some open-habitat-loving farmland birds) as well as cultures associated with extensive agriculture and traditional farming practices, and so these will of course have to continue to be managed into the future.  There is also a worry that using agricultural land more efficiently could simply cause people to consume more, and thus use more farmland anyway – a so-called rebound effect.  Nevertheless, the benefits of restoring nature to lands freed from farming are potentially vast – it’s also one of the most important pathways through which Europe can meet its international climate change and biodiversity commitments. And then there are enterprises such as the “rewilded” Knepp Estate in West Sussex, England, which are showing how landscape-scale restoration can potentially create economic opportunities through tourism and low impact harvesting. At a time when agricultural subsidies are increasingly uncertain, they offer a compelling alternative to conventional farming. All-in-all, there is little doubt that biodiversity is still under extreme threat in Europe and globally. However, we remain optimistic that emerging cultural changes, sustainable technological innovations, and an increasing recognition of the benefits of a wilder countryside might leave European biodiversity better off in future."
"Senior Morrison government ministers are publicly at odds about whether Australia will take a long-term emissions reduction target to global climate talks in November after Labor unveiled a target of net zero emissions by 2050. On Friday the finance minister Mathias Cormann confirmed the government “will be finalising a longer-term target in time for Cop26” but the emissions reduction minister would commit only to “a long-term strategy” despite repeatedly being asked about a new target.  As revealed by Guardian Australia, Anthony Albanese used a speech to a progressive thinktank on Friday to commit the ALP to adopting a net zero target by 2050 if it wins the next federal election, without the use of carryover credits from the Kyoto period. Scott Morrison is holding off from making a commitment to carbon neutrality by 2050, partly because of an internal brawl within the Coalition and partly because the prime minister says Australia should not sign up to targets in the absence of costings. Some in the government have noted publicly in recent weeks that Australia implicitly accepted the net zero pathway when the Coalition signed and ratified the Paris agreement, and Liberal moderates are now pushing to make net zero an explicit target beyond the 26-28% emissions reduction promised by 2030. Asked about the existing Paris commitment on Friday, Cormann told Sky News “we will be finalising our longer-term emissions reduction target in time for Cop26 in Glasgow later this year”, using the word “target” at least three times in his formulation of future government policy. “But what we will do as part of our process is to ensure that the agenda we determine to achieve any such target is environmentally effective and economically responsible,” he said. “And we will ensure that we can look the Australian people in the eye and say ‘this is what we’re aiming to achieve, this is what it will mean for electricity prices, this is what it will mean for jobs’.” Cormann accused Labor of committing to targets without setting out the cost, warning that Albanese is “making the same mistake” as former leader Bill Shorten. Later on Friday, Angus Taylor told Sky News the Coalition had a “very clear target for 2030” and a “very clear view that will drive emissions [reductions] beyond 2030”. Taylor repeatedly avoided the question about whether a technology roadmap to reduce emissions would result in a new longer-term emissions reduction target. “What we’ve said is we’ll have a long-term strategy and we’re not going to have any target that is uncosted, unfunded [and] unplanned,” he said. “We’re not going down that path.” “We’ve made very clear that we’re focused on a long-term strategy and technology will be the focus of it … not taxes.” “We are working towards a long-term strategy, a technology investment roadmap, we have a target for 2030, we’re going to meet and beat that target.” Despite the Coalition criticism, business rode to Labor’s defence. Australian Industry Group chief executive Innes Willox said the net zero target “is increasingly widely supported by Australian businesses, industry advocates such as Ai Group, the wider community and governments of all complexions”. “That growing consensus is important to guide and discipline the development of efficient, trade neutral and fair policies to get there,” he said. “We shouldn’t underestimate the challenge of net zero, which goes well beyond generating cleaner electricity. “Nor should we get too hung up on economic projections, which are about as reliable as trying in 1990 to estimate the cost and value of smartphones in 2020.” Every state and territory has expressed at least an aspirational objective of achieving net zero emissions by 2050, and Australia has been urged by the UK and its Pacific neighbours to sign up to that target. Albanese noted on Friday that the Business Council of Australia is calling for it as well as major corporates including AGL, Santos, BHP, Amcor, BP, Wesfarmers and Telstra. “Seventy-three countries, including the UK, Canada, France and Germany, many with conservative governments, have already adopted it as their goal,” he said. “Australia should too.” Earlier, Labor’s climate change spokesman, Mark Butler, told Radio National the opposition would set out a detailed policy about how to achieve targets and its cost “well before” the next election. Butler argued that the cost of reducing emissions should not be divorced from the cost of inaction and noted Melbourne University research had found actions to reduce emissions have a benefit cost ratio of 20 to one."
"Firms and governments must increasingly internalise the possibility – indeed, I would argue, the overwhelming probability – of an acceleration of four secular developments that influence what business and political leaders do and how they do it. Decision-makers should think of these trends as waves, which, especially if they occur simultaneously, could feel like a tsunami for those who fail to adapt their thinking and practices in a timely manner. The first and most important trend is climate change, which has evolved from a relatively distant concern, on which there is ample time to take remedial action, to an imminent and increasingly urgent threat. The mobilisation of various concerned segments of society, owing partly to unusual climatic disruptions in recent years, has greatly increased the pressure on companies to act now. BP’s recent announcement that it intends to achieve “net zero” carbon emissions by 2050 – a notable promise by an energy company that operates in several highly challenging settings – is the latest example of business responding to such calls. It is only a matter of time until this pressure also prompts governments to take further steps, not only to encourage green activities but also to tax and regulate those that cause pollution. Second, privacy concerns have grown alongside technical innovations involving artificial intelligence and big data. Society is increasingly recognising that recent technological advances allow not only for more efficient compilation of huge amounts of personal data but also for using this information to monitor and alter behaviours. Broadly speaking, data is controlled and exploited either by governments (particularly in China), big tech companies (as in the US), or more by users (as in Europe). But none of these three general operating paradigms seems to provide sufficient comfort and assurance to most people. The third secular force involves disruptions to the multi-decade process of economic and financial globalisation. The initial trigger for this was the trade-policy pivot by the US president Donald Trump’s administration – from cooperative conflict resolution to explicit confrontation, from multilateralism to bilateralism (or even unilateralism) and from rule-based to more ad hoc arrangements – aimed at creating a still-free but fairer trading system. But de-globalisation has been turbocharged by the outbreak of the deadly Covid-19 virus, which has disrupted the flow of goods and services in China and beyond. These challenges to globalisation have opened the door for governments to weaponise economic tools to meet objectives that transcend economics, such as national security. This, in turn, is calling into question conventional wisdom about cross-border supply chains, just-in-time inventory management and reliance on external demand to boost domestic growth. The final trend is demographic and concerns more than the ageing of societies in Europe and Asia and this trend’s economic and political implications. It also goes beyond the growing realisation that millennials’ starkly different expectations – regarding professional careers, personal engagement, political action and the delivery of goods and services – will persist and deepen. For starters, businesses need to be smarter about “anywhere, any place, any time” delivery. Furthermore, job loyalty and tenure are decreasing, while expectations of comprehensive job fulfilment and engagement are rising. Self-mobilisation for political and other causes, often with no visible leadership structure, has become a lot easier, yet often is less durable and raises tricky questions about what comes afterward. And all of this is taking place amid the continued migration of an ever-expanding range of interactions from physical to virtual spaces. Each of these secular forces will have an important impact on the effectiveness and success of companies and governments alike. And while being challenging overall, the four trends involve a diverse and geographically dispersed set of winners and losers. Executives and policymakers, therefore, must make timely revisions (including pre-emptive changes) not only to their business models and operational approaches but also to both their tactical and strategic mindsets. Getting this right will require cognitive diversity, openness to constructive criticism, repeated scenario analyses and multidisciplinary approaches. Moreover, because each of the secular forces involves a considerable degree of uncertainty (with lots of known unknowns, and probably more than a few unknown unknowns behind them), a combination of resilience, optionality and agility also is important. And this is even before one considers unanticipated periodic shocks such as the coronavirus outbreak. The challenges to sound decision-making and leadership in business and government are not limited to mapping each of the four secular forces and the required adaptation. Decision-makers also must consider correlations and causalities between these trends that can make their total impact multiplicative rather than merely additive. As a quick illustration, consider another aspect of demographic change: migration and the humanitarian challenges that often come with it. Climate change confronts countries with the possibility of waves of migratory human flows that they will find hard to accept and inhumane to refuse. The combination of de-globalisation and the misuse of AI and big data to infringe individual privacy is similarly troubling. This could lead to questionable behaviour by some governments and encourage malicious non-state actors to disrupt societies and economies. The world is in a period of accelerating change, the leading edge of which is the ever-growing list of developments that have gone from impossible to inevitable. Many (though by no means all) of the challenges facing business and political leaders may be broken down into four secular changes that can help anchor the timely formulation of required responses at the local, national, regional and global levels. The faster that companies and governments recognise this, the likelier they will be to alter the balance of benefits, costs and risks in their favour. • Mohamed El-Erian is chief economic adviser at Allianz. He served as chair of President Barack Obama’s Global Development Council and is a former deputy director at the IMF © Project Syndicate"
"
Share this...FacebookTwitterThat’s what one of Germany’s leading national dailies, DIE WELT,  writes here at it’s online site.Who knows! Maybe parts of the German media are beginning to see the the big block-letter writing on the wall and are now slowly taking baby-steps towards acknowledging the claims and science behind catastrophic global warming are not all what they are cracked up to be.
Maybe the far-fetched, cockamamie explanations on why all the cold is caused by warming has led the less zealous among the media to reconsider the science. Maybe all the phony predictions that keep turning out to be wrong are finally raising suspicions.
DIE WELT writes (emphasis added):
Forecasts made by many climate scientists, shortly before the year ended, prophesizing that 2010 would be the hottest on record have – once again – proven to be false.
According to CRU data, 2010 was in third place behind 1998 and 2005, and almost exactly tied with the year 2003. Certainly: The last decade was the warmest since records started being kept 130 years ago, but during the decade the warming – at least for the time being – stopped.”
 Colder winters are forecast for the future
The 4th cold winter in a row is also causing some people to wake up from their global warming trance and prompting them to ask questions. DIE WELT writes:
We are now experiencing the 4th cold winter in a row.”
And adds:
The climate scientists who are most loudly warning of global warming now say Central Europe must expect colder winters.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The reason for this, writes DIE WELT, is because of a change in the NAO, which in the 1980s and 1990s supplied Europe with mild winters with winds from the Atlantic keeping the temperature on the mild side. DIE WELT also writes that Europeans ought to get used to long cold winters because that’s pretty much what is now forecast for the years ahead.
Global warming causing cold winters – just another theory
And Die Welt seems a bit annoyed by all the changing science, and so explains that the colder winters caused by global warming, all triggered by a lack of sea ice in the Arctic, is just a theory and reminds us that:
During the especially mild winter of 2006/2007 a completely different story was told: Namely that climate change would be more noticeable in milder winter months and less so as hot summer periods and that this one particular mild season would be typical for winters in the future.”
and then adds later in their report:
The very climate scientists who are warning of global warming now say Central Europe must expect colder winters.”
Thanks, DIE WELT, for reminding readers of that. PIK; GISS and Hadley scientists would like to have everyone forget all those now embarrassing computer-model-based forecasts. It’s encouraging to see journalists start wondering about the new theories that keep popping up when things turn out differently.
Spring 2011 forecast to be cold
Finally, DIE WELT tells us that this spring in Central Europe is not going to be warm either, quoting meteorologists. Unfortunately it has been a long brutal winter for much of Europe. And with the end of February upon us, many Europeans are really getting itchy for spring and some real warming. Unfortunately, it’s going to take (quite) awhile longer, so reports DIE WELT.
But all that has changed, and this winter looks like it is about to get a long extension. Although temperatures are forecast to get above freezing by the weekend, meteorologists are forecasting that March and April will remain on the cool side.
How much cold is it going to take to make the rest of us doubt the bogus warming?
Share this...FacebookTwitter "
"Given the Coalition’s unconscionable track record, it is very, very hard to assume the Morrison government will approach anything in climate change policy from a position of good faith. But brace yourself, because I’m going to say something that might surprise you. I don’t think it’s dumb for Scott Morrison to be arguing that the Coalition should develop a roadmap before settling on a long-term emissions reduction target.  Before anyone chokes on their Weetbix, I think it is entirely possible the Morrison roadmap, expected sometime in the coming weeks, will be pap – either more vacuous window dressing or yet another stick to beat political opponents with. It could easily be a script for doing nothing, or not nearly enough, or a tedious apologia for outright target avoidance. It would be idiotic for me to forecast triumph in advance of the facts, so I’m not high-fiving a roadmap I haven’t seen, I’m just saying the core concept of having a roadmap leading to a target isn’t intrinsically terrible. If there is any good faith attached to the exercise, this could be a way to thread a needle. Humour me while I step you through my thinking. Once upon a time, before Tony Abbott weaponised climate change to win elections, it would have been possible for Australia to have just ploughed on with this transition without the endless handwringing and mud wrestling. There would have been a sensible policy mechanism to drive decarbonisation, with governments working around the edges to ensure the transition was as fair as it could be. No fuss, just steady progress. But the Coalition broke the debate. The Liberal and National parties deliberately injected unreason into the polity. Having rendered an entire discussion radioactive in order to re-elect sufficient numbers of Nationals in Queensland to hold on to government, the challenge now arrayed before the Coalition is how to reverse engineer their own bastardry. Given the epic scale of that bastardry – unpicking it takes some doing. Obviously, there is zero prospect of achieving any course correction if the Coalition doesn’t want to change direction. Sadly, it is entirely possible our government will pretend to care about climate change in places where it is electorally advantageous to do so while doing absolutely nothing of substance to arrest the danger. It is possible they are that cynical, even after the terrible portents of the summer. But some inside the government really do want to shift, and Morrison, for now, is leaving that option open. If we accept for a rash moment that the key people do want to pivot, if we accept there is any underlying sincerity or scintilla of national interest consideration going on here, then the case for action will need to be built in increments from the ground up. It has to be built in increments, for two reasons. The first is about internals. If the government pivots too quickly, it will blow up the Coalition. Poor Mathias Cormann made the mistake on Friday of saying the government would be finalising a longer-term target for the next UN-led meeting in Glasgow, only to find himself corrected by Angus Taylor, who declared it would be a long-term “strategy” (because the word target is equivalent to an improvised explosive device. I mean good God, spare us). The second is about external perceptions. The Coalition has been telling Australians for 10 years that ambitious emissions reduction targets are terrible, and nothing has to change. Now it has to contemplate how the opposite can also also true. There is only one way for the opposite to be true that I can see, and that’s to make the case that Australia can make a transition to lower emissions in an orderly way where people aren’t left behind, and in fact, are given new opportunities. So now we are back at the roadmap not being a dumb idea. In a strange way, the Coalition having to navigate its fraught internals, and having to workshop how it might gradually stop lying without losing all credibility, creates opportunity for politicians to tell a story that has only been told fitfully. For more than a decade, the climate debate in Australia has been either a wonkish seminar about carbon pricing, emissions trading, clean energy targets, renewable energy targets, national energy guarantees, safeguard mechanisms, abatement targets, carryover credits, and the like – concepts that are vitally important but carry absolutely no practical meaning for most people – or it has been a slasher movie replete with surround-sound alarmism, hyperbole, intrigue, betrayals and bouts of regicide. The policy is arcane, the politics has been absolutely rancid, and frankly, a lot of the reporting of this issue has been execrable too, and in many places, the reporting goes on being terrible, either deliberately as a corporate mission, or because reporters confuse balance with false balance. It is entirely understandable, given these realities, that voters now don’t know who or what to believe. When everything is a stinking mess, it makes sense to me to retreat to first principles, and build a case for action by, wait for it … telling people what action looks like. What are the new technologies? Are they job-creating or job-destroying? What is the role of government in rolling them out? What will be the consequences of the change for jobs, growth and living standards? Perhaps that story can be a reassuring one. Because what’s before us is more multidimensional than the Coalition, shambling, in fits and starts, towards reality; a government wondering, in its cups, how to market an implicit mea culpa. I think a number of progressive people who are now fully sold on the need for climate action fail to grasp something that is now pretty obvious. The truth is many voters remain unpersuaded. Australians report they are worried about climate change. Poll after poll tells us Australians are becoming more anxious about a lack of action. But a majority are not voting for climate action. They are prioritising other things. This seems a strange kind of dissonance. The election result last May tells us most voters prioritised their immediate economic security over climate action, which leads me to an obvious conclusion: climate action will not happen in Australia unless the debate shifts, and shifts compellingly, to an economic frame. Logically, that process starts with mapping out how decarbonisation intersects with future job prospects; it starts with answering a basic question – what options does this transformation give me? The climate debate in Australia has of course touched on these themes, but there has been no cut through narration by either of the major parties, largely because nobody in major party politics has the guts to say the safety of the planet means fossil fuels are on the way out, and because the whole system has been obsessed with either targets and mechanisms, or with gothic intrigue, like whether the latent communist Malcolm Turnbull has somehow snuck back in to lead the Liberal party, or whether Anthony Albanese looked sideways at a coalminer. So targets are really important. Policy mechanisms also remain critically important. But nothing lasts if voters fear the future. So here is my question: who in politics has the guts to navigate Australia out of climate change Groundhog Day by being honest about what this transition actually means?"
"Since the 1960s winters across Europe have become milder, and in the UK our winters have become warmer and wetter. Big snow dumps are less common now. Much of this change can be blamed on anthropogenic global warming, but it turns out that Europe’s efforts to clean its skies have also reduced the likelihood of harsh winter weather. Air pollution can have a significant impact on our climate. Some pollutants, like sulphate particles, scatter radiation and cause cooling. Air pollution can also change cloud properties. Since the 1970s countries across Europe and North America have significantly curbed their air pollution, resulting in more of the sun’s energy reaching Earth’s surface over these locations. Yuan Wang, from the California Institute of Technology, and colleagues have input air pollution data (gathered between 1970 and 2005) into climate models to investigate how cleaner skies may have impacted atmospheric circulation patterns and extreme weather. Their results, published in Nature Climate Change, reveal that the reduction in air pollution has altered the strength and location of high altitude winds, shifting the jet stream further to the north during winter. This change has suppressed cold extremes over northern Eurasia. Future pollution reduction over China is expected to exert a similar influence."
"

Pundits, politicians and the press have argued that global warming will bring disaster to the world, but there are good reasons to believe that, if it occurs, we will like it. Where do retirees go when they are free to move? Certainly not to Duluth. People like warmth. When weather reporters on TV say, “it will be a great day,” they usually mean that it will be warmer than normal. 



The weather can, of course, be too warm, but that is unlikely to become a major problem if the globe warms. Even though it is far from certain that the temperature will rise, the Intergovernmental Panel on Climate Change (the U.N. body that has been studying this possibility for more than a decade) has forecast that, by the end of the next century, the world’s climate will be about 3.6° Fahrenheit warmer than today and that precipitation worldwide will increase by about 7 percent. The scientists who make up this body also predict that most of the warming will occur at night and during the winter. In fact, records show that, over this century, summer highs have actually declined while winter lows have gone up. In addition, temperatures are expected to increase the most towards the poles. Thus Minneapolis should enjoy more warming than Dallas; but even the Twin Cities should find that most of their temperature increase will occur during their coldest season, making their climate more livable.



Warmer winters will produce less ice and snow to torment drivers, facilitating commuting and making snow shoveling less of a chore. Families will have less need to invest in heavy parkas, bulky jackets, earmuffs and snow boots. Department of Energy studies have shown that a warmer climate would reduce heating bills more than it would boost outlays on air conditioning. If we currently enjoyed the weather predicted for the end of the next century, expenditures for heating and cooling would be cut by about $12.2 billion annually.



Most economic activities would be unaffected by climate change. Manufacturing, banking, insurance, retailing, wholesaling, medicine, educational, mining, financial and most other services are unrelated to weather. Those activities can be carried out in cold climates with central heating or in hot climates with air conditioning. Certain weather‐​related or outdoor‐​oriented services, however, would be affected. Transportation would benefit generally from a warmer climate since road transport would suffer less from slippery or impassable highways. Airline passengers, who often endure weather‐​related delays in the winter, would gain from more reliable and on‐​time service.



The doomsayers have predicted that a warmer world would inflict tropical diseases on Americans. They neglect to mention that those diseases, such as malaria, cholera and yellow fever, were widespread in the United States in the colder 19th century. Their absence today is attributable not to a climate unsuitable to their propagation but to modern sanitation and the American lifestyle, which prevent the microbes from getting a foothold. It is actually warmer along the Gulf Coast, which is free of dengue fever, than on the Caribbean islands where the disease is endemic. My own research shows that a warmer world would be a healthier one for Americans and would cut the number of deaths in the U.S. by about 40,000 per year, roughly the number killed on the highways.



According to climatologists, the villain causing a warmer world is the unprecedented amount of carbon dioxide we keep pumping into the atmosphere. As high school biology teachers emphasize, plants absorb carbon dioxide and emit oxygen. Researchers have shown, moreover, that virtually all plants will do better in an environment enriched with carbon dioxide than in the current atmosphere, which contains only trace amounts of their basic food. In addition, warmer winters and nights would mean longer growing seasons. Combined with higher levels of CO2, plant life would become more vigorous, thus providing more food for animals and humans. Given a rising world population, longer growing seasons, greater rainfall, and an enriched atmosphere could be just the ticket to stave off famine and want.



A slowly rising sea level constitutes the only significant drawback to global warming. The best guess of the international scientists is that oceans will rise about 2 inches per decade. The cost to Americans of building dikes and constructing levees to mitigate the damage from rising seas would be less than $1 billion per year, an insignificant amount compared to the likely gain of over $100 billion for the American people as a whole. Let’s not rush into costly programs to stave off something that we may like if it occurs. Warmer is better; richer is healthier; acting now is foolish.
"
"
Share this...FacebookTwitter
Prof. Hans von Storch’s site Klimazwiebel here brings our attention to a new site called Information portal climate change posted by the Austrian ZAMG – Austria’s Central Bureau for Meteorology and Geophysics.
Normally Klimazwiebel posts in English, but because, I suppose, the links and related literature are in German, Prof. von Storch posted this one in German.
The new ZAMG site has the purpose of providing site visitors with a trustworthy resource on the subject of climate science. It is for informing the public.
One of the contributors to the site is Dr. Reinhard Böhm of the ZAMG. Translating the quote provided by Klimazwiebel, Reinhard Böhm writes, interestingly:
In more than 80 individual articles, which have about 1000 references and links and offer additional literature, we wish to create a work of ‘Public Science’ at a scientifically reasonable level. We think we have been successful in achieving this. With this, as a body of rational information, we hope to deliver a counter view – especially at this time – when the COP 16 conference in Mexico will again provide the usual international climate hype.”
Even a moderate warmist like Böhm admits what the IPCC is all about.
In my view the ZAMG Internet resource is warmist, yet rational and not alarmist. I haven’t yet closely examined the links and data the site uses to make the assertions it does on the subject of science, but to me my first impression is that it’s too warmist and does not explain why this warming is any different from previous warmings throughout history.
ZAMG is a government funded institution. One thing is sure: climate science is very politicised and depends heavily on the source of funding. One can only go out so far on a limb before it gets sawn off.
Reinhard Böhm is the author of the book Heiße Luft. Reizwort Klimawandel – Fakten, Ängste, Geschäfte (Hot Air Word of Controversy Climate Change – Facts, Fears, Business).

Share this...FacebookTwitter "
"

As part of a yearly tradition, the Cato Institute and Heritage Foundation co-host a debate in which interns of both think tanks debate whether conservatism or libertarianism is a better ideology. Following this year’s debate, the Cato Institute conducted a post-debate survey of attendees to ask who they thought won the debate and what they believe about a variety of public policy and social issues.   
  
The survey finds that millennial conservative and libertarian attendees agree on matters of free speech and religious liberty, the size and scope of government, regulation, health care and what to do about climate change. However, striking differences emerge between the two groups particularly on matters of immigration, the temporary Muslim travel ban, gender pronouns and bathrooms, government’s response to opioid addiction, the death penalty, religious values in government, domestic surveillance, foreign policy, as well as evaluations of the Trump administration.   
  
_Full LvCDebate Attendee Survey results foundhere_   
  
**Priority Differences and Similarities**   
  
Examining conservative and libertarian millennial attendees’ issue priorities offers a quick overview of their similarities and differences. The survey asked attendees how concerned they are about 21 different issues:   








As the chart shows above, conservative millennials are more concerned about morality in society, abortion, terrorism, national security, drug use, and immigration. Libertarian millennials are more concerned about government domestic surveillance, the criminal justice system, and trade. Top priorities shared by both groups include the size and scope of government, free speech, government spending and debt, the economy, and taxes. Notably, libertarians and conservatives share their lowest priority: few are concerned about income inequality.   




**Voting in 2016**   
  
Although 88% of conservative millennial attendees identify as Republican (and 100% do if you include independent leaners), only 51% voted for Donald Trump in 2016. Nevertheless, this is considerably higher than the 20% of libertarian millennial attendees who voted for Trump. Among both sets of Trump voters, fully 7 in 10 said their vote was _against_ Hillary Clinton rather than a vote _for_ Trump. Thus, President Trump received few enthusiastic votes among this group of politically engaged millennial conservatives and libertarians.   
  
While a majority of conservative attendees ultimately voted for Trump, a majority (55%) of libertarian attendees voted for Libertarian candidate Gary Johnson instead. Few voted for Democratic candidate Hillary Clinton (3%). In fact, more said they did not vote (19%) than voted for Clinton.   




**Party Identification**   
  
Conservative millennials overwhelmingly (88%) identify as Republicans, while most libertarian attendees identify with the Libertarian Party (42%) or as politically independent (36%).   




However, after asking independents and libertarians if they lean toward a political party, 100% of the conservative millennial attendees leaned with the Republican Party. A majority (58%) of libertarian millennial attendees did as well, while a third said they are truly independent, and 6% identified as Democrats.   
  
**Evaluations of Trump and Key Political Figures**   
  
Although few of the libertarian and conservative millennial attendees were enthusiastic supporters of Trump during the election, 64% of conservative attendees approve of Trump’s job performance. In stark contrast, 80% of libertarian attendees disapprove. Nevertheless, conservative approval is “soft” with only 12% “strongly” approving. Libertarians are more ardently opposed, with 53% who “strongly disapprove” of President Trump.   




Major differences also emerge in evaluations of key political figures. Notably, while 8 in 10 conservative millennial attendees have a favorable opinion of Attorney General Jeff Sessions, 8 in 10 libertarian attendees have an unfavorable view of him. Libertarian aversion likely stems from disagreements about the criminal justice system, policing, and drug policy. Conservative attendees also have favorable views of Kellyanne Conway (62%), former campaign manager and now Counselor to President Trump, while libertarian attendees do not (22%). Conversely, libertarian millennial attendees have positive views of former Gov. Gary Johnson (61%), while conservatives do not (18%). Conservative attendees are also about twice as likely as libertarians to have positive views of Senators Ted Cruz (88% vs 50%) and Marco Rubio (91% vs 48%).   
  
Libertarian and conservative millennial attendees come together in their shared favorable views of Senator Rand Paul, Education Secretary Betsy Devos (likely due to their shared support of school choice), and writer George Will. They also share unfavorable views of Steve Bannon, Milo Yiannopoulos, and Ann Coulter, figures more closely associated with the “alt-right.”   




**Where Do They Get their News?**   
  
Conservative and libertarian millennial attendees share similar news consumption habits, sharing five of their top six news outlets: the Wall Street Journal (85%, 86%), the New York Times (66%, 75%), the Washington Post (68%, 72%), CNN (50%, 48%) and National Review (76%, 61%). However, conservatives are about 30 points more likely to regularly watch Fox (74% vs 45%) and 15 points more likely to read the Federalist (51% vs 36%). Conversely, libertarians are 53 points more likely to read _Reason_ (24% vs 77%).   




**Culture Wars and Transgender Issues**   
  
Conservatives and libertarian millennials are starkly at odds when it comes to what pronouns to use when referring to transgender people. When referring to a transgender person, 68% of libertarian millennial attendees choose to use the person’s preferred gender pronouns. In contrast, 67% of conservative millennial attendees say they use the pronouns corresponding with the transgender person’s biological sex.   




This difference extends to bathroom access as well. Eight in 10 conservative millennials say transgender people should be required to use the restroom corresponding with their biological sex. Conversely, 70% of libertarian millennials say transgender people should be allowed to use the restroom of the gender they identify with.   
  
Most conservatives (81%) disagree that we as a society need to do more to ensure LGBT people feel fully accepted. Meanwhile, libertarians are split, with 50% believing more should be done for LGBT people to feel accepted and 47% agreeing with conservatives that no more needs to be done.   




Despite these differences, conservatives and libertarians agree on religious liberty: 99% of conservative and 87% of libertarian respondents believe that businesses should be allowed to refuse service to same-sex weddings.   
  
**Immigration**   
  
Conservative and libertarian millennials diverge dramatically on questions of illegal _and legal_ immigration. While 79% of libertarians support increasing the number of immigrants allowed into the country each year, only 20% of conservatives agree. Instead, most conservatives would prefer to decrease the number (35%) or keep it the same (45%).   




When it comes to handling illegal immigration, a majority (52%) of conservative attendees support deporting illegal immigrants and 24% wish to bar them from citizenship. In contrast, 70% of libertarian attendees want to allow illegal immigrants to stay in the US and be able to eventually apply for citizenship.   




Finally, 69% of conservatives favor building a wall along the Mexican border, but 90% of libertarians oppose. However, conservatives are less intensely in favor of the wall than libertarians are opposed to it: only 14% of conservatives “strongly favor” while 67% of libertarians “strongly oppose” its construction.   




When it comes to passing a temporary ban on Muslims immigrating to the United States, conservative attendees are evenly divided, while 86% of libertarian attendees are opposed.   




**Opioid Epidemic**   
  
While most likely share concerns about overuse of prescription painkillers, conservative and libertarian attendees starkly disagree about what government should do about it. Eight in 10 conservatives agree “government needs to do more” to combat prescription painkiller addiction, but 8 in 10 libertarians disagree that government should take on this role.   




**Organ Donations**   
  
It is currently illegal to buy or sell human organs. However, 92% of libertarian attendees believe such a market should be legal; 8% agree with the status quo. Conservatives are ardently opposed with 72% who think such a market should remain illegal, while 28% would favor legalization.   
  
**Foreign Policy and National Security**   
  
Conservative and libertarian millennials sharply disagree on questions of foreign policy. Nearly 90% of conservative attendees support either increasing (38%) or maintaining (51%) our military presence around the world, while 84% of libertarians support decreasing this presence. Moreover, 92% of libertarian respondents support cutting defense spending to help balance the federal budget, while 72% of conservatives oppose such cuts.   




Conservative and libertarian attendees also make different trade-offs between national security and privacy. Seven in 10 conservatives say they’d be willing to give up some personal freedom and privacy for the sake of national security. In contrast, 9 in 10 libertarians say they would not be willing to give up more freedom and privacy for security.   
  
In line with such priorities, 60% of conservative attendees approve of government collection of domestic telephone and Internet data, while 92% of libertarians disapprove of this collection.   
  
**Health Care**   
  
On matters of health care, conservatives and libertarians are often aligned—particularly when it comes to repealing the Affordable Care Act/Obamacare (99%, 95%). However, similar to what’s playing out at the Congressional level, libertarians and conservatives disagree about how to improve the health care system in the country. Three-fourths (76%) of libertarians say the health care system “needs to be completely rebuilt.” However, conservatives are evenly divided with 49% agreeing that the system should be rebuilt, but 46% who think the system “needs major reform but doesn’t need to be completely rebuilt.” Less than 5% of either group think the current system works well and only needs minor changes.   




**Climate Change**   
  
Libertarian and conservative millennial attendees disagree about the causes of climate change but they agree on a solution. Conservatives are more likely to believe climate change is a natural phenomenon, with 54% believing increases in Earth’s temperature are either mostly or entirely due to natural causes. Meanwhile, 62% of libertarians think climate change comes partially, mostly, or entirely from human activity. Despite these different underlying beliefs, 99% of conservative and libertarian attendees agree that technological innovation in the free market will better solve climate change than government regulation.   




**Criminal Justice**   
  
Majorities of conservatives and libertarians reach consensus on several criminal justice issues. Both agree that police departments using military weapons and drones are not necessary for law enforcement purposes. But libertarians (92%) agree more than conservatives (61%). Both groups also favor eliminating mandatory minimum prison sentences for people convicted of selling drugs (64% conservative, 84% libertarian).   
  
However, conservative and libertarian attendees diverge on the death penalty: a majority (56%) of conservatives favor it while a majority (75%) of libertarians oppose it. Respondents also diverge in their perception of racial equality before the law, with 54% of conservatives saying that African Americans and other minorities “receive equal treatment with whites” in the criminal justice system and 73% of libertarians believing that minorities do not receive equal treatment.   




**Free Markets and the Welfare State**   
  
Despite the variety of policy differences between libertarian and conservative attendees outlined above, the two groups largely agree about economic issues, the benefits of free markets, and trade.   
  
For instance, 100% of both groups say they favor a smaller government providing fewer services and low taxes. Nearly 100% of both oppose raising taxes on wealthy households and also agree that regulation too often does more harm than good. Eight in 10 conservatives and nearly 100% of libertarians believe free trade must be allowed even if domestic industries are hurt by foreign competition.   
  
**Religion**   
  
Conservative and libertarian millennials have different ideas about the role of religion in society. An overwhelming majority (83%) of libertarian attendees say religious values should _not_ play a more important role in government. But, 62% of conservative attendees disagree and think such values should play a more important role. Conservatives also believe that it’s important for kids to be brought up with religious values. Libertarians are divided, but tend to disagree (56%).   




Much of this contrast may stem from differences in religious affiliation and habits. While 95% of conservative respondents have a religious preference, 38% of libertarians describe themselves as “non-religious.” Moreover, conservative millennial attendees are twice as likely as libertarians to attend church weekly or monthly (80% vs 41%). A majority (58%) of libertarian respondents either never or rarely attend a religious service.   




**Who Won the Intern Debate?**   
  
Who won the intern debate depends on whom you ask. Among conservative millennial attendees: 53% said the conservative team won and 44% said the libertarian team won. Among libertarian millennial attendees, 94% said the libertarians won while 5% said the conservatives won. Among the moderates, liberals and progressives in the audience, 83% felt the libertarian team won and 12% thought the conservatives won.   
  
**Implications**   
  
This survey provides a useful snapshot of young politically engaged conservatives and libertarians who are interested enough in politics and public policy to intern in Washington or attend an event for Washington interns. Thus, this data offers an idea of the direction young activists may take public policy as they age and the cleavages that may animate policy debates into the future.   
  
_Full LvCDebate Attendee Survey results foundhere_   
  
_David Kemp contributed to this report._


"
"**Gyms and non-essential shops in all parts of England will be allowed to reopen when lockdown ends next month, the prime minister has announced.**
Boris Johnson told the Commons that the three-tiered regional measures will return from 2 December, but he added that each tier will be toughened.
Spectators will be allowed to return to some sporting events, and weddings and collective worship will resume.
Regions will not find out which tier they are in until Thursday.
The allocation of tiers will be dependent on a number of factors, including each area's case numbers, the reproduction rate - or R number - and the current and projected pressure on the NHS locally.
Tier allocations will be reviewed every 14 days, and the regional approach will last until March.
The PM, who is self-isolating after meeting an MP who later tested positive for coronavirus, told MPs via video link he expected ""more regions will fall - at least temporarily - into higher levels than before"".
He said he was ""very sorry"" for the ""hardship"" that such restrictions would cause business owners.
Speaking later at a Downing Street briefing, Mr Johnson added that ""things will look and feel very different"" after Easter, with a vaccine and mass testing.
He warned the months ahead ""will be hard, they will be cold"" - but added that with a ""favourable wind"" the majority of people most in need of a vaccination might be able to get one by Easter.
Until then, the PM said, there would be a three-pronged approach of ""tough tiering, mass community testing, and [the] roll-out of vaccines"".
Describing how the tiers had become tougher, the PM said:
Where pubs and restaurants are allowed to open, last orders will now be at 10pm, with drinkers allowed a further hour to finish their drinks.
Indoor performances - such as those at the theatre - will also return in the lower two tiers, although with reduced capacity.
In terms of households mixing, in tier one a maximum of six people can meet indoors or outdoors; in tier two, there is no mixing of households indoors, and a maximum of six people can meet outdoors; and in tier three - the toughest tier - household mixing is not allowed indoors, or in most outdoor places.
In all tiers, exceptions apply for support bubbles. From 2 December, parents with babies under the age of one can form a support bubble with another household.
Mr Johnson said the tiers would now be a uniform set of rules, with no negotiations on additional measures for any particular region.
Measures in Scotland, Wales and Northern Ireland continue to be decided by the devolved administrations, but a joint approach to Christmas, involving all four nations, will be set out later in the week.
The prime minister said: ""I can't say that Christmas will be normal this year, but in a period of adversity time spent with loved ones is even more precious for people of all faiths and none.
""We all want some kind of Christmas; we need it; we certainly feel we deserve it.
""But this virus obviously is not going to grant a Christmas truceâ¦ and families will need to make a careful judgement about the risks of visiting elderly relatives.""
For the third week running we have had some positive vaccine news, but the announcement about the toughened tiers is a reminder, if we needed any, that the next few months will be tough.
Ministers and advisers have been hinting for the past week that the tiers will be toughened - and that is exactly what has happened.
Attention will now naturally turn to which areas will be in which tiers.
Deciding that is a complex equation that will take into account whether the cases are going up or down, the percentage of tests that are positive, hospital pressures and infection rates among older age groups.
To give a flavour of how complex this is places in the North West and Yorkshire have some of the highest rates but they are falling the fastest.
London and the South East have lower rates and more hospital capacity but cases are going up.
Fine judgements will have to be made. We will find out on Thursday.
Mr Johnson also announced changes to sport for both spectators and participants.
While elite sport has continued behind closed doors during the lockdown, grassroots and amateur sport has been halted since 5 November.
From 2 December, outdoor sports can resume, while spectators will be allowed to return in limited numbers. Some organised indoor sports can also resume.
In the lowest risk areas, a maximum of 50% occupancy of a stadium, or 4,000 fans - whichever is smaller - will be allowed to return. In tier two, that drops to 2,000 fans or 50% capacity, whichever is smaller.
In tier three, fans will continue to be barred from grounds.
In tiers one and two, business events can also resume inside and outside with tight capacity limits and social distancing, as can indoor performances in theatres and concert halls, the government's plan says.
Labour leader Sir Keir Starmer described the government's return to the regional system as ""risky... because the previous three-tier system didn't work"".
He added that decisions on which areas will belong to each tier must be taken without delay - ""I just can't emphasise how important it is that these decisions are taken very quickly and very clearly so everybody can plan.
""That is obviously particularly important for the millions who were in restrictions before the national lockdown, because the message to them today seems to be 'you will almost certainly be back where you were before the national lockdown - probably in even stricter restrictions'.""
Helen Dickinson, of the British Retail Consortium, said shops would be ""relieved"" at the decision to allow them to reopen.
""Sage data has always highlighted that retail is a safe environment, and firms have spent hundreds of millions on safety measures including Perspex screens, additional cleaning, and social distancing and will continue to follow all safety guidance,"" she said.
But the UK hospitality industry warned the new rules ""are killing Christmas and beyond"" and said pubs, restaurants and hotels faced going bust.
Meanwhile, a further 15,450 positive coronavirus cases were recorded across the UK on Monday. There have also been a further 206 deaths within 28 days of a positive test. Figures can be lower on a Monday, due to a lag in reporting.
Earlier, it was announced that daily coronavirus tests will be offered to close contacts of people who have tested positive in England, as a way to reduce the current 14-day quarantine period.
Mr Johnson said people will be offered tests every day for a week - and they will not need to isolate unless they test positive.
He also said rapid tests will allow every care home resident to have up to two visitors tested twice a week."
"
From ICECAP

By Joseph D’Aleo CCM, AMS Fellow
2008 will be coming to a close with yet another spotless days according to  the latest solar image.

This will bring the total number of sunspotless days this month to 28 and for  the year to 266, clearly enough to make 2008, the second least active solar year  since 1900.

See larger image here.
The total number of spotless days this spolar minimum is now at around 510  days since the last maximum. The earliest the minimum of the sunspot cycles can  be is July 2008, which would make the cycle length 12 years 3 months, longest  since cycle 9 in 1848. If the sun stays quiet for a few more months we will  rival the early 1800s, the Dalton Minimum which fits with the 213 year cycle  which begin with the solar minimum in the late 1790s.

See larger image here.
Long cycles are cold and short ones like the ones in the 1980s and 1990s are  warm as this analysis by Friis-Christensen in 1991 showed clearly. 

See larger image here.
In reply to the arguments made that the temperatures after 1990 no longer  agreed with solar length, I point out that it was around 1990 when a major  global station dropout (many rural) began which led to an exaggeration of the  warming in the global temperature data bases. Also the length from max to max of  21 to 22 was 9.7 years and cycle 22 length min to min 9.8 years, both very short  suggesting warm temperatures in the 1990s. The interval of cycle 22 max to cycle  23 max centered in the mid 1990s began to increase at 10.7 years and the min to  min length of cycle 23 is now at least 12.3 years.
With the Wigley suggested lag of sun to temperatures of 5 years and  Landscheidt suggested 8 years, a leveling of should have been favored around  2000-2003 and cooling should be showing up now.  Looking ahead, put that  together with the flip of the PDO in the Pacific to cold and you have alarming  signals that this cooling of the last 7 years will continue and accelerate.


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e99a67de2',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

Americans just aren’t scared enough by global warming to take it seriously. That’s the lesson from the poll published in the _Washington Post_ last month showing prospective Savior‐​in‐​Chief Al Gore down by 19 percent. If people really thought Gore’s pet cause was such a threat, wouldn’t they vote to protect their children? Can “Clinton fatigue” be that bad? 



Our friends at the United Nations understand the need to get the United States more involved in stopping global warming. They also understand Americans’ basic sense of fair play and compassion, as evinced by our sending troops to Somalia, Haiti, Kosovo, Ersatz‐​Yugoslavia and maybe Dili. If some nation can convince us it’s getting the short end of the stick, U.S. largesse is not far behind. 



To enlist our help, the United Nations is currently holding a special conference of “island states” that view themselves as threatened by global warming in general and sea level rise in particular. 



“In low‐​lying areas, the sea has claimed our burial grounds,” said Samoan UN envoy Tuiloma Neroni Slade, chairman of the Alliance of Small Island States (AOSIS), an official UN hectoring organization. Slade added that in the Maldives, about 800 miles south of Bombay, “Climate change is already taking effect in terms of some of the life support systems.” Ditto for the Marshall Islands, Vanuatu et cetera. 



Slade is banking on Americans’ being too guilt ridden to check the facts and ratifying the Kyoto Protocol on global warming pronto in order to make up for our sins. Unfortunately, facts are just a click away. 



The 1995 report of the UN’s Intergovernmental Panel on Climate Change (IPCC) contains a whole chapter on sea level rise, complete with charts. The monitoring station closest to Samoa is Sydney, Australia, where there has been a truly tiny rise in sea level of only 3.14 inches in the last 100 years. But almost all of that took place before 1950. Since then, the rise in sea level, which has “claimed their burying grounds,” has been 0.4 inches. 



In the IPCC report, Bombay is the station nearest the Maldives. As Casey Stengel used to say, “You could look it up,” and there it is: sea level has fallen an inch in Bombay in the last 50 years. 



Nice try, Mr. Slade. 



Of course, the IPCC forecasts that sea level will rise in the next 100 years. The most recent projection gives two median values: 19.3 inches from one model and 10.6 inches from another termed “equally plausible.” But global warming is proceeding at a slower pace than those models assumed, so it’s probably a good idea to cut the totals by a third or so. Could Pacific Islanders adapt to 10 inches of sea‐​level rise in the course of a century? 



Consider the Outer Banks of North Carolina, where, every few years, the sea rises about 12 feet in 10 minutes. This is a hurricane. Because of hurricanes, up until 1950 or so very few people lived there. Fearing the wind, the handful of mainlanders who came in the 1950s built little one‐​story “flattop” homes, nestled beneath the dune crest to protect them from the wind. 



When away from home, those people were able to charge $100 or so a week for a summer “beachfront” rental, which really meant a human‐​eye view of the barrier dune. The flattop owners then discovered that wind wasn’t the problem after all, as their vacation houses were washed away into the sea by the numerous hurricanes of the 1950s and 60s. 



One day they got the fine idea of elevating their homes on stilts so the sea could rush harmlessly underneath during a hurricane. Of course, that didn’t protect them from a direct hit by the northeastern eyewall of a Category 3 storm. In that case, a beachfront home is usually plumb out of luck, but the damage swaths in such storms are surprisingly narrow considering the thousands of miles of developed coastline from Brownsville, Texas, to Eastport, Maine. Somehow, damaged areas tend to appear larger on TV. 



Stilts protect the houses from most every other hurricane. And, as a side benefit of the elevation of their homes, vacationers now view sunrises over the Atlantic Ocean and sunsets over Albemarle Sound from the same house. Rent skyrocketed, to $5,000 per week in high season. 



A trip to Kwajelein in the Marshall Islands reveals that most of the homes are as close to the ground as they were in North Carolina before someone discovered how to make big bucks and survive foot after foot of extremely rapid tidal inundation. It seems probable that the AOSIS people will figure out how to adapt to 10 inches of sea‐​level rise in 100 years. They don’t need our help to raise property values fiftyfold. And if they don’t, it won’t be because they couldn’t.
"
"Scott Morrison has described a report he may adopt a technology investment target to avoid signing up to a commitment of zero greenhouse gas emissions by 2050 as speculation, but confirmed his government will take a “technology over taxation” approach to climate change. On Tuesday, Morrison told reporters in Melbourne the report in the Australian was “very speculative”, but said it was true that emissions reductions were achieved through technology, not “meetings”.  The Australian suggested Morrison favoured a technology investment target as a way to help Australia resist an international push for a more explicit commitment to reduce emissions to net zero by mid-century at the next major UN climate summit in Glasgow in November. The government is expected to soon release what it calls a technology investment roadmap, but has said little about what it will entail. Several moderate Liberal MPs including Katie Allen and Trent Zimmerman have noted Australia effectively committed to net zero emissions by signing the 2015 Paris agreement. Under that deal, countries agreed to keep global heating above pre-industrial levels below 2C and to pursue policies to restrict it to 1.5C. Morrison has begun the political year torn between moderate Liberals attempting to build traction internally for the government to increase climate action and resistance from the Nationals, who want more government support for coal-fired power. Cabinet discussions have explored how to reposition the government’s climate policies, with a focus on technology, as the bushfire crisis caused a spike in concern about the environment and a hit to Morrison’s popularity. At a press conference on Tuesday, Morrison reiterated that Australia would not make commitments without “having thoroughly looked at what is the impact on jobs”. Asked if an investment target would create tension with those who want a net zero emissions commitment, Morrison said “currently no one can tell me that going down that path won’t cost jobs, won’t put up your electricity prices, and won’t impact negatively on jobs in the economies of rural and regional Australia”. A major report by CSIRO last year found there was no trade-off between strong economic growth and transitioning to zero emissions. The result of two years’ work by 50 leaders across the community, the CSIRO’s Australian National Outlook report found bold action to combat rising challenges could lead to GDP growth of 2.76% to 2.8% annually, a 90% increase in real wages and net zero emissions by 2050. On Tuesday, Morrison suggested investment in technology would enable steel plants to use hydrogen power and increase uptake of renewable energy. “The smart way in dealing with this to get emissions down … is to focus on the technology and making sure that that technology is affordable and it is scalable,” he said. “You want to get global emissions down? … You need technology that can be accessed and put in place, not just here in Australia, but all around the world. Meetings won’t achieve that, technology does. And I can tell you taxes won’t achieve it either.” The industry minister, Karen Andrews, told reporters at Flinders University she was focused on “what the solutions will be rather than endlessly discussing targets or whether or not climate change is real”. Andrews said it was “clear that we do have businesses … that are already investing”, citing BAE and the Innovative Manufacturing Cooperative Research Centre. Labor’s climate change spokesman, Mark Butler, said a technology target would be a throwback to 2007, when then prime minister John Howard and US president George W. Bush rejected emissions reduction targets agreed under the Kyoto Protocol in favour of a “technology approach” that including support for nuclear power and “clean coal”. “Of course technology is the key to lowering emissions, but industry has made it clear that the take-up of new technology requires a serious energy and climate policy,” Butler said. “This is something the Morrison government will never deliver as long as the hard-right climate deniers continue to run the show on climate policy.” The leader of the Greens, Adam Bandt, said: “Instead of a technology smokescreen, we need a Green New Deal to work with affected communities and workers to phase out coal, create new industries and look after the workers in those communities who are affected”. Erwin Jackson, policy director with the Investor Group on Climate Change, said acting on technology was important but not enough. He said all policy needed to be tied to an emissions goal. Net zero emissions was the most credible long-term scenario and had widespread backing, including within the business community. “Setting a path to net zero emissions is prudent economic risk management,” Jackson said. “The only question is if it’s going to happen in a smooth way or in a disruptive way.” Martijn Wilder, a partner at climate advisory and investment firm Pollination and chairman of the Australian Renewable Energy Agency (Arena), said technology would play a critical role in reaching net zero emissions if backed by strong policies. “With the right mix of technologies and policies, Australia is easily capable of reaching net zero well before 2050 and in doing so building an incredibly strong decarbonised economy,” he said. Wilder said having strong public institutions, such as Arena and the Clean Energy Finance Corporation, and policies similar to Britain’s promised ban on new petrol, diesel and hybrid cars by 2035 could “greatly accelerate the transition”. In addition to the technology investment roadmap, the government has commissioned a review of its climate policies led by the businessman Grant King, and promised an electric vehicle policy and a long-term emissions strategy. It has also made it known it is likely to give an $11m grant to the owners of the Vales Point coal-fired power plant in the May budget."
"When MPs announced a citizens’ assembly on the climate emergency last June, two crucial things hadn’t yet happened: Boris Johnson’s takeover of the Conservative party; and the subsequent general election campaign where the main opposition parties each offered radical plans to address the climate crisis, and then lost to Johnson, who had offered no plan at all. For everyone hoping for action on climate, the election was a particularly bruising experience. First throwing open the door to a previously unthinkable possibility – immediate, concrete plans to fight the crisis, far beyond anything proposed by the inadequate Paris Agreement – and then, just as quickly, slamming that door shut. Perhaps even more tightly than before, given Johnson’s disinterest in all things climate-related. And because now the party without an apparent serious climate plan is in charge of taking the critical first steps towards Theresa May’s government’s goal of hitting net-zero by 2050, while the parties willing to commit to action are shut out of power by the enormous Tory majority.  The climate assembly – which met for the second time in Birmingham last weekend – was created to act as a kind of workaround for traditional partisan deadlock, and to chart a safe route forward for governments to act on the climate crisis. Its conceit is that it offers direct access to the real will of the people: 110 citizens – chosen to be representative of the British population – attend sessions where they are briefed by experts on the issue; they then come up with a set of policies to solve it. A citizens’ assembly in Ireland helped the government to put forward the referendum that ended that country’s abortion ban last year. The mood at the assembly so far has been hopeful. The setup assumes that our political leaders have the best intentions, but are paralysed by indecision, both because of the unfathomable number of options and the fear that the public will punish them for the wrong choice. The whole function of politics here is outsourced to the public: they’re asked to make their own tough decisions. And so they have sat attentively listening to presentations on the greatest hits of never-tried climate policy, and have been asked to weigh them up. There was mention of personal carbon allowances (be your own carbon market), a very popular idea in 2008, and more recent schemes such as creating nationwide repair networks, and forcing manufacturers to build goods to last. All the solutions presented reflect the mainstream of British climate policy over the past two decades. The experts put forward a fairly cautious mix of technocratic market incentives and regulation, with large spending programmes reserved for infrastructure projects. Still, there were signs that the assembly might take some bold decisions. The experts barely mentioned free public transport, yet were quizzed on it incessantly. And the members wanted to know who was most responsible for emissions, and how they could be made to pay. The polls suggest that public opinion has leapt ahead of the government’s climate ambition over the past year, and it’s possible that the assembly recommendations will confirm that. But even if the assembly puts the most progressive options in front of the government, there’s still the sticky question of implementation. “How can we be sure the government will follow on our recommendations,” an assembly member asked Labour MP Rachel Reeves, one of only three elected officials in attendance. She didn’t have a good answer, but she could easily have told them it’s not on her – she’s only ever been in opposition during her 10-year tenure in parliament. The assembly’s most valuable contribution may be breaking the deadlock between equally effective but controversial policies – a land-use carbon tax or a consumption tax, for example – although the Tories may well choose neither. The problem with how to tackle the climate crisis in the UK is not partisan deadlock, but lack of government interest. A citizens’ assembly can’t change that. Its decisions aren’t guaranteed to sway the government from its prearranged course. The Irish assembly is held up as a model for its recommendations on the abortion referendum. However, it also submitted proposals on the climate crisis, from huge investments in peat restoration and public transport to a tax on agriculture emissions. The Irish government ignored all of these, in favour of what the prime minister, Leo Varadkar, called “ambitious but realistic” policies to “nudge people to change behaviour”. Nudging people towards radical change is a depressing continuation of the managerial away-day optimism that has dominated thinking on the climate crisis over the past 20 years. One would have better luck trying to nudge a stream back up a mountain. Instead, the best chance for tackling the crisis comes from politicians and parties who don’t wait for incremental solutions. Party politics has become an unexpected laboratory for fusing climate policies to big, popular, social spending programmes – based on the model of a green new deal, which was first put forward by experts in 2008. Unlike a sitting government, opposition policy shops are a frictionless environment: ambition can be doubled and redoubled without pushback. And if the programmes capture the public attention, they are quickly copied by other parties. Labour proposed a green industrial revolution based on using the resources of the state to foster green industries, creating jobs and increasing living standards. This in turn pushed the Liberal Democrats to try to match the offer – remarkable for a party committed to the decentralisation of state power. In the US, the Bernie Sanders campaign is not only currently leading the polls to win the Democratic nomination with the Green New Deal as a top-line policy, it has also pushed every other candidate towards embracing the idea – something that would have seemed impossible just four years ago.  For a long time it has been assumed that public opinion is a barrier to climate action. But the climate assembly will likely confirm what the polls have been indicating for the past year: that people are now ready to move further and faster on climate action than the minimal effort shown by the government. If their advice is ignored or diluted beyond recognition, then maybe citizens’ assemblies are an imperfect mechanism for the scale of change needed to tackle the climate crisis. Only a green new deal for the UK will do. • Stephen Buranyi is a writer specialising in science and the environment"
nan
"

Ten years ago the Alps endured a virtually snowless winter. Environmentalists blamed global warming. A Swiss lobbying group, Alp Action, wrote in 1991 that global warming would put an end to winter sports in the Alps by 2025. 



This year the Alps have had their greatest snowfall in 40 years, according to very preliminary data. Greenpeace has blamed global warming. 



How in the world can that be? Is it possible to blame global warming for every weather anomaly, even if two consecutive events are of opposite sign? Can such a claim have “scientific” justification? 



If one regards the United Nations as an authority on such things, the answer, unfortunately, is yes. Global warmers, thanks to the good offices of the U.N. Intergovernmental Panel on Climate Change, can blame any weather event on pernicious economic prosperity and resultant greenhouse gas emissions. 



The most recent IPCC summary on climate change was published three years ago. IPCC purports to be the “consensus of scientists” but in fact is a group of individuals hand‐​picked by their respective governments. Does anyone really expect Al Gore to send me to represent the United States at one of those meetings? (Thank you, no, I have been to one and that was enough.) 



Absent my sage advice, here’s what the United Nations wrote in 1995: “Warmer temperatures will lead to … prospects for more severe droughts and/​or floods in some places and less severe droughts and/​or floods in others.” 



As a punishment for not cleaning out the cat box, you might ask your kid to diagram this sentence. Rather than strain the graphics of this word processor, we’ll simply parse it. What the IPCC is saying is that global warming will cause in “some places” and/​or “others”: 



So, according to the “consensus of scientists,” it’s OK to blame a flood, or, if you’re in the mountains, a flood of snow, on global warming. It’s also OK to blame a drought or a snowless Alp on global warming. 



It’s even OK to blame weather that is more normal than normal (“less intense wet and dry periods”) on global warming. 



The IPCC statement, which cannot be proved wrong, is a cynical attempt to allow anyone to blame anything on global warming. As Julius Wroblewski of Vancouver, Canada, wrote to me, this logic “represents a descent into the swamp of the non‐​falsifiable hypothesis. This is not a term of praise. Falsifiability is the internal logic in a theory that allows a logical test to see if it is right or wrong.” 



A non‐​falsifiable theory is one for which no test can be devised, and the U.N. statement fits the bill perfectly. There is simply no observable weather or climate that does not meet its criteria, except one: absolutely no change in the climate, meaning no change in the average weather or the variability around that average. 



Every climatologist on the planet knows that is impossible. Climate has to change because the sun is an inconstant star and the Earth is a nonuniform medium whose primary surface constituent, water, is very near its freezing point. Freezing (or unfreezing) water makes the planet whiter (or darker), which affects the degree to which it reflects the sun’s warming rays. A flicker of the sun, therefore, ensures climate change. 



A hot young climatologist named Robert Mann, writing in Geophysical Research Letters, recently provided a powerful demonstration of this phenomenon. Using long‐​term records from tree rings and ice cores, he concluded that the planet was on a 900‐​year cooling streak between 1000 and 1900. Then we warmed up almost twice as much as we had cooled, but at least half of that warming was caused by our inconsistent sun. Two NASA scientists recently demonstrated that the sun has been warming throughout the last 400 years. As a result, if the last decade weren’t among the warmest in the last millennium, something would have been wrong with the basic theory of climate: The sun warms the Earth. 



That doesn’t mean we haven’t supplied a bit of greenhouse warming, too. But greenhouse warming behaves differently than pure solar warming: It occurs largely in the coldest air masses of winter. 



That’s a far cry from the United Nation’s nonsense about “some places” and “others” experiencing more unusual, less unusual or unusually usual weather. And it has nothing to do with avalanches or snowless winters, either.
"
"
Share this...FacebookTwitterH/T: Rudolf Kipp at Skeptical Science (see first reader comment).
That is about the same as the US annual GDP. That’s the number reported in a recent article appearing in the online Frankfurter Allgemeine Zeitung FAZ), Germany’s leading political daily.

Europe set to climax with green bloody self-flagellation.
The FAZ writes:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In the coming 40 years, the EU must invest 270 billion Euros annually if it wants to reach its long-term climate targets. That’s the result of a draft of a strategy paper that Climate Commissar Connie Hedegaard wants to introduce in early March. In order to reduce greenhouse gas emissions by 80 to 95% by 2050, large-scale investments for the expansion of renewable energies, energy grid, CCS, passive homes, electric vehicles and modern industrial plants will be needed.”
What return will European taxpayers and citizens get for this investment? Maybe a theoretical, imperceptible o.1°C of reduced warming, along with a phony belief they saved the planet. Not even the Soviets could have managed a failure of such proportions. What better way to drive an entire continent into ruin could one possibly conceive?
But the EU seems to think this is all bearable. After all, it is only 1.5% of the EU economy – about the amount paid by the EU because of the financial crisis. And it’s the way to keep Europe in the leading position as the world’s climate protector.
But it is doubtful that even the most green-eyed bureaucrats believe these targets are achievable. So they added an escape clause in the whole thing. The EU will also be allowed to count CO2 reductions achieved in third countries. I’m not clear on what that means exactly, but to me it’s an open door to accounting tricks.
Still, it is an ambitious plan, especially amid all the conference failures of the past. Indeed rather than discouraging Europe, these global failures to agree on reductions in emissions have only emboldened Europe to get even tougher. FAZ writes:
The European Parliament and enviironmental activists have repeatedly demanded that the EU unilaterally commit itself to even tougher reductions in order to give climate change negotiations more impetus.”
What do the EU bureaucrats care? After all, it’s the EU citizens who are going to have to bear all the pain. The citizens be damned. And besides, they’ll be able to enjoy the benefits of an imperceptibly cooler 0.1°C and a feeling they rescued planet. So let the self-flagellation begin.
Share this...FacebookTwitter "
"License holders will be allowed to kill some of Britain’s most endangered bird species under temporary permits licensed by Natural England and Natural Resources Wales. The monitoring and enforcement of these permits relies on self-reporting and regulation – loopholes which could be exploited to feed the demand for illegal bird products in Europe. The birds at risk throughout England and Wales include species whose numbers are threatened in the UK, according to the RSPB (Royal Society for the Protection of Birds). Bullfinches, meadow pipits and oystercatchers are all included in the permits and are amber-listed for intermediate conservation priority. Another species, the skylark, will be subject to licensed killing despite the RSPB red-listing it as a critical conservation priority for the UK. Both Natural England and Natural Resources Wales are sponsored by central government and are responsible for “promoting nature conservation” and “protecting people and the environment” according to their websites. They cite safety concerns to justify granting the permits and claim killing birds could prevent damage to crops and reduce interference with air traffic. Although the permits strictly outline the overall number of birds that are allowed to be killed, monitoring and enforcing this will be crucial. By licensing the shooting, trapping, and killing of songbirds in the UK, the government could be offering a route for supplying dead birds to the illicit trade across Europe. The illegal bird trade within the European Union is thought to be worth at least €10m a year. This doesn’t just refer to the trade in exotic species from outside Europe, but includes the widespread trade of songbirds for human consumption – particularly in parts of France and northern Italy where songbirds are regarded as forbidden delicacies. Dishes such as ortolan bunting or polenta ucelli – polenta with roasted songbird – are synonymous with luxury and prestige. Other songbirds such as sparrows and thrushes are trapped and eaten throughout Italy and Cyprus. The trade in songbirds makes for quick profits: one gram of songbird meat is estimated to sell for the equivalent of one gram of marijuana. The trapping and consumption of songbirds is widely illegal across the European Union, but it still occurs illegally in some member states such as France and Italy. Although the 1981 UK Wildlife and Countryside Act forbids wild birds being sold for food or shot for sport, enforcing this rule may be difficult to guarantee under the permits. Recent seizures illustrate that although the European bird trade is a lucrative branch of the illicit global trade in wildlife, it was neither part of the discussions nor mentioned in the declaration released after the London Conference on the Illegal Wildlife Trade in October 2018. The omission highlights that policy makers often turn a blind eye to Europe as a thriving market for illegal wildlife products, particularly those items that are “less attractive” or of a lower profile than rhino horn or ivory.  Demand driven by European consumers is one of the key issues that policymakers  often fail to address. The UK government has tried to position itself as a world leader on this issue, but its decision to grant licences to shoot endangered bird species – even in small numbers – undermines this. Even strict conservation laws are often difficult to enforce as legal and illegal activities become intertwined. For example, licensed hunters may hold the correct permits, but might use illegal methods, such as using calling devices or decoys to attract birds in greater numbers.  While the illegal killing of birds has received much more attention in the last couple of years, we still know too little about its commercial side. What drives demand? How are illegal products trafficked? How much profit do they bring in and what groups benefit? Which bird species are trafficked for human consumption and which for the pet market? There is simply a lack of official and up-to-date data on the networks, routes and products of this trade. Due to such uncertainty, there is a failure to grasp how changes in conservation laws in the UK could export criminal activity to other regions. A good example is the flourishing hunting tourism industry in Serbia, which caters particularly to hunters from Italy and Malta who have shifted their hunting abroad to avoid strict regulations in place at home. If a door is closed to supplying this trade, another is opened elsewhere, and so the source simply moves. There is cause to worry that songbird killing and trapping opportunities in the UK could prove to be another supply route for the pan-European trade. Policing those who are issued permits may encounter the same problems that regulating the protection of birds has met elsewhere in the UK. The RSPB published a report in 2017 that found a striking 67% of crimes against birds of prey in the UK were committed by gamekeepers and that self-regulation had evidently failed.  Other studies also found a correlation between the persecution of birds of prey and grouse shooting. For example, where birds of prey pose a hindrance to grouse shooting, gamekeepers may destroy their nests. Although complex structures are in place to prosecute wildlife crime in the UK, the RSPB found that in 2017 only four out of 68 cases of crimes against birds of prey were prosecuted. Given these difficulties in enforcing existing regulations, how can we be certain that those songbirds which are legally trapped or killed in the UK do not feed the demand for illegal songbirds in European restaurants? The seizures of illegal songbird shipments from the Balkans into the EU have shown that geographic distance and national borders pose no significant obstacle. Due to these challenges in monitoring and enforcing the regulations, the government’s permission to grant licenses to kill endangered bird species in Britain – even for public health or safety reasons – not only threaten bird populations but also undermine efforts to tackle the illegal trade in bird products in Europe. This article was amended on February 4 2019 to clarify the role of the permits and the responsibilities of Natural England and Natural Resources Wales."
"
Share this...FacebookTwitterBack in January I wrote about how the Arctic had gained 2000 cubic kilometres in ice volume. This was calculated by comparing the sea ice thickness of January 2008 to that of January 2011, see the following chart.
Mid-January Arctic sea ice thickness comparison of 2008 to 2011.
Blue color shows thin ice, while green shows thicker ice. Clearly the Arctic’s ice was much thicker in Jan-2011 than it was three years ago in Jan-2008. There is about twice as much solid green in 2011 than in 2008, thus yielding the net gain of about 2000 cubic kilometers.
5000 cubic km more sea ice?
Now that we’ve just past the March-peak in Arctic sea ice area, I thought it’s a good time to take another look at the Arctic sea ice thickness again. What follows is a comparison of March 23, 2008 (the old death spiral days) and March 25, 2011.
Source of charts: http://www7320.nrlssc.navy.mil/pips2/ithi.html
Well lo and behold, we see yet another huge increase. That ice thickness increase has grown even more.
Note the huge difference in just three years. Today practically the entire Arctic cap is green…meaning an average thickness of over 3 meters. In 2008 the average thickness down to about 2m. This is all further confirmed by the current Catlin Arctic Survey (h/t: tomnelson.blogspot).
Some of the ice we crossed was really thick multi-year ice that was just too thick to drill through. Our drill goes to four and a half metres and it wasn’t breaking through. In other areas we were able to drop our measuring devices down to a depth of 200 metres below the floating ice.”
I wonder if Mark Serreze is taking note. Hopefully he’ll behave as the scientist he claims to be and make the observation.
I haven’t systematically calculated the March differences above, but you can assume that the core cap itself has an area of 5 million square km. Now multiply that times one meter of added thickness and you get a volume growth of 5000 cubic km!
Okay, that’s just rough guesstimating (perhaps) on the high side, but the ice gain is whopping no matter what. Dr. “Freeze” Serreze can make the calculation himself. Predictions of higher sea ice extents come September are well-founded. The recovery appears to be well on the way. Arctic warming has disappeared.
Share this...FacebookTwitter "
"

From ETH in Zurich, this interesting essay on the last glacial period has some interesting points to ponder. h/t to Sid Stafford – Anthony

The last glacial period was characterised by strong climatic fluctuations. Scientists have now been able to prove very frequent and rapid climate change, particularly at the end of the Younger Dryas period, around 12,000 years ago. These fluctuations were accompanied by rapid changes in circulation in the oceans and the atmosphere.

Researchers are able to determine when glaciers were stable and when they melted by studying titanium content in glacial lake sediments. (Picture: siyublog/flickr)

Sediment deposits in lakes are the climate archives of the past. An international team of researchers from Norway, Switzerland and Germany have now examined sediments originating from the Younger Dryas period from the Kråkenes Lake in northwest Norway. In the sediments, they found clues that point to a “climate flicker” at the end of the last glacial period, oscillating between colder and warmer phases until the transition to the stable climate of the Holocene, our current interglacial period. The short-term, strong fluctuations of the Younger Dryas would have dwarfed the “extreme weather phenomena” seen today, according to Gerald Haug, professor at the Department for Earth Sciences at ETH Zürich and co-author of the study, which was published online yesterday in “Nature Geoscience”.
Seasonal sediment deposits
Seasonal sediment accumulation, for example, gave scientists clues to these strong climate fluctuations. They can be read in lakes in a similar way to reading rings on trees. In warmer phases and melting glaciers, the accumulation of sediments increases. More clues on the changes in glacier growth were given by the element titanium, which is present in the sediments. Glaciers erode their bedrock, and in doing so concentrate the titanium contained in the sediments they are carrying. The sediments containing titanium are washed into the glacier’s draining lakes in the meltwater. The amount of sediment and the titanium content can therefore allow us to deduce when the glaciers were stable and when they melted. The researchers interpreted the maxims, recurring every 10 years, as phases of strong glacier activity caused by temperature fluctuations and thus as warmer times.
A seemingly self-preserving cycle
The scientists also examined a sediment core from seabed deposits of the same age in the North Atlantic. They reconstructed the original temperature and salt concentration of the water based on microfossils and the oxygen isotope ratio in the sediment. It was shown that the results from the lake sediments corresponded to those from the sea sediments. “The melting of glaciers was caused by the warm Gulf stream advancing into this region,” Gerald Haug explains. This increase in temperature caused the west winds to shift to the north and brought warm air to northern Europe. However, the meltwater draining into the Atlantic lowered the salt concentration and the density of the surface water, changing the convection in the ocean, which in turn allowed new sea ice to form. Subsequently, the Gulf Stream and the west winds were again forced out of the North Atlantic area and the region cooled down once again. These processes were repeated for around 400 years, until the current interglacial period was able to stabilise itself.
The Würm glaciation began around 100,000 years ago and lasted until around 10,000 years ago. In this period, there were strong fluctuations between warm and cold phases, particularly in the North Atlantic area. The Younger Dryas, which ushered in the current interglacial period, is one of the best-known and best-researched abrupt climate changes of that glaciation. It began around 12,900 years ago and at first caused an abrupt temperature drop in the northern hemisphere, as well as a temperature rise of up to 10°C in less than 20 years towards the end, around 11,700 years ago.
Unclear mechanisms
Up until now, there have been several studies which document the glacial conditions during the Younger Dryas period of 1,200 years. However, the mechanisms which caused it, sustained it and finally led to an interglacial period have yet to be fully understood. The researchers believe that further high-resolution studies of this type could give insights into how glacial periods are triggered and how they are brought to an end.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9812b02e',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitterSweden’s English language The Local has the following headline today:

Coldest December in Sweden in 110 years
The last few days of the year look to be very cold throughout Sweden, according to a forecast by the Swedish meteorological agency SMHI. 
This means that several parts of Sweden, including the southern region Götaland and eastern Svealand, will have experienced the coldest December in at least 110 years.”
Read the complete article here.
This reality of course flies in the face of what climate models had predicted earlier. The SMHI (Sweden’s Met Office and devout warmist organisation) keeps archives, and so I thought surely there must be something there that had earlier forecast warmer winters for Sweden. I didn’t have to look very long to find it.
First there’s this report dated 16 September 2010 here: New climate projections indicate more extreme weather. Here are just a couple of excerpts (Warning – you might first want to tie your butt to yourself to keep from laughing it off!):
New climate projections for severe weather situations in 100 years also show that truly cold days will virtually disappear.”
And:
The new scenarios show the effects of global warming with more details than before, thanks to more computer power and high geographical resolution.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




And:
‘As a whole, the new ensembles are an important foundation for continued climate research. However, they can already be applied to many areas,’ says Grigory Nikulin.”
Does he mean like governments preparing for winters? And finally:
Truly cold weather, such as -10°C in Spain or -30°C in southern Sweden, is unlikely to occur in future.”
How stupid must they feel now? The assertions made above likely stem in part from an SMHI-published report 2 years ago called: Temperature and precipitation changes in Sweden; a wide range of model-based projections for the 21st century.
The report analyzed the climate change signal for Sweden in scenarios for the 21st century in a large number of coupled atmosphere-ocean general circulation models (AOGCMs), used in the AR4 by the IPCC.
At the SMHI Rossby Centre, regional climate models were run under different emission scenarios and driven by a few AOGCMs. They used the results of the runs as a basis in climate change in Sweden. What did they find? (Crap, of course, but read it for yourself):
Projected responses depend on season and geographical region. Largest signals are seen in winter and in northern Sweden, where the mean simulated temperature increase among the AOGCMs (and across the emissions scenarios B1, A1B and A2) is nearly 6°C by the end of the century, and precipitation increases by around 25%. In southern Sweden, corresponding values are around +4°C and +11%.
Okay, it’s still a long way to the end of the 21st century. But as Sweden’s 2010 December-of-the-century shows, the models and calculations seem to have forgotten a few important details. Back to the drawing board!
Share this...FacebookTwitter "
"
Previously on WUWT we discussed the media’s fascination with “melt” when it comes to ice shelves cracking off. Then there’s also this picture that keeps getting recycled.

http://www.ogleearth.com/wissm.jpg
It is clear from the photo above that we see a stress crack, not a melt. Now we have a time lapse satellite photo series of the Wilkins ice shelf that shows the process of currents and winds causing those stresses.
Mike McMillan writes:
Fox News is reporting that the Wilkins ice shelf bridge that’s been eroding  has finally collapsed.
http://www.foxnews.com/story/0,2933,518374,00.html
I  went back to the old ESA sat photos and noticed something interesting.   I downloaded the gif animation and did some highlighting.

In  the upper area, the shelf was previously fractured, then glued together  by new ice.  I highlighted a string of drift ice in green to show what  the currents were doing during the previous collapse.  The current runs down  from the top, compressing the fractured shelf and likely busting up the new  ice glue.  The current then reverses, pulling the fractured shelf ice out to  sea. The green drift ice looks almost like a fingertip crunching into the  shelf, and clearly shows the compression.
A different process works on  the lower side of the ice bridge.  A gyre pulls
off chunks of unfractured  ice.  I’ve highlighted a chunk of non-edge ice in
pink, and we can watch it  tumble out along with a companion berg.  Note the
sea immediately refreezes  in the open areas.  One of the gif frames shows the
gyre swirling the new  ice, and I’ve enlarged the frame.

http://i40.tinypic.com/erg287.jpg
UPDATE: I slowed down the original animation to 1 frame per second, with a 2 second pause at end, per requests in comments. -Anthony


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9678f743',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"
Share this...FacebookTwitter!!! UPDATE: Read the Europol Press Release here !!!
– Estimated 5 billion euros in damage for European taxpayers
– Massive fraud involving criminal networks / Middle East
Hat tip Reader Dirk H.
Here’s more proof that trading of CO2 emission certificates is fraught with fraud and attracts seedy criminal organizations – all costing the consumers and taxpayers billions.
Worse yet, it has spread out of control and appears that the authorities can’t keep up.
The Austrian online Kleine Zeitung here reports that Europol have raided an elaborate CO2 emissions scam in Italy and have arrested more than 100 persons.
The Kleine Zeitung writes: “The damage runs in the billions of euros”.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




According to Europol, the Italian tax authorities, directed by the Milan Prosecutor’s Office, have raided 150 companies in Italy. The fraud involves evasion of value added tax with CO2 emission certificates. More than 100 have been arrested and are suspected of being involved in organised crime.
The Kleiner Zeitung reports that the Italian Electric Utilities trading markets had earlier halted entire trading with emissions certificates “because a high number of abnormal transactions”. The loss in tax revenue just from VAT (MTIC (Missing Trader IntraCommunity Fraud) alone is estimated to be 500 million euros, the online Kleine Zeitung writes.
The fraud is widespread
According to reports, it’s been known since June of last year that criminal organizations have been using CO2-emissions trading for defrauding governments of value added tax.
This is not the first time that police raids of this scale have taken place. It’s the latest in a series of raids that have been carried out all over Europe this year, all involving the trading of CO2 emission certificates. It seems the authorities just can’t keep up with the multitudes of swindlers out there.
Norway, Switzerland and the EU countries Belgium, Czech Republic, Denmark, Latvia, the Netherlands, Slovak Republic and Portugal are all among the countries trying to identify the network of criminals behind this massive fraud – a fraud with links to criminal networks operating outside the EU and in other continents, like the Middle East.
2500 investigators – trying to identify. That’s means they haven’t yet. That’s a lot of fraud. The fraud has spread from science to finance. Expect a meltdown – sooner than later.
UPDATE 2: Recall this Danish fraud: http://climaterealists.com/index.php?id=6790
Share this...FacebookTwitter "
"Behind the scenes, away from the Commons chamber and the TV cameras, the UK’s parliamentary committees continue to debate and scrutinise the mountain of legislation needed in order to secure an orderly exit from the EU. Among the legislation currently under consideration is the government’s proposed Draft Environment (Principles and Governance) Bill. The bill itself is an attempt to fill some of the gaps that will inevitably emerge in UK environmental law once the country leaves the EU. For instance, the European Commission currently has the ability to take member states to the EU Court of Justice, as it has threatened to do with the UK, which is not presently not complying with air pollution limits. After Brexit, the EC would have no such power. Overall, there are several encouraging aspects to what is the first general environmental bill before parliament in more than 20 years. They include, for example, the first explicit incorporation into UK law of the environmental principles that have shaped EU environmental law.  Another positive initiative is the proposal to create a new “watchdog” in the form of an office for environmental protection whose job it will be to monitor the implementation of environmental law. On closer scrutiny, though, the bill is a missed opportunity. First, the incorporation of environmental principles into UK law is unlikely to provide the safe harbour that many commentators hope they do. The bill does require the environment secretary to prepare a statement setting out his or her interpretation of the principles. But, while other government ministers must then take the statement into consideration – they do not necessarily have to obey it. The duty also applies only to central government ministers and not public authorities more widely. In any case, environmental principles are rarely able to dictate a specific eco-friendly outcome. This is because they do not “establish” formal legal rules but instead invite a decision maker to consider competing interests through procedures such as risk assessments.  For example, the UK’s National Planning Policy Framework (NPPF) contains a presumption in favour of the principle of sustainable development, which at first glance sounds important from an environmental perspective. In reality, however, the principle and the NPPF still allows planning authorities lots of scope to decide what constitutes sustainable development in their local area, whether it is housing developments or protecting the greenbelt.   Though the creation of an “office for environmental protection” represents a potentially bold step by the government, its scope is too limited. The bill proposes that the office must “monitor the implementation of environmental law”. However, this doesn’t mean much as there is no reference to any standard to measure this implementation against.  The reference to “implementation” does make sense if the UK develops environmental law in response to EU or international obligations. But the whole point of Brexit is to do away with EU law and, as it stands, the bill contains no powers for the proposed office to monitor the implementation of international environmental law. Its lack of legal enforcement powers are also disappointing. The office would be limited to issuing so-called “decision notices” where a “serious” breach of the law has taken place. But what “serious” means is not defined in the bill. A “decision notice” also has little legal power as the recipient is merely obliged to respond in writing, explaining whether they agree with it and whether they intend to take any steps in response. A public authority handed a “decision notice” by the office for environmental protection could simply answer “no and no”, and the office would have little power to enforce its findings.  Many experts had hoped the proposed office would be given powers to fine the government. The air pollution cases brought against the UK government nicely highlighted the importance of fines, when the government conceded that it hoped to be compliant with EU emissions standards by 2020. That year was chosen not by science or policy plausibility, but because 2020 was likely to be the earliest point when the European Commission would take steps to fine the UK. However, a conscious decision has been made not to give the office for environmental protection any powers to fine the government. In light of this, the failure to include more innovative methods of enforcement is a disappointment.  The bill ought to include provisions for the office to negotiate so-called enforcement undertakings with the government, for instance. These are written agreements between a regulator and offender. The offender must agree to take specific steps to remedy the illegal activities, including restoring the environment to its previous state, and compensating any third parties who have suffered harm. Enforcement undertakings are already used extensively by the UK’s Environment Agency and have proved to be cost effective and efficient. In sum, the government’s proposal for a new post-Brexit environmental regime is laudable and disappointing in equal measures. Consequently, the bill is much like the government’s attempt to deliver Brexit itself: it is difficult to please everyone."
"

 _Global Science Report_ _is a feature from the_ _Center for the Study of Science_ _, where we highlight one or two important new items in the scientific literature or the popular media. For broader and more technical perspectives, consult our monthly “Current Wisdom.”_   




In examining the climate change output from the DICE model, we found that it projects a degree of future sea level rise that far exceeds mainstream projections and are unsupported by the best available science. The sea level rise projections from more than half of the future scenarios examined exceed even the highest end of the projected sea level rise by the year 2300 as reported in the _Fifth Assessment Report_ (AR5) of the UN’s Intergovernmental Panel on Climate Change (see Figure 1).   






_Figure 1. Projections of sea level rise from the DICE model (the arithmetic average of the 10,000 Monte Carlo runs from each scenario) for the five scenarios examined by the federal interagency working group (colored lines) compared with the range of sea level rise projections for the year 2300 given in the IPCC AR5 (represented by the vertical blue bar)._ _(DICE data provided by Kevin Dayaratna and David Kreutzer of the Heritage Foundation)_   
  
Interestingly, Nordhaus (2010b) recognizes that the DICE sea level rise projections are outside the mainstream climate view as expressed by the IPCC:   




“The RICE [DICE] model projection is in the middle of the pack of alternative specifications of the different Rahmstorf specifications. Table 1 shows the RICE, base Rahmstorf, and average Rahmstorf. _Note that in all cases, these are significantly above the IPCC projections in AR4._ ” [emphasis added]



The justification given for the high sea-level rise projections in the DICE model (Nordhaus, 2010) is that they well-match the results of a “semi-empirical” methodology employed by Rahmstorf (2007) and Vermeer and Rahmstorf (2009).   
  
However, as we have pointed out, recent science has proven the “semi-empirical” approach to projecting future sea level rise unreliable. For example, Gregory et al. (2012) examined the assumption used in the “semi-empirical” methods and found them to be unsubstantiated. Gregory et al (2012) specifically refer to the results of Rahmstorf (2007) and Vermeer and Rahmstorf (2009):   




The implication of our closure of the [global mean sea level rise, GMSLR] budget is that a relationship between global climate change and the rate of GMSLR is weak or absent in the past. The lack of a strong relationship is consistent with the evidence from the tide-gauge datasets, whose authors find acceleration of GMSLR during the 20th century to be either insignificant or small. It also calls into question the basis of the semi-empirical methods for projecting GMSLR, which depend on calibrating a relationship between global climate change or radiative forcing and the rate of GMSLR from observational data (Rahmstorf, 2007; Vermeer and Rahmstorf, 2009; Jevrejeva et al., 2010).



In light of these findings, the justification for the very high sea-level rise projections produced by the DICE model is not acceptable.   
  
Given the strong relationship between sea-level rise and future damage built into the DICE model, there can be no doubt that the SCC estimates from the DICE model are higher than the best science can allow and consequently, should not be accepted by the OMB as a reliable estimate of the social cost of carbon.   
  
We did not investigate the sea-level rise projections from the other two IAMs employed in the federal SCC determination, but such an analysis must be carried out prior to extending any confidence in the values of the SCC resulting from those models—confidence that we demonstrate cannot be assigned to the DICE determinations of the social cost of carbon.   
  
References:   
  
Gregory, J., et al., 2012. Twentieth-century global-mean sea-level rise: is the whole greater than the sum of the parts? _Journal of Climate_ , 26, 4476-4499, doi:10.1175/JCLI-D-12-00319   
  
Nordhaus, W. 2010a. Economic aspects of global warming in a post-Copenhagen environment. _Proceedings of the National Academy of Sciences_ 107(26): 11721-11726.   
  
Nordhaus, W., 2010b. Projections of Sea Level Rise (SLR), http://www.econ.yale.edu/~nordhaus/homepage/documents/SLR_021910.pdf   
  



"
"Increasing consumption of meat rich diets throughout the world in the 21st century raises pressing concerns about human health, animal welfare and environmental sustainability. Too much mass-produced meat is bad for us, bad for the livestock we eat, and bad for the planet on which we live. If we want to understand how the world arrived at this point, as well as how we might change it for the better, we should look back to the Victorian period, which laid the foundations for modern globalised meat production and consumption. Concerns today about what has become known as the “global meat complex” focus on the technologically driven overproduction and consumption of livestock. There’s a recognition in particular that “the middle classes around the world eat too much meat”, as a 2014 Friends of the Earth report put it. But the root of this problem can be traced to 19th-century Britain, when global meat markets emerged as a revolutionary way of dealing with a mid-Victorian “meat famine”. The famine was caused by a mismatch between a fast increasing, urbanising population and a levelling out in domestic meat production. What helped stave it off was the groundbreaking development of preservation and transportation technologies that enabled the British to eat livestock that was reared, slaughtered and processed in the Americas and Australasia. As a result of these innovations, products such as chilled and corned beef, frozen mutton and meat extracts including Bovril and Oxo became staples throughout British homes. Per capita meat consumption increased dramatically, rising from about 87lb per year in the 1850s to 127lb annually by 1914, despite the fact that Britain’s population nearly doubled in this period. Cost was the major factor driving this change. When one can get a half-price leg of mutton from the other side of the globe, remarked one prominent food writer, one sets aside “all sentimental considerations in favour of the roast beef of Old England”. Mass marketing campaigns alongside positive media coverage also helped promote these new forms of meat. Victorian commentators celebrated frozen meat’s capacity to feed the “energetic, flesh-fed men” required to sustain British industry and imperialism. Meanwhile “beef tea” was widely advertised as a life enhancing force in Britain’s fights against alcoholism, influenza, European rivals and imperial perils. Meat remained a luxury for the very poor in Victorian Britain. But as the 19th century came to a close, and as more and more British consumers grew accustomed to imported beef and mutton, the idea of meat – the more the better – as an essential part of everyday meals became increasingly popular among working-class as well as middle-class meat-eaters.  As global meat markets revolutionised the dining habits of the British nation, they also changed the face of the planet. Vast tracts of American and Australasian land were reshaped as pasture that supported the British breeds of cattle and sheep that Britons preferred to eat. And selective breeding programmes meant the bodies of these animals fattened faster and could be stored more easily in refrigerated holds: animals were bred with their carcasses in mind.  The globalisation of Victorian meat eating was revolutionary, then, but it was also highly controversial. Advocates of the canning and refrigeration industries championed their capacity to deliver healthy, wholesome, inexpensive and sustainable meat supplies from Britain’s colonies and the “new world”. But home-reared meat was seen to be of better quality and safer, especially early on in the development of these industries. Many potential customers were put off by scandals involving putrefied meat, as well as scare stories surrounding the meat’s origins. Metropolitan meat eaters feared that overseas farmers were feeding them offal or meat from diseased animals. In my archival research, I’ve even discovered concerns that boiled human babies were entering the food chain. It wasn’t just that the British were wary of eating long dead animals from far flung parts of the world. Overseas competition provoked demands to protect British agriculture, both to preserve traditional ways of life and to guarantee food security. Animal rights campaigners too were concerned at the increasingly intensive farming methods and assembly line slaughter techniques associated with developing meat markets. And at the same time, Britain’s growing vegetarian movement was promoting the economic, health and ethical benefits of a meat free diet. Writing in the 1880s, the prominent vegetarian and socialist Henry Salt predicted that “future and wiser generations will look back on the habit of flesh-eating as a strange relic of ignorance and barbarism”. Salt would be horrified by a 21st-century world struggling to cope with an ever growing demand for cheap, plentiful meat. Horrified, but perhaps not entirely surprised. The unhealthy, unethical and unsustainable way that the “global meat complex” operates today is the greedy, brutal and environmentally devastating extension of what his meat eating contemporaries did to the world. But this Victorian history can also help ongoing efforts to change the way our planet produces and consumes protein. First and foremost, it makes clear that there is nothing inevitable or “natural” about the way meat markets take shape. Hundreds of millions of people eat meat in the way and the quantities they do, not because they’re inherently designed to do so, but because of a global system set in motion by British imperial power. And we should keep in mind that this system’s development was an incredibly controversial process, marked by fierce debates as well as dramatic dietary change. At a time of year when many of us are thinking about how to transform our lives for the better, the prospect of giving up meat, or of eating insects or lab-grown meat, provokes widespread scepticism, hostility and disgust. We’d all do well to remember, therefore, that not so long ago the prospect of eating frozen lamb from the other side of the world provoked a similar range of reactions among the Victorian population."
"Unless you live in the tropical rainforests of South or Central America, most of the sloths you’ll encounter will be two-toed sloths. This is because they are able to eat quite a varied diet and are therefore relatively easy to keep in captivity. Their relatives, the three-toed sloths, on the other hand, have a very restricted diet, subsisting solely on Cecropia: a group of fast-growing tree species with soft wood and large, juicy leaves.  Or so it has always been thought. A paper published today by the Royal Society gives quite a different picture of the lifestyle of three-toed sloths. The authors of the paper looked at how the availability of different tree species, including those of the genus Cecropia, affected the survival and reproduction rates of sloths. Given that these trees are the sloths’ favourite food, this specialist sloth species might be expected to spend most of its time in them. However, the authors found that at certain life stages, sloths may desert their favoured tree for other species. Density of Cecropia is critical to the survival and reproductive success of adults, especially the males, but was not correlated with survival rates of juveniles. The authors attribute the differing importance of Cecropia at different life stages to the shape and growth habits of the tree, and they give a detailed analysis of its effects. Because Cecropia species grow fast and produce lots of leaves with few chemical defences rather than a few leaves that are defended by a lot of toxins, there are always young, palatable, easily-digestible leaves available for adult sloths. The leaves also contain essential nutrients that keep sloths in good health, which would suggest that juveniles should also favour them.  Cecropia foliage consists of a fan of large leaves at the end of a long stem or branch with no other leaves on it, giving it an “open structure”. This means the tree does not make a good hiding place for young sloths, who may be more vulnerable than adults to predators like jaguars or eagles, even though they are quite well camouflaged. Similarly, mothers with babies may choose trees that have a thicker canopy as their maternity ward, moving back to the Cecropia tree when the baby is older.  This open structure is important when it comes to mating. Sloths are solitary creatures with extremely poor vision and, when the time is right, they need to find a mate from a widely dispersed population. Since the males are not equipped to go rushing around the forest looking for a receptive female, it is vital that they are able to be seen and heard when communicating their intentions to the local females. The relatively sparse foliage of Cecropia species is ideal for this, allowing the mating calls of the lonely males to travel much further than in the denser canopy of other trees. The authors of this paper suggest that, when necessary, three-toed sloths are able to live in habitats that are less high quality than virgin forest. Young sloths and nursing mothers may use tree species that are less nutritious than Cecropia in order to avoid the risk of predation, and in conservation terms, that may mean that they can exist on a less specialised diet if it is necessary to move or breed away from their natural habitat. This may be an important finding for sloths in the wild, since cocoa cultivation is a very present factor in their environment. Cocoa trees require a shady environment and, in Brazil, are traditionally grown as an understory layer beneath native forest trees. This is great news for the three-toed sloth as these areas of “agroforest” provide both the open structured Cecropia trees and a variety of other, denser canopied species, so can accommodate all the life stages of the sloth. Because they are of commercial use to humans, the cocoa trees are also less likely to be felled, so the habitat is relatively secure.  Until now, it has been thought that three-toed sloths are not able to make use of this agro-forest as two-toed sloths can, but this paper suggests otherwise. Since the agro-forestry project in Brazil has an ultimate goal of 557,500 hectares of forest being used for cocoa production, it is important that sloths are able to make use of this habitat for at least part of their life cycle. The authors suggest that targeted conservation efforts, such as planting Cecropia trees as part of cocoa agro-forestry, could help sloths in areas such as Costa Rica, where they are of conservation concern.  This study may have significance for the conservation of other “specialised” herbivores across the globe, if it is found that sloths are not the only animal to be able to survive on less favoured plants. The authors remark that forests that are regenerating are better able to support specialist species than we thought – and given the current levels of deforestation globally, this must surely give us some hope for the future. 


      Read more:
      Sloths aren't lazy – their slowness is a survival skill


"
"
Share this...FacebookTwitterThe Climate-Berlin Wall surrounding Germany has been cracking for some time now, and crumbling with increasing speed. Die Zeit’s online smear The Abetters of Doubt takes aim at the skeptics, who have scaled the Wall and are exercising their right to free expression. They are hammering away at climate science dogmatism in Germany, much to the chagrin of the warministas. 
The warministas are horrified and have embarked on a campaign to intimidate and marginalize the freedom of expression that skeptics enjoy in Germany. Climate skeptics who speak up today do so while always having to look over their shoulder, knowing they could be hit by a vicious smear and attack campaign. This is hardly the environment for a free and open society. Yet, it confirms that warminista science is trapped in the corner.
The days of Germany’s once exemplary model of a free and open society would of course risk unraveling if the warministas got their way. Indeed skeptics have long since been denied a voice in Germany’s publicly funded media, and so as a result have moved to the Internet, where they’ve chipped away.
Like it or not, this discussion is going to take place. Live with it.
Greenshirts resorting to brown tactics
What does an arrogant class, so totally convinced of itself, do when exhausted of argument and finds itself badly losing the intellectual debate and, along with it, its dream of The Green Reich?
It does what Die Zeit newspaper has done in its latest piece called The Abetters of Doubt; it resorts to brown tactics. The latest from Die Zeit is a 4-piece attack and smear campaign, with the objective of intimidating, marginalizing and silencing climate skeptics. It’s the same we have recently seen from other major dailies like Der Spiegel and the Handelsbaltt, with the usual names popping up: Stefan Rahmstorf and attack canine Naomi Oreskes. Lacking journalistic talent, Die Zeit has stooped to rehashing old stories.
Not only does the piece smear skeptics and advocate they be denied a voice, it attempts to morally degrade them as well. Ironically, it is becoming obvious who is actually morally degraded. Being a dissident here behind the Climate Berlin Wall and watching these smear tactics, I’m beginning to have an idea of what it must have been like to be gay during Medieval times, or Jewish before Kristallnacht. Die Zeit’s message to the skeptics in Germany is clear: “Be worried – be very worried”.
Fortunately, it’s nothing more than a last desperate threat coming from an intellectually bankrupt media outlet and a few activist scientists hiding in the background.
Growing skepticism, and desperation
Die Zeit’s piece is eerily laced with a strange combination of fear, anger and desperation, and it makes clear that the warministas are fed up with the turn climate science has taken. For them, the science was settled years ago. Damn the skeptics, bloggers, Internet and Fred Singer. Damn EIKE and the few German politicians who are beginning to listen up. They have gone too far. Die Zeit frets that public opinion in Germany is waning and that it’s time to put an end to it. In its piece, Die Zeit puts the spotlight on the bloggers:
Last year’s failure in Copenhagen and the hacked e-mails from climate scientists, which supposedly proved falsification, have been making waves through the media. The deniers and skeptics of global warming have been gaining momentum. They are omnipresent, mainly in the Internet – and appear to strike a chord with people who are fed up with all the climate talk, or with people who don’t want to change their lives because of a warming planet.”
Damn that denier European Institute for Climate and Energy (EIKE)
Die Zeit then singles out Horst-Joachim Lüdecke, a retired professor who is now the Press Speaker of the European Institute for Climate and Energy (EIKE). Die Zeit haughtily implies that Lüdecke is imposturing as a climate scientist:
Professor emeritus for Physics and Computer Sciences, but to his audience he introduces himself as a ‘climate scientist.'”
Indeed Prof. Lüdecke has been busy spreading the skeptic message, and has been effective. Skepticism in Germany has almost doubled over the recent months – now 1/3 no longer believe CO2 is a problem. This has infuriated and alarmed the warministas. Lüdecke recently gave a presentation to the Nordoberpfalz business group. Here’s how Die Zeit describes it:
The audience was made up of company owners, the mayor, local dignitaries and decorated lieutenant colonels. Hardly anyone noticed that Lüdecke was citing outdated reports, asserting uncertainties that no longer exist and suppressed facts that were inconvenient.”
Not only is Lüdecke imposturing as a climate scientist, but he is also using phony data, Die Zeit wants its readers to believe.
Damn those bloggers and the Internet (again)
The warministas by far view the Internet and bloggers as their biggest problem. This ought not be a surprise, as skeptics have long since been denied their right to be given a voice by Germany’s massive public radio and television media, where they are viewed as unworthy of a public platform. Call it media-gatekeeping. So, naturally, skeptics use the resources that are available, i.e. the Internet, and the little money and time they have at their disposal.
On the topic of bloggers, Die Zeit interviews no. 1  crybaby Stefan Rahmstorf of the Potsdam Institute for Climate Impact Research(PIK), introducing him as: “one of the world’s leading oceanographers”. Die Zeit writes:
In Potsdam, at the Einstein-Science-Park on the Telegrafenberg, climate scientist Stefan Rahmstorf sits in front of his computer and moans. ‘In the Internet, the climate skeptics are by far the dominant force,’ says Rahmstorf. There, an amateur can hardly do any research. ‘There have always been skeptics since he’s been doing research, ‘but last year they have broken into the serious media.'”
It just really stinks when the opposition has a voice. Die Zeit continues:
Together with his colleagues, he [Rahmstorf] counters skeptic claims and erroneous media reports at the Internet blog KlimaLounge, where some think he is overly fervent. Rahmstorf says that this is no fun, but sees no alternative. No matter where he goes, in government offices, in politics, top management levels of business – everywhere you hear skeptic arguments.”
I find it astonishing that a scientist would spend his work time preoccupied with PR work and spin. I thought they are supposed to be doing science, and not PR damage control. Clearly Rahmstorf spends much of his time writing stories for the NYT Times, Die Zeit, Der Spiegel and blogging, and not on the work he is paid to do.
A big part of Rahmstorf’s problem is that he’s turned a lot of people off with his smear tactics, getting on the wrong side of a huge number of scientists. He’s got no shortage of enemies.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Damn Big Oil and industry
In the Die Zeit piece, out comes the old worn out tactic of linking skeptics to Big Oil oil or to the coal industry, which  only further confirms the bankrupt state of the global warming narrative.
Also the top managers of power giant RWE took part in a conference by mining engineers who presented ‘solidly established doubt’ on man-made climate change. Foremost, the coal lobby is spreading doubt about global warming in the 2009 annual report of the coal association one reads that the brakes have been put on climate change.
Employees of E.on, Bayer and BASF in the USA this year contributed at least 70,000 dollars to skeptic politicians.”
If Rahmstorf and the warministas had their way, private contributions to the organizations he disagrees with would be banned. Sounds undemocratic to me. Let’s not talk about the BILLIONS that flow into warmist coffers.
Falsehoods about Climategate and the IPCC are being spread
Die Zeit refuses to acknowledge any scandals in climate science, particularly Climategate and the IPCC 4AR, and twists itself into a pretzel defending the rogue institutes behind them. Not once did Die Zeit publish the damning e-mails, keeping them hidden and locked away from the public instead. Die Zeit claims that many of the ‘scandals’ have already been debunked, and even goes on to defend Mann’s crooked hockey stick. Die Zeit complains:
Even so, these supposed scandals have made their way through the Internet blogs thousands of times.”
Damn Internet. Of course, the entire climate science community knows that these scandals have not been debunked. Here’s a list of 94 scandals that have yet to be resolved. An updated and much expanded list is coming out soon. As far as the IAC is concerned, it Admits Well Is Poisoned, Yet Insists Water Is Safe.
Damn Fred Singer and the Heartland Institute 
Part 3 of Die Zeit’s piece focuses on Fred Singer, the Heartland Institute, tobacco in the 1960s and Oreskes’s Merchants of Doubt. This is old and is just a lazy rehash of what appeared already weeks ago at Der Spiegel here. Die Zeit goes on and marginalizes scientific debate:
As usual the debate took place for years in the scientific journals, and the uncertainty is pretty much cleared up. This is now just constant back and forth that has since taken place in the lurid light of the public,’ says Hartmut Grassl, the 71-year old doyen of German climate science. Skeptics cherry-pick uncertainties in such debates. But all this has nothing to do with skepticism, nothing to do with critique and testing.'”
In Die Zeit’s simple world, it all goes back to Fred Singer. And CO2 drives the climate, of course.
Damn EIKE and their climate conferences
Spreading skepticism in Germany, as it was done in USA, has been successful, thanks to Holger Thuss, spokesman and founder of EIKE. EIKE is funded exclusively by private donations and has less than a hundred members. This year in December it will hold its 3rd Climate Conference in Berlin, and this time they have just enough money to have the food catered instead of buying it at a supermarket like they had to do last year. Die Zeit:
The conference next week at the Maritim Hotel in Berlin shows that EIKE, despite its dubious science, has been successful in building a network. The list of speakers includes the former President of the German Steel Industry Association. One co-sponsor of the conference is the market-radical Berlin Manhattan Institute for Entrepreneurial Freedom, which has only a one-man office, but has a board filled with economic professors who convey an air of seriousness.”
Die Zeit also has jumped in on the mob-political-lynching of Marie-Luise Dött, German Parliamentarian and a central figure on Angela Merkel’s environmental committee, whose crime was to express skepticism on climate change. Read here.
Hans von Storch chimes in 
Die Zeit ends its piece quoting Hans von Storch, who assumes his comfortable perch on the fence.
“I’ve taken a look at such skeptic conferences twice.  The level for the most part was catastrophic’. Many go there to simply spread pre-packaged opinions. ‘A real interest in a discussion could not be detected.”
Detection is indeed a big problem in climate science. Some things that hardly exist, get overly detected, while other things staring right at you in the face are ignored. There’s a lot of confusion in climate science.
Skeptics’ reaction
Finally I asked EIKE for their thoughts on the Die Zeit piece, to which they kindly answered. In a nutshell, they didn’t seem the least bothered by the Die Zeit report, taking it in stride and actually welcoming it. A spokesman wrote it will bring much more attention to the discussion and generate even more interest in the Climate Conference in Berlin. Then he added, quoting Gandhi:
First they ignore you.
Then they ridicule you.
Then they attack you.
Then you win.
We are now at stage III.”
Like it or not, this discussion is going to take place. SO LIVE WITH IT.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterMatti Vooro presents his latest essay on colder winters in the UK. Matti’s last essay: Signs of strengthening global cooling, drew over 100 reader comments – a record at NoTricksZone.
================================================
Should Britons Buy Bermuda Shorts Or Long Johns?
by Matti Vooro
Snowfalls are now just a thing of the past. “
That was the headline in the UK’s The Independent newspaper in March of 2000. The CRU scientists claimed that within a few years winter snowfall would become “a very rare and exciting thing. Incapable of learning, even reconfirmed this as recently as January 10, 2010 when one of their scientists told the UK Mail:
The winter is just a little cooler than average, and I still think that snow will become an increasingly rare event”.
The Met Office then followed The Independent with their prediction of the 4°C temperature rise in only 50 years, predicting warmer temperatures, more heat waves and drought. The IPCC had the same message in their 2007 report with their prediction that:
Annual mean temperatures in Europe are likely to increase more than the global mean. The warming in northern Europe is likely to be largest in winter.”
Yet, only a few years after these predictions of unprecedented winter warming for UK and Europe, the exact opposite has emerged. Winters have been getting colder and there is no lack of snow. UK winters have declined in temperatures 4 years straight since 2007. So have the annual temperatures. The last two winters have been especially cold and wintry.
 Taking the UK as a whole and not just Central England or CET
2010 December [-1 C] coldest December since 1910
2009 December [2.1 C] 13th coldest December since 1910
2008 December [3.1 C] 26th coldest December since 1910
2007 December [3.77C] 56th coldest December since 1910
It is dramatic how the winter temperatures have shifted since 2007 winter, which was the 2nd warmest winter in the UK since 1910.
What follows are the mean winter temperatures for all of UK. The average mean winter temperature is around 3.6C
2007   5.56 C (2nd warmest)
2008   4.86 C
2009   3.21 C
2010   1.64 C (7th coldest)
The annual UK temperatures have been declining since 2006 as the following shows:
2006 9.73 C (warmest since 1910)
2007 9.59 C
2008 9.05 C
2009 9.17 C
2010 7.96 C (12th coldest since 1910)

Refer to the UK Met Office and the excellent data provided from the following source:
http://www.metoffice.gov.uk/climate/uk/datasets/Tmean/date/UK.txt
According to the Met Office:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




2010 was the 12 th coldest year in the 100 year series and the coldest since 1986. 2010 was the coldest year since 1919 in Scotland and Northern Ireland “
The following graph is a plot of winters in Central England as opposed to UK as a whole for the years 1948-2010. UK temperatures as a whole are similar but slightly lower by 0.5 to1 degree C.  Winter AO levels are also shown.

Typically La Nina winters used to be colder before the 1970’s but during the last 11 La Nina winters, 9 winters have been near normal or warmer for Central England. This did not happen in December 2010. January 2011 is more typical of La Nina winters. El Nino winters seem to set up colder and negative AO and colder winters except when they are extra strong like 1998. It would appear from field or observed data, that UK winters are not getting warmer as predicted but may actually be getting cooler instead and may be following the natural planetary cycles perhaps similar to what happened during the last cooler cycle about 1962-1987.
Man-made greenhouse gases have little to do with this cooling as CO2 keep rising in a minor way. The weather was supposed to get warmer as CO2 levels have gone up? It is not. Global warming science does not seem to be holding up and seems to need a serious rethink, see GWPF.
During the 26 years of the last colder period , 17  years or 2/3 of the winters were below the average mean winter temperature of 3.6 and about 12 (45%) were  much colder and below 3C,  Of the 78 winter months during 1962-1987,  41 months (53%) were below average mean of 3.6°C.
What was the main weather factor present during those cooler winters?
Number of winters where AMO was negative:   22 (84%)
Number of winters where AO was negative:   21 (80%) [dec/jan/feb]
Number of winters where PDO was negative: 15 (58%)
Number of winters NAO was negative:  13 (50%)
ENSO years neutral 8 years (30%), LA NINA 9 years (35%), EL NINO 9 years (35%)
Number of winters with a net negative AO [dec/jan/feb] during the last 60 years:
1950’s 6
1960’s 10 (very cold winters)
1970’s 6
1980’s 7
1990’s 4 (very warm winters)
2000’s 4 (very warm winters)
Clearly the presence of negative AO, AMO, PDO and NAO were the most frequently occurring climate factors happening during that time. With the exception of AMO, all these factors are again heading for or are already in their negative or cool mode.
The sun is also still in its low activity level and unusual extra warming seems unlikely. AMO is likely to go negative or cool within 5-10 years if not sooner. There were many instances of back to back months of very cold weather as well as back to back cold winters like the 1960’s.
Summary
What would you do if you were a member of the UK general public or an official in charge of transportation, roads, airports, fuel supply, electricity or other infrastructure, read here WUWT?
Clearly the some agencies charged with informing the public about seasonal or long term weather did not have their act together yet. There was a serious warming bias in many weather and climate forecasts due to an over-emphasis on global warming. For example, in the midst of the worst part of December 2010 winter storm, the focus of the chief climate scientist was not on how to help the public with better information during the crisis, but on global warming. Professor Slingo insisted in comments to the Independent newspaper on December 21, 2010:
The key message is that global warming continues.”
Some refuse to learn. People are finding out that a second opinion on winter weather is paying off. Many North American meteorologists like Joe Bastardi and Joe D’Aleo and independent UK meteorologists like Piers Corbyn, have been predicting cooler weather the last several years. 
In my judgment, no one can predict with certainty what the future of the climate will be for the next 10-30 years.
Each climate cycle is different. It will not be a mini ice age in my opinion as some are predicting, nor will all the global general temperatures go below those that existed before the 1976 Pacific climate shift, but more of a cyclic cooler period.
Once the North Atlantic ocean SST and AMO start to contribute to the global cooling in a more significant way, the global temperatures of US and Canadian east coasts, the western coast of Europe and the Arctic will be the cooling more consistently.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterAnyone know a good copyright attorney interested in splitting awards 50/50?
I wonder how many bloggers know what the symbol to the left means? Seems like there are some that don’t.
No, it doesn’t mean Christmas!
I really wish I didn’t have to write this. But I feel compelled to do so.
I’ve been blogging 9 months and I try to make sure I don’t violate copyright laws. Copyright laws do enter gray territory at times, and sometimes it isn’t really clear what is allowed and what is not. I hope that I’ve always stayed within the law. I use photos from Wikipedia, as it’s stated there what you can and cannot do. I hear a lot of people bring up “fair use”. Laws vary from country to country. It gets complicated and fuzzy.
When blogging, I often quote papers, online articles and provide excerpts, always making sure to cite and link to the sources. I think people who wrote the original material deserve credit and recognition for their work. I like linking to other sites, hoping that the little extra traffic I generate will make them happy. What goes around, comes around.
What do other bloggers think? I’d be interested to know. Maybe I’m naive about all this.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




I must say I’m surprised by the number of sites that simply copy and paste ENTIRE stories and paste them at their site, without even asking. This has been done to me more than once, and with increasing frequency lately. With one of my recent posts I found 6 other sites that did this with my story – not one asked for permission to do so. Sure they provided a link, but who needs to click on it when it’s all there?
I don’t want to sound like I’m crying about it, but geez, c’mon, it sure would be nice if people asked. Like hello! I’m not there to carry your water.
Don’t get me wrong, I love it when other blogs pick up on the stuff I write. It really feels rewarding. All I ask is that bloggers post the first 30% of the content or so, or excerpts, followed by a direct link to the original story. Is that too much to ask? Am I being naive?
Tom Nelson does an outstanding job in respecting other people’s work. Tom always posts a few main excerpts, some content, then followed by a link to the original story. This way everyone is happy.
I noticed some sites that copy and paste entire stories without permission also have advertisements at their sites, meaning they are gaining financially by copying and pasting. In this case the originators of the content have the right to compensation.
I’m not going to go after past infractions, but in the future I’m not going to let these things slide as much anymore. Bloggers who benefit financially from advertising at their sites, or receive donations, and decide to copy and paste entire stories from NTZ will have to expect to hand over a cut of their profits, willingly or unwillingly.
All in all, I’m just hoping everyone will agree to play fair in the future.  
Merry Christmas everyone.
Share this...FacebookTwitter "
"
UPDATE 1/28: Full text of Dr. Theon’s letter has been post on the Senate website and below.
This is something I thought I’d never see. This press release today is from the Senate EPW blog of Jame Inhofe.  The scientist making the claims in the headline, Dr. John S. Theon, formerly of  the Institute for Global Environmental Strategies, Arlington, Virginia, has a paper here in the AMS BAMS that you may also find interesting. Other papers are available here in Google Scholar. He also worked on the report of the Space Shuttle Challenger accident report and according to that document was a significant contributor to weather forecasting improvements:
The Space Shuttle Weather Forecasting Advisory  Panel, chaired by Dr. John Theon, was established by NASA  Headquarters to review existing weather support capabilities and  plans and to recommend a course of action to the NSTS Program.  Included on the panel were representatives from NASA, the National  Oceanic and Atmospheric Administration (NOAA), the Air Force, and the  National Center for Atmospheric Research.
For those just joining the climate discussion, Dr. James Hansen is the chief climate scientist at NASA Goddard Institute for Space Studies (GISS) and is the man who originally raised the alarm on global warming in 1988 in an appearance before congress. He is also the keeper of the most often cited climate data.
EPW press release below – Anthony

Washington DC, Jan 27th 2009: NASA warming scientist James Hansen, one of former Vice-President Al Gore’s closest allies in the promotion of man-made global warming fears, is being publicly rebuked by his former supervisor at NASA.
Retired senior NASA atmospheric scientist, Dr. John S. Theon, the former supervisor of James Hansen, NASA’s vocal man-made global warming fear soothsayer, has now publicly declared himself a skeptic and declared that Hansen “embarrassed NASA” with his alarming climate claims and said Hansen was “was never muzzled.”  Theon joins the rapidly growing ranks of international scientists abandoning the promotion of man-made global warming fears.
“I appreciate the opportunity to add my name to those who disagree that global warming is man made,” Theon wrote to the Minority Office at the Environment and Public Works Committee on January 15, 2009. “I was, in effect, Hansen’s supervisor because I had to justify his funding, allocate his resources, and evaluate his results,” Theon, the former Chief of the Climate Processes Research Program at NASA Headquarters and former Chief of the Atmospheric Dynamics & Radiation Branch explained.
“Hansen was never muzzled even though he violated NASA’s official agency position on climate forecasting (i.e., we did not know enough to forecast climate change or mankind’s effect on it). Hansen thus embarrassed NASA by coming out with his claims of global warming in 1988 in his testimony before Congress,” Theon wrote.  [Note: NASA scientist James Hansen has created worldwide media frenzy with his dire climate warning, his call for trials against those who dissent against man-made global warming fear, and his claims that he was allegedly muzzled by the Bush administration despite doing 1,400 on-the-job media interviews! – See: Don’t Panic Over Predictions of Climate Doom – Get the Facts on James Hansen  – UK Register: Veteran climate scientist says ‘lock up the oil men’ – June 23, 2008 & UK Guardian: NASA scientist calls for putting oil firm chiefs on trial for ‘high crimes against humanity’ for spreading doubt about man-made global warming – June 23, 2008 ]
Theon declared “climate models are useless.” “My own belief concerning anthropogenic climate change is that the models do not realistically simulate the climate system because there are many very important sub-grid scale processes that the models either replicate poorly or completely omit,” Theon explained. “Furthermore, some scientists have manipulated the observed data to justify their model results. In doing so, they neither explain what they have modified in the observations, nor explain how they did it. They have resisted making their work transparent so that it can be replicated independently by other scientists. This is clearly contrary to how science should be done. Thus there is no rational justification for using climate model forecasts to determine public policy,” he added.
“As Chief of several of NASA Headquarters’ programs (1982-94), an SES position, I was responsible for all weather and climate research in the entire agency, including the  research work by James Hansen, Roy Spencer, Joanne Simpson, and several hundred other scientists at NASA field centers, in academia, and in the private sector who worked on climate research,” Theon wrote of his career. “This required a thorough understanding of the state of the science. I have kept up with climate science since retiring by reading books and journal articles,” Theon added. (LINK) Theon also co-authored the book “Advances in Remote Sensing Retrieval Methods.” [Note: Theon joins many current and former NASA scientists in dissenting from man-made climate fears. A small sampling includes: Aerospace engineer and physicist Dr. Michael Griffin, the former  top administrator of NASA, Atmospheric Scientist Dr. Joanne Simpson, the first woman in the world to receive a PhD in meteorology, and formerly of NASA, Geophysicist Dr. Phil Chapman, an astronautical engineer and former NASA astronaut, Award-Winning NASA Astronaut/Geologist and Moonwalker Jack Schmitt, Award-winning NASA Astronaut and Physicist Walter Cunningham of NASA’s Apollo 7, Chemist and Nuclear Engineer Robert DeFayette was formerly with NASA’s Plum Brook Reactor, Hungarian Ferenc Miskolczi, an atmospheric physicist with 30 years of experience and a former researcher with NASA’s Ames Research Center, Climatologist Dr. John Christy, Climatologist Dr. Roy W. Spencer, Atmospheric Scientist Ross Hays of NASA’s Columbia Scientific Balloon Facility] 
Gore faces a much different scientific climate in 2009 than the one he faced in 2006 when his film “An Inconvenient Truth” was released. According to satellite data, the Earth has cooled since Gore’s film was released,  Antarctic sea ice extent has grown to record levels, sea level rise has slowed, ocean temperatures have failed to warm, and more and more scientists have publicly declared their dissent from man-made climate fears as peer-reviewed studies continue to man-made counter warming fears. [See: Peer-Reviewed Study challenges ‘notion that human emissions are responsible for global warming’ & New Peer-Reviewed Scientific Studies Chill Global Warming Fears ]
“Vice President Gore and the other promoters of man-made climate fears endless claims that the “debate is over” appear to be ignoring scientific reality,” Senator James Inhofe, Ranking Member of the Environment & Public Works Committee.
A U.S. Senate Minority Report released in December 2008 details over 650 international scientists who are dissenting from man-made global warming fears promoted by the UN and yourself. Many of the scientists profiled are former UN IPCC scientists and former believers in man-made climate change that have reversed their views in recent years. The report continues to grow almost daily. We have just received a request from an Italian scientist, and a Czech scientist to join the 650 dissenting scientists report. A chemist from the U.S. Naval Academy is about to be added, and more Japanese scientists are dissenting. Finally, many more meteorologists will be added and another former UN IPCC scientist is about to be included. These scientists are openly rebelling against the climate orthodoxy promoted by Gore and the UN IPCC.
The prestigious International Geological Congress, dubbed the geologists’ equivalent of the Olympic Games, was held in Norway in August 2008 and prominently featured the voices of scientists skeptical of man-made global warming fears. Reports from the conference found that Skeptical scientists overwhelmed the meeting, with  ‘2/3 of presenters and question-askers hostile to, even dismissive of, the UN IPCC’ ( See full reports here & here ]  In addition, a 2008 canvass of more than 51,000 Canadian scientists revealed 68% disagree that global warming science is “settled.”  A November 25, 2008, article in Politico noted that a “growing accumulation” of science is challenging warming fears, and added that the “science behind global warming may still be too shaky to warrant cap-and-trade legislation.”  More evidence that the global warming fear machine is breaking down. Russian scientists “rejected the very idea that carbon dioxide may be responsible for global warming”. An American Physical Society editor conceded that a “considerable presence” of scientific skeptics exists.  An International team of scientists countered the UN IPCC, declaring: “Nature, Not Human Activity, Rules the Climate”. India Issued a report challenging global warming fears. International Scientists demanded the UN IPCC “be called to account and cease its deceptive practices.”
The scientists and peer-reviewed studies countering climate claims are the key reason that the U.S. public has grown ever more skeptical of man-made climate doom predictions. [See: Global warming ranks dead last, 20 out of 20 in new Pew survey. Pew Survey:   & Survey finds majority of U.S. Voters – ‘51% – now believe that humans are not the predominant cause of climate change’ – January 20, 2009 – Rasmussen Reports ] 
The chorus of skeptical scientific voices grow louder in 2008 as a steady stream of peer-reviewed studies, analyses, real world data and inconvenient developments challenged the UN’s and former Vice President Al Gore’s claims that the “science is settled” and there is a “consensus.”
On a range of issues, 2008 proved to be challenging for the promoters of man-made climate fears.  Promoters of anthropogenic warming fears endured the following: Global temperatures failing to warm; Peer-reviewed studies predicting a continued lack of warming;  a failed attempt to revive the discredited “Hockey Stick“; inconvenient developments and studies regarding rising CO2; the Spotless Sun; Clouds; Antarctica; the Arctic; Greenland’s ice; Mount Kilimanjaro; Global sea ice; Causes of Hurricanes; Extreme Storms; Extinctions; Floods; Droughts; Ocean Acidification; Polar Bears; Extreme weather deaths; Frogs; lack of atmospheric dust; Malaria; the failure of oceans to warm and rise as predicted.
# # #
ORIGINAL FULL TEXT LETTER SENT VIA EMAILS:
—–Original  Message—–
From: Jtheon [mailto:jtheon@XXXXXXX]
Sent: Thursday,  January 15, 2009 10:05 PM
To: Morano, Marc (EPW) 
Subject: Climate models are  useless 
Marc, First, I sent several  e-mails to you with an error in the address and they have been returned to me.  So I’m resending them in one combined e-mail. 
Yes, one could say that I was,  in effect, Hansen’s supervisor because I had to justify his funding, allocate  his resources, and evaluate his results. I did not have the authority to give  him his annual performance evaluation. He was never muzzled even though he  violated NASA’s official agency position on climate forecasting (i.e., we did  not know enough to forecast climate change or mankind’s effect on it). He thus  embarrassed NASA by coming out with his claims of global warming in 1988 in his  testimony before Congress. 
My own belief concerning  anthropogenic climate change is that the models do not realistically simulate  the climate system because there are many very important sub-grid scale  processes that the models either replicate poorly or completely omit.  Furthermore, some scientists have manipulated the observed data to justify their  model results. In doing so, they neither explain what they have modified in the  observations, nor explain how they did it. They have resisted making their work  transparent so that it can be replicated independently by other scientists. This  is clearly contrary to how science should be done. Thus there is no rational  justification for using climate model forecasts to determine public policy. 
With best wishes, John 
# # 
From: Jtheon [mailto:jtheon@XXXXXX]
Sent: Tuesday, January  13, 2009 12:50 PM
To: Morano, Marc (EPW) 
Subject: Re: Nice seeing you 
Marc, Indeed, it was a pleasure  to see you again. I appreciate the opportunity to add my name to those who  disagree that Global Warming is man made.  A brief bio follows. Use as much or  as little of it as you wish. 
John S. Theon Education: B.S.  Aero. Engr. (1953-57); Aerodynamicist, Douglas Aircraft Co. (1957-58); As USAF  Reserve Officer (1958-60),B.S. Meteorology (1959); Served as Weather Officer  1959-60; M.S, Meteorology (1960-62); NASA Research Scientist, Goddard Space  Flight Ctr. (1962-74); Head Meteorology Branch, GSFC (1974-76); Asst. Chief,  Lab. for Atmos. Sciences, GSFC (1977-78);  Program Scientist, NASA Global  Weather Research Program, NASA Hq. (1978-82); Chief, Atmospheric Dynamics &  Radiation Branch NASA Hq., (1982-91); Ph.D.,  Engr. Science & Mech.: course  of study and dissertation in atmos. science (1983-85); Chief, Atmospheric  Dynamics, Radiation, & Hydrology Branch, NASA Hq. (1991-93); Chief, Climate  Processes Research Program, NASA Hq. (1993-94); Senior Scientist, Mission to  Planet Earth Office, NASA Hq. (1994-95); Science Consultant, Institute for  Global Environmental Strategies (1995-99); Science Consultant  Orbital Sciences  Corp. (1996-97) and NASA Jet Propulsion Lab., (1997-99). 
As Chief of several NASA Hq.  Programs (1982-94), an SES position, I was responsible for all weather and  climate research in the entire agency, including the  research work by James  Hansen, Roy Spencer, Joanne Simpson, and several hundred other scientists at  NASA field centers, in academia, and in the private sector who worked on climate  research. This required a thorough understanding of the state of the science. I  have kept up with climate  science since retiring by reading books and journal  articles. I hope that this is helpful. 
Best wishes, John 

Sponsored IT training links:
Best quality 640-553 dumps written by certified expert to help you pass 642-456 and 70-536 exam in easy and fast way.



			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e991001c7',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The Clinton administration has developed a nasty habit of using personal tragedy to further its global warming agenda. From the snowmelt‐​caused Red River flood last year, to Florida’s fires this summer (which blazed because there was too much vegetation), to Hurricane Mitch, if there’s any possible way to conflate human suffering with global warming, the administration will do so. 



Administration antics on Mitch, a real son of a gun when it came to flooding rain, began during the recent Buenos Aires conference on global warming. There, the head of the U.S. Agency for International Development, J. Brian Atwood, told CBS News that Hurricane Mitch, which killed an estimated 10,000 Central Americans, was a “classic greenhouse effect.” One hopes that Mr. Atwood actually knows better and is merely engaging in White House huckstering. 



In 1974 Hurricane Fifi killed the same proportion of the (then smaller) population of Honduras. In 1971 Hurricane Edith plowed into the northwestern tip of Honduras at Cabo Gracias a Dios as a Category 5 blaster. In 1955 Hurricane Janet, another Category 5 storm, had hit a couple of hundred miles to the south of where Edith landed. Only two storms of that magnitude have ever hit the United States. Flooding is another recurring phenomenon. In 1979 Tropical Storm Claudette produced five feet of rain in Texas — just like Mitch in Honduras — but killed about 10,000 fewer people. 



Certainly Atwood’s staffers could have apprised him of the refereed scientific literature on global warming and hurricanes. It contains two speculative papers saying that hurricanes may get worse and an overwhelming number of others proving that notion wrong. In addition, hurricane observations in warm years and during planetary warming argue more for the opposite — weaker storms. 



If there’s any possible way to conflate human suffering with global warming, the Clinton administration will do so. 



The first paper, published in 1987 in Nature by Kerry Emanuel, speculated that under unrealistic, physically impossible conditions, global warming would increase the strength of hurricanes. While that paper generated a lot of press, scientists knew it was merely an exercise in hurricane vortex mathematics. Emanuel threw more fuel on the fire when he published an article on “hypercanes” in the widely read American Scientist. 



The larger community of hurricanologists had by then had enough. In 1994 James Lighthill published in the Bulletin of the American Meteorological Society an extensive review finding no basis for Emanuel’s speculation. 



In 1996 a storm surge of literature blew apart Emanuel’s hypothesis. First, the aptly named Chris Landsea, a scientist at the National Hurricane Center, observed in Geophysical Research Letters that there had been a significant decline in the frequency of severe hurricanes in the Atlantic Basin over the last 50 years. The region warmed a few tenths of a degree during the period. 



Later that year Johnny Chan published in the same Journal an article finding no net trend in Pacific typhoons. Even the computer modelers got into the act. Europe’s Lennart Bengtsson published a paper in the journal Tellus showing that a computer climate simulation with an enhanced greenhouse effect predicted fewer hurricanes and lower average winds. 



Also in 1996 J. B. Elsner found that the regions in which hurricanes form had shifted in the last 40 years and now favor the development of weaker storms. And Landsea, in yet another report published by the United Nations Intergovernmental Panel on Climate Change in 1996, showed that there has been a statistically significant decline in the maximum windspeed measured in Atlantic hurricanes since World War II. 



Australian climatologist Ann Henderson‐​Sellers and 10 others re‐​reviewed the hurricane/​global warming situation in last January’s Bulletin of the American Meteorological Society. They called their work a “Post IPCC Assessment,” meaning that they believed it stood for the consensus of scientists on the issue. They simply could not find any increase in hurricane frequency or severity, and they looked everywhere. They also took pains to note that the conditions assumed in Emanuel’s initial work are just about impossible. That work and its ilk contain “known omissions [that] all act to reduce these increases” of hurricanes, Henderson‐​Sellers wrote. 



It’s rare to get such scientific consensus in climatology. But in one last attempt to bring the exaggerators and the alarmists to heel, Henderson‐​Sellers recently published in the journal Climate Change a paper detailing the whole sorry history of the campaign to hype hurricanes. Ironically, Henderson‐​Sellers herself is not shy about touting the dangers of global warming. 



Who gains here? Rumors persist that Vice President Gore has been advised to make global warming a central theme of his presidential run in 2000. Threatening hundreds of thousands with imminent drowning unless they vote for him is a crude but probably effective trick.
"
"Renewable energy is generally viewed as a long-term solution to climate change. It’s no surprise then that a great deal of effort is going into to powering the world by using only renewables, and researchers are even looking seriously at the prospect of Europe switching to 100% renewable energy by 2050.  However, there is a downside – renewable energy depends on natural resources that exist on planet Earth in fixed amounts and are very much non-renewable. The issue of rare earth elements, used in many technologies including solar panels and batteries, is well known. Although these elements are not always as rare as their name suggests, they are finite and not renewable. Also, just one country, China, presently has a monopoly on the production of most of these elements, which raises the question of energy security.  But apart from rare earths, there are other non-renewable materials used for renewable energy – and the metal lithium is a good example. As it’s highly reactive and relatively light, lithium is ideal for use in batteries. And the ability to store large amounts of energy is crucial to renewable energy, because sunshine and wind don’t simply appear at convenient times when humans need electricity.  Another major application of lithium is in the batteries of electric and hybrid vehicles. These vehicles certainly have lots of potential to reduce carbon dioxide emissions but, in the long term, their feasibility will be challenged by the use of lithium in their batteries. A quick calculation shows that, if all conventional cars (those using petrol/gas or diesel) were replaced by electric cars, the world would run out of lithium in around five decades.  I take the total amount of lithium from the US Geological Survey, which estimates there are currently 14m tonnes of proven reserves worldwide. I used industry figures for the total amount of passenger cars sold worldwide – about 69m in 2016. That same year, less than a million electric vehicles were sold, even including plug-in hybrids.  Now, if we imagine a future where all passenger cars were electric and the number of cars sold per year remains constant at 2016 levels, almost 69m (technically: 69.46m minus 0.75m) electric cars will have to be produced each year even at a very cautious estimate. Our assumption here that the demand of cars will remain constant is actually very conservative, as demand typically increases with time. Today, a compact electric vehicle battery (Nissan Leaf) uses about 4kg (9lb) of lithium. This means, around 250,000 tonnes of lithium would be required annually to produce enough electric cars to replace their petrol equivalents. At this rate, the 14m tonnes of proven reserves would be exhausted within 51 years.  The recycling of lithium from used batteries is not taken into account here. But it is important to note that electric cars are not the only product that use lithium. Currently, batteries use around 39% of total production, while the rest goes into ceramics and glass, lubricating greases, and other applications. So even if we imagine 100% of lithium in used batteries was recovered (not technically possible), much of that would still be used for other purposes, and supplies would still eventually be exhausted. The world is not running out of lithium yet because renewable energy and electric vehicles are nowhere near replacing fossil fuels completely. Demand will increase in future, however, which could prompt further exploration and perhaps the discovery of new reserves, or even improvements in mining technology to make more of the metal accessible to us. All these could make lithium last longer, but that does not mean we will be able to use huge amount of it indefinitely. Lithium is just one example of a worrying reliance within renewable energy on non-renewable natural resources that exist only in fixed amounts on Earth. Solar and wind power do have great prospects of coping with the problems of climate change, but much careful planning is needed and we cannot assume that renewables will solve all environmental problems.  Now is the right time to establish recycling plants for rare earth elements and other non-renewable natural resources used in renewable energy systems such as lithium. More importantly, it is necessary to reduce our consumption of natural resources. If we go on with mindless consumerism, we will only shift the problem from one natural resource to another."
"

In between inventing the automobile, penicillin and electricity, growing up as a missionary on the Amazon and supporting his fatherless family of 13 as a bootblack, and inspiring hit musicals and epic poetry, Vice President Al Gore is acting as a commander in chief wannabe. 



His role as propagandist on behalf of the administration’s disastrous war of aggression in the Balkans is reason enough to reject him in the year 2000. But his record on domestic issues is even worse. 



The vice president’s campaign minions are saying that he is a tough leader who pushed for military action against Yugoslavia. Mr. Gore certainly is talking tough: “Milosevic has barely begun to incur the damage he will feel.” 



Of course, Mr.Gore probably couldn’t do worse than Bill Clinton, who has bungled every step. Were the latter commander in chief during World War II, we would all be speaking German. 



However, Mr. Gore is attempting to do more than score political points by warmongering for peace. Of late, he’s been battling airlines over compensation for lost bags and pushing to create a special phone number to call about traffic jams. For this, newly independent American Colonies created a national government? 



A Gore associate explained that such measures will “add up to something more thematic, something bigger.” And they do. The vice president once said he believes government should be “like grandparents, in the sense that grandparents perform a nurturing role.” 



But Mr.Gore prefers to “nurture” with a mailed fist. As former ABC correspondent Bob Zelnick puts it in his devastating new book, “Gore: A Political Life” (Regnery): “Al Gore Jr. was a child of government and a student of government who grew up to be a man of government.” 



The vice president has been traversing the country telling audiences he embodies “practical idealism.” However, he has been able to cultivate the image of a moderate primarily because he once took more conservative stands on security and social issues. But 28‐​year‐​old candidate Gore ran a populist economic campaign — higher taxes on the rich, support for public jobs creation — to win election in 1976 to Congress from Tennessee. 



He generally fit well within the Democratic caucus. He was a reliable supporter of new spending programs, whether business subsidies or redistributive entitlements; higher taxes, especially on the upper‐​middle class; increased regulation, particularly for environmental purposes; and social engineering schemes, such as racial quotas. 



Mr.Gore was at his worst on taxes. Between 1981 and 1993, he opposed only one of 19 significant tax increases; he voted to collect an extra $9,000 per household. He supported a plethora of other tax increases, which failed to pass. 



At least all of these measures required votes. The vice president also backed the multibillion‐​dollar e‐​rate levy (or “Gore Tax”) on phone service, which has been imposed without public debate by the Federal Communications Commission. 



The vice president has placed himself on the extremist edge of environmental policy‐​making. In his book, “Earth in the Balance,” he declared: “We must make the rescue of the environment the central organizing principle of civilization.” 



He has pushed a variety of new energy taxes. He wants employers to subsidize workers who don’t drive to work. He advocates eliminating the internal combustion engine. He proposes banning packaging that is neither biodegradable nor recyclable. He advocates more foreign aid to Third World states for environmental purposes. 



Perhaps Mr.Gore’s most important environmental crusade involves global warming. In no small part due to his efforts, the administration signed the Kyoto Protocol to the Framework Convention on Climate Change, which mandates substantial reductions in global energy consumption. 



Yet years of scaremongering have proved to be inaccurate. Observed warming has been far below that predicted by the models upon which the convention was based. Even Mr. Gore admitted in 1995: “In truth, the scientists who are expert in this field will tell you that the precise causal relationship (between C02 and global warming) has not yet been established.” 



The Kyoto Treaty, as yet unratified by the Senate, would impose huge burdens on the United States. 



Yale University economist William Nordhaus figures the bill could run $2,000 per household every year. Wharton Econometrics Forecasting Associates estimates 1.8 million jobs could be lost; others predict losses of as many as 3 million. 



Mr. Gore has attempted to disguise his statist bias by heading up the president’s program to “reinvent” government. However, his claim to have saved $137 billion is belied even by the National Performance Review’s own reports. Federal employment has not fallen due to his efforts. Mr. Gore has held press conferences rather than recommend eliminating useless agencies. 



Commander in Chief Al Gore? It’s a terrible thought. But even worse would be President Gore running domestic policy.
"
"
And the hits just keep on coming…1,318,794 page views for January according to WordPress.

Thanks everybody!


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e987edbe3',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
"

The holiday season must be a time of mixed emotions for environmentalists critical of electrified America. It may be the season of good cheer and goodwill toward all, but it is also the time of the most conspicuous of all energy consumption. For the last month of the year, billions of small light bulbs illuminate America. Somber darkness and everyday lighting are transformed into magnificent beauty and celebration. Christmas lights are a great social offering — a positive externality in the jargon of economics — given by many to all.



Although doomsayer Paul Ehrlich once railed against “garish commercial Christmas displays,” energy conservationists have not engaged a public debate of the issue. Yet holiday season lighting is a glaring exception to the goal of reducing energy usage wherever possible. If holiday electricity guzzling is forgiven, shouldn’t open‐​air heating and cooling, bright central lighting and instant‐​on appliances that “leak” electricity be excused? Walking around the hotel room to turn on individual lights or waiting for the photocopier to warm up, after all, squanders the most scarce and depleting resource of all, a person’s time. Surely energy uses for human comfort and convenience, even when extravagant, should have priority over purely celebratory uses of electricity.



What about the holiday humbug that celebratory electricity usage depletes hydrocarbons, fouls the air and destabilizes climate? Good tidings abound! The world’s proven reserves of oil, natural gas and coal are at record levels. If probable resources are added to proven reserves, world supply is officially estimated at more than 2 thousand years for coal, 200 years for natural gas and 150 years for crude oil. Substitutes within the hydrocarbon family and derivatives from biomass make oil and gas inexhaustible.



The quality of our air has dramatically improved in recent decades despite record consumption of each hydrocarbon. As the Environmental Protection Agency stated in its last annual air‐​quality summary: “Since 1970, national total emissions of the six criteria pollutants declined 31 percent, while U.S. population increased 31 percent, gross domestic product increased 114 percent, and vehicle miles traveled increased 127 percent.” Few advocates of clearer air from industry, government or the environmental community believe that technological improvement of power plants and motor vehicles will not continue to hasten the clean air revolution. The only question is in the short‐​run cost to cushion the transition for consumers wed to affordable energy.



Are global warming and other climate change attributed to increasing concentrations of carbon dioxide in the atmosphere reason to decrease our use of coal, oil and, eventually, natural gas? Good tidings exist here as well. The warming properties of increased greenhouse gas concentrations in the atmosphere are more modest (and beneficial) than the new doomsayers are letting on. Climate models predicting high warming scenarios hinge on an unproven positive feedback with the most prevalent greenhouse gas, water vapor. Our most reliable global temperature records, from satellites and balloons, indicate that the enhanced greenhouse effect is highly overrated.



Carbon dioxide has never been regulated for a reason: it is not a pollutant but an environmental tonic that helps to “green” the earth through enhanced photosynthesis, improved use of water by plants and longer growing seasons. Carbon dioxide cycling, in short, is a continuing windfall of the hydrocarbon era.



Discretionary electricity consumption during the holiday season is more than a gift of beauty and goodwill, it benefits ratepayers as a class. With today’s electricity rates well above the marginal costs of generation and distribution in virtually every region of the country for almost all of the year, increased consumption allocates the utilities’ fixed cost over more units to lower rates overall. A study by Citizens for a Sound Economy estimated that increasing electricity usage up to 25 percent across the United States during the off‐​peak season (including December) would lower rates by a like amount since existing facilities would be more fully utilized. More holiday lighting may be only a small step toward more efficient use of our electricity infrastructure, but it is a beginning. There is much to be thankful for in our energy economy this holiday season.



All economic and environmental indicators for conventional energies are positive and open‐​ended. In the 1970s pervasive price and allocation regulation led to public edicts and private efforts to curtail holiday lighting, but today we find that market‐​oriented policies have made Christmas lighting more plentiful and affordable than ever. May one and all in good conscience enliven the darkness and lower electricity rates this holiday season. And with a more competitive electricity market on the horizon, and constantly improving technologies coming into play, Americans can look forward to ever‐​greater holiday celebrations in the years and decades ahead.
"
"
Share this...FacebookTwitterAnd attention morons, drones, and other like-minded believers in superstition and ritual behaviour.
At 8:30 p.m CET tomorrow evening it is Earth Hour. It’s that time again to participate in yet another useless collective ritual of madness, by switching off the lights along with all the other drones, in a bid to get the climate gods to bestow nice weather upon us (and even stop bush fires – to and solve all the other world’s problems. Let’s call it: Be Like North Korea Hour.
Here’s one example drone-controller spokesperson, treating Australians like a bunch three-year-olds, asking them to take part in this ridiculous embarrassment of a modern rain-dance:

Now that we are all done throwing up, let’s continue.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In Germany, parents tell their toddlers to finish their food on the plate, otherwise it’ll rain tomorrow. Well now it’s live in the dark, or else it’ll rain a lot, or not enough, or whatever. “Let’s all turn off our lights altogether now so that we can have a better future. Aren’t we all just good little boys and girls! Hooooray!”
Yes, it’s time to call the doctor. Personally, they can ram this retardation where it’s permanently dark.
I guess some creatures and criminals of the night will especially like this. Women stay home (and leave the lights on outside and inside).
Hungary International Airport to switch off tarmac lights!
And get a load of this craziness: A bunch of moron officials at the Budapest Hungary International Airport are going to play around with the tarmac lights! Talk about making the world a safer place to live. Read here.
Of course, we can counter all this madness. For example I’m going to upgrade my light bulbs.
This is better. Osram 200-watt incandescent bulb. (Photo credit: C. Kemme)
The 500-watt flood lights work well too.  Do your part to light up the world. Make your neighbourhood safer tomorrow. Light it up!
Further reading:
Celebrating ignorance
Celebrate Human Achievement Hour
It’s Earth Century in North Korea
Make it look like Christmas
Share this...FacebookTwitter "
"
Share this...FacebookTwitterIn the older days, whether selling detergents, applicances or food products, marketers often used the slogan: “new and improved!” to con consumers and to boost sales. The German online DIE WELT reports that companies have changed their marketing slogans to make their business more profitable and sustainable. The new slogan used to pitch products today is:
Almost every conceivable company is jumping on the enviro-bandwagon and claiming their products or services are “sustainable” and thus good for the planet, no matter if it’s an automaker, coal power plant, or an investment instrument. The “sustainable” product is better and safer for the environment. The movement indeed is religious. DIE WELT writes:
You can now invest sustainably, and even fight dandruff in a sustainable way.”
Well, I turned off the light in the room next door, and so now I’m blogging sustainably. In fact I just changed the slogan of my blog. I’m the first climate blogger to blog more sustainably – the world’s most sustainable climate science blogger. Blog here! Going to any other climate sicence blog means you’ll be ruining the planet.
Misuse of the word
As DIE WELT writes, not everyone is amused about companies slapping the “sustainable” slogan on the packaging of their products, and claim it borders on false advertising in many cases. (Not me. I really am blogging with the light off and drinking tap water). Author Ulrich Grober has written a book on the history of sustainability, and is quoted by Die Welt:
Indeed even oil companies like BP use the word “sustainable“ in their annual reports. ‘Recently in Switzerland the most “sustainable” autobahn of all time was inaugurated“, says Grober. it clouds the meaning of the word.’ “
Experts say the word “sustainable” is now being used so often and so incorrectly that it has virtually lost all its original meaning. Sociologist Klaus Kraemer says the word “sustainability is now being used in political debates as the ultimate moral argument. “Whatever is sustainable is not to be questioned.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Use of the word “sustainable” is dangerous
The term being misused is one thing. But using the word for the purpose of marketing may be “dangerous”, says chemist Michael Braungart. Die Welt quotes Braungart:
The concept is backwards-looking and puts the brakes on creativity because it is connected with feelings of guilt.”
I agree with that. If you don’t buy a product that is labelled as “sustainable”, then you are someone who is harming the planet, and so you ought to feel bad about it. That’s how a religion works. So in the end, I think there is going to be a backlash with respect to this blind sustainability movement. The whole thing is rather Medieval. Back to the Dark Ages.
Educated consumers really ought to feel insulted
How do I feel when I see the “sustainable” slogan being targeted at me? Of course it annoys the hell out of me because I feel the seller of the “sustainable” product assumes that I’m actually stupid enough to believe all the CO2 nonsense. I’m insulted that they’d treat me like that. I’m not a blind zealot in a cult. It’s a slogan that maybe works well with morons, dupes and religious greens. But it certainly isn’t a way to communicate with people who think for themselves.
The enviro-sustainable hacks and bosses behind this movement don’t even believe it themselves. See how they jetset all over the world and live lavish lives while raking it in as duped consumers gobble it up. To the half-witted believers out there – wake up – you’re being duped by this utter nonsense.
To my loyal readers, please do not think that I’m targeting you with my new slogan. I know you don’t believe the CO2/sustainability crap. The new blog slogan is aimed solely at other people, like dana1981.
(PS: Actually I have the lights on – and it’s daytime.)
Share this...FacebookTwitter "
"
Share this...FacebookTwitterMinus 30°C for days…13°C below normal…homeless people dying…hands and feet are freezing…
That’s what we are hearing from a few media outlets in Europe, those who have dared to mention the “cold-snap” word and to write about reality. It’s been cold in Scandinavia, much of Europe, North America and Russia too. Where’s all the warming? Heck, even the oceans are below normal.
Deep freeze is forecast to continue
The European part of Russia is stuck deep in the freezer, reports the Austrian online Krone.at. The extreme cold is due to a huge high pressure system in the Arctic which has kept Moscow in temperatures down as low as -30°C for days. Krone.at writes:
The Russian media have been talking about ‘the hardest winter in the last 100 years’, causing 10 million people to shiver.
”This abnormal frost has been an enormous challenge,’ says Moscow mayor Sergei Sobjanin. Meteorologists don’t see any let up in the days ahead, and even expect temperatures to drop further. In the European part of Russia, unusually deep cold has dominated the area over the last 14 days. The average temperature for February so far alone for Moscow is 11 to 13°C below normal.”
There are reports that homeless people are getting hit hard. Pleas for blankets and clothing are being made. Famous Moscow doctor Elisabeth Glinki says:
Many people on the street are dying, or their hands and feet are freezing.”
Looking at the above temperature forecast chart above, things are going to get even worse in the days ahead.
But we all know what the explanation for this is, right!
Share this...FacebookTwitter "
"

“Of course you realize, this means war!” – Bugs
War has been declared in the New  York court system over global warming regulation.
Indeck Corinth L.P., which operates the Corinth Generating Station, an  electric power plant in Corinth, NY, sued New York stateon January 29, 2009 claiming that the Regional  Greenhouse Gas Initiative (RGGI) that aims to reduce greenhouse gas emissions in  the Northeast U.S. is illegal.
Corinth Generating Station -click for interactive view- Source: Microsoft Live Earth
Maine, New Hampshire, Vermont, Connecticut, New York, New Jersey, Delaware, Massachusetts, Maryland, and Rhode Island have signed on to the RGGI agreement. You can read more about it here at:  http://www.rggi.org/home
This is the simple view of RGGI from their website:
The Regional Greenhouse Gas Initiative (RGGI) is the first mandatory, market-based effort in the United States to reduce greenhouse gas emissions. Ten Northeastern and Mid-Atlantic states will cap and then reduce CO2 emissions from the power sector 10% by 2018.
States will sell emission allowances through auctions and invest proceeds in consumer benefits: energy efficiency, renewable energy, and other clean energy technologies. RGGI will spur innovation in the clean energy economy and create green jobs in each state.
Indeck Corinth claims that New  York’s involvement with RGGI does the following:

Is ultra vires and violates the state constitution;
Imposes an impermissible tax not authorized by the state legislature; 
Is arbitrary and capricious as implemented by New York; 
Is pre-epmted by state and federal  regulations; 
Violates the Compact Clause of the U.S.  Constitution; and 
Violates Indeck Corinth’s due process and  equal protection rights 

See Indeck Corinth’s legal complaint.  (PDF)
Indeck Corinth and New  York State  are now arguing over the venue for the suit. Indeck Corinth wants the suit heard in Saratoga County where it is a major employer. New York wants the suit heard in Albany County where it has home field advantage.
This will be watched intensely by many on both sides of the energy -versus- environment issue.
h/t to Junkscience.com


			__ATA.cmd.push(function() {
				__ATA.initDynamicSlot({
					id: 'atatags-1460517861-5fc7e9708bedc',
					location: 120,
					formFactor: '001',
					label: {
						text: 'Advertisements',
					},
					creative: {
						reportAd: {
							text: 'Report this ad',
						},
						privacySettings: {
							text: 'Privacy settings',
						}
					}
				});
			});
		Share this:PrintEmailTwitterFacebookPinterestLinkedInRedditLike this:Like Loading...

Related
 "
