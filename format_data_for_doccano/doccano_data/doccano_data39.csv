nan
nan
"It’s going to be a rough year. The fatal combination of escalating climate breakdown and the capture of crucial governments by killer clowns provokes a horrible sense of inevitability. Just when we need determined action, we know that our governments, and the powerful people to whom they respond, will do everything they can to stymie it. Witness the disasters in Australia. In mid-December, on the day the nation’s lethal heatwave struck, Rupert Murdoch’s newspaper the Australian filled its front page with a report celebrating new coal exports and a smear story about the chiefs of the state fire services, who were demanding an immediate end to the burning of fossil fuels. The response of the prime minister, Scott Morrison, to the escalating catastrophe was to embark on a holiday overseas as his country burned.  Some of the Earth’s largest land masses – Australia, Russia, the US, Brazil, China, India and Saudi Arabia – are governed by people who seem to care little for either humankind or the rest of the living world. To maintain their grip on power, which means appeasing key oligarchs and industries, they appear prepared to sacrifice anything – including, perhaps, the survival of humanity. I know that the protesters who made 2019 the year of climate action will continue to step up. We will do all we can to focus the world’s attention on the greatest crisis human beings have ever faced. But with hostile governments blocking a collective international response to this emergency, the struggle will feel increasingly desperate. I admit that I’m feeling quite close to burnout. I believe resilience is the most useful human quality, and I’ve sought to cultivate it, but in 2019 I felt my resolve begin to weaken at times as it has never done before. Part of the reason is doubtless my continuing health issues: the repeated complications and procedures that have followed my cancer treatment two years ago. Sometimes it’s hard to disentangle the external and internal sources of despondency. For many people, there is no such separation. We now know that people living in heavily polluted places have higher levels of depression and suicide: air pollution has been implicated in brain inflammation and damage to nerve cells, both of which have been linked to mental illness. Research has also linked wellbeing with environmental quality: a recent study in Denmark suggests that people who grew up in places with plenty of green space have a lower chance of developing a psychiatric disorder than those who were surrounded only by artificial surfaces, even when all other factors have been taken into account. I have tried to keep my eco-anxiety at bay, to box it into my working life. But every month this becomes more difficult. The rising sense of panic I feel is entirely rational; we should all be feeling it. But we can’t live with it through every hour of every day. So my new year’s resolution is to spend more time on my sea kayak. It possesses almost miraculous properties: it is a four-metre, plastic rejuvenation machine. After a day on the water, ideally paddling as far as I can, sometimes until the coast is out of sight, I feel ready for anything. But even across this experience a shadow now falls: my gathering awareness of what I should be seeing at sea, and its resounding absence. The shocking and distressing fact is that the waters around the UK were once among the most abundant on Earth, and are now among the least. Armadas of bluefin tuna once stormed our coasts, harrying shoals of mackerel and herring many miles long. Halibut the size of barn doors and turbot like tabletops came into shallow water to feed. Cod commonly reached almost two metres; haddock grew to a metre. Plaice were the size of road atlases. Pods of fin whales and sperm whales could be seen from the shore, while Atlantic grey whales, now extinct, roamed our estuaries. Gigantic sturgeon poured up the rivers to spawn, pushing through traffic jams of salmon, sea trout, lampreys and shad. On some parts of the seabed the eggs of the herring lay six-feet deep. Much of the sea floor was covered by a continuous crust of life: reefs of oysters and mussels, soft corals and sea pens, sea fans and sponges, peacock worms and anemones, stabilising the sediments and filtering the water column, with the result that our seas might have been crystal clear. The abundance of everything, if we were to transport ourselves back a few hundred years, would blow our minds. Now, on some days, it’s a surprise to see anything. I might, if I’m lucky, spot a flock of shearwaters, skimming the waves with their velvety wings, a couple of gannets, a solitary razorbill, the occasional small bait ball. When I kayak in Cardigan Bay, in Wales, what I hope to find above all else is dolphins. Sometimes I do, and these days are the waymarks of my life. But my sightings seem to have become less common since scallop dredgers were allowed back into even the most “strictly protected” parts of the bay by the Welsh government, ripping up the seabed and destroying most of the life it harbours. The same applies to nearly all the “marine protected areas” around the UK’s coasts. But these zones amount to little more than lines on a map. While 36% of England’s waters are theoretically set aside for wildlife, commercial fishing – which has by far the greatest impact on the life of the seas – is excluded from less than 0.1% of that area. In fact, the trawling intensity in “protected” zones is higher than in unprotected places. It’s all so stupid. Commercial fishing is by far the greatest cause of ecological destruction at sea, but produces less income and employment in the UK than the industries it wrecks. Recreational angling alone, which is perpetually threatened by the absence of fish, generates more jobs and money than commercial fishing. Whale and dolphin watching, diving and snorkelling would, if allowed to prosper, greatly enhance the livelihoods of coastal people. And this is to say nothing of the immeasurable improvements in the lives of everyone connected to a thriving, abundant living system. If we stop dragging trawls and dredges through it, the life of the seas would recover with astonishing speed. Because most marine animals are highly mobile during at least one stage of their development, the rewilding of the seas needs little help from humans. But we could make a few useful interventions, such as the possibly crazy but wonderful idea once proposed by two researchers at the University of Central Lancashire of transporting Pacific grey whales to the Atlantic; and the less crazy but equally wonderful idea of reintroducing the Dalmatian pelican – a species that was native to the UK until the middle ages. Both species play a crucial role in marine food webs, and can fill our lives with wonder. Recharging nature recharges the human spirit. In 2020, we could all do with some of that. • George Monbiot is a Guardian columnist"
"Prince William has announced what was described as “the most prestigious environment prize in history” to encourage new solutions to tackling the climate crisis. The “Earthshot prize” will be awarded to five people every year over the next decade, the Prince said on Tuesday, and aims to provide at least 50 answers to some of the greatest problems facing the planet by 2030.  They include promoting new ways of addressing issues such as energy, nature and biodiversity, the oceans, air pollution and fresh water. The prize, inspired by US president John F Kennedy’s ambitious “Moonshot” lunar programme and backed by Sir David Attenborough, promises “a significant financial award”, a statement said. The Duke of Cambridge, a grandson of the Queen and second in line to the throne, said the Earth was “at a tipping point” and faced a “stark choice”. “Either we continue as we are and irreparably damage our planet or we remember our unique power as human beings and our continual ability to lead, innovate and problem-solve,” he said. “Remember the awe-inspiring civilisations that we have built, the life-saving technology we have created, the fact that we have put a man on the moon. People can achieve great things. “The next 10 years present us with one of our greatest tests – a decade of action to repair the Earth,” he said. ""The earth is at a tipping point and we face a stark choice: either we continue as we are and irreparably damage our planet or we remember our unique power as human beings and our continual ability to lead, innovate and problem-solve."" — The Duke of Cambridge @EarthshotPrize pic.twitter.com/SfGaKY9qsG The award, which will be launched later in the year and bestowed from 2021, is open to individuals as well as communities and businesses. It has the support of conservation campaigners, groups and scientists, including the veteran British natural history broadcaster Attenborough. “The spirit of the Moonshot can guide us today as we confront the serious challenges we face on Earth,” Attenborough said in a film to mark the launch. “This year Prince William and a global alliance launch the most prestigious environment prize in history ... designed to motivate and inspire a new generation of thinkers, leaders and dreamers to think differently.” Both Prince Charles and Prince Philip have campaigned for environmental causes and against the illegal trade in wildlife around the world. William is hoping to build on their work through the prize, which will initially be run by his and his wife’s own charitable foundation."
"
Share this...FacebookTwitter
Source: American Geophysical Union Fall Meeting
At next month’s American Geophysical Union (AGU) Fall Meeting in New Orleans (US), an independent researcher named Trevor Underwood will be presenting an equation-rich analysis that thoughtfully undermines the perspective that increases in CO2 concentrations are a fundamental variable affecting climate.
Instead, Underwood argues that the absorption band where CO2 emissivity could have an effect is likely already saturated, precluding the capacity of increased CO2 concentrations to produce atmospheric warming.
He also advances the position that solar irradiance changes can explain modern temperature variations, which is consistent with other recent analyses.
It seems that more and more of these papers questioning the “consensus” view on the efficacy of the CO2 within the greenhouse effect are being considered in scientific circles.  Several previous examples are listed below.
The volume of contrarian analyses would seem to suggest that the climate’s specific sensitivity to CO2 concentration changes is not yet settled.
And so the debate rages on.

•   Another New Paper Dismantles The CO2 Greenhouse Effect ‘Thought Experiment’
•   New Paper: CO2 Has ‘Negligible’ Influence On Earth’s Temperature
•   3 Chemists Conclude CO2 Greenhouse Effect Is ‘Unreal’, Violates Laws Of Physics, Thermodynamics
•   Ph.D. Physicist Uses Empirical Data To Assert CO2 Greenhouse Theory A ‘Phantasm’ To Be ‘Neglected’
•   Swiss Physicist Concludes IPCC Assumptions ‘Violate Reality’…CO2 A ‘Very Weak Greenhouse Gas’
•   Recent CO2 Climate Sensitivity Estimates Continue Trending Towards Zero
•   A Swelling Volume Of Scientific Papers Now Forecasting Global Cooling In The Coming Decades
•   Russian Scientists Dismiss CO2 Forcing, Predict Decades Of Cooling, Connect Cosmic Ray Flux To Climate
•   2 New Papers: Models ‘Severely Flawed’, Temp Changes Largely Natural, CO2 Influence ‘Half’ Of IPCC Claims
•   Leading Heat Transfer Physicists/Geologists Assert The Impact Of CO2 Emissions On Climate Is ‘Negligible’
•   New Atmospheric Sciences Textbook: Climate Sensitivity Just 0.4°C For CO2 Doubling
•  U of Canberra Expert: Doubling Atmospheric CO2 Would Increase ‘Heating By Less Than 0.01°C’
•   Uncertainties, Errors In Radiative Forcing Estimates 10 – 100 Times Larger Than Entire Radiative Effect Of Increasing CO2
•   New Paper Documents Imperceptible CO2 Influence On The Greenhouse Effect Since 1992



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Underwood, 2017
No Increase in Earth’s Surface Temperature From Increase in Carbon Dioxide 
A critical look at these different in situ measures of the Earth’s surface temperature identified a divergence between land and marine surface temperatures, with land surface air temperatures showing a significant and increasing rate of warming of around 0.5°C between 1880 and 1981, and 0.7°C between 1982 and 2010, whilst marine air temperatures show little if any change between 1880 and 2010 (Underwood (1) 2017). Recent academic literature is also beginning to question the accuracy of the adjusted in situ data (Kent et al. 2017).
In order for an increase in carbon dioxide or other greenhouse gas concentration in the atmosphere to result in an increase in the surface temperature of the Earth, it must be able to increase the absorption of infrared radiation emitted from the surface. This would result in an increase in the absorption factor, f. However, as seen above f is currently around 0.9444. Absorption of infrared radiation by molecules of greenhouse gases, involves increasing the internal energy of the molecule by changing the quantum state of the molecules, which can only occur at particular wavelengths, known as absorption bands. 
These absorption bands can be extended by what is referred to as pressure broadening (Strong and Plass 1950; Kaplan 1952), but when all of the emitted infrared radiation within these absorption bands has been absorbed by greenhouse gas molecules in the atmosphere, no further absorption of the terrestrial radiation is possible. The radiation with wavelengths falling outside of the absorption bands passes through the atmosphere and escapes into space.
Absorption of solar radiation in in the stratosphere is almost 100% efficient in the ultraviolet due to electronic transitions of oxygen (O2) and ozone (O3) and a significant amount of solar radiation is absorbed by water vapor (H2O) in the lower atmosphere. It is primarily the visible radiation that is absorbed at the Earth’s surface. In the infrared, absorption is again almost 100% efficient because of the greenhouse gases, but there is a window between 8 and 13 mm, near the peak of terrestrial emission, where the atmosphere is only a weak absorber except for a strong ozone feature at 9.6 mm. This atmospheric window allows direct escape of radiation from the surface of the Earth to space and is of importance in determining the temperature of the Earth’s surface (Jacob 1999).
Additional leakage could occur if the greenhouse gas concentration in the atmosphere were insufficient to absorb all of the infrared radiation in the absorption bands emitted by the Earth’s surface, but due to the extent of the atmosphere and its known unsaturated state, it is more likely that the current leakage corresponds to radiation in the part of the infrared spectrum that does not fall in the greenhouse gas absorption and emission bands, referred to as the “infrared window”. As a consequence, even in the case where there is leakage of infrared radiation from the Earth’s surface directly into space, as long as the atmosphere is able to absorb all of the upwelling infrared radiation in the greenhouse gas absorption bands, neither the amount of this leakage nor the amount of the absorption will depend on concentration of greenhouse gases in the atmosphere. From the emission spectra (a) and absorption percentages (b) in the diagram above (Fig. 2.6, Yang 2016), where the 255°K blackbody curve represents the terrestrial radiation, it appears that at the current surface temperature and absorption factor of 0.9444 all of the radiation within the emission bands is fully absorbed, and that the remaining 5.56 percent of the infrared emission represents radiation with wavelengths within the atmospheric window. If this is true, there can be no further increase in f [absorption], and no increase in the surface temperature with an increase in carbon dioxide.
Increase In Solar Forcing Explains Recent Warming
The difference between the minima showed an increase of 0.2812 W/m-2 for VIRGO; 0.4701 W/m-2 for ACRIM; and 0.2650 W/m-2 for ACRIM + TIM over the 11.6 year solar cycle 23 beginning in May 1996 and ending in January 2008; or 0.24 W/m-2 per decade for VIRGO, 0.40 W/m-2 per decade for ACRIM, and 0.23 W/m-2 per decade for ACRIM + TIM (pmodwrc website
 2016).
These decadal increases in TSI [Total Solar Irradiance] from ACRIM, SARR, VIRGO and ACRIM + TIM are sufficient to explain the whole of the increase in surface temperature estimated from in situ data during the last 100 years. They compare with the six published model-based estimates of forcing examined in Schwartz (2012) that showed forcing by incremental greenhouse gases and aerosols over the twentieth century ranging between 0.11 and 0.21 W/m-2 per decade.
Summary
Solution of the Greenhouse Effect equations based on a more realistic atmospheric model that includes absorption of solar radiation by the atmosphere, thermals and evaporation, and an examination of the fraction of terrestrial infrared radiation absorbed whilst passing through the atmosphere, suggests that the contribution of greenhouse gases to the surface temperature is close to its upper limit. Any further contribution would depend on an increase in the infrared absorption factor of the atmosphere from its current level of around 0.9444, which seems unlikely. As this appears to correspond to total absorption of all black body infrared emission from the Earth’s surface at wavelengths at which there are greenhouse gas absorption bands, including for water vapor, it seems likely that we are close to the thermodynamic limit of greenhouse warming for the current luminosity of the sun, and that any further increase in carbon dioxide concentration in the atmosphere will have little or no effect on the surface temperature of the Earth. Questions about the reliability of in situ measurements of surface temperatures also raise questions about current estimates of global warming. Moreover, recent evidence from satellite measurements of solar irradiance, indicate that any recent warming could be due to increasing solar irradiance.

A conference paper with a similar conclusion regarding the emissive/warming limitations of increased CO2 concentrations was presented by a molecular physicist, Dr. N. Doustimotlagh, at the World Conference On Climate Change in October, 2016.



Doustimotlagh and Mirzaee, 2016


So because of the limited values of electromagnetic waves that come from Earth and limitation of  absorption of greenhouse gasses, the greenhouse effect of greenhouse gasses should be limited.  In other words, after absorbing of all the IR waves that come from Earth by greenhouse gasses in the atmosphere, there are no IR waves to cause greenhouse effect. It means that “two things that cause greenhouse effect are greenhouse gasses and IR waves in absorption spectrum of these gasses, so if greenhouse gasses increases but there are no IR waves, it is natural that there is no greenhouse effect”.

If the concentration of CO2 in atmosphere increases until absorbs all the values of  electromagnetic waves that are absorbable for CO2, additional values of CO2 should not have greenhouse effect.



Share this...FacebookTwitter "
"
Share this...FacebookTwitterDr. Sebastian Lüning and Prof. Fritz Vahrenholt show that sea level rise at the Fiji Islands is being hyped up in order to generate money.
====================================================
Fijigate
Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
(Translated/edited by P. Gosselin)
The COP23 climate conference in Bonn had originally been planned to take place in the Fiji Islands. But in order to comfortably accommodate the approximately 25,000 representatives(!) from every country in the world, it was decided to hold it in Bonn.
It was reported in Spiegel about how the islands are becoming victims. At the start of the article author Axel Bojanowski referred to the rise in sea level and linked to an NOAA-website. But later throughout the rest of the article there was no mention of climate change submerging the islands.
Bojanowski was completely correct to emphasize that the most important reasons for the erosion of the islands is solely the fault of the island inhabitants. The uncontrolled deforestation reduces stability and resistance to the sea. Even persons who sail in the area report that there are 3-meter waves even in the absence storms in the region.
But let’s get back to sea level rise in the area. At the NOAA website there is also the possibility to download the data. And that is what we did.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Figure 1: Sea level rise at the Fiji Islands, 1990-2011. Data: NOAA. The linear trend is 6 mm per year.
The available NOAA data go back (with some gaps) to 1972, but the station was moved in March 1989 and this led to an upwards jump of about 10 cm. Thus we look only at the period from 1990 to the end of 2011, where unfortunately the data series ends. However we supplement the data for the area from satellites (see here:

Figure 2: Sea Surface Height (SSH at the Fiji Islands from satellite measurement. Source here.
After a peak in 2012 the level went down by about 10 cm by mid 2017. It is very much related to natural variations, in sync with the El Ninos (low levels) and La Ninas (high levels).
So what remains of the climate change horror stories in connection to the Fiji Islands? In the article, a 40-year old woman tells about her youth (i.e. around 1990), when she viewed the water as her friend and how today (2017) she regards it as an enemy. But just what should an approximately 8 cm rise (and not the often cited 17 cm that was generated by the powerful 2011/12 La Nina) lead to in 27 years with waves of 3 meters?
The contribution to erosion coming from climate change is certainly hardly noticeable by the residents. However for PR work, it works great for shaking down money.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterAt this point last year global warming alarmists and global socialism politicians were as giddy and as optimistic as ever. Everything was falling into place as it looked as if nothing would prevent them from imposing their green regime. The Pope was on their side, global temperatures had been near record highs (thanks to an El Nino event) , and Hillary Clinton would surely go on to become President of the USA.
Warmist agenda now getting torpedoed
With Clinton at the helm, the US would wholeheartedly commit to Paris and to strict decarbonization. Never did the green dream look so promising. But then came the mother of torpedoes, President Donald Trump.
And now there’s yet another torpedo about to slam into the already badly damaged warmunista ship: a rapidly approaching La Nina. In the wake of last year’s El Nino event, global temperatures had already been falling. A La Nina will only cause the globe to cool further. This is surprising because just months ago experts had predicted El Nino conditions to return.
La Nina powers in
The global warming alarmists are in sheer desperation and panic, as made evident by their hysterically shrill reactions to the recent hurricanes. The latest forecast shows a return to La Nina conditions (and a global cool-down).
Source: http://www.cpc.ncep.noaa.gov/shtml
The above chart shows La Nina conditions expected to persist into spring, 2018. This cooling will make itsself evident in satellite data with a lag of about 6 months. This means global temperature will fall even further next year, which means the warming pause will go beyond 20 years.
Note the intensifying La Nina conditions forecast for the end of the here in the following NCEP chart for the rest of the year:

 
This oncoming La Nina development led meteorologist Dr. Ryan Maue to comment on Twitter:

La Niña means extreme winter weather — colder global temperatures — and all sorts of interesting things.  Are you prepared for it?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




— Ryan Maue (@RyanMaue) September 14, 2017

Not only La Nina is serving to cool global surface temperatures, but so are the powerful hurricanes as well. Yesterday at the Weatherbell Daily Update, Joe Bastardi showed the effects of hurricanes Irma and Jose on sea surface temperature (SST).

Note the band of cool water through the Caribbean and a substantially cooled down Gulf of Mexico. Just a week ago reports abounded on how the waters there surface had been “bathwater warm”. So quickly can weather change. True, there remains considerable amounts of heat at the ocean surfaces.
Frigid winter projected for Europe
The recent winter projection for Europe issued by Meteociel below shows Europe possiby being gripped by a frigid winter. If the prognosis holds, it could be one of the coldest in years:

Meteociel/CFS prognosis from 30 August 2017,  850 hPa temperature deviation from the mean (about 1500m) in Europe for the 2017/18 winter. For Europe very icy conditions are expected (from left to right: December, January, February). Source: www.meteociel.fr/php
Arctic sea ice rebound
Also the Arctic has shown recovery over the past years. This year’s Arctic sea ice for mid September is about a full 1 million sq. km. over the record low set 5 years ago.

Overall Arctic sea ice has remained stable for the past 10 years, surprising global warming scientists. Source: National Snow and Ice Data center (NSIDC).
 
Share this...FacebookTwitter "
"For nearly 100 years, Mount Everest has been a source of fascination for explorers and researchers alike. While the former have been determined to conquer “goddess mother of the world” – as it is known in Tibet – the latter have worked to uncover the secrets that lie beneath its surface. Our research team is no different. We are the first group trying to develop understanding of the glaciers on the flanks of Everest by drilling deep into their interior. We are particularly interested in Khumbu Glacier, the highest glacier in the world and one of the largest in the region. Its source is the Western Cwm of Mount Everest, and the glacier flows down the mountain’s southern flanks, from an elevation of around 7,000 metres down to 4,900 metres above sea level at its terminus (the “end”). Though we know a lot about its surface, at present we know just about nothing about the inside of Khumbu. Nothing is known about the temperature of the ice deeper than around 20 metres beneath the surface, for example, nor about how the ice moves (“deforms”) at depth. Khumbu is covered with a debris layer (which varies in thickness by up to four metres) that affects how the surface melts, and produces a complex topography hosting large ponds and steep ice cliffs. Satellite observations have helped us to understand the surface of high-elevation debris-covered glaciers like Khumbu, but the difficult terrain makes it very hard to investigate anything below that surface. Yet this is where the processes of glacier movement originate. Scientists have done plenty of ice drilling in the past, notably into the Antarctic and Greenland ice sheets. However this is a very different kind of investigation. The glaciers of the Himalayas and Andes are physically distinctive, and supply water to millions of people. It is important to learn from Greenland and Antarctica, – where we are finding out how melting ice sheets will contribute to rising sea levels, for example – but there we are answering different questions that relate to things such as rapid ice motion and the disintegration of floating ice shelves. With the glaciers we are still working on obtaining fairly basic information which has the capacity to make substantial improvements to model accuracy, and our understanding of how these glaciers are being, and will be, affected by climate change. So how does one break into a glacier? To drill a hole into rock you break it up mechanically. But because ice has a far lower melting point, it is possible to melt boreholes through it. To do this, we use hot, pressurised water.  Conveniently, there is a pre-existing assembly to supply hot water under pressure – in car washes. We’ve been using these for over two decades now to drill into ice, but our latest collaboration with manufacturer Kärcher – which we are now testing at Khumbu – involves a few minor alterations to enable sufficient hot water to be pressurised for drilling higher (up to 6,000 metres above sea level is envisioned) and possibly deeper than before. Indeed, we are very pleased to reveal that our recent fieldwork at Khumbu has resulted in a borehole being drilled to a depth of about 190 metres below the surface. Even without installing experiments, just drilling the borehole tells us something about the glacier. For example, if the water jet progresses smoothly to its base then we know the ice is uniform and largely debris-free. If drilling is interrupted, then we have hit an obstacle – likely rocks being transported within the ice. In 2017, we hit a layer like this some 12 times at one particular location and eventually had to give up drilling at that site. Yet this spatially-extensive blockage usefully revealed that the site was carrying a thick layer of debris deep within the ice. Once the hole has been opened up, we take a video image – using an optical televiewer adapted from oil industry use by Robertson Geologging – of its interior to investigate the glacier’s internal structure. We then install various probes that provide data for several months to years. These include ice temperature, internal deformation, water presence measurements, and ice-bed contact pressure. All of this information is crucial to determine and model how these kinds of glaciers move and melt. Recent studies have found that the melt rate and water contribution of high-elevation glaciers are currently increasing, because atmospheric warming is even stronger in mountain regions. However, a threshold will be reached where there is too little glacial mass remaining, and the glacial contribution to rivers will decrease rapidly – possibly within the next few decades for a large number of glaciers. This is particularly significant in the Himalayas because meltwater from glaciers such as Khumbu contributes to rivers such as the Brahmaputra and the Ganges, which provide water to billions of people in the foothills of the Himalaya. Once we have all the temperature and tilt data, we will be able to tell how fast, and the processes by which, the glacier is moving. Then we can feed this information into state-of-the-art computer models of glacier behaviour to predict more accurately how these societally critical glaciers will respond as air temperatures continue to rise.  This is a big and difficult issue to address and it will take time. Even once drilled and imaged, our borehole experiments take several months to settle and run. However, we are confident that these data, when available, will change how the world sees its highest glacier."
"In the dying minutes of winter’s shortest days, something magical happens to the trees that cling to the steep slope on the eastern edge of Tunstall reservoir. Just before the setting sun dips behind Wolsingham Park Moor, the water surface becomes a mirror that bounces glancing sunbeams into the tree canopy. Seen from across the reservoir, trunks and branches are bathed in a golden glow, every twig etched with startling clarity against the gathering dusk within the wood. This afternoon, as I walked among those trees, the lighting was pure theatre. My vision struggled to accommodate its extremes, of dazzling white trunks of silver birch, rough-textured grey bark of ancient oaks, vivid green mosses and deep black shadows.  A nuthatch appeared as a fleeting monochrome silhouette, hanging on the shadowy underside of a branch, dagger beak wheedling out something from a fissure. It vanished behind the trunk, then reappeared in bright sunlight, in full colour, unmistakable in its bandit-mask black eye-stripe, slate-blue back, and apricot chest feathers. Just another nuthatch. But when I first came to live hereabouts, more than 40 years ago, these were rare birds in County Durham. Reading through my natural history notebooks from the 1970s I see that they merited a specific mention on the few occasions when I saw one. Now they are common. Their relentless northerly advance is often attributed to milder winters, brought about by climate change, though it’s far from clear exactly what factors have changed in their favour. It would now be noteworthy to walk in deciduous woodlands in Weardale and not see – or, more often, hear – these birds; they have a piercing call, especially in late winter and early spring when courtship begins. Personal nature notebooks, often ledgers of profit and loss, can be emotive documents, with the growing awareness of shifting baseline syndrome, where succeeding generations are denied the pleasure of seeing once-commonplace species that disappear during their parents’ lifetime. In the 1970s I recorded red squirrels here; I would now need to travel much further afield to see one. But in the case of the nuthatch, in this precious fragment of ancient woodland, the syndrome seems to have worked in reverse."
"
Share this...FacebookTwitterIn the wake of a fall storm ‘Xavier’ that struck Germany and claimed 7 lives, one of Germany’s most popular TV Talkshows, Maischberger 1 on ARD German public television, recently featured climate change in discussion round bearing the title: “Xavier and the weather extremes: has our climate reached the tipping point?”

Cologne and Berlin under water! Backdrop on set for ARD German television discussion round on climate change. Image cropped from Maischberger 1 here.
The discussion round included, among others, Prof Hans-Joachim Schellnhuber, high profile Swiss meteorologist Jörg Kachelmann, who by the way is a warmist, and Swiss science journalist Alex Reichmuth.
Not surprisingly, talk show moderator Sandra Maischberger introduced the show with dramatic scenes of a climate in collapse, and then asked the round if this if the recent storms Xavier and now Orphelia are unprecedented. So dramatic in fact were the images of Maischberger’s intro that even German daily Die Welt here commented that “ARD had allowed itself to be inspired a bit by Hollywood“.
When asked by Maischberger about the recent storms, Kachelmann immediately dumped cold water on the notion that they were unusual, noting that storm Xavier seemed worse because it hit in October when trees are still fraught with foliage and thus cause far more wind resistance and cause tress to fall more often. Overall, Xavier was just a normal fall storm, Kachelmann told the audience.
“Storms don’t come with a label”
When asked if this year’s heavy rains were due to global warming, Kachelmann responded:
“We don’t know case by case because all the thunderstorms and storms don’t come with a label stating: ‘I’m here only because of you, or your actions, to say it correctly’. The problem is that we don’t know.”
No detectable increase in storm frequency
Kachelmann went on to explain that experts evaluated the data from the German Weather Service and concluded there has been “no increase in frequency in these events“. He added:
Also with tropical storms, looking at it globally, from the data of the American weather agencies, up to now we see no increase in frequency.”
A few seconds later he responded to Maischberger’s inquiry about the “monster hurricanes” hitting the Caribbean and USA:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Yes, this is an active season. But it is not a record season. It is not anything that has not happened before. When we look back at the last 50 or 60 years, we see no trend.”
During the course of the Talkshow, moderator Maischberger globetrotted across the entire planet, it seemed, going from one weather disaster to the next. She pressed Kachelmann about the fires in northern California. Here too the Swiss veteran meteorologist dismissed the notion that it’s unusual, reminding the audience that California is a dry state and these things have always happened before:
When you look at the archives, we see no increase in frequency.”
Kachelmann also reminded that anyone can make a list of 100 disasters in any year.
Like Jehovah Witness
At about the 16-minute mark, Maischberger turned to Swiss journalist Alex Reichmuth, who like Schellnhuber studied physics and mathematics. Journalist Reichmuth, however, is far more critical of climate change,and told the audience that climate science is more a religion than science and that it all reminded him of the Jehovah Witness sect.
This is about a religious conversion – ride your bicycle more and we’ll be redeemed.”
Reichmuth reminded the audience that even the IPCC stated that there is no clear trend regarding weather extremes.
From 97% to 99%?
Reichmuth then slammed Schellnhuber for his outlandish predictions of the future, telling the “renowned” Potsdam professor that he “has clearly deviated from the scientific approach“. Just a minute earlier Schellnhuber had seemed to claim that 99% of the scientists agree with him.
Surprisingly guest Dorothee Bär of the conservative CSU party said she doubted that man was all responsible for the 1°C warming of the past century and that economy and well-being of the citizen had to be placed at the forefront of any energy policy. But later in the show the CSU politician hopped on the politically correct “we have to do something” facade – as did Kachelmann.
At the 34-minute mark, the talk switched to Trump’s backing out of the Paris Accord and whether the fight against the climate would hurt the economy. Most of the discussion was filled Marxist-brand utopian platitudes with few in the round grasping the technical implications of green energies. For example, suddenly Schellnhuber poased as an expert and leading authority on transportation technology, agriculture and economics, and gave the impression storage systems are all ready to go!
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterGlobal Warming Theory ‘Completely 
Disconnected From the Observations’


Extensive analysis of temperature trends in the Arctic reveals that there has been no detectable long-term change since the beginning of the 20th century, and thus predictions of a sea ice-free Arctic in the coming decades due to dramatically rising temperatures are not rooted in observation.


Butina, 2015  
IS THE ARCTIC MELTING?  
THEORY VS. OBSERVATIONS
Abstract
[T]he Arctic Circle is the most extreme place on our planet where seasonal changes can range from +35.0°C in July and -65.0°C in February; […] on average 75% of the year is spent below the melting point of water [and] on average the Arctic will be covered by ice/snow for the same proportion of time, i.e., 75% or 9 months of the year.
The same seasonal extreme variations in air temperatures are also observed in ice cover variations observed in the Arctic where the winter‘s ice cover can be between 14- 16 million km2, while during summer the area covered can vary between 4 and 8 million km2.  Based on observations, dating back to 1900, it can be concluded that it is physically impossible for the Arctic to be ice/snow free in the foreseeable future since the air temperatures were as cold in 2013 as they were in 1900.
Since ice cannot melt below 0.0°C, all these observations point towards the Arctic remaining ice-covered for the next 100 years. It must also follow that any theory predicting imminent melting of the Arctic ice cap cannot be based on thermometer-recorded data and, therefore, must be wrong and will merely be an artefact of using the term temperature where there is no true association with the calibrated thermometer, the instrument used to measure temperature in all physical, medical and engineering sciences.
Conclusion
So, what are the hard facts about Arctic that are based on the observations made by calibrated thermometers at 20 stations across the Arctic Circle and which conclusions can be made based on those observations?
1. Temperatures in the Arctic between 1900 and the present day are a long distance below 0.0°C for at least 9 months per year and can be as low as -64.0°C
2. It is impossible to separate the youngest from the oldest years using thermometerbased daily or monthly Tmax/Tmin data
3. The total ranges observed in daily Tmax/Tmin data can be as high as 100.0°C and as low as 75.0°C making the Arctic Circle the most variable and extreme area on our planet therefore making any accurate forecasting of future temperature patterns and trends impossible
4. The switches between the extreme hot to extreme cold temperatures are very frequent and very unpredictable and can occur within the same month, same year or between two consecutive years
5. The large observed ice gain/loss variations are pre-determined by the large observed variations in air temperatures
6. Since the air temperatures are chaotic in nature it must follow that the extent of the ice cover has to be chaotic as well and, since we cannot predict future events of a chaotic system, we cannot predict future trends of either air temperatures or ice cover patterns
Based on the facts above only one conclusion can be made in reference to the putative melting of the Arctic: historical thermometer-based data tells us that between 1900 and 2014 arctic temperatures were for 75% of the time consistently long distance below 0.0°C; the ice cover in the winter months is still consistently more than 14,000,000km2 and, therefore, it is physically impossible for the Arctic to be already melting since nothing has changed since 1900 till present day. The only sensible forecast for the future would be to expect the same extreme events to continue until thermometer-based evidence tell us otherwise.
Let me conclude this paper by answering the question asked in the first part of the title by a categorical No, the Arctic is not melting. As long as temperatures remain the same as they have been for the last 100 years the Arctic will remain frozen in the long winter months and partly melt during very short summer months.
The answer to the second question is that the theory of global warming is completely disconnected from the observations since their definition of temperature is based on some theoretical number that has nothing to do with the temperature that is measured by calibrated thermometer and, most importantly, used as an international standard by the scientific community. Since the theory is clearly wrong about forecasting the temperature patterns in the Arctic, all other predictions made by the theory must be wrong too.


New Paper: No  Greenland Temperature Or Sea Ice Changes Since 1600 Either

Kryk et al., 2017     
“Our study aims to investigate the oceanographic changes in SW Greenland over the past four centuries (1600-2010) based on high-resolution diatom record using both, qualitative and quantitative methods.  July SST during last 400 years varied only slightly from a minimum of 2.9 to a maximum of 4.7 °C and total average of 4°C. 4°C is a typical surface water temperature in SW Greenland during summer. … The average April SIC was low (c. 13%) [during the 20th century], however a strong peak of 56.5% was recorded at 1965. This peak was accompanied by a clear drop in salinity (33.2 PSU).”


Greenland Ice Sheet In Balance…Melt Added  Just 1.5 cm (0.6  Inch) To Sea Levels Since 1900

Fettweis et al ., 2017
“[T]he integrated contribution of the GrIS [Greenland Ice Sheet] SMB [surface mass balance] anomalies over 1900–2010 is a sea level rise of about 15 ± 5 mm [1.5 cm], with a null contribution from the 1940s to the 2000s“

Like The Arctic, Antarctica Has Not Warmed In The Last Century Either

Stenni et al., 2017
“[N]o continent-scale warming of Antarctic temperature is evident in the last century.”

Antarctica’s Ice Sheet Has Been Gaining Mass Since 1800

Thomas et al., 2017
“Our results show that SMB [surface mass balance] for the total Antarctic Ice Sheet (including ice shelves) has increased at a rate of 7 ± 0.13 Gt decade−1 since 1800 AD…”

Antarctica’s Mass Gains Have Reduced Sea Levels By -0.04 mm-¹ Per Decade Since 1900

Thomas et al., 2017
“…representing a net reduction in sea level of ∼ 0.02 mm decade−1 since 1800 and ∼ 0.04 mm decade−1 since 1900 AD.  The largest contribution is from the Antarctic Peninsula (∼ 75 %) where the annual average SMB during the most recent decade (2001–2010) is 123 ± 44 Gt yr−1 higher than the annual average during the first decade of the 19th century.”

In sum, there is nothing thermally unusual occurring today in either the Arctic or Antarctic, precluding the clear detection of an anthropogenic temperature or ice-melt signal in the polar regions.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterIn an opinion piece at the Mittel Bayerische daily, Harry Neumann, National Chairman of the environmental group Naturschutzinitiative e.V. declares Germany’s Energiewende (transition to renewable energies) a failure and writes: “The wind power industry and nature protection cannot be reconciled.”
Moreover Germany’s EEG green energy feed-in act is doing more harm than good, writes Neumann: “The EEG is impeding the research of environmentally compatible technologies.”
Neumann also notes that despite having installed close to 30,000 wind turbines, Germany’s “CO2 emissions are not dropping, but rather are rising again.” He adds:

During the expansion of renewable energies, they failed from the start to set impact limits too protect nature, species, forests and landscapes.”

He also blasted what he calls the “political-industrial complex“, which he says has nothing to do with nature and climate protection, “but rather with the full exploitation of billions in subsidies“.
In Neumann’s view, the wind industry and nature protection “cannot be reconciled” and thus calls for the immediate repeal of the EEG feed-in act.
Veteran journalist: German energy policy fraught with absurdities


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




On another note, veteran German science journalist Michael Miersch explains in an interview conducted by Dr. Benny Peiser of the Global Warming Policy Foundation the sheer absurdity and widespread damage German renewable energies are having on the environment.

When asked about the current status and dialogue surrounding the Energiewende, Miersch tells Peiser:
I would like the debate to be less ideological and to be held with less moral rigor. Nowadays you cannot criticize the Energiewende without being put into a corner and being accused of not caring about global climate change.”
He cites the U.S. as a model of how to go about energy policy:
If you think about the US for example, they have achieved a lot in terms of CO2 reduction with gas power plants. There are very few gas power plants in Germany. They are building hardly any new ones.”
 
Share this...FacebookTwitter "
"At the end of a tumultuous decade for biodiversity, in which a report based on the most comprehensive study of life on Earth warned that “nature is declining globally at rates unprecedented in human history”, we spoke to some of the world’s leading voices on the environment about their greatest fears for the next decade – and also their hopes. As the IPBES report’s authors noted: “It is not too late to make a difference, but only if we start now at every level from local to global.” We asked three questions:  Chair of the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES) 1. The species I am most directly worried about is our own! Humanity has reached a point that has enabled us to inflict large-scale and lasting damage on our natural world – destroying ecosystems, driving species extinctions and even changing our global weather patterns. Food security, energy, health and livelihoods all depend on nature’s contribution to people.  2. Humanity never seems to miss the opportunity of missing an opportunity. My greatest regret of the last 10 years is how many chances we have collectively missed to make better choices and drive better policies. We have known enough about the risks of our damage to nature, yet the determination to act and to change seems to be constantly deferred to a later date. Before we know it, we will run out of time and the question will no longer be what we regret most, but rather how could we have been so foolish. 3. The conservation work I am most excited by goes beyond traditional conceptions of conservation – acknowledging that the nature crisis must be understood as one that is also a social, ethical, economic, health and security crisis. This is the basis for the next work programme of the IPBES (until 2030). It has potential to finally tackle the root causes of the loss of nature – values and behaviours that drive the destruction of biodiversity and nature’s contributions to people. 17-year-old British Bangladeshi activist 1. The species I am most worried about is the critically endangered spoon-billed sandpiper (SBS), which breeds in the Arctic Russian tundra, migrates 8,000 km along the East Asian-Australasian Flyway, with a significant percentage wintering on intertidal mudflats on Sonadia Island in southern Bangladesh. My mother’s family are Bangladeshi and so I feel a huge connection to this tiny, captivating bird, with its minute spoon-shaped bill. In 2010 the population was down to 200 birds, the combined weight of which is less than a single mute swan. The SBS is still a long way from being out of danger, despite a captive breeding programme. With many current threats, from mudflat reclamation in China to a sea-level rise wiping out their wintering areas, the survival of this species is still on a knife-edge. 2. Ten years ago, we understood more than ever before about the habitats across the world, the life they support and their importance to our planet. This gave us a huge opportunity to conserve the rarest habitats by working with indigenous peoples who understand them best. However, these were opportunities either squandered or exploited by the large global conservation organisations, such as the WWF and Wildlife Conservation Society, who have instead continued to remove indigenous peoples from their land in Africa and Asia, leaving the land more vulnerable to commercial poaching. Poaching in Africa and Asia is now pandemic, with wildlife no longer being protected by indigenous peoples who would previously have been able to identify poachers before they killed, but are instead excluded from having an input in conserving their animals. 3. I see a change coming in the way that international conservation organisations work with local and indigenous peoples around the world to save habitats, species and retain biodiversity. The decolonisation of conservation is happening, with a final realisation that indigenous peoples who have been successfully managing their land for thousands of years will be able to stop the mass commercial poaching and hunting taking place in national parks. I have seen examples of indigenous peoples in Africa, South America, and Asia using conservation to support their heroic efforts to save rainforests and species like orangutan in Borneo and yellow-headed picathartes in Ghana. I have recently become a global ambassador for Survival International, which fights for the human rights of indigenous peoples. I am very hopeful about seeing this new type of conservation work rise and dominate over the next decade. I hope that all of this will finally stop the assumption that white, ‘educated’ people understand and can look after lands, habitats, and wildlife better. I intend to be at the forefront of that campaign, which is the international side of the coin to my campaigning so far which has been making the sector ethnically diverse in the UK, rather than 99.4% white as it is now. Naturalist and television presenter1. I am worried about those species that are highly specialised in terms of their habitats. The more specialised you are as an animal, requiring unique resources, diet and habitat to survive, the greater effect any rapidly changing circumstances will have. It doesn’t just have to be animals either, it also includes things like chalk streams and sandy lowland heath – in the UK we have more than anywhere else in the world and this habitat is rarer than tropical rainforests. Those things that are least resilient and least common need to be our focus. 2. Activism is growing now and we are seeing positive results with Extinction Rebellion and School Strikes for Climate having a big impact. But it has been dormant for some time and that’s disappointing. Second, our NGOs aren’t showing their muscle. Many are very broadly supported and have an enormous legacy of credibility: the RSPCA, the RSPB and Wildlife Trust for example have vast public support but they’ve not been wielding that to best benefit and that’s been disappointing. And lastly we have an enormous armoury of technologies, capabilities and tried-and-tested methods to implement practical conservation with almost immediate success but we haven’t been doing enough of it. 3. I’m interested in new techniques and also divesting our trust into younger ecologists and conservationists who are brave and take risks and who know that there are enormous gains to be had if we get off the fence and start actually calling some shots. I’m looking for a new attitude in conservation. In terms of the practicalities: rewilding. Through this there are opportunities for carbon capture and planting trees, landscape resilience in terms of preventing flooding, and biodiversity generating a more natural mosaic and ecosystems which would be good. Primatologist and founder of the Jane Goodall Institute 1. While governments delay or ignore their commitment to reducing emissions it is vitally important to protect and restore our forests. We are destroying these precious forests at a terrifying rate, thus releasing stored CO2 from the trees and the forest soils into the atmosphere. The forest habitat is home to many of the most endangered specieson the planet so protecting and restoring forests is an imperative for us all. 2. We have missed so many opportunities – introducing environmental education into schools around the world, doing more to alleviate poverty (the really poor destroy the environment as they try to make a living, buy the cheapest food to survive). But perhaps the biggest opportunity missed is the lack of government subsidies for innovative technologies that will help us to live in better harmony with the natural world. Instead of subsidising clean green energy – solar, wind and tide – it is the big oil and gas companies, with their billions of dollars, that are receiving the support and tax breaks that enable them to continue to pollute and destroy. 3. I am really excited about some of the technology that is already making a huge difference, for example radio collars and microchips that enable scientists to accurately map animal movements. Also drones that can help to map habitats and plot out the range of individual animals, and a newly developing field of thermal cameras on drones that can detect wildlife under forest cover … helping rangers to better protect wildlife from poachers. Director of Survival International 1. I am most worried about human beings, and especially those who live most differently to “ourselves”. Many, especially non-European, peoples hunt, herd or grow their own food and are, at least in part, self-sufficient. We can’t all live like this, but we can learn from them. If the keys to real change lie anywhere, it lies with them. 2. The greatest missed opportunity has been the complete failure of the billion-dollar conservation industry to do anything more than pay lip service to tribal, indigenous and local peoples, whilst at the same time continuing to steal their lands and destroy them, and failing to “conserve” nature as well as they had been!  3. My hope is for the exposure of the hypocrisy behind the conservation industry, which hopefully will force it to accept the rights of tribal, indigenous and local peoples to manage their own environments, as they have done for generations. This is a struggle between those who want to kick people off their land, and those who want and need to live sustainably on and from it. If the fundamentalist environmentalists win, it will lead to yet more pollution and destruction. Ecologist and writer 1. It’s hard to beat coral reefs for the prize of global catastrophic rapid decline of a habitat. But it’s not a contest, and globally, all habitat types are in declining health. Our footprint continues to expand. It’s been calculated that 70% of all birds on Earth are now poultry, about 60% of all non-human mammals are farmed cattle, pigs, sheep, and goats. If you think of all those deteriorating wild habitats as proxy for all plants and animals living in them, you come to one uncomfortable conclusion: the human species is no longer compatible with the rest of life on Earth. 2. We have proven ourselves quite capable of creating global problems but have not shown an ability to control or reverse those problems. As long as we must maintain a global human population growing at roughly the rate of a new Mexico annually (around 70 million), getting in front of problems of the environment and the extinction crisis is like running after a hot-air balloon. The biggest missed opportunity is our failure to have an intelligent and compassionate conversation about human population. 3. Because an expanding human population is the greatest driver of environmental degradation as well as a major driver of poverty and injustice, the greatest lever for conservation is women’s empowerment. The only thing that has worked to ease population is women making their own voluntary choices. On a different front we must transition rapidly away from fossil fuels and the greatest lever there is financial, divesting from fossil fuels, divesting from banks that finance fossil fuels, and investing in clean energy technologies. Natural England chair and author 1. I’ve spent much of my career working for the conservation and restoration of the tropical rainforests and I have been utterly mortified to see what has recently been happening, especially in Brazil this year, where vast fires have wiped out thousands of square kilometres of habitat. These ecosystems are vital not only for wildlife and human cultural heritage, but also in the battle against climate change and for water security. 2. In 2010 countries agreed an ambitious set of 10-year goals to conserve the Earth’s incredible wildlife riches. These Aichi Targets have been largely sidelined to the point where they will be missed. A meeting of the United Nations Convention on Biological Diversity will take place in China in 2020, where the global compass on this agenda will be reset for the coming decade. Countries really need to up their ambition in advance of that meeting, otherwise the mass extinction of species that is now gathering pace will soon turn into a tragedy of truly colossal proportions. 3. I am very excited about how in some countries the conservation agenda is beginning to shift, moving from the rather grim task of hanging on to the last remnants of wildlife-rich habitat and rare species, to increasingly being about large-scale nature recovery. This is happening here in England, and I hope and pray that during the coming few years we will be able to show the kind of leadership that is so desperately needed on the global stage, as we are doing on climate change. If we can begin to rebuild habitats and species here in our densely populated islands, then that will be a real beacon of hope, showing that it is possible to reverse historic trends, in the process setting a new path for the future. Now is the time to do this. Director of Strong Roots Congo 1. I am worried about all endemic species of animals and plants, but mostly about great apes; and especially the eastern lowland gorillas and mountain gorillas. Their numbers have dropped dramaticallyin the last two decades. 2. Separating human from nature has been the biggest missed opportunity! The contribution and role of local communities and indigenous peoples in sustainable conservation is well-documented worldwide. This was not taken into account when “modern conservation”, imported by colonialism, was imposed on mostindigenous lands. Driven mostly by big organisations, this type of conservation has invested enormous amounts of money, energy and time into conservation and the result is a high rate of species extinction and deforestation globally. We missed considering local communities and indigenous peoples in our conservation efforts. We missed an opportunity to understand the spiritual and cultural values of nature. 3. I am most excited about models that consider the rights of natural resources governance and the management of indigenous people! This is the only way we can reverse the damages we brought into “conservation” for lucrative benefits.  Environmental activist at Youth 4 Nature 1. I am honestly worried about all sorts of ecosystems, because man has driven them to the brink of collapse. But most definitely, I am most worried about forest habitats in the sub-Saharan Africa. We are losing our forests at a faster rate, through deforestation and land degradation.  2. The biggest missed opportunity of the last 10 years is not working with nature, indigenous frontline communities, and with youth. World leaders have wasted so many years talking and “negotiating” for their stomachs and unsustainable economic powers while ignoring the role of nature and indigenous communities in addressing climate change. They seem to be waking up now, but time is running out. 3. I am most excited about on-the-ground implementation of nature-based solutions, and especially land restoration across Africa.There is no sidelining of young people anymore. They need us, and we have real experience, not the boring conference reports and speeches. There is no 1.5°C without nature, and there is no 1.5°C without youth. Former IPBES chair 1. Coral reefs are incredibly susceptible to small changes in ocean temperature, and exacerbated by land-based pollution and ocean acidification.Other highly sensitive ecosystems include cloud forests and those at high latitudes. The species most at risk are endemic species who occupy small climatic zones and are unable to migrate fast enough to survive. 2. I would highlight three missed opportunities – failure to address climate change, which will become the greatest threat to biodiversity in the coming decades; failure to remove perverse agricultural subsidies that lead to loss of biodiversity; and failure to transform unsustainable agricultural production systems into sustainable agricultural systems using agro-ecological processes, coupled with reduced food waste and healthier diets. 3. We need to rethink protected areas, and a redesign of corridors that allow for migration under a changing climate. This should be accompanied by large-scale restoration projects. Sailor and founder of the Ellen MacArthur Foundation 1. All across the world, in habitats of every region, biodiversity is being destroyed. Not a single region of the planet is escaping the onslaught. Globally, a million animal and plant species are facing extinction. The driving force behind this mass extinction is our current linear economy. This “take-make-waste” system sees us remove ever more materials from the ground, make products from them that we mostly use for only a short period of time, and then throw them away as waste. Each element of this approach represents a direct threat to biodiversity. 2. The past 10 years have seen huge conservation efforts, including ocean clean-ups and protected zones – but, while necessary and laudable, they are not enough. The most effective strategy is one that prevents damage in the first place. In short, protecting biodiversity requires a fundamental shift in the way our economy works.  3. This vision is becoming reality. To secure long term economic development we need a system fit for the future, not one stuck in the past. What if our aim was not simply to do less harm, but to actively regenerate our natural world? We know what the solutions look like, and that the opportunities are out there. All we have to do is grasp them. Director of Science at the Royal Botanic Gardens, Kew 1. The overwhelming evidence makes me really, really concerned about the future of tropical rainforests. They are the most biologically diverse ecosystem on Earth, containing millions of species – many of them providing crucial benefits to us and playing essential roles in nature. Increased degradation of natural environments is not restricted to the Amazon: Madagascar lost some 366,000 hectares of forest in 2018, which is 4.3% of all its original rainforest. It is clear to see that this is insanely unsustainable. 2. To stop and revert this trend, we need radical changes across all segments of society. We should all stop and review our consumer choices – it could be by avoiding buying furniture made of non-certified wood, or avoiding products that contribute to deforestation, like meat and dairy produce fed with Brazilian soybeans. The media attention on the climate crisis has unfortunately overshadowed another huge, and arguably even more significant crisis: the loss of biodiversity. The problem with biodiversity loss is that if we lose a species to extinction, it is gone forever – and right now, currently one in five plants are threatened with extinction. 3. There are many great opportunities for tackling the climate and biodiversity crises at the same time, but we must get it right from the beginning. For instance, companies and governments have never been so keen to invest in carbon offsetting, in particular afforestation. But it is important we plant the right trees in the right place. If we do so, we can combine long-term carbon storage goals with habitat restoration, increasing opportunities for wildlife to re-establish and regain stable population sizes.The most important action, however, is to conserve what we already have whenever there’s a choice. Not destroying a native habitat is immensely better than trying to restore it afterwards. Indian activist, 15, who started a battle against plastic straws in 2018 1. The Aravalli biodiversity habitat is the only forest around Delhi and acts as its lungs. It is a haven for more than 900 species of terrestrial plants, 208 species of birds and at least 113 species of butterflies. The Aravallis offer a mosaic of micro habitats for a variety of species – from big mammals to small birds and even microbes. I am worried about many species like the Indian Tiger, great Indian bustard, one-horn rhinoceros, Indian vulture. 2. In India, we have missed the opportunity to implement extended producer responsibility at the time of opening up the economy in the late 1990s. We missed imbibing the fact that development should not be at the cost of the environment. These two factors have been missed not just in India but in all developing nations. 3. The River Cauvery is a forest-fed perennial river which is fast becoming a seasonal stream as 87% of tree cover has been removed in 50 years. “Cauvery Calling” is a campaign setting the standard for how India’s rivers can be revitalised. It will initiate the revitalisation of the Cauvery river and transform the lives of 84 million people. The project aims at helping farmers plant over 2.4bn trees through agro-forestry programmes. Marine conservationist at the University of York 1. Coral reefs are the richest, most vibrant and best loved of all ocean ecosystems. They provide habitat for countless species, from enormous whale sharks to vanishingly small snails, crabs and worms. On coral reefs, a quarter of all shallow-water marine species are crammed into an area of only one tenth of one percent of the surface of the ocean. But all is not well. Corals are incredibly fussy about temperatures, liking it hot but not too hot. As global warming has gripped the planet, there have been repeated mass die-offs of coral spanning the globe. There are dire but scientifically credible predictions of the near complete loss of coral if we stay on the current path to more than 2C of warming by the end of this century. 2. Ten years ago I participated in a meeting at the Royal Society of London to consider the future of coral reefs. What we concluded was that they were in critical trouble. Reefs had become the first habitat on the planet for which anthropogenic greenhouse gas emissions, then at 383 parts per million, had already overshot their safe zone (350 ppm). Not only would we have to bring down emissions to save them, we would have to recapture some of the carbon already emitted. We fed the findings of our meeting into the Copenhagen climate conference a year later. To little avail. We are still scrambling to mount an adequate response. 3. If we are to save reefs in any semblance of their present state we will have to give it everything we have. The world community is near achieving its goal of protecting 10% of the sea by 2020. But science tells us we need to protect at least 30% from extractive and damaging uses to safeguard wildlife and habitats. Coral rich countries around the world, like the UK and the Seychelles, have already committed to such protection. There is a good chance this target will soon be adopted across the planet, helping keep coral reefs on life support while fossil fuels are phased out. Sued the Indian government over climate crisis inaction 1. The model development governments have taken up is out of balance. Mass tree-felling is common to make way for dams, infrastructure and road projects, leading to a big fall in the species count throughout India due to habitat loss.  2. Kedarnath flash floods, Kerala floods, Bihar floods, Assam floods and many such disasters have taken hundreds of lives and continue to do so every year, but as a country we haven’t learned from these disasters. The government itself is saying that the spring water channels are drying up, the rivers are getting contaminated every day, the piles of solid waste are growing like a mountain in every corner of the country, biodiversity and habitat are being lost and still there no plans to mitigatethe situation. 3. The ongoing efforts to clean and rejuvenate the Ganges River and its tributaries have excited me the most."
"Greta Thunberg has said she wouldn’t have wasted her time talking to Donald Trump about climate change at the UN climate change summit in New York earlier this year – the same event she was pictured glaring at the one of the world’s leading climate-change deniers.  The Swedish climate activist made the comment during an interview on BBC Radio 4 on Monday morning, where she had been invited to guest-edit the programme. Thunberg, 16, was asked what she would have said to the leader who pulled the US – one of the world’s leading carbon emitters – out of the Paris climate accord, and who has taken radical steps to undo decades-old US pollution standards. She said: “Honestly, I don’t think I would have said anything. Because obviously he’s not listening to scientists and experts, so why would he listen to me?” She added: “So I probably wouldn’t have said anything, I wouldn’t have wasted my time.” Thunberg’s comments came several weeks after Trump attacked her for being named Time magazine’s person of the year. “So ridiculous. Greta must work on her Anger Management problem, then go to a good old fashioned movie with a friend! Chill Greta, Chill!” Trump tweeted at the time. She has also been attacked by Brazil’s far-right president Jair Bolsonaro. “It is staggering, the amount of coverage the press gives that brat,” Bolsonaro said at the time. Invited to respond to her critics, Thunberg told the program “those attacks are just funny because they obviously don’t mean anything”. She said: “I guess of course it means something – they are terrified of young people bringing change which they don’t want – but that is just proof that we are actually doing something and that they see us as some kind of threat.”"
"
Share this...FacebookTwitterElectric Vehicle Emissions 27-50% Greater
Than Internal Combustion Engine Vehicles

Image: Qiao et al., 2017
Sales of electric vehicles (EV) in China have exploded in recent years.
According to the New York Times (October, 2017), between 2014 and 2017, annual EV purchases by China’s citizens more than doubled, from 145,000 in 2014 to 295,000 (projected) for 2017.   By 2019, the annual sales of EVs are expected to swell to 814,000 for China alone, which will eclipse the expected EV sales for the rest of the world combined (602,000).
Good news for the climate, right?  After all, driving an EV is green.  Driving an EV reduces CO2 emissions.   Driving an EV is sustainable.  Right?
Well, no.  According to recently published scientific papers, driving an EV in China dramatically increases CO2 emissions relative to driving an internal combustion engine vehicle (ICEV).
Why?  Because China’s electricity grid is overwhelmingly powered by fossil-fuels (i.e., 88% of China’s energy consumption  (2015) is derived from coal, oil and gas).   Therefore, the energy used to charge up an electric vehicle in China is derived from a rapidly growing fossil fuel-based electrical grid.
Fossil fuel-powered electricity grids are growing in prevalence across the world.  And this will continue to be the case as “1,600 coal plants are planned or under construction in 62 countries” which will “expand the world’s coal-fired power capacity by 43 percent” (New York Times, July, 2017).
As long as EVs continue to be predominantly powered by the growing fossil fuel infrastructure in China (“Chinese corporations are building or planning to build more than 700 new coal plants at home and around the world”), driving EVs will not reduce CO2 emissions relative to driving ICEVs.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Put another way, purchasing and driving a conventional internal combustion engine vehicle will actually reduce China’s CO2 emissions.
According to Barkenbus (2017), “when EVs receive electricity with emission levels exceeding 559 gCO2/kWh, they, unfortunately, are net contributors to climate change when compared with conventional vehicles.”
China’s EVs receive electricity with emissions levels of 712 gCO2/kWh, which is 27% greater than the emissions associated with driving the average ICEV.

Image: Barkenbus, 2017
Not only that, but as the introductory image above indicates, the manufacture of battery-powered EVs emit 50% more greenhouse gas emissions (CO2) than ICEVs do.
Qiao et al., 2017
“In this study, the life cycle energy consumption and greenhouse gas emissions of vehicle production are compared between battery electric and internal combustion engine vehicles in China’s context. … Greenhouse gas emissions of battery electric vehicles are 50% higher than internal combustion engine vehicles.”
“Electric Drive Vehicles (EDVs) are considered to be environmentally-friendly and have attracted much attention worldwide, and Battery Electric Vehicles (BEVs) are the most popular vehicles among all kinds of EDVs. In China, the country with the world’s largest automotive market, the government is determined to develop BEV industry and produced over 250 thousand BEVs in 2015, and the annual growth rate was 420%. In addition, according to the production plan, the cumulative output of BEVs in China will reach 5 million in 2020, meaning that BEVs will gradually replace Internal Combustion Engine Vehicles (ICEVs).  BEVs [Battery Electric Vehicles] are designed to obtain more environmental benefits, but the energy consumption and GHG emissions of BEV production are much larger than those of ICEV [Internal Combustion Engine Vehicles] production in China.”
So why is it that advocates of CO2 emissions reductions so readily extol the explosion of EV purchases and use worldwide?
Share this...FacebookTwitter "
"Moths are the insect we truly love to hate. The press report almost annually on the looming threat of clothes moths. I have previously written in defence of diamondback moths, a migratory pest of cabbage crops, and highlighted the quirks of biology that drove the spectacle of thousands of Silver Y moths gatecrashing the Euro 2016 final without tickets. I am absolutely unapologetic about my love for these diverse and intriguing cousins of the much better-loved butterflies. Moths get a bad press thanks to a few species which negatively affect our lives (this also applies to other insects, such as wasps), but most are harmless (or beneficial), fascinating, and often even beautiful. And so we come to recent news reports of a plague of toxic caterpillars descending on London. The caterpillars in question are those of the oak processionary moth (Thaumetopoea processionea, or just OPM) – just about the only species for which I struggle to summon up much sympathy. So what’s the issue? It’s not to say that this is an unattractive moth. The grey-black colour scheme of the adults, active in late summer, lends them the look of having been delicately sketched in pencil. You are much more likely to encounter the caterpillars, which are covered in very long, white hairs. Colonies of OPM caterpillars form white silk nests on oak trees and can be spotted moving about in remarkable nose-to-tail processions. Other moth species form similar nests in the UK, including occasionally on oak: if such a nest is found outside London (especially in Essex or Cambridgeshire), it is more likely to be the Brown-tail moth. Unlike the Brown-tail, however, the oak processionary moth is not native to the UK. It was first recorded in Britain in 1983, but the species established properly in around 2006, when it’s believed some eggs arrived on imported oak trees. This isn’t in itself a reason to dislike OPM, as conservationists (including myself) can sometimes be hypocritical about non-native species: for instance, we are vocally concerned about the arrival of the horse-chestnut leaf-miner moth because it harms horse chestnut trees – even though the trees themselves are non-native. However, OPM is also a potentially a public health problem. Each of the caterpillar’s hairs contains a toxin called thaumetopoein. Touching an OPM caterpillar directly could bring you out in a rash – in fact, as a general rule it’s always best to avoid hairy caterpillars unless you know what you’re dealing with. The hairs of OPM caterpillars can also break off and drift on the air and, if there are sufficiently high densities of caterpillars, these hairs can cause rashes and respiratory problems even to bystanders.  Besides affecting people, OPM can also impact the oak trees on which it feeds. A particularly severe infestation could strip a tree completely bare of its leaves, though few cases of this taking place in the UK have been reported. Most people agree that something needs to be done, although the NGO Butterfly Conservation argues that, rather than tackling the moth wherever it appears, control efforts should focus on areas where the threat to human health is high or large numbers of trees are at risk of death. Nevertheless, controlling OPM outbreaks is difficult. The nests are constructed in the crowns of oak trees when they are in full leaf, and even if they can be reached, removing them manually requires full protective equipment to ward off the toxic hairs. For that reason the preferred approach is currently to tackle nests remotely, spraying trees with insecticide when the moth is most vulnerable – as a young caterpillar, between April and June.  There is currently no insecticide that is specific to OPM, so a bacterium known as “Bt” is used. Unfortunately, Bt is toxic not just to OPM, but to the caterpillars of all moths and butterflies. The financial cost of these control efforts is astronomical – estimated at around £1.2m per year in 2016-17. The uncounted, and incalculable, cost to the oak woodland ecosystem could be greater still. The loss of much of the insect biodiversity from our woodlands would be tragic in itself but is likely to have further implications for the bats, birds and other wildlife that rely on these insects for food during their breeding seasons – a study of an OPM control programme in woods near Pangbourne, Berkshire, suggested that blue and great tits were breeding less after spraying took place. It’s these losses that have put me off OPM. But let’s not panic – there are plenty of reasons to feel hopeful about the future of Britain’s oak woodlands. It’s true that the moth has been recorded “across vast regions of the south-east”, but that mostly only refers to the highly-dispersive males. To spread the outbreak requires the egg-laying females to travel, and they don’t fly nearly as far. This means that, for now, the toxic caterpillars are mainly confined to London. The outbreak has crossed the M25 ringroad in just a few places, and is still only expanding at a slow pace. Encouragingly, some of the more isolated sections of the outbreak also appear to be coming under control. New outbreaks in Watford, Barnet, and Pangbourne all appear to have been successfully removed. An outbreak at Bethlem Hospital, Croydon, estimated to contain 4,000 nests in 2012, was confined to just four trees by 2016. Vigilance is key, and this year the Forestry Commission is once again asking the public to report any potential sightings of OPM through its Tree Alert scheme. Finally, we may have some unexpected allies on our side. In its native southern and central Europe, OPM is not especially problematic because it rarely reaches sufficiently large population densities. That’s partly because its numbers are kept in check by its natural enemies – parasitoids. This is a catch-all term for various insects with a rather gruesome life-cycle: eggs are laid inside caterpillars and other insects, before the larvae eat their victim from inside out (killing it in the process) and emerge as a fully-formed fly or wasp ready to seek out new prey.  Often, when an insect expands its range by artificial means (as OPM did, entering the UK on imported trees), it can take some time for its parasitoids to catch up, and in this lag period the insect may do particularly well. However, a recent study found nearly half of the OPM caterpillars sampled from the Croydon outbreak in 2014 were infested by one such natural enemy, the Carcelia iliaca tachinid fly. This suggests the oak processionary moth may be reaching the end of its lag period in the UK, and as the flies attack more caterpillars, this could help the control efforts. My enemy’s enemy is truly my friend, and in this case, perhaps it is a tiny fly."
"
Share this...FacebookTwitterGeologist Dr. Norman Page left a comment which I’ve decided to upgrade to a post. In it he writes solar and La Nina observations fit well with his recent paper showing that climate is controlled by natural orbital and solar activity cycles.
Dr. Page is among a growing number of scientists who share the general view that natural solar and oceanic cycles are mostly driving the climate, just as they always have in the past.
Warming has already peaked, cooling ahead
And as a result, Dr. Page believes that the millennial temperature cycle peaked at about 2003/4 and the earth is now in a cooling trend, which will last until about 2650. Read background here.
Recently he published a paper titled: “The coming cooling: usefully accurate climate forecasting for policy makers“.
Models “unfit for purpose”
His paper argues that the methods used by the establishment climate science community are not fit for purpose and that a new forecasting paradigm should be adopted. A number of papers have been published over the recent years pointing out that climate models have been far short of reliable.
In the paper’s abstract Dr. Page writes that the Earth’s climate is the result of resonances and beats between various quasi-cyclic processes of varying wavelengths and that it is not possible to forecast the future unless there’s a good understanding of where the earth is in time in relation to the current phases of those different interacting natural quasi periodicities.
“Temperature decline in the coming decades and centuries”
He presents evidence specifying the timing and amplitude of the natural 60+/- year and the all important 1,000 year periodicities (observed emergent behaviors), which he and other scientists maintain are so obvious in the temperature record.

Fig. 8, HadSST3 temperature anomaly: “Over the last 135 years an approximate 60 year periodicity is clearly present in the temperature data.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




He projects cyclic trends forward and predicts a probable general temperature decline in the coming decades and centuries.
Large divergence by 2021
He also estimates the timing and amplitude of the coming cooling, writing: “If the real climate outcomes follow a trend which approaches the near term forecasts of his working hypothesis, the divergence between the IPCC forecasts and those projected by his paper will be so large by 2021 as to make the current, supposedly actionable, level of confidence in the IPCC forecasts untenable.”
The 1991 millennial solar activity peak is seen in Figure 10 neutron data.
Dr. Page notes that there is a varying lag between the solar activity peak and the corresponding peak in the different climate metrics because of the thermal inertia of the oceans. In the abstract he writes:
It has been independently estimated that there is about a 12-year lag between the cosmic ray flux and the temperature data – Fig. 3 in Usoskin (28).”
Page says this correlates with the millennial temperature peak seen at 2003/4 in the RSS data in Fig 4,

Fig 4. RSS trends showing the millennial cycle temperature peak at about 2003 (14)
Page also says that since the strong El Nino peak anomaly of 2016, the temperature has “declined rapidly” and: “The cooling trend is likely to be fully restored by the end of 2019.”
======================================
Dr. Norman J. Page    
 Email: norpag@att.net
 DOI: 10.1177/0958305X16686488
 Energy & Environment
 0(0) 1–18
 (C )The Author(s) 2017
 journals.sagepub.com/home/eae
Share this...FacebookTwitter "
nan
"Our colleague, the archaeologist Santiago Rivas, recently made a remarkable discovery. On a small plateau above the outskirts of Iquitos, a town in the northern Peruvian Amazon, he found a layer in the soil which contained small pieces of ceramic pottery, that were around 1,800-years-old. Digging deeper, he found another layer of soil, this time containing pottery that was about 2,500 years old. This is the archaeological site at Quistococha which has been occupied for at least the past 3,000 years. The pottery fragments are beautifully decorated, sometimes with subtle geometric scratch marks or boldly painted with bright red patterns. Not all of the fragments are small: erosion revealed the rim of a large cooking pot that would have been 40cm across when it was intact. Large pots were supported on an open fire by “elephant feet”: small clay pot rests also found in the archaeological layers. As a place for people to live, Quistococha would have had many advantages. It is located on a terrace above a fertile floodplain of the Amazon which is ideal for growing maize, while the surrounding palm swamp provides fruits and fibres. Just below the terrace, fresh water flows out of a spring. Researchers know that indigenous communities have had profound and complex relationships with Amazonian forest landscapes for thousands of years. However, it is still far from clear just how much deforestation took place before European colonisation in the 16th century.  Quistococha is an ideal place to search for answers – and we recently published a research article based on our work there. The site has an unusually good record of past environmental change thanks to a nearby floodplain lake and swamp. These preserve the remains of plants that grew there, and the charcoal from fires lit by people – both in the prehistoric period as well as during the expansion of Iquitos over the past two centuries. This combination allowed us to explore the relationship between ancient people and the extent of the surrounding forest. Charcoal in the sediment core from the nearby lake – an indicator of fire use – was abundant from about 2,500 years ago until the 1800s: people were, therefore, continuously present at that time. However fossil pollen from smaller trees that make up “secondary forest” growing on deforested land only became abundant over the past 150 years, when the nearby city expanded. Prior to that, for thousands of years, indigenous communities apparently had little impact on forest cover. Such new knowledge about ancient Amazonians is highly relevant for conservation today. For indigenous groups it provides historical context to their fight for land rights and recognition. Studies like ours also show that traditional uses of the landscape should be valued highly, and that Amazonian communities can support themselves without extensive deforestation. This philosophy is the basis for the work of our partners, the Instituto de Investigaciones de la Amazonia Peruana (IIAP), which promotes sustainable management of these floodplain forests. Last but not least, these discoveries are an opportunity to engage with the expanding urban populations of Amazonia: an important voice in the decision-making process. Iquitos is the largest city in the world not connected to a national road network. Recently, the Peruvian Congress has declared an ambitious range of road building projects in Amazonia as national priorities. The planned connection between Iquitos and the rest of Peru promises lower prices for food and other imports. But activists who warn of the adverse consequences of poorly planned development are struggling to be heard. The new road would represent a “first-cut” through indigenous territories and the most diverse and carbon-rich forests of Amazonia. And as these are issues of low importance to the urban majority, the only way to challenge it would be by engaging city dwellers in debates about the implications of future transport networks and of other options for land use. Locals and tourists alike throng to Quistococha on hot weekends to swim in the lake and relax in waterside cafes. Above and in sight of all of them, but now silent, there is a site that records thousands of years of humans living in a continuously forested landscape. The landscape and the stories it tells are an opportunity to reflect on how we might choose to continue the relationship between people and forests in the future."
"
Share this...FacebookTwitterI thought the following paper was interesting. 
No, lead-author Prof. Pierre Gosselin is not me from NTZ. But he very likely is a descendent the same family line. The first Gosselin (Gabriel) left Normandy-France and landed in Quebec City way back in 1653. As a devout Catholic, Gabriel earnestly started what was the population of Gosselins over North America and beyond over the next 364 years.
=============================================
Effects of climate and fine particulate matter on hospitalizations and deaths for heart failure in elderly: A population-based cohort study
In a recent study a team of scientists led by Prof. Pierre Gosselin assessed 112,793 people aged 65 years and older who had been diagnosed with heart failure in Quebec between 2001 and 2011. Over an average of 635 days, the researchers measured the mean temperature, relative humidity, atmospheric pressure and air pollutants in the surrounding environment and studied the data to see if there was any relationship.
Their results: for each decrease of 1°C in the daily mean temperature of the previous 3 and 7 days, the risk of heart failure events is increased of about 0.7%. In other words, a drop of 10°C in the average temperature over 7 days, which is common in the province of Quebec because of seasonal variations, is associated with increased risk to be hospitalized or to die for the main cause of heart failure of about 7% in elderly diagnosed with this disease.
The paper’s abstract:
We measured the lag effects of temperature, relative humidity, atmospheric pressure and fine particulate matter (PM2.5) on hospitalizations and deaths for HF in elderly diagnosed with this disease on a 10-year period in the province of Quebec, Canada.
Our population-based cohort study included 112,793 elderly diagnosed with HF between 2001 and 2011. Time dependent Cox regression models approximated with pooled logistic regressions were used to evaluate the 3- and 7-day lag effects of daily temperature, relative humidity, atmospheric pressure and PM2.5 exposure on HF morbidity and mortality controlling for several individual and contextual covariates.
Overall, 18,309 elderly were hospitalized and 4297 died for the main cause of HF. We observed an increased risk of hospitalizations and deaths for HF with a decrease in the average temperature of the 3 and 7 days before the event. An increase in atmospheric pressure in the previous 7 days was also associated with a higher risk of having a HF negative outcome, but no effect was observed in the 3-day lag model. No association was found with relative humidity and with PM2.5 regardless of the lag period
Lag effects of temperature and other meteorological parameters on HF events were limited but present. Nonetheless, preventive measures should be issued for elderly diagnosed with HF considering the burden and the expensive costs associated with the management of this disease.
Lower risk of death in summer
The authors also found:
The results showed a higher risk of hospitalization or death in the winter period of the year (October to April) compared to the summer period (May to September).”
Share this...FacebookTwitter "
"
Share this...FacebookTwitterRecently German SAT1 television broadcast a documentary on the state of the European and German increasingly green power grid: “How secure are our power grids?” Due to the volatile and unpredictable supply of wind and solar energy, the grid has become far more unstable, the documentary warns. The news is not good.
At best: the consumers are getting a far lousier product at a much higher price.
At the 17-minute mark, Bernd Benser of GridLab-Berlin tells viewers that while grid operator Tennet had to intervene only 3 times in 2002 to avert grid instability, last year he says the number was “over 1000” times — or “three times daily”.
These intervention actions, known as redispatching, cost the consumer about a billion euros last year alone, says Benser.  The SAT 1 voice-over warns that more power transmission lines are urgently needed if the Energiewende is to avoid “becoming a sinking ship“. However over the years acceptance by citizens has swung from a generally warm welcome to ferocious opposition. Politicians need to start noting that green energies have overstayed their welcome.
Major grid instability
And as wind and solar power capacity gets added to the grid without expanding transmission capability to offset the ever more wild fluctuations, grid operators are now constantly scrambling to keep the grid from spiraling out of control. At the 21-minute mark, Klaus Kaschnitz of the operations management of Austrian Power Grid remarks:
These fluctuations in the system that we now see have increased dramatically and are ultimately a product of weather events.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The fluctuations are having a profound impact, Kaschnitz explains. It is especially difficult to keep the grid at 50 Hz frequency, which makes keeping the grid from collapsing harder and the powering of modern industrial systems highly challenging. At the 25-minute mark, the report then switches the focus to the grids’ vulnerability to hackers.
Swiss daily: “danger of a blackout rising”
Also the Swiss online Baseler Zeitung (BaZ) here reports on major power grid woes in Switzerland, warning: “The danger of a blackout is rising” and that power grid operator Swissgrid “must intervene increasingly more often in the power grid“.
According to the BAZ, in 2011 Swissgrid had to intervene only twice over the entire year. But since then the grid has become far more unstable, and that at the current rate it will be necessary to intervene 400 times in 2017!
In summary, the green energies have resulted in two outcomes for citizens: 1) a supply that is now far more unstable and 2) power that is far more expensive. In a nutshell: Far less quality for a lot more money.
That’s the expected result whenever you have the wrong people (activists and politicians) deciding how to run complex technical systems.
 
Share this...FacebookTwitter "
"The European Union’s common agricultural policy is a sprawling programme of farming subsidies that covers everything from income support for farmers to supporting the promotion of products such as wine. No wonder then that the European Commission, the EU’s executive branch, wants to “modernise and simplify” the policy. This is why the EC has just published its legislative proposals for the common agricultural policy (CAP) after 2020. Its aim to make sure that the CAP continues to support farmers and rural communities, that it leads the sustainable development of EU agriculture, and that it reflects the EU’s ambitions on the environmental and climate change. Across the years 2021-2027, the proposed CAP’s total budget will be around €365 billion. However, the food system in Europe and the UK faces some critical sustainability challenges. And, despite some welcome new objectives – particularly on the environment – and new support payments for young farmers, the proposals clearly do not go far enough on areas such as health.  Europeans deserve an agricultural policy that addresses their health. Too much red and processed meat, and food with lots of fat, sugar and salt means more than 20% of the continent is now obese. Poor diets are also responsible for half of the burden of cardiovascular disease, which remains the leading cause of death in the EU.  However, the CAP announcement contains little on health measures. The EC recently called for better access to “nutritious valuable products such as fruit and vegetables”, yet the new CAP contains no new policy instruments and no specific targets for fruit and veg.  This is disappointing and, unless addressed in the final legislation, will not go unnoticed. From a public health perspective, this really is “low-hanging fruit” as the need for more fruit and vegetables is uncontroversial: the World Health Organisation’s 400 grams per day is a widely accepted minimum standard and Eurostat already provides comparable data on consumption in the EU.  Implying that the school scheme will do the trick is a severe disappointment. The scheme promotes the benefits of healthy eating to children and encourages them to increase their consumption of fruit, vegetables and milk – yet its current budget is just 0.33% of the CAP.  The CAP’s sometimes incoherent position on the use of public money is nicely illustrated by its support for the wine sector. Excessive alcohol consumption is a well known public health problem, and wine subsidies themselves have often been criticised. Yet the draft proposal for CAP reform accords considerable attention to wine promotion measures. Most perversely, the increased value of wine sales may well be one of the policy’s indicators of success. The CAP reform does outline new environmental initiatives such as a new agri-environmental scheme and the first ever initiative to address the decline of pollinating insects. This is welcome, bearing in mind the overuse of pesticides means three-quarters of flying insects have disappeared, jeopardising pollination and yields.  Taking into account that global food and farming production contributes 30% of all greenhouse gas emissions – with 18% from livestock – it will be important to see the details of how the new environmental scheme incentivises climate action. It was disappointing so see nothing specific on soils, as Europe loses 970m tonnes of topsoil every year to degradation and erosion. Europeans also waste 71kg of food per person every year costing €143 billion (2012 figures) in wasted resources and environmental impact – and there is nothing in today’s announcement on waste measures, which is a missed opportunity. What Europe desperately needs is a new comprehensive food policy – one that actually tackles these huge challenges to human health and society, or the environment, across the food system not just at farm level. The EC could start by making healthy and sustainable choices easier for regular people. That might involve new guidelines on public procurement, a continent-wide child obesity strategy or more CAP money set aside for promoting fruit and vegetables.  Europe could also set targets for using less antibiotics and pesticides and could integrate healthy and sustainable nutrition into school curricula. Rising food insecurity across the UK and Europe also means measures must be targeted at those vulnerable groups who aren’t able to access healthy diets. Protecting soils in the face of degradation and nutrient loss could deliver major environmental and health benefits, but the EU and member states have failed to act on this basis, and a proposed directive on soil has remained stalled since 2006. Targeting CAP payments for ambitious crop rotations with a minimum share of legumes would be a more positive approach. Perhaps the new environmental scheme could be more ambitious and include such measures. Incentives should be targeted at those practices across the food system that positively pursue improvements in the soil, water and biodiversity. Let’s also target specific payments for environmental services that favour mixed crop-livestock farms and grassland systems."
nan
"Australians should be proud of the country’s achievements on climate change, energy minister Angus Taylor has argued in a newspaper column that claims “quiet Australians” don’t accept the “shrill cries” of the government’s climate critics. The column, published in The Australian, makes a series of claims about Australia’s emissions and how they compare to other countries, as well as highlighting exports such as LNG that are “dramatically reducing emissions” in other countries.  So is Australia really a paragon of climate virtue – cutting emissions at home while helping the world to cut emissions? As is always the case when it comes to climate and energy policy, there is much to check and understand in Taylor’s article. Prof Frank Jotzo, director of the Centre for Climate and Energy Policy at the ANU Crawford School of Public Policy, told Guardian Australia: “I would characterise [Taylor’s article] as a selective use of statistics that make Australia’s emissions trajectory look good, when in reality it does not look good at all.” Taylor writes that Australia is “responsible for only 1.3 per cent of global emissions, so we can’t single-handedly have a meaningful impact without the co-operation of the largest emitters such as China and the US.” In the context of global emissions, there is much that Australia can, and does, do that has a meaningful impact. The 1.3% figure does not account for Australia’s contribution to global emissions from the fossil fuels we dig up and export. If this exported coal and gas was accounted for, one analysis suggests Australia would be responsible for almost 5% of the global carbon footprint from fossil fuel burning. When countries report their emissions to the United Nations Framework Convention on Climate Change, they only report emissions occurring inside their borders, so it could be argued that using this larger number is unfair. But the problem is that elsewhere in Taylor’s article, he says Australia’s exporting of LNG is helping countries cut emissions. Jotzo says: “If we are going to talk about impacts on global emissions of Australia’s energy exports, then we need to consider all fuels, including coal. Any exporting of coal will result in higher global emissions because it increases the availability and lowers the price of coal, and encourages the use of coal.” While Taylor admits that LNG processing in Australia has pushed domestic emissions higher, he claims that “our LNG exports are dramatically reducing emissions in customer countries such as Japan, South Korea and China — the equivalent of up to 30 per cent of our emissions each year”. But Jotzo says this claim depends heavily on what the LNG displaces. He says the “lion’s share” of the exports will actually replace gas from other sources, rather than displacing coal generation. There is also a risk, he says, that increasing LNG exports also encourages countries to build more gas infrastructure, making it harder to move away from the fossil fuel. He adds: “It is not clear that the availability of Australian LNG decreases emissions internationally.” “Australia meets and beats its emission-reductions targets, every time,” writes Taylor. “We beat our first Kyoto targets by 128 million tonnes. We ­expect to beat our 2020 targets by 411 million tonnes.” The key reason why Australia has easily beaten its targets, is that they were very low to begin with. Australia’s first Kyoto target allowed it to increase emissions by 8% between 1990 and 2010. The second target period required a 5% cut below 2000 levels by 2020. Much of Australia’s cuts to emissions in recent decades, says Jotzo, has been achieved through drops in land clearing, rather than reductions in other parts of the economy the government could have influence over. Australia wants to use some 411 million tonnes of CO2 “credits” amassed over the Kyoto periods against future targets under the separate Paris agreement, even though it admits it is probably the only country looking to use these “carryover credits”. Using carryover credits would cut the amount of emissions reductions Australia would need to find to meet its Paris target by about a half. At the latest UN climate talks in Madrid, Australia came under harsh criticism from more than 100 countries for its desire to use the credits, which some analysts say is a proposal with no legal basis. Australia was accused of “cheating” at the talks, but refused to back down on the carryover issue, leaving it unresolved. In his article, Taylor says “when you compare Australia’s emission-reduction track record with nations such as Canada and New Zealand”, Australia comes out on top. While Australia’s emissions have dropped 12.9% since 2005, writes Taylor, New Zealand’s have risen by 4% and Canada’s have dropped only 2%. Jotzo says the 12.9% figure Taylor is using includes changes to land use, such as land clearing, which are not major issues for other developed countries. As an example, Australia’s reporting to the UN shows that in 2005, emissions from land use, land-use change, and forestry (known as LULUCF) were +88mt. In 2017, LULUCF emissions were -19mt. That’s a net drop of 107mt. Using the same periods for New Zealand, the difference is a net increase of 4.8mt. A fairer global comparison, says Jotzo, is to use figures that remove these LULUCF emissions. This, he says, turns Australia’s 12.9% drop between 2005 and 2018 into a 6% rise. In his article, Taylor repeats a point that he made during his official speech to the Madrid climate talks that technological innovation would be a key to fighting climate change. In the article, Taylor points to the new national hydrogen strategy as an example of innovations with “enormous potential” for cutting future emissions. Jotzo says there is potential for an Australian hydrogen export industry to have a positive impact on global emissions. However, he says this comes with large caveats. Hydrogen can be produced using renewable energy, but also by using fossil fuels. If Australia was to use coal or gas, it would need to be able to capture most of the waste CO2 to claim the fuel as green. But analysis by Jotzo and colleagues shows that while rates of up to 95% carbon capture might be “technically possible” they have not yet been achieved. Only two plants – in Canada and the UK – currently capture CO2 when producing hydrogen from fossil fuels. The best capture rate is 80%. If the carbon capture rates were at 60%, then Jotzo says the net greenhouse gas footprint of hydrogen would be the same as just burning gas. According to Taylor, “Australia has strong targets, clear plans, an enviable track record” on climate change, and Australians should be proud of it. But when overseas groups look at Australia’s record compared to the rest of the world, the assessments come out differently. An analysis by Climate Action Tracker says Australia’s Paris targets are “insufficient” and inconsistent with the Paris goal of keeping global warming well below 2C. Australia has been placed consistently towards the bottom in the annual Climate Change Policy Index analysis of the world’s top 57 emitting nations. The most recent analysis ranked Australia as the sixth worst country on climate change overall. Jotzo, who attended the Madrid climate talks as an observer, said: “Australia was highly regarded at the talks for its technical competence, and it always has. But Australia is not highly regarded at all for its policies or for its efforts to water down effective ambition of the Paris agreement.” He said speaking with observers from other countries, Australia’s position was seen “with quite some bewilderment” especially with the backdrop of the current devastating fire season. Jotzo adds: “They are flabbergasted that Australia is digging in to its stance of getting an easier deal when it would so obviously be in its national interest to encourage strong global action.”"
"At close to 90 years old, Brazil’s most venerated indigenous leader, Raoni Metuktire, has returned to the spotlight to challenge the man he calls the worst president of his lifetime, Jair Bolsonaro. In an interview with the Guardian, the Kayapó chief said he wanted to speak out about the far-right administration’s plans to allow mining in indigenous territory and he warned that Brazil’s Amazon policies threatened global efforts to protect nature and address the climate emergency.  “Ï have seen many presidents come and go, but none spoke so badly of indigenous people or threatened us and the forest like this,” he said. “Since he [Bolsonaro] became president, he has been the worst for us.” Raoni has lived through 24 administrations since first making contact with the world outside his rainforest home, and is at the forefront of a reinvigorated indigenous movement in South America’s biggest nation. Along with Davi Kopenawa Yanomami, he is leading the resistance against government plans to open up the rainforest to land speculators, cattle ranchers, loggers and gold miners. With his lip disc, beads, earrings and flowing grey hair, Raoni is probably the best-known Amazonian in the world. But he spent the first 18 or so years of his life unknown to anyone outside his forest community. Raoni was a young, jenipapo-painted warrior when his tribe, the Metuktire Kayapó, was first contacted by non-indigenous invaders in the early 1950s, according to a new book by the veteran British explorer John Hemming. The intruders brought gifts of metal blades and beads but left behind European diseases such as malaria, influenza and measles that decimated the population. In the 1970s and 1980s, Raoni was among the leaders of the often deadly fight against the BR-080 road, cattle ranchers and the Belo Monte dam. He rose to international prominence thanks to his friendship with the rock star Sting. In the years that followed, he was feted by world leaders and met the pope, gaining a level of prestige and leverage that challenged the prejudices of the many Brazilians who see indigenous people as poor and uneducated. This helped the Kayapó to secure government recognition of their territorial rights across a vast chain of reserves, which formed the spine of a north-south firewall against deforestation. “From many years ago, I fought in campaigns and appeared in the media. Then, when we won the victory of having our lands demarcated, I stopped because everything seemed fine, everything was tranquil,” he recalled. “But the new president threatens indigenous people, so I came back to fight again.” Recent government figures show Amazon deforestation has surged to the highest level in a decade. Farmers and land-grabbers have started more fires to clear land, which is pumping huge quantities of carbon dioxide into the atmosphere, disrupting the water cycle and destroying the world’s most biodiverse land habitat. They have been emboldened by a government that has spent its first year weakening environmental protections, encouraging loggers and heaping scorn on conservation groups and forest dwellers. Even before entering office, Bolsonaro frequently abused indigenous groups as an obstacle to economic development. “It’s a shame the Brazilian cavalry hasn’t been as efficient as the (North) Americans who exterminated the Indians,” he said in 1998. Now in power, he has promised to halt demarcation of new reserves and to open up territories to mining and agriculture businesses. Anthropologists have warned these actions will result in the genocide of uncontacted tribes. Among the greatest threats is encroachment and environmental destruction by Brazil’s tens of thousands of garimpeiros (artisanal gold miners). Almost all are illegal, but Bolsonaro has expressed far more support for this group than previous state leaders. For him it is partly a personal issue. Bolsonaro’s father was a part-time gold miner, and the president has said he himself panned for gold while serving in the army. “Bolsonaro is a garimpeiro. It explains the way he thinks, always trying to explore more land,” said Davi Kopenawa Yanomami. “He has a sickness in his head. He doesn’t think about others, or about the future.” An author, shaman and environmentalist, Kopenawa is arguably the most prominent intellectual voice of the more than 300 different indigenous groups in Brazil. His book The Falling Sky outlines the very different cosmology of traditional forest peoples and warns that humankind is breaking the forest pillars that hold up the sky – an allusion that stretches beyond the climate crisis. He said that in the past year Yanomami lands (which stretch across Brazil’s border with Venezuela) had been invaded by the biggest wave of illegal miners since the 1980s. “They are poisoning our rivers, killing our fish, and our people are starting to get sick with malaria again,” he told the Guardian. Quietly spoken but defiant, Kopenawa said the problem was greater than Bolsonaro. Although the president had made matters worse, he said, mining companies from Canada, China and Japan were behind the push for resources. “Our politicians are selling our wealth. This brings no benefit to our people, just destruction. Who is getting rich? It’s the foreigners. The big companies are behind this.” The threats are not just to the forest. Raoni has two bodyguards and is a target for attention-seeking nationalists who are trying to ingratiate themselves with Bolsonaro. At a recent gathering of forest defenders in Altamira, a small group of land grabbers and farmers attempted to disrupt proceedings by surging towards the top table, prodding and shouting in the face of a young indigenous woman who was speaking about the killings of her people. Raoni wagged his finger reprovingly, a sign for half a dozen Kayapó warriors to push the intruders back to their seats. The scuffle prompted exaggerated claims on rightwing social media that Raoni had “ordered an attack”. In fact, it was a defence – and a reminder of what has been happening across the Amazon for decades. The jostling is now in the courts. The academic who organised the disruption has filed a criminal accusation against the Kayapó chief. The organisers of the event had already lodged a complaint against the protest organisers for making threats. Raoni said the fracas should not distract from the more important issue of how to save the Amazon. “I was very sad at what happened. The people who want to destroy the forest came to disrupt things. I felt it was important to talk so I asked people to hold them back.” The landowners were much quieter from that moment on. Civil society organisers claimed this as a victory for the majority in Brazil who want to protect the rainforest. They hope to build alliances across the Amazon and throughout the world to counter the threat posed by Bolsonaro and extractive industries. There are signs this may be happening under the leadership of Raoni, Kopenawa and others. Indigenous tribes once fought each other as well as riverine settlers and quilombolas (descendants of runaway slaves who moved into the forest). Today, however, many of these different groups are allied against tree-clearing and river-poisoning intruders. Raoni invited people across the world to join a peaceful resistance against the forces threatening indigenous territory, the Amazon and the world. “They have the money and the guns. We don’t have that. I don’t have that,” he said after the interview. But with temperatures climbing and the forest under increasing threat, he said, it was necessary to act to help Brazil and avert a grimmer future for people around the globe. “Nature is essential for us to breathe,” he said. “I hope people, not just in Brazil, will take my hand and join our forces to save nature, the forest and everything inside it, including the animals and the people.”"
"Sometime around 3600BC, people in the Balkan peninsula reached a major milestone: their mining and metal smelting created enough pollution for us to detect it today. Our research has revealed this was the beginning of the Bronze Age in the region, and the birth of large-scale metallurgy in Europe. To give some context for how early this was: at the same time, the first ever writing was just being developed in Sumer, Mesopotamia, while Britain was still in the Stone Age. Egypt’s first pyramids were still a thousand years in the future. We already knew about these Bronze Age Balkans from patchy archaeological records of axes, adzes and beads. But we are now able to learn more about them thanks to traces of pollution they left behind.  Metal is extracted from its ore through a process known as smelting. This releases microscopic particles of lead into the atmosphere, which are then transported long distances by winds until they settle on the ground.  Peat bogs are ideal repositories for these particles because atmospheric transport is the main pathway by which pollutants can reach these sites. As peat bogs grow in small layered increments each year, they can give us a clear history of the environment in which they grew. When many such chemical analyses of known ages from the many layers of the bog are put together, a sequence of changing pollution can be developed. In our new paper published in PNAS, we present the first such record of changing pollution from south-eastern Europe, reconstructed from the changing concentrations of lead in a peat deposit from western Serbia.  We found evidence of raised lead levels dating back to 3600BC. This is the oldest known environmental metal pollution on Earth, and places the Balkans very much at the forefront of the period of metallurgical discovery and development in the very earliest Bronze Age. Previously the oldest known European environmental pollution happened about 3000BC in southern Spain. Our findings pushes this back by more than 500 years. Indeed, it would take western Europe another 1,000 years to catch up to the same level of metallurgical development.  Lead pollution in the Balkans has continued almost ever since. Those same peat bogs show spikes in the level of pollution during the Late Bronze Age, and unsurprisingly, during the Roman period. At this time, the Balkans were known around the Roman world as one of the main sources of silver used for coins. Since silver is regularly found in ores alongside lead, silver smelting releases lots of lead into the atmosphere.  What is more interesting, is what happened after the Roman Empire fell in the 3rd and 4th centuries CE. In Serbia at least, it appears lead pollution continued, and even increased, indicating that local people continued the strong mining and smelting culture developed by the Romans.  This goes against the long-held view of barbaric hordes with little technological know-how ousting the Romans, leading to the “Dark Ages”, as we term the 1,000 years following the fall of Rome. This may have been true in much of western Europe, but the Balkans were in fact rather well-lit. The culture of metalworking and mining continued into the medieval period in Serbia, as the peat shows almost constant increases in the amount of lead pollution until the 17th century. Periods of pollution reduction often coincide with periods of plague or pestilence, but they are always short-lived, suggesting metallurgy was a key feature of local populations recovering after such periods of strife.  Our data suggests the Balkans played a major role in medieval mining and metallurgy right up until the Ottoman invasions, whereupon steadily increasing taxation and bureaucracy in the region caused many mines to close. This leading role is evidenced by levels of lead pollution in the 17th century comparable with known centres of medieval western mining such as the Black Forest in Germany and the north-west of England. Our work presents an alternative view on how the hugely socio-economically important metallurgical industry developed in Europe. People in the Balkans were clearly pioneers of very early metalwork, and remained at the forefront through the Dark Ages and medieval period."
"
Share this...FacebookTwitterWeather and climate analyst Schneefan here writes that the 2017/18 winter in Europe could be one of the coldest of the last 20 years.
In mid September NOAA’s CFSv2 weather model once again crunched out a cold temperatures across Europe for all three winter months (December (left), January (center), February (right)) for the coming 2017/18 winter:

Meteociel/CFS prognosis dated 1 September 2017 for the temperature deviation from the long-term mean at 850 hPa (approx. 1500 m) in Europe for the 2017/18 winter. Source: http://www.meteociel.fr/modeles/cfsme_cartes.php
Schneefan writes one has to go back to the 1990s to find a negative 2.0°C deviation from the 1961-1990 mean that is projected for Germany. That deviation translates to almost 3°C when compared to the 1981-2010 mean. That would would be awfully cold.
The following chart shows the winter temperature anomalies for Germany for each year since 1901:

If projections come true, Germany would face one of its coldest winters in the last 50 years. Source: http://www.wzforum.de/forum2/read.php?6,3260663,3260663#msg-3260663
The latest CSFv2 model run confirms the earlier cold projections that have been calculated since mid June, 2017.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Cooler than normal autumn
Projections for this fall (September, October, November) are also on the cool side. An analysis from 17 September shows that Central Europe will see temperatures that are about 1°C below the 1981-2010 mean. So far the first three weeks have been right on the money.
Schneefan warns that it’s still too early to rely on the latest trend and to bank on it, but adds: “If these cold projections for the 2017/18 winter keep appearing in the next model runs this fall, then the probability increases.” 

Also Schneefan writes that we should not expect any general warming trend soon after the coming winter, due to the lowest solar activity is 200 years, the cooling La Niña that is beginning to take hold, and the already falling temperatures taking place in the wake of the 2015/15 El-Niño.
There are other signs that change is possibly in the works:
After the ice mass growth in Greenland for the first time in the current century and a new record cold July temperature (-33°C) set in Greenland, no one should be surprised that the 2017/18 winter will be the coldest in Europe and other parts of the northern hemisphere this century.
And to potentially make matters worse, the Bali volcano Agung is now at warning level “orange”. The last eruption was in 1963 with a VEI of 5!. So rapidly could global climate unexpectedly and naturally change.
Readers need to note that the projections involve considerable uncertainty, and the winter of course may develop completely differently. Yet, many meteorologists had projected earlier this year a severe hurricane season this year based on oceanic patterns, and that has come true.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterBy Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
German text edited/translated by P Gosselin)
Satellite measurements of Antarctic sea ice do not go back even 40 years. That’s not very much, especially when we consider that many natural climate cycles have periods of 60 years and more.
Luckily we have the field of climate reconstruction. Using historical documents and sediment cores, the development of ice cover can be estimated. In November, 2016, Tom Edinburg and Jonathan Day examined shipping log books from the time of Antarctic explorers and published on ice extent in The Cryosphere:
Estimating the extent of Antarctic summer sea ice during the Heroic Age of Antarctic Exploration
In stark contrast to the sharp decline in Arctic sea ice, there has been a steady increase in ice extent around Antarctica during the last three decades, especially in the Weddell and Ross seas. In general, climate models do not to capture this trend and a lack of information about sea ice coverage in the pre-satellite period limits our ability to quantify the sensitivity of sea ice to climate change and robustly validate climate models. However, evidence of the presence and nature of sea ice was often recorded during early Antarctic exploration, though these sources have not previously been explored or exploited until now. We have analysed observations of the summer sea ice edge from the ship logbooks of explorers such as Robert Falcon Scott, Ernest Shackleton and their contemporaries during the Heroic Age of Antarctic Exploration (1897–1917), and in this study we compare these to satellite observations from the period 1989–2014, offering insight into the ice conditions of this period, from direct observations, for the first time. This comparison shows that the summer sea ice edge was between 1.0 and 1.7° further north in the Weddell Sea during this period but that ice conditions were surprisingly comparable to the present day in other sectors.”
The surprising result: with respect to sea ice extent 100 years ago things looked similar to what we have today, with the exception of the Weddell Sea. A study by Hobbs et al. 2016 also looked back at the last century, here using geoscientific sea ice reconstructions. Once again the strong discrepancies between the real ice development and model simulations were criticized:
Century-scale perspectives on observed and simulated Southern Ocean sea ice trends from proxy reconstructions
Since 1979 when continuous satellite observations began, Southern Ocean sea ice cover has increased, whilst global coupled climate models simulate a decrease over the same period. It is uncertain whether the observed trends are anthropogenically forced or due to internal variability, or whether the apparent discrepancy between models and observations can be explained by internal variability. The shortness of the satellite record is one source of this uncertainty, and a possible solution is to use proxy reconstructions, which extend the analysis period but at the expense of higher observational uncertainty. In this work, we evaluate the utility for change detection of 20th century Southern Ocean sea ice proxies. We find that there are reliable proxies for the East Antarctic, Amundsen, Bellingshausen and Weddell sectors in late winter, and for the Weddell Sea in late autumn. Models and reconstructions agree that sea ice extent in the East Antarctic, Amundsen and Bellingshausen Seas has decreased since the early 1970s, consistent with an anthropogenic response. However, the decrease is small compared to internal variability, and the change is not robustly detectable. We also find that optimal fingerprinting filters out much of the uncertainty in proxy reconstructions. The Ross Sea is a confounding factor, with a significant increase in sea ice since 1979 that is not captured by climate models; however, existing proxy reconstructions of this region are not yet sufficiently reliable for formal change detection.”
A paper published by Ellen & Abrams 2016 even looked back 300 years ago and showed that the increase in sea ice from 1979-2016 has been part of a long-term growth trend of the 20th century:
Ice core reconstruction of sea ice change in the Amundsen-Ross Seas since 1702 A.D.
Antarctic sea ice has been increasing in recent decades, but with strong regional differences in the expression of sea ice change. Declining sea ice in the Bellingshausen Sea since 1979 (the satellite era) has been linked to the observed warming on the Antarctic Peninsula, while the Ross Sea sector has seen a marked increase in sea ice during this period. Here we present a 308 year record of methansulphonic acid from coastal West Antarctica, representing sea ice conditions in the Amundsen-Ross Sea. We demonstrate that the recent increase in sea ice in this region is part of a longer trend, with an estimated ~1° northward expansion in winter sea ice extent (SIE) during the twentieth century and a total expansion of ~1.3° since 1702. The greatest reconstructed SIE occurred during the mid-1990s, with five of the past 30 years considered exceptional in the context of the past three centuries.”
Share this...FacebookTwitter "
"
Share this...FacebookTwitterBefore getting to the subject of climate models, first two small points worth bringing up:
Eco-Trumpism spreading
Firstly, it appears that Trump’s policies are sending powerful political impulses worldwide. For example ultra-alarmist German climate and energy site klimaretter here bemoans that leading socialist Sigmar Gabriel seems to be turning into an “Eco-Trump”. Gabriel actually had the audacity to remind Germany that economics need have as great as or greater priority than climate change does, something causing a bit of political indigestion at klimaretter.
Fears of German companies moving to USA
Secondly, German business daily Handelsblatt here cites a study that tells us Germany will likely see jobs lost due to Trump’s tax reforms. It is feared that a number of German companies may opt to flock over to USA to take advantage of lower taxes, cheaper energy and less stringent regulation. Germany helping MAGA!
===================================
Climate models totally fail in practice: Can atmospheric circulation be simulated at all?
By Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
(German text translated / edited by P Gosselin)
A large part of international climate policy is based on prognoses delivered by climate models. Here the key players act as if they are highly robust and thus serve as a good basis for policy decision making. But what hardly ever makes it through the media filter is the rather hectic discussion taking place behind the scenes among climate modelers.
In September 2014 Theodore Shepherd of the University of Reading summarize the entire extent of the problems in an article published in Nature Geoscience. The models simply fail to grasp the atmospheric circulation. And Shepard feels that will remain the case also in the future:
Atmospheric circulation as a source of uncertainty in climate change projections
The evidence for anthropogenic climate change continues to strengthen, and concerns about severe weather events are increasing. As a result, scientific interest is rapidly shifting from detection and attribution of global climate change to prediction of its impacts at the regional scale. However, nearly everything we have any confidence in when it comes to climate change is related to global patterns of surface temperature, which are primarily controlled by thermodynamics. In contrast, we have much less confidence in atmospheric circulation aspects of climate change, which are primarily controlled by dynamics and exert a strong control on regional climate. Model projections of circulation-related fields, including precipitation, show a wide range of possible outcomes, even on centennial timescales. Sources of uncertainty include low-frequency chaotic variability and the sensitivity to model error of the circulation response to climate forcing. As the circulation response to external forcing appears to project strongly onto existing patterns of variability, knowledge of errors in the dynamics of variability may provide some constraints on model projections. Nevertheless, higher scientific confidence in circulation-related aspects of climate change will be difficult to obtain. For effective decision-making, it is necessary to move to a more explicitly probabilistic, risk-based approach.”
Also accounting for solar irradiance is causing a lot of problems, as Zhou et al. 2015 point out:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




On the incident solar radiation in CMIP5 models
Annual incident solar radiation at the top of atmosphere should be independent of longitudes. However, in many Coupled Model Intercomparison Project phase 5 (CMIP5) models, we find that the incident radiation exhibited zonal oscillations, with up to 30 W/m2 of spurious variations. This feature can affect the interpretation of regional climate and diurnal variation of CMIP5 results. This oscillation is also found in the Community Earth System Model. We show that this feature is caused by temporal sampling errors in the calculation of the solar zenith angle. The sampling error can cause zonal oscillations of surface clear-sky net shortwave radiation of about 3 W/m2 when an hourly radiation time step is used and 24 W/m2 when a 3 h radiation time step is used.”
Currently the author teams for the planned 6 IPCC climate report are getting together. Are the considerable problems surrounding climate models resolved? No sign of that. On October 11, 2017, Stony Brook University set off the alarms: The models still are not running properly! And the German press prefers to keep silent about this. The Stony Brook press release follows:
Study Reveals Need for Better Modeling of Weather Systems for Climate Prediction
Computer-generated models are essential for or scientists to predict the nature and magnitude of weather systems, including their changes and patterns. Using 19 climate models, a team of researchers led by Professor Minghua Zhang of the School of Marine and Atmospheric Sciences at Stony Brook University, discovered persistent dry and warm biases of simulated climate over the region of the Southern Great Plain in the central U.S. that was caused by poor modeling of atmospheric convective systems – the vertical transport of heat and moisture in the atmosphere. Their findings, to be published in Nature Communications, call for better calculations in global climate models.
The climate models analyzed in the paper “Causes of model dry and warm bias over central U.S. and impact on climate projections,” included a precipitation deficit that is associated with widespread failure of the models in capturing actual strong rainfall events in summer over the region. By correcting for the biases, the authors found that future changes of precipitation over the US Southern Great Plain by the end of the 21st Century would be nearly neutral. This projection is unlike what has been predicted as a drying period by the majority of current climate models. The correction also reduces the projected warming of the region by 20 percent relative to projections of previous climate models.
“Current climate models are limited by available computing powers even when cutting-edge supercomputers are used,” said Professor Zhang. “As a result, some atmospheric circulations systems cannot be resolved by these models, and this clearly impacts the accuracy of climate change predictions as shown in our study.” Professor Zhang and colleagues believe climate models will become more accurate in the coming years with the use of exsascale supercomputing, now in development worldwide.”
Already in 2014 Mauri et al complained of enormous discrepancies between the real and simulated developments for precipitation and temperature in Europe 5000 years ago. Modelling of the past, i.e. the calibration, didn’t work at all. With so much disappointment one has to ask where all the confidence surrounding models being reliable forecasters comes from.
The paper’s abstract follows:
The influence of atmospheric circulation on the mid-Holocene climate of Europe: a data–model comparison
The atmospheric circulation is a key area of uncertainty in climate model simulations of future climate change, especially in mid-latitude regions such as Europe where atmospheric dynamics have a significant role in climate variability. It has been proposed that the mid-Holocene was characterized in Europe by a stronger westerly circulation in winter comparable with a more positive AO/NAO, and a weaker westerly circulation in summer caused by anti-cyclonic blocking near Scandinavia. Model simulations indicate at best only a weakly positive AO/NAO, whilst changes in summer atmospheric circulation have not been widely investigated. Here we use a new pollen-based reconstruction of European mid-Holocene climate to investigate the role of atmospheric circulation in explaining the spatial pattern of seasonal temperature and precipitation anomalies. We find that the footprint of the anomalies is entirely consistent with those from modern analogue atmospheric circulation patterns associated with a strong westerly circulation in winter (positive AO/NAO) and a weak westerly circulation in summer associated with anti-cyclonic blocking (positive SCAND). We find little agreement between the reconstructed anomalies and those from 14 GCMs that performed mid-Holocene experiments as part of the PMIP3/CMIP5 project, which show a much greater sensitivity to top-of-the-atmosphere changes in solar insolation. Our findings are consistent with data–model comparisons on contemporary timescales that indicate that models underestimate the role of atmospheric circulation in recent climate change, whilst also highlighting the importance of atmospheric dynamics in explaining interglacial warming.”
 
Share this...FacebookTwitter "
"It’s hard to be optimistic about British homes in the future. The lack of accommodation and the “broken” housing market are perpetually in the news. Millennials, even middle earners, are unlikely to own their own home.  The country’s new-builds are the smallest in Europe, with families “so cramped there isn’t enough space for them to live comfortably, sit down and eat together or even store necessities such as a vacuum cleaner”. As for the ageing population, the UK is said to be 
“woefully underprepared” for offering them appropriate housing and care.  These woes are variously blamed on “greedy developers” or Margaret Thatcher’s Right to Buy policy for council houses. Many commentators believe we need to build around 240,000 homes a year to solve the problem, while praising government schemes to increase ownership, such as the Help to Buy policy.  I want to argue for a different approach to making housing more viable. It focuses on a major cultural shift that has contributed to the housing problem, but too often gets overlooked.  Nearly one-third of the population now live on their own. Key demographic changes, such as young people deferring marriage and children until later in life and older women outliving their spouses and living on their own, has resulted in the average UK household size falling from almost three per household in 1970 to 2.4 for the past decade. This is part of why the number of households is rising, currently at about 1% a year.  UK household composition 1961-2011 Accompanying these shifts have been changes in people’s expectations. Householders nowadays consider a spare bedroom a necessity, while children are less likely to share a room with siblings than was once the case. Despite all the talk of “rabbit hutch homes”, domestic space per person is actually increasing. With this in mind, here are a couple of alternative responses to the housing crisis that begin to make sense:  The 1850 British census defined the family as “the wife, children, servants, relatives, visitors, and persons constantly or accidentally in the house”. This is an interesting reminder that past home life was much more communal than today.  The UK government already incentivises renting to lodgers through its Rent a Room scheme, offering homeowners up to £7,500 tax-free income per year if they let space. Taking a more creative approach to our perception of family and who we are willing to live with could make a big difference to the housing crisis. It would also be good for our wallets, not to mention wellbeing, helping people with loneliness and filling empty nests.   While the tiny house movement is becoming known for freeing people from mortgages and giving them more time to do what they love, this does not mean everyone needs to move into a 25 square metre home. Just downsizing from a large family house once you reach a certain age would help address our “ticking household bomb”.  The trouble is that people’s willingness to move house declines drastically after the age of 45 as they become more attached to their homes and wider community. Older householders often only move when they are forced by factors such as injury, illness or the death of their partner.  To combat this, organisations such as this one in the US, where there is a similar debate taking place, are already working to put a positive spin on downsizing and to market “downsizer homes” to people of a certain age. If we want to do something about the housing crisis, putting more emphasis on the upside of downsizing cannot be understated.  My research compares experiences of living in different sizes of house and household, and what motivates them. People expect more space for different reasons. Sometimes it makes it easier to enjoy living with your family members: more bathrooms per household reduces the potential for conflicts over who’s next in the shower, for instance. Or it may be because people have become accustomed to having a bedroom or study where they can do what they want and retreat from the company of others. We need to play up the counterarguments: why is bigger always better, for example, when we know it often locks us into unaffordable mortgages and more housework and gardening?  We also need to encourage people to accommodate these desires in different ways. Soundproofing walls can create a better sense of privacy than having more rooms. Sofa beds can create temporary guest bedrooms that need not be empty for the majority of the year.  The answer to the housing crisis is not to build vast numbers of new homes and help people to own more space than they need. Instead, we need to make do with less and learn to appreciate it."
"Most people enjoy the warmer, longer days that summer months bring – but plant allergy sufferers will have mixed emotions. Roughly one in five Europeans suffers from allergic reactions to tree, grass and weed pollen causing pollinosis, hay fever and allergic asthma. Allergies to substances such as pollen are driven by errors in the body’s immune system, which means it mounts a response to otherwise benign substances from plants. On first exposure to pollen, the body decides if some of the otherwise harmless proteins in the pollen are dangerous. If it decides they are, the immune system produces immunoglobin E (IgE) antibodies in a process called sensitisation.  The next time the body is exposed to pollen, it remembers the proteins and mounts another response. The IgE antibodies detect the pollen in, or on, the body, and cause cells to release histamine and a variety of other chemicals. This results in symptoms ranging from itchy eyes and nose, to production of mucous, inflammation and sneezing fits.  But while we know that “pollen” causes this response, at present we still don’t know all the types of pollen that cause the body to react.  In the UK, a daily pollen forecast is generated by the UK Met Office in collaboration with the National Pollen and Aerobiology Unit (NPARU), to help allergy sufferers. This forecast is created using data from a network of pollen traps which operate throughout the main pollen season (March to September) and measure how many pollen grains are present on a daily basis.  Pollen from different types of tree can be identified using microscopes, but grass pollen grains all look the same. As a result the pollen forecast for grasses (of which there are 150 types in the UK alone) is based on the broad, undifferentiated category of “grass”. That is despite grass pollen being the single most important outdoor aeroallergen.  We already know that different species of grass pollinate at different times in the year, and allergic reactions can occur at different times throughout the allergy season. What we need to figure out is whether allergies are caused by all species, specific species, or a combination of species of grasses. We also need to learn how pollen grains change in composition in time and space. While pollen is known for being very tough and is often well preserved in sediments, it can be very fragile in certain circumstances, such as bursting when in contact with rain drops.  To find out which grasses are linked to the allergic response, we need to know many things, such as where and when species of grass are releasing pollen. We also need to uncover how the pollen moves through the atmosphere, quantify the exposure of grass pollen species in time and space, and work out how allergies develop across broad geographical and temporal scales. Our Natural Environment Research Council (NERC) PollerGEN project team is now working on a way to detect airborne pollen from different species of allergenic grass. We’re also developing new pollen source maps, and modelling how pollen grains likely move across landscapes, as well as identifying which species are linked with the exacerbation of asthma and hay fever. We’re going to be using a new UK plant DNA barcode library, as well as environmental genomic technologies to identify complex mixtures of tree and grass pollens from a molecular genetic perspective. By combining this information with detailed source maps and aerobiological modelling, we hope to redefine how pollen forecasts are measured and reported in the future. We have just started the third year of pollen collection and hope to road test the combined forecasting methods over the next year. In the long run, our vision is to be able to provide specific pollen forecasts for grass, and unravel which species of grass pollen are most likely causing allergic responses. More broadly, we also want to provide information to healthcare professionals and charities, who can translate this information to help pollen allergy sufferers live healthier and more productive lives. In the meantime, if you suffer from pollen allergies, sneeze or wheeze during spring, speak to a doctor or pharmacist to prepare an action plan. You can also get support from Allergy UK, and information about the pollen forecast from the UK Met Office."
"Earlier this year, Trudi Beck, a general practitioner from Wagga Wagga, wrote to councillors across New South Wales urging them to acknowledge the climate crisis and declare a local emergency. Some responses were positive. Others less so.  Mark Hall, a Lachlan shire councillor and Baptist pastor, told Beck: “Stick to medicine – you have utterly no clue about climate science. Your email intrusion is truly not welcome.” So far, 84 jurisdictions in Australia covering about a quarter of the population – mostly cities and local government areas – have declared a climate emergency. The first elected body in the world to act, Darebin council in Victoria, is credited with starting a movement that is now supported by governments representing 800 million people worldwide, including the European Union and Bangladesh. In Australia, as ever when it comes to climate policy, the process has been polarising and frustrating. The leaders of one town might have recognised the climate crisis and committed to developing adaptation measures to help the community deal with the impacts of global heating. The next town over might have decided that climate change has nothing to do with local government business such as carting rubbish or fixing potholes. “We went from talking about the climate emergency, to now all of a sudden we’re living in it,” says Sarah Mollard, a general practitioner from the coastal NSW town of Port Macquarie. “It was incredibly unsettling to experience the sky going from blue to red in the space of a few hours. It’s extraordinarily unsettling to be in your home and see smoke haze in your home. This is my home, this is my safe space, and I can’t keep my children safe in it.” A few months ago, Mollard and other community members began to lobby for the Port Macquarie council to declare a climate emergency. In September, a relatively benign council motion to develop a “climate change action plan” was deadlocked at four-all. The mayor’s casting vote shelved the idea indefinitely. Since the vote, and since the November bushfire crisis that blanketed Port Macquarie in an orange haze, community members have turned up to council meetings, where residents are allowed to take the floor before formal debates, to discuss the climate change impacts of relevant items of business. In November, Mollard spoke about the need for the council to develop a heat plan. “It’s constructive in a sense. At the moment the council does not have someone on their payroll who is looking at the actions of council through a climate lens,” Mollard says. “I prefer gardening to public speaking, and would rather spend my day off work with the kids at the beach than rallying for our government to simply do its job. “As a doctor I am familiar with the term emergency. An emergency is a threat to people, property or society that has the potential to overwhelm them. “An emergency requires action to stop the problem from getting out of control and then return to safety. In an emergency, timing is critical – if you wait to act, the problem gets worse, more damage is done, the cost of repair is increased. “The more involved I’ve been getting the more I’ve had people coming up to me in the street and saying thank you. That’s a really strong indicator that people feel strongly about an issue.” One of the most remarkable aspects of the climate emergency movement is how it has put debate on the agenda in places that might have otherwise buried their heads in the nearest sandy riverbed. The Glen Innes Severn council has made a declaration and the mayor, Carol Sparks, has emerged from the bushfire crisis as a credible voice for regional people demanding climate action. Newcastle, the home of the world’s largest coal export port, has declared an emergency and has a policy to work towards a just transition. The Wollongong City c-ouncil – which along with Newcastle was for decades an industrial and steelmaking hub – has also recognised the climate crisis. In Queensland, where climate politics is most fraught amid a rush to support coal exports, only the Noosa council has declared an emergency. It also set a zero net emissions target by 2026. “I see it as both symbolic but also practical for sure,” Noosa mayor Tony Wellington says. “Of course, when we declared a climate emergency I did receive some hate mail. Let’s just say I did expect that. There wasn’t a large amount of pushback but there are inevitably in our community a number of people who [don’t accept climate change]. “Noosa has a history of being somewhat adventurous and pioneering as a council. We’re also for example the only council that has joined the alliance for gambling reform. We take a rather intrinsic view to development per se. We have a proud history of environmental conservation.” Two communities in the area, Noosa North Shore and Peregian Beach, were evacuated under threat from bushfires earlier this year. Wellington says the incidents were a “wake-up call” for the need to adapt the council’s plans and operations for a changed climate. “We’re acutely aware the impacts of climate change will resonate,” he says. “The costs of not preparing are far greater than doing something now.” Conservative Wagga Wagga, home of the deputy prime minister, Michael McCormack, earlier this year declared a climate emergency. A few weeks later, after an increasingly nasty debate, councillors rescinded that declaration. Outraged councillors would later demand the mayor, Greg Conkey, drive an electric vehicle to Sydney and back. He did and has said the journey was a success. Beck had been instrumental in building local support in Wagga Wagga, and in July, while the city was locked in debate about the declaration, she contacted other council areas soliciting support. “No way!” replied the Tenterfield deputy mayor, Don Forbes. When Beck responded by referring to the water security issues facing the Tenterfield community, which have got worse in the months since, Forbes asked not to receive further emails. When the same email reached Wollondilly shire councillor Simon Landow, a former candidate for Liberal preselection, he replied to say it was not the council’s role. “The term ‘climate emergancy’ (sic) … is very misleading to my residents of Wollondilly,” Landow wrote. “I respect your view to have an opinion on the theory that man is causing catastrophic global warming. “I would like you to respect my view that there is none, and I won’t be deviating from a stance thats is filled with so may (sic) flaws and misconceptions.” Another councillor, Murray Thomas from the Bland Shire, said in response that climatic changes had no relationship to carbon dioxide and would soon be proved to be caused naturally. “How do you propose explaining that [to] hordes of angry rorted,” Thomas said. “You’ve obviously made your choice, suggest you reconsider while you have the opportunity. Just tell ’em you were experimenting with psychotic drug samples … or in a moment of stress you fell victim to carbon rort hype.”"
"
Share this...FacebookTwitterWe’re always hearing from the European media how winters supposedly have been getting warmer. Yet when we look out the window and look at the hard data, the claim crumbles.
Atlanta ice box, 8-15 cm of snow!
Today snow is forecast across much of Europe. In the US Dr. Ryan Maue tweets if there is any place other than southern Florida where it is not snowing, and: “Atlanta suffering from 3-6″ of global warming today.”
Record low in Hokkaido
Kirye from Japan tweeted telling us that Ikutahara, Hokkaido Prefecture saw with a reading of -24°C the coldest December 9 temperature in at least 40 years:

Hokkaido temperatures giving CO2 the cold shoulder. Data Source: JMA.
-12°C and snow in the UK!
Meanwhile the UK Met Office has issued the amber warning as temperatures are forecast to plummet to -12°C and snow to pile up to 20 cm.
Of course all of the above do not come as any surprise to those who ignore the media climate propaganda and focus on the real data.
Cooling Central Europe winters past 30 years
The European Institute for Climate and Energy Friday posted on Germany mean winter temperature over the past 30 years. A vastly huge majority of Germans would tell you that Germany winters have gotten warmer – because of “global warming” – and so confirm they are only parroting the fake German news.
Yet, EIKE writes what the reality in Germany really is: “Winter has been giving global warming the cold shoulder“.
A chart of the last 30 winters from data from the German DWD national weather services show that German winters in fact have been cooling:


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Chart: Josef Kowatsch.
Going back more than 100 years, since 1910, it is also clear that German mean winter temperatures have nothing to do with CO2:

Germany mean winter temperatures with smoothed polynomial trend curve. Chart: Josef Kowatsch.
So before the German DWD weather services and media blurt claims that winters are becoming warmer, they first ought to look at their own data. There hasn’t been any warming since “global warming” started being an issue at the end of the 1980s.
No winter warming elsewhere
The following charts of winter mean temperature depict locations well beyond Germany, showing Central England, Sapporo Japan, Östersund in Central Sweden, and Oymyakon, Eastern Siberia respectively:



No warming trends seen over the past 30 years at these locations. And the eastern Siberian permafrost is not going anywhere anytime soon, Stefan Krämpfe’s chart tells us.
Of course global temperatures as a whole have warmed since 1980, but it’s nothing that comes close to what the alarmists would like to have us believe. The truth is that there’s been very little warming at all since the late 1990s.
 
Share this...FacebookTwitter "
"As I write, immense protests are taking place in India against the new anti-Muslim law and Hong Kong activists, who have been protesting for their own rights for months, stand in solidarity with the Uighur people being persecuted on the other side of China. The decade will end in protest. But who can look back a decade when a week in Trump time is like a century, and hardly anyone can remember the overstuffed chaos of the month before, let alone 2017, to say nothing of the remote era before he was president? Seriously, people keep forgetting what came before, which is why they fail to recognise patterns, consequences and the real power of movements. For instance, the wave of feminism called #MeToo is often treated as a sudden eruption out of nowhere when in fact it came out of a very specific somewhere: a ferocious upsurge of global feminism over the past decade that had been spawning news, protests, hashtags and action about feminism before #MeToo in 2017. That upsurge was itself the culmination of feminist analysis and action for decades before. All that happened in October of 2017 was that movie stars got involved.  But my real fear is that the 2010s will, like the 1980s, be misremembered through oversimplification. People dismissively say the 1980s were “Reagan”, as though several billion people on several continents were one reactionary old white man in America. Ronald Reagan was horrible, and his regime launched the reversal of decades of progress towards economic equality and security in the United States. But beyond and all around, the 1980s saw remarkable activism with immediate consequences – the overthrow of the Marcos regime in the Philippines through people power in 1986, the overthrow of the South Korean military dictatorship in 1987, the toppling of the whole eastern bloc of Soviet states in 1989, the beginning of the end of the apartheid era in South Africa (and powerful but unsuccessful uprisings in Burma and China). But a lot of groundwork was also laid for what was to come, with feminism, Aids activism and queer rights organising, and the beginning of a profound shift toward recognising racial and social issues in the environmental movement. Even deeper than that was the evolution of new, inclusive, less hierarchical, nonviolent organising strategies that rejected some of the failed tactics and principles of past activism and have been important ingredients in movements ever since. So one could dismiss this decade as the rise of Donald J Trump and authoritarians around the world (and yes, there have been plenty of them, from the Philippines to Hungary). But there have also been plenty of moves in the opposite direction. If protests had a slow start in the teens, they woke up fast with the Arab spring in January of 2011, one of the most powerful waves of anti-authoritarianism the world has ever seen. Regimes toppled in Tunisia, Egypt and Libya, with protests spreading from Sudan to Iraq. Of course the Syrian version turned into the long nightmare of civil war, and many of the nations involved in the Arab spring did not end up better in any simple way. But the protests made clear that even dictators backed by armies are not invulnerable, that ordinary people together sometimes have extraordinary power, that the longing for democracy is powerful in the Islamic world and that history is sometimes written by the vanquished when they cease to be vanquished. In October of the same year came Occupy Wall Street. The feminist upheaval has been global, with significant eruptions in Chile, Mexico, South Korea, Japan, Pakistan, Kenya and beyond in this decade. And Occupy was influenced by the Arab spring and anti-capitalist movements in Greece and elsewhere. And eventually outposts of Occupy were established in cities from Kyoto to Auckland to small towns in Alaska. The climate movement grew in power, reach and sophistication, often led by indigenous people, from the Arctic to Ecuador to the South Pacific and beyond. It became a powerful force that needs to grow yet more so in the next year and needs to win in the next decade. But what has been more important than any single movement in this decade is disillusionment – and I mean that in the positive sense: of letting go of illusions. Black Lives Matter, founded in 2013, and other anti-racist movements around the world shattered the sense that racism was over and linear progress was trustworthy and inevitable. Feminism went deeper into the nature of oppression and rose higher in its demands for equality. Same-sex marriage came into law in Argentina, Mexico, Iceland and Portugal in 2010, followed by many other countries including Britain and the US later in the decade. And the question of what equality meant for LGBTQ people also went deeper. So did questions about how gender is constructed and deconstructed as trans rights grew in visibility. What lay underneath all this disillusionment was a readiness to question foundations that had been portrayed as fixed, inevitable, unquestionable – whether that foundation was gender norms, heterosexuality, patriarchy, white supremacy, the age of fossil fuels or capitalism. To see beyond what we had seen before, or to change the “we” whose perceptions define the real, the important and the possible. With this came a capacity to understand more complex, subtle and hidden forms of oppression, and to think – encapsulated in that beautifully valuable word, contributed in 1989 by law scholar Kimberlé Crenshaw intersectionally – about how multiple identities overlap (and thus do multiple forms of oppression or privilege). The decade began in the wake of global economic collapse, and Occupy Wall Street was one of the reactions to the sheer greed, destructiveness and shortsightedness of the financial system. That the current economic arrangements don’t work for ordinary people has also prompted protests that don’t fit into a left framework. These included the gilets jaunes protests in France, the people who voted for Trump in the belief that he was an economic populist and the British voters who said yes to Brexit because they felt the system didn’t work for them. A surprising late-in-the-decade form of resistance has arisen among the employees at Facebook, Amazon and Google, protesting aspects of their corporations’ amorality. Employees at all three walked out as part of the September climate strike. The climate movement is inevitably an anti-capitalist movement. That capitalism is the best or only way to do things was, in the triumphalism after the collapse of the Soviet Union, affirmed again and again. That mood fell apart in the wake of episode after episode of corruption, destruction and failure – and the rise of a young generation ready to rethink the alternatives and, often, embrace versions of socialism. The nonviolent strategist George Lakey argues that polarisation brings clarity and a volatility that makes positive change more possible. We have the polarisation and the disillusionment, and with perspective about how we got here and when we won, we can claim the possibilities in the decade to come. • Rebecca Solnit is an author and journalist. Her latest book is Whose Story is This? Old Conflicts, New Chapters"
"Sweden has built the first smart road that will allow electric vehicles to charge as they drive. The eRoadArlanda pilot scheme, which covers two kilometres of road outside Stockholm, is an attempt to solve one of the biggest challenges that the transport industry faces. Namely, how to move freight and people in a way that neither damages the climate through greenhouse gas emissions nor the quality of air through nitrogen oxide pollution. The eRoadArlanda scheme is supposed to extend the range of electric vehicles beyond what was previously possible. Yet as an engineer, I have concerns about the durability of this road. Even more significantly, the cost of the technology and the disruption that building it causes is likely to restrict any mass scale replication. If this solution cannot be widely replicated then it is really no solution at all. Instead, a serious attempt to reduce hazardous emissions should focus on more practical solutions, such as the development of long-range batteries and the building of more electric charging stops. This new Swedish smart road will feel familiar to anybody who has played with slot car racing toys, such as Scalextric. Although, unlike slot cars, drivers will still have to steer vehicles. Instead, electric vehicles will collect power from charging rails set into the surface of the road. When on the road, a pickup arm attached to the bottom of the vehicle will extend downwards until it senses the rails, before slotting in and making electrical contact. This device is flexible, allowing the vehicle to move from side to side and the pickup can be retracted and reinserted in case a truck wants to overtake or turn off the road. But questions remain over the road’s durability. To prevent electrocution or damage by the elements, the live rails are hidden from view. This means that vehicles can only begin charging when the pickup is inserted into the rail slot. One can imagine the damage that might be caused to the road and the vehicle if the pickup fails to disengage cleanly before the vehicle attempts an overtaking manoeuvre.  An alternative technology that bypasses this problem is inductive transmission. Unlike eRoadArlanda, inductive transmission enables wireless charging. Conductors that are set into the road create an electromagnetic field, which then transmits energy to coils mounted to the bottom of vehicles. Such technology is already used by wireless phone chargers and can be adapted on a bigger scale for electric vehicles. Canadian manufacturer Bombardier has successfully demonstrated how this could work, and the US company Qualcomm has designed a system that has charged the racing cars of Formula E. Yet this technology, like eRoadArlanda, is particularly expensive and disruptive to install. Rail enthusiasts might claim that rail electrification has already solved the problem of clean, rapid movement of freight in a way that reduces road congestion. But rail transport can’t deliver goods from door to door and only the largest manufacturers are able to justify the operation of their own rail terminals. Even with rail electrification, there is a need to move freight between rail and road, just as we need to connect rail links with ferry ports and airports. Battery charging on the move seems an attractive solution, particularly at a time when we have too few electric vehicle charge points. But the cost of eRoadArlanda (£870,000 per kilometre) and the disruption it would cause if it were extended nationwide, makes other options more appealing. For example, long-range batteries and hydrogen fuel cell vehicles have the potential to overcome issues of price, disruption and durability. In fact, these options are already becoming cheaper. The price of lithium-ion batteries has fallen by 24% since 2016 and will fall further as more people adopt electric vehicles. As batteries improve and get cheaper, digging up our motorways seems an extreme solution. Even something as simple as building more electricity and hydrogen recharging truck stops would be preferable to vehicle-charging roads. All vehicles spend a large part of their lives stationary, so a far simpler and less disruptive solution is to charge vehicles at stops and destinations as electric cars currently do.  Whichever low carbon transport solution ends up dominating the market, we should not forget the options of transferring more freight to the railways or simply moving less of it around. This would require all of us to consume less and use products for longer. It might not be as exciting as building vehicle-charging roads, but extending the life span of products you already own, through recycling, refurbishment and remanufacturing is the cheapest and least disruptive way to reduce hazardous fumes."
"Almost every climate scientist agrees human-caused climate change is a major global threat. Yet, despite efforts over the past 30 years to do something about it, emissions keep increasing. Any successful coordinated international response will require action from businesses. However, some organisations, especially those in sectors that significantly contribute to environmental degradation such as the oil industry, seem rather reluctant to embrace the challenge. Those climate initiatives they have embraced were more often than not prompted by litigation risks or enforced by governmental policies rather than a result of an intrinsic “green” commitment. This isn’t the impression the industry likes to give off, of course, and it’s no wonder oil companies’ statements on corporate social responsibility and environmental reporting tend to highlight their greenest side. Yet the fact these documents give the oil firms the opportunity to construct their own narrative means they are a useful source for my research in applied linguistics. When a huge volume of language is analysed, features and patterns can emerge that would be invisible to the casual human reader.  My latest study looked at the “climate change reality” constructed by oil industry in its corporate reporting, what language was used to create this reality, and how this changed over time. This sort of analysis of language is important. Language not only mirrors the social world but acts as a lens through which objects, situations and people are given meaning. Features and associations that are foregrounded can point to some level of significance, while what is kept in the background or not mentioned at all can highlight a lack of interest.  This is why I used corpus-linguistic tools – essentially, using a computer to analyse vast amounts of text for certain patterns – to investigate nearly 500 corporate documents produced between 2000 and 2013 by major oil companies (including all the big names). This comprised some 14.8m words published in corporate social responsibility and environmental reports and relevant chapters in annual reports. That’s a lot of words – roughly equivalent to 25 copies of War and Peace.  Using software program Sketch Engine, I looked at how frequently the key corporate terms “climate change”, “greenhouse effect”, and “global warming” were used in each year to reveal how patterns of attention changed over time. My analysis shows that the most frequently adopted term in the studied sample is “climate change”, while other terms such as “global warming” and “greenhouse effect” are rarely used. The preference for “climate change” and near absence of “global warming” reflects patterns observed in public and media discourse, too. The use of the term “climate change” experienced peaks and troughs over time, with most mentions between 2004 and 2008, and fewer and fewer mentions since 2010. Less attention to climate change in public debates and overt anti-climate change attitudes on the parts of some governments in recent years might have contributed to the decline in attention given to climate change in corporate reporting. I then looked at words used alongside “climate change” to gather clues as to the company’s attitude towards it. This showed a significant change in the way it has been portrayed. In the mid-2000s, the most frequent associated terms were “tackle”, “combat” and “fight”, showing climate change was seen as a phenomenon that something could be done about.  However, in recent years, the corporate discourse has increasingly emphasised the notion of “risks”. Climate change is portrayed as an unpredictable agent “causing harm” to the oil industry. The industry tends to present itself as a technological leader, but the measures it proposes to tackle climate change are mainly technological or market-based and thus firmly embedded within the corporate world’s drive for profits. Meanwhile, social, ethical, or alternative solutions are largely absent. It seems that climate change has become an elusive concept that is losing its relevance even as an impression management strategy. The proactive stance of a decade earlier is now offset by a distancing strategy, often indicated through the use of qualifying words like “potential” or “eventual”, which push the problem into the future or pass responsibility to others. In doing so, the discourse obscures the oil sector’s large contribution to environmental degradation and “grooms” the public to believe that the industry is serious about tackling climate change."
"
Share this...FacebookTwitter‘Marked And Steady Increase’ 
In Modern Penguin Abundance

Perhaps because of their unique visual appeal and heavy representation in children’s books and movies (and climate blogs), penguins may subjectively rank second only to polar bears in their polar popularity.
The Polar Bear As ‘Global Warming’ Icon
Advocates of climate alarm have historically used images of forlorn and starving polar bears stranded on melting ice floes to spur human guilt and policy action.  In 2008, polar bears were even classified as endangered due to modeled expectations of their imminent demise.
According to recently published peer-reviewed scientific papers, however, polar bears have been defying the narrative that says dangerous anthropogenic global warming (DAGW) is targeting them for extinction.
That’s because in recent decades 92% of Canadian polar bear subpopulations have remained stable or increased, leading scientists to conclude that “it seems unlikely that polar bears (as a species) are at risk from anthropogenic global warming” (York et al., 2016).  Local Inuit populations even report that there are “too many polar bears now” (Wong et al., 2017).
How About Penguins?
Since the Arctic’s polar bears have not been cooperating with the DAGW narrative (by failing to die off in greater numbers), perhaps penguins, another beloved polar species, could take their place.  After all, the plight of Antarctica’s penguins has not received nearly as much worldwide attention or sympathy.
But scientists have found that penguins have not been cooperating with DAGW expectations either.
In recent decades, and over the course of the last 200 years, penguin numbers have either increased or remained stable.
Penguin Population Dynamics And Climate
Scientists have historically determined that increasing Adélie penguin numbers seem to coincide with warm periods, whereas cooling periods elicit population declines (Emslie et al., 2007;  Huang et al., 2009).
According to Yang et al. (2018), however, increases in penguin abundance coincide with cooling periods.  They note that there were higher Adélie penguin numbers in the Ross Sea region during the Little Ice Age (1600s to early 1800s) than during the 19th and 20th centuries.
Interestingly, though, these scientists also found that there has been no net change in penguin population since the 1800s, a determination that would not appear to fit the perspective that modern climate changes are unprecedented or even unusual.
Furthermore, when it’s considered that there has been no significant regional temperature change between the 1880s and mid-2000s, and that the Ross Sea has undergone a dramatic cooling trend (-1.59°C per decade) since 1979 (Sinclair et al., 2012), any decreasing population trend in recent decades would necessarily coincide with a cooling rather than warming climate.

In another paper published in the journal Nature Communications a few months ago, Che-Castaldo et al. (2017) analyzed 267 Adélie penguin colonies residing on the Antarctic continent and found their numbers have undergone a “marked and steady increase“ between 1982 and 2015.
Reindeer, Perhaps?


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




With both polar bears and penguins perpetually failing to support the narrative invoking deep concern about the species-depleting effects of anthropogenic global warming, perhaps a new animal icon foreshadowing the dangers of climate change will emerge at some point.
Reindeer are polar animals that are seasonally quite popular.   Perhaps they could take the place of polar bears and penguins.
Or not.
Bårdsen et al., 2017     The Pursuit of Population Collapses: Long-Term Dynamics of Semi-Domestic Reindeer in Sweden … We investigated the population dynamics of Swedish semi-domestic reindeer from 1945 to 2012 at the reindeer herding district-level (Sameby) to identify possible population collapses or declines […] but found no evidence of large-scale reindeer population declines and no visible synchrony across adjacent populations. Our findings were unexpected as both reindeer populations and the pastoral lifestyle face increased habitat loss, predation, fragmentation and climate change.

Pan-Antarctic analysis aggregating spatial
estimates of Adélie penguin abundance…
Che-Castaldo et al., 2017
[A]ggregated abundance [for 267 Adélie penguin colonies] across all sites in this region showed extended periods of both increasing and decreasing abundance over the last three decades [1982-2015]. 
We also find a long-term decline in abundance in the South Orkney Islands, following an initial period of increase in the early 1980s. In contrast, we found a marked and steady increase in abundance around the rest of the Antarctic continent, including both Eastern Antarctica and the Ross Sea.
Commensurate with other studies [Lynch et al., 2013], we find that the population of Adélie penguins on the Antarctic Peninsula declined between 2000 and 2008, though we found an unexpected rebound in abundance starting in 2008. This regional increase in abundance may, in part, be driven by sites in the Marguerite Bay area, where Adélie penguins are stable or even increasing. However, this increase may also reflect a cessation of regional warming on the Antarctic Peninsula since the late 1990s [Turner et al., 2016], which may benefit ice-dependent species like the Adélie penguin. 
We find that while Eastern Antarctica appears to have been increasing steadily in abundance since at least 1982, the increasing abundance of Adélie penguins in the Ross Sea is more recent, beginning in 2002.
 


Oceanographic mechanisms and penguin population increases 
during the Little Ice Age … southern Ross Sea, Antarctica
Yang et al., 2018
Adélie penguin populations as inferred from […] southern Cape Bird declined slightly from ∼1450 to ∼1600 AD, began to rise afterward and reached their highest level in ∼1700 AD, then declined with fluctuations to the lowest levels through ∼1900 AD. For the past 100 yr, Adélie penguin populations experienced a sharp rise and drop. 
Monitoring data have shown that Adélie penguins at Cape Bird had an increasing trend in the 1970s, likely linked with changes in sea-ice extent and polynya size, but also with variation in competition with minke whales (Ainley et al., 2005; Wilson et al., 2001). Our study suggests that the penguin populations increased in the 1960s as well, consistent with their research.
Over the past 500 yr at Cape Bird, Adélie penguin populations increased during the cold period (∼1600–1825 AD), which is inconsistent with the general pattern in other studies, for example, penguin populations increased when climate became warmer, and vice versa (Emslie et al., 2007; Huang et al., 2009; Sun et al., 2000).


Share this...FacebookTwitter "
"
Share this...FacebookTwitterFirst, at Twitter here Swiss high-profile meteorologist Jörg Kachelmann presented a video on how he thinks German public television failed to adequately warn the public before North Sea storm Xavier barreled through northern Germany on October 5.

Swiss meteorologist Jörg Kachelmann says he believes German media (image above) inadequately informed the public of the danger of storm ‘Xavier’. Seven Germans lost their lives to a storm that was “nothing out of the ordinary”. Image: ZDF German Public Television.
Media warnings of storm were inadequate
As a result 7 people were killed by falling limbs and trees. Some of these deaths could have been prevented had the media issued stronger warnings of the storm’s danger before it hit, believes Kachelmann, who in his twitter video pointed out that Xavier was not an unusual storm by any means and that there should not have been so many deaths.
Kachelmann said:
Why it happened has a bit to do with the media and what they could do. That’s the big difference to the USA when you look at the reporting there concerning hurricanes or even tornadoes, where the main reporting does not come after everything has happened — after all the deaths, injuries and everything laying around — but the main reporting is before where people are helped and told what to do and accompanies the people during this time with reporters out there in the storm with microphones, which here is ridiculed. But it helps.”
Kachelmann says the media here could have done this too to make the danger clear to people: “They could have done this here too. The storm did not just come as a surprise.”
The media hype comes afterwards, when it’s too late
As to why the German press did so little to warn the people of the storm, Kachelmann can only speculate: Maybe they were just “infinitely lazy“. Kachelmann thinks that had the media given stronger warnings, some of the lives would have been spared.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The average observer could say that German public media seems to have a habit of underrating storms before they hit, and then exaggerating them after they leave. For example, on Monday earlier this week – after Ophelia had already hit Ireland – North German NDR radio presented Ophelia as something that no one in Ireland “could recall ever happening“.
Certainly a bit of hyperbole here.
Here one could successfully argue that this is an exaggeration and that the German media are simply just too lazy to look back into the archives, or just aren’t interested in presenting accurate reports. Joe Bastardi at Weatherbell on Tuesday highlighted Ophelia in his Daily Update and showed that Ophelia “was not as bad as Debbie in 1961“.

Track of Hurricane Debbie in 1961, which was worse than Ophelia. Source: cropped from Weatherbell Saturday Summary.
Maybe the pre-storm downplaying and post-storm hyping is unwittingly intended by the German media. By neglecting to warn people beforehand, they get to blare out bigger, more spectacular headlines of death and destruction after the storm passes.
Of course no one seriously thinks it’s intentional on the German media’s part, yet the bottom line is that the German public is getting a distorted reporting both before and after the storm. They deserve far better for their exorbitant mandatory public television and radio fees.
Ophelia not unprecedented
And at his Saturday Summary of October 14, the veteran meteorologist blasted the hurricane hysteria coming from the usual US activists. He shows how Hurricane Faith in 1966 remained a hurricane far north of Ireland, and didn’t peter out until it reached the North Pole! There’s nothing unusual about Ophelia. Bastardi added:
I feel very strongly about these people who are using these storms […] for their agenda, and so what I’m doing here is that I’m letting you know that I’m showing you beforehand that there is visible evidence that this has happened before.”
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterPresident Donald Trump once again rejected “global governance” and rigid multi-nation trade deals before a packed and highly enthusiastic rally in Pensacola, Florida, yesterday.

Donald Trump leaves door wide open to a fair climate deal. Photo credit: Shealah Craighead, public domain photo.
The US President re-emphasized the importance of American national sovereignty and independence from “global bureaucrats” residing in foreign countries.
In his speech the President brought up the Paris Climate Accord, a deal he refused to sign early this year, thus setting off outrage among the UN, climate activists, global bureaucrats and Accord cash-beneficiary countries worldwide.
“Would have been one of the great catastrophes”
He reminded the packed audience that he had promised “to withdraw…from the horrible Paris climate accord” — another promise he has kept.
He said: “It costs us a fortune. China doesn’t start until 2030, I think. Russia doesn’t have to go back to like a recent date; they go back to somewhere like the 1990s, which was a high pollution time. Other countries we end up giving money to. This would have been one of the great catastrophes.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“I could come back to the Paris Accord”
But Trump left the door open for a better deal, one that would be much fairer: “And I can come back into the deal at a much better price. I could come back to the Paris Accord.”
Trump then repeated his commitment to “clean air” and “crystal clean water”. He said that if the USA pledged to accept the numbers set forth by the Paris Accord, “We would have to close factories and businesses in order to qualify by 2025. […] In the meantime some of those countries are spewing out stuff like you wouldn’t believe.”
US has already significantly cut emissions
Over the past decade the USA has already substantially cut back its “greenhouse gas” emissions, by an amount that equals Germany’s total annual output – something that never gets mentioned in the media. Paris Climate Accord-promoting Germany on the other hand, has not cut its emissions in almost a decade. Currently the USA emits roughly half as much as China does.
Performance by other climate protection-preaching western countries is not always what it is made out to be. Many western countries are meeting their targets by merely offshoring their CO2-emitting industries over to low environmental standards regions like China and India. The result: global CO2 emissions are in reality higher rather than lower.
Inefficient wind energy
Trump then mocked wind energy as being unreliable and inefficient, saying: “Windmills are wonderful. But you know when the wind doesn’t blow, they really do cause problems. ‘We have no energy this week! Well hopefully the wind will start blowing pretty soon!'”
Share this...FacebookTwitter "
"After years studying the climate, my work has brought me to Sydney where I’m studying the linkages between climate change and extreme weather events. Prior to beginning my sabbatical stay in Sydney, I took the opportunity this holiday season to vacation in Australia with my family. We went to see the Great Barrier Reef – one of the great wonders of this planet – while we still can. Subject to the twin assaults of warming-caused bleaching and ocean acidification, it will be gone in a matter of decades in the absence of a dramatic reduction in global carbon emissions.  We also travelled to the Blue Mountains, another of Australia’s natural wonders, known for its lush temperate rainforests, majestic cliffs and rock formations and panoramic vistas that challenge any the world has to offer. It too is now threatened by climate change. I witnessed this firsthand. I did not see vast expanses of rainforest framed by distant blue-tinged mountain ranges. Instead I looked out into smoke-filled valleys, with only the faintest ghosts of distant ridges and peaks in the background. The iconic blue tint (which derives from a haze formed from “terpenes” emitted by the Eucalyptus trees that are so plentiful here) was replaced by a brown haze. The blue sky, too, had been replaced by that brown haze. The locals, whom I found to be friendly and outgoing, would volunteer that they have never seen anything like this before. Some even uttered the words “climate change” without any prompting. The songs of Peter Garrett and Midnight Oil I first enjoyed decades ago have taken on a whole new meaning for me now. They seem disturbingly prescient in light of what we are witnessing unfold in Australia. The brown skies I observed in the Blue Mountains this week are a product of human-caused climate change. Take record heat, combine it with unprecedented drought in already dry regions and you get unprecedented bushfires like the ones engulfing the Blue Mountains and spreading across the continent. It’s not complicated. The smoke is so thick in Katoomba tourists are opting for photos with billboards, rather than the Three Sisters themselves. @abcsydney pic.twitter.com/MOWvH8UBgs The warming of our planet – and the changes in climate associated with it – are due to the fossil fuels we’re burning: oil, whether at midnight or any other hour of the day, natural gas, and the biggest culprit of all, coal. That’s not complicated either. When we mine for coal, like the controversial planned Adani coalmine, which would more than double Australia’s coal-based carbon emissions, we are literally mining away at our blue skies. The Adani coalmine could rightly be renamed the Blue Sky mine.  In Australia, beds are burning. So are entire towns, irreplaceable forests and endangered and precious animal species such as the koala (arguably the world’s only living plush toy) are perishing in massive numbers due to the unprecedented bushfires. The continent of Australia is figuratively – and in some sense literally – on fire. Yet the prime minister, Scott Morrison, appears remarkably indifferent to the climate emergency Australia is suffering through, having chosen to vacation in Hawaii as Australians are left to contend with unprecedented heat and bushfires. Morrison has shown himself to be beholden to coal interests and his administration is considered to have conspired with a small number of petrostates to sabotage the recent UN climate conference in Madrid (“COP25”), seen as a last ditch effort to keep planetary warming below a level (1.5C) considered by many to constitute “dangerous” planetary warming. But Australians need only wake up in the morning, turn on the television, read the newspaper or look out the window to see what is increasingly obvious to many – for Australia, dangerous climate change is already here. It’s simply a matter of how much worse we’re willing to allow it to get. Australia is experiencing a climate emergency. It is literally burning. It needs leadership that is able to recognise that and act. And it needs voters to hold politicians accountable at the ballot box. Australians must vote out fossil-fuelled politicians who have chosen to be part of the problem and vote in climate champions who are willing to solve it. Michael E Mann is distinguished professor of atmospheric science at Pennsylvania State University. His most recent book, with Tom Toles, is The Madhouse Effect: How Climate Change Denial Is Threatening Our Planet, Destroying Our Politics, and Driving Us Crazy (Columbia University Press, 2016)."
"Puffins are facing a perilous future. Population numbers have fallen sharply, and there are even fears the sea bird could be heading towards extinction within the next 100 years.  A much loved and enigmatic creature, puffins are easily identified by their wonderfully coloured beaks. They waddle around in a characterful fashion and make the strangest of noises. Their endearing features have been used as the symbol of children’s books, and to illustrate many stamps – but they are now also appearing on lists of endangered species.   On Britain’s Farne Islands, numbers have gone down 12% on average over just five years, with one island’s population falling by 42%. The common puffin, named after its puffed-up swollen appearance (although its scientific name, Fratercula arctica, arises from its resemblance to a friar wearing robes) has an extensive range across the northern hemisphere, with breeding colonies from Norway to Newfoundland.  Around 90% of the global population is found in Europe, with 60% of the population breeding in Iceland (which is also home to a tradition which involves children rescuing young, wayward puffins – “pufflings” – and returning them to the safety of the sea). The UK is home to 10% of the global puffin population, breeding on many islands and mainland coastal areas. Although there are around 450,000 puffins in the UK, the species is threatened with extinction due to their rapid and ongoing population decline. Recent surveys of the Farne Islands revealed that despite a steady increase over the previous 70 years, numbers have declined by as much as 42% over the past five years. Unfortunately, we know very little about the ecology of the puffin outside the breeding season. Although the birds amass in large numbers to breed, they spend two-thirds of their life alone, out in the north Atlantic sea. Consequently, they are very difficult to monitor. Firstly, although puffins live for a fairly long time (the oldest recorded so far reached the age of 34), their breeding population is limited to a small number of sites. They also have a low reproductive rate, laying just one egg a year, which makes them particularly vulnerable to adverse changes in the environment and means they can take a long while to recover from negative impacts. They are also hunted – by humans and other animals. Smoked or dried puffin is considered a delicacy (or a flavouring for porridge) in some places, such as Iceland and the Faroe Islands. But although they were once over harvested by people, hunting is now maintained at a sustainable level. During the breeding season, puffins nest in burrows on clifftops. Although this offers the nest protection from aerial predators, such as gulls, chicks and eggs are not safe from mammals, including weasels and foxes. On Lundy Island in the Bristol Channel, the population of puffins fell to just 10 pairs, but since the eradication of rats there, things are looking up. Nevertheless, the Arctic skua can be a particular problem as it steals food from adult puffins which is intended for their young. Living on the open ocean makes the puffin highly susceptible to pollution such as oil spills. After the Torrey Canyon oil spill in 1967, the number of puffins breeding in France the following year decreased by a massive 85%. The puffin feeds almost entirely on small fish, including sandeels, herring and capelin, which make up over 90% of the diet of pufflings.  The birds have a specialised beak with backwards facing spines, which prevents their prey (up to around 60 fish at a time) from falling out of their mouths when foraging. But in years where the main food source is low, many chicks starve to death.   Puffins have also suffered increased mortality from the rising frequency and intensity of extreme weather events associated with climate change. A recent succession of severe storms caused 54,000 seabirds, half of which were puffins, to be washed up along coasts. Starvation was cited as the main cause of death.  Sea temperatures have increased over the past 30 years, causing indirect effects on puffin survival. The rise in temperature decreases the abundance of plankton, which in turn leads to a reduction in the growth and survival of young sandeel and herring on which the puffins rely, particularly during the breeding season. Conditions in the North Sea are even causing some puffins to travel into the Atlantic, rather than the North Sea, in search of food – a perilous trek involving greater distances and different habitats. It seems that a combination of factors are to blame for the decline in puffins, but the reduction in their food supply, particularly as a result of increased sea temperatures, appears to be the main culprit.  We need to continue monitoring puffins worldwide to better understand factors affecting populations. Hopefully, we can put measures in place to minimise pollution, reduce introduced predators and promote sustainable harvesting to try and ensure that the fate of this wonderful bird is not the same as that of the dodo."
"The UK government has published a new clean air strategy for consultation. The document sets out plans to tackle emissions from a range of sources, including agriculture, industry and even wood-burning stoves. It all adds up to a subtle but important shift in emphasis away from simply meeting air quality targets to also reducing wider impacts on health and the environment. In Britain, much of the debate about air quality has focused on roads and the issue of local roadside hotspots where air pollution exceeds legal limits. It is no surprise that the government’s critics have repeatedly pointed to a lack of action on diesel cars.  While this is understandable, it is one area where things are slowly getting better, partly through local actions and partly because, on average, newer vehicles actually do emit less pollution than the vehicles they are replacing. The highest concentrations of nitrogen dioxide (NO₂) in UK cities were typically seen in 2010 and in many places have declined since then. Some of the forecasts of future road transport emissions may also have been overly pessimistic. The strategy is banking on this trend continuing, and it restates the long-term target of phasing out fossil fuel-only vehicles by 2040.  This is part of a wider focus which acknowledges that air pollution is much more than just a roadside problem. One eye-catching component concerns fine particulate matter, known as PM2.5 since the particles are less than 2.5 micrometers across. These are really too tiny to see with the eye, yet present a major health risk as particles this small can easily find their way deep into the lungs and finally the bloodstream.  The strategy now recognises the most stringent World Health Organisation limits for PM2.5, and includes an ambition to halve the number of people living in areas with concentrations above that limit. This would mean the UK was working toward meeting tougher PM2.5 standards than virtually every other industrialised nation. The strategy sets out plans for reducing emissions across different source types rather just tackling each pollutant in isolation, in some cases with proposals for specific actions, in others more general ambitions for reductions. Interesting examples in the strategy include a proposed gradual retirement of diesel trains and the voluntary labelling of solvents in consumer products. It also looks serious about finally tackling the long-standing issue of ammonia emissions from agriculture, and it raises the emerging health issue of managing indoor air quality. This sort of multi-pollutant approach makes sense, since different classes of chemical can interact with one another to form secondary air pollution. For instance, nitrogen oxides (NOx) from combustion combine with ammonia from farming to create a substantial fraction of the particulate matter that is found in the air. That same NOx can also combine with gaseous solvents to form ozone. Real improvements can only be achieved by simultaneously reducing emissions from these disparate sectors. While the government will always be measured first against its ability to deliver good air quality at a local level, the strategy reflects that the health and ecological impacts do not stop at the borders of individual countries. Although less reported on than the ambient air quality standards, the EU also sets specific limits on the total emissions that each country can make, to minimise the spread of pollution between countries.  The need to manage air pollution at an international level is frequently cited in the new strategy. Limits on pollution emissions from each EU country are set in the National Emissions Ceiling Directive (NECD) and are straightforward to understand, if complex to actually estimate or measure. On this issue Brexit will not change things since the UK’s commitments to reduce emissions are mirrored in the standalone UNECE Convention on Long Range Trans-boundary Air Pollution, to which the UK (along with 50 other countries, including the US and Canada) is a signatory.  The location of the UK means that it has the good fortune to be less affected by trans-boundary pollution than many other countries – there is a very large and generally very clean Atlantic ocean upwind. But even Britain feels the effects on occasion and many of its smoggiest days, particularly in southern England, are exacerbated by pollution flowing in from mainland Europe. A focus in the strategy on reducing emissions, even when local air quality targets have been met, reflects that UK has signed up to new binding limits for trans-boundary pollution in 2020 and 2030, and these will require close to a halving of emissions of some pollutants. One reason why NECD has rarely been in the headlines is that up until now, meeting these national targets has been quite straightforward, as compared to ambient concentrations where limits have been frequently exceeded. By the 2020s it seems likely that ambient concentrations will have fallen further. At this point, meeting the increasingly tough international emissions targets may become the new legal compliance challenge. There will no doubt be debate over whether the strategy moves far or fast enough to clean up the outstanding urban hotspots. Quantifying the broader success, or failure, of the strategy over the long-term will however be complex. It’s relatively easy to measure the concentration of pollutants in the air, but it is more difficult to devise metrics that capture all the intended benefits, such as improvements to health, productivity and to the wider environment. This is where the government will need to work hard to convince those that may have to pay, that investment in emissions reduction is money well spent."
"Earlier this year, an unusual weather pattern dubbed the Beast from the East covered much of Britain in heavy snow. But once the beast had passed, things soon returned to normal and, at the beginning of March, the temperature in London jumped by more than 10℃ in just two days. Water pipes that had been frozen solid quickly thawed, and the sudden flood soon overwhelmed the capital’s creaky infrastructure, causing many pipes to burst. More than 20,000 homes in the city were left without water, and residents had to queue for handouts.  Could this become a common sight in future? The UK’s Environment Agency certainly thinks so, as it warns in a new report that England could suffer major water shortages by 2030 and that London is particularly at risk. The BBC agrees, placing London on its recent list of 11 cities most likely to run out of drinking water along with the likes of Cape Town, where an ongoing water crisis has caused social and economic disruption.  London is unlikely to experience such shortages this summer. It is the winter (not the summer) weather that determines whether or not the city runs out of water, and winter 2017/18 had plenty of rain. But what happens after a dry winter? At Oxford’s Environmental Change Institute, colleagues and I have addressed the question of how to prevent London from becoming the next Cape Town. Our research shows that if no action is taken the city is indeed set to experience more frequent and severe water shortages in the future. This is mainly down to population growth, but climate change complicates things further as it will mean more frequent and intense droughts. In agreement with the plans developed by Thames Water – the private utility responsible for providing water and sewage services for most Londoners – our research shows that aggressive demand management to reduce consumption and losses in the distribution system (called leakage) is a priority to be implemented immediately. But reducing leaks from London’s old water pipes is not an easy task. Over the past few years, Thames Water has missed its leakage reduction targets. In 2017, the failure to meet these leakage reduction commitments cost the water company an £8.55m fine from the water regulator, only a fraction of the £100m the water company paid investors in dividends in the same year. Recognising the scale of investment and effort required, the company now says it is directing all its resources towards upgrades and maintenance rather than dividends. But there are limits to what can be achieved just by fixing leaky pipes or getting people to water their lawns less often. Though such measures are useful, they will not safeguard London’s water supplies against the more extreme combinations of growth and climate change.  Instead, the city’s water managers have been thinking about innovative ways to augment supplies. Potential solutions include building new reservoirs or transferring water from other parts of the country. More radically, London could start recycling its wastewater back into the river Thames. This would involve advanced treatment of wastewater from a sewage treatment works that is then returned to the Thames river downstream of an abstraction point. This would allow for more abstraction upstream, without compromising the environment’s water needs. How should London choose between these different alternatives? The city needs something that’s not too expensive, that keeps residents happy with the price, taste and appearance of the water, while also reducing the risk of the taps running dry.  My colleagues and I looked at the various options – new reservoirs, water transfers, desalination and recycling – and the model we developed shows that the recycling of treated wastewater back into the river makes most sense from an economic and risk reduction standpoint. Water recycling works in Singapore, where water is reused time and again, thus closing the loop between supply and demand – an example of the circular economy. Yet all this requires a change of thinking. Traditionally, investments in new pipes or reservoirs are based on estimates of future water availability and needs. These estimates are based on past observations, which means that water engineers look at how much rain there was in the past and then assume that there will be as much in the future. Typically, this results in infrastructure that delivers a secure supply of water at the lowest cost possible – under “normal” conditions. However, the future will be significantly different from anything imagined when water supply systems were first built. We will have to leave more water in the rivers for aquatic ecosystems to thrive. We will have to deal with more erratic rainfall. To prevent London from becoming the next Cape Town, individual residents will have to use water as wisely as possible. And their water managers will have to focus on what will work even in an era of significant climate change."
"
Share this...FacebookTwitterOne thing is clear: Germans were fooled and deceived by politicians and activists into thinking that the transition to renewable energies would not cost much, reduce pollutants, create a clean environment, improve the climate and create many jobs.
None of these have come true.
Electricity prices have skyrocketed, the landscape is being industrialized and Germany has not reduced its greenhouse gases in more than 7 years. Moreover the climate is still the same. Now Germany’s industrial base is eroding.
Today we will look at the first point: cost. Yesterday the online industry journal Deutsche Mittelstand Nachrichten (Midsize Company News) here carried the headline:
“Association of Energy: Electricity Prices To Rise Significantly”
So the bad news continue, and this will further adversely impact consumers, small businesses and the all-important Mittelstand.
And because the Mittelstand employ some 70% of all workers in Germany, most of them highly skilled and well-paid, the news is bad. The Mittelstand is already facing crisis on a number of fronts. First is the lack of skilled workers on the labor market. Second: many of these companies are now being handed down to the next generation, but there are no successors. In fact Chinese companies have been busily snapping up the companies along with their patents and technical expertise.
Now, thirdly, comes the extreme energy prices (and volatile supply) – thanks to Germany’s mad and poorly thought-out rush into utopian green energies.
Rising feed-in costs


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The main factor driving the higher prices remains the EEG green energy feed-in act, the site reports. Association head Christian Otto told German daily Bild that the feed-in subsidy will rise to 7 cents per kwh (currently 6.88 cents). 
Three of the four power transmission grid operators have already announced an increase in the grid fees.”
The four German grid operators are Amprion, TransnetBW, Tennet and 50Hertz. Only East Germany-based 50Hertz does not plan to raise the fees for the time being.
Uncompetitive
The higher feed-in surcharges make push German electricity prices to 30 cents a kwh, almost three times more expensive than power in the USA, for example. Little wonder that some are now calling to “make Germany great again”.
Moreover, citing analysts, the site reports that heating oil as well has become 11 percent more expensive, and warns more increases lie ahead as winter approaches.
Grid instability adding to costs
The Deutsche Mittelstand Nachrichten site cited the head of the BDEW energy association, Stefan Kapferer, who blasted the “constantly more frequent and expensive interventions that are needed to keep the grid stable due to the fluctuating feed-in of renewable energies“.
 
Share this...FacebookTwitter "
"…all progress in capitalistic agriculture is a progress in the art, not only of robbing the labourer, but of robbing the soil; all progress in increasing the fertility of the soil for a given time, is a progress towards ruining the lasting sources of that fertility. – Karl Marx, Capital vol 1 Following the collapse of the Soviet Union and an economic shift in China it seemed that capitalism had become the only game in town. Karl Marx’s ideas could safely be relegated to the dustbin of history. However the global financial crash of 2008 and its aftermath sent many rushing back to the bin.  For good or ill, the German philosopher’s ideas have affected our world more profoundly than any other modern social or political thinker. Yet on Marx’s recent 200th birthday, discussion of his continuing relevance was still dominated by “traditional” understandings of Marxism. Commentators, whether hostile or sympathetic, focused on his critique of the exploitation and inequality of capitalism and imperialism, and the struggle to transform society in a socialist direction.  Sadly, there was little – far too little – on Marx’s thinking on the relations between humans and nature.  After all, the steady but accelerating destruction by modern capitalism of the very conditions which sustain all life, including human life, is arguably the most fundamental challenge facing humanity today. This is most widely recognised in the shape of one of its most devastating symptoms: climate change. But there is much more to it, including toxic pollution of the oceans, deforestation, soil degradation and, most dramatically, a loss of biodiversity on a geological scale. Some will say that these are new problems, so why should we expect Marx, writing more than a century ago, to have had anything worthwhile to offer to us today? In fact, recent scholarship has demonstrated that the problematic, often contradictory relationship between humans and the rest of nature was a central theme in Marx’s thinking throughout his life. His ideas on this remain of great value – even indispensable – but his legacy is also quite problematic and new thinking is needed.  Marx’s early philosophical manuscripts of 1844 are best known for developing his concept of “alienated labour” under capitalism, yet commentators hardly ever noticed that for Marx the fundamental source of alienation was our estrangement from nature. This began with enclosure of common land, which left many rural people with no means of meeting their needs other than to sell their labour power to the new industrial class. But Marx also talked of spiritual needs, and the loss of a whole way of life in which people found meaning from their relationship to nature. The theme running through his early manuscripts is a view of history in which exploitation of workers and of nature go hand-in-hand. For Marx, the future communist society will resolve the conflicts among humans and between humans and nature so that people can meet their needs in harmony with one another and with the rest of nature: Man lives on nature – means that nature is his body, with which he must remain in continuous interchange if he is not to die. That man’s physical and spiritual life is linked to nature means simply that nature is linked to itself, for man is a part of nature. In these writings Marx makes vital contributions to our understanding of the human-nature relationship: he overcomes a long philosophical tradition of viewing humans as separate from and above the rest of nature, and he asserts the necessity for both survival and spiritual well-being of a proper, active relationship with the rest of nature. At the same time he recognises this relationship has gone wrong in the capitalist epoch. In his later writings Marx develops this analysis with his key concept of “mode of production”. For Marx, each of the different forms of human society that have existed historically and across the globe has its own specific way of organising human labour to meet subsistence needs through work on and with nature, and its own specific way of distributing the results of that labour. For example, hunter-gatherer societies have usually been egalitarian and sustainable. However feudal or slave-owning societies involved deeply unequal and exploitative social relations, but lacked the limitlessly expansive and destructive dynamic of industrial capitalism.  This concept of “modes of production” immediately undermines any attempt to explain our ecological predicament in such abstract terms as “population”, “greed” or “human nature”. Each form of society has its own ecology. The ecological problems we face are those of capitalism – not human behaviour as such – and we need to understand how capitalism interacts with nature if we are to address them.  Marx himself made an important start on this. In the 1860s he wrote about soil degradation, a big concern at the time. His work showed how the division of town and country led to loss of soil fertility while at the same time imposing a great burden of pollution and disease in the urban centres.  Modern writers have developed these ideas further, including the late James O’Connor, the sociologist John Bellamy Foster, who identified an endemic tendency of capitalism to generate an “ecological rift” with nature, and those in the UK associated with the Red Green Study Group. I suggested above that Marx’s ideas were indispensable but also problematic. There are places where he appears to celebrate the huge advances in productivity and control over the forces of nature achieved by capitalism, seeing socialism as necessary just to share the benefits of this to everyone. Recent scholarship has challenged this interpretation of Marx, but historically it has been very influential. It is arguable that the disastrous consequences of the Stalinist drive for rapid industrialisation in Russia came from that interpretation.  But there is another point. The newer ecological marxists argue, rightly, that capitalism is ecologically unsustainable, and that socialism is necessary to establish a rational relationship to the rest of nature. However, to build a movement capable of transforming society in this way, we need to recall Marx’s early emphasis on both the material and spiritual needs that can be met only by a fully rewarding and respectful relationship to the rest of nature: in short, we need a Marxism that is green, as well as ecological."
"
Share this...FacebookTwitterAtmospheric Scientists Slam Fundamentals
of the Anthropogenic Global Warming Theory

Scafetta et al., 2017    Natural climate variability, part 1: Observations versus the modeled predictions
[T]he AGWT [Anthropogenic Global Warming Theory] was globally advocated by the IPCC in 2001 because it appeared to be supported by the ‘infamous’ Hockey Stick temperature reconstructions by Mann et al. [10]* and by specific computer climate models mainly based on radiative forcings [4,11]. Those temperature reconstructions claimed that only a very modest change in the Northern Hemispheric climate had occurred during the pre-industrial times from A.D. 1000 to 1900, while an abrupt warming did occur just in the last century. Energy balance and general circulation climate models (GCM) were used to interpret the Hockey Stick climatic pattern as due mostly to anthropogenic greenhouse gas emissions such as CO2 because of coal and oil fuel consumption, which has been accelerating since the beginning of the 20th century [11].
However, since 2005 novel Northern Hemisphere proxy temperature reconstructions were published revealing the existence of a large millennial oscillation that contradicts the Hockey Stick temperature pattern
* see reference list



Wilson et al., 2016

Wilson et al., 2016


Abrantes et al., 2017

The new findings were consistent with alternative climatic and solar activity records showing that a quasi-millennial oscillation occurred throughout the entire Holocene for the last 10,000 years [16, 17].
The severe discrepancy between observations and modeled predictions found during the 1922-1941 and 2000-2016 periods further confirms, according to the criteria proposed by the AGWT advocates themselves, that the current climate models have significantly exaggerated the anthropogenic greenhouse warming effect.
In 2009 AGWT advocates acknowledged that: “Near-zero and even negative trends are common for intervals of a decade or less in the simulations, due to the model’s internal climate variability. The simulations rule out (at the 95% level) zero trends for intervals of 15 year or more, suggesting that an observed absence of warming of this duration is needed to create a discrepancy with the expected present-day warming rate” [24]. Thus, according to the AGWT advocates’ own criteria, a divergence between observations and climate models occurring at the bi-decadal scale would provide strong convincing evidences that the GCMs used to support the AGWT are severely flawed.
In conclusion, the temperature records clearly manifest several fluctuations from the inter-annual scale to the multidecadal one. Detailed spectral analyses have determined the likely existence of harmonics at about 9.1, 10.5, 20 and 60- year periods [7, 8, 9]. By contrast, the CMIP5 GCMs simulations used by the IPCC (2013) to advocate the AGWT show a quite monotonic accelerating warming since 1860, which is at most temporarily interrupted by volcano eruptions and only slightly modulated by aerosol emissions. Thus, the models are not able to reproduce the natural variability observed in the climate system and should not be trusted for future energy planning [33].


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




It has been suggested that non-radiative physical processes connected with solar activity and the “resonant” orbital motions of the moon and the planets can cast light on the otherwise incomprehensible temperature fluctuations [34, 35]. In fact, the magnetic activity of the sun and, probably, also the planetary motions modulate both the solar wind and the flux of the cosmic rays and interstellar dust on the earth with the result of a modulation of the clouds coverage.

Scafetta et al., 2017  Natural climate variability, part 2: Observations versus the modeled predictions

Several studies based on general circulation model (GCM) simulations of the Earth’s climate concluded that the 20th century climate warming and its future development depend almost completely on anthropogenic activities. Humans have been responsible of emitting in the atmosphere large amount of greenhouse gases (GHG) such as CO2 throughout the combustion of fossil fuels. This paradigm is known as the Anthropogenic Global Warming Theory (AGWT).
[S]ince 2001 AGWT was actually supported by the belief that the “hockey stick” proxy temperature reconstructions, which claim that an unprecedented warming occurred since 1900 in the Northern Hemisphere, were reliable [2,5] and could be considered an indirect validation of the available climate models supporting the AGWT [6]. However, since 2005 novel proxy temperature reconstructions questioned the reliability of such hockey stick trends by demonstrating the existence of a large millennial climatic oscillation [7-10]. This natural climatic variability is confirmed by historical inferences [11] and by climate proxy reconstructions spanning the entire Holocene [12, 13]. A millennial climatic oscillation would suggest that a significant percentage of the warming observed since 1850 could simply be a recovery from the Little Ice Age of the 14th – 18th centuries and that throughout the 20th century the climate naturally returned to a warm phase as it happened during the Roman and the Medieval warm periods [9, 11, 14-16].
We … critically analyze the year 2015-2016, which has been famed as the hottest year on record. We show that this anomaly is simply due to a strong El-Niño event that has induced a sudden increase of the global surface temperature by 0.6 °C. This event is unrelated to anthropogenic emissions. In fact, an even stronger El-Niño event occurred in 1878 when the sudden increase of the global surface temperature was 0.8 °C.
Herein, the authors have studied the post 2000 standstill global temperature records. It has been shown that once the ENSO signature is removed from the data, the serious divergence between the observations and the CMIP5 GCM projections becomes evident. Note that Medhaug et al. [28] claim that the models agree with the post 2000 temperature trend. However, these authors did not remove the ENSO signal and used annual mean temperature records up to 2015 that camouflage the real nature of the 2015-2016 ENSO peak.
Moreover, a semi-empirical model first proposed in 2011 based on a specific set of natural oscillations suggested by astronomical considerations plus a 50% reduced climatic effect of the radiative forcing, which includes the anthropogenic forcing, performs quite better in forecasting subsequent climate changes. Thus, the GCMs used to promote the AGWT have been also outperformed [by a natural oscillation/astronomical/anthropogenic “semi-empirical” model][15]. This result is indeed consistent with recent findings. In fact, although the equilibrium climate sensitivity (ECS) to CO2 doubling of the GCMs vary widely around a 3.0°C mean [3,4], recent studies have pointed out that those values are too high.
Since 2000 there has been a systematic tendency to find lower climate sensitivity values. The most recent studies suggest a transient climate response (TCR) of about 1.0 °C, an ECS less than 2.0 °C [20] and an effective climate sensitivity (EfCS) in the neighborhood of 1.0 °C [29].
Thus, all evidences suggest that the IPCC GCMs at least increase twofold or even triple the real anthropogenic warming. The GHG theory might even require a deep re-examination [30].

Share this...FacebookTwitter "
"Japanese knotweed (Fallopia japonica var. japonica) was introduced into Europe in the mid-19th century by Philipp Franz Balthasar von Siebold, a German botanist and physician living in The Netherlands. In 1850, von Siebold sent a specimen of Japanese knotweed to Kew Gardens in London and by 1854, knotweed had travelled as far as the Royal Botanical Gardens in Edinburgh. Just over 30 years later, in 1886, Japanese knotweed was found growing in the “wild” for the first time, in Maesteg, south Wales. Now, the plant is found in over 70% of UK hectads – 10km × 10km grid squares used to measure animal and plant distributions – although it is worth noting that this does not necessarily indicate high abundance in all areas. It is also established across mainland Europe, North America and the southern hemisphere. This global spread is astonishing – particularly as, to date, it has only occurred via plant fragments (vegetative) and not from (viable) seed. In the UK alone, it is estimated that controlling Japanese knotweed costs the economy around £170m every year. There are at least 15 different active control methods and herbicides used in the country, and an extensive control industry has built up around the plant.  But, until now, there has never been a study of the scale needed to truly test how effective these treatments are. They are being sold to home and land owners with no unbiased research to back up their worth. However, we have recently completed the largest Japanese knotweed field trial ever conducted globally, and working with academic and industry partners, found the best way of treating the plant long term. The key to our approach was to understand the plant, in order to control it. Japanese knotweed’s ease of spread and rapid growth from a deep rhizome (root) system was initially prized for planting schemes. However, from an ecological perspective, these plant traits are precisely why it has become a huge problem for native biodiversity and, increasingly, wider society. Rapid growth from early in the growing season (February onwards in the UK) excludes most native plants from well-established Japanese knotweed patches (known as “stands”). This is because the dense canopy of leaves shades out other species. This shading effect is amplified as insects do not graze on knotweed plants, and native diseases don’t keep the plant in check either. Knotweed also produces a thick leaf litter, and chemicals that inhibit the germination and growth of native plants. It dominates non-native habitats, displacing native plants and altering how local ecosystems function – for example, in soil nutrient cycling. During our research, it became apparent that because a Japanese knotweed stand contains significant underground and spreading biomass, we would need to do large field trials, to reflect real world conditions. So, we set up 58 different 15 metre × 15 metre (225 square metre) field trial plots, located in south Wales (UK), and repeated each method three times in these areas.  Between 2011 and 2016, we tested all control methods and herbicides used for controlling knotweed in the UK, Europe and North America – 19 in all. This experiment continues to be unique in terms of scale, duration and scientific rigour. But it is plain to see why this research has not been conducted before – the commercial cost has been (conservatively) estimated at £1.2m. However, given the ongoing costs of managing knotweed in the UK, the value of the experiment is self-evident.  Our research has highlighted the most appropriate way to treat established Japanese knotweed stands and, surprisingly, a number of other methods which are poor or totally ineffective at field scale. We now know that glyphosate-based herbicides are significantly better than all other herbicide groups currently used for knotweed control, and that physical methods such as covering up and cutting down knotweed simply do not work. Importantly, we are not describing eradication (which is almost impossible to acheive), but rather a type of extended “dormancy” where the plant does not grow above ground. Additionally, we have also found that understanding when to apply the herbicide by considering the biology of the plant, specifically the seasonal surface-rhizome resource flows, is critically important. From this, we have defined a new patent pending approach to Japanese knotweed treatment, The 4-Stage Model™, which links herbicide selection and application with the seasonal surface-rhizome flows in the knotweed plant.  We are now working to replace outdated guidance based on short-term experiments and anecdotal information. We’re discovering how best to tackle invasive plants in real world conditions, informed by the evidence of what actually works.  While we acknowledge the current political debate surrounding glyphosate use and licence renewal for this herbicide, the effective outcomes of using glyphosate-based treatment seasonally requires lower doses of herbicide across the whole treatment life cycle. It is also more sustainable than other control methods that do not work.  All in all, our ongoing experimental approach delivers a more affordable knotweed treatment that is also more environmentally friendly than traditional, blanket application of herbicides."
"
Share this...FacebookTwitterThe Sun in November 2017
By Frank Bosse and Prof. Fritz Vahrenholt
(Translated and edited by P Gosselin)
In November the sun was unusually quiet with respect to activity. The observed sunspot number (SSN) was merely 5.7, which is only 14% of what is typically normal for month number 108 into the cycle. The current cycle number 24 began in December 2008. The sun was completely spotless 19 of 30 days in November.
At the end of the month some activity appeared, but only at a very low level. The following chart depicts the current cycle’s activity:

Figure 1: The monthly SSN values for the current solar cycle 24 (red) 108 months into the cycle, the curve for the mean of the previous 23 cycles (blue), and the similar solar cycle number 5 (black).
The next chart shows a comparison of all observed solar cycles thus far:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Figure 2: The monthly accumulated anomalies of the cycles up to 108 months into the cycle. Cycle number 24 has taken third place for the most inactive.
The situation thus remains unchanged: such a weak solar cycle has not been witnessed in 200 years. It is anticipated with quite high certainty that also the upcoming solar cycle number 25 will be about as weak, because the sun’s polar fields are about as strong as they were during the minimum between cycle number 23 and cycle number 24.
The very weak solar north pole so far has recovered significantly over the past few months since June. What this means now and for the future can be seen graphically at the chart posted here. You can find the latest information at www.solen.info/solar.
LaNina is here
An update to our last post here is surely of interest. We were sure of a La Nina by the end of December, and in the meantime the Australian Bureau of Meteorology officially announced a La Nina in its most recent bulletin. The current model forecast shows continued falling sea surface temperatures along the equatorial eastern Pacific until about February, 2018:

Figure 3: The model for El Nino/La Nina in the Pacific, Source: NOAA. All forecasts point to a moderately strong La Nina event until spring. A powerful La Nina such as the one observed in 2011/12 is currently not projected by the models (which incidentally did not even forecast a La Nina just a few months ago).
The impacts on global temperatures lag behind by about 3 to 4 months, and so we should expect a La Nina dip by spring.
Share this...FacebookTwitter "
"The Organisation of the Petroleum Exporting Countries, better known as OPEC, meets in Vienna on June 22. OPEC is a group (many call it a cartel) of 14 of the biggest oil nations, representing most of the world’s reserves and just under half of current oil production. For the first time in many years, the cartel will assemble against a background of tightening supply and significantly rising oil prices.  Though the emergence of Russia and the US as major oil producers means OPEC no longer wields quite as much power as it did in the 1970s, its announcements are still big news. So will OPEC choose to raise production? Will its members even be able to agree at all? These are questions we have looked at in our recent research on OPEC’s decision making. We found that whether or not all cartel members agreed on what to do significantly affected how the market reacted to an OPEC announcement.  This latest meeting comes as the price of oil is fast increasing, several years after it crashed from around $100 a barrel (bbl) to $50/bbl. OPEC members, who generally produce their oil relatively cheaply, will also be aware that long-run prices above $50/bbl will motivate shale production in rivals, including Canada and the US.  Commentary on OPEC decisions usually focuses on “spot prices” – that is, how much a barrel of oil would cost if you actually wanted it right away. But trading is also possible where we assume delivery at some specified future date, that is “forward prices”. The other way to assess OPEC decisions is to look at these forward prices – and especially at the relationship between forward and spot prices. This relationship is known as the oil market’s “term structure”. The graph below provides a simple demonstration of the importance of term structure in the oil markets. The blue line shows the familiar story of spot prices, including the 2014 crash and the recent rally. However, the red line, showing the price of three-year forward oil, tells a very different story. There was no distinct fall in forward prices around the OPEC policy change of 2014 and no recent recovery. Rather, there was a continuous and gradual decline followed by an ongoing period of stability.  Given OPEC decisions have long-term ramifications, an analysis that looked only at spot prices would certainly not tell the whole story. This is why our recent study also examined the impact of OPEC decisions on how forward prices relate to spot prices – the term structure.  We found that decisions to raise production tend to be followed by forward price increases, while decisions to cut tend to be followed by forward price decreases. These findings are seemingly counter-intuitive – after all, basic economics tells us that more supply leads to lower prices and vice versa. So why would additional OPEC production actually make oil more expensive in the future? Things are best explained by viewing OPEC decisions as something of a too-little-too-late response to market fundamentals. If you look back at the graph above, you’ll see that the OPEC decision to allow (spot) prices to fall in 2014 appears to have been anticipated by the forward market over several years. Most OPEC meetings aren’t as dramatic as 2014, of course. The most frequent outcome is “no change” in production quotas and previous research has found that prices rise following such decisions. We however have divided the “no change” decisions into those which follow a unanimous decision by members to maintain production unchanged, and those which follow a failure by OPEC members to make a unanimous decision (leading to no change by default). For example during the two OPEC meetings in 2015, both decisions were no change in production. However, the June meeting was based on “agreement to maintain”, while the December meeting was based on “failure to agree”. We found that the market reacts significantly differently to the two types of no-change decision. Failure to agree decisions lead to higher spot prices and expectations of higher prices, over a prolonged period. This is not the case for agreement to maintain decisions. The implication is that there is more market optimism following an agreement to maintain production and more caution following a failure to agree.  So OPEC members should not be fooled by the current short-run price increases. Though reports suggest an increase in production is likely, they may yet (and, from a purely self-interested point of view, should) decide not to increase their production quotas, at least so long as forward prices remain low.  But there will no doubt be some voices at the table looking to boost production because prices in the short term suggest that more oil could be absorbed by the market.  If there is no quota change, it will be interesting to watch whether members agree to maintain production unchanged, or whether they maintain production unchanged as a result of a failure to agree. Our study shows that when it comes to OPEC decisions, agreement matters."
nan
"We’re increasingly aware of how plastic is polluting our environment. Much recent attention has focused on how microplastics – tiny pieces ranging from 5 millimetres down to 100 nanometres in diameter – are filling the seas and working their way into the creatures that live in them. That means these ocean microplastics are entering the food chain and, ultimately, our bodies. But fish and shellfish aren’t our only food sources that can contain microplastics. And, in fact, other sources that don’t come from the sea might be much more worrying. A portion of consumer-grade mussels in Europe could contain about 90 microplastics. Consumption is likely to vary greatly between nations and generations, but avid mussel eaters might eat up to 11,000 microplastics a year. It’s harder to know how many microplastics we might be consuming from fish. Most studies to date have only analysed the stomach and gut content of these organisms, which are usually removed prior to consumption. But one study has found microplastics in fish liver, suggesting particles can get from digestive tissues to other body parts. Microplastics have also been found in canned fish. Numbers identified were low, so the average consumer might only eat up to five microplastics from a portion of fish this way. The particles found might also come from the canning process or from the air. Another marine food source of microplastics is sea salt. One kilogram can contain over 600 microplastics. If you eat the maximum daily intake of 5 grams of salt, this would mean you would typically consume three microplastics a day (although many people eat much more than the recommended amount).  However, other studies have found varying amounts of microplastics in sea salt, possibly because of different extraction methods used. This is a widespread problem in microplastics research that makes it hard or impossible to compare studies. For example, one study seems to only have looked for microfibres (tiny strands of artificial materials such as polyester) while a further study only looked for microplastics larger than 200 micrometres.  The sea salt study mentioned above didn’t attempt to remove and count all the microplastics from its salt samples and instead gave an estimate based on the proportion of particles that were recovered. This means it showed 1 kilogram of salt contained at least 600 microplastics – but the actual figure could be a lot higher. Despite these findings, other research demonstrates that far more microplastics in our food are likely to come from other sources than the sea. Land animals also eat microplastics although – as with fish – we tend not to eat their digestive systems. There’s limited data about this part of the food industry, but a study of chickens raised in gardens in Mexico found an average of 10 microplastics per chicken gizzard – a delicacy in some parts of the world. Scientist have also found microplastics
in honey and beer. We might be swallowing tens of microplastics with each bottle of the latter. Perhaps the biggest known source of microplastics that we consume is bottled water. When researchers examined a variety of types of glass and plastic water bottles, they found microplastics in most of them. Single-use water bottles contained between two and 44 microplastics per litre, while returnable bottles (designed for collection under a deposit scheme) contained between 28 and 241 microplastics per litre. The microplastics came from the packaging, which means we could be exposing ourselves to more of them every time we fill up a plastic bottle in order to reduce waste. There is also evidence that microplastics in food come from indoor dust. A recent study estimated that we could get an annual dose of almost 70,000 microplastics from the dust that settles on to our dinner – and that is only one of our daily meals.  So, yes, we are eating small numbers of microplastics from marine products. But it may only take drinking a litre of bottled water a day to consume more microplastics than you would from being an avid shellfish eater. And the other question scientists have yet to answer when it comes to microplastics in our food is how much harm they actually do."
"Madagascar is in the midst of a toxic invasion. Since around 2010, an army of invasive Asian toads (Duttaphrynus melanostictus) has gained a foothold in and around the eastern port of Toamasina after they were accidentally introduced from South-East Asia. This has dismayed conservationists who worry about the island’s already beleaguered endemic fauna.  Now, our worst fears have been confirmed by recent findings by a research team led by Bangor University masters student Ben Marshall and including myself. In our new study published in Current Biology, we show that most of Madagascar’s unique native wildlife can indeed be poisoned by the introduced toad. Invasive species are one of the major drivers of extinction worldwide. Most impact native species by eating them (cats, rats), competing with them (grey squirrels in the UK) or altering local vegetation (rabbits, goats). Invasive toads, on the other hand, primarily affect native predators through their skin toxins, poisoning any animal that takes a toad into its mouth. “True toads”, of the family Bufonidae, synthesise potent toxins in large prominent parotoid glands on their backs. These “bufotoxins” impede the regulation of sodium and potassium levels in cells, leading to rapid heart failure and death. Where toads occur naturally, some predators avoid them, but others have evolved resistance to their toxins, allowing them to routinely eat toads. Recent investigations have revealed the mechanism of resistance: two specific amino acid substitutions in the target molecule of bufotoxin, the ATPase sodium-potassium pump, are all that is required to change a dangerously poisonous toad into a potential meal. Remarkably, the same solution has evolved in a diverse array of species, from rats to lizards to the toads themselves, and this appears to be the only way for any vertebrate to resist the toads’ toxins. The problem for Madagascar’s animals is they developed on an island with plenty of frogs but no toads. Native species therefore had no reason to evolve any resistance to toad toxins nor any sense that they should avoid toads. And when toads do invade these previously toad-free areas, it may result in mass mortality among some naive predators. For instance the infamous cane toad invasion of Australia caused larger apex predators, such as monitor lizards, some snakes and the marsupial quolls to become locally rare or extinct. This in turn affected numerous other species in the food web, though some, such as some smaller lizards and snakes, actually flourished due to the removal of the larger predators. So will Madagascar’s predators suffer just like Australia’s? As we now know that there is a single molecule responsible for toad resistance in vertebrates, this gives biologists an invaluable tool: simply examining the relevant gene will reveal whether an animal can eat a toad with impunity or is liable to be poisoned. In our study, we used this approach on a range of Malagasy predators. The results confirmed our worst fears: out of 29 reptiles, eight mammals, 12 frogs and 28 birds tested, all except one rodent (the white-tailed antsangy) lack resistance to toad toxins. This presents an immediate conservation concern in an already troubled biodiversity hotspot, and has serious implications for the many species that exist only on Madagascar. For instance, all Malagasy snakes tested are vulnerable to toad toxins, and anecdotal reports have already documented snakes dying from eating toads.  It gets worse: in the past, a loss of snakes has led to booms in rodent populations and fears over public health. Such fears are warranted once again, given that non-native rats are resistant to bufotoxins. However, rodents appear to be unique in this respect. Among the toad-sensitive mammals are Madagascar’s most charismatic and widely recognised residents. Three representatives from three lemur families are vulnerable. Fortunately lemurs eat plants and sometimes insects, and only rarely prey on small vertebrates, which will likely limit the toad’s impact on them. The same cannot be said for the carnivores of Madagascar: the habits of the enigmatic fossa and others make them extremely likely to encounter and consume the toxic invader. As cane toads in Australia highlighted, it’s tough to predict exactly which species will be most affected by an invader. The sharing of habits and habitats will likely be the biggest factor influencing how much the toad will impact a species, but subtle differences in their interactions can make big differences to the outcome. For instance, small variations in breeding times made large differences in the relative success of native and invasive tadpoles in Australia. Possible hope comes in the form of animals’ unrelenting ability to adapt. Australia offers many examples of native species evolving or learning quickly to avoid toads, or to only eat the least toxic parts of toads. Whether vulnerable Malagasy species, restricted to much more fragmented habitat patches serving as their final refuges, can display similar resilience remains to be seen. The invasion of Madagascar by these toads has attracted considerable attention, but initial efforts to eliminate them have faltered. We hope that the confirmation that these toads pose a real threat to native species will reinvigorate efforts to protect native species from the toad’s further expansion."
"
Share this...FacebookTwitterWhile global warming alarmists continue to fantasize crude oil use getting drastically reduced already starting next year, OPEC sees it totally differently. German online. center-left weekly Die Zeit reports “OPEC anticipates growing oil demand until 2040.”
This poses a huge dilemma for the activists and alarmists who are urgently pressing to transform society –based on the fear and belief that the globe will warm rapidly if we don’t act now.
The other fear is that rising oil consumption over the next 25 more years accompanied stalling global temperatures will forever expose climate science as a ruse.
Still decades away from peak oil consumption
Die Zeit writes that OPEC is sure that the planet is still years away from peak oil demand, and the reason is because cars will continue to be mostly powered by petroleum even beyond 2040. Obviously a number of leading energy experts believe electric cars will remain a pipe dream for quite some time.

Chart: OPEC
Oil consumption will climb almost 20%
Firstly OPEC sees the world population growing from 7.6 billion today to 9.2 billion by 2040 and global GDP growing by a whopping 126%. That’s all going to require lots of reliable and efficient energy. The good news is that overall energy efficiency will increase, as only 96% more energy will be needed to power the 126% GDP boost.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Most of the energy increase will come from India and China. Moreover, the global average standard of living will also grow strongly as the per capita energy use will be much higher. OPEC projects that much of the added energy demand will be supplied by natural gas (see chart above). Part of the increased demand will be met by coal and oil.
OPEC also foresees continued domination by the internal combustion engine for passenger cars, even in 2040:

Chart: OPEC
Electric mobility still decades away
OPEC also projects there will be more than 2 billion automobiles on the world’s roads by 2040, almost double today’s number. Over 80% of these will continue to be powered by internal combustion engines. The electric vehicle age appears to be something for the late 21st century. Visions of a near zero carbon society within the next 30 years are more fantasy than reality.
Not surprisingly, most of the increased energy demand will come from Asia and Africa. The richer, developed OECD countries will see little growth in energy demand.
The OPEC video summarizes by saying that it is “committed to reducing energy poverty“. That target of course would make a huge contribution to alleviating the overall misery that still plagues many poor countries. Oil, coal and gas will continue to play the leading role.
Of course, jet-setting climate activists and alarmists would like to deny poor countries access to affordable and reliable energy.
Good luck with that.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterEngineering Prof. Questions Temperature
Record, Models, CO2 Climate Sensitivity 

Photo California Baptist University

 Pontius, 2017  
Sustainable Infrastructure: 
Climate Changes and Carbon Dioxide 
Temperatures Record ‘Unreliable’, ‘Arbitrarily Adjusted’, And Of ‘Poor Data Quality’
Temperature measurement stations have been installed at various locations across the globe. The number of temperature monitoring stations is decreasing and many areas across the globe do not have any temperature monitoring stations. Consequently, average surface temperature is an unreliable metric for assessing global temperature trends.
Computer models are used to analyze data sets. In science and engineering (and this paper) the term “data” refers to actual physical measurement at a point in time and space. In some temperature data sets, however, computer simulated values have been added in or data may have been arbitrarily adjusted long after the physical measurement was taken. Such practices undermine the credibility of the data set.   Computer generated values are estimates, projections, or simulations and are of a different quality than physical measurements. Physical measurements represent a physical quantity whereas computer simulations represent numerical calculation.
The HADCRU, GISTEMP, and NOAA surface temperature archives rely on the same underlying input data and therefore are not independent data sets. Limitations of the GHCN affect all data sets. Sampling discontinuities, urbanization and land use changes have decreased the quality of GHCN data over time. Differences in data processing methods between research teams do not compensate for poor underlying data quality inherent in the GHCN data. A similar situation exists with historical Sea Surface Temperature (SST) data sets which are derived primarily from the International Comprehensive Ocean-Atmosphere Data Set (ICODADS).
Climate Models ‘Unreliable For Long-Term Climate Prediction’
Computer simulations involve mathematical models implemented on a computer imitating one or more natural processes. Models are based on general theories and fundamental principles, idealizations, approximations, mathematical concepts, metaphors, analogies, facts, and empirical data (Peterson, 2006, Meehl et al., 2012). Judgments and arbitrary choices must be made in model construction to apply fundamental laws to describe turbulent fluid flow. The large size and complexity of the atmosphere prohibit the direct application of general theory.
In general, ensemble model forecasts have been found unreliable for long-term climate prediction (Green and Armstrong, 2007, Mihailović et al., 2014).
“The forecasts in the [IPCC] Report were not the outcome of scientific procedures. In effect, they were the opinions of scientists transformed by mathematics and obscured by complex writing. Research on forecasting has shown that experts’ predictions are not useful in situations involving uncertainly and complexity. We have been unable to identify any scientific forecasts of global warming. Claims that the Earth will get warmer have no more credence than saying that it will get colder.”  –  Green and Armstrong, 2007.
“This analysis, set into context of the climate modeling, points out the fact that there exists set of domains where the environmental interface temperature cannot be calculated by the physics of currently designed climate models.” – Mihailović et al., 2014

Climate Sensitivity To Changing CO2 Concentrations
Global
The global atmospheric system is dynamic and is constantly in a state of change and adjustment. The sun is the primary climate change driving force.   
Using a Climate Sensitivity best estimate of 2°C, the increase in [global] temperature resulting from a doubling of atmospheric CO2 is estimated at approximately 0.009°C/yr which is insignificant compared to natural variability.
CO2 is a non-toxic trace gas constituting approximately 0.04% of the earth’s atmosphere. The global atmospheric concentration of CO2 increased from a pre-industrial value of about 280 ppmv to 379 ppmv in 2005 . The average CO2 concentration at the monitoring station at Mauna Loa, Hawaii for May 2017 is 409.65 ppmv.  A rising concentration of atmospheric CO2 will contribute to warming of the Earth’s atmosphere. The physics of CO2 in the atmosphere is very different than the physics of the heating effect occurring in a physical “greenhouse” for growing plants. The term “greenhouse effect” is commonly used to refer to the warming of the earth from “greenhouse” gases such as CO2 in the atmosphere. The term “greenhouse” is not used here to refer to the Earth’s warming to avoid equivocation.
Estimates of climate sensitivity differ widely suggesting that this characteristic of the climate system is not well-understood (Schwartz et al., 2014). 


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




A simple model predicts that a doubling of the CO2 concentration in the atmosphere would result in a small increase of the Earth’s surface temperature, from approximately 0.[5] to < 0.7°C  (Kissin, 2015).
“[A] doubling the CO2 concentration in the Earth’s atmosphere would lead to an increase of the surface temperature by about +0.5 to 0.7 °C, hardly an effect calling for immediate drastic changes in the planet’s energy policies.” – Kissin, 2015
A best estimate of 2.0°C (Otto et al., 2013) is assumed here. If CO2 increases at the current rate of approximately 2 ppmv per year, a temperature increase of approximately 0.009°C/yr could be expected.
To date the impact of CO2 is assessed universally within a global reference frame. Although atmospheric CO2 has steadily increased the average satellite global temperatures have flattened since approximately 1995. 

 From such trends, it must be inferred that changes in global lower troposphere average temperature correspond to fundamental changes in the climate system beyond internal variability.
Riverside, California
The impact of future atmospheric CO2 warming on the Riverside locational reference frame must be estimated.  GCMs [climate models] could be applied to project future global temperatures and those projections could be downscaled to the Riverside area. However, such efforts would be potentially misleading because of the limitations of GCMs discussed previously.  Detailed assessments of the CO2 effect have been performed analyzing the Earth’s energy balance in the total atmosphere column and the reduction of the upward infrared radiation emission at the tropopause. The impact of CO2 on warming of the Earth is expressed in terms of “climate sensitivity,” which is the amount of warming that could be  expected as a result of doubling of the CO2 concentration.
Available temperature data from both the Riverside Fire Station No. 3 and the Riverside Municipal Airport demonstrate horizontal trends within a wide band of  variability. Historical evidence of a significant increase in surface temperatures due to increases in atmospheric CO2 is absent from these data.   [C]limate models are useful but limited in their representation of underlying physical processes.  Uncertainties and other limitations discussed previously render such models unreliable for long-term global temperatures or local climate change prediction.
Climate sensitivity may be applied to estimate the warming effect of CO2 on the locational reference frame.  Factors affecting Climate Sensitivity are not well-understood and estimates differ among researchers. Alternatively, a site-specific model could be developed to estimate the future impact of CO2 warming on a particular location. If atmospheric CO2 continues to increase at its current rate the small annual temperature increase expected at Riverside will likely be insignificant (e.g. < 0.01°C/yr) compared to natural temperature variability.
A slight increase in minimum daily temperature is noticeable at Riverside Fire Station No. 3 after 1998 (Figure 8, lower) with a corresponding slight decrease in the daily temperature range (Figure 9). This trend is most likely due to the urban heat island effect (Tam et al., 2015) resulting from increased development within and around downtown Riverside over this extended period.

Share this...FacebookTwitter "
"
Share this...FacebookTwitter‘Two-Thirds Of Climate Warming’ 
Since 1750 Due To ‘Solar Causes’
– Dr. Alan D. Smith, Geoscientist

Though advocates of the dangerous anthropogenic global warming (AGW) narrative may not welcome the news, evidence that modern day global warming has largely been driven by natural factors – especially solar activity – continues to pile up.
Much of the debate about the Sun’s role in climate change is centered around reconstructions of solar activity that span the last 400 years, which now include satellite data from the late 1970s to present.
To buttress the claim that solar forcing has effectively played almost no role in surface temperature changes since the mid-20th century, the IPCC has shown preference for modeled reconstructions of solar activity (i.e., the PMOD) that show a stable or decreasing trend since the 1970s.  Why?  Because if the modeled results can depict steady or decreasing solar activity since the last few decades of the 20th century – just as surface temperatures were rising – then attributing the post-1970s warming trend to human activity becomes that much easier.
The trouble is, satellite observations using ACRIM  data (which have been affirmed to be accurate by other satellite data sets and are rooted in observation, not modeled expectations) indicate that solar activity did not decline after the 1970s, but actually rose quite substantially.  It wasn’t until the early 2000s that solar activity began to decline, corresponding with the denouement of the Modern Grand Maximum.

ACRIM Composite Is ‘Data Driven’, While The PMOD Composite Is ‘Model Driven’

Willson, 2014
• Comparison of the results from the ACRIM3, SORCE/TIM and SOHO/VIRGO satellite experiments demonstrate the near identical detection of TSI variability on all sub-annual temporal and amplitude scales during the TIM mission.   A solar magnetic activity area proxy [developed in 2013] for TSI has been used to demonstrate that the ACRIM TSI composite and its +0.037 %/decade TSI trend during solar cycles 21–23 [1980s-2000s] is the most likely correct representation of the extant satellite TSI database. 
• The occurrence of this trend during the last decades of the 20th century supports a more robust contribution of TSI variation to detected global temperature increase during this period than predicted by current climate models.
• One of the most perplexing issues in the 35 year satellite TSI database is the disagreement among TSI composite time series in decadal trending. The ACRIM and PMOD TSI compostite time series use the ERB and ERBE results, respectively, to bridge the Gap. Decadal trending during solar cycles 21–23 is significant for the ACRIM composite but not for the PMOD.  A new [2013] TSI-specific TSI proxy database has been compiled that appears to resolve the issue in favor of the ACRIM composite and trend. The resolution of this issue is important for application of the TSI database in research of climate change and solar physics.
• The ACRIM TSI composite is data driven. It uses ACRIM1, ACRIM2, ACRIM3 and Nimbus7/ERB satellite results published by the experiments’ science teams and the highest cadence and quality ACRIM Gap database, the Nimbus7/ERB, to bridge the ACRIM Gap. 
• The PMOD TSI composite, using results from the Nimbus7ERB, SMM/ACRIM1, UARS/ACRIM 2 and SOHO/ VIRGO experiments, is model driven. It conforms TSI results to a solar-proxy model by modifying published ERB and ACRIM results and choosing the sparse, less precise ERBS/ERBE results as the basis for bridging the ACRIM Gap (Frohlich and Lean 1998).
• The Earth’s climate regime is determined by the total solar irradiance (TSI) and its interactions with the Earth’s atmosphere, oceans and landmasses. Evidence from 35 years of satellite TSI monitoring and solar activity data has established a paradigm of direct relationship between TSI and solar magnetic activity. (Willson et al. 1981; Willson and Hudson 1991; Willson 1997, 1984; Frohlich and Lean 1998; Scafetta and Willson 2009; Kopp and Lean 2011a, 2011b)  This paradigm, together with the satellite record of TSI and proxies of historical climate and solar variability, support the connection between variations of TSI and the Earth’s climate.   The upward trend during solar cycles 21–23 coincides with the sustained rise in the global mean temperature anomaly during the last two decades of the 20th century. 

Assessment Of The Sun’s Climate Role Largely Depends On The TSI Model Adopted

Van Geel and Ziegler, 2013
• [T]he IPCC neglects strong paleo-climatologic evidence for the high sensitivity of the climate system to changes in solar activity. This high climate sensitivity is not alone due to variations in total solar irradiance-related direct solar forcing, but also due to additional, so-called indirect solar forcings. These include solar-related chemical-based UV irradiance-related variations in stratospheric temperatures and galactic cosmic ray-related changes in cloud cover and surface temperatures, as well as ocean oscillations, such as the Pacific Decadal Oscillation and the North Atlantic Oscillation that significant affect the climate.
• [T]he cyclical temperature increase of the 20th century coincided with the buildup and culmination of the Grand Solar Maximum that commenced in 1924 and ended in 2008.
• Since TSI estimates based on proxies are relatively poorly constrained, they vary considerably between authors, such as Wang et al. (2005) and Hoyt and Schatten (1997). There is also considerable disagreement in the interpretation of satellite-derived TSI data between the ACRIM and PMOD groups (Willson and Mordvinov, 2003; Fröhlich, 2009). Assessment of the Sun’s role in climate change depends largely on which model is adopted for the evolution of TSI during the last 100 years (Scafetta and West, 2007; Scafetta, 2009; Scafetta, 2013). 
• The ACRIM TSI satellite composite shows that during the last 30 years TSI averaged at 1361 Wm-2, varied during solar cycles 21 to 23 by about 0.9 Wm-2, had increased by 0.45 Wm-2 during cycle 21 to 22 [1980s to 2000s] to decline again during cycle 23 and the current cycle 24 (Scafetta and Willson, 2009). 
• By contrast, the PMOD TSI satellite composite suggests for the last 30 years an average TSI of 1366, varying between 1365.2 and 1367.0 Wm-2 that declined steadily since 1980 by 0.3 Wm-2.

Total Solar Irradiance Increased By 3 W m-2 Between 1900 And 2000

Van Geel and Ziegler, 2013 (continued)
• On centennial and longer time scales, differences between TSI estimates become increasingly larger. Wang et al. (2005) and Kopp and Lean (2011) estimate that between 1900 and 1960 TSI increased by about 0.5 Wm-2 and thereafter remained essentially stable, whilst Hoyt and Schatten (1997) combined with the ACRIM data and suggest that TSI increased between 1900 and 2000 by about 3 Wm-2 and was subject to major fluctuations in 1950-1980 (Scafetta, 2013; Scafetta, 2007). 
• Similarly, it is variably estimated that during the Maunder Solar Minimum (1645- 1715) of the Little Ice Age TSI may have been only 1.25 Wm-2 lower than at present Wang et al., 2005; Haig, 2003; Gray et al., 2010; Krivova et al., 2010) or by as much as 6 ± 3 Wm-2 lower than at present (Shapiro et al., 2010; Hoyt and Schatten, 1997), reflecting a TSI increase ranging between 0.09% and 0.5%, respectively.

Graph Source: Soon et al., 2015

After Removing Instrumental ‘Adjustments’, Urban Bias, Temperatures Follow Solar Activity

The combined Hadley Centre and Climatic Research Unit (HadCRUT) data set — which is featured in the Intergovernmental Panel on Climate Change (IPCC) reports — underwent a revision from version 3 to version 4 in March of 2012.  This was about a year before the latest IPCC report was to be released (2013).  At the time (early 2012), it was quite inconvenient to the paradigm that HadCRUT3 was highlighting a slight global cooling trend between 1998 and 2012, as shown in the graph below (using HadCRUT3 and HadCRUT4 raw data from WoodForTrees).  So, by changing versions, and by adjusting the data, the slight cooling was changed to a slight warming trend.

Source: WoodForTrees
As recently as 1990, it was widely accepted that the global temperature trend, as reported by NASA (Hansen and Lebedeff, 1987), showed a “0.5°C rise between 1880 and 1950.”
Pirazzoli, 1990
This 0.5°C rise in global temperatures between 1880-1950 (and 0.6°C between 1880 and 1940) can clearly be seen in the NASA GISS graph from 1987:

Schneider, S. H. 1989. The greenhouse effect: Science and policy. Science 243: 771-81.
Today, it is no longer acceptable for the NASA global temperature data set to graphically depict a strong warming trend during the first half of the 20th century.  This is because anthropogenic CO2 emissions were flat and negligible relative to today during this abrupt warming period.
So as to eliminate the inconvenience of a non-anthropogenic warming trend in modern times, NASA has now removed all or nearly all the 0.5°C of warming between 1880 and 1950.

NASA GISS graph

 

Soon et al., 2015   
• [B]etween 65-80% of the apparent warming trend over the 1961-2000 period for the Beijing and Wuhan station records was probably due to increasing urban heat islands.  [T]he temperature trends increase from +0.025°C/decade (fully rural) to … +0.119°C/decade (fully urban). … If we assume that the fully rural stations are unaffected by urbanization bias, while the other subsets are, then we can estimate the extent of urbanization bias in the “all stations” trends by subtracting the fully rural trends. This gives us an estimate of +0.094°C/decade urbanization bias over the 1951-1990 period [+0.38°C of additional non-climatic warmth]– similar to Wang & Ge (2012)’s +0.09°C/decade estimate.
•We have constructed a new estimate of Northern Hemisphere surface air temperature trends derived from mostly rural stations – thereby minimizing the problems introduced to previous estimates by urbanization bias.  
• Similar to previous estimates, our composite implies warming trends during the periods 1880s-1940s and 1980s-2000s. However, this new estimate implies a more pronounced cooling trend during the 1950s-1970s. As a result, the relative warmth of the mid-20th century warm period [1930s-1950s] is comparable to the recent [1980s-2000s] warm period – a different conclusion to previous estimates. Although our new composite implies different trends from previous estimates, we note that it is compatible with Northern Hemisphere temperature trends derived from (a) sea surface temperatures; (b) glacier length records; (c) tree ring widths.
• However, the recent multi model means of the CMIP5 Global Climate Model hindcasts failed to adequately reproduce the temperature trends implied by our composite, even when they included both “anthropogenic and natural forcings”. One reason why the hindcasts might have failed to accurately reproduce the temperature trends is that the solar forcings they used all implied relatively little solar variability. However, in this paper, we carried out a detailed review of the debate over solar variability, and revealed that considerable uncertainty remains over exactly how the Total Solar Irradiance has varied since the 19th century. 
• When we compared our new composite to one of the high solar variability reconstructions of Total Solar Irradiance which was not considered by the CMIP5 hindcasts (i.e., the Hoyt & Schatten reconstruction), we found a remarkably close fit. If the Hoyt & Schatten reconstruction and our new Northern Hemisphere temperature trend estimates are accurate, then it seems that most of the temperature trends since at least 1881 can be explained in terms of solar variability, with atmospheric greenhouse gas concentrations providing at most a minor contribution. 
• This contradicts the claim by the latest Intergovernmental Panel on Climate Change (IPCC) reports that most of the temperature trends since the 1950s are due to changes in atmospheric greenhouse gas concentrations (Bindoff et al., 2013).


New Paper: Since 1750, About 0.8°C – 0.9°C Of CET Increase Is Due To Solar Forcing

Smith, 2017
Yearly mean temperatures in the CET [Central England Temperature] record show an increase in temperature of approximately 1.3°C degrees from the end of the 17th Century to the end of the 20th Century/beginning of 21st Century.  …  Subtle difference in timing between the warming/cooling phases between the Central England record and the other localities may reflect local climate variation, but the similarity in events between continents suggests the CET [Central England Temperature] record is recording global temperature patterns.
Records of sunspot numbers began in 1610 such that detailed estimates of solar variation for the years covered by the CET record can be made without resort to the use of proxy data. Reconstructions of TSI [e.g. 16-18] differ in magnitude (Table 1), but there is agreement in form with 4 peaks and 4 to 6 troughs occurring over the time-scale of the CET record (Fig. 4). These are: a minimum in TSI associated with the Maunder Sunspot Minimum in the latter half of the 17th Century; a peak, possibly bi-modal approaching modern TSI values during the 18th Century; a well-defined trough corresponding with the Dalton Sunspot Minimum between 1800- 1820; a poorly defined TSI peak in the mid 19th Century; a reduction in TSI during the late 19th Century; increasing TSI during the early 20th Century; a decrease in TSI from around 1950- 1975; and a second phase of TSI increase in the late 20th Century [1980s-2000s]. There is good correspondence with TSI throughout the CET record, with warm events correlating with high TSI and cool phases correlating with plateaus or decreases in TSI .
However, for temperature increases from the beginning of the Industrial Revolution (Maunder Minimum and Dalton Minimum to end of 20th Century), high TSI models can account for only 63-67% of the temperature increase. This would suggest that one third of Global Warming/Climate Change can be attributed to AGW. … Approximately two-thirds [0.8°C to 0.9°C] of climate warming since the mid-late 18th Century [1.3°C] can be attributed to solar causes, suggesting warming due to anthropogenic causes over the last two centuries is 0.4 to 0.5°C.


All Over The Globe, Trends In Solar Forcing Correlate With Temperature Changes



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Christiansen and Lungqvist (2012)


Stoffel et al., 2015


Schneider et al., 2015  and  Wilson et al., 2016


Kim et al., 2017


Yamanouchi, 2011


Box et al., 2009


Southern Hemisphere

Schneider et al., 2006


De Jong et al., 2016
“[T]he period just before AD 1950 was substantially warmer than more recent decades.”


de Jong et al., 2013


Zinke et al., 2016


Turney et al., 2017


Elbert et al., 2013


“[I]n the framework of empiric [observable] models, the estimate of the solar activity contribution in the variation in the air global temperature in the 20th century is about 70%.” – Kovalenko and Zherebtsov, 2014
Share this...FacebookTwitter "
"Across the North Atlantic, shipwrecks scatter the seabed like the carcasses of prehistoric creatures. Bygone relics of sea exploration, trade, migration and conflict, these historical monuments are important sites of cultural interest. But they also form the basis of a burgeoning recreational dive tourism industry, and contribute substantially to the biodiversity and abundance of marine life. The stories behind how these ships came to rest on the sea bed are intriguing. In the Orkney Isles off the north coast of Scotland, for example, a major naval event at the close of World War I set the wheels in motion for the creation of a world-renowned dive site of considerable historic and environmental value. The captured German warship fleet (comprising 74 vessels) was interned in Scapa Flow, a sheltered body of water between the Orkney Mainland and the South Isles. In June 1919, as its fate was debated by the allied powers, Rear Admiral Ludwig von Reuter gave the order to his German crews to scuttle all ships to prevent the flotilla falling into enemy hands. A total of 52 ships were successfully sunk, leaving Scapa Flow an undersea graveyard of WWI German shipwrecks. But this is a graveyard full of life. A hundred years later, marine habitats around the Scapa Flow wrecks are thriving. The waters teem with an abundance of sea creatures including crab, lobster, starfish, sea urchin and a variety of fish species. Recent surveys around the Orkney wrecks by Seasearch volunteer divers have also reported rarities such as the fan mussel and the common skate, now internationally scarce. As well as being habitats where marine species can establish and flourish, these wrecks may also be some of the most biodiverse habitats of northern waters. Take the Karlsruhe for instance, a 112-metre German light cruiser, once operated by a crew of 475 men, now lying abandoned on its starboard side in 25 metres of water in Scapa Flow. Its toppled foredeck guns and armoured control tower can still be seen, creating an eerie but captivating scene. Nearby, off the deck side of the resting hull, the seabed is covered by a horse mussel bed that hosts a range of species including brittle stars, herringbone hydroids, large predatory spiny starfish and seven-armed starfish. Sea cucumbers can also be found here among the dense clumps of horse mussels. Horse mussels are bivalve molluscs similar to the well-known, edible blue mussel, but much larger and longer-lived. Their notable size and longevity, along with an ability to bind to one another via thin secreted threads called byssus, allow these creatures to create extensive reefs that provide habitat for a wealth of other marine creatures. This makes horse mussel reefs biodiversity hotspots, with some supporting hundreds of other species. These reefs also provide a number of beneficial ecosystem “services” including the provision of nursery grounds for commercial fisheries species. Unfortunately, these reef habitats are currently listed as endangered in European waters, in part due to their historical destruction by bottom-towed fishing equipment like trawlers. But under climate change these optimal habitat conditions will vastly dwindle and push these reef habitats to their northern limits as more southern temperature conditions become intolerable. Research into the specific impact of climate change on horse mussels is only beginning to emerge, but studies have shown that the effects on other bivalve species such as the blue/common mussel  include changes in shell strength, disease and reproduction issues.  Marine biologists believe wrecks like the Karlsruhe may be instrumental in creating ideal hydrodynamic conditions for vital habitats like horse mussel reefs, but this is yet to be fully investigated.  Wrecks may help to produce good feeding conditions, that is, sufficient water flow to maintain a supply of algal feed but not too vigorous that animals are unable to filter feed, and natural breaks and eddies in high current flow that allow for the settlement of mussel larvae on the sea floor. Larvae are the microscopic offspring of invertebrate species which use ocean currents to move across large areas. In doing so, they ensure important genetic mixing between populations. Recent research by Heriot-Watt University researchers in collaboration with Scottish Natural Heritage has shown that the Karlsruhe horse mussel reef is likely genetically connected (via the transfer of genetic material between sites, termed “gene flow”) to horse mussel reefs hundreds of miles away along the west coast of Scotland. Whether settlement of larvae arriving from far-off sites could occur without the wreck in place remains unknown, but it’s certainly an interesting question to ponder. Unfortunately, deterioration of these sites is a major challenge faced by local, recreational and scientific diving operations in northern communities whose livelihoods rely heavily on these underwater habitats. The marine environments of northern regions like the Orkneys also face a serious threat from global climate change due to the accelerated rates of change occurring in polar regions. However, a lack of research on impacts has meant that communities face huge challenges in finding ways to react and adapt to these problems. Dive operators, researchers and volunteers continue to work together to document these historic relics and their ecological associations. June 2019 marks the centenary of the sinking of German WWI warships in Scapa Flow. We should mark this important moment in history by drawing attention to these precious marine habitats so they may be protected for hundreds of years to come."
"
Share this...FacebookTwitter Cooling, Not Warming, Leads To
  Weather and Climate Instability 

Image Source: Loisel et al., 2017

1. Significant Decreasing Trend In Severe Weather Since 1961
Zhang et al., 2017
Based on continuous and coherent severe weather reports from over 500 manned stations, for the first time, this study shows a significant decreasing trend in severe weather occurrence across China during the past five decades. The total number of severe weather days that have either thunderstorm, hail and/or damaging wind decrease about 50% from 1961 to 2010. It is further shown that the reduction in severe weather occurrences correlates strongly with the weakening of East Asian summer monsoon which is the primary source of moisture and dynamic forcing conducive for warm-season severe weather over China.

2. Most Frequent Climate Instability During Global Cooling/Reduced CO2 Periods
Kawamura et al., 2017
Numerical experiments using a fully coupled atmosphere-ocean general circulation model with freshwater hosing in the northern North Atlantic showed that climate becomes most unstable in intermediate glacial conditions associated with large changes in sea ice and the Atlantic Meridional Overturning Circulation. Model sensitivity experiments suggest that the prerequisite for the most frequent climate instability with bipolar seesaw pattern during the late Pleistocene era is associated with reduced atmospheric CO2 concentration via global cooling and sea ice formation in the North Atlantic, in addition to extended Northern Hemisphere ice sheets.

3. Hurricane Activity Is ‘Subdued’ During Warm Periods (1950-2000)
Heller, 2017


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The hurricane analysis conducted by Burn and Palmer (2015) determined that hurricane activity was subdued during the [warm] Medieval Climate Anomaly (MCA) (~900-1350 CE) and became more produced during the [cold] Little Ice Age (LIA) (~1450-1850 CE), followed by a period of variability occurred between ~1850 and ~1900 before entering another subdued state during the industrial period (~1950-2000 CE). In general, the results of this study corroborate these findings … [W]hile hurricane activity was greater during the LIA, it also had more frequent periods of drought compared to the MCA (Burn and Palmer 2014), suggesting that climate fluctuations were more pronounced in the LIA compared to the MCA. The changes in the diatom distribution and fluctuations in chl-a recorded in this study starting around 1350 also indicate that variations in climate have become more distinct during the LIA and from ~1850-1900.
[C]limate variability has increased following the onset of the Little Ice Age (~1450-1850 CE), however it is difficult to distinguish the impacts of recent anthropogenic climate warming on hurricane activity from those of natural Atlantic climate regimes, such as ENSO.

4. Surface Warming Weakens Cyclone Activity
Chen et al., 2017
Results indicate that the midlatitude summer cyclone activity over East Asia exhibits decadal changes in the period of 1979–2013 and is significantly weakened after early 1990s. …  Moreover, there is a close linkage between the weakening of cyclonic activity after the early 1990s and the nonuniform surface warming of the Eurasian continent. Significant warming to the west of Mongolia tends to weaken the north–south temperature gradient and the atmospheric baroclinicity to its south and eventually can lead to weakening of the midlatitude cyclone activity over East Asia.

5. More Hydroclimatic Variability During Cold Periods…Models Say Warming Causes More Instability, So The 21st Century Will Be Like The Little Ice Age, With More Instability/Megadrought
Loisel et al., 2017
Our tree ring-based analysis of past drought indicates that the Little Ice Age (LIA) experienced high interannual hydroclimatic variability, similar to projections for the 21st century. This is contrary to the Medieval Climate Anomaly (MCA), which had reduced variability and therefore may be misleading as an analog for 21st century warming, notwithstanding its warm (and arid) conditions. Given past non-stationarity, and particularly erratic LIA, a ‘warm LIA’ climate scenario for the coming century that combines high precipitation variability (similar to LIA conditions) with warm and dry conditions (similar to MCA conditions) represents a plausible situation that is supported by recent climate simulations. … Our comparison of tree ring-based drought analysis and records from the tropical Pacific Ocean suggests that changing variability in El Niño Southern Oscillation (ENSO) explains much of the contrasting variances between the MCA and LIA conditions across the American Southwest. The Medieval Climate Anomaly (MCA, ~950–1400 CE) is often used as an analog for 21stcentury hydroclimate because it represents a warm (and arid) period. The MCA appears related to general surface warming in the Northern Hemisphere, prolonged La Niña conditions, and a persistent positive North Atlantic Oscillation mode. It has been referred to as a stable time interval with ‘quiet’ conditions in regards to low perturbation by external radiative forcing. In this study, we demonstrate that the Little Ice Age (LIA, ~1400–1850 CE) might be more representative of future hydroclimatic variability than the conditions during the MCA megadroughts for the American Southwest, and thus provide a useful scenario for development of future water-resource management and drought and flood hazard mitigation strategies.

Share this...FacebookTwitter "
"
Share this...FacebookTwitterGermany Temperatures Baffle: September Mean Shows Hardly Any Warming In 70 Years
By Josef Kowatsch and Dr. Sebastian Lüning
(Translated and edited by P Gosselin)
Temperatures are rising and rising and rising. That’s what we read in any case in the daily newspaper, and that’s what some television professors, activists and climate scientists are telling us. Strangely rarely are temperature curves ever shown. Why is this so? One example is the September mean temperature for Germany, which we use to illustrate this peculiar media documentation gap.
Here we use the official DWD German Weather Service data. When we look at the past 100 years we see a very modest warming of just a few tenths of a degree (Fig. 1). This is no surprise as we find ourselves in the warming phase since the Little Ice Age, the coldest phase of the last 10,000 years. It would have been terrible had the climate stayed at this non-representative low level.

Figure 1: Chart depicting Germany September mean temperature over the past 100 years. Data source: DWD. 
It is easy to see the long cycles in the temperature curve. Above we a cold phase between 1920 and 1930, followed by a warm period during the Nazi time, and then followed by a long-term cold dip.
Beginning in 1985, September began to warm up again before reaching a plateau that took hold just before the year 2000 and at which we currently find ourselves. Based on the past development one could speculate that we are headed towards a slight cooling.
Now let’s look at the period from the end of WWII until today, more than 70 years, the time of the last temperature plateau until today. Immediately we see that we are far from worrisome climate warming (Fig. 2):


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Figure 2: Chart depicting Germany September mean temperature over the past 70 years. Data source: DWD. 
Finally we take a look at the past 13 years (Fig. 3), i.e. the development since 2004. Again there has not been any significant warming. In fact there’s been some cooling. Everything other than a climate catastrophe.

Figure 3: Chart of September mean temperatures in Germany over the past 13 years. Data source: DWD. 
Getting back to the primary question of why isn’t the German media showing the real German temperature curve, obviously the real facts are just too inconvenient. A pert of the public could even lose its faith in the much-preached climate catastrophe and end up sharply criticizing the harsh sacrifices now being made because of the climate fear that has been instilled by policymakers.
It’s high time for the issue to be made transparent and to push back against the activism. What’s needed is a new environmental protection ethic, one which addresses all the problems.
The excessive focus on the climate question is no longer sustainable and is even counterproductive. Other more important problems that can be solved over the short term require greater attention — clean water, clean air and clean food being evenly distributed — would be a common ethical goal for mankind to strive for. The fear-mongering climate protection issue is a repeat of the earlier business model of sin and the sale of indulgences.
 
Share this...FacebookTwitter "
"It is not exactly glamorous work. Measuring sheep farts is smelly, time consuming and expensive. But for Dr Suzanne Rowe, a scientist who is breeding strains of sheep that emit less methane than regular flocks, there is nothing more important she feels she could be doing.  “New Zealand has really become a global leader in this space and there’s a lot of buzz around at the moment – it’s hugely exciting,” she says. The release of methane gas from New Zealand sheep and cattle accounts for one third of the country’s greenhouse gas emissions, the single largest contributor in the country. Since 1990, methane gas from stock has increased by 10%, according to the ministry of the environment, along with a 70% increase in dairy cattle and a 44% drop in sheep numbers. Accordingly, methane has found itself in the crosshairs of the government’s climate action policy, and scientists around the country are being given the green light to run free with their best and brightest ideas to lower emissions. In November, the prime minister, Jacinda Ardern, pushed the Zero Carbon Act through parliament with cross-party support, saying the world was “undeniably warming” and all greenhouse gases must be reduced to net zero by 2050 in order to honour agreements made under the Paris climate accords. “We have to start moving beyond targets,” Ardern said. “We have to start moving beyond aspiration. We have to start moving beyond hope and deliver signs of action.” Under the zero carbon legislation, the methane targets are separated out from other greenhouse gases, with the goal of reducing biogenic methane by 10% by 2030, and 24-47% by 2050.  “Agriculture is incredibly important to New Zealand,” she said. “But it also needs to be part of the solution. That is why we have listened to the science and also heard the industry and created a specific target for biogenic methane.” In Nelson, at the top of the South Island, the Cawthron Institute has recently been awarded government funding to cultivate and research a red native seaweed known as Asparagopsis armata. The agriculture minister, Damien O’Connor, has said if this seaweed is able to be mass produced, it could be a “game-changer for farmers here and around the world” as it has been proven to reduce stock methane emissions by as much as 80% when added as a feed supplement at quantities as low as 2%. “The holy grail is going to be getting enough of the stuff,” Cawthron chief executive Prof Charles Eason told the Guardian. “It’s got a complicated life cycle. One of the barriers to getting this to market is growing enough of the stuff in ways that are cost effective to make it commercially viable.” O’Connor said the potential of the seaweed not only in New Zealand but around the globe was “huge”. “Other products typically provide reductions of between 10% and 20%,” he said. “Australian research estimates that if just 10% of global ruminant producers adopted Asparagopsis as an additive to feed their livestock, it would have the same impact for our climate as removing 50 million cars from the world’s roads.” Several hundred kilometres north in the Waikato city of Hamilton – the heart of dairy country – Dr Bjorn Oback from AgResearch has been given NZ$10m in government funding to design a “climate-smart cow”. There are more cows than people in New Zealand and while they have become a mainstay of the economy they also draw the ire of environmentalists for their high emissions profile, intensive impact on the land and muddying and polluting waterwarys. Oback says specialised breeding programmes can “take decades” to reach their desired outcome, but using genetic modification his team can design a climate-smart cow much more quickly and precisely – though the process remains controversial. As well as working towards manipulating the genes that control methane admissions , Oback is also focused on designing cattle that will thrive in a climate-altered future – animals that are more heat tolerant, hardy and productive. The five-year programme is only three months in, but is already targeting coat colour adapation – creating cows that are lighter in colour, making them more heat-tolerant. An important feature of the programme was using “elite” cattle, Oback said, so not only would they emit less methane and be climate adaptive – they’d also be super performers. “We’re taking a high-performing elite dairy background, and then we’re putting mutations on top of it.” Oback’s work is intertwined with Rowe’s research into low emitting sheep in the south of the country. For Oback to study – and try to manipulate – the genes of low-emitting stock, he is invested in the survival of Rowe’s flocks, who are now in their third generation. Rowe and her team’s work is creating a buzz in the agricultural community, because not only do her sheep have low methane emissions, they also produce more wool and are shown to be hardier and healthier than normal sheep. At this point no-one knowns why, but 20 major breeders have already signed on to produce flocks, and the low-emission sheep have the backing of industry body Beef and Lamb New Zealand. “We have demonstrated a 10% difference in methane produced between the average sheep in both the high and low methane breeding lines,” says Rowe. “It is a very natural system, it’s been used for eons. Domestication and breeding animals is something everybody is familiar with and it’s cumulative. If you breed an animal tomorrow the affect is always there and the offspring that come after them always have that effect.” Many of the methane reduction programmes in New Zealand are in their nascency, with climate smart cows at least five years away and the mass roll-out of Asparagopsis armata at least five to ten. But agricultural scientists and farmers say after decades of indecision and uncertainty concrete action is finally underway in New Zealand – and it is drawing global attention. Myles Allen is a climate change scientist at Oxford University and head of its Climate Dynamics group. He recently visited New Zealand and said he was impressed with how engaged industry leaders, government and the farming community were. “I only wish New Zealand could convince others … to take such a sensible approach.” he said.  "
nan
"
Share this...FacebookTwitter 
In assessing the global-scale trends in near-surface (0-20 m) ocean temperatures between 1900 and 2010, Gouretski et al. (2012) determined that the world’s oceans warmed by about 1.1°C between 1900 and 1945 (~0.24°C per decade), but then only warmed by an additional net 0.3°C between 1945 and 2010 (~0.046°C per decade), including a cooling trend between 1945 and 1975.
The early 20th century warming was therefore about 4 to 5 times greater both in magnitude and rapidity as the post-1945 warming.
Gouretski et al., 2012
“Both time series show a temperature increase from 1900 to about 1945, a slight decrease to the mid-1970s, and a temperature rise to the end of the record.”


Image: University of Hamburg, Gouretski et al., 2012
Interestingly, Gouretski et al. (2012) also point out that large regions of the oceans have been cooling since the 1990s.
“Decadal mean SST and 0–20 m layer anomalies calculated relative to the reference decade 2001–2010 give evidence of the general warming of the global ocean since 1900. However, large regions of the oceans have experienced cooling since the 1990s. Whereas cooling in the tropical Eastern Pacific ocean is associated with frequent La Nina events in the past decade, the cause of the cooling within the Southern Ocean remains unknown.”
According to Riser and 26 co-authors (2016), the globe’s oceans have warmed in some places, cooled in others, and the overall net change has been a warming of a little less than 0.2°C (0-1000 m) since about 1950, or about 0.03°C per decade.

The achievement of a few tenths of a degree of added warmth over the course of the last 6 ½ decades has been realized largely because the regions of the world where the oceans have been warming have slightly exceeded the cooling regions in volume.
The net difference between the warming and cooling trends for the globe is oddly referred to as global warming even though the warming trends have not been global, but regional.
Riser et al., 2016
“Most regions of the world ocean are warmer in the near-surface [0-700 m] layer than in previous decades, by over 1° C in some places. A few areas, such as the eastern Pacific from Chile to Alaska, have cooled by as much as 1° C, yet overall the upper ocean has warmed by nearly 0.2° C globally since the mid-twentieth century.”  
According to climate models and anthropogenic global warming theory, it has been expected that a long-term, gradually rising warming trend in the world-wide ocean would follow the trends associated with the rise of CO2 emissions.
The the oceans have not cooperated.
Instead, the world’s regional oceans have followed a decadal-scale variability, with pronounced warming and cooling episodes.   The lack of consistency with climate models has thus led scientists to conclude that it is “very difficult to determine whether significant anthropogenic change in [regional 0-2000 m ocean temperatures] … have occurred” (Yashavaev and Loder, 2017).
Below are several examples of the wide swaths of the Earth where ocean cooling (or non-warming) has been ongoing for at least the last decade to last several decades, including the North Atlantic Ocean, Pacific Ocean, Southern Ocean, and Indian Ocean.

North Atlantic

Yashayaev and Loder, 2017
“As a result of this intermittent recurrence of intensified Labrador Sea Water formation, the annual average temperature and density in the region’s upper 2000m have predominantly varied on a bi-decadal time scale, rather than having a long-term trend as might be expected from anthropogenic climate change. … [I]ntermittent recurrence of enhanced deep convection periods in the Labrador Sea, and the associated formation of major LSW classes, are contributing to a predominant decadal-scale variation in hydrographic properties which makes it difficult to determine whether anthropogenic changes are occurring. … This strong, apparently natural, decadal-scale variability makes it very difficult to determine whether significant anthropogenic changes in LSW formation and properties have occurred.”



Robson et al., 2017     
“In the 1990s anomalously strong ocean heat transport convergence dominates the SPG [Subpolar Gyre, North Atlantic] warming. … The cooling of the SPG [Subpolar Gyre, North Atlantic] after 2005 is dominated by a reduction in ocean heat transport convergence, particularly in the eastern SPG. The reduced ocean heat transport is largely due to a weakening ocean circulation.  By focusing on three independent case-studies of North Atlantic decadal change events the analysis presented here gives further support to the important role of ocean heat transport and ocean circulation in driving the observed changes in North Atlantic ocean heat content in the recent past.”


Gladyshev et al., 2017 
“After 2010, a sharp and stable freshening and cooling of SPMWs [Subpolar Mode Water] started in the eastern part of the North Atlantic. In the years 2010–2016, the mean temperature of the SPMW [Subpolar Mode Water] core in the Rockall Trough dropped by -0.73°C (-0.12°C/yr); in the Iceland Basin it dropped by -2.12°C (-0.35°C/yr), and salinity decreased by 0.12 psu (0.02 psu/yr) and 0.23 psu (0.04 psu/yr), respectively.”

Kim et al., 2017


de Jong and de Steur, 2016


Rosenthal et al., 2017


Pacific Ocean

Cheung, 2017


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->






Wilson et al., 2017


Li, 2017     
“In the Southern Ocean, the increasing trend of the total OHC slowed down and started to decrease from 1980, and it started to increase again after 1995. In the warming context over the whole period [1970-2009], the Pacific was losing heat, especially in the deep water below 1000 m and in the upper layer above 300 m, excluding the surface 20 m layer in which the OHC kept increasing through the time.”


Southern Ocean

Kusahara et al., 2017 
“In contrast to a strong decrease in Arctic sea ice extent, overall Antarctic sea ice extent has modestly increased since 1979. Several hypotheses have been proposed for the net Antarctic sea ice expansion, including atmosphere/ocean circulation and temperature changes, sea ice-atmospheric-ocean feedback, increased precipitation, and enhanced basal meltwater from ice shelves. Concomitant with this positive trend in Antarctic sea ice, sea surface temperatures (SSTs) over the Southern Ocean south of approximately 45°S have cooled over this period [since 1979].”

Latif et al., 2017
“The Southern Ocean featured some remarkable changes during the recent decades. For example, large parts of the Southern Ocean, despite rapidly rising atmospheric greenhouse gas concentrations, depicted a surface cooling since the 1970s, whereas most of the planet has warmed considerably. In contrast, climate models generally simulate Southern Ocean surface warming when driven with observed historical radiative forcing. The mechanisms behind the surface cooling and other prominent changes in the Southern Ocean sector climate during the recent decades, such as expanding sea ice extent, abyssal warming, and CO2 uptake, are still under debate. Observational coverage is sparse, and records are short but rapidly growing, making the Southern Ocean climate system one of the least explored. It is thus difficult to separate current trends from underlying decadal to centennial scale variability.”

Turney et al., 2017     
“Occupying about 14% of the world’s surface, the Southern Ocean plays a fundamental role in ocean and atmosphere circulation, carbon cycling and Antarctic ice-sheet dynamics. … As a result of anomalies in the overlying wind, the surrounding waters are strongly influenced by variations in northward Ekman transport of cold fresh subantarctic surface water and anomalous fluxes of sensible and latent heat at the atmosphere–ocean interface. This has produced a cooling trend since 1979.”


Jones et al., 2016


Indian Ocean

Zinke et al., 2016


Yan et al., 2015


Both The Cooling And The Warming Trends For Recent Decades Follow Natural Oscillatory Patterns, Not Trends In Anthropogenic CO2 Emissions

Gong et al., 2017
“The inter-annual relationship between the boreal winter Arctic Oscillation (AO) and summer sea surface temperature (SST) over the western tropical Indian Ocean (TIO) for the period from 1979 to 2015 is investigated. The results show that the January–February–March AO [Arctic Oscillation] is significantly correlated with the June–July–August SST and SST tendency. … The multi-month SST tendency, i.e., the SST difference of June–July–August minus April–May, is correlated with the winter AO at r = 0.75.Investigation of the regional air–sea fluxes and oceanic dynamics reveals that the net surface heat flux cannot account for the warming, whereas the oceanic Rossby wave plays a predominant role. During positive AO winters, the enhanced Arabian High causes stronger northern winds in the northern Indian Ocean and leads to anomalous cross-equatorial air-flow. … The winter AO-forced Rossby wave propagates westward and arrives at the western coast in summer, resulting in the significant SST increase.”


Belohpetsky et al., 2017
“It is well known that most short term global temperature variability is due to the well-defined ENSO natural oscillation (see: Wang and Fiedler, 2006). During strong El Niño events global average temperature rises by a few tenths Kelvin and reverts back subsequently. … The residual dynamics left after adjusting global surface temperature anomalies (1950-2014) for short-term variability from El Niño Southern Oscillation (ENSO) and volcanic eruptions have a staircase pattern. Linear trends for three quasi-stable periods 1950-1987, 1988-1997 and 1998-2014 are near zero with nearly all warming occurring during two step-like shifts in the years 1987/1988 and 1997/1998.  A notable consequence of the staircase dynamics of recent warming is that observed temperature anomalies (HadCRUT4.5) from 1950 till 2014 could be almost reproduced as the linear sum of only two factors(!) : ENSO variability and the staircase function.”

Gong et al., 2017
“During the past three decades, the most rapid warming at the surface has occurred during the Arctic winter. By analyzing daily ERA-Interim data, we found that the majority of the winter warming trend north of 70°N can be explained by the trend in the downward infrared radiation (IR). This downward IR trend can be attributed to an enhanced poleward flux of moisture and sensible heat into the Arctic by poleward propagating Rossby waves, which increases the total column water and temperature within this region.”

He et al., 2017 
“As pointed out by Cohen et al. (2014) that continental winter SAT [surface temperature] trends since 1990 exhibit cooling over the midlatitudes. The negative trends extend from Europe eastward to East Asia, with a center of maximum magnitude to the west of the Baikal.  As reviewed above, the AO/NAO [Arctic Oscillation/North Atlantic Oscillation] shows an in-phase relationship with the SAT [surface temperatures] over Eurasia. … [T]he negative trend in the AO/NAO might explain the recent Eurasian winter cooling. … Additionally, the relationship between the winter AO and surface-climate anomalies in the following spring might be modulated by the 11-year solar cycle (Chen and Zhou, 2012). The spring temperature anomalies in northern China related to the previous winter AO were larger and more robust after high solar cycle winters. However, spring temperature anomalies became very small and insignificant after the low solar cycle winters. … Numerous atmospheric scientists have documented that the AO could impact significantly the climate over Europe and Far East. …  It is evident that a positive winter AO causes warmer winters over East Asia through enhancing Polar westerly jet which prevents cold Arctic air from invading low latitudes.”


Wu et al., 2017
“The enhanced warming observed in the Eastern China Coastal Waters (ECCW) during the last half-century has received considerable attentions. However, the reason for this warming is still a subject of debate. Based on four different Sea Surface Temperature datasets, we found that the most significant warming occurred in boreal winter during 1982–1998, although the warming trends derived from these datasets differ in magnitude. We suggest that the rapid warming during winter is a result of the asymmetry in the El Niño–Southern Oscillation teleconnection, through which El Niño events induce significant warming over the ECCW at its peak, whereas La Niña events fail to do the opposite that would completely reverse the trends; in addition, there were more El Niño than La Niña events during the recent decades. All these contribute to the winter warming during 1982–1998.”

Mermelstein, 2017     
“[T]he 1940-1978 decrease in CONUS [continental U.S.] temperatures was caused more by the negatively trending oscillatory modes of the AMO/PDO than other factors, and the 1978-2001 increase in temperatures was caused more by the positively trending oscillatory modes of the same oscillations. The small increase, or rather stagnant nature in U.S. CONUS temps since 2001, was likely due to peaking positive modes of the AMO/PDO. In the same way that the AMO and PDO can modify the regional temperatures, we see the same types of effects on precipitation, snowfall and drought in the different regions of the U.S. … It was not until 2003 (Anastasios, Swanson, & Kravtsov, 2003, 2007) that models were created that suggested that these cycles, namely the Pacific Decadal Oscillation (PDO) and the Atlantic Multidecadal Oscillation (AMO) synchronized with each other. Using this as a base, we can explain the major climate shifts that have occurred since scientists began collecting data in the late 1800’s: 1908, 1932, 1973, and 2000. While the most noticeable change in these shifts was on global temperature, effects on the regional, sensible weather in the U.S. were also identified in these same time frames. Through analysis it has been theorized that these shifts are caused by the oceans, and are in fact the main drivers of the climate, and the sensible weather experienced in the United States (Klotzbach & Gray, 2009).”

Fan and Yang, 2017
“The wintertime Arctic temperature decreased from 1979 to 1997 and increased rapidly from 1998 to 2012, in contrast to the global mean surface air temperature [which] increased between 1979 and 1997, followed by a hiatus… A recent study suggests a possible role of the Pacific Ocean decadal oscillation in regulating wintertime climate in the Arctic (Screen and Francis 2016).  … The ‘‘greenhouse effect’’ of water vapor and clouds [CO2 not mentioned as contributing to the GHE] may amplify the effect of winds on Arctic winter climate. …  The objectives of this study are to assess how much natural–internal variability has contributed to climate changes in these [Arctic] regions from 1979 to 2012 … In summary, the correlation analyses presented in this paper shows a natural mode of Arctic winter variability resulting from the Nordic–Siberian seesaw of meridional winds […] is associated with two-thirds of the interannual variance [cooling-warming] of winter-mean Arctic temperature between 1979 and 2012, and possibly contributed a substantial fraction of the observed Arctic amplification [1998-2012 warming] in this period.”


Piecuch et al., 2017
“The subpolar North Atlantic (SPNA) is subject to strong decadal variability, with implications for surface climate and its predictability. In 2004–2005, SPNA decadal upper ocean and sea-surface temperature trends reversed from warming during 1994–2004 to cooling over 2005–2015. … Over the last two decades, the SPNA has undergone a pronounced climate shift. Decadal OHC and SST trends reversed sign around 2004–2005, with a strong warming seen during 1994–2004 and marked cooling observed over 2005–2015. These trend reversals were pronounced (> 0.1 °C yr−1 in magnitude) in the northeastern North Atlantic (south and west of Iceland) and in the Labrador Sea. … To identify basic processes controlling SPNA thermal variations, we diagnose the SPNA heat budget using ECCOv4. Changes in the heat content of an oceanic control volume can be caused by convergences and divergences of advective, diffusive, and surface heat fluxes within the control volume.  [Advective heat convergence] explains 87% of the total [ocean heat content] variance, the former [warming] showing similar decadal behavior to the latter [cooling], increasing over 1994–2004, and decreasing over 2005–2015. … These results demonstrate that the recent SPNA decadal trend reversal was mostly owing to advective convergences by ocean circulation … decadal variability during 1993–2015 is in largest part related to advection by horizontal gyres.”


Cheung, 2017
“The sea surface temperature (SST) of the Eastern Equatorial Pacific (EEP) exerts primary control on global surface temperature (e.g. Halpert and Ropelewski 1992; Wigley 2000) and regional climate (e.g. Ropelewski and Halpert 1987) through different modes of climate variability including the El Niño Southern Oscillation (ENSO) and the Pacific Decadal Oscillation (PDO). With such profound impacts, it is important to understand the evolution of SST in EEP, specifically the dynamics of these climate modes. Rigorous studies over the past decades have shed insights on these two climate modes. ENSO is known to affect regional and global climates on interannual timescales. During an El Niño event, a weakening of easterly trade wind stimulates propagation of Kelvin waves from the western equatorial Pacific to the EEP, which in turn reduces the slope of the thermocline and suppresses upwelling. The decrease in pressure gradient reinforces the weakening of the trade winds through the Bjerknes feedback and ultimately creates an El Niño condition (e.g. Collins et al. 2010). The reorganization of the ocean and the atmosphere due to El Niño raises the global mean surface temperature (e.g. Halpert and Ropelewski 1992; Wigley 2000) and alters regional climate, for example causing drought in Australia (Cai et al. 2011), pluvial in Southwest United States (Ropelewski and Halpert 1987), and changing tropical cyclone frequencies in the Western North Pacific (Camargo and Sobel 2005; Chan 1985). The opposite spatial pattern and teleconnections happen during a La Niña event.”

Wang et al., 2017
“The driving forces of climate change were investigated and the results showed two independent degrees of freedom —a 3.36-year cycle and a 22.6-year cycle, which seem to be connected to the El Niño–Southern Oscillation cycle and the Hale sunspot cycle, respectively. … Solar variability has been shown to be a major driver of climate in central Europe during the past two millennia using Δ14C records. Furthermore, this result is essentially in good agreement with the findings of Scafetta e.g. refs 17, 18, 19, who found that the climate system was mostly characterized by a specific set of oscillations and these oscillations (61, 115, 130 and 983 years) appeared to be synchronous with major astronomical oscillations (solar system, solar activity and long solar/lunar tidal cycles).”
Share this...FacebookTwitter "
"When Donald Trump recently announced tariffs on steel and aluminium imports he was condemned by proponents of free trade across the world. His critics said the US president had not understood how protectionist policies would spell disaster for the world economy. Fair enough. But this is the same Trump whose decision to withdraw from the Paris climate agreement also met with massive disapproval.  Trump is simultaneously chided for refusing to cut emissions, and for promoting a trade policy that reduces the causes of such emissions. Both sets of critics may be right on their own terms, but the contradiction between the two reproaches exposes big problems in the mainstream modern worldview. Is it really reasonable to advocate for both more trade and greater concern for the environment? For centuries world trade has increased not only environmental degradation, but also global inequality. The expanding ecological footprints of affluent people are unjust as well as unsustainable. The concepts developed in wealthier nations to celebrate “growth” and “progress” obscure the net transfers of labour time and natural resources between richer and poorer parts of the world.  For instance, the household of an average American couple with one child has the equivalent of an invisible servant working full time for it outside the nation’s borders, while the average Japanese household with one child uses three hectares of land overseas. Yet such material asymmetry appears to be a side issue for mainstream economists, who continue to assert the overall benefits of free trade. This same ignorance is particularly apparent in the fight against climate change. Most environmentalists and researchers put their faith in new technologies for harnessing the sun and wind, and hope that politicians can be persuaded to act. But solar panels and wind farms are not merely products of human ingenuity that have been revealed to us by nature. Nor are they magical keys to limitless energy.  Renewable energy technologies emerged in this specific human society – inequality, globalisation and all – and their very feasibility is dependent on world market prices. Like other modern technologies they depend on high domestic purchasing power combined with cheap Asian labour, Brazilian land, or Congolese cobalt.  Almost 50 years ago the ecological economist Nicholas Georgescu-Roegen warned that the notion that solar power could replace fossil energy was an illusion, because it would require such enormous volumes of materials to harness the requisite amounts of diffuse sunlight to satisfy a modern high-tech society. Some of these materials are rare and expensive and degrade the environment. Moreover, the United Nations Environmental Programme recently warned that the world is heading for ecological disaster unless we use less resources per dollar of economic growth.  The Czech-Canadian energy researcher Vaclav Smil has found that switching to renewable energy would use up vast amounts of land, reversing the land-saving benefits of the Industrial Revolution. Meanwhile the money to invest in solar is still ultimately generated from cheap labour and cheap land. The fact that solar panels have recently become less expensive is partly because they are increasingly being manufactured by low-wage labour in Asia.  When viewed this way it is perhaps no wonder that renewable energy has not even begun to replace fossil energy, and has only been added to the still-increasing use of oil, coal and gas. Solar power still only accounts for about 1% of global energy use. It has hardly made a dent on the global use of energy for electricity, industry, or transports. And this cannot be blamed on the oil lobby, as is illustrated by the case of Cuba. Nearly all of the island’s electricity still derives from fossil fuels. There is obviously something problematic about shifting to solar power that goes beyond corporate obstruction. To explain it in terms of a lack of capital or in terms of the vast land requirements are two sides of the same coin.  So here is the impasse of modern civilization: the free trade promoted by most economists and politicians continues to drive a substantial part of the greenhouse gas emissions that they want to reduce, and yet the sustainable technologies they propose to cut emissions are in themselves dependent on economic growth, international trade, and the use of more and more natural resources.  So how to break this impasse? Economists could start by recognising that the economy is not insulated from nature, just as engineering is not insulated from world society. Global challenges of sustainability, justice and resilience all demand much more integrated thinking. This will involve confronting conventional ideologies of technological progress and free trade. Rather than nervously safeguarding world trade with its escalating greenhouse gas emissions, we have every reason to reconsider what might be perceived as true human progress and quality of life. Instead of economic policies maximising economic growth and resource use, humankind needs to develop an economy that is aligned with the constraints of our fragile biosphere – and a science of engineering that takes account of global inequalities."
"
Share this...FacebookTwitterA new paper by renowned Swedish sea level expert Prof. Axel Mörmer published in the International Journal of Earth & Environmental Sciences dumps lots of cold water on the premise that today’s sea level rise is caused by man and is unusual.
Mörner’s paper looks back at the last 500 years of sea level rise and shows that natural variables are the major drivers, and not man-made CO2-driven global warming.
Previously no study in the Fiji Islands had been devoted to the sea level changes of the last 500 years and so no serious prediction can be made. What was needed was a good understanding of the sea level changes today and in the past centuries. Mörner’s study helps to fill that gap and to answer questions concerning today’s sea level rise.
The Swedish scientist summarizes in the paper’s abstract that there is a total absence of data supporting the notion of a present sea level rise; on the contrary all available facts indicate present sea level stability.

Source: Mörner, Int J Earth Environ Sci 2017, 2: 137, https://doi.org/10.15344/2456-351X/2017/137
Sea level changes over the past 500 years at Ysawa Islands, Fiji, show that sea level was +70 cm high in the 16th and 17th centuries, -50 cm low in the 18th century and that stability (with some oscillations) prevailed in the 19th, 20th and early 21st centuries.
This, Mörner writes, is almost identical to the sea level change documented in the Maldives, Bangladesh and Goa (India), and thus would point to a mutual driving force.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The pattern is the same at other locations:

Source: Mörner, Int J Earth Environ Sci 2017, 2: 137, https://doi.org/10.15344/2456-351X/2017/137
The paper also states that the recorded sea level changes are anti-correlated with the major changes in climate during the last 600 years. Therefore, Mörner concludes that glacial eustasy cannot be the driving force.
The explanation behind the sea level changes, Mörner believes, seems to be rotational eustasy with speeding-up phases during Grand Solar Minima forcing ocean water masses to the equatorial region, and slowing-down phases during Grand Solar Maxima forcing ocean waster massed from the equator towards the poles.
The paper summarizes:
This means there are no traces of a present rise in sea level; on the contrary: full stability.”
About the author:
Nils-Axel (”Niklas”) Mörner took his Ph.D. in Quaternary Geology at Stockholm University in 1969. Head of the institute of Paleogeophysics & Geodynamics (P&G) at Stockholm University from 1991 up to his retirement in 2005. He has written many hundreds of research papers and several books. He has presented more than 500 papers at major international conferences. He has undertaking field studies in 59 different countries. The P&G institute became an international center for global sea level change, paleoclimate, paleoseismics, neotectonics, paleomagnetism, Earth rotation, planetary-solar-terrestrial interaction, etc.  Among his books; Earth Rheology, Isostasy and Eustasy (Wiley, 1984), Climate Change on a Yearly to Millennial Basis (Reidel, 1984), Paleoseismicity of Sweden: a novel paradigm (P&G-print, 2003), The Greatest Lie Ever Told (P&G-print, 2007), The Tsunami Threat: Research & Technology (InTech, 2011), Geochronology: Methods and Case Studies (InTech, 2014), Planetary Influence on the Sun and the Earth, and a Modern Book-Burning (Nova, 2015).
 
Share this...FacebookTwitter "
"The New South Wales police investigation into energy minister Angus Taylor over the use of doctored documents to attack Sydney’s lord mayor has been referred to the Australian federal police. The minister greeted the news by claiming NSW police had confirmed to his solicitors they had “concluded their investigation”.  “The minister welcomes that fact,” his spokeswoman said. “This supports his repeated previous statements that neither he, nor any member of his office, altered the document in question. “Of course minister Taylor will cooperate with the AFP and any enquiries they wish to make, although he fully expects they will conclude that this matter is baseless.” The matter was referred to the federal police on December 20, but has only just been made public. A statement from the NSW police confirmed the matter had been handed over in a two line press release. “Following an investigation by the State Crime Command’s Financial Crimes Squad, the matter has been referred to the Australian Federal Police. “No further comment is available.” Sky News reported the NSW police have handed over the investigation because of jurisdictional issues, arguing any crime, if there was one, occurred in Canberra, not Sydney. Sydney’s lord mayor, Clover Moore, declares a climate emergency, which is endorsed by the council. 29 September: The energy and emissions reduction minister, Angus Taylor, writes to Clover Moore, claiming the council’s annual report “shows your council spent $1.7m on international travel and $14.2m on domestic travel” in 2017-18. The Daily Telegraph publishes a story on page three and online accusing Moore of hypocrisy over the council’s emissions. The story quotes a letter from Taylor which says the council spent more than $15m on domestic and international travel. Moore disputes the Telegraph story and asks the reporter to provide evidence of this claim. The reporter provides a page from the council’s annual report. The page contains two figures purporting to show the council spent “$14.2” in expenses on interstate travel and “$1.7” on overseas visits.  Moore checks the annual report: on page 14 it shows councillors spent $4,206.32 on interstate travel and $1,727.77 on overseas visits. Moore angrily disputes the story with Taylor via Twitter. Moore writes to Taylor asking him to “ correct a stark error in your letter” saying the $15m figure was grossly inaccurate. Moore lodges complaint with the Press Council. The AFP confirmed the investigation had been referred. “The AFP can confirm it received a referral from New South Wales Police on December 20, 2019, in relation to the alleged doctoring of a document,” it said in a statement. “While this matter is being evaluated, it would be inappropriate to comment any further.” The Labor leader, Anthony Albanese, said the opposition would continue to pursue the matter when parliament resumed in February. “One of the reports indicates that the AFP have been investigating for some time that it hasn’t been announced today,” he told the ABC. “We need to know exactly what the circumstances are. And we’ll continue to scrutinise that. And scrutinise as well the level of cooperation that is there with any inquiry to make sure that happens. This is pretty clear. There’s a document, it came from somewhere. The evidence will be there, in terms of computer trails of where it came from.” The NSW police launched an investigation into the origins of an altered document which was used to politically attack the Sydney lord mayor, Clover Moore, after Labor contacted the police to ask if an investigation was warranted. A strike force investigation into the issue was launched, leading Scott Morrison to personally contact NSW police commissioner Mick Fuller, by phone, leading to allegations, which were strongly denied, he had influenced the investigation. At the time, Fuller said he believed the matter would be wrapped up in a week. “To be honest with you, I actually don’t feel as though the allegations themselves are serious, in terms of the things that I would normally stand up and talk about the types of crimes,” he said. “But at the end of the day they’re public figures, and at the end of the day I’m assuming that the public and the media would expect that we take all matters seriously against public figures.” Exclusive: Angus Taylor investigation referred by NSW cops to Australian Federal police with NSW cops arguing that if an offence occurred, it occurred in Canberra, in the AFP's jurisdiction"
"The world is about to experience one of the biggest housing booms in history over the coming decades. In the UK, the housing crisis is a recurring news story, with a lack of affordable housing and problems with the quality of the housing stock. However, these problems pale in comparison with the scale of the housing crisis facing much of the world. The UN estimates the world’s population will grow by an additional two billion people by 2050, with most growth from developing countries in Africa and Asia. Hundreds of millions of additional homes will be needed in these regions over the coming decades. At the same time, another problem looms: our need to reduce our global greenhouse emissions by 60% in order to keep average global warming below 2°C. Read more: The world needs to build more than two billion new homes over the next 80 years  Much of modern housing is built with two materials: fired bricks and concrete. These have served well in terms of their strength, durability and ease of use. Their major drawback lies in their environmental impact – in particular, the carbon emissions associated with their production. Both materials need high temperatures in order to bring about the chemical changes that enable them to gain strength. This is typically over 1,000°C for firing bricks, and 1,450°C for the production of cement. Cement production alone accounts for between 5-10% of global greenhouse gas emissions. Given the scale of reductions in greenhouse gas emissions required, and the sheer number of homes that will need to be built, it is clear that we need new construction materials. These construction materials must provide comparable benefits to concrete and brick – but with an acceptably low impact on the environment, and they will need to be affordable and socially desirable in the developing countries where most house-building will occur. There won’t be one single material that replaces fired brick and concrete. Instead, there’s likely to be a range of new construction materials and building technologies, each suited for different regions to reflect their local resources, climate and culture. One promising candidate material we have been investigating in our ongoing research into stronger and more sustainable bricks for the future, are geopolymer-stabilised soil materials (GSSM). A geopolymer is a hard and durable substance similar to cement, made out of chains of aluminium, silicon, alkali metal and oxygen atoms. Geopolymer-based materials are a young family of human-made materials, first demonstrated in 1940. They can be made in many ways using different starting ingredients, but so far have mostly used industrial waste, such as ash from coal power stations. Some geopolymer materials have already been used in construction since the 1970s, but usually in small volumes for specialist applications. There is untapped potential in using soil as the starting ingredient, which is found in large quantities over much of the planet. Geopolymer-stabilised soil materials are made in a simple process. Soil (taken from beneath the valuable surface layer) is mixed with an alkaline activating solution, which contains chemicals similar to those found in household cleaning products. This activating solution dissolves the clay minerals in soil into their constituent atoms. The resulting mix has a play-dough like consistency, and can be shaped in moulds. The moulded bricks are heated at 80-100°C. During firing, the dissolved atoms rearrange to form a geopolymer. Once fired, the strong geopolymer that was formed in the process acts to stabilise the remaining soil, forming the final brick. The potential benefits of GSSM are the low temperatures required to heat the brick, and the abundance of the primary starting ingredient – soil. Depending on the exact soil, chemical recipe and heating process used, these bricks could have half the carbon emissions of concrete, and a quarter of the amount produced by conventional fired bricks. GSSM would not be used in high-strength applications like high-rise buildings, but it has the potential to be a very good replacement for concrete in low and mid-rise housing, which is how much of the new housing in developing countries is being built. These materials are still yet to be ready commercially. Although bricks have been made this way using individual soils, there is still work to be done around designing a “chemical recipe book” for different types of soils, as its composition varies between regions, and can even vary within one field. Although soil itself is abundant, the alkaline-activating solutions currently used require industrial chemicals. While these are available in bulk quantities and are relatively inexpensive, more research is required into identifying more abundant locally available sources for the activating solution, such as plant ashes from agricultural waste. Aside from technical challenges, there will also be a battle of persuasion. Given that much of the world has moved away from traditional earth-based housing in recent years, there needs to be a culture of confidence – rather than conservatism – about using innovative materials for homes. The stakes are high – housing supply and climate change are two of the biggest issues of our time. These more sustainable bricks may well be a piece of the puzzle."
"In early December, delegates representing Young Liberal branches across the state voted overwhelmingly in approval of a motion recognising the reality of climate change and the need for action. The NSW Liberal party’s youth wing recognises this a particularly important issue facing our generation, as our generation will have to face the risks brought about by climate change.  It is the duty of government to be awake to the challenges of the future. As the great conservative thinker Edmund Burke recognised, current generations hold the present in trust for the future. And climate change is a significant risk that will affect the future of my generation. The climate change debate in Australia has unfortunately become a bit of a poisoned well. While Boris Johnson’s Conservatives went to the UK election with a commitment to achieve net zero emissions by 2050, any talk of climate change can spell the death of a Liberal leader in Australia. It has become divisive and transformed into a tribal marker between those who are “woke” and those who are “anti-PC”. It shouldn’t be this way. It should not be a matter of conservative versus progressive. It should be a matter of science and economics. Liberals should not surrender the debate to those on the left who would use this challenge to push an anti-market, big-government agenda that would do great damage to our economy and hurt those who can least afford it. As the prime minister has rightly said, we do not need to choose between a strong economy and action on climate change. We don’t need to choose between low electricity prices, a decent standard of living for all and ensuring a liveable planet in the future. The science and the economics tells us we can mitigate the effects of climate change through more efficient energy use and allowing greater investment in more efficient renewable energy generation. It is already the case that renewables are on track to become cheaper and more efficient than traditional fossil fuel sources. This is why calls for the government to directly own and operate coal-fired power stations is misguided. Not only is it illiberal – calling for the socialisation of power – but it makes zero economic sense for the government to be using taxpayer money to make a long-term investment in current coal technology that will be less efficient and more expensive than other alternatives in the future. The government shouldn’t pick winners – whether solar, wind, hydro, nuclear or coal. It should create the framework within which there is investment certainty for the private sector to develop and invest in the most efficient technologies. In short, we need policy settings that will provide the investment certainty we’ve been lacking in order to allow the market to work. Government should ensure that Australia is well placed to take advantages of innovation in the clean energy space. Good policy will ensure we keep energy costs low, using the most efficient technology available. Government should incentivise the energy sector and private sector generally to find innovative ways to reduce their emissions without heavy-handed taxes, extravagant subsidies and heavy-handed bans on mining that will have disastrous economic consequences and hurt low and middle-income Australians. Government can do this through tax break incentives and asset write offs, as well as smart public investment in research and development. Let coal, nuclear, wind, solar, gas, pumped hydro, hydrogen and other technologies compete to be the cheapest and most efficient. If we have the right policy settings that encourage innovation and provide certainty to the market, we will see the market picking cheaper, lower emissions technology. It’s time to stop turning climate change into a climate culture war and time to start pursuing good policy which will ensure we have low energy costs and use the most efficient, low-emissions technology available. Our future depends on it. Chaneg Torres is the president of the NSW Young Liberals"
"A magnitude 5.5 earthquake shook the industrial city of Pohang in South Korea on 15 November 2017, injuring almost 100 people and damaging thousands of buildings at a cost of millions of US dollars. Six months on, two academic papers have suggested that fracking was probably the cause of this earthquake. A local geothermal energy project had been injecting highly pressurised water into two, four kilometre-deep boreholes for almost two years. This process, known as hydraulic fracturing or fracking, creates or enhances fractures in rock to harness the heat stored there. Once the network of fractures connected the two boreholes, the plan was to pump water into one, circulate it through the granite rock, absorbing heat, then extract it from the other borehole and use the heat to generate electricity. Afterwards, the cooled water would be reinjected to begin the process over again. The proposed cause-and-effect connections now identified make the Pohang earthquake by far the largest recorded for which fracking is the likely cause. The previous record holder linked to geothermal development – of magnitude 3.4 – occurred a decade ago in the Swiss city of Basel. Ten days after the Pohang earthquake, the South Korean government suspended the geothermal plant’s operations and ordered an investigation into a possible link, which is still ongoing. The first of the papers reports a collaboration between researchers from Zurich, Switzerland, Potsdam, Germany, and myself. We used public domain data from seismographs (instruments for recording ground motion caused by earthquakes) and remote-sensing satellites to determine the location and position of the geological fault that slipped in the earthquake. Both types of data indicate the rupture of a fault running southwest to northeast and dipping steeply to the northwest. Recorded by satellites, the rock above this fault moved upward, lifting the Earth’s surface by four centimetres. This analysis indicates that the fault slipped over a depth range of three to six kilometres, encompassing the depth of the injection and passing within hundreds of metres of the boreholes. This points strongly to a connection between the high-pressure fluid injection and the earthquake.  The second paper, by Korean colleagues, reports the locations of the many aftershocks of the Pohang earthquake, more accurately defining its fault plane. As shown in the cross-section below, their study indicates the fault passing between the bases of the two boreholes where water was injected, cutting across one borehole at a depth of around three to four kilometres. The results from the two papers are consistent with each other, despite the different types of data used, providing strong confirmation. Preliminary surveys for the geothermal project over a decade ago identified a fault (its position at a depth of four kilometres is shown in orange on the image below) with essentially the same position and alignment as that which slipped in the November 2017 earthquake (its position, also at a depth of four kilometres, is shown in red for comparison). Taking into account the uncertainties in each of the analyses, this comparison indicates that these surveys revealed the fault that is now known to have slipped in the November 2017 earthquake.  Further surveys taken before drilling started led the developers to revise their plans to focus on a WNW-ESE fracture in the granite beneath the site. The boreholes, shown in the cross-section above, were designed for flow in this direction. During drilling, fluid (or drilling “mud”) leaked from one borehole at roughly the depth where the fault cuts across the well bore, with fault gouge – crushed rock debris produced by the movement of the rocks on either side of a fault – indicating a fault there before any injection had taken place. More than 20 years ago, a set of criteria was devised for assessing whether earthquakes are caused by high pressure fluid injection. The Pohang earthquake meets most of these criteria. Notably, it occurred within hundreds of metres of the injection, and at the same depth. Also, during much of the injection, the pressure was high enough for standard calculations to predict the slipping of the fault, if water at this pressure reached this fault. However, the two-month interval between the end of injection in September 2017 and the earthquake in November provides a potential argument against any cause-and-effect connection. A possible explanation for this delay is that once water entered the fault it began to dissolve the granite, gradually weakening the fault so it eventually failed and slipped. It seems entirely plausible at this stage to believe that the earthquake was caused by the injection, and to examine the implications. One reason this seismicity is significant is the disproportion between the size of the main shock and the volume of water injected. A theory has been developed for determining the “worst case scenario” earthquake feasible for a given volume of fluid injection. The overall volume injected at Pohang was roughly 12,000 cubic metres, whereas this theory requires around 1,000 times more volume to cause an earthquake as large as magnitude 5.5. This suggests that the theory needs improving, possibly to incorporate the injection pressure as well as the injected volume. In the meantime, designs for future geothermal fracking projects might require independent expert assessment, as is already the case for projects involving fracking for shale gas in the UK. At Pohang, this could have highlighted the potential implications of a fault cutting across the site, leading to recommendations such as limiting the injection pressure, which could have lessened the force of the earthquake. Elsewhere in the world, successful deep geothermal projects have been designed to incorporate circulation of water along faults, requiring high pressure injection to create or enhance fractures. Pohang illustrates the need for accurate information on the geometry of faults on which project designs can be based.  Still, the disproportion between the small volume of water injected at Pohang and the size of the November 2017 earthquake may give geothermal fracking developers worldwide pause for thought. It may well be the game changer for their industry."
"It may not seem like one of life’s great mysteries, but a quick internet search reveals that people from across the world – London to Hong Kong, Cape Town to Buenos Aires – are asking this same question: for all the pigeons out there in our cities, where are all the dead ones? Alas they’re not pondering the presence of pigeon heaven, but rather, where are all the bodies?  Pigeons are as ubiquitous in the world’s cities as bad traffic, buskers, and late-night takeaways. London alone is estimated to contain more than a million pigeons, inhabiting the many parks and gardens that crisscross its 1,000 square miles. Given these vast numbers – and the fact that an urban pigeon seldom lives for more than three or four years – it’s a wonder why they are not strewn across city streets. There are several possible reasons for this. First, pigeons are just one part of a wide array of creatures to have adopted our cities as their home. Foxes, rats, gulls, crows and ravens all do a wonderful job of cleaning up any carrion they come across, including deceased pigeons. These species perform inestimable services to the urban ecosystem, reducing human exposure to rotting matter and helping cut the transmission of infectious diseases. Alongside these native janitors, domestic cats are equally happy to take care of a dead or injured pigeon. It is estimated that there are half a million cats living in London alone  –  roughly two pigeons per cat – and if you’re “lucky” they might bring one home as a present. Whether a resident moggy or some other carnivore, this network of surreptitious street cleaners will usually whisk away any pigeon corpses long before they’re seen by human eyes. Most pigeons, however, don’t simply drop dead on the ground. To understand where pigeons themselves are likely to go when feeling vulnerable or unwell, we need to delve into their origins. The pigeons we see in cities are domestic pigeons who have undergone some serious “rewilding”. They were originally bred as homing pigeons, trained birds who relayed important messages over large distances long before telephones. These pigeons even won prestigious medals in both world wars. Going back further, the original homing pigeons were bred centuries ago from wild rock doves, a species which inhabits sea cliffs and coastal caves. Cities, with their high-rise buildings and elevated ledges, provide ideal nest sites for feral pigeons, and create an environment reminiscent of their ancestral homes. This background means that, when sick or injured, pigeons instinctively retreat to dark, remote places – ventilation systems, attics, building ledges – hoping to remain out of reach and unnoticed by predators. The predators don’t see them, but neither do we: often when pigeons expire, they are in hiding. But what actually causes a pigeon to die? As they get older, pigeons become more susceptible to disease, and often become slower to react to oncoming predators. It is well-established that when a predator attacks a flock of birds, slower individuals can become isolated from the group, making them easy prey. Dying of old age is not a luxury afforded to most pigeons: as soon as they shows signs of slowness or sickness, many are snapped up by peregrine falcons, sparrowhawks, or other predators. One slightly macabre alternative that occurs in big cities, involves the netting that often hangs around buildings. Birds can easily fly into it and become entangled: not just old or sick pigeons, but any bird unfortunate enough not to notice it. Netting is usually high above the ground, so after some fruitless struggling dead pigeons usually hang there, away from the scavengers below.  Whether snatched midair by birds of prey, entangled by man made obstacles or alone in a remote corner of a skyscraper’s roof garden, there are many ways that pigeons pass on from this world. But they all take place within an internal urban ecosystem, that, for the most part, is hidden from our sight."
"Most people would never think of London as a forest. Yet there are actually more trees in London than people. And now, new work by researchers at University College London shows that pockets of this urban jungle store as much carbon per hectare as tropical rainforests. More than half of the world’s population lives in cities, and urban trees are critical to human health and well-being. Trees provide shade, mitigate floods, absorb carbon dioxide (CO₂), filter air pollution and provide habitats for birds, mammals and other plants. The ecosystem services provided by London’s trees – that is, the benefits residents gain from the environment’s natural processes – were recently valued at £130m a year.  This may equate to less than £20 a year per tree, but the real value may be much higher, given how hard it is to quantify the wider benefits of trees and how long they live. The cost of replacing a large, mature tree is many tens of thousands of pounds, and replacing it with one or more small saplings means you won’t see the equivalent net benefit for many decades after. Trees absorb CO₂ during photosynthesis, which is then metabolised and turned into organic matter that makes up nearly half of their overall mass. Urban trees are particularly effective at absorbing CO₂, because they are located so close to sources such as fossil fuel-burning transport and industrial activity.  This carbon storage potential is an extremely important aspect of their value, but is very hard to quantify. A 120-year-old London plane tree can be 30 metres tall and weigh 40 tonnes or more, and some of the carbon in its tissues will have originated from Victorian coal fires.  Measuring the height of a tall tree is difficult, because it’s rarely clear exactly where the topmost point is; estimating its mass is even harder. Typically, tree mass is estimated by comparing the diameter of the trunk or the height of the tree to the mass of similar trees (ideally the same species), which have been cut down and weighed in the past. This process relies on the assumption that trees of a certain species have a clear size-to-mass ratio.  But a fascinating property of trees is how variable they can be, depending on their environment. So inferring the mass of urban trees from their non-urban counterparts introduces large uncertainties. The UCL team use a combination of cutting-edge ground-based and airborne laser scanning techniques, to measure the biomass of urban trees much more accurately. Lidar (which stands for light detection and ranging) sends out hundreds of thousands of pulses of laser light every second and measures the time taken for reflected energy to return from objects up to hundreds of metres away.  When mounted on a tripod on a city street, lidar builds up a millimetre accurate 3D picture of everything it “sees”, including trees. The team are using lidar methods, which they pioneered to measure some of the world’s largest trees, and applying them to trees in the university’s local London Borough of Camden.   Point cloud of Russell Square by kungphil on Sketchfab  The UCL team used publicly available airborne lidar data collected by the UK Environment Agency, in conjunction with their ground measurements, to estimate biomass of all the 85,000 trees across Camden. These lidar measurements help to quantify the differences between urban and non-urban trees, allowing scientists to come up with a formula predicting the difference in size-to-mass ratio, and thus measuring the mass of urban trees more accurately.  The findings show that Camden has a median carbon density of around 50 tonnes of carbon per hectare (t/ha), rising to 380 t/ha in spots such as Hampstead Heath and Highgate Cemetery – that’s equivalent to values seen in temperate and tropical rainforests. Camden also has a high carbon density, compared to other cities in Europe and elsewhere. For example, Barcelona and Berlin have mean carbon densities of 7.3 and 11.2 t/ha respectively; major cities in the US have values of 7.7 t/ha and in China the equivalent figure is 21.3 t/ha. Trees matter, to all of us. Recent protests in Sheffield, Cardiff, London and elsewhere, over policies of tree management and removal show how strongly people feel about the trees in their neighbourhood. Finding ways to value trees more effectively is critical to building more sustainable and liveable cities. Measuring trees in new ways also helps us to see them from a new perspective. Some of these trees have incredible stories to tell. Just one example is an ash, tucked away in the grounds of St. Pancras Old Church, one of London’s (and indeed Britain’s) oldest Christian churches.  The tree has an extraordinary arrangement of gravestones around its roots, placed there when the railway was built from St Pancras in the mid-19th century. The job of rehousing the headstones was apparently given to a young Thomas Hardy, working as a railway clerk before going on to achieve literary fame. The UCL team’s 3D lidar data are helping monitor the state of this “Hardy Ash” tree in its dotage. This is just one of the ways new science is helping tell the stories of old trees.   Hardy Tree (Camden, UK) and gravestones by kungphil on Sketchfab "
"Sulphur will be cut drastically from global shipping transport fuels in 2020, in a move that should reduce some forms of air pollution, and may help towards tackling the climate emergency – but which could also lead to a rise in the price of flights. From 1 January 2020, ships will only be allowed to use fuel oil with a very low sulphur content, under rules brought in by the International Maritime Organisation. This cut in sulphur content has been more than a decade in the planning, and almost all shipping around the world is expected to comply, or face penalties.  “Member states, the shipping industry and fuel oil suppliers have been working for the past three years to prepare for this major change – I am confident that the benefits will soon be felt and that implementation will be smooth,” said Kitack Lim, the secretary general of the IMO. “This [is a] hugely important change which will have significant positive benefits for human health and the environment.” The new regulations are aimed at cleaning up sulphur emissions, which can cause acid rain and other forms of air pollution, rather than tackling the climate emergency. However, the dirty forms of fuel that contain high levels of sulphur are usually higher carbon too, and the costs of cleaning up sulphur may spur shipping companies to become more efficient in their fuel use, which would cut greenhouse gas emissions directly. Moving to cleaner fuels could add substantially to costs, from an estimated $400 (£303) a tonne for fuel oil today to as much as $600 a tonne, according to the International Chamber of Shipping. Higher shipping costs may be absorbed throughout the manufacturing and transport supply chains. The cost impact may also spread beyond shipping, according to the energy analyst firm Wood Mackenzie. “Knock-on effects from the cap on sulphur emissions in marine bunker fuel could even wind up giving you a more expensive plane ticket in 2020,” the company said. The IMO estimates that the new limit – of 0.5% sulphur content compared with the previous limit of 3.5%, enforced under the international convention for the prevention of pollution from ships – will cut sulphur oxide emissions from ships by 77%, an annual reduction of about 8.5m tonnes. Fuel oil for shipping has long been one of the dirtiest forms of fossil fuel, made up of the sort of low-value and cheap crude oil that is unsuitable or expensive to refine into high-grade products such as petrol for cars, or kerosene for planes. Ship engines have been designed to cope with such low-grade fuel, and the emissions they belch out as a result mostly happen far from land, making the accompanying pollution less visible and, for many decades, largely ignored by governments. But the damaging effects of the pollution have grown as globalisation has led to a massive increase in shipping transport. Shipping consumed about 3.8m tonnes of fuel oil a day in 2017, according to Wood Mackenzie, equivalent to half of global fuel oil demand. Carbon from shipping makes up about 3% of global total carbon emissions, but is expected to rise to 17% by mid-century. Fuel oil with a high sulphur content produces sulphur oxides, which can cause acid rain and particulate pollution. Alternatives to low-grade, high-sulphur fuel oil are increasingly available, albeit at a higher price. Liquefied natural gas is still a fossil fuel, but much cleaner, and infrastructure allowing its use is becoming more widespread. Biofuels are also being explored as an alternative – one enterprising cruise company is using fish guts for its fuel – and there are high hopes for harnessing hydrogen fuels in the form of ammonia for ship engines. Vessels can also be fitted with “scrubbers” to remove sulphur, though some of this is then released into the sea as effluent. Ports have also become increasingly concerned at the pollutants from cargo and passenger ships, and some operate zones where sulphur content is even more drastically reduced. Shipping is subject to complex international regulation, overseen by the IMO, the London-based UN body. However, for historic reasons, it has been excluded from calculations of international greenhouse gas emissions, and thus exempted from governments’ obligations under UN climate agreements, including the landmark Paris accord of 2015. That has meant shipping companies have felt less pressure to cut carbon, and progress on all forms of shipping pollution has been slow and often tortuous. The new sulphur regulations were first enacted by the IMO in 2008, after years of debate, but had to be re-confirmed in 2016 before finally coming into force on Wednesday. Climate change campaigners want to see much faster adoption of regulations to cut greenhouse gas emissions from shipping. The next major public meeting will be an IMO conference in London in late March and early April, where countries will come under pressure to lay out a clear plan on cutting carbon from the sector, ahead of a major UN climate conference in Glasgow in November. The IMO has a longterm aim to halve carbon from shipping by 2050, but few concrete plans to achieve it. An increasing number of countries, including the UK, are aiming for net zero carbon emissions by 2050, in line with scientific warnings on the urgency of the climate emergency. The ICS has also proposed a $2 a tonne levy on shipping fuels that would pay into a fund for research and development on zero-carbon forms of shipping, which will be explained to member states at the spring IMO conference, but could take years to come into force."
"There is something special and awe-inspiring about watching new land form. This is what is now happening in Hawai’i as its Kīlauea volcano erupts. Lava is reaching the ocean and building land while producing spectacular plumes of steam. These eruptions are hugely important for the creation of new land. But they are also dangerous. Where the lava meets the ocean, corrosive acid mist is produced and glass particles are shattered and flung into the air. Volcanic explosions can also hurl lava blocks hundreds of metres and produce waves of scalding hot water.  At Kīlauea, lava is erupting from a line of vents on the volcano’s flanks, and is moving downslope to the edge of the island, where it enters the ocean. This is a process that has been witnessed many times at Hawai’i and other volcanic islands. And it is through many thousands of such eruptions that volcanic islands like Hawai’i form. The new lava being added to Hawai’i by this latest Kīlauea eruption replaces older land that is being lost by erosion, and so prolongs the island’s lifespan. In contrast, older islands to the north-west have no active volcanoes, so they are being eroded by the ocean and will eventually disappear beneath the waves. The opposite is happening to the south-east of Hawai’i, where an underwater volcano (Lōʻihi Seamount) is building the foundations of what will eventually become the next volcanic island in this area. The lava erupting from the current Kīlauea vents has a temperature of roughly 1150 degrees °C, and has a journey of between 4.5km and 5.5km to reach the ocean. As this lava moves swiftly in channels, it loses little heat and so it can enter the ocean at a temperature of over 1000 degrees°C. We are witnessing one of the most spectacular sights in nature - billowing white plumes of steam (technically water droplets) as hot lava boils seawater. Although these billowing steam clouds appear harmless, they are dangerous because they contain small glass shards (fragmented lava) and acid mist (from seawater). This acid mist known as “laze” (lava haze) can be hot and corrosive. If anyone goes to near it, they can experience breathing difficulties and irritation of their eyes and skin. Apart from the laze, the entry of lava into the ocean is usually a gentle process, and when steam is free to expand and move away, there are no violent steam-driven explosions. But a hidden danger lurks beneath the ocean. The lava entering the sea breaks up into blobs (known as pillows), angular blocks, and smaller fragments of glass that form a steep slope beneath the water. This is called a lava delta.  A newly formed lava delta is an unstable beast, and it can collapse without warning. This can trap water within the hot rock, leading to violent steam-driven explosions that can hurl metre-sized blocks up to 250 metres. Explosions occur because when the water turns to steam it suddenly expands to around 1,700 times its original volume. Waves of scalding water can also injure people who are too close. People have died and been seriously injured during lava delta collapses  So, the ocean entry points where lava and seawater meet are doubly dangerous, and anyone in the area should pay careful attention to official advice on staying away from them. Once lava deltas have cooled and become stable they represent new land. Studies have revealed that lava deltas have distinctive features, and this has enabled volcanologists to recognise lava deltas in older rocks. Remarkable examples of lava deltas have been discovered near the top of extinct volcanoes (called tuyas) in Iceland and Antarctica. These deltas can only form in water and the only plausible source of this water in this case is melted ice. This means that these volcanoes had melted water-filled holes up to 1.5km deep in ice sheets, which is an astonishing feat. In fact, these lava deltas are the only remaining evidence of long-vanished ice sheets.  It is a privilege to see these incredible scenes of lava meeting the ocean. The ongoing eruptions form part of the natural process that enables beautiful volcano islands like Hawai'i to exist. But the creation of new land here can also bring danger to those who get too close, whether it be collapsing lava deltas or acid mist."
"Smart cities are often discussed as being the key to future urban living. The increase in capacity for more complex information can help solve human and environmental problems by saving energy and regulating traffic flow. A study has now highlighted the potential of adapting the concept of “smart” for national parks. Historically, outdoor recreation gained its popularity because of its juxtaposition to urbanisation. Motivations included adventure, simplicity and immersion in “wilderness” – away from human progress. In many cases this is still true. We are often told that greater exposure to green space and natural environments benefits health and well-being.  But how can the so-called “smart” tech improve our relaxing countryside experience? The challenge lies in integrating technology into outdoor recreation while retaining these crucial elements of the experience. Here are some simple smart options for the future ramblers. The Lake District example suggests that sensors on bins can alert the national park authorities when they are full, which reduces the problem of litter and helps conserve the landscape. Research has shown that these kinds of messages work.   It is also suggested that “smart” car parks will transmit information to motorists when car parks are full. This can reduce carbon emissions by reducing trips to multiple car parks. However, encouraging more travel by public transport and non-motorised modes of transport reduces carbon emissions more effectively. This alternative should be given priority. Planners and managers of national parks have long seen the need to reduce visitor car use. Aside from decreasing carbon emissions, visual pollution from large numbers of cars in natural areas is a long-standing problem. It takes away from the “natural” and “simple” aesthetics which are so important in attracting visitors. There is considerable need to encourage car sharing, especially because the infrastructure in rural areas is less resilient to large numbers of cars. Academics are increasingly pointing to the power of new data sharing and smart capability to solve this through measures such as car-sharing apps and better planning for integrated travel.  People who travel to, from and within national parks can do so sustainably with greater confidence if they have reliable information on public transport – as well as walking and cycling options – at their fingertips. Research on walking tourism in natural settings highlights the growing use of mobile technology as a navigational aid. The internet is increasingly used both to showcase and research walks in national parks. But practitioners urge caution for more adventurous forms of recreation.  Interviews with national park staff revealed that in particular, mountain rescue services can be stretched when hillwalkers rely too much on technology. Navigating solely with a mobile phone or GPS cannot substitute map-reading skills when faced with difficulties. Walking tourists differ in their preference for heavily managed walking routes. Some look for relative simplicity of “wild” surroundings, isolation and solitude. Others prefer abundant directional signs, information and flat, well managed paths. These differences point to an important dichotomy as technology permeates more of the previously “untouched” areas of the world. Technology is redefining how we engage with the natural environment. Sport England’s research on UK outdoor activity acknowledges a need for connectivity even in the most remote natural areas and particularly for younger participants. Rapidly improving mobile technology and information capacity epitomise the fast society many live in.    There are clear benefits to integrating smart technology into rural and natural areas. Tourist in particular are a key focal point because of the capability for improving sustainability in national parks. But the wider implications surrounding this development still need to be considered. National parks should continue to cater for all preferences and preserve some “smart-free” elements, enhancing the experience of those seeking adventure and wilderness. People should also rethink their relationship with nature. If smart technology can help the environment, preserve biodiversity and protect sensitive areas, then it should be considered as an antidote to past human negative effects."
"The energy powering Volgograd’s World Cup stadium floodlights is generated by the nearby Volgograd HydroElectric Station. This is the largest hydro power plant in Europe, and a dam which has played a pivotal role in driving sturgeon – the source of the iconic Russian delicacy, black caviar – to the brink of extinction. The 725m long and 44m high concrete giant sits about 20km outside the city centre and dissects Europe’s longest and most powerful river, the Volga. Construction began in the 1950s, as part of post-war industrialisation initiatives known as the “Great Construction Projects of Communism”. This in a city which during World War II – when it was known as Stalingrad – was the site of one of the bloodiest battles in history. The dam was completed in 1961 and today produces around 12 billion KW-hours of energy a year. The station was groundbreaking in both scale and energetic output. For a few years, it may have been the single largest power plant in the world. But despite the benefits to the climate of “clean” hydro-powered energy, the Volgograd station has been particularly damaging for the sturgeon species that attempt to migrate from the Caspian Sea to reproduce in the upper reaches of the Volga. Sturgeon, affectionately referred to as “Tsar Fish” are perhaps more critically endangered than any other group of species on the planet. There are 27 species in all, of which four are found in the Volga: Russian sturgeon, sterlet, stellate, and the beluga which is famous for producing the world’s finest caviar. These fish are often described as “living fossils”. They’ve been around since dinosaurs walked the earth 150m years ago, and individual fish can live for more than a century. Sturgeon have attained a cultural and historical significance in Russia and are a source of national pride. But socioeconomic change in Russia has been disastrous for these fish. Their rivers have been polluted, fragmented and dammed and this – along with overfishing and poaching for caviar – has caused populations in the Volga to plummet by 90% since 1970.  A slow reproductive cycle means numbers cannot recover quickly. Females do not carry eggs annually, they take many years to reach sexual maturity and, of the 250,000 - 400,000 eggs they can release at one time, only two or three fish will survive. As the last of eight hydroelectric works in the Volga-Kama cascade of dams, the Volgograd Hydroelectric station is the first barrier sturgeon migrating upstream from the Caspian Sea will encounter. In theory sturgeon can pass the dam thanks to a hydraulic fish-lift in the original design. However, it is not clear whether the lift is still operational and, even if it is, its benefits have been counteracted by further dams built upstream. Even if fish do manage to cross the dam, the return journey can prove fatal, as it often requires passing through turbines that can weigh as much as a 747 aeroplane.  The Volgograd Hydroelectric station not only blocks sturgeon migration, but alters the natural flow and temperature of the river. Sturgeon are very sensitive and rely upon signals such as flow speed and temperature to determine when and where to reproduce. Therefore, the dam is said to have directly reduced the spawning grounds of sturgeon from 3600 hectares to only 430 hectares. For beluga sturgeon in particular, 90% of their natural spawning grounds have disappeared as a result of the Volgograd dam. It is undeniable that the Volgograd station has played a part in the demise of the Russian caviar industry. Due to rapidly declining wild sturgeon populations, Russia banned commercial sturgeon fishing and black caviar exports in 2002. Now, Russia allows just 9 tonnes of the delicacy to be sold on the domestic market annually, produced by a few government-regulated fish farms. These farms cannot come close to producing enough caviar to meet Russian, let alone worldwide, demand. As a result an illegal trade meets the shortfall, with reports suggesting that 250 tonnes of illegal caviar are produced each year. Unsurprisingly then, almost all migrating spawners are poached below the Volgograd dam, and a particular hotspot is Russia’s so-called “Caviar Capital”, Astrakhan, around 400km downstream from Volgograd. There, illegal poaching of sturgeon and trade in caviar is said to be rampant – and beluga caviar fetches up to $10,000/kg. This has devastating ecological impacts –  when sturgeon are removed at this point in the river the fish have not had the chance to reproduce. The situation looks bleak. Despite Russia releasing 50m or more sturgeon raised in hatcheries, there is sparse evidence that restocking is successful. In fact, despite such releases there has been an overall decline over the past decade. And it seems counter-intuitive to release millions of juvenile sturgeon when the Volgograd dam still prevents their migration and spawning – and given that downstream poaching is rife. Greater enforcement against poaching would be a good start, along with assertive efforts to help fish move along their natural rivers (something similar has helped shortnose sturgeon in the US). So, for football fans visiting Volgograd for the World Cup the best way to help sturgeon is to avoid the lure of purchasing any black caviar as souvenirs. But, if you are that way inclined, make sure to stick to customs regulations and try your utmost to ensure the caviar is from reputable farmed sources."
"
Share this...FacebookTwitterVolcano Agung in Bali is showing worrisome signs of a major eruption, writes German climate blogger Schneefan here. The highest level of activity with multiple tremor episodes were just recorded. You can monitor Agung via live cam and live seismogram.
The 3000-mter tall Agung has been at the highest warning level 4 since September 21.
Schneefan writes that the lava rise has started and that “an eruption can be expected at any time“.
So far some 140,000 people have been evacuated from the area of hazard, which extends up to 12 km from the volcano. Schneefan writes:
Yesterday ground activity by far exceeded the previous high level. Quakes have become more frequent and stronger, which indicates a stronger magma flow (see green in the histogram). Since October 13 there has been for the first time a “nonharmonic trembling (tremor), which can be seen in red at the top of the last two bars of the histogram.”



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The colors of the columns in the bar chart from bottom to top stand for perceptible earthquakes (blue) low eartnhquakes (green) surface quakes (orange . Just recently red appeared, signifying non harmonic tremors.  The seismogram below shows what are at times longer period quakes: meaning magma is violently flowing in the volcano. Source: https://magma.vsi.esdm.go.id/.
Since yesterday the seismogram for AGUNG has been showing powerful rumbling (red).

The seismogram of AGUNG shows powerful tremors (level RED). The seismogram is updated every 3 minutes: Source: Seismogramm
Because Agung is located near the equator, a major eruption with ash flying up into the stratosphere would have short-term climatic impacts that could last a few years.
Agung last erupted in 1963 with an explosivity index of VEI 5, sending a plume of ash some 25 km into the atmosphere before leading to a cooling of 0.5°C. The eruption of Pinatubo in the Philippines in 1991 led to a global cooling of 0.5°C.
 
 
Share this...FacebookTwitter "
"“The Darfur conflict began as an ecological crisis”, wrote the then-UN secretary general Ban Ki-Moon back in 2007, about an ongoing war which arose, he said, “at least in part from climate change”. Since then the idea that climate change has caused and will cause human conflict and mass migrations has become more and more accepted – just look at the claimed effects of droughts in Syria and Ethiopia. The media has even started using terms such as “climate refugees” and “environmental migrants” to describe people fleeing their homes from these climate-driven conflicts. But it isn’t clear whether there is much evidence for this link between climate change and conflict –  there certainly seems to be no consensus within the academic literature. In our recent paper, my student Erin Owain and I decided to test the climate-conflict hypothesis, using East Africa as our focus. The region is already very hot and very poor, making it especially vulnerable to climate change (in fact neighbouring Chad is by some measures the single most vulnerable country in the world).  As the planet warms, East Africa’s seasonal rains are expected to become much more unpredictable. This is a particular problem as recent economic development has been concentrated in agriculture, a highly climate-sensitive sector that accounts for more than half of the entire economy in countries like Ethiopia or Sudan. One study led by the European Commission found that declining rainfall over the past century may have reduced GDP across Africa by 15-40% compared with the rest of the developing world.  East Africa also has a long history of conflict and human displacement, which persists in some countries to this day, such as the civil wars in Sudan and Somalia. The UN Refugee Agency reports there were more than 20m displaced people in Africa in 2016 – a third of the world’s total. The World Bank predicts this could rise up to 86m by 2050 due to climate change.  To test the climate-conflict hypothesis, Erin and I therefore focused on the ten main countries in East Africa. We used a new database that records major episodes of political violence and number of total displaced people for the past 50 years for each of the ten countries. We then statistically compared these records both at a country and a regional level with the appropriate climatic, economic and political indicators. We found that climate variations such as regional drought and global temperature did not significantly impact the level of regional conflict or the number of total displaced people. The major driving forces on conflict were rapid population growth, reduced or negative economic growth and instability of political regimes. Numbers of total displaced people were linked to rapid population growth and low or stagnating economic growth.  The evidence from East Africa is that no single factor can fully explain conflict and the displacement of people. Instead, conflict seems to be linked primarily to long-term population growth, short-term economic recessions and extreme political instability. Halvard Buhaug, a professor at the Peace Research Institute Oslo, looked at the same questions in 2015 and his study reached much the same conclusion: sociopolitical factors were more important than climate change. Things were different for “refugees”, however – those displaced people who were forced to cross borders between countries. Refugee numbers were related to the usual demographic and socio-economic factors. But in contrast to total displaced people and occurrence of conflict, variations in refugee numbers were found to be related significantly to the incidence of severe regional droughts. And these droughts can in turn be linked to a long-term drying trend ascribed to anthropogenic climate change. However, it is important to consider the counterfactual: had there been slower population growth, stronger economies and more stable political regimes, would these droughts still have led to more refugees? That’s beyond the scope of our study, which may not be a definitive test of the links between climate change and conflict. But the occurrence of peaks in both conflict and displaced people in the 1980s and 1990s across East Africa suggest that decolonisation and the end of the Cold War could have been key issues.  Nonetheless, while conflict has decreased across the region since the end of the Cold War, the number of displaced people remains high. We argue that with good stable governance there is no reason why climate change should lead to greater conflict or displacement of people, despite the World Bank’s dire predictions. Water provides one reason to be optimistic. The UN reports that, over the past 50 years, there have been 150 international water resource treaties signed compared to 37 disputes that involved violence.  What our study suggests is the failure of political systems is the primary cause of conflict and displacement of large numbers of people. We also demonstrate that within socially and geopolitically fragile systems, climate change may potentially exacerbate the situation particularly with regards to enforced migration."
"
Share this...FacebookTwitterThe online German business daily Handelsblatt here writes that European wind energy company Siemens Gamesa will eliminate 6000 jobs.
That means the German-Spanish company will shed more than a fifth of it 26,000 workers. This is the latest bad news slamming the green energy industry in Germany and Europe. Over the years Germany has seen almost every major solar panel and equipment manufacturer become insolvent. Spain too has been hit hard by renewable energy insolvencies.
Once ballyhooed as the sector for the future, German solar and and wind energy industry has taken huge hits. The country’s last remaining major solar manufacturer, Bonn-based Solarworld, earlier this year announced it would file for bankruptcy. Solarworld’s demise was the last of a spectacular series of solar manufacturer bankruptcies that swept across Germany over the past years, with names like Solon, Solar Millenium and Q-Cells going under.
Now the bloodbath is expanding to the wind industry, a branch of green energy that looked far more feasible in Germany than solar energy did.
The announcement by Siemens-Gamesa coincides with the COP 23 Bonn climate conference now taking place, which is calling for more wind and sun energy at a time the industry is collapsing at full speed in Germany.
According to Siemens-Gamesa Board Chairman Markus Tacke: “Our business result is still not at the level where we would like to see it.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Last year Spanish Gamesa and German Siemens combined their wind power operations to form one of the world’s largest producers of wind turbines.
Handelsblatt writes the Siemens daughter company was reacting to “changing market conditions” and that the move will impact 6 countries.
The company also expects to see its revenue for the coming fiscal year fall to 9 billion euros from almost 11 billion.
The Handelsblatt also reports that “no improvement is foreseen in the new fiscal year“.
Other links in English:
– www.expatica.com/de/news/country-news/Germany-layoffs
– http://uk.businessinsider.com/r-siemens-gamesa-to-6000-jobs=T
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterDo Supernova Events Cause
Extreme Climate Changes?
“Global warming will not be reduced by reducing man made CO2 emissions”

In recent years, mass die-offs of large animals – like the sudden deaths of 211,000 endangered antelopes within a matter of weeks – have been described as “mysterious” and remain largely unexplained.
Determining the cause of the retreat to ice ages and the abrupt warmings that spawned the interglacial periods has remained controversial for many decades.
William Sokeland, a heat transfer expert and thermal engineer from the University of Florida, has published a paper in the Journal of Earth Science and Engineering that proposes rapid ice melt events and ice age terminations, extreme weather events leading to mass die-offs, and even modern global warming can be traced to (or at least correlate well with) supernova impact events.
The perspectives and conclusions of researchers who claim to have found strong correlations that could explain such wide-ranging geological phenomena as the causes of glacials/interglacials, modern temperatures, and mysterious large animal die-offs should at least be considered…while maintaining a healthy level of skepticism, of course.
Discovery – if that is potentially what is occurring here – is worth a look.

Sokeland, 2017
Scientists generally state that debris from supernova does not impact our planet.  They have no concept that incoming particles from exploding stars are focused by our sun’s gravity and the magnetic fields of the sun and earth.
[M]any harmful effects are possible in the Supernova and Nova Impact Theory, SNIT, including extreme changes of the climate.
Supernova Impacts and Solar Activity, Global Warming Correlation
The scattering of solar energy due to the small particles of supernova debris is also reflected in TSI data as shown in Fig. 3. The timing of impact for supernova debris streams allows the identification of the times and duration time periods for supernova debris streams impacting our planet. Fig. 3 indicates the duration of a single supernova debris stream flowing past our planet is at least 50 years and at times more than 100 years.

Fig. 3 shows an excellent correspondence between sunspot minimums, irradiance depressions, and supernova impact times. The six smaller dips in TSI generated by nova WZ Sagittae in the red portion of the TSI curve of Fig. 3 beginning with the Dalton minimum indicate we have been impacted by six different debris streams from the nova. The last one was in the 1965 to 1970 time region and it is the debris stream of Nova WZ Sagittae that started our current global warming episode near 1966.
Supernova Impacts and Ice Ages, Ice Sheet Melts, and Warm Period Correlations
Incoming supernova debris streams cause warming and melting ice caps that produce increased sea levels.  The increase in sea levels that correlates with supernova impact times is shown in Table 5. 
Termination of the last ice age results due to melting of numerous supernova impacts that correlate time of impact by changing sea level and geothermal energy released for 2,800 years from the exit crater of Dr. J. Kennet’s nano-diamond meteor theory and part of the process involves Dr. O’Keefe’s tektite theory. Correlation of Dr. Frezzotti’s ice melt Antarctica data with supernova impact times over the past 800 years establishes the Global Warming model in conjunction with the November 2016 Antarctic sea ice melt.
Supernova 393 debris impacted earth near 857 AD and started the Medieval Warming Period. When the warm part of the supernova oscillation or cycle stopped and the cooling occurred, the Little Ice Age began near 1250 AD. Supernova 393 also caused the decline of the Mayan Empire near 900 AD. Supernova 393 is proposed to have caused a gamma ray attack upon earth 1,200 years ago.
Two supernovas, G299 and G296.7-0.9, impacted the earth to produce first the Roman warming period shown in Fig. 4 with the normal cooling and then a third unknown supernova created some warming with a lot of cooling dropping temperature to a minimum near 1,100 years ago (900 AD). This cold period produced the Dark Ages. Then SN 393 occurred causing more warming than cooling, but the end result was the Little Ice Age. The Dark Ages and the Little Ice Age were very disastrous periods of time for our planet’s human populations. It should be concluded that the increase in CO2 caused by supernovas 1006 and 1054 that is currently being observed is a boon to mankind and will protect us from the coming cold phase that will be caused by these currently impacting supernovas. 
Consider the Minoan Warming of Fig. 4. The incoming carbon from supernova G29.6+0.1 causes the warm up as shown by the increased Greenland ice core temperatures.
Supernova Impacts and Timings of Megafauna Extinctions and Civilization Collapses
Noted megafauna extinctions in the past 50,000 years are correlated with the times when the debris of supernova explosions impact earth. The time of extinction should be near the time of impact of a supernova debris stream. The time of impact is derived from the time the light of the supernova explosion was seen on earth by adding a correction for the fact that the debris from the explosion moves slower than the speed of light and is shown in the second equation. The severity of the extinction will depend on the distance of the supernova from our planet, the type supernova that indicates the power of the explosion and the surroundings of the supernova when it explodes.  In general, most major disturbances of earth’s biosphere can be attributed to the explosion of supernovas.
Due to the scattering of light for small particles, the sunspots will tend to disappear when a hollow sphere of small particles enters our solar system between the sun and the earth. Other signs of the presence of the small particles are the increase of animal die offs for birds, bees, and fish and a decrease in TSI (total solar irradiance).
Recent outstanding examples of animal and human die offs due to the incoming debris were the Saiga antelope in Asia in May of 2014 and people dying in  India in May of 2015-2016.  These die offs were caused by SN 1006 and would have been called megafauna extinctions if the populations were restricted to small island land areas.  The deaths of the destructive hollow spheres for supernovas 1054 and 1006 will be minimal in the beginning but will increase in intensity as the years of higher particle mass and densities are approached.  Since these supernovas are over 7,000 light-years away from our planet, the effects should not be as severe as the extinctions listed in Table 2 that were due to supernova remnants that were closer to our planet.
Supernova G32.0-4.9 impact time of 4,530 ya corresponds to the fall of Egypt’s fourth dynasty in 2494 BC. It is reported that Ancient Europeans vanished 4,500 years ago. Could supernova debris actually destroy the structure of an empire and change DNA in Europe? An impact time of 4,210 years ago matches the 4.2 Kiloyear Event.
Supernova W50 with an impact time of 17,600 ya and a declination +4 appears to have caused rapid melting of the Patagonian ice sheet 17,500 years ago and corresponds to the last glacial maximum of 18,000 years ago.
Supernova G31.9+0.0 impact time of 8,092 ya produces another correlated woolly mammoth extinction event at Lake Hill on St Paul Island in the Bering Sea 7,600 years ago. The climate change produced by this supernova caused these mammoth to die due to lack of fresh water or drought.
Supernova W51C provides the impact time of 8,130 years ago and this date coincides with the end of the 8.2 Kiloyear Event.
The W50 meteor at 12,800 ya matches the beginning of deglaciation in Antarctica 12,500 years ago. Supernova Vela has a range of impact times shown in Table 1 and Fig. 7 suggests the change of temperature date of 11,700 years ago should be used. Vela’s thermal impact in the northern hemisphere was large because it is the second closest supernova to our planet.
Supernova G82.2+5.3 in Table 3 with an impact time of 5,903 ya produced the 5.9 Kiloyear Event and it is so close in time to the Piora Oscillation that the two different events due to different supernovas are often considered the same event 
The SNIT [Supernova and Nova Impact Theory] Model vs. Climate Models
Any model that claims to know the energy source for global warming must predict the past effects like Antarctic melts in Fig. 3a. Then the model can be successfully used to predict global warming effects in the future. If the proposed model cannot predict past global warming events from previously recorded independent data, the model is useless.
The SNIT model shows unusual and distinct conditions for beginning and ending ice ages. To start an ice age, a close supernova explosion like SN Monogem Ring must produce an extreme amount of iron on earth’s surface. To end an ice age a meteor from a supernova explosion must penetrate earth’s mantle and release geothermal energy over a long period of time to melt the ice.
Applying Occam’s razor, supernova debris impact is the simplest method that explains all these extinction and biosphere disturbance events because the only assumption is all debris streams travel at the same velocity from the remnant to our planet.
What Can We Do?
The debris streams of supernovas 1006 and 1054 have already began to destroy life on earth. When President Obama received a rough draft of this work, he issued an executive order to NASA stating, “Space weather has the potential to simultaneously affect and disrupt health and safety across entire continents”.
Since supernovas 1054 and 1006 are currently incoming, the planet’s average temperatures should continue to increase, global warming. Global warming will not be reduced by reducing man made CO2 emissions and in reality the only defense is to move to a cooler hemisphere, harvest CO2 from the atmosphere, or stop the incoming particles.  
Share this...FacebookTwitter "
"Plastic pollution in the oceans has got a lot of attention lately, seemingly triggered by the BBC’s Blue Planet II and its haunting image of a pilot whale grieving her dead calf. But water pollution isn’t just a problem in the sea – local waters are suffering too, often from pollutants found in common household products.   Even in very low amounts, some medications, hygiene products and pesticides may cause aquatic organisms to change their behaviour or find their homes are no longer habitable. The issue has been on the authorities’ radar for some time, not least the European Union which introduced the first watchlist of emerging pollutants in 2013. But you can help too, with a few simple changes to your everyday habits. Everyday tasks such as washing your hands or brushing your teeth could mean you are unwittingly polluting a river. Hand gel or toothpaste may contain anti-bacterial agents such as triclosan, which mimics the hormone estrogen in animals and can inhibit 
their reproductive systems and ability to swim.  Once in the water, triclosan sticks to the soil on the riverbed where it will be consumed by all the tiny creatures that call the river home. It then accumulates as it is passed on through the food chain, meaning larger predators are worst affected.  Due to these concerns, some companies have began removing triclosan from their products, but in the meantime you can get ahead of the game and chose not to use products that contain it. Metaldehyde is the active ingredient found in many slug pellets. It is of course toxic – that’s why it’s used to kill slugs. But the pellets are washed into drains and ditches, and from there they wind their way into river systems, affecting animals much larger than slugs or snails. And just like triclosan, metaldehyde is passed on up the food chain to predators like hedgehogs or birds.  Metaldehyde is a particular problem when it gets into waterways. In your garden it usually breaks down within a few days. But when it enters the water system, the chemical is much more stable which slows down the degradation, allowing it to hang around in the environment, where it and can then be consumed by aquatic life.  Alternatives to metaldehyde include copper strips, said to deter slugs, or parasitic nematode worms which naturally kill slugs and snails. Or you could simply encourage predators such as hedgehogs and frogs into your garden.  Microplastics are small fragments of plastics less than 5mm in size. There has been a lot of talk about the problems caused when these fragments are ingested by fish or other aquatic animals, but less attention has been paid to the plastics leaching toxic materials as they break down into their original components. These toxic products have been linked to neurological, fertility and immune health problems. Large items such as plastic packaging or tyres are often the source, as they break down into smaller and smaller fragments over time until they are tiny bits of microplastics floating through the water.  But synthetic clothing is another significant source. Every time you wash synthetic fibres, small parts of the material will fragment away and wash into the water system. So the next time you go to wash the fleece that you only wore for ten minutes a couple of weekends ago, think: is this really necessary? Pharmaceutical products are another cause for concern as even in very small quantities they may be considered toxic. Medicines and drugs such as painkillers, antidepressants and contraceptives all end up in waterways as they pass through the human body unaffected and are flushed down the toilet. These drugs can affect the natural reproductive cycle, behaviour and growth of many fish species.  We’re not suggesting people should stop taking their medication. But you can help by making sure old pills are disposed of properly. Many people flush them down the loo, or chuck them in the bin when really they should be returned to a pharmacy where they can be properly disposed of.  It’s too easy to associate river pollution with large factories and heavy industry but we too play a part by just going about our everyday lives. Such simple small changes could make a real difference to the water quality of your local river and it’s so easy to do."
"
Share this...FacebookTwitterOver the recent days we’ve been hearing about record snowfall in Montana, record low temperatures in Minnesota and Ontario, New York City “blowing away” a 103-year old record, vicious cold gripping  Lebanon, PA. Moreover, Arctic sea ice and Greenland ice have surprised climatologists with a comeback over the past year.
UPDATE: And now the UK braces for 3 weeks of unusual cold.
What is more, the NOAA has just officially announced the return of the La Niña, after earlier this year an El Niño had been forecast instead.
“Gangbusters cold” in the works
And lately we’ve been hearing a number of meteorologists warning of a harsh coming winter for both Europe and USA.
At his Friday Daily Update, veteran meteorologist Joe Bastardi warns of a “fierce cold” being on the table for this winter for the United States. Bastardi says he does not believe the US climate models at all, and instead could even be as harsh of the notoriously cold 2013 winter.

Bastardi believes the month of December will in fact turn out to be the opposite of what was projected by the US weather models, which foresaw a warm December. See his latest Saturday Summary at Weatherbell.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




In his Friday Daily Update Bastardi thinks the cold will be “gangbusters” and that there is a real chance for a “December to remember”. All in all, the Pennsylvania State University meteorologist sees an excellent ski season in the works for the USA.
-40°C for Christmas In Central Europe?
Over in Europe, there have been a growing number of warnings of a “winter of the century“. For example the German nachrichten.de here reports Christmas temperatures of -40°C!
Naturally such forecasts need to be viewed with great skepticism as they are highly speculative. Long-term prediction methods may indicate which way a winter is tending, but I wouldn’t believe any forecast 6 to 8 weeks out.
Nachrichten.de cites information from the Augsburg Meteorological Institute and the Hamburg Weather Warning station. The Federal Office for Weather Observations advises citizens “to prepare for an ice cold Christmas.”“Snowmageddon”?
The express.co.uk warns of a “SNOWMAGEDDON”, thanks to a La Niña bringing a “Big Freeze”. The express.so.uk  writes that “winter weather 2017 is set to be the harshest for years“.
For the winter of 2017/2018 the PDO will bring a winter of deadly blizzards and killer freezes as a perfect storm of catastrophic weather systems gather. Analytical meteorologist Tyler Sotock suggested America was facing Snowmaggedon. And spokesman for another YouTube weather channel Hurricane and Winter Tracker warned of chaos.”
While wild speculation swirls, one thing seems certain: just as Joe Bastardi shows at his Saturday Summary, the US forecasts of a warm November/December period across Canada and the US East are looking more flawed than ever.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterQuartz.com here presents an interesting chart which tells us the green energy revolution of the past 30 years has resulted in practically nothing. It’s been a flop. Fossil fuels remain as wildly popular as ever.

Global fossil fuel use as a share of total energy has risen since James Hansen’s 1988 testimony. Chart: Quartz.com.
In the 1970s the big worry was that fossil fuels would soon run out, and so we should “use them wisely”. But in the 1980s the risk changed to one of an overheating planet, and so we should not use them at all.
Higher than 1988, when James Hansen testified
We can all recall a sweating James Hansen’s 1988 stage-crafted testimony before Congress, warning that increasing atmospheric CO2 concentrations would lead to spiraling global warming. And unless action was taken urgently, the ice caps would soon melt and the earth would sizzle.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Countries as a result mobilized 100s of billions of dollars to eliminate the use of these “dangerous, climate-killing” fossil fuels.
Today for all that money you’d think that tremendous progress in reducing fossil fuels would be the result. You couldn’t be more wrong.
The sad truth is that after hundreds of billions spent, and trillions committed, fossil fuels’ share of total energy consumption globally has in fact risen since Hansen’s doomsday testimony, see the figure above!
Bringing the dead back to life
What may be a surprise to many is that whenever we burn fossil fuels, which originate primarily from ancient plants that died and were naturally sequestered in the earth as “fossils” eons ago, we are in fact taking this once live carbon and recirculating it back into the current, living ecosystem. The result: More carbon-based life is getting produced today. The planet is greening. Now the earth is teeming with more life than it has seen in millions of years. That’s fundamental science.
So if you want the system to have more life, just add carbon to it. One way is to add old carbon (by burning fossil fuels) that’s been locked away in the ground.
On the other hand if you want to limit life, then remove the carbon from the eco-system. Funny how the alarmists claim to be worried about life being under threat on earth, yet are striving to remove its very source.
Share this...FacebookTwitter "
nan
"The New South Wales division of the Young Liberals’ push for a change in the government’s climate policy was spurred on by a membership which understands the risk it faces if no action is taken, its president has said. The NSW branch passed a motion at the Young Liberal council in early December calling for a “practical, market-based means” for Australia to cut emissions by 30% of Kyoto levels by 2030, and provide energy market certainty.  The motion puts them at odds with the Morrison government, which has stubbornly insisted no change is needed to Australia’s emissions reduction strategy, even as the nation struggles through an unprecedented and deadly bushfire crisis, but is in line with the views of the NSW environment minister, Matt Kean, who has demanded more action. The NSW Young Liberal president, Chaneg Torres, said his branch was just doing its job. “The Young Liberals exist within the party to reflect the views of young people, to our MPs, to our wider party, and I think from our point of view, climate change is a particularly important issue for our generation because it concerns the actions of today affecting the lives and quality of life of my generation, but also generations to come,” he said. “And we certainly understand that it is a risk that we will particularly face, my generation, if nothing is done in the present.” Torres did not go so far as to criticise the Morrison government, which he said had its “heart in the right place”. “And the prime minister has said very clearly that his government and the Liberal party generally accepts the reality of climate change and accepts the need for action,” he said. “But this is an area of policy that has been fraught for many years. I think that, in many ways, the policy well has been poisoned and it has been a very divisive issue in our community. “And I think that in many ways the government has to deal with that fact. “From our point of view, as a young liberal movement, we are reflecting the views of our generation and providing encouragement to the government to take the issue seriously and do what they can to ensure they is a stable and certain policy environment where investment can be made in renewable energy.” Part of the plan endorsed by the division includes taxation write-offs for green assets, a nationwide solar scheme, and repealing the state and federal ban on nuclear energy so it can be considered for baseload power, as well as increased investment in hydro. The Queensland Young LNP also passed motions calling for nuclear energy to be explored as part of a push to lower emissions at its last conference, but did not go as far as the NSW branch in calling for an emissions reduction blueprint. The NSW branch will send a brief to its state and federal MPs, which are in power at both levels. But Torres said he understood the difficulties, with MPs such as Craig Kelly – an influential backbencher in the party room – still denying the need for any action. “Well, as you know, the Liberal party is a very broad church, but I would say that the youth wing of the party, the vast majority of us believe in the need to take action on climate change and actually I’d say the majority of our membership generally in the Liberal party believe that things need to be done to address climate change,” he said. “I think the vast majority of Liberals would agree with what the prime minister said that we don’t need to choose between a strong economy and action on climate change. “Now we can have a debate about what particular measures we need to take, but I think that the vast majority would agree that climate change is real and the action needs to be taken.” The NSW division, like the state environment minister, Matt Kean, who made similar comments, has received some pushback from within the party and its supporters for its stance, which Torres would not comment on. But it did receive support from other Young Liberals who said things needed to change. “NSW are being proactive about what real Young Liberals stand for and the other states need to grow a spine,” one senior member said. “Federally, the Young Libs have a policy vacuum and we haven’t got anything to move away from the previous president’s controversial positions on the ABC, gay marriage and Muslim immigration.”"
"Dozens of people have been killed, and with many more missing, after Volcán de Fuego (Fuego) in Guatemala erupted on June 3 2018.   In recent years, Fuego has regularly ejected small gas and ash eruptions, which hold little risk to surrounding populations. But Fuego also has a reputation for producing larger explosive eruptions. These larger eruptions have two main primary hazards – falling ash and bombs (collectively known as tephra), and pyroclastic flows. Of these two, pyroclastic flows are the big killers, and are responsible for the deaths from the  latest eruption. So, just what are these flows and why are they such killers? And what can people do to avoid them? Footage taken from a road bridge over a dry valley from on June 3 at Fuego shows what appears to be a soft and billowing ash cloud gently flowing down the volcano. It looks innocuous. Spectators and officials watch mesmerised, but then the cloud moves into the valley and heads directly towards the bridge. Unease spreads and soon alarms sound before people rush away just in time. Footage shows the ash cloud quickly pass under and then over the bridge. These spectators escaped death by seconds, as this benign-looking ash cloud is the notorious killer that is a pyroclastic flow. Pyroclastic flows (also known as pyroclastic density currents) contain a hellish combination of hot rock fragments (pyroclasts), superheated air, and volcanic gases. You can expect temperatures of 100-600°C and they can travel fast – very fast on steep slopes. Speeds generally range between 70-200mph, but they have been recorded reaching 450mph. As they are heavier (denser) than air, they tend to be funnelled into valleys. But their higher density also gives them momentum, so they can travel up the sides of valleys, and even over mountains. The worst place to be when a pyroclastic flow is on the move is in a valley, which is why the spectators at the road bridge were lucky to escape.  What surprised many volcanologists (including us) is that people were actually standing and taking pictures while watching this billowing cloud descend. It is certainly a hypnotic and beautiful phenomenon to observe, but any volcanic cloud moving even vaguely in your direction is a clear sign to flee. This suggests that further education of people living in and around Fuego of its volcanic hazards would not only be helpful, it would save lives. It is rare that eruptions from Fuego produce such large pyroclastic flows that travel so far. This leaves the authorities in an impossible situation. Because if you create exclusion zones based on worst-case scenarios, then decades if not centuries may pass without a worst-case eruption. And all that time people will be grumbling about good and fertile land being inaccessible without good reason. One notorious example of a pyroclastic flow happening elsewhere was the eruption of Mount Pelée on the island of Martinique on May 8 1902. Pyroclastic flows destroyed the town of Saint-Pierre and killed an estimated 30,000 people. Only a handful survived, one of whom was a prisoner in a jail cell. This was the largest loss of life from a pyroclastic flows in the past two centuries. One of the most famous historical examples of the devastation and loss of life caused by pyroclastic flows is what happened at Pompeii and Herculaneum when Vesuvius erupted in 79AD. An important lesson from this eruption is the fickleness of human memory. Because Vesuvius had been dormant for at least 700 years, it wasn’t recognised as a potential threat.  Volcanologists know from their studies that the frequency of large eruptions at a specific volcano may be one every few centuries or every few thousand years. But on a human time scale these numbers lose impact because there may be no aged relatives around who remember past eruptions, and so a complacent sense of “all is well” pervades.  Many communities living around volcanoes have other more immediate concerns, including other natural hazards. It’s an unresolved paradox. The eruption of Vesuvius produced a number of pyroclastic flows which led to the deaths of at least 1,400 people, and the burial of the settlements by volcanic material. Recent excavations revealed evidence of a new type of death from this eruption – a person being struck by a large block, possibly carried by one of the pyroclastic flows.  It is too early to tell what will happen next with Fuego. But given its recent history, this eruption is an unusually large and extreme event. Fortunately, these tend to be infrequent. It is sincerely hoped that there will be no sudden repeat of the large and far-travelled pyroclastic flows that took so many lives. Whatever happens, there will be a huge amount of work to do in rebuilding communities and working through collective grief."
"The fire situation in eastern Australia continues to rapidly escalate. At this stage we cannot predict when this will come to an end, but with losses of lives and property mounting on the south coast of NSW, eastern Victoria, South Australia, southwestern WA and Tasmania, we now have a nationally significant catastrophe that affects city and country alike. The magnitude of these fires alone (about 5 million hectares and rapidly rising), apart from their human and environmental consequences, simply shows us that we now confront a new, more flammable world: a coupling of people, ecosystems and fire that is now irrevocably transformed. As a society we should admit that our current policy, operational, knowledge-gathering and research capacity is inadequate to deal with such a new, fiery world. How do Australians in our most populated regions live in a future defined by heat, drought, powerful wind and lightning storms that lead to inevitable uncontrolled fires? The answers must lie in the intersection of the resilience of human communities, our experience and love of place, with the natural environment. Yet understanding this intersection demands resolving the various contribution of climate change, land management and community preparation and resilience. How many lives, properties, threatened species, ecosystems and their services, did our current management and response capacity actually save? What was the return on human and financial investment in fire preparation and emergency? Are existing administrative arrangements in firefighting and emergency management appropriate? What is the right balance between community/individual responsibility vs. centralised command and control? What is the role and sustainable capacity of volunteer fire management? What can Indigenous fire knowledge bring to bear in stem these blazes? How can biodiversity and ecosystem services, like water and carbon storage, be protected? Comprehensive answers to these questions are not simple to acquire, because of the interlocking nature of the process involved. But to effectively adapt to the challenges the future compels us to deal with them on a scale never attempted before. The temptation to make sense of this unprecedented crisis, and identify a way forward, is to repeat the same model of inquiry that has played out over the last century. Every major bushfire season in Australia has been followed by inquiries and their recommendations, that have led, haltingly, to improved capacity to fight and co-exist with fire threats. However there are a plethora of recommendations that have failed to gain traction in the complex administrative and political ecosystems that reinforce the status quo. For instance, while the Stretton Royal Commission, that followed the 1939 fires in Victoria, established the foundations of modern, organised fire management capacity, its successor, the 2009 Victorian Bushfires Royal Commission has had a lesser impact. Critical recommendations concerning planning, land use and management lie discarded after failure of implementation. As with most inquiries, the feasibility, cost-effectiveness and capacity of governments and citizens to implement its recommendations were beyond both the scope, resources and timeframe of the last Royal Commission. The legacy of a State-centric approach to land, fire and emergency management, so often viewed as a strength by many abroad is now a conspicuous weakness when it comes to the challenges of a world transformed. In sum, all the old bets are off. While laudable initiatives such as the post season summit proposed by ex-fire chiefs or politicians have been proposed, these will be inadequate to derive viable solutions until basic questions about the bushfire crisis are thoroughly explored and answered. The scope and scale of the ongoing national catastrophe requires a significant and non-partisan investments in national capacity to research, investigate, understand and innovate to meet the challenges ahead. There are no short-term fixes or answers to be had in response to the challenge. Setting out to protecting old orthodoxies, and thereby eschewing more innovative solutions, will lead us down a well-trodden path that is incapable of comprehending and adapting to our new circumstances. There needs to be robust and evidence-based debate, encouragement for trying new approaches, and fostering diversity of opinion, outlook and experience. There is an urgent need to develop a nationally co-ordinated, but not centrally controlled, approach to resolving the key questions posed above. This initiative should fully harness the intellectual capacity of our management, research and training institutions, focusing their immense technological capacity for analysis of the fire, human, climate and environment nexus. Without such an approach unprecedented amounts of information yielded by the events of 2019/20 will evaporate, the hard lessons will be skipped, and the vulnerability to another fire crisis will remain. Simply stated, as a nation we are being transformed by drought, heat and fire, to adapt Australians must transform our understanding of these fundamentals, in order to plan, cope and live in a more flammable world. Professor Ross Bradstock is the director at the Centre for Environmental Risk Management of Bushfires at the University of Wollongong. David Bowman is the professor of pyrogeography and fire science, and the director of the Fire Centre Research Hub at the University of Tasmania "
"The targets set in the Paris Agreement on climate change are ambitious but necessary. Failure to meet them will lead to widespread drought, disease and desperation in some of the world’s poorest regions. Under such conditions mass migration by stranded climate refugees is almost inevitable. Yet if richer nations are to be serious in their commitment to the Paris target, then they must begin to account for the carbon emissions contained within products they import. Heavy industry and the constant demand for consumer goods are key contributors to climate change. In fact, 30% of global greenhouse gas emissions are produced through the process of converting metal ores and fossil fuels into the cars, washing machines and electronic devices that help prop up the economy and make life a little more comfortable. As one might expect, the wealthier parts of the world with their higher purchasing power do more than their fair share of consuming and polluting. For every item bought or sold there is a rise in GDP, and with each 1% increase in GDP there is a corresponding 0.5 to 0.7% rise in carbon emissions. The growing demand for day-to-day conveniences exacerbates this problem. For metal ores alone, the extraction rate more than doubled between 1980 and 2008, and it shows no sign of slowing. Every time you buy a new car, for instance, you effectively mine 3-7g of “platinum group metals” to coat the catalytic converter. The six elements in the platinum group have the greatest environmental impact of all metals, and producing just one kilo requires the emission of thousands of kilos of CO₂.  That car also consumes one tonne of steel and you can add to that some aluminium, a whole host of plastics and, in the case of electric cars, rare earth elements.  Often, no one is held accountable for the carbon emissions connected to these materials, because they are produced in countries where “dirty” industry is still politically acceptable or seen as the only way to escape poverty. In fact, of the carbon emissions that European consumers are personally responsible for, around 22% are allocated elsewhere under conventional carbon accounting practices. For consumers in the US, the figure is around 15%. Carbon emissions from the exhaust pipe tell only part of the story. To get a full sense of the carbon footprint of a car, you have to consider those emissions that go into producing the raw materials and digging a hole in the ground twice – once to extract the metals contained in the car, once to dump them when they can no longer be recycled.  Buying a new car and dumping the old one might be justifiable if the change was made because the new vehicle is more fuel efficient, but it is certainly not when it’s a question of personal taste or corporate-level planned obsolescence. The same is true for any number of high tech items, including smartphones that run on software that renders them unusable in the medium term. The environmental consequences of replacing a smartphone, in terms of carbon emissions alone, are considerable. Apple found that 83% of the carbon dioxide associated with the iPhone X was directly linked to manufacture, shipping and recycling. With these kinds of figures, it is hard to argue a sustainable case for upgrades – regardless of how many solar panels Apple sticks on the roof of its offices. Governments of richer countries that import products but not their emissions must stop pointing the finger at China or other manufacturing or mining giants and start taking responsibility. This means going further than they have been willing to go so far, and implementing sustainable material strategies that address a product’s entire lifecycle from mining to manufacturing, use, and eventually to disposal.  On an individual level people must vote with their money. It’s time to leave behind the laggards who hide the cost of the carbon contained within their products and who design them to fail in order to put profits before people and the environment."
nan
nan
"Navy ships and army aircraft have been dispatched to help fight devastating bushfires on Australia’s south-east coast that are feared to have killed at least 17 people, amid a spiralling debate over the government’s stance on the climate emergency. Thousands of people have fled apocalyptic scenes, abandoning their homes and huddling on beaches to escape raging columns of flame and smoke that have plunged whole towns into darkness and destroyed more than 4m hectares of land. Thousands of firefighters were still battling more than 100 blazes in New South Wales (NSW) state and nearly 40 in Victoria on Wednesday, with new fires being sparked daily by hot and windy conditions and, more recently, dry lightning strikes created by the fires themselves. At the end of Australia’s hottest-ever decade, Canberra, the capital, was blanketed in a cloud of dense smoke that left its air quality more than 21 times the hazardous rating. The haze drifted more than 1,200 miles (2,000km) to the South Island of New Zealand, where it turned the daytime sky orange. Fanned by soaring temperatures, strong winds and a terrible three-year drought, huge blazes have ravaged a tinder-dry landscape, causing immense destruction: since November, more than 900 homes have been lost in NSW alone. With three months of the summer still to go, the early and devastating start to the country’s fire season has led authorities to rate it the worst on record and prompted urgent questions about whether the conservative government of the prime minister, Scott Morrison, has taken enough action on global heating. Polls show a large majority of Australians see the climate emergency as an urgent threat and want tougher government action, but Morrison has focused instead on the nation’s response to the bushfire crisis and defending Australian business, while other government officials have publicly disparaged climate activists. In his New Year’s Eve address to the nation, Morrison did not make any connection between the bushfires and global heating, suggesting that while they were a terrible ordeal, Australians had faced similar trials throughout history. Past generations had “also faced natural disasters, floods, fires, global conflicts, disease and drought” and overcome them, the prime minister said in a video message. “That is the spirit of Australians, that is the spirit that is on display, that is a spirit that we can celebrate as Australians.” On Wednesday, he acknowledged at a reception that the fires were “a time of great challenge for Australia”, but deflected debate about the underlying cause of the fires, concentrating again on the nation’s resilience. Fire experts and scientists have described the scale and impact of this year’s fires as unprecedented and said that greenhouse gas emissions, while they do not cause fires, play a proven role in raising temperatures and creating the exceptionally dry conditions that make the risk of fire extreme or catastrophic. Although slightly cooler conditions on New Year’s Day gave the country a moment to take stock of the devastation, conditions were set to deteriorate again over the weekend, said the NSW state premier, Gladys Berejiklian. Dangerous fire conditions were forecast to return to eastern Victoria and NSW on Saturday, with temperatures again likely to reach the mid-40s. “We are assuming that weather conditions will be at least as bad as what they were yesterday,” Berejiklian said. “All of us have to brace ourselves.” While most of the destruction occurred on Tuesday, the ferocity of the fires meant many people were unable to find out basic information until New Year’s Day. Electricity and communications lines were cut for extended periods in many areas. Roads in and out of towns remained closed. Officials in NSW and Victoria said on Wednesday another five people were confirmed dead, and another man was presumed to have been killed. Scores more were missing and the death toll was likely to continue to rise, they said. Three bodies were found on Wednesday at Lake Conjola on the south coast of NSW, bringing the death toll in the state to 15. About 4,000 people in the coastal Victoria town of Mallacoota fled to the shore as winds pushed a fire toward their homes under a sky turned dark by smoke and turned red by flames. Dozens of homes burned before winds changed direction late on Tuesday, sparing the rest of the town. Mark Tregellas, a resident who spent the night on a boat ramp, said only a late shift in the wind direction spared lives. “The fire just continued to grow and then the black started to descend,” he said. “I couldn’t see the hand in front of my face, and then it started to glow red and we knew the fire was coming. “Ash started to fall from the air and then the embers started to come down. At that point, people started to bring their kids and families into the water. Thankfully, the wind changed and the fire moved away.” The Victoria state premier, Daniel Andrews, said four people remained missing after a massive blaze ripped through Gippsland, a rural region about 310 miles (500km) east of Melbourne. Mick Roberts from East Gippsland had been unaccounted for since Monday and was found dead in his home on Wednesday. “He’s not missing any more … sorry but his body has been found in his house,” his niece, Leah Parsons, said on social media. At Malua Bay, on the NSW south coast, survivors spoke of how 1,000 people spent the night on the beach. “Everyone was on the beach, just covered in ash and smoke,” Al Baxter, a retired rugby union international, told Guardian Australia. “There was a strange calmness. People were as close to the water’s edge as they could be. People were literally just lying on the beach trying to keep out of the smoke and ash.” Criticism of the Morrison government’s climate stance has intensified as the fires have raged. Australia is the world’s largest exporter of coal and liquefied natural gas, but the prime minister, who won a surprise election victory in May, last month rejected calls to downsize Australia’s lucrative coal industry. His government has pledged to cut greenhouse gas emissions by 26-28% by 2030, a modest figure compared with the centre-left opposition Labor party’s pledge of 45%. The leader of the minor Australian Greens party, Richard Di Natale, demanded a royal commission, the nation’s highest form of inquiry, on the crisis. “If he (Morrison) refuses to do so, we will be moving for a parliamentary commission of inquiry with royal commission-like powers as soon as parliament returns,” Di Natale said in a statement. Australia’s armed forces, including helicopters, fixed-wing aircraft and naval vessels, were being deployed to help fight the fires, bring water, food and fuel to towns where supplies were depleted and roads cut off, as well as evacuate residents. Victoria’s emergency management commissioner, Andrew Crisp, said the 176-metre HMAS Choules, due to arrive by Friday, may be used to evacuate many of those stranded in Mallacoota, although its capacity of 1,000 people would not be enough on its own to handle everyone who needed to get out. “It doesn’t have the current capacity for everyone at Mallacoota,” Crisp said. “We are exploring all our options … and certainly to look at evacuating some people from Mallacoota by sea is an option we’re seriously considering.” Besides the deployment of HMAS Choules, Australia’s defence force said it had been providing support to bushfire efforts in all states except Tasmania since 8 November and was dispatching Taipan, Black Hawk and Chinook helicopters plus two Spartan aircraft to Victoria, where they would help with firefighting efforts and provide humanitarian assistance to isolated communities."
nan
"
Share this...FacebookTwitterOzone Measurement Error  
‘Shatters’ Established Theory

It was 10 years ago this month that scientists revealed an order-of-magnitude-sized error in molecular chemistry measurement that threatened to severely undermine the commonly accepted explanation for  how ozone depletion occurs.
The iconoclastic discovery fomented “much debate and uncertainty in the ozone research community.”    The mechanism that causes polar ozone destruction had, with one measurement, become more “unknown” than known.
With the stunning new evidence, a leading ozone researcher proclaimed that, “Our understanding of chloride chemistry has really been blown apart.”
But then, in the ensuing months and years after the measurement error had been exposed, there was . . . silence.
Rarely, if ever, was this discovery of a molecular rate change “substantially [ten times] lower than previously thought” brought to the public’s attention again.  Ostensibly because of a lack of appetite for admitting they may be wrong, scientists just seemed to . . . move on.
The 1980s zeitgeist that insisted we humans are the predominant cause of ozone depletion due to our ozone depleting substances emissions has been maintained for more than 3 decades now despite a growing body of contrary evidence that says variations in ozone density are predominantly determined by natural phenomena (meteorology, volcanic eruptions), not human emissions.
The ozone “hole” narrative and the widely-held perception that governmental policies determine how small or large the “hole” gets would appear to be analogous to the current climate debate and its connection to the governmental push to dramatically limit CO2 emissions.

Schiermeier, 2007
As the world marks 20 years since the introduction of the Montreal Protocol to protect the ozone layer, Nature has learned of experimental data that threaten to shatter established theories of ozone chemistry. If the data are right, scientists will have to rethink their understanding of how ozone holes are formed and how that relates to climate change.
Markus Rex, an atmosphere scientist at the Alfred Wegener Institute of Polar and Marine Research in Potsdam, Germany, did a double-take when he saw new data for the break-down rate of a crucial molecule, dichlorine peroxide (Cl2O2). The rate of photolysis (light-activated splitting) of this molecule reported by chemists at NASA’s Jet Propulsion Laboratory in Pasadena, California, was extremely low in the wavelengths available in the stratosphere — almost an order of magnitude lower than the currently accepted rate. “This must have far-reaching consequences,” Rex says. “If the measurements are correct we can basically no longer say we understand how ozone holes come into being.”  What effect the results have on projections of the speed or extent of ozone depletion remains unclear.
The rapid photolysis of Cl2O2 is a key reaction in the chemical model of ozone destruction developed 20 years ago (see graphic). If the rate is substantially [10 times] lower than previously thought, then it would not be possible to create enough aggressive chlorine radicals to explain the observed ozone losses at high latitudes, says Rex. The extent of the discrepancy became apparent only when he incorporated the new photolysis rate into a chemical model of ozone depletion. The result was a shock: at least 60% of ozone destruction at the poles seems to be due to an unknown mechanism, Rex told a meeting of stratosphere researchers in Bremen, Germany, last week.
Other groups have yet to confirm the new photolysis rate, but the conundrum is already causing much debate and uncertainty in the ozone research community. “Our understanding of chloride chemistry has really been blown apart,” says John Crowley, an ozone researcher at the Max Planck Institute of Chemistry in Mainz, Germany.
 “Until recently everything looked like it fitted nicely,” agrees Neil Harris, an atmosphere scientist who heads the European Ozone Research Coordinating Unit at the University of Cambridge, UK. “Now suddenly it’s like a plank has been pulled out of a bridge.”

Ozone ‘Hole’ Size Naturally Determined  


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




CFCs Ban Effects Not Detectable In Trends
NASA, 2013
NASA scientists have revealed the inner workings of the ozone hole that forms annually over Antarctica and found that declining chlorine in the stratosphere has not yet caused a recovery of the ozone hole. …. [T]wo new studies show that signs of recovery are not yet present, and that temperature and winds are still driving any annual changes in ozone hole size.
The classic metrics create the impression that the ozone hole has improved as a result of the Montreal protocol. In reality, meteorology was responsible for the increased ozone and resulting smaller hole, as ozone-depleting substances that year were still elevated. The study has been submitted to the journal of Atmospheric Chemistry and Physics.
“Ozone holes with smaller areas and a larger total amount of ozone are not necessarily evidence of recovery attributable to the expected chlorine decline,” said Susan Strahan of NASA’s Goddard Space Flight Center in Greenbelt, Md.
“We are still in the period where small changes in chlorine do not affect the area of the ozone hole, which is why it’s too soon to say the ozone hole is recovering,” Strahan said. “We’re going into a period of large variability and there will be bumps in the road before we can identify a clear recovery.”
Barnes et al., 2016
Trends in trace atmospheric constituents can be driven by trends in their (precursor) emissions but also by trends in meteorology. Here, we use ground-level ozone as an example to highlight the extent to which unforced, low-frequency climate variability can drive multi-decadal trends. … Ozone trends are found to respond mostly to changes in emissions of ozone precursors and unforced climate variability, with a comparatively small impact from anthropogenic climate change. Thus, attempts to attribute observed trends to regional emissions changes require consideration of internal climate variability, particularly for short record lengths and small forced trends.
Pozzoli et al., 2012
The changes in meteorology (not including stratospheric variations) and natural emissions account for 75 % of the total variability of global average surface O3 concentrations.
Regionally, annual mean surface O3 concentrations increased by 1.3 and 1.6 ppbv over Europe and North America, respectively, despite the large anthropogenic emission reductions between 1980 and 2005.
Hossaini et al., 2015
Scientists report that chemicals that are not controlled by a United Nations treaty designed to protect the Ozone Layer are contributing to ozone depletion.

In the new study, published today in Nature Geoscience, the scientists also report the atmospheric abundance of one of these ‘very short-lived substances’ (VSLS) is growing rapidly.


The Ozone ‘Hole’ Reached Record Levels In Oct., 2015
     Ivy et al., 2017
Recent research has demonstrated that the concentrations of anthropogenic halocarbons have decreased in response to the worldwide phaseout of ozone depleting substances. Yet, in 2015 the Antarctic ozone hole reached a historical record daily average size in October.
Model simulations with specified dynamics and temperatures based on a reanalysis suggested that the record size was likely due to the eruption of Calbuco, but did not allow for fully-coupled dynamical or thermal feedbacks. We present simulations of the impact of the 2015 Calbuco eruption on the stratosphere using the Whole Atmosphere Community Climate Model with interactive dynamics and temperatures. Comparisons of the interactive and specified dynamics simulations indicate that chemical ozone depletion due to volcanic aerosols played a key role in establishing the record-sized ozone hole of October 2015. The analysis of an ensemble of interactive simulations with and without volcanic aerosols suggests that the forced response to the eruption of Calbuco was an increase in the size of the ozone hole by 4.5 million km2.

Today: The Assumption That Humans Determine Ozone
Destruction Persists Despite Growing Contrary Evidence
National Geographic
Chlorofluorocarbons (CFCs), chemicals found mainly in spray aerosols heavily used by industrialized nations for much of the past 50 years, are the primary culprits in ozone layer breakdown.
Science News
In a rare bright spot for global environmental news, atmospheric scientists reported in 2016 that the ozone hole that forms annually over Antarctica is beginning to heal. Their data nail the case that the Montreal Protocol, the international treaty drawn up in 1987 to limit the use of ozone-destroying chemicals, is working.
[P]ublic engagement was key to solving the ozone problem, with people coming together to identify an issue that threatened society and develop new technologies to fix it. In that respect, the most successful environmental treaty in history holds lessons for dealing with a much bigger threat, she says — climate change.
Share this...FacebookTwitter "
"It can be easy to overlook the monstrous scale of the Antarctic ice sheet. Ice, thick enough in many places to bury mountains, covers a continent roughly the size of the US and Mexico combined. If it were all to melt, as it has in the past, global sea levels would rise by 58 metres. While this scenario is unlikely, Antarctica is so massive that just a small fraction of this ice melting would be enough to displace hundreds of millions of people who live by the coast. Low-lying cities face the threat of flooding when extreme weather coincides with high tides. Although typically rare, these events are already increasing in frequency, and will become commonplace as global sea levels increase. Over the coming decades, rising sea levels from melting ice and the expansion of warming oceans will strain societies and economies worldwide. Improving our understanding of how much Antarctica has contributed to sea level rise in the past, and how much it will contribute in the future, is vital to informing our response to climate change. Achieving this is impossible without satellites. Antarctica is too vast, too remote – satellites are our only means of monitoring its behaviour on a continental scale. Satellites launched by the European Space Agency and NASA allow scientists to monitor changes in ice height, ice velocity and ice mass through changes in Earth’s gravity field. Each of these satellites provide an independent way to measure Antarctica’s past contribution to sea level rise. The ice sheet mass balance inter-comparison exercise (IMBIE) is an international effort: a team of 84 polar scientists from 44 organisations, including both of us, working together to provide a single, global record of ice loss from Earth’s polar ice sheets. In our latest assessment, published in Nature, we used 11 different satellite missions to track Antarctica’s sea level contribution since the early 1990s. We have found that since 1992 Antarctica has lost 2,720 billion tonnes of ice, raising global sea levels by 7.6mm. What is most concerning, is that almost half of this ice loss has occurred in the past five years. Antarctica is now causing sea levels to rise at a rate of 0.6mm a year – faster now than at any time in the past 25 years.  Most of this ice loss has come from West Antarctica. In the Amundsen Sea Embayment (named after Roald Amundsen, one of the first explorers to reach the South Pole) warming ocean temperatures have reduced the floating ice shelves which slow the flow of the mighty Pine Island and Thwaites Glaciers, resulting in a rapid acceleration of ice losses. Between 1992 and 2017 we have observed a threefold increase in the rate of ice loss from West Antarctica, from 53 to 159 billion tonnes a year. In the Antarctic Peninsula, the collapse of the Larsen B and Wilkins ice shelves in the 2000s has had similar consequences: an abrupt acceleration in the rate local glaciers drain into the ocean. This new knowledge will help us better predict sea level rise in the future. In 2014 the intergovernmental panel on climate change (IPCC) published its fifth assessment report, which includes modelled projections of Antarctica’s contribution to sea level rise over the century. By mapping our measured sea level contribution on top of these projections, we found that our previous assessment of Antarctic sea level contribution, which measured ice loss until 2012, was tracking the IPCC’s lowest projection. In light of the acceleration in ice loss we have observed over the past five years, we now find sea level rise from Antarctica to be tracking the IPCC’s highest projection. This amounts to an additional 15cm in global sea level rise from Antarctica alone by 2100. We have long suspected that changes in Earth’s climate will affect the polar ice sheets. The rapid increase in Antarctic ice loss and consequent sea level rise we have measured over the past 25 years are a clear indicator of climate change. Limiting global warming to 2℃ by 2100, as set by the Paris Agreement, looks increasingly unlikely. The rate at which ice losses from Antarctica will increase in response to a warming world remains uncertain. It is important, now more than ever, that we continue to use satellites to monitor Antarctica in order to better prepare ourselves for the challenges ahead."
"In 2004, the Guardian correctly predicted that the developed world’s overreliance on meat would be one of the most pressing issues for the survival of our species. “Britons need to say goodbye to burgers and meat,” we wrote, “because the overemphasis on meat in the western diet is one of the things that stifles sustainable food production.” Thankfully, the past decade and a half has seen an unprecedented interest in meat-free diets. In 2004, veganism was seen as a fussy, faddish lifestyle choice. Now, buoyed by blockbuster documentaries such as Cowspiracy and What the Health, and celebrities such as Ellie Goulding and Ariana Grande, it has never been so popular. According to the Vegan Society, 600,000 Britons are vegan. In 2006, this figure was just 150,000.  Demand for vegan food is at unprecedented levels: in 2018, the UK launched more vegan products than any other nation; in 2019, 250,000 people signed up for the Veganuary challenge, pledging to avoid all meat and dairy for the month of January. For those not willing to make the jump to full veganism, one in three Britons had at least reduced their meat consumption in 2018. But there is still a long way to go. The west’s overconsumption of meat and dairy continues to fuel global warming. Livestock is responsible for approximately 14.5% of greenhouse gas emissions; 70% of global deforestation takes place in order to grow animal feed. In 2019, the EAT-Lancet Commission on Food, Planet and Health determined that substantial dietary shifts must take place by 2050. “Global consumption of fruits, vegetables, nuts and legumes will have to double, and consumption of foods such as red meat and sugar will have to be reduced by more than 50%,” the panel of experts judged. By 2050, meat-eating could seem like a throwback, according to some experts. “Our current method of growing crops to feed to animals so we can eat animals is shockingly inefficient,” says Bruce Friedrich of the Good Food Institute, which works to develop alternatives to meat. “By 2050, [almost] all meat will be plant-based, or cultivated.” Plant-based “meat” is already here. The Beyond Burger and the Impossible Burger are widely available, but Friedrich predicts a widening of the products on offer to consumers. “We’ll have plant-based meat that doesn’t exist yet, whether it’s pork chops, steaks, tuna or salmon.” Anna Taylor, the executive director of the Food Foundation, thinks that plant-based substitutes will find their way into processed foods, without consumers being any the wiser. “Plant-based alternatives to animal protein will appear automatically in foods we eat, without consumers having to change their habits.” The real challenge will be persuading consumers to embrace cultivated meat. Also known as lab-grown meat, this is developed from animal or fish cells in the nutrient bath of a “bioreactor”. It is not yet on the market, but at least 40 private companies are working on cultivated-meat alternatives. In the UK, scientists at the University of Bath are growing bacon on blades of grass. The California startup JUST has created chicken nuggets in a bioreactor. Friedrich is optimistic that we will all be eating cultivated meat by 2050. “There won’t be factory farms or abattoirs in 2050,” he predicts. “People will look back at the idea of growing live animals for meat in the same way that we look back at horse-drawn carriages for getting from London to Brussels.” But we won’t entirely stop eating meat from animals reared for slaughter. “There will be some heritage breed farms and slaughterhouses where the animals are treated well,” Friedrich concedes. But it will be a limited market. If this all sounds like science fiction, strap yourselves in for the predictions of the food designer and futurist Chloé Rutzerveld. “We’ll skip all of our current existing foods and switch towards a whole new eating system where we build food with microorganisms,” she says. “Instead of growing crops or raising animals, we’ll use microorganisms such as fungi, bacteria, yeast and microalgae to directly produce the carbs, proteins and fats we need.” This food will again be produced in bioreactors, before being filtered and dried into powders. But we won’t be consigned to a joyless diet of flavourless dust. Rutzerveld claims that 3D-printing technology will be able to replicate the textures and flavours of regular food. “We can make a library of mouth feels and texture at a nanoscale,” she says, “in order to recreate sensations like freshness or juiciness.” Eating microorganisms grown in bioreactors will be transformative. “We’ll be able to make the food-production system so much more efficient, saving the land and water and energy resources.” Not everyone agrees. “Lab-grown meats are a red herring,” says Prof Pete Smith of the University of Aberdeen. “We don’t need them. We can get most of the protein we need from plant-based foods.” He is also doubtful that we will move to other protein sources such as insects. “Wealthy countries are already massively overconsuming protein,” he says. “We don’t need to move to alternate protein sources – if we cut in half the amount of protein we are already eating, we would be at healthier levels.” But even if we are not eating insects directly, they can still be of use in our wider food chain. “Insects could be really important for feeding animals,” says Taylor. “One of the problems we have is that ruminant animals (including cattle, sheep and goats) are often fed on soya, which is grown in deforested parts of the planet. If we could shift to other ways of feeding animals, such as insects, we can reduce deforestation.” Consumers will continue to turn to plant-based alternatives to dairy. Plant-based milks are already big business, and egg substitutes will be next: JUST and Zero Egg have already developed egg substitutes, and we can expect other brands to emerge. By 2050, climate change will dramatically affect what we can eat and drink. Speciality crops such as avocados, coffee and wine grapes – which can only be grown in a very narrow climate range – will be at risk. “A one- or two-degree change in climate could mean make-or-break for some regions growing speciality crops,” says Prof Gregory V Jones, an expert in wine growing at Linfield College’s department of environmental studies. “Wine-growing regions such as Greece, southern Italy, southern Spain and Portugal will all potentially experience issues,” he continues. In turn, though, wine-growing will become possible in places such as Scandinavia and the north of England. “They aren’t world-class producing regions now, but by 2050, if climate change continues, they could be.” If real progress isn’t made to halt global heating, food production in the global south will be imperilled. “Things aren’t looking good for the developing world,” says Smith. “China has a big problem with water – overextraction means that the soil is becoming too salty. India also has a problem with unsustainable groundwater use.” Rice-growing in China and wheat-growing in the northern plains of India could be affected. All of this augurs badly, given that China and India’s population is forecast to rise to a combined 3.2 billion by 2050. “Water stress will become more acute,” Smith warns. “The more the climate warms, the more droughts we will have. This will take place in areas that are already struggling to feed themselves.” In the UK, much imported produce may become prohibitively expensive. “We are very reliant on countries that are vulnerable to climate change and water scarcity,” says Taylor. The grapes and berries we now import from Morocco, Spain, India and South Africa will become scarce. We will have to return to more seasonal patterns of eating – bananas, for example, will no longer be a cheap household staple. Widening inequality will play out on dinner tables across the world. “I’m worried about uneven dietary development,” says Prof Corinna Hawkes of the Centre for Food Policy at City, University of London. “The diets of rich people will get better, and those of poorer people will get worse, and we’ll end up with appalling inequalities in what we’re eating.” She predicts that wealthy people will continue to reduce their meat and processed-food intake, and embrace indigenous food such as sorghum and amaranth. “These foods were once eaten by poor people, but will become resurgent in higher-income groups, as they become more interested in novel foods.” As fast-food companies continue to make inroads into developing nations, junk-food consumption will rise in historically impoverished communities. “These people have inadequate diets already,” says Hawkes, “and they will be adding junk food into their diet, which only contributes to the problem of obesity.” Rising obesity rates are a real cause for concern; half of Britain’s population is predicted to be obese before 2050. Globally, 60% of men and 50% of women will be obese by 2050, if current trends continue. “We need to put into place policies that level the playing field and reduce the abilities of the companies selling junk food to make it so affordable and accessible,” says Hawkes. “Junk food needs to be taken out of the spotlight and nutritious food put centre-stage.” So, what should we be eating in 2050? It’s fairly simple – what we should be eating now. (And what the Guardian predicted in 2004.) “We need to be eating more fruit and vegetables, wholegrains, less junk food, and less meat and dairy,” advises Taylor. “If we do these four things, we stand a chance of reversing some of the diet-related diseases that are currently crippling the NHS, and also bringing down the carbon and biodiversity impacts of our diet.” But whether we’ll be able to resist the siren pull of processed food and cheap meat remains to be seen. Whole grains: 232g a day, 811 calories Tubers or starchy vegetables (ie potatoes, cassava): 50g a day, 39 calories Vegetables: 300g a day, 78 calories Fruits: 200g a day, 126 calories Dairy foods (whole milk or equivalents): 250g a day, 153 calories Combined protein sources (to include beef, lamb, pork, chicken, eggs, or fish): 84g a day, 151 calories Other protein sources (to include legumes or nuts): 125g a day, 575 calories Fats from oil: 51.8g, 450 calories Added sugars: 31g a day, 120 calories Source: the EAT-Lancet Commission"
"Hurricane Irma passed directly over the tiny Caribbean island of Barbuda in September 2017. Irma was the fifth strongest hurricane ever recorded in the Atlantic, and it reached peak intensity just before landfall, when 180mph winds damaged almost every structure on the island, flattening many of them. Just days later, a mere “normal” hurricane, Jose, also passed over Barbuda. By that point, almost all of the island’s 1,600 or so inhabitants and tourists had been evacuated to nearby Antigua. Yet the prime minister of Antigua and Barbuda (the two islands jointly form a sovereign state), Gaston Browne, had an unusual reaction to the catastrophe. His first legislative response was not to set out a reconstruction plan or to provide funds for housing and essential services. Instead, he focused on land reform.   The proposed Barbudan Land Management (Amendment) Act, announced by the prime minister right after the hurricanes and still being debated in parliament, aims to introduce individual property rights on the island for the first time since its colonisation by the English in the 17th century.  Barbuda’s system of communal land ownership has been in place since slavery was abolished in 1834. Citizens do not own the land but have the right to use it after applying to a locally-elected council. As one council member put it: “A cleaner can apply for beachfront property and get it, and so can a doctor. So there’s no great inequality in Barbuda.” Leases are possible with the approval of the cabinet and the consent of the majority of the people, but what matters most is that each Barbudan has a right to a plot for a house, a plot to farm, and a plot for commercial enterprise. This scenario has not avoided tensions, but has been described by many as egalitarian and just. However, the government sees it as an obstacle to foreign investment or loans. The prime minister appears to have swallowed the idea, linked to the influential Peruvian economist Hernando de Soto, that property rights and foreign investment are the key drivers of growth in developing economies. In this worldview, the island’s reconstruction and development can only be achieved with the intervention of the market, but banks and investors won’t be interested without the guarantee of a clearly identified property title.  The government therefore wants to impose a standardised and uniform property regime. It sees land rights, individual ownership and foreign investments as untriggered opportunities that must be offered to residents and foreigners in order to transform Barbudan land into an asset of the global financial market. Forget public money for reconstruction, forget polluting countries’ responsibilities for climate change and forget the people’s right to be compensated for ecological and historical debts. In this view, the future of the island depends on individual debts and wealthy tourists.  All over the world, the idea of awakening sleepy capital through land reform and privatisation has entrenched inequality and concentrated land in fewer and fewer hands – even in places not affected by catastrophes and climate change. So its adoption in a situation like Barbuda raises further concerns. For example, some on the island have accused the government of taking advantage of the hurricanes to implement a shock doctrine that will favour local and international elites. The current system is egalitarian, they claim, while a market for property would inevitably gentrify and separate the community. To pick one high profile example, the backers of a proposed luxury resort – including Hollywood actor Robert De Niro – have been accused of exploiting the post-hurricane chaos to carry out a “land grab”. Other locals have raised the issue of legitimacy: reform is being implemented without proper consultation and while most people are not even living on the island. This may make it harder for them to claim land or even a mere economic reimbursement.  So who is set to benefit? The banks, for a start. Empirical studies from many different countries have demonstrated that land titling does not guarantee access to credit – people may end up “owning” land only to find it is soon repossessed by a bank or public authorities. It seems likely that Barbudans themselves will get squeezed by a growing appetite for their land and deprived of real control. At best, reform will still mean a very unequal distribution of properties, with elites concentrated along the best beaches and “quasi-slums” arising elsewhere. At worst, Barbudan land reform could lead to an island almost entirely owned by banks. The situation is typical of what happens when land is transformed from a common good into private property. Once Barbudan land becomes a global commodity, enjoyed by tourists but mainly by the investors behind the resorts, those investors will demand unconstrained, cheap and formalised access to their land.  And what of informal rights, traditions, mandatory consultations and the link between a people and the island? Some will see them as obstacles to maximising a return on their investments. Unfortunately, these same obstacles are exactly what allowed Barbudans to construct a unique society where access to land for housing and agriculture is a right not a privilege."
"Just six months into his presidency, Trump announced America’s withdrawal from the Paris climate accord – making the US, the world’s second-largest emitter of greenhouse gases, one of the only countries on earth that does not participate.  Trump has also gutted the Environmental Protection Agency, forced out or muzzled scientists whose research contradicts the administration’s political preferences, and appointed industry insiders to federal agencies. He has rolled back Obama-era emissions standards, massively expanded the federal land and water available for oil and gas drilling, and propped up the dying and environmentally ruinous coal industry. In only three years, Trump has appointed 185 judges – a staggering re-molding of the federal judiciary. (By comparison, Obama appointed 55 appeals judges in eight years.) Even if Trump loses re-election in 2020, the judges he has appointed will remain in office for decades. Rightwing activist groups have seized the opportunity to push new restrictions on abortion rights, including state abortion bans unprecedented since Roe v Wade. Ohio legislators even attempted to pass a bill that would force doctors to re-implant ectopic pregnancies, a dangerous procedure with no medical justification. Many of these radically anti-abortion laws are explicitly designed to test Roe v Wade, and there is a strong chance Roe could come up for review in the US supreme court, where liberal justices are now a minority. Healthcare policy has come under continual challenge, with the Trump administration attempting to dismantle or roll back many parts of the Affordable Care Act, better known as Obamacare. Trump kicked off his presidency by banning immigration from several Muslim countries; he has also separated migrant children from their parents, escalated Ice deportations, and worked to sideline immigration courts. One of Trump’s advisors, Stephen Miller, is suspected to be an outright white nationalist. Thanks in part to aggressive gerrymandering, the Republican party has consolidated control of state legislatures and governorships across the US. These lawmakers’ ultra-conservative agendas are often wildly unrepresentative of their more moderate, and increasingly diverse, populations. The past several years have seen a dangerous rise in voter suppression, a trend the Guardian is documenting in our ongoing special series on voting rights. Under now-governor Brian Kemp, Georgia purged hundreds of thousands of voters from its rolls before the 2018 midterm elections. Wisconsin is currently on the brink of purging more than 200,000 voters – in a state where Trump won by fewer than 23,000 votes. Despite an increasing bipartisan consensus on the need to reform the criminal justice system, the Trump administration has often promoted ineffective, backwards, and punitive criminal justice policies. Under former attorney general Jeff Sessions, the Department of Justice largely abandoned oversight of troubled police departments operating under federal consent decrees. He also worked to undo state-based marijuana legalization and drug law reform. Trump habitually denigrates US allies, the Nato military alliance, and his own intelligence agencies, while praising dictatorships including Russia, China, North Korea and Saudi Arabia. Polls show global opinion of the US, and Trump’s leadership, at de-moralized levels. In December, the Trump administration announced new work requirements that will strip 700,000 Americans of food stamps. We profiled some of the people who will be affected. Secretary of education Betsy DeVos, an ardent critic of public education, has loosened Obama-era restrictions on predatory for-profit colleges. She’s also fought attempts to introduce debt relief for the millions of Americans who owe a collective $1.6tn in student loans. Since Trump took power, the Guardian has been a constant and rigorous watchdog. We commit enormous resources to our investigative and public interest reporting. In all our journalism, we also work to give voices to the human beings affected by Trump’s policies. Next year America will face a momentous choice. The future of the White House, the supreme court, abortion rights, climate policy and a range of other issues are in play – at the same time that misinformation makes rigorous reporting more important than ever. That’s why robust, fact-based and independent reporting is critical: we hope to raise $1.5m to fund our journalism in 2020. With your help, we’ll continue to fight for the progressive values we hold dear – democracy, civility, truth. Please consider making a contribution. And as always, thanks for reading."
nan
nan
"
Share this...FacebookTwitterIn the wake of Category 5 hurricanes Irma and Harvey, Dr. Sebastian Lüning and Prof. Fritz Vahrenholt presented an analysis of what’s behind the hurricane activity and literature at their well known Die kalte Sonne climate website. Their hope is to bring the hurricane discussion back to some rationality.
The German media of course have been covering the story quite intensely, and at times hysterically. The general tenor of most statements: Hurricanes are not directly caused by climate change, however their power and destructiveness is increasing due to global warming.
The claim is that warmer oceans are providing the fuel to produce larger hurricanes.
As plausible as the theory may sound, Vahrenholt and Lüning decided to investigate the category 4 and 5 hurricanes and plotted their frequency over the past 100 years:

Fig. 4: The number of category 4 and 5 hurricanes between 1924 and 2016
There were quite a number of hurricanes in the 1930s and 1950s, as well as in the 2000s, but the trend has been sharply downward since 2010, despite the warming, and so considerable doubt swirls surrounding all the claims heard in the media.
No correlation found between man and hurricanes
Vahrenholt and Lüning looked at some scientific literature on hurricanes. For example a 2014 paper by Holland et al attempted to show man’s impact on hurricanes. Unfortunately the authors went back only to 1975, and produced the following plot:

Fig. 5: The dependency of the share of Cat.4-5 storms on modelled temperature rises (ACCI) in different oceans, green represents the Atlantic region, red is for the Indian Ocean, and blue for the Pacific. Source: Fig. 5b from Holland et.al (2014).
Even using the data from the carefully selected 1975 to 2011 period does not produce any significant trend, Vahrenholt and Lüning note. Moreover the two German analysts say Holland relied on too few data points coming from the Indian ocean and falsely applied them to claim a “global” trend.
Using the great number of typhoons in the Pacific for the carefully chosen period yielded absolutely no correlation (R² = 0.03). Vahrenholt and Lüning add:
Adding in earlier data also leads to a collapse in correlation for the Atlantic, as the paper only sees a man-made share first starting in 1960. Here the a carefully selected period was sought out and found.”
Decadal variability in hurricane energy and the literature shows an influence by the AMO. A paper Kevin J.E. Walsh of the University of Melbourne tells us just how difficult it is to get understand hurricane strength:
However, the Atlantic basin is noted for having significant multidecadal variability in TC (Tropic Cyclons, d.A.) activity levels. The basin was characterized by a more active period from the mid-1870s to the late 1890s as well as the mid-1940s to the late 1960s. These periods may have had levels of activity similar to what has been observed since the mid-1990s.”
No evidence of a link
“Using the trends from the 1975…2011 period to infer a powerful anthropogenic impact of the recent powerful Atlantic events in light of what we know, borders on sheer audacity,” Lüning and Vahrenholt write. “Apparently the claimed evident relationship between man-made climate change and strengthening hurricanes is not supported.”
Hurricanes driven by Passat winds
Vahrenholt and Lüning cite a new paper  to explain what impacts the energy of a hurricane. Mark A. Saunders of Great Britain and the USA diligently examined observations going back to 1878 and discovered a factor that describes the energy of a hurricane very well: the strength of the northern Passat winds.



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Fig 6: Correlation (r, blue) and its significance (p<0.1 is highly significant, red) on the hurricanes energy (ACE), -solid blue curve – and its number -dotted blue curve – with the Passat winds. Source: Fig. 3a from Saunders et.al (2017).
Driven by temperature differences between regions
A second related factor improves the correlation further: The temperature difference between the Main Developing Region (MDR) located within 10° – 20°N and 85°W – 20°W and the global tropical area within 10°S to 10°N. It is long known that hurricane development during times of El Nino is dampened and during La Nina it is enhanced, thus it has to do much more with natural oceanic variability.
Figure 8 below depicts the difference in sea surface temperature (SST) between the main developing region (MDR) and the tropics using observations ERSSTv5, with 10-year smoothing applied. 

Fig. 8: The black line is not the horizontal axis, rather its is the linear trend! One observes the AMO similar pattern.
Lüning and Vahrenholt also cite literature showing that the Passat winds will not increase with climate change, but rather indicate a decrease in hurricanes.
All science that seriously looks at hurricanes show no worsening of hurricanes being caused by climate change.”
And what about the thermodynamics of greater evaporation leading to more hurricane energy? A report by Friederike Otto of Oxford finds that there are many possible interactions involved in this highly complex weather phenomenon:
Dynamical factors and thermodynamic aspects of climate change can interact in complex ways and there are many examples where the circulation is as important as the thermodynamics.”
Otto also points out that the climate models are far from adequate:
But in practice this requires climate models that are able to reliably simulate the weather systems in questions over and over again to assess the likelihood of its occurrence.”
Some media outlets have responsibly pointed out that the problems and destruction caused by hurricanes have much more to do with people living in hurricane vulnerable areas.
Sea level rise not a real factor in hurricane flooding
The claim that rising sea levels (10 cm since 1960) caused by global warming is major factor in hurricane destruction is also a non-factor in view of the fact that hurricanes generate waves that are 6 meters high!
Big driver: SST difference between MDR and tropics
In summary, the real hurricane driver of hurricanes is the SST difference between the MDR region and the global tropics. The following graph tells us why current hurricane activity is so high.

Fig. 9: The current sea surface temperature difference between the MDR and the global tropical oceans, source: Tropical Tidbits.
People living in hurricane vulnerable areas need to hope that the curve soon returns to zero. Here and with the bPassat winds doe we find the real reasons for the terrible hurricanes.  Every thing else is propaganda on behalf of a “good cause”
 
Share this...FacebookTwitter "
"As a conservation professor I believe people need to understand why protecting nature matters to them personally. Appealing to human self-interest has generated support for conservation in Switzerland, for example, where the government protects forests partly because they help prevent landslides and avalanches, or among communities in Botswana which conserve wildlife partly because of the value of trophy hunting. But this understanding risks being obscured by unhelpful arguments over terminology. The story starts in 2005, when the Millennium Ecosystem Assessment was published. This document, the result of five years work by more than 1,300 scientists around the world, demonstrated beyond doubt that global ecosystems were in decline and that this really mattered. Perhaps its most significant legacy was a diagram which presented the ways human wellbeing is influenced by different categories of what it termed “ecosystem services”. For example, maintaining healthy seas is important because of the “provisioning services” provided to fishing communities, while mangrove forests may provide “regulating services” protecting people from coastal storm surges. The ecosystem services idea has since been hugely influential in mainstream politics and there are now degree courses, textbooks and whole journals framed around the concept. Jump forward 13 years and another global scientific effort has produced another conceptual framework. This time it’s IPBES: the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services. In a high-profile article senior scientists representing its consortium of 129 countries have replaced the term ecosystem services with what they argue is a new concept: “Nature’s Contributions to People”.  IPBES was one of the global initiatives set up in response to the success of the ecosystem services concept, so for scientists there to reject the term has caused quite a debate. There are two questions to answer: Is the concept of Nature’s Contributions to People substantially new? And, secondly, is it helpful? First, is it new? Proponents of the Nature’s Contributions to People concept argue that while the ecosystem services idea was readily taken up by ecologists and economists, it has failed to engage a range of perspectives from the social sciences and humanities. For them, there is too much emphasis on services which are easy to quantify, such as the value that insects contribute to agriculture through pollination and pest control (US$57 billion each year in the US alone, apparently). This has resulted in some world views being sidelined in policy debates. It is certainly true that a number of South American governments strongly dislike the concept of ecosystem services which they consider commodifies what are better seen as gifts from “mother earth”.  My sister Katherine Jones works in communications for RSPB Scotland. She agrees that while the term “ecosystem services” can be useful in discussions with British policy makers, it has never resonated with the general public. “When talking to ordinary people”, she told me “it is much more effective to appeal to their innate passion for nature than suggesting that nature provides a service, like a utility company or a bank”. However, the editor of the journal Ecosystem Services responded to the IPBES publication with a scathing editorial in which he argues convincingly that far from being new, Nature’s Contributions to People is simply a non-technical explanation of the same thing. He, and others, suggest that in trying to mark clear water between the two, the authors of the latest paper are wilfully ignoring both the large body of work which addresses issues such as commodification, and the success of ecosystem services in generating political interest in the environment. The second question concerns whether it is helpful. Prominent conservation scientist Kent Redford, and colleagues, pre-empted the recent debate when they pointed out some years ago that conservation suffers repeatedly from fads. Concepts or approaches are enthusiastically promoted for a few years then dropped only for a new concept to be introduced – which looks substantially like the old one but with a snappy new name. The risk they highlight is that by regularly rejecting, reinventing and repackaging approaches, the conservation community fails to learn the lessons from the failures of previous approaches as they view the new concept as so completely new that old issues don’t apply.  If Nature’s Contributions to People can help bring more actors to the table and address some of the limitations of ecosystem services, this will be helpful. However, problems and challenges will remain. Take the feeling of wellbeing many people get when they connect with nature, for example. “Cultural services” such as these have often been given less prominence because they are difficult to value – but it is not clear whether framing the issue around Nature’s Contributions will help solve this.  If you have read this far, then you may be wondering why this rather semantic argument should interest you. However, I would suggest that there are few issues more important than communicating to society at large why nature matters. If we need a new concept to keep this point fresh and alive in the minds of politicians and the general public then so be it. But let’s not argue. To paraphrase the classic number sung by Fred Astaire and Ginger Rogers: you say “Ecosystem Services” and I say “Nature’s Contributions to People”. The point made by both is that destroying nature ultimately harms us all."
"‘Good morning. Here is the shipping forecast for midday, 21 June, 2050. Seas will be rough, with violent storms and visibility ranging from poor to very poor for the next 24 hours. The outlook for tomorrow is less fair.” All being well, this could be a weather bulletin released by the Met Office and broadcast by the BBC in the middle of this century. Destructive gales may not sound like good news, but they will be among the least of the world’s problems in the coming era of peak climate turbulence. With social collapse a very real threat in the next 30 years, it will be an achievement in 2050 if there are still institutions to make weather predictions, radio transmitters to share them and seafarers willing to listen to the archaic content. I write this imaginary forecast with an apology to Tim Radford, the former Guardian science editor, who used the same device in 2004 to open a remarkably prescient prediction on the likely impacts of global warming on the world in 2020. Journalists generally hate to go on record about the future. We are trained to report on the very recent past, not gaze into crystal balls. On those occasions when we have to venture ahead of the present, most of us play it safe by avoiding dates that could prove us wrong, or quoting others. Radford allowed himself no such safe distance or equivocation in 2004, which we should remember as a horribly happy year for climate deniers. George W Bush was in the White House, the Kyoto protocol had been recently zombified by the US Congress, the world was distracted by the Iraq war and fossil fuel companies and oil tycoons were pumping millions of dollars into misleading ads and dubious research that aimed to sow doubt about science. Radford looked forward to a point when global warming was no longer so easy to ignore. Applying his expert knowledge of the best science available at the time, he predicted 2020 would be the year when the planet started to feel the heat as something real and urgent. “We’re still waiting for the Earth to start simmering,” he wrote back in that climate-comfortable summer of 2004. “But by 2020 the bubbles will be appearing.” The heat of the climate movement is certainly less latent. In the past year, the world has seen Greta Thunberg’s solo school strikes morph into a global movement of more than six million demonstrators; Extinction Rebellion activists have seized bridges and blocked roads in capital cities; the world has heard ever more alarming warnings from UN scientists, David Attenborough and the UN envoy for climate action, Mark Carney; dozens of national parliaments and city councils have declared climate emergencies; and the issue has risen further to the fore in the current UK general election than any before it. With only weeks to go until 2020, the bubbles of climate anxiety are massing near the surface. Radford’s most precise predictions relate to the science. Writing after the record-breaking UK heat of 2003, he warned such scorching temperatures would become the norm. “Expect summer 2020 to be every bit as oppressive.” How right he was. Since then, the world has sweltered through the 10 hottest years in history. The UK registered a new high of 38.7C this July, which was the planet’s warmest month since measurements began. He also correctly anticipated how much more hostile this would make the climate – with increasingly ferocious storms (for the first time on record, there have been category 5 hurricanes, such as Dorian and Harvey, for four years in a row), intensifying forest fires (consider the devastating blazes in Siberia and the Amazon this year, or California and Lapland in 2018) and massive bleaching of coral reefs (which is happening with growing frequency across most of the world). All of this has come to pass, as have Radford’s specific predictions of worsening floods in Bangladesh, desperate droughts in southern Africa, food shortages in the Sahel and the opening up of the northwest passage due to shrinking sea ice (the huge cruise liner, Crystal Serenity, is among the many ships that have sailed through the Bering Strait in recent years – a route that was once deemed impossible by even the most intrepid explorers). A couple of his predictions were slightly premature (the snows on Kilimanjaro and Mt Kenya have not yet disappeared, though a recent study said they will be gone before future generations get a chance to see them), but overall, Radford’s vision of the world in 2020 was remarkably accurate, which is important because it confirms climate science was reliable even in 2004. It is even more precise today, which is good news in terms of anticipating the risks, but deeply alarming when we consider just how nasty scientists expect the climate to become in our lifetime. Unless emissions are slashed over the next decade, a swarm of wicked problems are heading our way. How wicked? Well, following Radford’s example, let us consider what the world will look like in 2050 if humanity continues to burn oil, gas, coal and forests at the current rate. The difference will be visible from space. By the middle of the 21st century, the globe has changed markedly from the blue marble that humanity first saw in wondrous colour in 1972. The white northern ice-cap vanishes completely each summer, while the southern pole will shrink beyond recognition. The lush green rainforests of the Amazon, Congo and Papua New Guinea are smaller and quite possibly enveloped in smoke. From the subtropics to the mid-latitudes, a grimy-white band of deserts has formed a thickening ring around the northern hemisphere. Coastlines are being reshaped by rising sea levels. Just over 30cm at this stage – well short of the 2 metres that could hit in 2100 – but still enough to swamp unprotected stretches of land from Miami and Guangdong to Lincolnshire and Alexandria. High tides and storm surges periodically blur the boundaries between land and sea, making the roads of megacities resemble the canals of Venice with increasing frequency. On the ground, rising temperatures are changing the world in ways that can no longer be explained only by physics and chemistry. The increasingly hostile weather is straining social relations and disrupting economics, politics and mental health. Generation Greta is middle aged. Their teenage fears of the complete extinction of the human race have not yet come to pass, but the risk of a breakdown of civilisation is higher than at any previous time in history – and rising steadily. They live with a level of anxiety their grandparents could have barely imagined. The world in 2050 is more hostile and less fertile, more crowded and less diverse. Compared with 2019, there are more trees, but fewer forests, more concrete, but less stability. The rich have retreated into air-conditioned sanctums behind ever higher walls. The poor – and what is left of other species – is left exposed to the ever harsher elements. Everyone is affected by rising prices, conflict, stress and depression. This is a doorway into peak climate turbulence. Global heating passed the 1.5C mark a couple of years earlier and is now accelerating towards 3C, or possibly even 4C, by the end of the century. It feels as if the dial on a cooker has been turned from nine o’clock to midnight. Los Angeles, Sydney, Madrid, Lisbon and possibly even Paris endure new highs in excess of 50C. London’s climate resembles Barcelona’s 30 years earlier. Across the world, droughts intensify and extreme heat becomes a fact of life for 1.6bn city dwellers, eight times more than in 2019. For a while, marathons, World Cups and Olympics were moved to the winter to avoid the furnace-like heat in many cities. Now they are not held at all. It is impossible to justify the emissions and the world is no longer in the mood for games. Extreme weather is the overriding concern of all but a tiny elite. It wreaks havoc everywhere, but the greatest misery is felt in poorer countries. Dhaka, Dar es Salaam and other coastal cities are hit almost every year by storm surges and other extreme sea-level incidents that used to occur only once a century. Following the lead set by Jakarta, several capitals have relocated to less-exposed regions. But floods, heatwaves, droughts and fires are increasingly catastrophic. Healthcare systems are struggling to cope. The economic costs cripple poorly prepared financial institutions. Insurance companies refuse to provide cover for natural disasters. Insecurity and desperation sweep through populations. Governments struggle to cope. “By 2050, if we fail to act, many of the most damaging, extreme weather events we have seen in recent years will become commonplace,” warns Michael Mann, the director of the Earth System Science Center at Pennsylvania State University. “In a world where we see continual weather disasters day after day (which is what we’ll have in the absence of concerted action), our societal infrastructure may well fail … We won’t see the extinction of our species, but we could well see societal collapse.” Adding to the anxiety is the erratic temperature of the planet. Instead of rising smoothly it jolts upwards, because tipping points – once the stuff of scientific nightmares – are reached one after another: methane release from permafrost; a die-off of the tiny marine organisms that sequestered billions of tonnes of carbon; the dessication of tropical forests. People have come to realise how interconnected the world’s natural life-support systems are. As one falls, another is triggered – like dominos or the old board game, Mouse Trap. In some cases, they amplify one another. More heat means more forest fires, which dries out more trees, which burn more easily, which releases more carbon, which pushes global temperatures higher, which melts more ice, which exposes more of the Earth to sunlight, which warms the poles, which lowers the temperature gradient with the equator, which slows ocean currents and weather systems, which results in more extreme storms and longer droughts. It is also now clear that positive climate feedbacks are not limited to physics, but stretch to economics, politics and psychology. The Amazon is turning into a savannah because the loss of forest is weakening rainfall, which makes harvests lower, which gives farmers an economic motivation to clear more land to make up for lost production, which means more fires and less rain. On our current course, carbon concentrations in the atmosphere will pass 550 parts per million by midcentury, up from around 400ppm today. Katharine Hayhoe, an atmospheric scientist and director of the Climate Science Center at Texas Tech University, explains how this stacks the odds in favour of disaster. “By 2050, we’d be seeing events that are far more frequent and/or far stronger than we humans have ever experienced before, are occurring both simultaneously and in sequence.” Her greatest concern is that food production and water supply systems could buckle under the strain, with dire humanitarian consequences in areas that are already vulnerable. Generation Greta live with a level of anxiety their grandparents could barely have imagined Hunger will rise, perhaps calamitously. The United Nations’ International Panel on Climate Change expects food production to decline by 2% to 6% in each of the coming decades because of land-degradation, droughts, floods and sea-level rise. The timing could not be worse. By 2050, the global population is projected to rise to 9.7 billion, which is more than two billion more people to feed than today. When crops fail and starvation threatens, people are forced to fight or flee. Between 50 and 700 million people will be driven from their homes by midcentury as a result of soil degradation alone, the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES) estimated last year. Fires, floods and droughts will prompt many others to migrate within and across borders. So will the decline of mountain ice, which is a source of meltwater for a quarter of the world’s population. The poorest will be worst affected, though they have the least responsibility for the climate crisis. For the US author and environmentalist, Bill McKibben, this injustice will make the greatest impact in 2050. “Forcing people to move from their homes by the hundreds of millions may do the most to disrupt the world. And, of course, it’s a deep tragedy, because these are precisely the people who have done the least to cause the problem,” he says. In 2050, climate apartheid goes hand-in-hand with increasingly authoritarian politics. Three decades earlier, worried electorates voted in a generation of populist “strongmen” in the hope they could turn back the clock to a more stable world. Instead, their nationalism made a global solution even harder to achieve. They preferred to focus on the immigration consequences of global heating rather than the carbon-capital causes. When voters realised their mistake, it was too late. The thugocracy refused to give up power. They no longer deny the climate crisis; they use it to justify ever-more repressive measures and ever-wilder efforts to find a technological fix. In the past 20 years, nations have tried volcano mimicking, cloud brightening, albedo modification and carbon dioxide removal. Most were expensive and ineffective. Some made weather circulation even less reliable. Powerful countries now threaten rivals not just with nuclear weapons, but with geo-engineering threats to block sunlight or disrupt rainfall patterns. This is not an inevitable future. Unlike Radford’s prediction for 2020, this vision of 2050 factors in human behaviour, which is more volatile and less predictable than the laws of thermodynamics. Many of the horrors above are already baked into the climate, but our response to them – and each other – is not predetermined. When it comes to the science, the dangers can be substantially reduced if humanity shifts decisively away from business-as-usual behaviour over the next decade. When it comes to the psychology and politics, we can make our situation better immediately if we focus on hope in shared solutions, rather than fears of what we will lose as individuals. That means putting faith in institutions, warning one another about risks, and treasuring shared eccentricities and traditions – a bit like the shipping forecast. A storm is certainly brewing. The science is clear on that. The question now is how we face it."
"Snow and glaciers in New Zealand have turned brown after being exposed to dust from the Australian bushfires, with one expert saying the incident could increase glacier melt this season by as much as 30%. On Wednesday many parts of the South Island woke up to an orange haze and red sun, after smoke from the Victorian and New South Wales blazes drifted east on Tuesday night, smothering many parts of the island for most of the day.  On Thursday, pictures taken from the Southern Alps showed the smoke haze carrying particles of dust had tinged snow-capped mountain peaks and glaciers a shade of caramel, with former prime minister Helen Clark expressing concern for the long-lasting environmental impacts on the mountains. “Impact of ash on glaciers is likely to accelerate melting,” Clark tweeted. “How one country’s tragedy has spillover effects.” There are more than 3,000 glaciers in New Zealand and since the 1970s scientists have recorded them shrinking by nearly a third, with current estimates predicting they will disappear entirely by the end of the century. Professor Andrew Mackintosh is head of the school of earth, atmosphere and environment at Monash University, and the former director of the Antarctic Research Centre. He said in nearly two decades of studying glaciers in New Zealand he had never seen such a quantity of dust transported across the Tasman, and the current event had the potential to increase this season’s glacier melt by 20-30%, although Mackintosh stressed this was no more than an estimate. “It is quite common for dust to be transported to New Zealand glaciers, but I would say that the amount of transport right now is pretty phenomenal – I don’t think I’ve ever seen anything like it,” Mackintosh said.“It is concerning to me to see so much material being deposited on the glaciers.” #AUSTRALIANBUSHFIRES pic.twitter.com/7XDjERi71n Mackintosh said the whiteness of snow and ice reflected the sun’s heat, and slowed melting. But when this whiteness was obscured the glacier could melt at a faster rate. The higher glaciers around Mount Cook could likely get more snowfall soon, Mackintosh said, but the lower glaciers may not get another dump till March, and the dust would sit there until then, likely turning pink when algae began to grow. The impacts of the dust event would likely last no longer than a year, Mackintosh said, but if Australia continued to be impacted by extreme wildfires and droughts “it will be one of the factors that is accelerating the demise of glaciers in New Zealand overall”. The recent smoke haze drifting over New Zealand is the fourth such event this summer, the Met Service said, and despite no official health warnings being issued, many with asthma said they were choosing to remain indoors during the unusual conditions. This the view from the top of the Tasman Glacier NZ today - whole South island experiencing bushfire clouds. We can actually smell the burning here in Christchurch. Thinking of you guys. 😢#nswbushfire #AustralianFires #AustraliaBurning pic.twitter.com/iCzOGkou4o The Met Service said most of the smoke remaining over New Zealand would clear by Friday. Early in December travel writer Liz Carlson took pictures of regions of the Southern Alps turning pink following exposure to smoke from Australia early in the bushfire season. In a blog Carlson wrote: “It’s pretty remarkable to see the impact of the fires from so far away.” “Our glaciers don’t need any more battles as they are already truly endangered; it puts the impact of climate change into even more stark reality we can’t ignore.” Hazy sunrises for the North Island today! The main band of smoke has moved north from yesterday, while another band of smoke lingers over the South Island. ^Tahlia pic.twitter.com/eafnnsu89q Residents in Auckland and some parts of the North Island woke to an unusually bright orange sun on Thursday, thought to be a result of the bushfires 2,000km across the Tasman sea. The Ministry of the Environment has been contacted for comment."
"With the ongoing bushfire crisis, it is clear that we are in the middle of a historic event that will change the way we manage fire in the Australian environment. As a bushfire protection scientist, I am mostly focused on the practical problems these major events have given rise to, what went wrong or right, how we can solve these and prepare for the future.  My personal perspective is based on working as a fire protection planner in the aftermath of the 2003 Canberra fires and the 2009 Victorian Black Saturday fires. These events resulted in significant changes to how we protect communities, which have been successful in a lot of cases. There are still several months to run in the current fire season, but we can already start to look to the big questions that we will need to answer once the fires are finally out. We fight fires with data as much as water these days. Apps and social media are now an essential part of warning communities and coordinating evacuations. While this is an amazing capability that simply did not exist at the time of the Canberra and Victorian fires, it was clear from tweets by fire protection experts, such as Bianca Nogrady in the Blue Mountains, that our telecommunications system was being pushed to its absolute limit as people tried desperately to keep on top of where the fires were.  The data-hungry nature of modern bushfire management isn’t just in the emergency operations space. Following Black Saturday, I was part of a team that used state-of-the-art fire prediction models to design bushfire shelters for Victorian public schools, as well as enhanced ember prediction models for the design of new suburbs in Canberra. Previous disasters have taught us lessons around designing houses for bushfire that have served us well in the current situation, but I predict that coming out of this current fire emergency we will see another review of building standards in bushfire zones, as well as an increased focus on adapting Indigenous burning prescriptions to create a more open, park-like vegetation structure near our suburban edges. These two broad approaches complement each other – both reducing the hazard and increasing the resilience of housing stock. Building standards will be a good starting point, as we have control over the design of bushfire resilient houses, and there is a well-established testing and verification system. There is almost certainly going to be an adjustment to building standards, particularly for Queensland. New South Wales has recently reviewed the bushfire planning system, and has effectively prohibited building in areas where there is likely to be direct flame contact with houses.  The fire danger index we use to scale the fire models was previously set at 50 (out of 100) for Queensland. We are now regularly seeing conditions in the 80-100 range, something that was previously an extremely rare event in northern areas of Australia. This was predicted in climate change models, with humidity and moisture dropping to levels similar to NSW and Victoria pretty much when they were expected to. It’s time to adapt our new buildings and look seriously at what we can do to strengthen existing stock that was built to an older, gentler climate that no longer exists. The inevitable calls for more broad-scale hazard reduction burning remind me of the saying that for every complex problem there is a solution that is clear, simple, and wrong. It seems so obvious – burn the fuel while conditions are mild and dangerous fire weather has less capacity to harm us. Like most things in life, the reality is more difficult – large scale fuel reduction is one of the most complex land management treatments to organise and implement. Weather conditions and fuel moisture have to be in a “goldilocks zone”, allowing the fire to burn hot enough to consume lots of fuel, but not so severely that control is impossible. Treating a large enough area requires large numbers of staff (paid and volunteer) to control the burn and protect assets. Even when everything goes right there is the further complication of smoke pollution affecting vulnerable groups. Despite this, agencies met or exceeded their hazard reduction targets this year, which highlights how overwhelming the fire weather extremes have been – in a lot of areas the fuel reduction simply didn’t work. As has been reported several times there have been significant losses during the current fires even in areas where hazard reduction burns had been carried out. The structure of these forests still allowed damaging crown fires to develop, even where the low-level fuels had been reduced. Improving the effectiveness of hazard reduction in the places where suburbs and bushland meet is going to be a major focus. I spend a lot of my time working to balance fire protection with maintaining aesthetic and ecological values. The analysis of house loss on Black Saturday recommended changing the structure of forests and woodlands immediately adjacent to houses. This involves significant thinning of trees and shrubs in the areas near houses to create an open, park-like look and feel that can be maintained in the long term. This needs to be combined with resilient house design and intensive fuel reduction in the immediate surrounds of houses (within eight metres) for maximum effect. So we need to look at approaches that allow us to safely and intensively reduce fuels close to the urban fringe and deliver this vegetation structure. This has led us back to some of the original methods used to work with fire and the landscape, perfected by the oldest continual culture on our planet. Our growing respect for Indigenous culture has seen an increased use of traditional knowledge in fire management, and I have worked on recent projects where we were able to embed this approach in management systems. We are increasingly seeing Indigenous fire managers working alongside bushfire assessors, and it has been fascinating to see how they measure the right time to burn based on fuel condition and knowledge of local weather. There are two significant advantages of traditional burning that make it a good fit for property protection. Firstly, it can be implemented safely close to assets with minimal equipment. The second advantage is that it has an ecological end-state as an objective, often aiming to create an open, park-like vegetation structure that has much less potential for damaging crown fires. The obvious and significant disadvantage is that so much knowledge has been lost, coupled with significant increases in permanent (and very valuable) assets in the landscape. It is critical that we take advantage of the elders who are focused on preserving Indigenous burning regimes and adapt these to fit our modern lifestyles. As a minimum every fire manager should read Dark Emu by Bruce Pascoe. The chapters on fire provide a well-researched summary of how Indigenous fire management worked that is accessible to a wide audience. The houses and lives lost hit us hard as fire planners; it is more than just a job for most of us. We are going to honour those people by learning as much as we can from this disaster and continuing to adapt. Cormac Farrell is an environmental scientist specialising in both the vegetation management and building protection aspects of bushfire management "
"Countries with high levels of human well-being are more likely to show increasing forest growth. That’s the finding of a new study by a group of Finnish scientists, published in PLOS ONE. Their work shows that countries exhibiting annual increases in the amount of trees typically score highly on the UN’s Human Development Index (HDI), a scoring system that uses measures of life expectancy, education, and income to assess development status. Meanwhile, countries with a net annual forest loss typically score lower on the HDI. The logical leap of faith here is to think that a remedy for the ongoing loss and degradation of much of the world’s forests would be a massive push for development in deforested countries. But while such a noble undertaking would be desirable in many ways, these apparent environmental links warrant scrutiny. The authors themselves discuss caveats to their findings, and these should not be ignored. For example, switching from net forest loss to net gain may simply involve sourcing things like wooden furniture or paper pulp from abroad, often from poorer nations with weaker environmental policies and safeguards. This process, known as “leakage”, was perhaps best described and documented by the geographer Patrick Meyfroidt and colleagues in 2010. Among other examples, they illustrate leakage by looking at Vietnam, where national increases in forest cover were linked to sharp increases in imported wood, about half of which was illegal. If such processes are occurring, then how far, and for how long, can the buck of exporting environmental impacts be passed? In any case, those recovered forests often aren’t all they seem. Under some definitions they can include plantations of oil palm or rubber – technically “forests”, yet with few of the ecological benefits of the environment they replace. Even the supposedly naturally-recovered forests are rarely, if ever, as biologically diverse and well-functioning as their natural predecessors.  Things can be worsened by forest restoration schemes which may have human, rather than ecological, motives at heart. In Indonesia, for example, I have witnessed forest restoration work in national parks that favoured useful exotics over native forest species. In Tanzania, local NGOs such the as the Tanzania Forest Conservation Group lobby for policies that promote forest conservation over (and in addition to) tree planting, citing both ecological and well-being benefits. The clear message here is that it is far preferable to prevent damage in the first place, than to try and restore former conditions at a later point. Modern concepts of sustainable development are typified by the UN’s 17 Sustainable Development Goals (SDGs), which cover a variety of topics, including matters of well-being, infrastructure and environment. Studies of how these goals interact (whether for better or worse) are important if we are to achieve truly sustainable development. The latest forest cover study uses a composite index to investigate forest trends, which may disguise a more complex picture. Past work has shown that improved education (SDG 4) is commonly associated with reducing deforestation, while the effect of increasing GDP (SDG 8) on forests is far more complicated. The authors use a metric that combines these components (along with life expectancy), which does not explain how they interact. Further complexities involve other areas of development, which have their own effects. For example, in countries with high levels of inequality (SDG 10), development can exacerbate deforestation rates, rather than remedy them. In Brazil, for example, national efforts to raise people’s development status proved more damaging to forests in municipalities with high levels of land inequality than in those where land was more fairly shared. Some work suggests that improvements in gender equality (SDG 5) could have positive outcomes for forests, while forest-degrading activities witnessed during times of conflict suggest that peaceful relations (SDG 16) are also conducive to healthy forests.  On the flipside, achieving global food security (SDG 2), meeting energy needs (SDG 7), and developing sustainable infrastructure (SDG 11) will all require careful planning and monitoring to ensure that their environmental impacts are minimised. Ultimately, this paper gives reasons to feel positive about the inevitable development of humans and the fate of the world’s forests. It implies that, at a certain level of development, forests lost or degraded in the processes of developing will begin to regenerate or repair (whether naturally or with human assistance). I sincerely hope that the Finnish team’s work encourages nations across the world, developed or otherwise, to restore as much forest as they can. Nevertheless, in an age of rapid climate change, biodiversity loss, and human population growth, we need our remaining forests more than ever. The world must find sustainable ways to develop that do not involve destroying what forests are left.  Following in the footsteps of already developed nations, and simply replacing forests at a later point, should not be considered a viable course of action."
"The Coalition’s history of reluctance to take action on climate change was alive and well in 1998 and 1999, with cabinet records giving a familiar insight into debates still raging 20 years on. As last year’s cabinet papers release illustrated, the government was nervous in the lead-up to the international climate summit in Kyoto at the end of 1997, not wanting to commit to anything it perceived could harm Australia’s interests.  By March 1998, the cabinet was acknowledging the good deal Australia believed it had negotiated. A submission by the foreign affairs minister, Alexander Downer, the minister for the environment, Robert Hill, and the minister for resources and energy, Warwick Parer, reported the Kyoto agreement was “fair and realistic” and met “all of Australia’s primary objectives”. Key to those was Australia’s insistence on differentiated emissions reduction targets for developed countries, “reflecting individual circumstances”. Another “important achievement”, the submission said, was “securing agreement to emissions from the land use change and forestry sector being treated in essentially the same way as those from other anthropogenic sources”. This controversial deal still allows Australia to claim it is meeting its international obligations, even while its emissions are going up, or flatlining at best. Australia negotiated an increase in emissions  limited to 8% above 1990s levels for the period from 2008 to 2012 at Kyoto – one of only three countries not required to make cuts. That would still be a “considerable challenge”, the cabinet heard, although other wealthy countries had pledged more significant cuts. Europe promised to reduce emissions by 8% and Japan and Canada by 6%.The submission argued Australia’s commitment represented “a cut in our projected business-as-usual growth of 30%”, comparable to the average for industrialised countries. It also noted its disappointment there had been no progress on commitments to cut emissions from developing nations. Australia’s role in Kyoto was controversial, with Hill aggressively pursuing a special deal for Australia at a meeting determined to reach consensus. The land-clearing clause meant that because clearing had declined between 1990 and 1997, emissions from burning fossil fuels could rise substantially while overall emissions could be kept to an 8% rise. The cabinet knew its Kyoto deal was internationally resented. Relations with the EU and Pacific nations were strained, the method for calculating emissions from land clearing had yet to be finalised, and “those who consider that Australia secured too good a deal at Kyoto may use this to try to increase our emission reduction task”. The submission notes that in the lead-up to further climate meetings later that year, “some EU member states have a vested interest in maintaining the EU’s hardline green position because it makes good domestic politics”. That strain continues, as do the repercussions of Australia’s deal under the Kyoto protocol. At December’s Madrid climate change conference, Australia was accused of “cheating” and was named by other countries and observers as one of a few nations that had thwarted the aims of the meeting. Australia planned to use carryover credits, an accounting measure linked to the Kyoto protocol, to meet targets it set at the Paris summit four years ago. Australia signed the Kyoto protocol in April 1998, and the cabinet discussed whether to ratify it, which would have made commitments binding. In the end, it deferred any decision, as ratification was “a completely separate and longer-term question ... important issues remain unresolved”. The Howard government never ratified the protocol, arguing it was not in Australia’s interests. Kevin Rudd’s Labor government, elected in 2007, made ratifying Kyoto one of its first priorities."
nan
"You walk through a park in a city on a warm day, then cross out to a narrow street lined with tall buildings. Suddenly, it feels much hotter. Many people will have experienced this, and climate scientists have a name for it: the urban heat island effect.  Heavily urbanised areas within cities are between 1℃ and 3℃ hotter than other areas. They are contributing to global warming and damaging people’s health, and it’s set to get worse as urbanisation intensifies.  Numerous cities around the world are trying to do something about this problem. But there is a very long way to go. So what is holding us back, and what needs to happen? Urban heat relates to how most cities have been designed. Many rows of tall buildings are organised into blocks which resist any natural breeze. Streets and roofs are clad in dark materials like asphalt and bitumen, which retain more heat than lighter materials and natural surfaces like soil. Natural ground absorbs rain, which is evaporated by the sun’s rays on a warm day and released into the air, cooling everything down. In a city, the rain just runs into the sewer system instead.  Urban areas tend to lack trees. Trees help reduce the air temperature by blocking the sun’s rays, while cutting the levels of pollution by absorbing harmful particles.  Cities are also warmer because they are full of human activity. Everything from transport to industry to energy output makes them hotter than they otherwise would be.   Urban heat has various consequences. Combined with heatwaves and global warming, both of which are also on the rise, these hotspots are producing conditions that kill and hospitalise growing numbers of people. The worst affected are the elderly and other vulnerable groups like the homeless.  The World Health Organisation (WHO) has warned that increased city temperatures lead to more pollutants in the air. These can aggravate respiratory diseases, particularly among children. As cities get bigger, more and more people will be affected by these threats to their health. Higher city temperatures are one reason why we are using more and more air conditioning. One US study found that the urban heat island effect in Florida was responsible for over $400m (£287m) of extra aircon, for example.  Aircon feeds climate change by producing more carbon emissions through the extra electricity demand, creating a vicious circle where it gets hotter because more aircon is required. The increased energy demand means a greater risk of summer blackouts, causing both human discomfort and economic damage.  Hotter city roads and pavements also raise the temperature of storm-water runoff in sewers. This in turn makes rivers and lakes warmer, which can affect fish and other aquatic species in relation to things like feeding and reproduction.  Finally, there are major economic consequences to hotter cities. One paper from last year predicts that all the extra wear and tear caused by the excess heat would amount to between 1% and 10% of lost GDP in thousands of cities around the world.  The solutions to the problem are clear enough: they include using paler more reflective building materials, and wiser urban planning that incentivises more parks, tree planting and other natural open spaces.  When it comes to taking these steps, however, it’s a very mixed picture. Countries and municipal authorities have typically become very good at adopting plans to cut emissions of carbon dioxide and other greenhouse gases. They are not so good at taking steps to adapt to climate change. A study from 2014 found that most European cities had failed to introduce urban heat plans, and the situation looks little better today.  This being the case, city administrations that have gone the extra mile look particularly enlightened – even though they tend to be somewhat sporadic. Melbourne, for instance, has substituted its trademark bluestone pavements in several areas with a permeable version that absorbs rainwater, thereby increasing the amount of evaporation.  New York City’s Cool Roof Initiative has seen thousands of volunteers painting some of the city’s flat bituminous roofs with a reflective polymer material. Lately, Los Angeles has launched an initiative to paint roads white, part of a pledge by city hall to lower the temperature by 3℃ in the next 20 years. Beijing, meanwhile, has been  introducing zoning measures to reduce smog.  Other administrations have been encouraging green roofs – rooftops covered in vegetation: they are a legal requirement for big new developments in Toronto; there are floor area bonuses for developers who include them in Portland, Oregon; and Chicago had a funding scheme for a while. In Swiss cities and regions,  green roofs have been a legal requirement for many buildings for years.  These are all just pockets of activity, however. Many other mayors and city administrations need to start implementing the kinds of bylaws and incentives to adapt to the reality of hotter cities.  The cities of the future can still be green and cool, but only if they move up the agendas of many city halls. The laggards need to follow the example of those that have been leading the way. The reality is that the social, environmental and economic costs of urban heat islands add up to a bill that is too high for humanity to pay."
"You benefit from plastic from the moment you get up and use your toothbrush or kettle. Plastic is embedded in agriculture – and it keeps you alive if you end up in hospital. Even some of our money is made from it. Yet I can’t watch the news without being bombarded by the evils of plastic. As a polymer scientist, it feels like my life’s work is dismissed as immoral by even my hero Sir David Attenborough, simply because I deal with plastics.  But plastic itself is inanimate and cannot be evil – what’s morally wrong is what humans do with it. But some plastic packaging does have benefits – even for the environment. Some packaging, for instance, prevents enough food waste (and therefore deforestation, fertiliser use or vehicle emissions) to balance out the inevitable litter. So how can you tell what is and isn’t worth it? One reason this is so hard to figure out is down to the nature of the material itself. Different kinds of plastic have to be separated for recycling because they contain tiny building blocks that don’t mix at the molecular level. For instance, even many chemists don’t realise that polyethylene (PE) and polypropylene (PP) don’t mix, though they are the two of the most common forms of plastic and both have the same empirical formula of n(CH2). That’s why separating plastics at the recycling centre is so important. A sports drink, for instance, can have three different and incompatible types of plastic in the bottle, the shrink-wrapped film, and the lid. All three components can be individually recycled but they are rarely separated other than by shredding.  Or look at black plastic trays. Their only function is to amplify the colour of a product, yet they also prevent recycling as sorting machines cannot detect black pigment.  In many cases, the packaging does have a genuine function and prevents waste by, for example, sealing in moisture or gas. But this can also mean certain thin films of plastic become impossible or prohibitively expensive to separate. Packaged fruit and vegetables are egregious examples of excess plastic because they already come in a protective skin. Bananas already come in a perfectly designed wrapper – individuals can be snapped off a from a bigger pack, the skin splits length ways to expose the product, and it is truly biodegradable. Prepacked orange segments, meanwhile, last about four days whereas a whole orange can last months. Compare the environmental lifetime of orange peel (months) and polyethylene (effectively eternity) – all for the convenience of not peeling an orange. Such packaging serves little practical purpose, yet only a minority of supermarket fresh fruit and veg is offered “loose”. Consumers are waking up to some of the worst excesses – see the recent furore over an M&S cauliflower steak that was pulled after complaints. But none of this is simple. Given that prepacked fruit and vegatables enable some disabled people to access fresh food, one person’s lazy and profligate is another’s lifesaver. So what can be done to reduce single-use plastic? A society that valued the environment over marketing could make evidence-based choices. On a larger scale, this involves policies such as the UK’s 5p carrier bag charge, which has driven an 80% reduction in single-use bags. But personal actions matter, too. Take the choices involved in a simple packed lunch of a falafel wrap, prepared at home. For the wrap, many advocate reusing aluminium foil rather than clingfilm. But foil has to be reused nearly 200 times to release less greeenhouse gases than clingfilm – 5g of aluminium versus 0.2g of film at six times more embedded energy and nine times more GHG per gram.  Compare this to a reusable plastic sealed bag made from 14g of the same material as the clingfilm. This only needs to be used 70 times to get ahead (on GHG emmisions) of using new clingfilm every time, while there is no daily clingfilm or weekly foil going to landfill. Or consider bottled water. The logical approach here is to reuse thicker bottles 100 times or more, but this may require a deposit scheme, collection and return, wash and refill – all of which costs. Thin single-use bottles are the lowest price, whereas refilling and reusing has the lowest environmental burden. Companies’ balance sheets and our pockets lead us to single-use plastics in the sea. Single-use plastic is a complex issue – in some cases it is very useful, in others just the opposite. But consumers can make conscious choices, businesses can act responsibly and governments can enforce good policy to rid ourselves of pollution for profit."
"On May 7 1813, when Argentina was beginning the process of becoming a sovereign country, the first Argentinian law for the promotion of mining was sanctioned. The day has now become a national day of mining. But mining in Argentina is surrounded by a series of controversies that invite us to question this celebratory commemoration. Most notably, resistance to what is known as “open-pit” or “mega” mining is growing. Open-pit mining is a type of large-scale mining that extracts minerals found in low concentrations from the surface of the earth rather than from tunnelling, generating large craters. This method requires large amounts of explosives and water, and the use of chemicals such as cyanide and sulphuric acid for the separation of metals. From Argentina to Colombia and Mexico, open-pit mining has been at the centre of environmental and human rights conflicts in Latin America for decades now. It is often referred to as the archetype of extractivism due to the magnitude of its environmental and human impact and the alliances between governments and transnational capital that lie behind it.  Mining took place in Argentina throughout the 19th and 20th centuries, but was never one of the country’s principle economic activities. Not until 1993, when under the neoliberal government of Carlos Menem, new mining legislation was introduced. This legislation improved the benefits to transnational companies and laid the ground for the beginning of large-scale open-pit mining for metalliferous minerals such as copper, gold and silver.  In 2017, Mauricio Macri’s government signed a new mining deal, with the objective of attracting even more foreign investment. While the government has claimed the agreement aims to improve environmental regulation, academics, lawyers and activists alike have criticised it for its disregard of current environmental laws. Furthermore, the agreement states that mining companies will now be able to have input on the way mining is taught about in schools, a move seen as an attempt to construct a social license for mining through education. Regional governments of mining provinces continue to argue that mega mining brings jobs, money and investment in infrastructure. But inhabitants of mining regions have told me how the jobs brought by this type of mining are few, and mostly limited to the construction phase of projects. The influx of resources promised by mining companies and provincial governments are also few and far from what is promised. As a teacher from Andalgalá, a town in the province of Catamarca told me, they promise paved roads and new facilities, and end up giving away a few footballs. What mining towns are left with instead is environmental wreckage and health problems. In Andalgalá, two decades of mining have led to draught and polluted water sources. The local paediatric hospital reported a 63% increase of respiratory diseases in children in the first four years that the Bajo la Alumbrera mine was in operation. They stopped publishing statistics after that – and requests for further research and statistics on health problems continue to be brushed aside by the authorities.  Meanwhile in the province of San Juan, the Veladero mine (operated by Barrick Gold) has had multiple spillages of cyanide-contaminated water, one of which was has led to criminal charges and a multi-million fine. While the effects of mega mining are grave and often irreversible, the media have only reported on the harmful consequences of this activity on occasions when it was politically strategic to do so. But communities across the country have come together to fight against mining projects that threaten the environment and their way of life.  In Esquel, in the south of the country, a referendum in 2003 resulted in 81% of opposition to mega mining. In Famatina, in the north-west, the threat of mega mining led to a mass uprising in 2012.  The people of Andalgalá meanwhile, where the Bajo la Alumbrera mine has been in operation for two decades, have stopped the opening of Agua Rica, a mining project three times the size of the former, for eight years now. The local assembly organising against mining, Asamblea del Algarrobo, has pursued a number of routes in their fight, from legal challenges to direct action.  Most prominent in Andalgalá is the range of creative actions that have sprung up. An inter-generational group of local women called Las mujeres del silencio (the women of silence) have staged performative protests outside the headquarters of mining companies. A community radio has been created. And a wealth of murals celebrating the right to life and to water – and denouncing the repression of protest – can be found covering the walls of the town. The fight against mega mining is part of a far wider struggle in Argentina and Latin America against the expansion of an extractive economic model. This approach leads to what sociologist Maristella Svampa and environmental lawyer Enrique Viale call maldesarrollo (bad development). Resistance to such practices is not just about pollution, but also about saving (or rebuilding) the social fabric that is torn apart by extractive activities, and establishing the right to self-determination.  In Andalgalá, I am often told that even though the fight against mining is far from over, the cultural battle has been won. The myths of progress associated with mining have been debunked – and the struggle has generated a creative space for thinking about alternative economic and governing models. At the present time, the government and national and international mining companies are pushing to reverse some of the wins. But as transnational companies and the government attempt to intensify extraction, cultural resistance offers a space for imagining alternatives to false and bad developments."
nan
"
Share this...FacebookTwitter
Graph from Perner et al. (2018) that shows modern-day Arctic sea ice (furthest left navy trend line) is still only slightly lower than during the Little Ice Age (LIA), and much more extensive (more ice) than during the Medieval Climate Anomaly (MCA), Roman Warm Period (RWP), and nearly all of the last 7,000 years.

Song et al., 2018
[A] general warm to cold climate trend from the mid-Holocene to the present, which can be divided into two different stages: a warmer stage between 6842 and 1297 cal yr BP and a colder stage from 1297 cal yr BP to the present.
The general cooling trend may represent a response to decreasing solar insolation; however, the relative dryness or wetness of the climate may have been co-determined by westerlies and the East Asian summer monsoon (EASM). The climate had a teleconnection with the North Atlantic region, resulting from changes in solar activity.




<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Perner et al., 2018
[W]e find evidence of distinct late Holocene millennial-scale phases of enhanced El Niño/La Niña development, which appear synchronous with northern hemispheric climatic variability. 
Phases of dominant El Niño-like states occur parallel to North Atlantic cold phases: the ‘2800 years BP cooling event’, the ‘Dark Ages’ and the ‘Little Ice Age’, whereas the ‘Roman Warm Period’ and the ‘Medieval Climate Anomaly’ parallel periods of a predominant La Niña-like state. 
Our findings provide further evidence of coherent interhemispheric climatic and oceanic conditions during the mid to late Holocene, suggesting ENSO as a potential mediator.



Share this...FacebookTwitter "
"
Share this...FacebookTwitterFear of natural catastrophes among German citizens has dwindled over the past 10 years. Back in 2007, just on the heels of Al Gore’s An Inconvenient Truth – the peak of the global warming scare – natural catastrophes took second place among the ranking of top fears for Germans.
Today ten years later in 2017 natural catastrophes are not even in the top three according to German ARD public television, which cited a study by R+V Insurances:

Chart source: R + V Versicherungen, via ARD German television screenshot.
Ranking in the top three are 1) terrorism, 2) political extremism and 3) tensions concerning the influx of foreigners.
South German SWR public broadcasting here cites the R + V study and writes that this year 56% of those surveyed said they feared “natural catastrophes”, putting that factor in fourth place in the ranking. A variety of other social and economic issues followed closely behind.
The ranking of fears is strongly linked to what issue happens to be dominating the news cycle at the time surveys are conducted. Coverage of climate and natural disasters comes and goes in cycles, and at times disappears for weeks or months from the German media radar.
Recently the Atlantic hurricane season was the top stories in the news, and so a survey done last week would have reflected a greater fear of natural disasters. But once the hurricane season dies out later this fall and the La Nina-induced fall of global temperatures starts to happen, the media of course will go to other bad news to feature.
Made-up news: Ice-free Arctic!


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




This year there has not been any record ice melt, and global temperatures have in fact begun to ease off. There really isn’t much left out there to report. And when facts aren’t there, some even make them up. For example just before its prime time 8 p.m. news, meteorologist Karsten Schwanke of flagship ARD German television announced on 15 September 2017 that the northeast and northwest passages of the Arctic were ice-free, which is a flat out lie:

No ice-free Arctic passages this year, according to the National Snow and Ice data Center (NSIDC). See details here.
Little wonder that most Germans harbour irrational fears of climate change.
Fear a function of media coverage, not observation
German fear of climate and natural disasters is not really related to real world observations made by citizens, but largely depends on media coverage. When media cover it, or make it up, they get afraid. When they don’t cover it, the fear disappears.
Obviously there’s risk involved in the media featuring climate and natural disasters constantly, namely people would simply tune it out. So the German media instead focusses only the major natural disasters, always trying not to overdo it but to keep it at a level that keeps the fear alive.
Keeping fear at high levels is a very tough and challenging job, especially when reports of growing doom don’t match real life observations, or when the reports are organized propaganda.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterDr. Ryan Maue here reports at Twitter that although the Atlantic hurricane season “is going gangbusters“, the Pacific is in fact seeing “one of quietest Typhoon seasons on record“.
Last month in the media, amid the aftermath of Harvey and Irma, the public heard a long stream of hysterical reports claiming that the tropical storms were sure sure signs of man-made climate change.
Yet, according to Dr. Maue, the globe has seen significantly below average cyclone activity, despite the near record hurricane activity observed in the Atlantic this season.


Chart above shows cyclone activity globally being well below normal in a year awash with media hurricane hysteria. Status: October 10, 2017. Source: http://wx.graphics/tropical/

Though the North Atlantic is running at 240% of normal, the entire northern hemisphere is near normal at 98%. Astonishing is the fact that Southern Hemisphere cyclonic activity is near record-breaking low of 47%.
Globally the figure is mere 86%. This is an embarrassment and highly baffling to the media and climate alarmists, who have recently been giving false impressions of “unprecedented” storm activity this year.
“WHERE have all the cyclones gone?”
Even the Australian news site www.news.com.au here asks: “WHERE have all the cyclones gone?
Scientists are puzzled as to how global warming is having the opposite effect on storms from what is often claimed.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Dr. Maue’s following chart shows that the overall hurricane trend has been downward over the past quarter century:

Figure:  Global Hurricane Frequency (all & major) — 12-month running sums. The top time series is the number of global tropical cyclones that reached at least hurricane-force (maximum lifetime wind speed exceeds 64-knots). The bottom time series is the number of global tropical cyclones that reached major hurricane strength (96-knots+). Adapted from Maue (2011) GRL. Source: http://wx.graphics/tropical/.
Record-low Southern Hemisphere
In his last chart chart at the above website, shown is how the southern hemisphere has been trending down to a near record low.
In fact the abstract of a recent peer-reviewed paper appearing at the Geophysical Research Letters, confirms the trends, writing (emphasis added):
In the pentad since 2006, Northern Hemisphere and global tropical cyclone ACE has decreased dramatically to the lowest levels since the late 1970s. Additionally, the frequency of tropical cyclones has reached a historical low. Here evidence is presented demonstrating that considerable variability in tropical cyclone ACE is associated with the evolution of the character of observed large-scale climate mechanisms including the El Nino Southern Oscillation and Pacific Decadal Oscillation.”
Reckless media neglect
This is information and data that alarmist climate scientists like Dr. Michael E. Mann or media such as the AP’s Seth Borenstein apparently recklessly neglected to examine before making hysterical statements to the public.
 
Share this...FacebookTwitter "
"It has become one of the fastest growing political campaigns in human history, surpassing similar battles against the tobacco industry and the fight against apartheid in South Africa. Its logic is simple: the only way to avoid climate change and dangerous levels of greenhouse gases in the atmosphere is for most fossil fuel reserves to stay in the ground.  Campaigners launched the fossil fuel divestment campaign in the early 2010s. Their argument was that you curb consumption of fossil fuels if you stop investing in the companies involved in extracting and burning them. Create a significant enough stigma, they argued, and this issue will shoot up the political agenda.  In the past five years or so, investment funds, public institutions and individuals have duly divested around US$6.15 trillion (£4.6 trillion) of fossil fuel assets. It has helped that the campaign attracted a number of prestigious institutions early on, including 
the British Medical Association, University College London, University of California, the Church of England and the World Council of Churches (representing more than a half billion Christians globally).  The campaign gained further traction after a London-based think tank argued that fossil fuels were in any case a bad investment because the true costs of environmental damage had not been priced in and that at some point there would be a severe correction.  The battle is far from over, however, as demonstrated by the recent decision of the Church of Scotland not to divest. One of the cornerstones of European faith, whose teachings have helped shape everyone from Robert Burns to Rupert Murdoch, its annual general assembly held an impassioned two-hour debate on whether to remove oil and gas stocks from its £443m investment fund.  The Church of Scotland has form in this regard: it had already divested its coal and tar sands investments two years earlier. Ahead of the latest debate, its official general assembly report summarised the issue as follows: It is deeply uncomfortable for the Church, as a caring organisation concerned about climate justice, to continue to invest in something which causes the very harm it seeks to alleviate.  While we have profited from oil and gas exploration in the past, we now understand that financing the future exploration and production will take us away from fulfilling the Paris Agreement and delay the transition to a low carbon economy.  Yet the approximately 1,000 commissioners attending the General Assembly Hall on the city’s Mound, next to Edinburgh Castle, narrowly disagreed: 47% in favour and 53% against. Coming from a nation which already gets most of its electricity from renewable sources, and whose government has indicated the end is in sight for fossil fuel vehicles on the roads, it was undeniably a disappointment.  Representatives were persuaded that it was better to stay invested and seek to influence better behaviour than to pull out altogether. Reverend Jenny Adams, who had brought the motion in the first place, argued that all the evidence suggests oil and gas companies have little intention of changing quickly enough to satisfy the Paris agreement. She said:  There is a need for climate emissions to peak by 2020 and if we just keep talking, too much time passes and change is not coming fast enough. She is surely right about this. There may be traditional wisdom in engaging with fellow shareholders and board members on matters pertaining to large companies, but the church’s decision looks naïve in relation to this sector.  To give just one example, consider that approximately 94% of shareholders of the oil giant Royal Dutch Shell voted last year and again this year to reject emission targets that would comply with the Paris climate accord, as it was deemed “not in the best interest of the company”. How do you persuade a bloc like that to change its mind? While the Church of Scotland’s decision to sidestep divestment may have been a setback to the movement, there have been recent successes, too. The Church of Ireland committed to divest its fossil fuel assets earlier in May, while an international coalition of Catholic institutions, including the Scottish Catholic International Aid Fund, pledged in April to divest investments totalling £6.6 billion.  Municipal administrations including New York and Paris are also divesting from fossil fuels and shifting their investments towards renewable energy sources – evidence that the global divestment is making an impact on public policy.  This certainly seems prudent, as newly published research suggests that the “carbon bubble” could “burst” in the next two decades as demand for fossil fuel energy falls despite population increases and burgeoning global economic growth.  The study projects that the global fossil energy demand will drop by as much as 40% by 2050. If that comes to fruition, it would mean containing global warming levels to 1.5 °C, which is the aspirational goal of the Paris climate accord.  That would be great news for environmentalists, most especially for those living on the front lines of climate change such as in the Pacific, less so for investors in fossil fuel businesses – Presbyterian or otherwise. It’s a strong signal that this global divestment movement may still be a long way from its peak."
"
Share this...FacebookTwitter
Click image to watch.

It is the duty of every Catholic to persecute heretics.”

– Pope Gregory IX, 1170-1241, organizer of the Inquisition

They that approve a private opinion, call it opinion; but they that mislike it, heresy: and yet heresy signifies no more than private opinion.“

– Thomas Hobbes (1588-1679), Leviathan
Now fast forward almost 800 years since Pope Gregory IX’s proclamation on heretics. Guided by global warming dogmatists such as Hans Joachim Schellnhuber of the ultra-alarmist Potsdam Institute for Climate Impact Research, Pope Francis went on to issue a Papal encyclical on the environment and human ecology: Laudato Si, thus making environmentalism a part of Catholic dogma – one that makes challenging climate science heresy.
Now in the wake of Hurricane Irma, Pope Francis has sharply criticized man-made climate change skeptics, implying they are “stupid”. He said on Monday during an in-flight press conference:
Those who deny it [climate change] should go to the scientists and ask them. They speak very clearly. I am reminded of a phrase from the Old Testament, I think from the Psalm: ‘Man is stupid, he is stubborn and he does not see.'”
So says His Holiness, whose Church obstinately took 390 years to apologize for the persecution of heretic Galileo. The bad weather was in fact brewed by skeptics and bad people, the pope insists.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Today a number of (Democrat) politicians and overzealous alarmists in USA are hysterically calling for the criminalization of climate science skepticism and skeptic opinion. For those having read some religious history, this may sound very familiar and remind them of the ugly Inquisition days of the Catholic Church – time when “heretics” were rounded up, brutally tortured and murdered by Church higher-ups — all over Europe, 600 years long.
Papal pact with population control advocate Paul Ehrlich
For many of the faithful, something is terribly amiss with this Pope. Recently he even suggested that Donald Trump was not really a pro-lifer, thus seemingly turns a blind eye to the US Democratic Party’s staunch support for Planned Parenthood abortion factories and a US health care law that forces faithful Catholics to fund abortions against their will.
Earlier this year “population control activist Paul Ehrlich spoke at a Vatican conference on how to save the natural world” today despite an outcry by members of the Catholic faithful“, writes LifeSiteNews.com here. Pope Francis appears to be opening the door to population control.
This is all getting monstrous, hysterical and irrational, and so it’s time for global warming skeptics and the faithful to take a firm and resolved stand against this rogue Pope.
We must never cave in to the Pope’s destructive Inquisition-like dogma and demonization of free thought. We have to remain steadfast, and possibly so for decades or even centuries. Those who want us to forget the past, are those who long for its repeat. Inline with his “stupid and stubborn” statement, Pope Francis seems to harbor a detestation for the human race.
I’m done listening to this Pope.
 
Share this...FacebookTwitter "
nan
"The year 2019 was the hottest on record for Australia with the temperature reaching 1.52C above the long-term average, data from the Bureau of Meteorology confirms. The year that delivered crippling drought, heatwaves, temperature records and devastating bushfires was 0.19C hotter than 2013, the previous record holder. Climate scientists told Guardian Australia that climate change pushed what would have been a hot year into record territory, driving heat extremes and the risk of deadly bushfires. The Bureau of Meteorology data shows the average temperature across the country was 1.52C above the long-term average taken between 1961 and 1990. The second hottest year was 2013, followed by 2005, 2018 and 2017. The data, from the bureau’s long-term ACORN-SAT data, will be used as part of the bureau’s annual climate statement due for release on 9 January. Prof Mark Howden, the director of the ANU Climate Change Institute, said the continued rising levels of greenhouse gases in the atmosphere, caused mainly by burning fossil fuels, was the underlying driver of the heat. He said: “It’s very clear that greenhouse gas emissions are changing the radiation balance of the Earth. Other contributors are minor in comparison.” He said two other climate systems had also played a role in delivering the record hot year. The Indian Ocean Dipole system had drawn moisture away from the centre of the continent, causing extra heat to build there. Another system known as the Southern Annular Mode had also contributed to the heat. The data also shows that 2019 was the hottest year on record for New South Wales, with temperatures 1.95C above the long-term average, beating the previous record year, 2018, by 0.27C. Western Australia also had its hottest year, with temperatures 1.67C above average, beating the previous 2013 heat record by 0.58C. The Northern Territory and South Australia both had their second hottest years, with 2019 coming in fifth hottest for Victoria and sixth hottest for Queensland, according to the data. Tasmania had a relatively cool year, but was still 0.41C above the long-term average. The previous summer of 2018-19 was the hottest on record. The spring of 2019 also delivered the worst bushfire weather since at least 1950, when the Forest Fire Danger Index data began. On Wednesday 18 December, Australia experienced its hottest day on record with an average maximum temperature of 41.9C (107.4F), beating the previous record by 1C that had been set only 24 hours earlier. Dr Sarah Perkins-Kirkpatrick, a climate scientist at the University of New South Wales specialising in extreme events, said 2019 had started hot, with the previous summer being the hottest on record. She said: “The extremes have been seen in lots of heatwaves and, of course, the bushfires, that are a consequence of the very hot and dry conditions.” She said while natural climate cycles had pushed temperatures higher, “climate change has given them a boost”. “2019 would not have been pleasant anyway, but climate change has made it worse. We are focusing now on the bushfires, but the underlying heat has been driving these conditions for much of the year. “Climate change isn’t the outright cause, but it’s an undeniable contributor to this extreme year on all accounts.” A bureau spokesperson said it would provide official comment on the 2019 temperatures in its annual climate statement on January 9 that would include a “comprehensive analysis of the year’s weather events and climate context, including any records of note”."
"
Share this...FacebookTwitterVolatile winds, burning generators, failing blades and buckling towers: these are just some of the technical problems plaguing wind power and thus making it a highly undependable and unreliable source of electricity.
Now we hear from NDR German public broadcasting reported of yet another problem plaguing the North Sea Riffgat offshore wind park located off the coast of the island of Borkum: an exposed underwater power transmission line.
Apparently the huge underwater cable delivering the green power from Riffgat to the mainland had been embedded below the seabed, but for some reason last April it somehow worked itself up above the seabed and is now exposed – vulnerable to North Sea maritime traffic.
Today a ship and a crew remain standing guard at the sea surface above the exposed cable in what the NDR calls “probably the most boring job on the planet”.
Riffgat is 15 kilometers northwest of Borkum and just north of the bustling shipping channel in the southern North Sea. Its 30 wind turbines are built over an area of 6 square kilometers and have a total capacity of 113 megawatts.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




It’s not the first time that Riffgat has seen big problems. Between November 2015 and April 2016, transmission troubles kept Riffgat from exporting power.
According to NDR, the Dutch “Faxaborg” patrol ship manned by a crew of four has been floating at the site since April in order to “warn ships of an unusually dangerous area of hazard”.
NDR writes that some 200 meters of the transmission cable became exposed above the seabed one year ago, a condition that has been confirmed by grid-operating company Tennet.
According to NDR, the 50 kilometer long cable was laid with great effort 3 meters below the seabed and that the 200-meter section was washed away by the turbulent North Sea. Tennet says the exposed cable poses no hazard, but the Faxaborg ship was dispatched to stand guard as “a precautionary measure” to make sure “no fishing nets or anchors get caught with the cable”. The cable is only 8 meters below the sea surface at the location.
Guarding the cable is expected to continue indefinitely, at least until summer when the cable can be buried again. But a real solution remains elusive, NDR writes, adding:
Finding the right solution is no easy task. Just how much the entire affair will cost – indirectly to the consumers – was not stated by the Tennet spokesperson. That’s a company secret.”
Share this...FacebookTwitter "
nan
"
Share this...FacebookTwitter
Not climate change: forest fires in the USA controlled by El Nino, arson and land use changes
By Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
(German text translated/edited by P Gosselin)
Droughts increase the risk of forest fires; that’s logical. However it is false to reflexively assign every forest fire to climate change. There have always been droughts and forest fires. Anyone wishing to shift the blame over to climate change first has to show that the trend has already deviated from the range of natural variability. For many, that is simply too much work.
Thus they prefer to claim something and hope that nobody will bother to fact check the claim. They don’t like climate skeptics because they have the silly habit of carefully examining the facts. They prefer the silent, non-questioning audience who immediately say yes and amen in response to all alarmist claims.
And when the facts indeed do contradict their alarmist claims, they get personal. They attack the occupation of the skeptic, or education, or skin color, or, or, or.
Nowadays we can find a load of facts in the Internet. Example: forest fires in the USA. The size of the areas ravaged by forest fires is provided by a table from the National Interagency Fire Centers. Strangely the data are not offered in graphical form. You are forced to make your own, which is no problem. Most people however simply are left in the dark. Steven Goddard (Tony Heller) shows such a charts at his Real Science blog.


2004 – 2014 burn acreage trend is falling. Chart source: Tony Heller.
One cannot always just pull climate change at of his magic hat every time a forest fire appears. The University of Colorado at Boulder recently calculated that 84% of all forest and bush fires in den USA are caused by humans. Read the press release from February 2017:
Humans have dramatically increased extent, duration of wildfire season
Humans have dramatically increased the spatial and seasonal extent of wildfires across the U.S. in recent decades and ignited more than 840,000 blazes in the spring, fall and winter seasons over a 21-year period, according to new University of Colorado Boulder-led research. After analyzing two decades’ worth of U.S. government agency wildfire records spanning 1992-2012, the researchers found that human-ignited wildfires accounted for 84 percent of all wildfires, tripling the length of the average fire season and accounting for nearly half of the total acreage burned. The findings were published today in the journal Proceedings of the National Academy of Sciences.
“There cannot be a fire without a spark,” said Jennifer Balch, Director of CU Boulder’s Earth Lab and an assistant professor in the Department of Geography and lead author of the new study. “Our results highlight the importance of considering where the ignitions that start wildfires come from, instead of focusing only on the fuel that carries fire or the weather that helps it spread. Thanks to people, the wildfire season is almost year-round.”  The U.S. has experienced some of its largest wildfires on record over the past decade, especially in the western half of the country. The duration and intensity of future wildfire seasons is a point of national concern given the potentially severe impact on agriculture, ecosystems, recreation and other economic sectors, as well as the high cost of extinguishing blazes. The annual cost of fighting wildfires in the U.S. has exceeded $2 billion in recent years.
The CU Boulder researchers used the U.S. Forest Service Fire Program Analysis-Fire Occurrence Database to study records of all wildfires that required a response from a state or federal agency between 1992 and 2012, omitting intentionally set prescribed burns and managed agricultural fires. Human-ignited wildfires accounted for 84 percent of 1.5 million total wildfires studied, with lightning-ignited fires accounting for the rest. In Colorado, 30 percent of wildfires from 1992-2012 were started by people, burning over 1.2 million acres. The fire season length for human-started fires was 50 days longer than the lightning-started fire season (93 days compared to 43 days), a twofold increase. “These findings do not discount the ongoing role of climate change, but instead suggest we should be most concerned about where it overlaps with human impact,” said Balch. “Climate change is making our fields, forests and grasslands drier and hotter for longer periods, creating a greater window of opportunity for human-related ignitions to start wildfires.”
While lightning-driven fires tend to be heavily concentrated in the summer months, human-ignited fires were found to be more evenly distributed across all seasons. Overall, humans added an average of 40,000 wildfires during the spring, fall and winter seasons annually—over 35 times the number of lightning-started fires in those seasons. “We saw significant increases in the numbers of large, human-started fires over time, especially in the spring,” said Bethany Bradley, an associate professor at University of Massachusetts Amherst and co-lead author of the research. “I think that’s interesting, and scary, because it suggests that as spring seasons get warmer and earlier due to climate change, human ignitions are putting us at increasing risk of some of the largest, most damaging wildfires.” “Not all fire is bad, but humans are intentionally and unintentionally adding ignitions to the landscape in areas and seasons when natural ignitions are sparse,” said John Abatzoglou, an associate professor of geography at the University of Idaho and a co-author of the paper. “We can’t easily control how dry fuels get, or lightning, but we do have some control over human started ignitions.”
The most common day for human-started fire by far, however, was July 4, with 7,762 total wildfires started on that day over the course of the 21-year period. The new findings have wide-ranging implications for fire management policy and suggest that human behavior can have dramatic impact on wildfire totals, for good or for ill. “The hopeful news here is that we could, in theory, reduce human-started wildfires in the medium term,” said Balch. “But at the same time, we also need to focus on living more sustainably with fire by shifting the human contribution to ignitions to more controlled, well-managed burns.” Co-authors of the new research include Emily Fusco of the University of Massachusetts Amherst and Adam Mahood and Chelsea Nagy of CU Boulder. The research was funded by the NASA Terrestrial Ecology Program, the Joint Fire Sciences Program and Earth Lab through CU Boulder’s Grand Challenge Initiative.”
In July 2017 the Institute for Basic Science explained that the risk of forest fires on the US Southwest was strongly dependent on the temperature differences between the Pacific and Atlantic Oceans. Ultimately the ocean cycles are the real drivers. Press release (via Science Daily):

Atlantic/Pacific ocean temperature difference fuels US wildfires
New study shows that difference in water temperature between the Pacific and the Atlantic oceans together with global warming impact the risk of drought and wildfire in southwestern North America
An international team of climate researchers from the US, South Korea and the UK has developed a new wildfire and drought prediction model for southwestern North America. Extending far beyond the current seasonal forecast, this study published in the journal Scientific Reports could benefit the economies with a variety of applications in agriculture, water management and forestry.

Over the past 15 years, California and neighboring regions have experienced heightened drought conditions and an increase in wildfire numbers with considerable impacts on human livelihoods, agriculture, and terrestrial ecosystems. This new research shows that in addition to a discernible contribution from natural forcings and human-induced global warming, the large-scale difference between Atlantic and Pacific ocean temperatures plays a fundamental role in causing droughts, and enhancing wildfire risks.
‘Our results document that a combination of processes is at work. Through an ensemble modeling approach, we were able to show that without anthropogenic effects, the droughts in the southwestern United States would have been less severe,’ says co-author Axel Timmermann, Director of the newly founded IBS Center for Climate Physics, within the Institute for Basics Science (IBS), and Distinguished Professor at Pusan National University in South Korea. ‘By prescribing the effects of human-made climate change and observed global ocean temperatures, our model can reproduce the observed shifts in weather patterns and wildfire occurrences.’
The new findings show that a warm Atlantic and a relatively cold Pacific enhance the risk for drought and wildfire in the southwestern US. ‘According to our study, the Atlantic/Pacific temperature difference shows pronounced variations on timescales of more than 5 years. Like swings of a very slow pendulum, this implies that there is predictability in the large-scale atmosphere/ocean system, which we expect will have a substantial societal benefit,’ explains Yoshimitsu Chikamoto, lead author of the study and Assistant Professor at the University of Utah in Logan.
The new drought and wildfire predictability system developed by the authors expands beyond the typical timescale of seasonal climate forecast models, used for instance in El Niño predictions. It was tested with a 10-23 month forecasting time for wildfire and 10-45 for drought. ‘Of course, we cannot predict individual rainstorms in California and their local impacts months or seasons ahead, but we can use our climate computer model to determine whether on average the next year will have drier or wetter soils or more or less wildfires. Our yearly forecasts are far better than chance,’ states Lowell Stott, co-author of the study from the University of Southern California in Los Angeles.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Bringing together observed and simulated measurements on ocean temperatures, atmospheric pressure, water soil and wildfire occurrences, the researchers have a powerful tool in their hands, which they are willing to test in other regions of the world: ‘Using the same climate model configuration, we will also study the soil water and fire risk predictability in other parts of our world, such as the Mediterranean, Australia or parts of Asia,’ concludes Timmermann. ‘Our team is looking forward to developing new applications with stakeholder groups that can benefit from better soil water forecasts or assessments in future fire risk.’
Paper: Yoshimitsu Chikamoto, Axel Timmermann, Matthew J. Widlansky, Magdalena A. Balmaseda, Lowell Stott. Multi-year predictability of climate, drought, and wildfire in southwestern North America. Scientific Reports, 2017; 7 (1) DOI: 10.1038/s41598-017-06869-7″



Ocean cycles (El Nino, La Nina) were also identified by Mason et al. 2017 as the forest fire drivers in the USA:
Effects of climate oscillations on wildland fire potential in the continental United States
The effects of climate oscillations on spatial and temporal variations in wildland fire potential in the continental U.S. are examined from 1979 to 2015 using cyclostationary empirical orthogonal functions (CSEOFs). The CSEOF analysis isolates effects associated with the modulated annual cycle and the El Niño–Southern Oscillation (ENSO). The results show that, in early summer, wildland fire potential is reduced in the southwest during El Niño but is increased in the northwest, with opposite trends for La Niña. In late summer, El Niño is associated with increased wildland fire potential in the southwest. Relative to the mean, the largest impacts of ENSO are observed in the northwest and southeast. Climate impacts on fire potential due to ENSO are found to be most closely associated with variations in relative humidity. The connections established here between fire potential and climate oscillations could result in improved wildland fire risk assessment and resource allocation.”
El Nino also plays a large role in the US Northwest for controlling driving forst fires, according to Barbero et al. 2015:
Seasonal reversal of the influence of El Niño–Southern Oscillation on very large wildfire occurrence in the interior northwestern United States
Satellite-mapped fire perimeters and the multivariate El Niño–Southern Oscillation index were used to examine the impact of concurrent El Niño–Southern Oscillation (ENSO) phase on very large fire (VLF) occurrences over the intermountain northwestern United States (U.S.) from 1984 to 2012. While the warm phase of ENSO promotes drier and warmer than normal conditions across the region during winter and spring that favor widespread fire activity the following summer, a reduction in VLFs was found during the warm phase of ENSO during summer concurrent with the fire season. This paradox is primarily tied to an anomalous upper level trough over the western U.S. and positive anomalies in integrated water vapor that extend over the northwestern U.S. during summers when the warm phase of ENSO is present. Collectively, these features result in widespread increases in precipitation amount during the summer and a curtailment of periods of critically low-fuel moistures that can carry wildfire.”
Overall forest fires in the USA have decreased significantly compared to the previous century (see article by Larry Kummer at Fabius Maximus).
In Colorado no forest fire trend could be found over the past centuries, see the press release from the University of Colorado issued in 2014:
Colorado’s Front Range fire severity today not much different than in past, says CU-Boulder study
The perception that Colorado’s Front Range wildfires are becoming increasingly severe does not hold much water scientifically, according to a massive new study led by the University of Colorado Boulder and Humboldt State University in Arcata, Calif. The study authors, who looked at 1.3 million acres of ponderosa pine and mixed conifer forest from Teller County west of Colorado Springs through Larimer County west and north of Fort Collins, reconstructed the timing and severity of past fires using fire-scarred trees and tree-ring data going back to the 1600s. Only 16 percent of the study area showed a shift from historically low-severity fires to severe, potential crown fires that can jump from treetop to treetop.
The idea that modern fires are larger and more severe as a result of fire suppression that allowed forest fuels to build up in the past century is still prevalent among some, said CU-Boulder geography Professor Thomas Veblen, a study co-author. ‘The key point here is that modern fires in these Front Range forests are not radically different from the fire severity of the region prior to any effects of fire suppression,’ he said. A paper on the subject was published Sept. 24 in the journal PLOS ONE. The study was led by Associate Professor Rosemary Sherriff of Humboldt State University and involved Research Scientist Tania Schoennagel of CU-Boulder’s Institute of Arctic and Alpine Research, CU-Boulder doctoral student Meredith Gartner and Associate Professor Rutherford Platt of Gettysburg College in Gettysburg, Pa. The study was funded by the National Science Foundation.
‘The common assumption is that fires are now more severe and are killing higher percentages of trees,’ said Sherriff, who completed her doctorate at CU-Boulder under Veblen in 2004. ‘Our results show that this is not the case on the Front Range except for the lowest elevation forests and woodlands.’ One important new finding comes from a comparison of nine large fires that have occurred on the Front Range since 2000 — including the 2002 Hayman Fire southwest of Denver, the 2010 Fourmile Canyon Fire west of Boulder and the 2012 High Park Fire west of Fort Collins — with historic fire effects in the region. ‘It’s true that the Colorado Front Range has experienced a number of large fires recently,’ said Schoennagel. ‘While more area has burned recently compared to prior decades – with more homes coming into the line of fire – the severity of recent fires is not unprecedented when we look at fire records going back before the 1900s.’
In addition, tree-ring evidence from the new study shows there were several years on the Front Range since the 1650s when there were very large, severe fires. The authors looked at more than 1,200 fire-scarred tree samples and nearly 8,000 samples of tree ages at 232 forest sample sites from Teller County to Larimer County. The study is one of the largest of its kind ever undertaken in the western United States. The team was especially interested in fire records before about 1920, when effective fire suppression in the West began in earnest. ‘In relatively dry ponderosa pine forests of the West, a common assumption is that fires were relatively frequent and of low severity, and not lethal to most large trees, prior to fuel build-up in the 20th century,’ said Veblen. ‘But our study results showed that about 70 percent of the forest study area experienced a combination of moderate and high-severity fires in which large percentages of the mature trees were killed.’
Along the Front Range, especially at higher elevations, homeowners and fire managers should expect a number of high-severity fires unrelated to any kind of fire suppression and fuel build-up, said Schoennagel. ‘This matters because high-severity fires are dangerous to people, kill more trees and are trickier and more expensive to suppress.” “Severe fires are not new to most forests in this region,’ said Sherriff. ‘What is new is the expanded wildland-urban interface hazard to people and property and the high cost of suppressing fires for society.’ In addition, a warming Colorado climate — 2 degrees Fahrenheit since 1977 — has become a wild card regarding future Front Range fires, according to the team. While fires are dependent on ignition sources and can be dramatically influenced by high winds, the team expects to see a substantial increase in Front Range fire activity in the low and mid-elevations in the coming years as temperatures continue to warm, a result of rising greenhouses gases in Earth’s atmosphere.”
2016 was a bad year of forest fires in California. Al Gore immediately pointed the finger at climate change. But later it was discovered that a series of arsons was behind most of the fires. The house of climate alarm quickly collapsed. Also the University of Arizona found that the fires were promoted by poor land use practices. Press release:

Forest Fires in Sierra Nevada Driven by Past Land Use
Changes in human uses of the land have had a large impact on fire activity in California’s Sierra Nevada since 1600, according to research by a UA researcher and her colleagues.
Forest fire activity in California’s Sierra Nevada since 1600 has been influenced more by how humans used the land than by climate, according to new research led by University of Arizona and Penn State scientists. For the years 1600 to 2015, the team found four periods, each lasting at least 55 years, where the frequency and extent of forest fires clearly differed from the time period before or after. However, the shifts from one fire regime to another did not correspond to changes in temperature or moisture or other climate patterns until temperatures started rising in the 1980s. ‘We were expecting to find climatic drivers,’ said lead co-author Valerie Trouet, a UA associate professor of dendrochronology. ‘We didn’t find them.’
Instead, the team found the fire regimes corresponded to different types of human occupation and use of the land: the pre-settlement period to the Spanish colonial period; the colonial period to the California Gold Rush; the Gold Rush to the Smokey Bear/fire suppression period; and the Smokey Bear/fire suppression era to present. ‘The fire regime shifts we see are linked to the land-use changes that took place at the same time,’ Trouet said. ‘We knew about the Smokey Bear effect — there had been a dramatic shift in the fire regime all over the Western U.S. with fire suppression. We didn’t know about these other earlier regimes,’ she said. ‘It turns out humans — through land-use change — have been influencing and modulating fire for much longer than we anticipated.’
Finding that fire activity and human land use are closely linked means people can affect the severity and frequency of future forest fires through managing the fuel buildup and other land management practices — even in the face of rising temperatures from climate change, she said. The team’s paper, ‘Socio-Ecological Transitions Trigger Fire Regime Shifts and Modulate Fire-Climate Interactions in the Sierra Nevada, USA 1600-2015 CE,’ was scheduled for publication in the online Early Edition of the Proceedings of the National Academy of Sciences this week. Trouet’s co-authors are Alan H. Taylor of Penn State, Carl N. Skinner of the U.S. Forest Service in Redding, California, and Scott L. Stephens of the University of California, Berkeley.
Initially, the researchers set out to find which climate cycles, such as the El Niño/La Niña cycle or the longer Pacific Decadal Oscillation, governed the fire regime in California’s Sierra Nevada. The team combined the fire history recorded in tree rings from 29 sites all along the Sierra Nevada with a 20th-century record of annual area burned. The history spanned the years 1600 to 2015. However, when large shifts in the fire history were compared to past environmental records of temperature and moisture, the patterns didn’t match. Other researchers already had shown that in the Sierra, there was a relationship between forest fire activity and the amount of fuel buildup. Team members wondered whether human activity over the 415-year period had changed the amount of fuel available for fires.
By using a technique called regime shift analysis, the team found four distinct time periods that differed in forest fire activity. The first was 1600 to 1775. After 1775, fire activity doubled. Fire activity dropped to pre-1775 levels starting in 1866. Starting in 1905, fire activity was less frequent than any previous time period. In 1987, fire activity started increasing again. However, the frequency of forest fires did not closely track climatic conditions, particularly after 1860. The researchers reviewed historical documents and other evidence and found the shifting patterns of fire activity most closely followed big changes in human activity in the region. Before the Spanish colonization of California, Native Americans regularly set small forest fires. The result was a mosaic of burned and unburned patches, which reduced the amount of fuel available to fires and limited the spread of any particular fire.
However, once the Spanish arrived in 1769, Native American populations rapidly declined because of disease and other causes. In addition, the Spanish government banned the use of fire. Without regular fires, fuels built up, leading to more and larger fires. The influx of people to California during the Gold Rush that began in 1848 reduced fire activity. The large numbers of livestock brought by the immigrants grazed on the grasses and other plants that would otherwise have been fuel for forest fires. In 1904, the U.S. government established a fire suppression policy on federal lands. After that, fire activity dropped to its lowest level since 1600. Starting in the 1980s, as the climate warms, fire frequency and severity has increased again. Fires now can be ‘bad’ fires because of a century or more of fire suppression, according to lead co-author Taylor, a professor of geography at Penn State. ‘It is important for people to understand that fires in the past were not necessarily the same as they are today,’ Taylor said. ‘They were mostly surface fires. Today we see more canopy-killing fires.’”

Share this...FacebookTwitter "
"Finding oneself improperly dressed for the weather can have fatal consequences – just ask a white-coated weasel. Animals that live in areas that usually have a lot of snow in the winter often change their coats to match their surroundings. Some weasels have evolved so that in the autumn they moult their brown summer coat and change to a white version. In spring they reverse the process and return to the brown version. A new study published in Nature Scientific Reports suggests that there is a strong relationship between the quantity and duration of snow in a forest in Poland during the winter and the number of weasels wearing white the following winter. Clearly this is not a fashion decision on the part of the local weasel population.  The scientists behind the study looked at two subspecies of the rather charmingly named “least weasel”, Mustela nivalis nivalis, which does change its colour in the winter, and Mustela nivalis vulgaris, which does not. Both species live in Białowieża, an ancient relic of the vast primeval forest that once covered most of Europe, and both compete for similar resources. Yet the authors determined that when there is no snow cover the white-coated weasels were more likely to be eaten by the foxes, wolves and buzzards that see them as prey.  When there is deep snow, weasels of both colours tend to hunt underneath it, which, after all, is where the small rodents are, so the predation rates for both species are reduced. In periods of winter without snow, however, white weasels are much more conspicuous than brown ones. In recent years, because of climate change, there have been more days without snow, and the date when snow disappears in the spring has become progressively earlier. The conclusion of the study is that the proportion of white weasels in the population is influenced by predation rates due to lack of camouflage during the previous winter. The question for the future seems to be how long M. n. nivalis as a subspecies can survive, given the milder, largely snow-free winters that are becoming more frequent as climate change takes hold. One of the important factors in this is likely to be its ability to change the time when it moults its coat, both in the autumn and the spring. Though the weasels are already showing signs of moulting into their brown summer coats earlier in the spring, they have not yet changed the time of their autumn moult. This is thought to be because autumn temperatures have remained relatively stable while springs have been getting warmer. Another problem for the weasel is that, even when it can shed its winter coat earlier, that will not be helpful in the increasing periods during the winter when there is no snow. Since moulting and growing a new coat is a serious business, chopping and changing during the season is not possible. In addition to ambient temperature, moulting and therefore coat colour change is triggered by day length, the change in which affects the weasel’s hormonal state. So starting a new moult is only possible during spring and autumn when the days reach the appropriate length as well as the right temperature. The issue of having the wrong coat for the weather is not confined to weasels. At least 22 species change their coat colour in winter including snowshoe hares, mountain hares and Arctic foxes. The various hare species face similar problems to weasels and arctic foxes, in so far as they are more vulnerable to predation when they are not camouflaged. However, as plant eaters, they don’t have to worry about also being more visible to their prey. For some animals, such as the Arctic fox, the changes in the timing of seasonal events, known as phenological changes, are not the only problems brought on by climate change. Because of the warming climate and the spread of their prey species northwards, red foxes, which are larger, bolder and more robust than their Arctic cousins, have been able to extend their range northwards, too, and are now threatening to out-compete Arctic foxes.  The situation with the weasels is not quite the same, as it is greater visibility to predators that is threatening the white colour morph, rather than direct physical competition with another species. However, unpublished data from the authors of the study suggest that M. n. vulgaris (the one that stays brown) has a higher resting metabolic rate that M. n. nivalis, which means they will require more calories just to stay alive. This may just give the white form of the weasel the edge in the winter when calories are scarce, which may help to redress the balance.  If not then the potential loss of this beautiful and unique subspecies of the least weasel because of anthropogenic climate change will be one more crime to be laid at the door of humanity."
"
Share this...FacebookTwitterThe Swiss online SRF public television site here reports that German power engineering giant Siemens plans to eliminate some 6900 employees, half of them in Germany. Hit will be the conventional power plant and electric drive systems branch.
German energy sector in turmoil
The SRF writes that the power plant branch “is suffering due to the Energiewende“, Germany’s attempted transition to renewable energies. This branch alone will see 6100 job reductions. Turbine plants in Görlitz, Leipzig, Offenbach, Erfurt, Erlangen, Berlin and Mülheim (Ruhr) will be impacted. The announcement just before the start of the Christmas holiday season has angered trade unionists.
The Handelsblatt here reports that some of the impacted engineering workers are “in shock and in tears” over the news. Protests and strikes have been announced.
The news of the jobs cutbacks are the latest in a series of huge jobs reductions seen throughout the German energy sector. Over the past years, power giants such as RWE and Eon have announced the layoffs of thousands of its workers as the German Energiewende has eroded profits in the conventional energy sector and has led to skyrocketing electricity prices for consumers.
The misery has not only hit conventional energy jobs, but also renewable energy sector as well. Due to cheap imports from China and an uncertain investment future in the wind sector, most of Germany’s solar power equipment producers have become insolvent and thousands of jobs have been lost. Recently wind energy equipment producers such as Nordex have announced job cuts as well and the future for wind energy in Germany looks bleak.
As more volatile wind and solar energy capacity have come online, steam and gas turbines have become uneconomical to operate and investments in conventional power plants have taken a hit over the recent years.
The BBC has reported on the jobs cutback by Siemens, but made no made no mention of the Energiewende.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterIt’s safe to say that the only people who still believe the ultra-alarmist scenarios of the Potsdam Institute for Climate Impact Research are the leftist media and green activists. Even the government funders of this institute know they aren’t really true. After all Germany hasn’t cut CO2 emissions in close to 10 years.
Dr. Sebastian Lüning at Die Kalte Sonne exposes the latest dubious attempt by Potsdam scientist, Stefan Rahmstorf, to spread climate fear and to attack on journalist Daniel Wetzel of flagship daily Die Welt, who not long ago dared to question the science.
The method of attack used by Rahmstorf is every time the same:

Smear the dissenting journalist as a con-man.
Insist the science has long been settled (it isn’t).
Float out charts that use statistical trickery to mislead.

===================================
Again and again: Stefan Rahmstorf and his solar trick
By Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
(Translated/edited by P Gosselin)
The Sherlock Holmes of climate sciences, Stefan Rahmstorf, at his climate blog post “Klimawandel XY Ungelöst” at Klimalounge, warned on 29 July 2017 – of climate con-men, fraudsters and hustlers:
The global CO2 increase: the facts and the tricks of con-men
The facts surrounding CO2 rise are clear, unequivocal and agreed on – yet Die Welt again and again gladly recycles old, worn-out climate skeptic myths. Are forests to blame for the CO2 rise?”
Here Stefan Rahmstorf’s rails against an article by Daniel Wetzel “Kurzschluss bei der Energiewende” [The Energiewende shorts out] in Die Welt, where Wetzel dared to question Rahmstorf’s favorite project. The main focus was man’s share of the total CO2 budget, which is a rather dry issue in itself. Also the article looked at the magnitude and its signficance. Depending on its toxicity, even small amounts can have an impact. The same old stuff.
But looking at his Figure 5, Rahmstorf’s seriousness really needs to be called into question. It involves his favorite chart which he regularly presents. Here it is (Fig. 2):


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->





Figure 2.  Chart from Rahmstorf’s Blog posting “The global CO2 rise: the facts and the tricks of the con-men” dated 29 July, 2017.
Rahmstorf*’s text concerning the chart follows:
Curves showing global temperature, CO2-concentration and solar activity. Temperature and CO2 are scaled so that they correspond to the expected CO2-effect on climate (e.g. the best estimation of climate sensitivity). The amplitude of the solar curve is scaled in such a way as to correspond to the observed correlation between solar data and temperature data. (Details are explained here). You can generate this chart here and copy a code there that allows you to install the chart as a widget on your own website (like at my home page) – where here every year it is updated with the latest data. Thanks to Bernd Herd, who programmed it).
First remark: Contrary to Rahmstorf’s claim, there is no “best estimate of climate senstitivity“. The 5th IPCC report intentionally left this value open as no agreement among the report’s authors could be reached. Instead a very broad range of 1.5°C to 4.5°C for a doubling of CO2 was given, which ranges from manageable to catastrophic.
Second remark: The scaling of the solar curve was designed so as to make it impossible to detect a trend. Also the solar curve that was purposely selected is not really representative if one looks at the solar reconstructions of isotopes and cosmic rays. A more scientifically robust version of the chart would look as follows:

Figure 2: Global temperature (GISS), CO2-concentration and solar activity (Steinhilber et al. 2009).
Rahmstorf complains about con-men and tricksters, but completely fails himself when put to the test. Is this person, who gladly speaks at Green Party campaign events, really as credible as he fancies himself to be?
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterI lived in Arizona for a few years and I remember flash floods occurring regularly during the rainy summer season. Good to see things are getting less extreme there. Thanks, global warming! -PG
===================================
Researchers surprised: extreme rainfall in Arizona has decreased over the past 50 years despite climate warming
By Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
(German text translated and edited by P Gosselin)
Climate change leads to more extreme rainfall events and worse flooding. That’s the common claim that gets made before one looks at the data.
Bit by bit, however, researchers are filling in the data gaps and finding one surprise after another. For example the University of Bristol put out a press release last October on the extreme rainfall trends in Arizona: In summary: Extreme rainfall has decreased over the past half century despite climate warming.
Moreover, total rainfall fortunately has risen. The alarmist general assumption of increased extreme rainfall events has failed to materialize. The press release follows:
Rainfall trends in arid regions buck commonly held climate change theories
The recent intense hurricanes in the Atlantic have sharply focused attention on how climate change can exacerbate extreme weather events. Scientific research suggests that global warming causes heavier rainfall because a hotter atmosphere can hold more moisture and warmer oceans evaporate faster feeding the atmosphere with more moisture. However, this link between climate warming and heavy rainfall has only been examined in particular regions where moisture availability is relatively high. Until now, no research has been undertaken that examines this relationship in dryland regions where short, sharp rainstorms are the dominant source of precipitation and where moisture availability on land is extremely limited.




<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




To explore the links between climatic warming and rainfall in drylands, scientists from the Universities of Cardiff and Bristol analysed more than 50 years of detailed rainfall data (measured every minute) from a semi-arid drainage basin in south east Arizona exhibiting an upward trend in temperatures during that period. The analysis demonstrated a decline in rainfall intensity, despite an increase in total rainfall over the years. Interestingly, the study shows that there is a long-term decline in heavy rainfall events (greater than 25 mm/h) and an associated increase in the number of smaller storms each delivering less rainfall. This result is contrary to commonly held assumptions about rainfall trends under climate change.
Lead author, Dr Michael Singer from School of Earth and Ocean Sciences at Cardiff University, said: “In drylands, convective (or short, intense) rainfall controls water supply, flood risk and soil moisture but we have had little information on how atmospheric warming will affect the characteristics of such rainstorms, given the limited moisture in these areas.” Co-author, Dr Katerina Michaelides, from the School of Geographical Sciences and Cabot Institute at the University of Bristol, said: “Our findings are consistent with previous research in the Colorado Basin which has revealed a decline in runoff in the upper part of the Basin. “Our work demonstrates that there is a more regional decline in water resources in this dryland region, which may be found in other dryland regions of the world.”
Since trends in convective rainfall are not easily detected in daily rainfall records, or well-simulated by global or regional climate models, the researchers created a new tool to assess the effects of climate change on rainfall patterns and trends in dryland areas. Their new model, STORM, simulates individual rainstorms and their expression over a river basin, and it can represent different classes of climate change over many decades. Drs Singer and Michaelides employ STORM to show that the historical rainfall trends likely resulted in less runoff from this dryland basin, an effect they expect to have occurred at many similar basins in the region. Dr Singer added: “We see this model as a useful tool to simulate climate change in regions and cases where traditional models and methods don’t capture the trends.”
Paper: ‘Deciphering the expression of climate change within the Lower Colorado River basin by stochastic simulation of convective rainfall’ by M. Bliss Singer and K. Michaelides in Environmental Research Letters.”
In May 2015 extensive flooding occurred in Texas and Oklahoma due to heavy rainfall. This was not caused by a hurricane, but rather by an El Nino, which indeed leads to extreme rainfall events, according to a study by Wang et al. 2015.
In another study by Wang et al. 2014, researchers found a flooding pattern in the region of the Missouri River which happened to follow a Pacific Ocean cycle. Knowledge of this link now allows better forecasts to be made and preventive efforts to be taken. Washington University in St. Louis reminded that not everything can be attributed in knee-jerk fashion to climate change. Extreme rain at the end of 2015 in Missouri led to terrible flooding. Part of the blame here was assigned to the changes in building in flood-prone areas of the river which led to an obstruction of the outflow.
In another study the University of Colorado in Denver was able to show that also the state of Colorado is poorly prepared for flooding. Important bridges and infrastructure urgently need to be upgraded. Damage that occurred in a flood in 2013 would have been much less had the structures been strengthened and better taken care of.

Share this...FacebookTwitter "
nan
nan
"In her first interview since taking the helm, new director-general of the National Trust Hilary McGrady has said she wants things to get “radical” at the charity, by looking to more urban conservation. McGrady told the BBC:   I want to reach more people, and more people live in urban areas. The days of walking in to one of our beautiful houses and saying a family lived here, that’s not going to do it. We need to think about what’s relevant – why would someone in the middle of Birmingham say that’s interesting? What is it in Birmingham that they would get more value from? The future McGrady sees is one where the trust expands its conservation role into cities by working with organisations including community groups and local authorities. But who is to say that the countryside can’t be just as radical? Surprisingly, McGrady might find an unlikely ally in Downton Abbey character the Dowager Countess of Grantham. In a 2015 episode that saw Downton’s doors open to the public for a fundraiser day, the dowager countess poured scorn: “Roll up! Visit an actual dining room complete with a real life table and chairs!” She would have called for a stiff drink had she learned what was ahead – in 2017 alone, 24.5m visitors paid to tour a National Trust property.  Downton doesn’t show us the National Trust, but plotlines about fading grandeur and financial ruin fill us in on the crises that befell country houses after the First World War. Many a threatened house ended up in National Trust care, including Knole, and Basildon Park – home to some of Downton’s interior sets. Highclere Castle, the “real” Downton Abbey, is still in private hands, but open to the public in summer. But was stockpiling country houses really in the national interest? Back in the 1980s, the academic and writer Patrick Wright railed against preserving country piles. For him, the “heritage industry” was deeply conservative and out of touch. He might be pleased to see Hilary McGrady finally catching up.  However, as a rural researcher, I’m troubled by how McGrady’s idea of “radical” means “urban”. She’s talking Birmingham, not Betws-y-Coed. It’s as if nothing radical could happen in the countryside, and nothing countryside could matter to city-dwellers.  Imagine telling that to the Kinder Scout trespassers. 86 years ago this week, they claimed the right to roam against violent landowner opposition. Many trespassers came from the nearby cities of Manchester and Sheffield. They saw in the countryside not just rights worth fighting for, but a refuge from industrial smoke and graft. Some went to jail for the cause – and are commemorated by the National Trust.  Though the dowager countess would have released the hounds, at least one Downton resident might have joined in on a mass trespass. Kitchen maid Daisy cheered on the Abbey’s open day: “I think all these houses should be open to the public. What gives them the right to keep people out?” Rights and justice have spurred on many a rural radical. The 19th century Luddites are now wrongly remembered as stick-in-the-muds, but their fight was for livelihoods and labour rights. They met out on the moors, where non-conformist preachers – radically outside the stuffy established church – also roused crowds who gathered from miles around. A few centuries earlier, non-conformism led the Diggers to their radical vision for farming common land. Fast forward to our own time and a piece of former commons was reclaimed as the Greenham Common Women’s Peace Camp.  We needn’t just look back through history for radical rural goings-on. Too often, the countryside gets imagined as though it is a kind of surviving past, a bit simple, and more prone to outbreaks of Morris dancing than anything actually interesting. I suspect this is why Hilary McGrady thinks that a more relevant National Trust must be a more urban one. A real radical plan could be quite otherwise.  The countryside is and can be innovative. Through the ROBUST project, I’m joining with colleagues from across Europe to rethink the narrow notion that if cities are economic engines, then the countryside is just a carriage pulled along for the ride. We’re investigating better and more beneficial interconnections between rural and urban – even the middle of Birmingham.  I take a cue from another radical, William Morris. There’s probably some of the Victorian designer’s wallpaper in Downton, and the dowager countess would have approved of his soft furnishings. But he would have found more to talk about downstairs with Daisy. For Morris, utopia wasn’t back in a staid rural past or ahead in a science fiction city: he saw a radical rural future. Perhaps a radical National Trust might be just as visionary."
"Since the dawn of history, human societies have ascribed sacred status to certain places. Areas such as ancestral burial grounds, temples and churchyards have been given protection through taboo and religious belief. As many of these places have been carefully managed for many years an interesting side effect has occurred – the sites often retain more of their natural condition than surrounding areas used for farming or human habitation. As a result, they are often called “sacred natural sites” (SNS).   Today, as many other natural habitats have become degraded, researchers worldwide are increasingly interested in the role of SNS in biodiversity conservation. Most of the world’s belief systems, including Christianity, give places sacred status.  In Mediterranean Europe, for instance, the grounds of churches – with their associated ancient trees – have become important SNS.  One of the best examples is in the mountainous region of Epirus in north-western Greece. In the municipalities of Zagori and Konitsa almost every village has one or more sacred grove. These places have been protected through religious belief systems for hundreds of years.  The groves are either protective forests that lie uphill from the village, or groups of mature trees surrounding outlying churches, monuments or other works of religious art. Activities such as the cutting of trees or livestock grazing have been either prohibited or strictly regulated in these places (and disobeying these prohibitions sometimes led to excommunication). We have recently been studying these Greek SNS as part of our SAGE (SAcred Groves of Epirus) project. Our team wanted to find out, using a rigorous research approach, whether SNS are more biodiverse than other forest areas, and, if so, what lessons conservationists could learn from this.  To do this, our international and multidisciplinary group has recently completed the world’s first replicated systematic investigation into the claims that areas conserved as SNS are more biodiverse for different types of plant and animal. For our recently published study, we selected eight SNS in Epirus that covered a wide range of environmental conditions. Each was closely matched with a nearby non-sacred “control” forest which had been managed conventionally – sometimes through natural regeneration. We then conducted a detailed inventory in each site, of eight different groups of organisms. These ranged from fungi and lichens, through herbaceous and woody plants to nematodes, insects, bats and passerine birds. We found that SNS do indeed have a small but persistent biodiversity advantage. This is expressed in a number of ways, most clearly through the existence of more distinct communities of species among the sacred groves than in the control sites (this phenomenon is known as beta diversity).  The group with the most notably higher biodiversity in the SNS than in control sites were the fungi. These often grow in dead wood or old trees, which usually get removed in conventionally managed forests. Of the species of passerine birds (a group that includes many songbirds) that are designated as having special conservation importance at a European level, we found twice as many species present in the SNS as in the control sites. Because these sacred sites are often quite small it is often said that their conservation benefits are marginal. But we found that the influence of size is relatively weak – even small SNS can play a significant role in biodiversity conservation. But Epirus’s sacred sites are now in peril. The rules that linked belief and conservation that once protected the SNS have become difficult to enforce, due to changing population and land-use. The value of forests which protect from landslides and floods is no longer being recognised. The value of SNS is not just on the land that is sacred itself, these places can act as a nucleus, around which biodiversity can expand. In Epirus, forests have regenerated around many of the sites we studied over the past 70 years – despite humans farming the land. It should be noted that this can increase risks such as fire, as dense young Mediterranean forest is very flammable. Evidently the already well-conserved SNS are of great environmental importance across the world. So the next step is to link these sites into conventional conservation schemes. But it is vital that such strategies are closely aligned with the cultural status of SNS. Local communities are often highly motivated to maintain their sacred sites and associated belief systems but lack the resources to do so. A fully collaborative approach between conservation professionals and local communities could offer a solution that conserves both biodiversity and local cultural values."
"More than 1,000 illegal waste sites spring up across England each year – and there are probably far more that haven’t been recorded. In 2016, the head of the Environment Agency in England called waste the “new narcotics”, saying of waste crime: “it feels to me like drugs felt in the 1980s: the system hadn’t quite woken up to the enormity of what was going on, and was racing to catch up.” This goes far beyond fly-tipping in country lanes. Waste crime can be highly organised criminal enterprises. The UK received the unwelcome accolade of having had Europe’s largest illegal site. A single site in Derry contains more than 1m tonnes of illegally deposited waste – that’s more municipal waste than Northern Ireland produces in a year.  The government has stepped up its fight against waste criminals. The environment secretary, Michael Gove, announced a comprehensive review of how the government should respond to the threat. This review is timely – there is a real need to strengthen the government’s approach.  Waste crime is also a growing problem internationally. Interpol has identified it as one of the fastest growing areas of organised crime, having the potential to rival drug trafficking in terms of scale and profits. It is estimated to cost EU countries a staggering €72 to €90 billion each year, covering landfill tax evasion, losses to responsible waste disposal businesses and clean up costs. There are other costs too: in the UK, the fire service spends £16m every year putting out fires at waste sites. Yet Europe has been exporting larger volumes of its trash abroad. About one-fifth of all the EU’s plastic waste was sent to China in 2016. Now that the Chinese government refuses to accept large volumes of foreign waste, some European countries are facing a deepening waste crisis. This can only increase the amount of waste ending up in illegal sites. 


      Read more:
      China bans foreign waste – but what will happen to the world's recycling?


 Illegal waste sites don’t have adequate safeguards, so they can present a significant threat to public health and the environment. The waste site in Derry is dangerously close to the River Faughan, which supplies the city with drinking water. Rates for some types of cancer – linked to toxic waste dumping in the Naples region of Italy – were found to be up to 80% above the national average. Some European governments have turned a blind eye to waste crime. Detecting waste sites takes time and money, as investigations, prosecutions and clean up don’t come cheap. Other governments have allocated significant resources to tackling the illegal waste problem. The English Environment Agency received an extra £30m for waste enforcement in last year’s budget. Despite the clear efforts of some regulators to tackle waste crime, it is more entrenched than ever. Regulatory bodies rely heavily on tip offs, as it’s challenging to detect illegal waste sites. Waste dumping is highly profitable, so it is organised and deliberate, which means that sites are typically clandestine. Traditional regulatory approaches have been critiqued for not being proactive in identifying illegal waste sites. Satellite technology offers a promising means of solving this problem – that’s why, together with an Earth observation professor and a military intelligence analyst, I co-founded Air and Space Evidence (ASE) – an academic spin-off company originating from University College London. Using satellite data and waste site identification algorithms, we became the first company in the world to offer a space monitoring service to detect illegal waste sites. A space-based system is a game changer, because it makes it possible to monitor whole countries routinely and cheaply. Illegal sites can be detected more quickly, so authorities can intervene earlier. Sites can also be categorised in terms of risk, so regulators can prioritise ground inspections at the most dangerous sites. To date there has been limited uptake of satellite data in the waste sector, but that could soon change. Since April 2018, landfill tax can be levied at illegal sites (it only applied to legal sites before). This means there is now a direct financial incentive in the UK to tackle the issue. And the gains can be substantial. In one 7,000 km² test area in the UK, we identified 207 sites which were classified as suspicious. A countrywide monitoring programme would be expected to identify potential tax penalties in the hundreds of millions of pounds. In the UK, all of this tax goes to HMRC and not to the environmental regulator. By contrast, in Ireland the regulator is incentivised, because it can receive up to 80% of the tax income gained from illegal waste sites. It would certainly make sense for HMRC to work more closely with regulatory agencies to tackle waste crime, and to share some of the spoils. Modern satellites can see objects as small as manhole covers and are already used in the UK to detect agricultural subsidy fraud and oil spills. The government could make use of these technological advancements, to clear up the rising tide of pollution in its own backyard."
"Gaza has often been invaded for its water. Every army leaving or entering the Sinai desert, whether Babylonians, Alexander the Great, the Ottomans, or the British, has sought relief there. But today the water of Gaza highlights a toxic situation that is spiralling out of control.  A combination of repeated Israeli attacks and the sealing of its borders by Israel and Egypt, have left the territory unable to process its water or waste. Every drop of water swallowed in Gaza, like every toilet flushed or antibiotic imbibed, returns to the environment in a degraded state.  When a hospital toilet is flushed, for instance, it seeps untreated through the sand into the aquifer. There it joins water laced with pesticides from farms, heavy metals from industry, and salt from the ocean. It is then pumped back up by municipal or private wells, joined with a small fraction of freshwater purchased from Israel, and cycled back into people’s taps. This results in widespread contamination and undrinkable drinking water, about 90% of which exceeds the World Health Organisation (WHO) guidelines for salinity and chloride. Incredibly, conditions are getting worse, thanks to the emergence of “superbugs”. These multi-drug resistant organisms have developed thanks to an over-prescription of antibiotics by doctors desperate to treat the victims of the seemingly endless assaults. The more injury there is, the more chance there is of re-injury. Less regular access to clean water means infections will spread faster, bugs will be stronger, more antibiotics will be prescribed – and the victims will be ever-more weakened. The result is what has been termed a toxic ecology or “biosphere of war”, of which the noxious water cycle is just one part. A biosphere refers to the interaction of all living things with the natural resources that sustain them. The point is that sanctions, blockades and a permanent state of war affects everything that humans might require in order to thrive, as water becomes contaminated, air is polluted, soil loses its fertility and livestock succumb to diseases. People in Gaza who may have evaded bombs or sniper fire have no escape from the biosphere. War surgeons, health anthropologists and water engineers – including ourselves – have observed this situation developing wherever protracted armed conflict or economic sanctions grind on, as with water systems in Basrah and health systems throughout Iraq or Syria. It’s now well past time to clean it up. It’s not as if there is no fresh water nearby to alleviate the situation in Gaza. Just a few hundred metres from the border are Israeli farms that use freshwater pumped from Lake Tiberias (the Sea of Galilee) to grow herbs destined for European supermarkets. As the lake is around 200km to the north and lies 200 metres below sea level, a massive amount of energy is used to pump all that water. The lake water is also fiercely contested by Lebanon, Jordan, Syria and Palestinians in the West Bank, each of which is seeking their legal entitlement of the Jordan River basin.  Meanwhile, Israel desalinates so much seawater these days that its municipalities are turning it down. Excess desalinated water is being used to irrigate crops, and the country’s water authority is even planning to use it to refill Tiberias itself – a bizarre and irrational cycle, considering the lake water continues to be pumped the other direction into the desert. There is now so much manufactured water that some Israeli engineers can declare that “today, no one in Israel experiences water scarcity”.  But the same cannot be said for Palestinians, especially not those in Gaza. People there have resorted to various ingenious filters, boilers, or under-the-sink or neighbourhood-level desalination units to treat their water. But these sources are unregulated, often full of germs, and just another reason children are prescribed antibiotics – thus continuing the pattern of injury and re-injury. Doctors, nurses, and water maintenance crews meanwhile try to do the impossible with the minimal medical equipment at their disposal.  The implications for all those who invest in Gaza’s repeatedly destroyed water and health projects are clear. Providing more ambulances or water tankers – the “truck and chuck” strategy – might work when conflicts are at their most acute, but they are never more than a band aid. Yes, things will get better in the short term, but soon enough Gaza will be onto the next generation of antibiotics, and dealing with teflon-coated superbugs.  Donors must instead design programmes suited to the all-pervasive and incessant biosphere of war. This means training many more doctors and nurses, providing more medicines, and infrastructure support for health and water services. More importantly, donors should build-in political “cover” to protect their investments (if not the local children), perhaps by calling for those who destroy the infrastructure to foot the bill for repairs.  And there is an even bigger message for the rest of us. Our research shows that war is more than simply armies and geopolitics – it extends across entire ecosystems. If the dehumanising ideology behind the conflict was confronted, and if excess water was diverted to people rather than to lakes, then the easily avoidable repeated injuries suffered by people in Gaza would become a thing of the past. Palestinians would soon find their biosphere a whole lot healthier."
"The final day of the year is a good time to look back at the year’s weather and forward to what might happen next. Weather in the UK in 2019 did not match the two major events of the previous year – the “beast from the east” of February and March 2018 and that summer’s World Cup heatwave. We did, however, see the UK’s record high when, on 25 July, the temperature at Cambridge Botanic Garden reached 38.7C (101.7F), narrowly beating the previous high of 38.5C set in Kent in August 2003.  For the first time, however, television and newspaper reports did not treat high temperatures with unalloyed glee. Instead, they rightly regarded this as a clear indication that climate change and global warming – now more properly called the climate emergency and global heating – have begun to take hold. The marker of this new weather is, above all, unpredictability, which makes it harder to keep faith in the old certainties of weather lore. So, as a valedictory farewell to the old year, I leave you with this ancient rhyme: If New Year’s Eve the wind bloweth south, It betokeneth warmth and growth, If west, much milk, and fish in the sea; If north, much cold, and storms there will be; If east, the trees will bear much fruit, If north-east, flee it man and brute."
"In the spring of 2001, the UK countryside turned into a crematorium. A foul-smelling haze settled over parts of the country as 6m cows, sheep, pigs and goats were slaughtered and their carcasses burned in the fields. This was the result of a bid to control an outbreak of foot-and-mouth disease – a contagious disease of livestock named after the ulcers it causes in the mouth and between the hooves of  farm animals.  The outbreak began on Burnside farm, Northumberland, where uncooked swill – food leftovers – were illegally fed to a barn of pigs. Under the regulation at the time, food wastes had to be cooked to sterilise them and prevent disease-transfer. This was the first step in a series of unfortunate events which led to the costliest animal epidemic the country had ever seen.   Unknown to the farmer, that unprocessed food waste contained imported meat infected with the foot-and-mouth virus. Once it was introduced the virus spread across the country, exacerbated by a slow response from the Ministry for Agriculture, Fisheries and Food (which was later rebranded as DEFRA, in an effort to move on from the debacle). The ministry’s efforts to limit livestock movements were ultimately too little too late. In the aftermath of the outbreak, the government had to do something, and so the use of all food wastes in animal feed was banned (irrespective of heat-treatment), ending the 9,000-year-old practice of swill-feeding. This UK-wide ban was extended to the EU within a year. But 17 years on, a growing body of research is questioning whether the ban on swill was the right response for farmers, pigs, and the planet. The European livestock industry, in many ways, is in a squeeze. The emergence of large-scale commercial livestock farming operations in the world’s rapidly developing economies has increased the international competition for animal feeds, driving up feed prices and shrinking profits. This is compounded by the bad press that surrounds the use of soybeans, the archetypal modern animal feed ingredient, demonised for its role in deforestation in the forests and savannahs of Latin America. Media headlines criticising the livestock sector for its impact on the environment are becoming all too familiar – on a finite planet with a growing population, some argue it is an unacceptable waste of resources to grow grains to feed livestock instead of people. It is in light of these challenges that interest in swill has been rekindled. Legalising heat-treated swill once more would put livestock back at the heart of a sustainable food system, a green, circular economy where livestock play a key role in recycling nutrients and disposing of food waste. Indeed, when compared with turning food waste into biogas or compost, using it as swill is hands-down the option with the lowest environmental impact. And it can be done – in the same year that the UK banned the use of swill, Japan introduced the opposite policy, establishing a modern, regulated system for safely producing swill. Today, countries like Japan and South Korea recycle around 40% of their food waste as animal feed. Farmers use swill not because of its environmental benefits, but because it reduces their feed costs by 40-60%. They have achieved these remarkable results through a science-based approach to swill feeding; swill manufacturers in Japan and South Korea must be registered and their compliance with food safety regulation is monitored. Food wastes are treated for three minutes at 80°C, or 30 minutes at 70°C – both enough to deactivate viruses such as foot-and-mouth. Since the introduction of these regulated systems, no disease outbreaks have been associated with swill feeding in these countries. The potential in Europe is huge. Recent research suggests that if Europe were to recycle food waste as swill on the scale achieved in East Asia, this would reduce the land required to produce pig feed by 20%. This an area the size of Wales, including more than a quarter of a million hectares of Latin American soybeans.  For Europe to mimic the success of these East Asian systems requires first and foremost the development of safe operating systems for the European context, based on the regulated heat-treatment and production of swill.  Importantly, legalising swill would require the right level of enforcement to ensure it is done safely. There should arguably be zero tolerance for people caught bending the rules – in stark contrast with the Ministry for Agriculture, Fisheries and Food’s softly-softly approach, which facilitated the 2001 foot-and-mouth outbreak (a MAFF inspector even visited the farm where the outbreak occurred two weeks beforehand, but didn’t clamp down on the farm’s questionable practices). And when weighing up the risks and benefits of reintroducing swill, we also must not forget the risks of the current “ban”, which is frequently broken – a survey of smallholder UK pig farmers for example found that a quarter of them feed uncooked food wastes to their pigs. All of this would, of course, be irrelevant if farmers weren’t willing to use swill. But a new study conducted at the UK’s largest pig industry trade-fair shows that 75% of pig farmers and other attendees of the event supported its relegalisation, and half of all pig farmers surveyed said they would consider using swill on their farm.  Support was not unanimous – some people were understandably concerned about disease risks and consumer acceptance of swill. But overall, the level of support for swill was similar to other alternative animal feeds, such as much-hyped insects. As the meat industry battles to clean up an image tarnished by environmental impacts, questions over animal welfare, and food safety scandals, swill remains a good news story rarely told. A key question to all in the sector is how livestock products can be repositioned as a sustainable part of people’s diets, reared in ways that respect traditional farming practices, with high food safety.  Well, perhaps where there’s swill, there’s a way."
"
Share this...FacebookTwitterCould Earth’s Shifting Plates 
Be Driving Modern Climate?

Within the last year, Dr. Arthur Viterito (geography professor) has published multiple scientific papers documenting the significant correlation (r=0.80) between the seismic activity changes in the Earth’s high geothermal flux areas (HGFA) and both El Niño events and global temperatures.
The HGFA/global temperature correlation has been found to be stronger than the correlation for CO2 concentration changes (r=0.74) for recent decades (1979-2016).
Other recent research has provided further support for the significant influence of seismic activity (i.e., there is a very high correlation [r=0.935] between geothermal flux and North Magnetic Dip Pole movement).
These robust and well-documented seismic activity associations have led Dr. Viterito to call for a reconsideration of the paradigm that says variations in atmospheric CO2 concentrations drive changes in global temperatures.

Viterito, 2016



Viterito, 2017
“The Correlation of Seismic Activity and Recent Global Warming (CSARGW) demonstrated that increasing seismic activity in the globe’s high geothermal flux areas (HGFA) is strongly correlated with global temperatures (r=0.785) from 1979-2015. The mechanism driving this correlation is amply documented and well understood by oceanographers and seismologists.”
“Namely, increased seismic activity in the HGFA (i.e., the mid-ocean’s spreading zones) serves as a proxy indicator of higher geothermal flux in these regions. The HGFA include the Mid-Atlantic Ridge, the East Pacific Rise, the West Chile Rise, the Ridges of the Indian Ocean, and the Ridges of the Antarctic/Southern Ocean. This additional mid-ocean heating causes an acceleration of oceanic overturning and thermobaric convection, resulting in higher ocean temperatures and greater heat transport into the Arctic. This manifests itself as an anomaly known as the “Arctic Amplification,” where the Arctic warms to a much greater degree than the rest of the globe.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




“As illustrated in CSARGW, jumps in HGFA seismic activity can amplify an El Niño event, a phenomenon referred to as a SIENA or a Seismically Induced El Niño Amplification.  Accurately predicting two of these amplified El Niños (i.e., the 2015/2016 event plus the1997/1998 episode) is an important outcome of the HGFA seismicity/temperature relationship.”

“Applying the same methodology employed in CSARGW, an updated analysis through 2016 adds new knowledge of this important relationship while strengthening support for that study’s conclusions. The correlation between HGFA seismic frequency and global temperatures moved higher with the addition of the 2016 data: the revised correlation now reads 0.814, up from 0.785 for the analysis through 2015. This yields a coefficient of determination of .662, indicating that HGFA [high geothermal flux area] seismicity accounts for roughly two-thirds of the variation in global temperatures since 1979.”

Viterito, 2017
“[T]he idea that increased flux of oceanic geothermal heat (as indicated by increased seismic activity in these areas) can significantly alter temperature counters the hypothesis that increasing carbon dioxide has been the primary driver of recent global temperature change. Despite the general “non-acceptance” of this hypothesis, a recent study by Williams (2016) links a seemingly unrelated geophysical phenomenon to mid-ocean seismicity; thus a new paradigm may be emerging from this important association. Specifically, Williams shows that the speed at which the North Magnetic Dip Pole (NMDP) moves is highly correlated (r=0.935) with mid-ocean seismic activity.”
“More importantly, multiple regression analysis corroborates the findings of the previous studies: mid-ocean seismic activity is significantly correlated (p<0.05) with changing temperatures.”

“However, CO2 concentrations, along with NMDP [North Magnetic Dip Pole] displacements, do not explain a significant percentage of the total variance (p>0.05) when they are included and must be dropped from the analysis. This high degree of multicollinearity is a prominent finding.”
“However, it is important to note that, despite high correlations, CO2 increases cannot be causing an intensification of mid-ocean seismic activity nor can higher CO2 concentrations be driving the acceleration of the NMDP [North Magnetic Dip Pole]. There is simply no plausible mechanism that can be invoked here.”
“Clearly, there is far more in play than is currently accounted for in our understanding of earth’s climate. The shifting of plates, along with the concurrent shifts of earth’s NMDP, should spur the geophysical community to create a new and enduring paradigm that links these phenomena to changing global temperatures.”
Share this...FacebookTwitter "
"
Share this...FacebookTwitterScience journalist Axel Bojanowski at German flagship, center-left news weekly Spiegel here dismissed a recent study published by Columbia University scientists Wolfram Schlenker and Anouch Missirian, who had claimed climate warming was driving masses of environmental refugees to Europe.
The two scientists claimed in Science to have found a relationship between weather disasters and refugees migrating to Europe.
However, the far-fetched conclusions by the two scientists has since been met with sharp and harsh criticism for its loose use of statistics. The study was financed by the JRC of the European Union. One member of the JRC, Juan-Carlos Ciscar, said it was time for policy makers to act.
Paper gets “crushing assessment” from other scientists
However, Spiegel’s Bojanowski reports that a number of leading experts dismissed the paper’s claims. For example Thomas Bernauer and Vally Koubi the Zurich-based ETH said: “Politicians would be ill-advised to orient themselves based on this study.”
Bojanowski added that other experts SPIEGEL ONLINE asked gave it “a damning verdict“.
“Dumbest use of statistics”
The Spiegel journalist also took jabs at other leading media outlets, such as the Guardian, Reuters and AP, implying they uncritically used the study for hype.
Bojanowski then cited statistics expert William Briggs of Cornell to assess the methodology used by the study:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The dumbest, most idiotic use of statistics that I’ve seen in a long time.”
Tobias Ide of the Georg Eckert Institute also warned against simplifying “the relationship between warmer temperatures and migration“, Bojanowski wrote, and that Jonas Vestby of the Peace Research Institute in Oslo was surprised the paper ever got by peer-review.
Christiane Fröhlich of the University of Hamburg is considering a rebuttal at Science, Spiegel’s Bojanowski wrote.
Confusion
Fröhlich says the Columbia University authors confused environmental refugees with political refugees. In Europe refugee status is given to persons who are politically oppressed and persecuted and not to those moving due to the environment. Critics of the study also called the projections and correlations claimed by Schlenker and Missirian as “highly speculative“.
Fröhlich also told Spiegel the paper “ignored numerous studies” on migration and warming. Briggs added that the two authors based their assumptions on only “15 years and in only one region” and “ignored 6000 years of human history“.
Bojanowski cited Briggs:
Just how important it would have been to include other regions was made clear by Briggs using one provocative question: ‘Why then don’t asylum applications in cool Chile rise after heat waves in the warm neighboring countries?'”
 
Share this...FacebookTwitter "
nan
"Nine of 13 of Africa’s oldest and largest baobab trees have died in the past decade, it has been reported. These trees, aged between 1,100 and 2,500 years, appear to be victims of climate change. Scientists speculate that warming temperatures have either killed the trees directly or have made them weaker and more susceptible to drought, diseases, fire or wind.  Old baobabs are not the only trees which are affected by climatic changes. Ponderosa pine and Pinyon forests in the American West are dying at an increasing rate as the summers get warmer in the region. In Hawaii the famous Ohi’a trees are also dying at faster rates than previously recorded. There are nine species of baobab trees in the world: one in mainland Africa, Adansonia digitata, (the species that can grow to the largest size and to the oldest age), six in Madagascar, and one in Australia. The mainland African baobab was named after the French botanist Michel Adanson, who described the baobab trees in Senegal. The African baobab is a remarkable species. Not only because of it’s size and lifespan but also in the special way it grows multiple fused stems. In the space between these stems (called false cavities) bark grows, which is unique to the baobab.  Since baobabs produce only faint growth rings, the researchers used radiocarbon dating to analyse samples taken from different parts of each tree’s trunk and determined that the oldest (which is now dead) was more that 2,500-years-old.  They also have more than 300 uses. The leaves, rich in iron, can be boiled and eaten like spinach. The seeds can be roasted to make a coffee substitute or pressed to make oil for cooking or cosmetics. The fruit pulp has six times more vitamin C than oranges, making it an important nutritional complement in Africa and in the European, US and Canadian markets.  Locally, fruit pulp is made into juice, jam, or fermented to make beer. The young seedlings have a taproot which can be eaten like a carrot. The flowers are also edible. The roots can be used to make red dye, and the bark to make ropes and baskets.  Baobabs also have medicinal properties, and their hollow trunks can be used to store water. Baobab crowns also provide shade, making them an idea place for a market in many rural villages. And of course, the trade in baobab products provides an income for local communities.  Baobab trees also play a big part in the cultural life of their communities, being at the centre of many African oral stories. They even appear in The Little Prince.  Baobab trees are not only useful to humans, they are key ecosystem elements in the dry African savannas. Importantly, baobab trees keep soil conditions humid, favour nutrient recycling and avoid soil erosion. They also act as an important source of food, water and shelter for a wide range of animals, including birds, lizards, monkeys and even elephants – which can eat their bark to provide some moisture when there is no water nearby. The flowers are pollinated by bats, which travel long distances to feed on their nectar. Numerous insects also live on the baobab tree. Ancient as they are, baobab trees can be cultivated, as some communities in West Africa have done for generations. Some farmers are discouraged by the fact that they can take 15-20 years to fruit – but recent research has shown by grafting the branches of fruiting trees to seedlings they can fruit in five years.  Many “indigenous” trees show great variation in fruit morphological and nutritional properties – and it takes years of research and selection to find the best varieties for cultivation. This process, called domestication, does not refer to genetic engineering, but the selection and cultivation of the best trees of those available in nature. It seems straightforward, but it takes time to find the best trees – meanwhile many of them are dying. The death of these oldest and largest baobab trees is very sad, but hopefully the news will motivate us to protect the world’s remaining large baobabs and start a process of close monitoring of their health. And, hopefully, if scientists are able to perfect the process of identifying the best trees to cultivate, one day they will become as common in our supermarkets as apples or oranges."
nan
"Around the world, cities endeavour to cut greenhouse gas emissions, while adapting to the threats – and opportunities – presented by climate change. It’s no easy task, but the first step is to make a plan outlining how to meet the targets set out in the Paris Agreement, and help limit the world’s mean temperature rise to less than two degrees Celsius above pre-industrial levels.  About 74% of Europe’s population lives in cities, and urban settlements account for 60-80% of carbon emissions – so it makes sense to plan at an urban level. Working to meet carbon reduction targets can also reduce local pollution and increase energy efficiency – which benefits both businesses and residents.  But it’s just as important for cities to adapt to climate change – even if the human race were to cut emissions entirely, we would still be facing the extreme effects of climate change for decades to come, because of the increased carbon input that has already taken place since the industrial revolution.  In the most comprehensive survey to date, we collaborated with 30 researchers across Europe to investigate the availability and content of local climate plans for 885 European cities, across all 28 EU member states. The inventory provides a big-picture overview of where EU cities stand, in terms of mitigating and adapting to climate change.  The good news is that 66% of EU cities have a mitigation or adaptation plan in place. The top countries were Poland – where 97% of cities have mitigation plans – Germany (81%), Ireland (80%), Finland (78%) and Sweden (77%). In Finland, 78% of cities also had a plan for adapting to climate change.  But only a minority of EU countries – including Denmark, France, Slovakia and the UK – have made it compulsory for cities to develop local climate plans. In these countries, cities are nearly twice as likely to have a mitigation plan and five times as likely to have an adaptation plan. Throughout the rest of the EU, it is mainly large cities that have local climate plans.  There were some shortcomings worth noting: 33% of EU cities (that’s 288 cities) have no standalone climate plans whatsoever – including Athens (Greece), Salzburg (Austria), and Palma de Mallorca (Spain). And not one city in Bulgaria or Hungary has a standalone climate plan. Only 16% of cities – that’s a total of 144 – have joined-up mitigation and adaptation plans, and most of these were in France and the UK – though cities such as Brussels (Belgium), Helsinki (Finland) and Bonn (Germany) had joined-up plans as well.  Some cities have made climate initiatives a common feature in planning activities, often aiming for broader environmental goals, such as resilience and sustainability. Some of these forward-looking cities – Rotterdam and Gouda in the Netherlands, for example – may not have standalone climate change mitigation or adaptation plans, per se. Instead, climate issues are integrated into broader development strategies, as also seen in Norwich, Swansea, Plymouth and Doncaster in the UK. Plans for mitigating the effects of climate change are generally straightforward: they look at ways to increase efficiency, transition to clean energy and improve heating, insulation and transport. In doing so, they are likely to result in financial savings or health benefits for the municipality, and the public. For example, more low-emission vehicles on the road doesn’t just mean less carbon emissions – it also means better air quality for the city’s residents.  Adapting to climate change is not always so simple. Each area will need to adapt in different ways. Some adaptations – such as flood defences – can require huge investment to build, and only rarely prove their effectiveness. Yet there are plans and measures that cities can take, to both mitigate the threats from climate change and adapt to the changes that are already coming.  One way for cities to become more resilient to climate change is to integrate infrastructures for energy, transport, water and food, and allow them to combine their resources. A sensors become more commonplace across European cities, it’s easier to monitor the impacts of local plans to reduce emissions and stay on top of extreme weather. The University of Newcastle in the UK is home to the Urban Observatory, which provides one of the largest open-source digital urban sensing networks in the world. Across the board, cities need to improve the way they manage water at the surface and below ground. Installing more green features in city centres or strategic locations can help urban areas adapt to heatwaves, extreme rainfall and droughts all at once. To find out what works and what doesn’t, it’s essential for cities to network and share knowledge, to create and improve on their local climate plans.  There is simply too much at stake for the world’s cities to go their separate ways when it comes to climate change. We have found that international climate networks make a big difference to countries and cities, as they develop and implement their climate plans. For instance, 333 EU cities of our sample are signatories of the Covenant of Mayors and through that are given support and encouragement as they engage in climate change planning and action. Our study shows that cities are taking climate change threats seriously, but there is clearly more work to be done. It is a near certainty that if cities do not plan and act now to address climate change, they could find themselves in a far more precarious position in the future.  While there is plenty that cities can do, national governments must still take the lead – providing legal and regulatory frameworks and guidance. Our study has demonstrated that this is one of the most effective ways to make sure that cities – and their citizens – are well prepared for the threats and opportunities that climate change will bring."
"
Share this...FacebookTwitterThe extreme German leftists Die Linke (The Left) Party in Germany issued a press release blasting Angela Merkel’s decision not to personally attend the “One Planet Summit” in Paris.
Apparently climate change is not an issue important enough for the German chancellor to devote her time to.
Merkel “a total no-show”
German Leftist Party climate and energy politician Lorenz Gösta Beutin said:
Climate-politically Merkel is a total no show: In Paris the head of the German government could have sent a powerful signal in support of implementing the UN Climate Treaty, and against the unspeakable anti-climate protection course of President Donald Trump, who announced the USA’s withdrawal from the Climate Accord and requested renegotiations. Instead the federal government sent Environment Minister Barbara Hendricks, who had to explain that Germany would resoundingly miss its self-stated climate targets.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




International “faux pas”
The “One Planet Summit” in Paris was held by France, the United Nations and the World bank. Reutin commented further:
That Angela Merkel did not attend the climate conference – where over 50 state and government heads from all around the world wanted to push ahead the historic Paris climate protection accord from two years ago – is a disgrace par excellence. After an embarrassing appearance at the UN climate conference in Bonn, where the chancellor only delivered empty words instead of concrete measures, the self-proclaimed climate chancellor demonstrated a faux pas on the international stage.”
The neoliberal belief held by French President Emmanuel Macron that the free market and private capital would put the brakes on global warming and remove the damage to climate change caused by man and nature is faulty. Capitalism functions only through unbridled growth and the profit of a few. It is the cause, not the solution, of humanity-problem climate change.
The LINKE (LEFTISTS) demand the introduction of a financial transaction tax, whose revenue would in part be allocated to southern countries as climate change support funds. Also the industrialized countries must meet their obligations and pay 100 billion dollars annually  into the Green Climate Fund beginning in 2020, and do so without offsetting already existing development aid funds.“
 
Share this...FacebookTwitter "
nan
"Imagine studying animals without seeing them. Does that sound ludicrous? To people like us, who first got interested in biology because we love animals and enjoy studying them, yes, it sounds like a poor deal. Yet, if you think about what forensic investigators do when they seek DNA evidence at a crime scene, or what doctors do when they detect a pathogen in a patient’s blood, it is exactly that: they detect life forms without seeing them. DNA is life’s blue print. It is present in virtually every organism on Earth, and we usually study it by extracting it from a piece of tissue or a blood sample. But DNA, really, is everywhere: animals shed it constantly, when they scratch themselves, when they release urine, eggs, saliva, excrement and, of course, when they die. Every environment, from your bed to the deepest recesses of the oceans, is full of “biological dust”, mostly cellular material, which contains the DNA of the organisms that left it behind. This, we call “environmental DNA”, or eDNA. Assisted by increasingly fast, accurate and affordable technology, scientists have begun, in recent years, to sequence this trace DNA from many environments. And this “micro” approach has even proved to be useful to scientists investigating environments as vast as the oceans. Many marine animals are large, rare, elusive and highly mobile. Sharks are an obvious example: in the oceans they make up a small proportion of the biomass, most of them are pretty difficult to catch, and they have been in conflict with humans since we started venturing at sea. With a few exceptions, they avoid us, and because of us many have become threatened with extinction.  This is why we thought it would be interesting to see if, just by sampling a few bottles of ocean water (and the DNA fragments therein), we could rapidly map shark presence and distribution, without engaging in wild chases or employing time and resource-intensive shark fishing methods. We were happy to find out that, indeed, this was possible, and that different species could be detected in different geographical regions, although the areas that had been more affected by humans would show scant presence of sharks. But the true measure of the efficiency of this eDNA approach to shark monitoring would only be revealed when contrasted against established, tried-and-tested methodologies, such as scuba-diving visual censuses or baited underwater camera recordings.  This was the focus of our most recent study, conducted with colleagues based in the South Pacific archipelago of New Caledonia, France, Australia and the US, and now published in the journal Science Advances. The results were very exciting: 22 water samples collected over a few weeks detected more sharks than hundreds of baited underwater camera observations over two years, and thousands of scuba dives over a period of decades. Nearly half of the species detected through environmental DNA could not be found at all using traditional methods. And while eDNA could detect the presence of some sharks in about 90% of the samples, underwater cameras could only manage just over 50%, and scuba diving around 15%.  Interestingly, eDNA outperformed the other methods in both pristine and impacted areas. A range of shark species were detected even in busy, noisy and depleted areas, where they were thought to be extirpated. This suggests some “dark diversity” may still be present, in the form of remnant individuals and groups requiring protection. Similarly, eDNA can help by revealing the appearance of newly established, alien species that are expanding their range. All of this is good news for everyone, and this is why. Given the speed and efficiency of eDNA sampling, a much larger portion of the sea can be screened, in a shorter time, to gather an overview of the patterns of diversity across large areas and habitats, along various environmental gradients, and at different times. Potentially, we could rapidly build maps of species diversity and use them to create predictive models and identify the factors that influence diversity, while methods are being developed to improve the quantitative aspect of eDNA detection, also in other charismatic species. All of it will be of great help to those who must devise plans to protect crucial habitats and ecosystems.  Environmental DNA science is still rapidly developing. The databases that we use to match the unknown sequences retrieved from the sea must be enriched with new DNA references of many existing species – every multi-species eDNA study to date has detected large amounts of sequences that could not be matched against any reference. A significant proportion of these belong to organisms that are yet to be described by scientists.  The “DNA probes” currently available will have to become longer, as short sequences may sometimes fail to distinguish closely related species. For instance, the blacktip shark shared some identical sequences with the grey reef shark along the DNA stretch used in our study. Nevertheless, all the initial indications suggest that this approach can get us a step closer to understanding and better managing the largest ecosystem on Earth."
"
Share this...FacebookTwitterUpdated: The Shrinking 
CO2 Climate Sensitivity

A recently highlighted paper published by atmospheric scientists Scafetta et al., (2017) featured a graph (above) documenting post-2000 trends in the published estimates of the Earth’s climate sensitivity to a doubling of CO2 concentrations (from 280 parts per million to 560 ppm).
The trajectory for the published estimates of transient climate response (TCR, the average temperature response centered around the time of CO2 doubling) and equilibrium climate sensitivity (ECS, the temperature response upon reaching an equilibrium state after doubling) are shown to be declining from an average of about 3°C earlier in the century to below 2°C and edging towards 1°C for the more recent years.
This visual evidence would appear to indicate that past climate model determinations of very high climate sensitivity (4°C, 5°C, 6°C and up) have increasingly been determined to be in error.  The anthropogenic influence on the Earth’s surface temperature has likely been significantly exaggerated.

Scafetta et al., 2017   “Since 2000 there has been a systematic tendency to find lower climate sensitivity values. The most recent studies suggest a transient climate response (TCR) of about 1.0 °C, an ECS less than 2.0 °C and an effective climate sensitivity (EfCS) in the neighborhood of 1.0 °C.”
“Thus, all evidences suggest that the IPCC GCMs at least increase twofold or even triple the real anthropogenic warming. The GHG theory might even require a deep re-examination.”

An Update On The Gradually Declining Climate Sensitivity


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The graph shown in Scafetta et al. (2017) ends in 2014, which means that papers published in the last 3 years are not included.   Also, there were several other published climate sensitivity papers from the last decade that were excluded from the analysis, possibly because they did not include and/or specify TCR and/or ECS estimates in isolation, but instead just used a generic doubled-CO2 climate sensitivity value (shown in purple here).
Below is a new, updated graph that (1) includes some of the previously unidentified papers and (2) adds the 10 – 12 climate sensitivity papers published in the last 3 years.  Notice, again, that the trend found in published papers has continued downwards, gradually heading towards zero.  The reference list for the over 20 additional papers used for the updated analysis is also included below.
For a more comprehensive list of over 60 papers with very low (<1°C) climate sensitivity estimates, see here.



Smirnov, 2017 (~0.4°C)
It is shown that infrared emission of the atmosphere is determined mostly by atmospheric water. One can separate the flux of outgoing infrared radiation of the atmosphere from that towards the Earth. The fluxes due to rotation-vibration transitions of atmospheric   CO2  molecules are evaluated. Doubling of the concentration of  CO2 molecules in the atmosphere that is expected over 130 years leads to an increase of the average Earth temperature by (0.4±0.2) K mostly due to the flux towards the Earth if other atmospheric parameters are not varied.

Smirnov, 2016
[W]e take into account that CO2 molecules give a small contribution to the heat Earth balance and, therefore, one can use the altitude distribution of the temperature for the standard atmosphere model [1], and a variation of the CO2 concentration does not influence this distribution.  …  [I]njection of CO2 molecules into the atmosphere leads to a decrease of the outgoing radiation flux that causes a decrease of the average Earth temperature. But this decrease is below 0.1K that is the accuracy of determination of this value.  Thus, the presence of carbon dioxide in the atmosphere decreases the outgoing atmospheric radiative flux that leads to a decrease of the Earth temperature by approximately (1.8 ± 0.1) K. The change of the average temperature at the double of the concentration of atmospheric CO2 molecules is determined by the transition at 667cm−1 only and is lower than 0.1K.
In particular, doubling of the concentration of CO2 molecules compared to the contemporary content increases the global Earth temperature by ΔT = 0.4 ± 0.2K. … From this we have that the average temperature variation ΔT = 0.8 ◦C from 1880 up to now according to NASA data may be attained by the variation of the water concentration by 200ppm or Δu/u ≈ 0.07, Δu = 0.2. Note that according to formula (2) the variation of an accumulated concentration of CO2 molecules from 1959 (from 316ppm up to 402ppm) leads to the temperature variation ΔT = 0.15°C. One can see that the absorption of a water molecule in infrared spectrum is stronger than that of the CO2 molecule because of their structures, and the injection of water molecules in the atmosphere influences its heat balance more strongly than the injection of CO2 molecules.



Reinhart, 2017 (<0.24°C)
Our results permit to conclude that CO2 is a very weak greenhouse gas and cannot be accepted as the main driver of climate change. … The assumption of a constant temperature and black body radiation definitely violates reality and even the principles of thermodynamics. … [W]e conclude that the temperature increases predicted by the IPCC AR5 lack robust scientific justification. … A doubling [to 800 ppm] of the present level of CO2 [400 ppm] results in  [temperature change] < 0.24 K. … [T]he scientific community must look for causes of climate change that can be solidly based on physics and chemistry. … The observed temperature increase since pre-industrial times is close to an order of magnitude higher than that attributable to CO2.

Abbot and Marohasy, 2017  (0.6°C equilibrium)
The largest deviation between the ANN [artificial neural network] projections and measured temperatures for six geographically distinct regions was approximately 0.2 °C, and from this an Equilibrium Climate Sensitivity (ECS) of approximately 0.6 °C [for a doubling of CO2 from 280 ppm to 560 ppm plus feedbacks] was estimated. This is considerably less than estimates from the General Circulation Models (GCMs) used by the Intergovernmental Panel on Climate Change (IPCC), and similar to estimates from spectroscopic methods.
The proxy measurements suggest New Zealand’s climate has fluctuated within a band of approximately 2°C since at least 900 AD, as shown in Figure 2. The warming of nearly 1°C since 1940 falls within this band. The discrepancy between the orange and blue lines in recent decades as shown in Figure 3, suggests that the anthropogenic contribution to this warming could be in the order of approximately 0.2°C. [80% of the warming since 1940 may be due natural factors].

 Harde, 2016 (0.7°C equilibrium)
Including solar and cloud effects as well as all relevant feedback processes our simulations give an equilibrium climate sensitivity of CS = 0.7 °C (temperature increase at doubled CO2) and a solar sensitivity of SS = 0.17 °C (at 0.1 % increase of the total solar irradiance). Then CO2 contributes 40 % and the Sun 60 % to global warming over the last century.

Bates, 2016  (~1°C)
Estimates of 2xCO2 equilibrium climate sensitivity (EqCS) derive from running global climate models (GCMs) to equilibrium. Estimates of effective climate sensitivity (EfCS) are the corresponding quantities obtained using transient GCM output or observations. The EfCS approach uses an accompanying energy balance model (EBM), the zero-dimensional model (ZDM) being standard. GCM values of EqCS and EfCS vary widely [IPCC range: (1.5, 4.5)°C] and have failed to converge over the past 35 years. Recently, attempts have been made to refine the EfCS approach by using two-zone (tropical/extratropical) EBMs. When applied using satellite radiation data, these give low and tightly-constrained EfCS values, in the neighbourhood of 1°C. … The central conclusion of this study is that to disregard the low values of effective climate sensitivity (≈1°C) given by observations on the grounds that they do not agree with the larger values of equilibrium, or effective, climate sensitivity given by GCMs, while the GCMs themselves do not properly represent the observed value of the tropical radiative response coefficient, is a standpoint that needs to be reconsidered.

Evans, 2016 (<0.5°C equilibrium)
The conventional basic climate model applies “basic physics” to climate, estimating sensitivity to CO2. However, it has two serious architectural errors. It only allows feedbacks in response to surface warming, so it omits the driver-specific feedbacks. It treats extra-absorbed sunlight, which heats the surface and increases outgoing long-wave radiation (OLR), the same as extra CO2, which reduces OLR from carbon dioxide in the upper atmosphere but does not increase the total OLR. The rerouting feedback is proposed. An increasing CO2 concentration warms the upper troposphere, heating the water vapor emissions layer and some cloud tops, which emit more OLR and descend to lower and warmer altitudes. This feedback resolves the nonobservation of the “hotspot.” An alternative model is developed, whose architecture fixes the errors. By summing the (surface) warmings due to climate drivers, rather than their forcings, it allows driver-specific forcings and allows a separate CO2 response (the conventional model applies the same response, the solar response, to all forcings). It also applies a radiation balance, estimating OLR from properties of the emission layers. Fitting the climate data to the alternative model, we find that the equilibrium climate sensitivity is most likely less than 0.5°C, increasing CO2 most likely caused less than 20% of the global warming from the 1970s, and the CO2 response is less than one-third as strong as the solar response. The conventional model overestimates the potency of CO2 because it applies the strong solar response instead of the weak CO2response to the CO2 forcing.

Gervais, 2016 [full]  (<0.6°C transient)
Conclusion: Dangerous anthropogenic warming is questioned (i) upon recognition of the large amplitude of the natural 60–year cyclic component and (ii) upon revision downwards of the transient climate response consistent with latest tendencies shown in Fig. 1, here found to be at most 0.6 °C once the natural component has been removed, consistent with latest infrared studies (Harde, 2014). Anthropogenic warming well below the potentially dangerous range were reported in older and recent studies (Idso, 1998; Miskolczi, 2007; Paltridge et al., 2009; Gerlich and Tscheuschner, 2009; Lindzen and Choi, 2009, 2011; Spencer and Braswell, 2010; Clark, 2010; Kramm and Dlugi, 2011; Lewis and Curry, 2014; Skeie et al., 2014; Lewis, 2015; Volokin and ReLlez, 2015). On inspection of a risk of anthropogenic warming thus toned down, a change of paradigm which highlights a benefit for mankind related to the increase of plant feeding and crops yields by enhanced CO2 photosynthesis is suggested.

Marvel et al., 2016 (1.8°C transient, 3.0°C equilibrium)
Assuming that all forcings have the same transient efficacy as greenhouse gases, and following a previous study, the best estimate (median) for TCR is 1.3°C. However, scaling each forcing by our estimates of transient efficacy (determined from either iRF or ERF), we obtain a best estimate for TCR of 1.8°C. This scaling simultaneously considers both forcing and ocean heat uptake efficacy. Other estimates of TCR which differ slightly due to choices of base period and uncertainty estimates and the aerosol forcing used, are similarly revised upward when using calculated efficacies.  We apply the same reasoning to estimates of ECS. Using an estimate4 of the rate of recent heat uptake Q = 0.65 ± 0.27 W m-2, we find, assuming all equilibrium efficacies are unity, a best estimate of ECS = 2.0°C, comparable to the previous result of 1.9°C.  However, as with TCR, accounting for differences in equilibrium forcing efficacy revises the estimate upward; our new best estimate (using efficacies derived from the iRF) is 2.9°C. If efficacies are instead calculated from the ERF, the best estimate of ECS is 3.0°C. As for TCR, alternate estimates of ECS are revised upward when efficacies are taken into account.

Soon, Connolly, and Connolly, 2015 [full] (0.44°C)
Nonetheless, let us ignore the negative relationship with greenhouse gas (GHG) radiative forcing, and assume the carbon dioxide (CO2) relationship is valid. If atmospheric carbon dioxide concentrations have risen by ~110 ppmv since 1881 (i.e., 290→400 ppmv), this would imply that carbon dioxide (CO2) is responsible for a warming of at most 0.0011 × 110 = 0.12°C over the 1881-2014 period, where 0.0011 is the slope of the line in Figure 29(a). We can use this relationship to calculate the so-called “climate sensitivity” to carbon dioxide, i.e., the temperature response to a doubling of atmospheric carbon dioxide. According to this model, if atmospheric carbon dioxide concentrations were to increase by ~400 ppmv, this would contribute to at most 0.0011 × 400 = 0.44°C warming. That is, the climate sensitivity to atmospheric carbon dioxide is at most 0.44°C.

Lewis and Curry, 2015 (1.33°C  transient, 1.64°C  equilibrium)
Energy budget estimates of equilibrium climate sensitivity (ECS) and transient climate response (TCR) are derived using the comprehensive 1750–2011 time series and the uncertainty ranges for forcing components provided in the Intergovernmental Panel on Climate Change Fifth Assessment Working Group I Report, along with its estimates of heat accumulation in the climate system. The resulting estimates are less dependent on global climate models and allow more realistically for forcing uncertainties than similar estimates based on forcings diagnosed from simulations by such models. Base and final periods are selected that have well matched volcanic activity and influence from internal variability. Using 1859–1882 for the base period and 1995–2011 for the final period, thus avoiding major volcanic activity, median estimates are derived for ECS of 1.64 K and for TCR of 1.33 K.

Johansson et al., 2015 (2.5°C  equilibrium)
A key uncertainty in projecting future climate change is the magnitude of equilibrium climate sensitivity (ECS), that is, the eventual increase in global annual average surface temperature in response to a doubling of atmospheric CO2 concentration. The lower bound of the likely range for ECS given in the IPCC Fifth Assessment Report was revised downwards to 1.5 °C, from 2 °C in its previous report, mainly as an effect of considering observations over the warming hiatus—the period of slowdown of global average temperature increase since the early 2000s. Here we analyse how estimates of ECS change as observations accumulate over time and estimate the contribution of potential causes to the hiatus. We find that including observations over the hiatus reduces the most likely value for ECS from 2.8 °C to 2.5 °C, but that the lower bound of the 90% range remains stable around 2 °C. We also find that the hiatus is primarily attributable to El Niño/Southern Oscillation-related variability and reduced solar forcing.

Kissin, 2015 (~0.6°C)
[A] doubling the CO2 concentration in the Earth’s atmosphere would lead to an increase of the surface temperature by about +0.5 to 0.7 °C, hardly an effect calling for immediate drastic changes in the planet’s energy policies. An increase in the absolute air humidity caused by doubling the CO2 concentration and the resulting decrease of the outgoing IR flux would produce a relatively small additional effect due to a strong overlap of IR spectral bands of CO2 and H2O, the two compounds primarily responsible for the greenhouse properties of the atmosphere.

Kimoto, 2015  [full] (~0.16°C)
The central dogma is critically evaluated in the anthropogenic global warming (AGW) theory of the IPCC, claiming the Planck response is 1.2K when CO2 is doubled. The first basis of it is one dimensional model studies with the fixed lapse rate assumption of 6.5K/km. It is failed from the lack of the parameter sensitivity analysis of the lapse rate for CO2 doubling. The second basis is the Planck response calculation by Cess in 1976 having a mathematical error. Therefore, the AGW theory is collapsed along with the canonical climate sensitivity of 3K utilizing the radiative forcing of 3.7W/m2 for CO2 doubling. The surface climate sensitivity is 0.14 – 0.17 K in this study with the surface radiative forcing of 1.1 W/m2.

Ollila, 2014 (~0.6°C equilibrium)
According to this study the commonly applied radiative forcing (RF) value of 3.7 Wm-2 for CO2 concentration of 560 ppm includes water feedback. The same value without water feedback is 2.16 Wm-2 which is 41.6 % smaller. Spectral analyses show that the contribution of CO2 in the greenhouse (GH) phenomenon is about 11 % and water’s strength in the present climate in comparison to CO2 is 15.2. The author has analyzed the value of the climate sensitivity (CS) and the climate sensitivity parameter (l) using three different calculation bases. These methods include energy balance calculations, infrared radiation absorption in the atmosphere, and the changes in outgoing longwave radiation at the top of the atmosphere. According to the analyzed results, the equilibrium CS (ECS) is at maximum 0.6 °C and the best estimate of l is 0.268 K/(Wm-2 ) without any feedback mechanisms.

Loehle, 2014  (1.1°C  transient, 2.0°C  equilibrium)
Estimated sensitivity is 1.093 °C (transient) and 1.99 °C (equilibrium).  Empirical study sensitivity estimates fall below those based on GCMs.

Skeie et al., 2014  (1.8°C  equilibrium)
Equilibrium climate sensitivity (ECS) is constrained based on observed near-surface temperature change, changes in ocean heat content (OHC) and detailed radiative forcing (RF) time series from pre-industrial times to 2010 for all main anthropogenic and natural forcing mechanism. The RF time series are linked to the observations of OHC and temperature change through an energy balance model (EBM) and a stochastic model, using a Bayesian approach to estimate the ECS and other unknown parameters from the data. For the net anthropogenic RF the posterior mean in 2010 is 2.0 Wm−2, with a 90% credible interval (C.I.) of 1.3 to 2.8 Wm−2, excluding present-day total aerosol effects (direct + indirect) stronger than −1.7 Wm−2. The posterior mean of the ECS is 1.8 °C, with 90% C.I. ranging from 0.9 to 3.2 °C, which is tighter than most previously published estimates.

Scafetta, 2013 (1.5°C)
A quasi 60-year natural oscillation simultaneously explains the 1850–1880, 1910–1940 and 1970–2000 warming periods, the 1880–1910 and 1940–1970 cooling periods and the post 2000 GST plateau. This hypothesis implies that about 50% of the ~ 0.5 °C global surface warming observed from 1970 to 2000 was due to natural oscillations of the climate system, not to anthropogenic forcing as modeled by the CMIP3 and CMIP5 GCMs. Consequently, the climate sensitivity to CO2 doubling should be reduced by half, for example from the 2.0–4.5 °C range (as claimed by the IPCC, 2007) to 1.0–2.3 °C with a likely median of ~ 1.5 °C instead of ~ 3.0 °C.

Asten, 2012 (1.1°C)
Climate sensitivity estimated from the latter is 1.1 ± 0.4 °C (66% confidence) compared with the IPCC central value of 3 °C. The post Eocene-Oligocene transition (33.4 Ma) value of 1.1 °C obtained here is lower than those published from Holocene and Pleistocene glaciation-related temperature data (800 Kya to present) but is of similar order to sensitivity estimates published from satellite observations of tropospheric and sea-surface temperature variations. The value of 1.1 °C is grossly different from estimates up to 9 °C published from paleo-temperature studies of Pliocene (3 to 4 Mya) age sediments. 

Lindzen and Choi, 2011 (0.7°C)
As a result, the climate sensitivity for a doubling of CO2 is estimated to be 0.7K (with the confidence interval 0.5K – 1.3K at 99% levels). This observational result shows that model sensitivities indicated by the IPCC AR4 are likely greater than the possibilities estimated from the observations.

Florides and Christodoulides, 2009 (~0.02°C)
A very recent development on the greenhouse phenomenon is a validated adiabatic model, based on laws of physics, forecasting a maximum temperature-increase of 0.01–0.03 °C for a value doubling the present concentration of atmospheric CO2. 

Gray, 2009 (~0.4°C)
CO2 increases without positive water vapor feedback could only have been responsible for about  0.1 – 0.2 °C of the 0.6-0.7°C global mean surface temperature warming that has been observed since the early 20th  century.  Assuming a doubling of CO2 by the late 21st  century (assuming no  positive water vapor feedback), we should likely expect to see no more than about 0.3-0.5°C global surface warming and certainly not the 2-5°C warming that has been projected by the GCMs [global circulation models].

Chylek et al., 2007 (~0.39°C)
Consequently, both increasing atmospheric concentration of greenhouse gases and decreasing loading of atmospheric aerosols are major contributors to the top-of atmosphere radiative forcing. We find that the climate sensitivity is reduced by at least a factor of 2 when direct and indirect effects of decreasing aerosols are included, compared to the case where the radiative forcing is ascribed only to increases in atmospheric concentrations of carbon dioxide. We find the empirical climate sensitivity to be between 0.29 and 0.48 K/Wm-2 when aerosol direct and indirect radiative forcing is included.
Share this...FacebookTwitter "
"
Share this...FacebookTwitterSometimes you have to wonder which are the biggest fraud: Germany’s claim that its cars are clean, or its claim of being a leader in climate protection. Both, it turns out, are very fake and even downright frauds.
While German Chancellor Angela Merkel and German activists like going around and scolding Donald Trump for his “irresponsible” stance on “greenhouse” gas emissions, it is coming to light that Germany’s climate posturing is indeed a total swindle.
While USA’s greenhouse gas emissions have declined impressively over the past decade, Germany’s have gone nowhere.
Flat for 9 years
And now Cleanenergywire.org here reports that Germany again this year (2017) will again fail to reduce its CO2 equivalent emissions for the 9th year running. Ironically one of the reasons cited for this year by Cleanenergy.org is “cold weather” (again).

Germany’s CO2 equivalent CO2 greenhouse gas emissions in metric tonnes since 2009. This year (2017) CO2 equivalent emissions are expected to be slightly over those seen in 2016. Data taken from German Ministry for environment.


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




The reality is that Trump and America has nothing to learn from the Green-preaching Germans, except on how to deceive and mislead the public. Cleanenergy.org writes that the higher energy demand is “triggered by economic growth and colder weather“.
Cleanenergy.org cites the AG Energiebilanzen (AGEB), which said in a press release: “From January until September, energy demand was 1.9 percent higher than in the same period last year.” And thus the AG Energiebilanzen expects “energy-related CO2 emissions will rise slightly in 2017”.
“A disaster”
Merkel’s glaring failure, however, did not prevent her from taking a swipe at Trump, the Handelsblatt reports. Unfortunately for Merkel there is no disguising Germany’s failure to meet its own imposed targets. The Environment Ministry says the 2017 emissions figures are “a disaster for Germany’s international reputation as a climate leader.”
CO2 emissions reduction is pie-in the sky, another hoax
Whatever gets decided in Bonn will be pure meaningless self-deception. The fact remains that China, India and the rest of the developing world are going to continue boosting their fossil fuel energy consumption and CO2 emissions are going to keep rising for quite awhile. The recent OPEC report makes that very clear.
Some advice for Merkel: Forget the CO2 reductions. Cutting Germany’s puny 2% global share would theoretically lead to a temperature reduction of 1 or 2 hundredths of a degree Celsius, meaning some 100 trillion euros per °C. It’s pure economic insanity. Take the idea and discard it quickly into the dustbin for good.
 
Share this...FacebookTwitter "
"
Share this...FacebookTwitterRecently I wrote about 7 signs showing that the earth has been cooling and likely will continue to cool.
To back this up, Kenneth Richards commented in a reply that this year has seen 7 new peer-reviewed papers that show us that the earth’s surface temperature at the poles and elsewhere has been cooling since about a decade. What’s worrisome is that the southern hemisphere surface is mostly ocean.
Eastern North Atlantic cooling since 2010
The first paper is Gladyshev et al., 2017  which states in its abstract that there’s been “a sharp and stable freshening and cooling of SPMWs [Subpolar Mode Water] in the eastern part of the North Atlantic since 2010 . In the years 2010–2016, the mean temperature of the SPMW [Subpolar Mode Water] core in the Rockall Trough dropped by -0.73°C (-0.12°C/yr); in the Iceland Basin it dropped by -2.12°C (-0.35°C/yr), and salinity decreased by 0.12 psu (0.02 psu/yr) and 0.23 psu (0.04 psu/yr), respectively.”
Subpolar North Atlantic trend reversal in 2005
In another paper, Piecuch et al., 2017,  the authors notes that subpolar North Atlantic (SPNA) is subject to strong decadal variability and found that in 2004–2005 the SPNA decadal upper ocean and sea-surface temperature trends reversed from warming during 1994–2004 to cooling over 2005–2015.
Southern Ocean now cooling
On the other side of the planet at the South Pole the story is pretty much the same. A study this year by Kusahara et al., 2017 showed that in contrast to a strong decrease in Arctic sea ice extent, the overall Antarctic sea ice extent has modestly increased since 1979. The paper’s abstract adds:
Concomitant with this positive trend in Antarctic sea ice, sea surface temperatures (SSTs) over the Southern Ocean south of approximately 45°S have cooled over this period.”


<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Remaining at the South Pole, a new paper by Turney et al., 2017 here states that the Southern Ocean, which occupies a massive 14% of the world’s surface, plays a fundamental role in ocean and atmosphere circulation — and thus climate — and found that it has produced a cooling trend since 1979.
Cooling since 1999
Also Oliva et al., 2017 points out that a recent analysis by Turner et al., 2016 has shown that “the regionally stacked temperature record for the last three decades has shifted from a warming trend of 0.32 °C/decade during 1979–1997 to a cooling trend of − 0.47 °C/decade during 1999–2014“. Oliva et al tell us that “this recent cooling has already impacted the cryosphere in the northern AP [Antarctic Peninsula], including slow-down of glacier recession, a shift to surface mass gains of the peripheral glacier and layer of permafrost in northern AP islands“.
Fernandoy et al., 2017 here also points out:
The firn stable isotope composition reveals that the near–surface temperature at the Antarctic Peninsula shows a decreasing trend (−0.33 °C y−1) between 2008 and 2014.”
“No evidence” of snow decline
Finally, moving on land to the Tibetan Plateau, a recent paper appearing in Nature by Wang et al. 2017 shows there’s been “no evidence of widespread decline of snow cover on the Tibetan Plateau over 2000–2015“.
That’s 7 fresh papers telling us that large, important areas of the earth’s surface have stopped warming and begun cooling. Time is running out for the global warming hoaxsters.
Note: Recently desparate climate warming trolls have been appearing in force (trying to keep their sham alive). Serious comments are welcome, but trolling comments will be deleted.
Share this...FacebookTwitter "
"Greta Thunberg’s father has opened up about how activism helped his daughter out of depression but still worries about how she will deal with the impact of her international fame. Speaking to the BBC to mark his daughter’s guest-editing slot on the Today programme, Svante Thunberg revealed he thought it was a “bad idea” for Greta to stage the school strike that catapulted her into the public eye. The programme also featured a discussion between Greta Thunberg and the veteran naturalist Sir David Attenborough, in which the latter praises the teenager for raising awareness of the climate crisis. She had “achieved things that many of us who have been working on it for 20-odd years have failed to achieve – that is you have aroused the world”, said Sir David, adding that she was the main reason climate was discussed during the British election campaign. Svante Thunberg reveals how activism had changed the outlook of the teenager, who suffered from depression for “three or four years” before she began her school strike protest outside the Swedish parliament. She was now “very happy”, he said. “She stopped talking ... she stopped going to school,” he said of her illness, adding that it was the the “ultimate nightmare for a parent” when Greta began refusing to eat. Svante Thunberg, an actor, said he and his wife, the opera singer Malena Ernman, scaled back their professional lives to spend more time with Greta in order to help her overcome her depression. He became vegan and his wife stopped travelling to concerts by plane. He said Greta became energised about green issues as the family began talking more about environmental issues. He accompanied her on her tour of the United States and visit to the Madrid climate crisis this year “I did all these things, I knew they were the right thing to do ... but I didn’t do it to save the climate, I did it to save my child,” Svante Thunberg said. “I have two daughters and to be honest they are all that matters to me. I just want them to be happy. “You think she’s not ordinary now because she’s special, and she’s very famous, and all these things. But to me she’s now an ordinary child – she can do all the things like other people can,” he said. “She dances around, she laughs a lot, we have a lot of fun – and she’s in a very good place.” He was concerned about the negative comments his daughter attracted in the media and online and “all the hate that that generates”. But his daughter dealt with it “incredibly well”. “Quite frankly, I don’t know how she does it, but she laughs most of the time. She finds it hilarious.”"
"Seafood consumption is both a love and a necessity for hundreds of millions of people worldwide. And its supply is a key part of maintaining food security for the whole planet. But during a time of rapid population growth and increasing demand, stocks of wild fish and invertebrates (such as mussels and prawns) are declining.  The problem is that policies and plans designed to make sure there are enough fish and invertebrates almost exclusively target fishing activity. But we also need to protect the critical habitats that are essential for the sustainability of these stocks and fisheries. Most species that are fished require more than a single habitat to live and thrive. Atlantic cod (Gadus morhua), for example, spends its adult life shoaling in deep water where it lives, feeds and spawns. But juveniles require more stable habitat such as seagrass meadows. So, if we want to manage fish and invertebrate stocks for sustainability reasons, it is essential to protect the supporting habitats of targeted species.  Seagrass meadows are just one of these critical habitats. These large areas of marine flowering plants are abundant in shallow seas on all continents except Antarctica. They support biodiversity and in turn the productivity of the worlds fisheries. As seagrass meadows occur from the intertidal – the area exposed by the daily ebb of the tide – to about a depth of 60 metres in clear waters, they are an easily exploitable fishing habitat.  Though it is clear that seagrasses are a vital part of ocean ecosystems, until now, there has been no information on the role that meadows play in supporting the productivity of world fisheries. But we have now published the first quantitative global evidence on the significant roles that seagrasses play. Nursery grounds in seagrass meadows are a safer, less exposed, environment for eggs to be laid and young animals to find food and protection from predators as they grow. The very fact that they are there means that there are places for commercial fish stocks such as tiger prawns, conch, Atlantic cod and white spotted spinefoot to be caught by global fisheries. In fact, a fifth of the world’s most landed fish – including Atlantic cod and Walleye pollock – benefit from the persistence of extensive seagrass meadows. But it is not just large-scale fishing industries that benefit from the presence of seagrass meadows. As they are an easily accessible fishing ground, small scale artisanal and subsistence fisheries around the world also use them.  Seagrass is also essential for communities that take part in gleaning – fishing for invertebrates such as sea cucumbers in water that is shallow enough to walk in. This is often done by women and children, and provides a source of essential protein and income for some of the most vulnerable people in tropical coastal communities.  It is a common and increasingly visible activity, but it is not usually included in fishery statistics and rarely considered in resource management strategies. And the benefits of seagrasses don’t only lie in the meadows themselves, their presence supports nearby fishing areas, as well as deep water habitats. They do this by creating expansive areas rich in fauna, from which there are vast quantities of living material, organic matter and associated animal biomass that supports other fisheries. Seagrasses also promote the health of connected habitats (like coral reefs), and have the capacity to support whole food webs in deep sea fisheries. The coastal distribution of seagrass means that it is vulnerable to a multitude of threats from both land and sea. These include land runoff, coastal development, boat damage and trawling. On a global scale, seagrass is rapidly declining, and when seagrass is lost associated fisheries and their stocks are likely to become compromised with profound and negative economic consequences.  The importance of seagrass meadows for fisheries productivity and hence food security is not reflected by the policies currently in place. These are urgently needed to continue enjoying the benefits that healthy and productive seagrass meadows provide.  Fisheries management must be broadened from just targeting fishing activity to also targeting the habitats on which fisheries depend. Awareness of the role of seagrass in global fisheries production – and, so, food security – must be central to any policy, and major manageable threats to seagrass, such as declining water quality, must be dealt with.  Seagrass can be a resilient and supportive habitat – but only if we take action to continue to enjoy the benefits it provides."
nan
"Thousands of people fled to the lake and ocean in Mallacoota, as bushfires hit the Gippsland town on Tuesday. The out-of-control fire reached the town in the morning and about 4,000 people fled to the coastline, with Country Fire Authority members working to protect them. The town had not been told to evacuate on Sunday when the rest of East Gippsland was, and authorities decided it was too dangerous to move them on Monday. People reported hearing gas bottles explode as the fire front reached the town, and the sound of sirens telling people to get in the water. By 1.30pm the fire had reached the water’s edge. A local man, Graham, told ABC Gippsland he could see fire in the centre of the town, and 20m high flames on the outskirts where he believed homes were alight. Fire appears at the #Mallacoota water front. Via Snapchat #VictoriaFires pic.twitter.com/mu521uwVnG “We saw a big burst of very big flames in Shady Gully,” he said. “As I speak to you I’m looking across Coull’s Inlet and there are big flames … and they would be impacting houses. That’s not good at all.” People in Mallacoota posted in community social media groups estimates of about 20 houses lost, with the school, bowling club and golf club also hit. Guardian Australia has not been able to confirm these reports. Hundreds more evacuees sheltered in the community centre. “There are a lot of people at the waterfront jetty, in the lake, on the sand spit between the lake and the ocean, and there are people on a sandbar, and some on boats,” Charles Livingstone told Guardian Australia from the community centre. He said there were at least 350 people in the community centre, many with children and pets. He, his wife and their 18-month-old baby were at the jetty on Monday night but moved to the community centre to avoid the heavy smoke. “The CFA advised yesterday they would protect the waterfront jetty and the hard stands that go along the lower lake here, just in front of where we are. They were saying we’ll protect you down there if the worst comes to worst,” he said. “I’m sure the CFA will do what they said, but the relief centre to us seemed like to best option. They’re pretty busy and we haven’t had an update in a while.” 10:30am update from Dad at the wharf in Mallacoota - “fire front not far away” #Mallacoota #bushfirecrisis pic.twitter.com/MvgeiZqujM Livingstone said he was barely thinking about their holiday house to the south of town. “We’ll be happy to get out of here ourselves,” he said. “It’s mayhem out there, it’s armageddon … The other issue is how the hell we’re all going to get out of here – there’s one road in and one road out.” The fire, which hit the town on Tuesday, started on Sunday in Wingan. Livingstone said there had been “confusion”, with roads closing and reopening, and so he and his family had not left. The Victorian premier, Daniel Andrews, said the decision had been made on Monday afternoon that the safest option for people in Mallacoota was to stay there. “At the community level and regional level [authorities] had to work through what their options were and undertake a risk assessment of that,” Andrews said. “We decided it would be unsafe to move them back along the Princes Highway.” Livingstone said the temperature dropped from above 40C to about 20C later in the morning, and people in the water were getting cold. Community radio in #Mallacoota: ""The power is cut. We are isolated. It's a holocaust, basically. Some have been sent to the hall, some to the lake, others have to stay in their homes. There's a lot of thunder. The fires are creating their own weather."" While speaking to Guardian Australia Livingstone said he heard an extraordinarily loud boom outside – Mallacoota does not have gas lines and many people have gas bottles that are likely exploding in the heat. Matt Manning heard it too. He spoke to Guardian Australia from his boat in the lake at the back of Goodwin Sands, about 3km from the centre of town. “It was a big explosion but we don’t know what it was,” he said. “Up until an hour or so ago it was pitch black, you couldn’t see 10 feet in front of you. It was just insane. [Now] it looks like it’s 8.30 in the evening.” Manning, who has been coming to Mallacoota for 20 years, said there were about 30 boats out where he was, most of them locals. He and his wife, their two friends, and his dog had been camping at the foreshore camp park. He packed up the boat on Monday morning, and late last night they all moved to the waterfront to sit out the fire. At about 3am they decided to get in the boat. “We’re pretty safe here, hopefully we don’t get any hot embers,” he said. There’s a lot of debris and ash but so far there’s no hot embers. Winds are about 30-40 knots coming from the south.” Francesca Winterson is in a building on the main street of #Mallacoota and describes the wind, darkness and falling embers as fires burn about 500 metres away.She says it's too late to leave and fire crews are on hand to offer as much protection as they can. pic.twitter.com/6Tjfb4nyUR “We are completely isolated,” community radio broadcaster Francesca Winterson told ABC Breakfast on Tuesday morning. “We’ve been broadcasting for 48 hours without a break and we’re all very tired,” she said. “Now we are here in the station and I’m just watching my town burn.” This picture just in from family boarding boat in #Mallacoota #MallacootaFires approx time of photo 9:45am pic.twitter.com/WJEQScDp9f The township was under one of eight emergency warnings in the East Gippsland area. More than 200,000 hectares have burned, including about 80,000 just in the last 24 hours. Four people were unaccounted for in Victoria on Tuesday morning, and three more in NSW. Fire authorities said there have been “significant losses” of property, but it was too early to confirm numbers. Sister in a BRIGHT ORANGE work suit blending in with the #Mallacoota sky pic.twitter.com/SfK93GhbUU"
"In the late 1960s a patch of land to the east of Amsterdam was reclaimed from the sea for industry. Following the 1973 oil crisis this plan was abandoned and flocks of geese moved in. As the geese grazed the land they created changing mosaics of vegetation and a rich and unique environment spontaneously developed. Dutch ecologists saw this and were inspired by the potential for animal grazing to restore a thriving “self-willed” ecosystem. They proposed a unique experiment to recreate the mix of large herbivores that inhabited the region after the last ice age, around 8,000 years ago, and to let natural forces, rather than human management, decide what environment they would create. The Oostvaardersplassen nature reserve, or OVP, was established on the site in 1986. Founder herds of konick horses, heck cattle and deer were introduced and the reserve became an iconic example of “rewilding”. Recently, however, this 55 square kilometre reserve became front-page news in the Netherlands after images of starving animals spread outrage across social media. Plans to cull 3,000 weakened animals led to protests and prompted activists to throw bales of hay over the fence. Rangers and ecologists associated with the project have even faced death threats. Yet hungry animals at winter’s end is a natural situation. Allowing nature to take its course means that animal numbers will fluctuate, and following a series of mild winters the reserves’s populations of konik horses, heck cattle and deer were unusually high. In a hard winter, like the most recent, the grass stops growing and many animals will starve and die.  Nonetheless, some have suggested the experiment has failed, and that OVP simply creates animal suffering. But the starving animals and public outcry is a failure of politics rather than a failure of rewilding itself. All rewilding projects operate on the principle that herbivore populations should fluctuate as nature intended. This is important because it leads to varied vegetation with lots of different species. As a “first generation” rewilding experiment, the OVP left its animals to fend entirely for themselves.  Elsewhere in the Netherlands, ecologists quickly modified the concept into something more inclusive and entrepreneurial that takes animal welfare into account. This is why second generation rewilding projects, such as those at Gelderse Poort, the Border Meuse and Kempen Broek are less controversial and hence less well known.  These reserves are developing the category of “kept wild”, where herds behave like wild animals and perform the same ecological role, but are also managed by humans to some degree. In these projects, the condition of each animal is assessed at the end of each winter. Those that would suffer and die from starvation or predation if left to nature are fed until they regain their condition in the spring. At this point they are removed to new rewilded areas or harvested and sold as “wild meat”. This approach has created flourishing rewilding areas where visitors can feel the tingle of unease that comes from being in the proximity of large free-living animals. In the Netherlands there are now dozens of such areas along the coast and rivers. Social enterprises have emerged to manage the wilded herds on these second generation projects. Some are developing breeds better suited to living in rewilded landscapes and with public access. Traits include a more docile temperament, smaller udders to reduce injury, or larger horns for defence against wolves and feral dogs.  For rewilding purists, there is a trade-off: carcasses are removed, even though they are key to restoring natural scavengers – anything from vultures to carrion beetles – and the ecological processes they encourage. However, contemporary attitudes to processes of death, decay and decomposition are mostly negative. In places with lots of visitors, people find rewilding principles easier to accept if there are no dead animals around. In April this year, a local government committee advised that the number of large herbivores on the OVP should be “reset” and actively managed at sustainable levels. But, if adopted, the park’s natural cycle of grazing-induced ebbs and flows in different species at different times would be constrained, and the experimental principle lost. The OVP was previously part of a progressive vision – the OostvaardersWold – to create a natural corridor linking it with the Veluwe, a national park to the south. This would have created the conditions for animals to move with the seasons and for predators such as wolves to establish themselves. Although the Dutch state had acquired most of the land in the corridor, the policy was abandoned in 2010 following a change of government and a new minister who thought it a waste to convert good agricultural land to nature. The corridor idea should be reignited. Politicians are wary of trying things again, but ideas are emerging to present a new strategy that integrates “kept wild” approaches. This is important because in the longer term even an expanded system may not avert population booms and the starvation events that follow. Studies from Africa show that it is the availability of food, not the presence of predators, that limit populations of larger herbivores (which are too big for Europe’s lynx and foxes to tackle anyway). Opening up the corridor even in a limited way would enable hungry animals to leave the OVP. They would migrate in social groups and decisions could be made on what to do with each group – a sort of “tap” in the system. The main expense would be building an ecoduct over the A6, a large motorway that runs north from Amsterdam around the edge of the park. The land for the corridor is already bought and the Netherlands has plans to build another 20 ecoducts. Society and rewilding have both moved on since the Oostvaardersplassen was created, but the OVP “experiment” has not been able to do so. It’s time to change this – after all, rewilding should be about the future, not the past."
"
Share this...FacebookTwitterTornadoes have become less frequent since 2010: Pacific ocean cycles control storm frequency
By Dr. Sebastian Lüning and Prof. Fritz Vahrenholt
(German text translated/edited by P Gosselin)
We haven’t heard much about tornadoes lately. For a while they were the favorites among climate activists. When did the love affair end? Here we cast a look at the official NOAA tornado statistics:

Fig. 1: Cumulative curve showing the number of tornadoes. Chart: NOAA.
Here we see 2017 was (fortunately) only average. The tornado trend over the past 60 years below shows the comparisons clearly. From 2005-2010 we saw an increased frequency of tornadoes in the USA, but they’ve since become less frequent. That’s bad news for the purveyors of catastrophe stories.

Fig. 2: Number of tornadoes in the USA since 1950. Source: NOAA.
With respect to the dangers of tornadoes, Hannes Stein asked in 2013 at German daily Welt, why Americans do not build build more stable structures, for example homes made of stone instead of wood:



<!--
google_ad_client = ""ca-pub-3545577860068042"";
/* neu test */
google_ad_slot = ""6412247007"";
google_ad_width = 200;
google_ad_height = 200;
//-->




Tornado damage is terrible – so why don’t Americans build better homes? That’s what Europeans ask, and thus prove their ignorance and arrogance.”
By the way, one finds an excellent display of global winds at ventusky.com.
So why does tornado activity fluctuate so much over the course of decades? Scientists at the University of Missouri found the answer: Tornadoes are influenced by the Pacific Decadal Oscillation (PDO), as explained in a press release dated October 10, 2013:
Pacific Ocean Temperature Influences Tornado Activity in U.S., MU Study Finds
Meteorologists often use information about warm and cold fronts to determine whether a tornado will occur in a particular area. Now, a University of Missouri researcher has found that the temperature of the Pacific Ocean could help scientists predict the type and location of tornado activity in the U.S.
Laurel McCoy, an atmospheric science graduate student at the MU School of Natural Resources, and Tony Lupo, professor and chair of atmospheric science in the College of Agriculture, Food and Natural Resources, surveyed 56,457 tornado-like events from 1950 to 2011. They found that when surface sea temperatures were warmer than average, the U.S. experienced 20.3 percent more tornados that were rated EF-2 to EF-5 on the Enhanced Fuijta (EF) scale. (The EF scale rates the strength of tornados based on the damage they cause. The scale has six category rankings from zero to five.). McCoy and Lupo found that the tornados that occurred when surface sea temperatures were above average were usually located to the west and north of tornado alley, an area in the Midwestern part of the U.S. that experiences more tornados than any other area. McCoy also found that when sea surface temperatures were cooler, more tornadoes tracked from southern states, like Alabama, into Tennessee, Illinois and Indiana.
“Differences in sea temperatures influence the route of the jet stream as it passes over the Pacific and, eventually, to the United States,” McCoy said. “Tornado-producing storms usually are triggered by, and will follow, the jet stream. This helps explain why we found a rise in the number of tornados and a change in their location when sea temperatures fluctuated.” In the study, McCoy and Lupo examined the relationship between tornadoes and a climate phenomenon called the Pacific Decadal Oscillation (PDO). PDO phases, which were discovered in the mid-1990s, are long-term temperature trends that can last up to 30 years. […]. “In the warm phase, which lasted from 1977 to 1999, the west Pacific Ocean became cool and the wedge in the east was warm.”
Also the El Ninos and the La Ninas (ENSO) impact tornadoes, as documented by Lepore et al. 2017:
ENSO-based probabilistic forecasts of March–May U.S. tornado and hail activity
Extended logistic regression is used to predict March–May severe convective storm (SCS) activity based on the preceding December–February (DJF) El Niño–Southern Oscillation (ENSO) state. The spatially resolved probabilistic forecasts are verified against U.S. tornado counts, hail events, and two environmental indices for severe convection. The cross-validated skill is positive for roughly a quarter of the U.S. Overall, indices are predicted with more skill than are storm reports, and hail events are predicted with more skill than tornado counts. Skill is higher in the cool phase of ENSO (La Niña like) when overall SCS activity is higher. SCS forecasts based on the predicted DJF ENSO state from coupled dynamical models initialized in October of the previous year extend the lead time with only a modest reduction in skill compared to forecasts based on the observed DJF ENSO state.”
There are also tornadoes in Germany from time to time. However, there has been no discernable trend over the past 15 years as shown by he Figure 7 in the DWD report.
Share this...FacebookTwitter "
"Climate change is one of the great security challenges of the 21st century. As the world warms, conflicts over water, food or energy will become more common and many people will be forced from their homes. Scientists, think-tanks, NGOs, militaries and even the White House (albeit under President Obama) all agree that climate change threatens human safety and well-being. Yet the organisation charged with global security has remained relatively silent. The UN Security Council, responsible for maintaining international peace and security, is comprised of 15 countries. Five seats are reserved for permanent members with veto powers (China, France, Russia, the UK and the US) while the other ten members are elected to represent their region (“Africa”, “Asia-Pacific” etc) for two year terms.  Together, this semi-rotating group of 15 takes binding decisions for all 193 UN members. This alone makes the Security Council a very powerful institution, but combined with its capacity to sanction, and intervene in the affairs of states it has an influence far exceeding that of any other international body. It is, in many respects, the executive of the international system. For this reason the council has considered contemporary security challenges such as international terrorism, nuclear weapon proliferation, and transnational crime. Positive results include an international crackdown on the financing of terrorism, the sharing of information to tackle various criminal problems, stronger border controls for nuclear materials, and the global mobilisation of experts to address a health epidemic.  The fact the Security Council has helped combat these varied and largely unrelated challenges shows its potential to do good. Yet these interventions also pose the critical question of why it has yet to engage climate change in any meaningful way. Article 41 sanctions would be available to the council in the event of states not meeting their Paris Agreement obligations. Economic sanctions could also be placed upon corporations, that currently operate with relatively little international scrutiny. What the council brings is an ability to coerce – something that is currently lacking throughout international climate law. The council hasn’t entirely ignored climate change, of course. In 2007 the first open debate on the matter took place, though this was based on the unofficial proviso that no binding output would follow. Similar discussions were held in 2011 and 2013 but again stark divides among the members prevented any meaningful outputs. What this represents is a lack of unity over whether climate change really belongs on the agenda. While most states now agree climate change is a priority – as exhibited by the success of the Paris conference in 2015 – there is no consensus on what role, if any, the Security Council should play. From one perspective, countries like New Zealand and Germany view climate change as a security issue of immense proportions and worthy of the council’s attention. On the other hand, states such as China and South Africa argue that if the council engages with climate change it will undermine the sovereignty of states, fracturing the international system. These positions are entrenched, reflecting vastly opposing ideologies in relation to both climate change and international relations, thus precluding any meaningful intervention. Yet this does not necessarily mean that the Security Council is frozen indefinitely. The council has a history of taking tentative steps when moving into new territory, and climate change will not be an exception. In 2011 a statement made by then-president of the Security Council (a position that rotates between member states each month) loosely linked climate change and traditional security challenges. In 2017, the council unanimously adopted Resolution 2349, which hinted that climate change had contributed to conflict and instability around Lake Chad and the wider Sahel region. And in January 2018 a second presidential statement twice referenced climate change in the context of instability in the Sahel region. These statements fall short of finding climate change an explicit security threat, but do they show the council is steadily becoming more comfortable with the subject. And without that degree of comfort we would likely not have seen the passing of Resolution 2408 on March 27, 2018. This resolution, again adopted unanimously, extended the mandate of the UN mission in Somalia for another year and became the latest council resolution to include reference to climate change. The language remains speculative and the council is careful to only recall its 2011 statement instead of making a bolder standalone declaration on climate security. However, inclusion of the expression “grave concern” in regard to the drought and famine engulfing Somalia is proof that the council is experiencing a change of perspective. It is beginning to make discursive links between environmental realities and security, using the language often reserved for terrorism or nuclear weapon proliferation. The resolution fails to indict climate change as the cause of these problems yet it is nonetheless progress. After years of dispute council members are starting to agree on the inclusion of the words “climate change” in a resolution – a big step forward for the world’s most powerful but politically polarised body. So where are we? The Security Council has access to the tools the world so desperately needs to enforce state and private action on climate change, and although it is taking its time there is some advancement. That does not mean climate change is about to be recognised as a security concern in its own right, but each step taken is valuable and the council is certainly on the right path to identifying climate change as the security threat it so clearly is."
nan
